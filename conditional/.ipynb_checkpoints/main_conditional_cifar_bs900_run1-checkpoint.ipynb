{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='3,4,5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "if args.controlled_tol:\n",
      "    import lib.odenvp_conditional_tol as odenvp\n",
      "else:\n",
      "    import lib.odenvp_conditional as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz = modules.GaussianDiag.logp(mean, logs, z).view(-1,1)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(z)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol},)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z = modules.GaussianDiag.sample(mean, logs)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            loss.backward()\n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, conditional=True, controlled_tol=False, conv=True, data='mnist', dims='64,64,64', divergence_fn='approximate', dl2int=None, imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume=None, rtol=1e-05, save='../experiments_published/cnf_conditional_bs900_run1', seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5)\n",
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=1568, bias=True)\n",
      "  (project_class): LinearZeros(in_features=784, out_features=10, bias=True)\n",
      ")\n",
      "Number of trainable parameters: 828890\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc.py:286: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "Iter 0010 | Time 7.5142(17.1166) | Bit/dim 17.7869(19.1112) | Xent 2.2718(2.2991) | Loss 18.9228(20.2608) | Error 0.8544(0.8769) Steps 410(410.00) | Grad Norm 127.8008(142.7265) | Total Time 10.00(10.00)\n",
      "Iter 0020 | Time 7.6257(14.6321) | Bit/dim 14.5767(18.2983) | Xent 2.1876(2.2806) | Loss 15.6705(19.4386) | Error 0.4889(0.8263) Steps 410(410.00) | Grad Norm 87.2380(133.0006) | Total Time 10.00(10.00)\n",
      "Iter 0030 | Time 7.7812(12.8098) | Bit/dim 12.0926(16.9316) | Xent 2.0649(2.2387) | Loss 13.1250(18.0510) | Error 0.3067(0.7062) Steps 410(410.00) | Grad Norm 43.2170(113.9536) | Total Time 10.00(10.00)\n",
      "Iter 0040 | Time 7.6622(11.4781) | Bit/dim 10.1912(15.3649) | Xent 1.9279(2.1730) | Loss 11.1551(16.4514) | Error 0.2378(0.5903) Steps 410(410.00) | Grad Norm 21.6441(91.6936) | Total Time 10.00(10.00)\n",
      "Iter 0050 | Time 7.5273(10.4768) | Bit/dim 8.7335(13.7813) | Xent 1.8031(2.0893) | Loss 9.6351(14.8260) | Error 0.2522(0.5010) Steps 410(410.00) | Grad Norm 14.8531(71.9714) | Total Time 10.00(10.00)\n",
      "Iter 0060 | Time 7.6913(9.7380) | Bit/dim 7.3116(12.2427) | Xent 1.7032(1.9970) | Loss 8.1632(13.2413) | Error 0.2344(0.4330) Steps 410(410.00) | Grad Norm 15.0186(57.0667) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0001 | Time 39.0518, Epoch Time 574.2391(574.2391), Bit/dim 6.5058(best: inf), Xent 1.6445, Loss 7.3280, Error 0.2158(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0070 | Time 7.5988(9.2069) | Bit/dim 6.0431(10.7667) | Xent 1.6232(1.9091) | Loss 6.8547(11.7213) | Error 0.2000(0.3783) Steps 410(410.00) | Grad Norm 11.8791(45.5336) | Total Time 10.00(10.00)\n",
      "Iter 0080 | Time 7.6892(8.8028) | Bit/dim 4.8696(9.3430) | Xent 1.6336(1.8386) | Loss 5.6864(10.2623) | Error 0.2044(0.3389) Steps 410(410.00) | Grad Norm 10.4076(36.4391) | Total Time 10.00(10.00)\n",
      "Iter 0090 | Time 7.8994(8.5064) | Bit/dim 3.8628(8.0089) | Xent 1.6803(1.7933) | Loss 4.7029(8.9056) | Error 0.2222(0.3103) Steps 416(410.18) | Grad Norm 8.2805(29.3272) | Total Time 10.00(10.00)\n",
      "Iter 0100 | Time 8.3306(8.4223) | Bit/dim 3.2081(6.8178) | Xent 1.7645(1.7771) | Loss 4.0904(7.7063) | Error 0.2511(0.2918) Steps 422(413.01) | Grad Norm 5.4128(23.3597) | Total Time 10.00(10.00)\n",
      "Iter 0110 | Time 8.2182(8.3539) | Bit/dim 2.8312(5.8036) | Xent 1.8413(1.7878) | Loss 3.7519(6.6975) | Error 0.2711(0.2847) Steps 422(415.37) | Grad Norm 3.6575(18.3559) | Total Time 10.00(10.00)\n",
      "Iter 0120 | Time 8.6442(8.3476) | Bit/dim 2.5826(4.9785) | Xent 1.9062(1.8123) | Loss 3.5356(5.8846) | Error 0.3011(0.2840) Steps 440(418.91) | Grad Norm 2.4560(14.2951) | Total Time 10.00(10.00)\n",
      "Iter 0130 | Time 8.7065(8.4862) | Bit/dim 2.4258(4.3240) | Xent 1.9245(1.8387) | Loss 3.3880(5.2433) | Error 0.2889(0.2853) Steps 440(424.45) | Grad Norm 1.8928(11.0896) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0002 | Time 38.9457, Epoch Time 591.3101(574.7512), Bit/dim 2.3937(best: 6.5058), Xent 1.9125, Loss 3.3500, Error 0.2713(best: 0.2158)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0140 | Time 8.7891(8.5535) | Bit/dim 2.3413(3.8135) | Xent 1.9219(1.8594) | Loss 3.3022(4.7432) | Error 0.2989(0.2846) Steps 440(428.53) | Grad Norm 1.5853(8.6292) | Total Time 10.00(10.00)\n",
      "Iter 0150 | Time 8.7791(8.6217) | Bit/dim 2.2877(3.4200) | Xent 1.8890(1.8716) | Loss 3.2322(4.3558) | Error 0.2600(0.2833) Steps 440(431.54) | Grad Norm 1.3912(6.7477) | Total Time 10.00(10.00)\n",
      "Iter 0160 | Time 8.7262(8.6453) | Bit/dim 2.2618(3.1191) | Xent 1.8609(1.8717) | Loss 3.1922(4.0550) | Error 0.2644(0.2773) Steps 440(433.76) | Grad Norm 1.2994(5.3291) | Total Time 10.00(10.00)\n",
      "Iter 0170 | Time 8.5766(8.6471) | Bit/dim 2.2451(2.8919) | Xent 1.8009(1.8595) | Loss 3.1455(3.8217) | Error 0.2544(0.2695) Steps 434(434.88) | Grad Norm 1.2961(4.2648) | Total Time 10.00(10.00)\n",
      "Iter 0180 | Time 8.3149(8.5921) | Bit/dim 2.2323(2.7199) | Xent 1.7350(1.8385) | Loss 3.0998(3.6392) | Error 0.2444(0.2677) Steps 434(434.65) | Grad Norm 1.2412(3.4656) | Total Time 10.00(10.00)\n",
      "Iter 0190 | Time 8.0337(8.5249) | Bit/dim 2.2381(2.5915) | Xent 1.6662(1.8027) | Loss 3.0712(3.4929) | Error 0.2211(0.2600) Steps 428(433.18) | Grad Norm 1.2024(2.8803) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0003 | Time 39.0194, Epoch Time 620.0789(576.1110), Bit/dim 2.2183(best: 2.3937), Xent 1.5924, Loss 3.0145, Error 0.2244(best: 0.2158)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0200 | Time 8.0391(8.4648) | Bit/dim 2.2269(2.4949) | Xent 1.5963(1.7586) | Loss 3.0250(3.3741) | Error 0.2489(0.2551) Steps 428(431.82) | Grad Norm 1.1690(2.4414) | Total Time 10.00(10.00)\n",
      "Iter 0210 | Time 8.2214(8.4148) | Bit/dim 2.2252(2.4230) | Xent 1.5220(1.7032) | Loss 2.9861(3.2746) | Error 0.2500(0.2478) Steps 428(430.82) | Grad Norm 1.1626(2.1062) | Total Time 10.00(10.00)\n",
      "Iter 0220 | Time 8.2998(8.4114) | Bit/dim 2.2119(2.3694) | Xent 1.4131(1.6392) | Loss 2.9184(3.1890) | Error 0.2089(0.2404) Steps 428(430.08) | Grad Norm 1.1124(1.8492) | Total Time 10.00(10.00)\n",
      "Iter 0230 | Time 8.3161(8.3945) | Bit/dim 2.2246(2.3304) | Xent 1.3063(1.5653) | Loss 2.8777(3.1130) | Error 0.1833(0.2300) Steps 428(429.53) | Grad Norm 1.1345(1.6597) | Total Time 10.00(10.00)\n",
      "Iter 0240 | Time 8.1961(8.4074) | Bit/dim 2.2199(2.3003) | Xent 1.2375(1.4918) | Loss 2.8386(3.0462) | Error 0.1822(0.2223) Steps 428(429.13) | Grad Norm 1.0720(1.5103) | Total Time 10.00(10.00)\n",
      "Iter 0250 | Time 8.3713(8.4109) | Bit/dim 2.2040(2.2765) | Xent 1.1705(1.4148) | Loss 2.7893(2.9838) | Error 0.2011(0.2150) Steps 428(428.83) | Grad Norm 1.0570(1.4007) | Total Time 10.00(10.00)\n",
      "Iter 0260 | Time 8.1618(8.3757) | Bit/dim 2.1862(2.2548) | Xent 1.0905(1.3369) | Loss 2.7315(2.9232) | Error 0.1711(0.2057) Steps 428(428.61) | Grad Norm 1.0047(1.3132) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0004 | Time 38.5815, Epoch Time 604.4637(576.9616), Bit/dim 2.1805(best: 2.2183), Xent 1.0383, Loss 2.6997, Error 0.1696(best: 0.2158)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0270 | Time 8.6750(8.3638) | Bit/dim 2.1687(2.2351) | Xent 1.0337(1.2604) | Loss 2.6856(2.8654) | Error 0.1811(0.1966) Steps 428(428.45) | Grad Norm 0.9457(1.2278) | Total Time 10.00(10.00)\n",
      "Iter 0280 | Time 8.7402(8.3686) | Bit/dim 2.1487(2.2163) | Xent 0.9712(1.1912) | Loss 2.6343(2.8119) | Error 0.1844(0.1917) Steps 428(428.33) | Grad Norm 0.9884(1.1539) | Total Time 10.00(10.00)\n",
      "Iter 0290 | Time 8.4326(8.3701) | Bit/dim 2.1540(2.1986) | Xent 0.9122(1.1241) | Loss 2.6101(2.7607) | Error 0.1567(0.1855) Steps 434(429.54) | Grad Norm 1.0071(1.1037) | Total Time 10.00(10.00)\n",
      "Iter 0300 | Time 8.2828(8.3925) | Bit/dim 2.1249(2.1829) | Xent 0.8730(1.0639) | Loss 2.5614(2.7148) | Error 0.1678(0.1809) Steps 434(430.71) | Grad Norm 0.8387(1.0500) | Total Time 10.00(10.00)\n",
      "Iter 0310 | Time 8.2175(8.3779) | Bit/dim 2.1195(2.1654) | Xent 0.8197(1.0041) | Loss 2.5294(2.6675) | Error 0.1711(0.1748) Steps 434(431.58) | Grad Norm 0.8109(1.0048) | Total Time 10.00(10.00)\n",
      "Iter 0320 | Time 8.1414(8.3872) | Bit/dim 2.0872(2.1472) | Xent 0.7707(0.9478) | Loss 2.4725(2.6211) | Error 0.1478(0.1687) Steps 434(432.21) | Grad Norm 1.0509(0.9711) | Total Time 10.00(10.00)\n",
      "Iter 0330 | Time 8.3279(8.4023) | Bit/dim 2.0725(2.1309) | Xent 0.7610(0.8976) | Loss 2.4530(2.5797) | Error 0.1644(0.1649) Steps 434(432.68) | Grad Norm 0.9349(0.9842) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0005 | Time 39.3756, Epoch Time 608.7580(577.9155), Bit/dim 2.0696(best: 2.1805), Xent 0.7245, Loss 2.4318, Error 0.1414(best: 0.1696)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0340 | Time 8.4464(8.4161) | Bit/dim 2.0763(2.1134) | Xent 0.7285(0.8553) | Loss 2.4405(2.5411) | Error 0.1489(0.1621) Steps 434(433.03) | Grad Norm 1.0551(0.9872) | Total Time 10.00(10.00)\n",
      "Iter 0350 | Time 8.7133(8.4928) | Bit/dim 2.0437(2.0965) | Xent 0.7200(0.8175) | Loss 2.4037(2.5053) | Error 0.1522(0.1580) Steps 440(434.72) | Grad Norm 1.5084(1.0008) | Total Time 10.00(10.00)\n",
      "Iter 0360 | Time 8.5764(8.5471) | Bit/dim 2.0252(2.0813) | Xent 0.6713(0.7776) | Loss 2.3609(2.4701) | Error 0.1544(0.1530) Steps 440(436.10) | Grad Norm 1.4193(1.0520) | Total Time 10.00(10.00)\n",
      "Iter 0370 | Time 8.8514(8.6083) | Bit/dim 2.0124(2.0644) | Xent 0.6263(0.7409) | Loss 2.3256(2.4349) | Error 0.1378(0.1493) Steps 440(437.13) | Grad Norm 1.8353(1.1489) | Total Time 10.00(10.00)\n",
      "Iter 0380 | Time 8.6738(8.6503) | Bit/dim 1.9960(2.0495) | Xent 0.6092(0.7107) | Loss 2.3007(2.4049) | Error 0.1333(0.1467) Steps 440(437.88) | Grad Norm 1.2396(1.2133) | Total Time 10.00(10.00)\n",
      "Iter 0390 | Time 8.7352(8.6846) | Bit/dim 1.9808(2.0334) | Xent 0.6636(0.6895) | Loss 2.3126(2.3782) | Error 0.1611(0.1466) Steps 440(438.44) | Grad Norm 0.8264(1.2455) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0006 | Time 39.3677, Epoch Time 629.0188(579.4486), Bit/dim 1.9780(best: 2.0696), Xent 0.5670, Loss 2.2615, Error 0.1240(best: 0.1414)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0400 | Time 8.6808(8.7030) | Bit/dim 1.9767(2.0197) | Xent 0.5894(0.6660) | Loss 2.2714(2.3527) | Error 0.1333(0.1443) Steps 440(438.85) | Grad Norm 1.4814(1.3515) | Total Time 10.00(10.00)\n",
      "Iter 0410 | Time 8.6825(8.7205) | Bit/dim 1.9621(2.0068) | Xent 0.5873(0.6437) | Loss 2.2557(2.3287) | Error 0.1456(0.1416) Steps 440(439.15) | Grad Norm 2.5787(1.5601) | Total Time 10.00(10.00)\n",
      "Iter 0420 | Time 9.1344(8.7338) | Bit/dim 1.9453(1.9933) | Xent 0.5574(0.6224) | Loss 2.2240(2.3045) | Error 0.1244(0.1385) Steps 440(439.37) | Grad Norm 3.2514(1.9561) | Total Time 10.00(10.00)\n",
      "Iter 0430 | Time 8.7886(8.7530) | Bit/dim 1.9339(1.9803) | Xent 0.5597(0.6083) | Loss 2.2137(2.2845) | Error 0.1422(0.1384) Steps 440(439.54) | Grad Norm 4.9729(2.2817) | Total Time 10.00(10.00)\n",
      "Iter 0440 | Time 9.1442(8.7989) | Bit/dim 1.9201(1.9677) | Xent 0.5412(0.5899) | Loss 2.1907(2.2626) | Error 0.1356(0.1366) Steps 446(440.66) | Grad Norm 4.1749(2.4828) | Total Time 10.00(10.00)\n",
      "Iter 0450 | Time 8.8240(8.8179) | Bit/dim 1.9099(1.9541) | Xent 0.5573(0.5766) | Loss 2.1886(2.2424) | Error 0.1433(0.1364) Steps 446(442.06) | Grad Norm 1.5015(2.4026) | Total Time 10.00(10.00)\n",
      "Iter 0460 | Time 8.8619(8.8408) | Bit/dim 1.8974(1.9404) | Xent 0.5382(0.5587) | Loss 2.1664(2.2197) | Error 0.1500(0.1330) Steps 446(443.10) | Grad Norm 1.8397(2.4191) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0007 | Time 40.4492, Epoch Time 638.5105(581.2204), Bit/dim 1.8819(best: 1.9780), Xent 0.4925, Loss 2.1282, Error 0.1197(best: 0.1240)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0470 | Time 8.8063(8.8463) | Bit/dim 1.8769(1.9259) | Xent 0.4709(0.5431) | Loss 2.1124(2.1974) | Error 0.1133(0.1296) Steps 446(443.86) | Grad Norm 1.9284(2.4801) | Total Time 10.00(10.00)\n",
      "Iter 0480 | Time 8.9018(8.9043) | Bit/dim 1.8344(1.9096) | Xent 0.5032(0.5253) | Loss 2.0860(2.1722) | Error 0.1322(0.1269) Steps 446(444.42) | Grad Norm 1.8203(2.6174) | Total Time 10.00(10.00)\n",
      "Iter 0490 | Time 8.8575(8.8751) | Bit/dim 1.8211(1.8883) | Xent 0.4928(0.5201) | Loss 2.0675(2.1484) | Error 0.1422(0.1284) Steps 446(444.84) | Grad Norm 4.9572(2.9756) | Total Time 10.00(10.00)\n",
      "Iter 0500 | Time 9.1589(8.9711) | Bit/dim 1.7749(1.8635) | Xent 0.4799(0.5095) | Loss 2.0148(2.1183) | Error 0.1367(0.1282) Steps 452(445.99) | Grad Norm 2.3196(2.8832) | Total Time 10.00(10.00)\n",
      "Iter 0510 | Time 9.1483(9.0338) | Bit/dim 1.7404(1.8357) | Xent 0.4774(0.5008) | Loss 1.9792(2.0861) | Error 0.1411(0.1270) Steps 452(447.57) | Grad Norm 1.1879(2.6085) | Total Time 10.00(10.00)\n",
      "Iter 0520 | Time 9.2909(9.1016) | Bit/dim 1.6881(1.8052) | Xent 0.4534(0.4888) | Loss 1.9148(2.0496) | Error 0.1311(0.1253) Steps 452(448.73) | Grad Norm 1.6792(2.5060) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0008 | Time 40.3756, Epoch Time 655.5709(583.4510), Bit/dim 1.6796(best: 1.8819), Xent 0.4282, Loss 1.8937, Error 0.1083(best: 0.1197)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0530 | Time 9.3743(9.1291) | Bit/dim 1.6958(1.7773) | Xent 0.4394(0.4761) | Loss 1.9155(2.0154) | Error 0.1189(0.1231) Steps 452(449.59) | Grad Norm 3.5067(2.4093) | Total Time 10.00(10.00)\n",
      "Iter 0540 | Time 9.5145(9.1456) | Bit/dim 1.6643(1.7499) | Xent 0.3938(0.4630) | Loss 1.8612(1.9814) | Error 0.1022(0.1203) Steps 452(450.38) | Grad Norm 2.7296(2.4359) | Total Time 10.00(10.00)\n",
      "Iter 0550 | Time 9.7970(9.2932) | Bit/dim 1.6263(1.7234) | Xent 0.4416(0.4561) | Loss 1.8471(1.9514) | Error 0.1156(0.1192) Steps 476(453.40) | Grad Norm 7.7975(2.9668) | Total Time 10.00(10.00)\n",
      "Iter 0560 | Time 9.7972(9.4097) | Bit/dim 1.6481(1.7007) | Xent 0.4396(0.4491) | Loss 1.8679(1.9252) | Error 0.1256(0.1183) Steps 476(456.87) | Grad Norm 8.7240(4.0007) | Total Time 10.00(10.00)\n",
      "Iter 0570 | Time 10.0514(9.4919) | Bit/dim 1.6048(1.6769) | Xent 0.3466(0.4382) | Loss 1.7782(1.8960) | Error 0.0900(0.1157) Steps 464(458.74) | Grad Norm 5.0730(4.1092) | Total Time 10.00(10.00)\n",
      "Iter 0580 | Time 9.4933(9.5530) | Bit/dim 1.6188(1.6558) | Xent 0.3717(0.4274) | Loss 1.8046(1.8695) | Error 0.1233(0.1143) Steps 464(460.47) | Grad Norm 9.6345(4.6112) | Total Time 10.00(10.00)\n",
      "Iter 0590 | Time 9.5331(9.5861) | Bit/dim 1.5566(1.6369) | Xent 0.4506(0.4249) | Loss 1.7819(1.8493) | Error 0.1267(0.1157) Steps 464(462.00) | Grad Norm 7.4509(5.3953) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0009 | Time 43.4853, Epoch Time 692.9692(586.7365), Bit/dim 1.5625(best: 1.6796), Xent 0.3756, Loss 1.7503, Error 0.1001(best: 0.1083)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0600 | Time 9.7459(9.6090) | Bit/dim 1.5441(1.6196) | Xent 0.4110(0.4184) | Loss 1.7496(1.8288) | Error 0.1267(0.1142) Steps 464(462.83) | Grad Norm 3.7908(5.1718) | Total Time 10.00(10.00)\n",
      "Iter 0610 | Time 10.0083(9.7187) | Bit/dim 1.5532(1.6018) | Xent 0.3370(0.4099) | Loss 1.7217(1.8068) | Error 0.0978(0.1116) Steps 482(466.19) | Grad Norm 3.6206(4.7949) | Total Time 10.00(10.00)\n",
      "Iter 0620 | Time 10.0292(9.7756) | Bit/dim 1.5850(1.5889) | Xent 0.4153(0.4024) | Loss 1.7926(1.7901) | Error 0.1222(0.1095) Steps 464(468.67) | Grad Norm 16.2553(5.8629) | Total Time 10.00(10.00)\n",
      "Iter 0630 | Time 10.3909(9.8436) | Bit/dim 1.5226(1.5745) | Xent 0.3597(0.3943) | Loss 1.7024(1.7717) | Error 0.1022(0.1081) Steps 488(471.53) | Grad Norm 7.1236(6.3058) | Total Time 10.00(10.00)\n",
      "Iter 0640 | Time 9.9862(9.9073) | Bit/dim 1.5229(1.5607) | Xent 0.3788(0.3887) | Loss 1.7123(1.7550) | Error 0.1089(0.1072) Steps 488(475.30) | Grad Norm 8.4857(5.7436) | Total Time 10.00(10.00)\n",
      "Iter 0650 | Time 9.9326(9.9790) | Bit/dim 1.5369(1.5498) | Xent 0.3732(0.3861) | Loss 1.7235(1.7428) | Error 0.1056(0.1067) Steps 482(477.98) | Grad Norm 9.2626(6.1529) | Total Time 10.00(10.00)\n",
      "Iter 0660 | Time 10.0470(10.0254) | Bit/dim 1.5068(1.5369) | Xent 0.3858(0.3789) | Loss 1.6997(1.7263) | Error 0.1067(0.1054) Steps 482(480.15) | Grad Norm 3.5012(5.8971) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0010 | Time 44.3049, Epoch Time 721.7430(590.7867), Bit/dim 1.4874(best: 1.5625), Xent 0.3437, Loss 1.6593, Error 0.0959(best: 0.1001)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0670 | Time 9.4787(10.0596) | Bit/dim 1.5168(1.5308) | Xent 0.3228(0.3767) | Loss 1.6782(1.7191) | Error 0.0933(0.1047) Steps 470(481.05) | Grad Norm 15.5407(7.8489) | Total Time 10.00(10.00)\n",
      "Iter 0680 | Time 9.9814(10.1011) | Bit/dim 1.5033(1.5236) | Xent 0.3305(0.3726) | Loss 1.6686(1.7099) | Error 0.0911(0.1031) Steps 482(482.39) | Grad Norm 9.4187(8.4547) | Total Time 10.00(10.00)\n",
      "Iter 0690 | Time 10.0348(10.1461) | Bit/dim 1.4713(1.5128) | Xent 0.3002(0.3651) | Loss 1.6214(1.6953) | Error 0.0811(0.1013) Steps 482(482.89) | Grad Norm 3.2388(7.8729) | Total Time 10.00(10.00)\n",
      "Iter 0700 | Time 9.9947(10.1583) | Bit/dim 1.4915(1.5019) | Xent 0.3202(0.3576) | Loss 1.6516(1.6807) | Error 0.1022(0.1003) Steps 482(483.91) | Grad Norm 8.5267(7.4302) | Total Time 10.00(10.00)\n",
      "Iter 0710 | Time 10.1351(10.1666) | Bit/dim 1.4479(1.4924) | Xent 0.3337(0.3510) | Loss 1.6147(1.6679) | Error 0.0989(0.0989) Steps 482(484.04) | Grad Norm 6.7788(6.6110) | Total Time 10.00(10.00)\n",
      "Iter 0720 | Time 10.3331(10.1950) | Bit/dim 1.5121(1.4894) | Xent 0.3540(0.3507) | Loss 1.6891(1.6648) | Error 0.1033(0.0998) Steps 488(484.78) | Grad Norm 18.8217(8.4087) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0011 | Time 44.5111, Epoch Time 734.5156(595.0986), Bit/dim 1.4561(best: 1.4874), Xent 0.3209, Loss 1.6165, Error 0.0941(best: 0.0959)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0730 | Time 10.1308(10.2102) | Bit/dim 1.4488(1.4800) | Xent 0.3357(0.3455) | Loss 1.6166(1.6527) | Error 0.1111(0.0991) Steps 488(485.32) | Grad Norm 7.5822(8.4396) | Total Time 10.00(10.00)\n",
      "Iter 0740 | Time 10.2938(10.2365) | Bit/dim 1.4495(1.4711) | Xent 0.3116(0.3411) | Loss 1.6053(1.6417) | Error 0.1000(0.0982) Steps 494(485.62) | Grad Norm 3.3925(7.8387) | Total Time 10.00(10.00)\n",
      "Iter 0750 | Time 9.8358(10.2276) | Bit/dim 1.4430(1.4637) | Xent 0.3250(0.3393) | Loss 1.6055(1.6334) | Error 0.0978(0.0987) Steps 482(486.41) | Grad Norm 15.2393(8.1588) | Total Time 10.00(10.00)\n",
      "Iter 0760 | Time 10.7183(10.2858) | Bit/dim 1.4161(1.4570) | Xent 0.3655(0.3360) | Loss 1.5988(1.6250) | Error 0.0833(0.0977) Steps 500(487.83) | Grad Norm 11.1315(8.8727) | Total Time 10.00(10.00)\n",
      "Iter 0770 | Time 10.7032(10.2748) | Bit/dim 1.4194(1.4542) | Xent 0.3813(0.3392) | Loss 1.6101(1.6238) | Error 0.0967(0.0992) Steps 500(488.37) | Grad Norm 12.2891(9.7273) | Total Time 10.00(10.00)\n",
      "Iter 0780 | Time 10.1201(10.2677) | Bit/dim 1.4580(1.4583) | Xent 0.3415(0.3500) | Loss 1.6288(1.6333) | Error 0.1067(0.1026) Steps 482(488.23) | Grad Norm 13.9202(11.8684) | Total Time 10.00(10.00)\n",
      "Iter 0790 | Time 10.6643(10.2713) | Bit/dim 1.4275(1.4521) | Xent 0.3297(0.3477) | Loss 1.5923(1.6260) | Error 0.1111(0.1022) Steps 512(491.85) | Grad Norm 3.0271(10.3646) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0012 | Time 45.4899, Epoch Time 739.7368(599.4377), Bit/dim 1.4133(best: 1.4561), Xent 0.3116, Loss 1.5691, Error 0.0898(best: 0.0941)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0800 | Time 10.3104(10.3665) | Bit/dim 1.3952(1.4433) | Xent 0.3485(0.3431) | Loss 1.5694(1.6149) | Error 0.1056(0.1008) Steps 506(496.94) | Grad Norm 5.6732(9.2538) | Total Time 10.00(10.00)\n",
      "Iter 0810 | Time 11.1311(10.4171) | Bit/dim 1.4370(1.4384) | Xent 0.3406(0.3366) | Loss 1.6073(1.6067) | Error 0.0867(0.0994) Steps 506(498.30) | Grad Norm 22.3857(10.1481) | Total Time 10.00(10.00)\n",
      "Iter 0820 | Time 10.0949(10.4178) | Bit/dim 1.3968(1.4344) | Xent 0.2796(0.3325) | Loss 1.5366(1.6007) | Error 0.0844(0.0982) Steps 488(497.44) | Grad Norm 8.4241(10.5300) | Total Time 10.00(10.00)\n",
      "Iter 0830 | Time 10.0505(10.3621) | Bit/dim 1.3640(1.4247) | Xent 0.2944(0.3286) | Loss 1.5112(1.5890) | Error 0.0933(0.0971) Steps 482(496.17) | Grad Norm 4.7311(8.9535) | Total Time 10.00(10.00)\n",
      "Iter 0840 | Time 9.9158(10.3619) | Bit/dim 1.4133(1.4176) | Xent 0.2653(0.3153) | Loss 1.5459(1.5753) | Error 0.0778(0.0937) Steps 482(494.45) | Grad Norm 15.0447(8.5989) | Total Time 10.00(10.00)\n",
      "Iter 0850 | Time 10.1968(10.3651) | Bit/dim 1.3961(1.4114) | Xent 0.3370(0.3164) | Loss 1.5646(1.5696) | Error 0.1022(0.0944) Steps 494(493.02) | Grad Norm 15.3945(8.7093) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0013 | Time 44.3087, Epoch Time 746.4774(603.8489), Bit/dim 1.3812(best: 1.4133), Xent 0.2893, Loss 1.5259, Error 0.0807(best: 0.0898)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0860 | Time 10.5599(10.3579) | Bit/dim 1.4079(1.4124) | Xent 0.3200(0.3106) | Loss 1.5679(1.5677) | Error 0.0878(0.0924) Steps 500(493.11) | Grad Norm 15.9923(10.5720) | Total Time 10.00(10.00)\n",
      "Iter 0870 | Time 10.1932(10.3468) | Bit/dim 1.3857(1.4095) | Xent 0.2692(0.3085) | Loss 1.5203(1.5638) | Error 0.0811(0.0920) Steps 494(493.70) | Grad Norm 5.9821(11.1007) | Total Time 10.00(10.00)\n",
      "Iter 0880 | Time 10.2037(10.3438) | Bit/dim 1.4049(1.4053) | Xent 0.2912(0.3060) | Loss 1.5505(1.5583) | Error 0.0789(0.0907) Steps 488(494.56) | Grad Norm 16.1487(11.4755) | Total Time 10.00(10.00)\n",
      "Iter 0890 | Time 10.1343(10.3470) | Bit/dim 1.3802(1.4004) | Xent 0.2724(0.3042) | Loss 1.5164(1.5526) | Error 0.0833(0.0907) Steps 500(495.55) | Grad Norm 4.1755(11.2941) | Total Time 10.00(10.00)\n",
      "Iter 0900 | Time 10.7193(10.3480) | Bit/dim 1.3859(1.3945) | Xent 0.3301(0.3050) | Loss 1.5510(1.5470) | Error 0.0889(0.0909) Steps 500(495.94) | Grad Norm 9.3499(10.3008) | Total Time 10.00(10.00)\n",
      "Iter 0910 | Time 10.4962(10.4103) | Bit/dim 1.3620(1.3905) | Xent 0.2463(0.2996) | Loss 1.4851(1.5403) | Error 0.0889(0.0904) Steps 500(496.87) | Grad Norm 7.7158(10.7360) | Total Time 10.00(10.00)\n",
      "Iter 0920 | Time 10.1950(10.4653) | Bit/dim 1.3920(1.3905) | Xent 0.2900(0.2954) | Loss 1.5370(1.5382) | Error 0.0756(0.0891) Steps 494(498.11) | Grad Norm 13.9830(11.9843) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0014 | Time 45.6444, Epoch Time 749.1806(608.2089), Bit/dim 1.3813(best: 1.3812), Xent 0.2675, Loss 1.5151, Error 0.0791(best: 0.0807)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0930 | Time 11.3520(10.5083) | Bit/dim 1.3698(1.3887) | Xent 0.3643(0.2997) | Loss 1.5519(1.5386) | Error 0.1111(0.0907) Steps 524(499.97) | Grad Norm 17.0382(12.3963) | Total Time 10.00(10.00)\n",
      "Iter 0940 | Time 10.3150(10.5538) | Bit/dim 1.4238(1.3929) | Xent 0.3074(0.2991) | Loss 1.5775(1.5425) | Error 0.0956(0.0902) Steps 488(500.64) | Grad Norm 21.7859(14.0660) | Total Time 10.00(10.00)\n",
      "Iter 0950 | Time 10.3670(10.5548) | Bit/dim 1.3599(1.3932) | Xent 0.3132(0.2968) | Loss 1.5165(1.5416) | Error 0.0944(0.0895) Steps 494(500.32) | Grad Norm 5.8641(13.9262) | Total Time 10.00(10.00)\n",
      "Iter 0960 | Time 10.2551(10.5374) | Bit/dim 1.3588(1.3849) | Xent 0.2737(0.2960) | Loss 1.4957(1.5329) | Error 0.0811(0.0894) Steps 494(498.97) | Grad Norm 4.0514(11.8434) | Total Time 10.00(10.00)\n",
      "Iter 0970 | Time 10.5108(10.5157) | Bit/dim 1.3519(1.3762) | Xent 0.2169(0.2889) | Loss 1.4604(1.5207) | Error 0.0700(0.0870) Steps 500(498.15) | Grad Norm 7.9554(10.0200) | Total Time 10.00(10.00)\n",
      "Iter 0980 | Time 10.3745(10.5065) | Bit/dim 1.3427(1.3687) | Xent 0.3003(0.2860) | Loss 1.4928(1.5117) | Error 0.0922(0.0859) Steps 494(497.17) | Grad Norm 2.5812(8.5565) | Total Time 10.00(10.00)\n",
      "Iter 0990 | Time 11.3521(10.5631) | Bit/dim 1.3820(1.3640) | Xent 0.2729(0.2794) | Loss 1.5184(1.5037) | Error 0.0844(0.0844) Steps 530(499.16) | Grad Norm 26.6205(8.7381) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0015 | Time 44.1092, Epoch Time 757.2284(612.6794), Bit/dim 1.4208(best: 1.3812), Xent 0.2746, Loss 1.5581, Error 0.0843(best: 0.0791)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1000 | Time 10.3049(10.5873) | Bit/dim 1.3791(1.3723) | Xent 0.2764(0.2800) | Loss 1.5173(1.5123) | Error 0.0900(0.0851) Steps 500(500.14) | Grad Norm 14.6641(11.7663) | Total Time 10.00(10.00)\n",
      "Iter 1010 | Time 10.2290(10.6069) | Bit/dim 1.3769(1.3816) | Xent 0.2384(0.2810) | Loss 1.4961(1.5221) | Error 0.0711(0.0854) Steps 494(501.17) | Grad Norm 14.2889(13.6544) | Total Time 10.00(10.00)\n",
      "Iter 1020 | Time 10.6382(10.6037) | Bit/dim 1.3516(1.3770) | Xent 0.2726(0.2810) | Loss 1.4879(1.5175) | Error 0.0789(0.0858) Steps 500(502.17) | Grad Norm 6.9224(12.6161) | Total Time 10.00(10.00)\n",
      "Iter 1030 | Time 10.7715(10.5971) | Bit/dim 1.3278(1.3691) | Xent 0.2847(0.2768) | Loss 1.4702(1.5076) | Error 0.0889(0.0855) Steps 500(501.42) | Grad Norm 3.2785(10.6780) | Total Time 10.00(10.00)\n",
      "Iter 1040 | Time 10.4127(10.5433) | Bit/dim 1.3429(1.3610) | Xent 0.2223(0.2770) | Loss 1.4540(1.4995) | Error 0.0700(0.0846) Steps 500(500.10) | Grad Norm 2.8578(8.6881) | Total Time 10.00(10.00)\n",
      "Iter 1050 | Time 10.5650(10.6164) | Bit/dim 1.3451(1.3542) | Xent 0.2603(0.2699) | Loss 1.4752(1.4892) | Error 0.0744(0.0817) Steps 500(501.60) | Grad Norm 12.4848(8.0331) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0016 | Time 47.5144, Epoch Time 766.1780(617.2844), Bit/dim 1.3265(best: 1.3812), Xent 0.2435, Loss 1.4482, Error 0.0712(best: 0.0791)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1060 | Time 10.4675(10.6874) | Bit/dim 1.3234(1.3491) | Xent 0.2601(0.2683) | Loss 1.4535(1.4832) | Error 0.0789(0.0811) Steps 494(503.48) | Grad Norm 9.5971(8.5101) | Total Time 10.00(10.00)\n",
      "Iter 1070 | Time 11.3048(10.7798) | Bit/dim 1.3321(1.3435) | Xent 0.2024(0.2633) | Loss 1.4333(1.4751) | Error 0.0667(0.0797) Steps 518(506.36) | Grad Norm 6.1256(8.0283) | Total Time 10.00(10.00)\n",
      "Iter 1080 | Time 10.7275(10.8023) | Bit/dim 1.3902(1.3447) | Xent 0.2161(0.2662) | Loss 1.4982(1.4778) | Error 0.0711(0.0802) Steps 506(507.08) | Grad Norm 18.6607(10.0994) | Total Time 10.00(10.00)\n",
      "Iter 1090 | Time 10.3984(10.8009) | Bit/dim 1.4081(1.3548) | Xent 0.2493(0.2661) | Loss 1.5328(1.4878) | Error 0.0789(0.0803) Steps 500(507.52) | Grad Norm 19.6654(12.7307) | Total Time 10.00(10.00)\n",
      "Iter 1100 | Time 10.4472(10.8657) | Bit/dim 1.3720(1.3582) | Xent 0.2627(0.2652) | Loss 1.5033(1.4908) | Error 0.0889(0.0799) Steps 494(508.78) | Grad Norm 15.0812(13.3919) | Total Time 10.00(10.00)\n",
      "Iter 1110 | Time 11.3676(10.8574) | Bit/dim 1.3561(1.3554) | Xent 0.2910(0.2631) | Loss 1.5016(1.4870) | Error 0.0989(0.0801) Steps 524(508.43) | Grad Norm 14.0186(12.6666) | Total Time 10.00(10.00)\n",
      "Iter 1120 | Time 11.2142(10.8077) | Bit/dim 1.3330(1.3483) | Xent 0.2675(0.2613) | Loss 1.4667(1.4790) | Error 0.0878(0.0796) Steps 524(507.03) | Grad Norm 14.5187(12.0123) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0017 | Time 46.9984, Epoch Time 777.5726(622.0930), Bit/dim 1.3236(best: 1.3265), Xent 0.2493, Loss 1.4482, Error 0.0746(best: 0.0712)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1130 | Time 11.2847(10.7974) | Bit/dim 1.3267(1.3434) | Xent 0.2819(0.2590) | Loss 1.4676(1.4729) | Error 0.0878(0.0786) Steps 518(506.83) | Grad Norm 5.8149(10.5125) | Total Time 10.00(10.00)\n",
      "Iter 1140 | Time 11.3805(10.8538) | Bit/dim 1.3339(1.3369) | Xent 0.2257(0.2546) | Loss 1.4468(1.4642) | Error 0.0667(0.0774) Steps 518(509.60) | Grad Norm 4.5779(9.3198) | Total Time 10.00(10.00)\n",
      "Iter 1150 | Time 11.8793(11.0026) | Bit/dim 1.3175(1.3317) | Xent 0.2354(0.2492) | Loss 1.4352(1.4563) | Error 0.0733(0.0761) Steps 518(512.49) | Grad Norm 4.9071(8.0940) | Total Time 10.00(10.00)\n",
      "Iter 1160 | Time 10.8232(10.9698) | Bit/dim 1.3465(1.3295) | Xent 0.2584(0.2491) | Loss 1.4757(1.4540) | Error 0.0822(0.0754) Steps 500(512.12) | Grad Norm 17.1402(8.9934) | Total Time 10.00(10.00)\n",
      "Iter 1170 | Time 11.2704(11.0179) | Bit/dim 1.3260(1.3273) | Xent 0.2162(0.2449) | Loss 1.4341(1.4497) | Error 0.0700(0.0743) Steps 524(513.15) | Grad Norm 10.5643(9.3529) | Total Time 10.00(10.00)\n",
      "Iter 1180 | Time 10.9765(11.0474) | Bit/dim 1.3359(1.3220) | Xent 0.2251(0.2449) | Loss 1.4484(1.4444) | Error 0.0689(0.0745) Steps 524(513.77) | Grad Norm 13.0578(8.2225) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0018 | Time 48.4110, Epoch Time 795.9582(627.3090), Bit/dim 1.3004(best: 1.3236), Xent 0.2328, Loss 1.4168, Error 0.0694(best: 0.0712)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1190 | Time 11.0632(11.0720) | Bit/dim 1.2952(1.3182) | Xent 0.2406(0.2459) | Loss 1.4155(1.4411) | Error 0.0789(0.0742) Steps 524(514.53) | Grad Norm 7.7186(8.3009) | Total Time 10.00(10.00)\n",
      "Iter 1200 | Time 11.1982(11.0831) | Bit/dim 1.3213(1.3184) | Xent 0.2655(0.2441) | Loss 1.4541(1.4404) | Error 0.0689(0.0732) Steps 536(516.07) | Grad Norm 10.4259(9.6178) | Total Time 10.00(10.00)\n",
      "Iter 1210 | Time 11.3080(11.0606) | Bit/dim 1.3480(1.3263) | Xent 0.2098(0.2406) | Loss 1.4529(1.4466) | Error 0.0589(0.0716) Steps 518(515.19) | Grad Norm 11.8662(11.9342) | Total Time 10.00(10.00)\n",
      "Iter 1220 | Time 11.2310(11.0038) | Bit/dim 1.3280(1.3324) | Xent 0.2283(0.2414) | Loss 1.4422(1.4531) | Error 0.0656(0.0726) Steps 542(516.44) | Grad Norm 5.6494(13.3212) | Total Time 10.00(10.00)\n",
      "Iter 1230 | Time 11.2998(11.0478) | Bit/dim 1.3034(1.3278) | Xent 0.1943(0.2373) | Loss 1.4006(1.4464) | Error 0.0533(0.0717) Steps 524(518.40) | Grad Norm 4.8031(12.3146) | Total Time 10.00(10.00)\n",
      "Iter 1240 | Time 11.2391(11.1215) | Bit/dim 1.2973(1.3215) | Xent 0.2551(0.2351) | Loss 1.4249(1.4390) | Error 0.0722(0.0712) Steps 524(520.61) | Grad Norm 7.5536(11.0954) | Total Time 10.00(10.00)\n",
      "Iter 1250 | Time 10.7985(11.1194) | Bit/dim 1.3074(1.3161) | Xent 0.2557(0.2340) | Loss 1.4352(1.4331) | Error 0.0756(0.0706) Steps 518(520.62) | Grad Norm 6.6615(9.6272) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0019 | Time 47.0896, Epoch Time 795.1734(632.3449), Bit/dim 1.2973(best: 1.3004), Xent 0.2047, Loss 1.3996, Error 0.0621(best: 0.0694)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1260 | Time 11.1047(11.1489) | Bit/dim 1.3021(1.3111) | Xent 0.2535(0.2305) | Loss 1.4288(1.4264) | Error 0.0811(0.0695) Steps 524(521.68) | Grad Norm 4.3979(8.4280) | Total Time 10.00(10.00)\n",
      "Iter 1270 | Time 11.3203(11.1899) | Bit/dim 1.2881(1.3071) | Xent 0.2108(0.2251) | Loss 1.3935(1.4196) | Error 0.0722(0.0692) Steps 524(522.12) | Grad Norm 3.8148(7.9756) | Total Time 10.00(10.00)\n",
      "Iter 1280 | Time 11.2571(11.2051) | Bit/dim 1.2976(1.3056) | Xent 0.1930(0.2219) | Loss 1.3941(1.4165) | Error 0.0578(0.0679) Steps 530(522.31) | Grad Norm 9.5929(8.8540) | Total Time 10.00(10.00)\n",
      "Iter 1290 | Time 11.7114(11.1422) | Bit/dim 1.2982(1.3104) | Xent 0.2312(0.2214) | Loss 1.4138(1.4211) | Error 0.0622(0.0672) Steps 524(518.98) | Grad Norm 3.6726(11.0376) | Total Time 10.00(10.00)\n",
      "Iter 1300 | Time 10.0129(11.0605) | Bit/dim 1.3549(1.3204) | Xent 0.2332(0.2241) | Loss 1.4715(1.4325) | Error 0.0756(0.0678) Steps 500(518.12) | Grad Norm 17.0978(13.2044) | Total Time 10.00(10.00)\n",
      "Iter 1310 | Time 11.3325(11.1112) | Bit/dim 1.3181(1.3191) | Xent 0.2159(0.2228) | Loss 1.4261(1.4304) | Error 0.0711(0.0677) Steps 536(520.49) | Grad Norm 8.9088(12.3392) | Total Time 10.00(10.00)\n",
      "Iter 1320 | Time 11.3967(11.1695) | Bit/dim 1.2890(1.3131) | Xent 0.2506(0.2209) | Loss 1.4144(1.4236) | Error 0.0822(0.0675) Steps 524(521.71) | Grad Norm 2.8813(10.4163) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0020 | Time 48.5691, Epoch Time 800.5283(637.3904), Bit/dim 1.2900(best: 1.2973), Xent 0.1965, Loss 1.3882, Error 0.0578(best: 0.0621)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1330 | Time 11.5330(11.2104) | Bit/dim 1.2779(1.3065) | Xent 0.1998(0.2166) | Loss 1.3777(1.4148) | Error 0.0622(0.0661) Steps 524(522.62) | Grad Norm 3.3381(8.7373) | Total Time 10.00(10.00)\n",
      "Iter 1340 | Time 10.7502(11.2125) | Bit/dim 1.2993(1.3021) | Xent 0.2030(0.2111) | Loss 1.4008(1.4076) | Error 0.0644(0.0645) Steps 512(521.38) | Grad Norm 12.6166(8.4525) | Total Time 10.00(10.00)\n",
      "Iter 1350 | Time 10.8593(11.1853) | Bit/dim 1.2952(1.2998) | Xent 0.1607(0.2092) | Loss 1.3756(1.4044) | Error 0.0544(0.0640) Steps 500(519.59) | Grad Norm 8.4908(8.3161) | Total Time 10.00(10.00)\n",
      "Iter 1360 | Time 9.9613(11.1610) | Bit/dim 1.4907(1.3102) | Xent 0.2338(0.2140) | Loss 1.6076(1.4172) | Error 0.0767(0.0662) Steps 482(520.10) | Grad Norm 33.2509(11.3624) | Total Time 10.00(10.00)\n",
      "Iter 1370 | Time 11.2357(11.1665) | Bit/dim 1.3708(1.3249) | Xent 0.2384(0.2223) | Loss 1.4900(1.4360) | Error 0.0678(0.0676) Steps 536(522.84) | Grad Norm 24.7569(13.6177) | Total Time 10.00(10.00)\n",
      "Iter 1380 | Time 10.8106(11.0942) | Bit/dim 1.3295(1.3246) | Xent 0.1968(0.2225) | Loss 1.4279(1.4358) | Error 0.0600(0.0685) Steps 506(518.60) | Grad Norm 9.0729(13.2107) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0021 | Time 46.3093, Epoch Time 794.4745(642.1030), Bit/dim 1.2974(best: 1.2900), Xent 0.1939, Loss 1.3943, Error 0.0603(best: 0.0578)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1390 | Time 10.9416(11.0218) | Bit/dim 1.2791(1.3189) | Xent 0.1565(0.2167) | Loss 1.3573(1.4273) | Error 0.0511(0.0665) Steps 512(516.59) | Grad Norm 3.2935(11.5797) | Total Time 10.00(10.00)\n",
      "Iter 1400 | Time 10.8642(11.0181) | Bit/dim 1.2797(1.3099) | Xent 0.2088(0.2128) | Loss 1.3841(1.4163) | Error 0.0556(0.0650) Steps 512(515.25) | Grad Norm 4.1705(9.8506) | Total Time 10.00(10.00)\n",
      "Iter 1410 | Time 10.8679(10.9565) | Bit/dim 1.2789(1.3022) | Xent 0.1830(0.2061) | Loss 1.3704(1.4052) | Error 0.0556(0.0631) Steps 512(512.86) | Grad Norm 6.2427(8.3778) | Total Time 10.00(10.00)\n",
      "Iter 1420 | Time 10.7401(10.9669) | Bit/dim 1.2835(1.2953) | Xent 0.1864(0.2005) | Loss 1.3767(1.3955) | Error 0.0511(0.0610) Steps 506(511.91) | Grad Norm 10.6106(7.6801) | Total Time 10.00(10.00)\n",
      "Iter 1430 | Time 11.1576(10.9924) | Bit/dim 1.2624(1.2905) | Xent 0.1830(0.1988) | Loss 1.3539(1.3898) | Error 0.0611(0.0607) Steps 518(512.91) | Grad Norm 4.4291(7.1846) | Total Time 10.00(10.00)\n",
      "Iter 1440 | Time 10.8474(10.9979) | Bit/dim 1.2780(1.2865) | Xent 0.1679(0.1972) | Loss 1.3620(1.3851) | Error 0.0578(0.0592) Steps 512(513.32) | Grad Norm 5.1953(7.1204) | Total Time 10.00(10.00)\n",
      "Iter 1450 | Time 11.0583(11.0261) | Bit/dim 1.2789(1.2843) | Xent 0.1903(0.1907) | Loss 1.3740(1.3797) | Error 0.0589(0.0581) Steps 512(513.42) | Grad Norm 8.6083(6.7315) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0022 | Time 48.8275, Epoch Time 788.1442(646.4842), Bit/dim 1.2658(best: 1.2900), Xent 0.1725, Loss 1.3521, Error 0.0519(best: 0.0578)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1460 | Time 10.9615(10.9954) | Bit/dim 1.2702(1.2805) | Xent 0.2013(0.1889) | Loss 1.3708(1.3750) | Error 0.0667(0.0582) Steps 506(512.77) | Grad Norm 14.2521(7.1220) | Total Time 10.00(10.00)\n",
      "Iter 1470 | Time 10.9901(11.0216) | Bit/dim 1.2976(1.2965) | Xent 0.1335(0.1946) | Loss 1.3644(1.3938) | Error 0.0433(0.0601) Steps 524(513.87) | Grad Norm 9.7110(10.6254) | Total Time 10.00(10.00)\n",
      "Iter 1480 | Time 11.3055(11.0520) | Bit/dim 1.3001(1.3041) | Xent 0.2276(0.2032) | Loss 1.4139(1.4057) | Error 0.0689(0.0627) Steps 530(517.91) | Grad Norm 12.5951(11.7330) | Total Time 10.00(10.00)\n",
      "Iter 1490 | Time 10.9558(10.9683) | Bit/dim 1.2833(1.2994) | Xent 0.2183(0.1984) | Loss 1.3925(1.3987) | Error 0.0678(0.0614) Steps 512(514.71) | Grad Norm 8.4631(10.6270) | Total Time 10.00(10.00)\n",
      "Iter 1500 | Time 11.0374(10.9576) | Bit/dim 1.2562(1.2937) | Xent 0.1557(0.1941) | Loss 1.3340(1.3907) | Error 0.0467(0.0599) Steps 506(513.94) | Grad Norm 4.7784(9.4942) | Total Time 10.00(10.00)\n",
      "Iter 1510 | Time 10.7713(10.9330) | Bit/dim 1.2670(1.2886) | Xent 0.1413(0.1862) | Loss 1.3376(1.3817) | Error 0.0456(0.0570) Steps 512(513.38) | Grad Norm 6.2758(8.2532) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0023 | Time 49.5385, Epoch Time 787.4521(650.7132), Bit/dim 1.2634(best: 1.2658), Xent 0.1724, Loss 1.3496, Error 0.0524(best: 0.0519)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1520 | Time 10.5059(10.9324) | Bit/dim 1.2602(1.2843) | Xent 0.1395(0.1862) | Loss 1.3300(1.3774) | Error 0.0400(0.0570) Steps 506(513.79) | Grad Norm 5.5859(8.3401) | Total Time 10.00(10.00)\n",
      "Iter 1530 | Time 10.7800(10.9223) | Bit/dim 1.2633(1.2804) | Xent 0.1587(0.1793) | Loss 1.3427(1.3701) | Error 0.0478(0.0550) Steps 518(513.49) | Grad Norm 8.6817(8.3745) | Total Time 10.00(10.00)\n",
      "Iter 1540 | Time 10.7628(10.8869) | Bit/dim 1.2803(1.2780) | Xent 0.1634(0.1758) | Loss 1.3621(1.3658) | Error 0.0511(0.0542) Steps 518(513.48) | Grad Norm 7.7769(8.6332) | Total Time 10.00(10.00)\n",
      "Iter 1550 | Time 11.6493(10.9069) | Bit/dim 1.2508(1.2726) | Xent 0.1220(0.1698) | Loss 1.3118(1.3575) | Error 0.0433(0.0526) Steps 518(513.82) | Grad Norm 4.2104(7.7576) | Total Time 10.00(10.00)\n",
      "Iter 1560 | Time 11.8416(10.9190) | Bit/dim 1.2860(1.2715) | Xent 0.1540(0.1635) | Loss 1.3630(1.3532) | Error 0.0522(0.0499) Steps 530(514.53) | Grad Norm 26.5566(8.0668) | Total Time 10.00(10.00)\n",
      "Iter 1570 | Time 11.0164(10.9885) | Bit/dim 1.3641(1.2984) | Xent 0.1732(0.1707) | Loss 1.4507(1.3838) | Error 0.0578(0.0529) Steps 524(516.57) | Grad Norm 23.8793(12.1823) | Total Time 10.00(10.00)\n",
      "Iter 1580 | Time 10.8712(10.9851) | Bit/dim 1.2750(1.2977) | Xent 0.2257(0.1751) | Loss 1.3879(1.3853) | Error 0.0700(0.0542) Steps 512(518.71) | Grad Norm 3.6489(11.0395) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0024 | Time 47.2708, Epoch Time 784.4697(654.7259), Bit/dim 1.2787(best: 1.2634), Xent 0.1484, Loss 1.3529, Error 0.0452(best: 0.0519)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1590 | Time 10.6409(10.9221) | Bit/dim 1.2641(1.2915) | Xent 0.2033(0.1736) | Loss 1.3658(1.3782) | Error 0.0522(0.0532) Steps 506(516.55) | Grad Norm 4.2735(9.2850) | Total Time 10.00(10.00)\n",
      "Iter 1600 | Time 10.7872(10.8747) | Bit/dim 1.2582(1.2844) | Xent 0.1511(0.1684) | Loss 1.3337(1.3685) | Error 0.0422(0.0518) Steps 506(513.75) | Grad Norm 5.4667(7.8077) | Total Time 10.00(10.00)\n",
      "Iter 1610 | Time 10.9206(10.8824) | Bit/dim 1.2593(1.2766) | Xent 0.1344(0.1628) | Loss 1.3265(1.3580) | Error 0.0344(0.0502) Steps 500(511.69) | Grad Norm 7.9112(7.0711) | Total Time 10.00(10.00)\n",
      "Iter 1620 | Time 10.7236(10.8580) | Bit/dim 1.2556(1.2710) | Xent 0.1591(0.1531) | Loss 1.3351(1.3476) | Error 0.0489(0.0473) Steps 506(510.51) | Grad Norm 3.2131(6.2455) | Total Time 10.00(10.00)\n",
      "Iter 1630 | Time 10.3941(10.8068) | Bit/dim 1.2412(1.2650) | Xent 0.1175(0.1498) | Loss 1.3000(1.3399) | Error 0.0433(0.0462) Steps 506(509.19) | Grad Norm 4.4311(5.6588) | Total Time 10.00(10.00)\n",
      "Iter 1640 | Time 10.7800(10.7966) | Bit/dim 1.2598(1.2626) | Xent 0.1218(0.1452) | Loss 1.3207(1.3352) | Error 0.0278(0.0447) Steps 512(509.39) | Grad Norm 5.0162(6.3905) | Total Time 10.00(10.00)\n",
      "Iter 1650 | Time 11.1304(10.9026) | Bit/dim 1.3699(1.2801) | Xent 0.1379(0.1461) | Loss 1.4388(1.3531) | Error 0.0400(0.0450) Steps 518(513.18) | Grad Norm 18.1379(10.5090) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0025 | Time 47.2783, Epoch Time 776.8867(658.3907), Bit/dim 1.2769(best: 1.2634), Xent 0.1273, Loss 1.3406, Error 0.0402(best: 0.0452)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1660 | Time 10.7693(10.9455) | Bit/dim 1.2910(1.2976) | Xent 0.1368(0.1488) | Loss 1.3594(1.3720) | Error 0.0433(0.0461) Steps 512(515.37) | Grad Norm 5.6086(12.0706) | Total Time 10.00(10.00)\n",
      "Iter 1670 | Time 10.8029(10.9322) | Bit/dim 1.2843(1.2954) | Xent 0.1516(0.1459) | Loss 1.3601(1.3684) | Error 0.0444(0.0454) Steps 518(515.59) | Grad Norm 8.0356(11.7726) | Total Time 10.00(10.00)\n",
      "Iter 1680 | Time 11.1634(10.9142) | Bit/dim 1.2650(1.2879) | Xent 0.1388(0.1418) | Loss 1.3345(1.3587) | Error 0.0411(0.0445) Steps 518(514.81) | Grad Norm 8.5123(10.3670) | Total Time 10.00(10.00)\n",
      "Iter 1690 | Time 10.7540(10.8786) | Bit/dim 1.2638(1.2808) | Xent 0.1156(0.1395) | Loss 1.3216(1.3506) | Error 0.0367(0.0432) Steps 506(513.89) | Grad Norm 10.3620(9.3184) | Total Time 10.00(10.00)\n",
      "Iter 1700 | Time 10.7491(10.8608) | Bit/dim 1.2594(1.2738) | Xent 0.1271(0.1361) | Loss 1.3230(1.3418) | Error 0.0389(0.0419) Steps 512(513.39) | Grad Norm 3.4897(7.9317) | Total Time 10.00(10.00)\n",
      "Iter 1710 | Time 10.7878(10.8567) | Bit/dim 1.2386(1.2654) | Xent 0.1005(0.1288) | Loss 1.2889(1.3298) | Error 0.0367(0.0400) Steps 512(513.49) | Grad Norm 1.8528(6.8636) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0026 | Time 47.2774, Epoch Time 779.6428(662.0283), Bit/dim 1.2392(best: 1.2634), Xent 0.1143, Loss 1.2964, Error 0.0361(best: 0.0402)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1720 | Time 11.0977(10.8654) | Bit/dim 1.2588(1.2610) | Xent 0.1492(0.1268) | Loss 1.3334(1.3244) | Error 0.0467(0.0392) Steps 512(512.21) | Grad Norm 4.4143(6.0008) | Total Time 10.00(10.00)\n",
      "Iter 1730 | Time 11.0982(10.8512) | Bit/dim 1.2381(1.2561) | Xent 0.1000(0.1250) | Loss 1.2881(1.3187) | Error 0.0267(0.0384) Steps 506(511.64) | Grad Norm 7.6779(6.3411) | Total Time 10.00(10.00)\n",
      "Iter 1740 | Time 10.6592(10.8205) | Bit/dim 1.2284(1.2539) | Xent 0.1268(0.1251) | Loss 1.2918(1.3165) | Error 0.0378(0.0383) Steps 506(510.46) | Grad Norm 5.1834(7.0186) | Total Time 10.00(10.00)\n",
      "Iter 1750 | Time 10.6278(10.8490) | Bit/dim 1.2310(1.2502) | Xent 0.0695(0.1213) | Loss 1.2657(1.3108) | Error 0.0244(0.0369) Steps 506(510.09) | Grad Norm 2.7939(6.3132) | Total Time 10.00(10.00)\n",
      "Iter 1760 | Time 10.8063(10.8245) | Bit/dim 1.2390(1.2466) | Xent 0.1100(0.1197) | Loss 1.2940(1.3065) | Error 0.0267(0.0361) Steps 512(510.34) | Grad Norm 9.1933(5.6909) | Total Time 10.00(10.00)\n",
      "Iter 1770 | Time 10.4557(10.8431) | Bit/dim 1.2472(1.2470) | Xent 0.1341(0.1182) | Loss 1.3143(1.3061) | Error 0.0456(0.0365) Steps 506(511.80) | Grad Norm 13.1334(7.1862) | Total Time 10.00(10.00)\n",
      "Iter 1780 | Time 10.4297(10.8756) | Bit/dim 1.2324(1.2462) | Xent 0.1247(0.1165) | Loss 1.2948(1.3045) | Error 0.0422(0.0364) Steps 512(512.73) | Grad Norm 12.1908(8.6275) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0027 | Time 47.1094, Epoch Time 778.8799(665.5339), Bit/dim 1.2328(best: 1.2392), Xent 0.1027, Loss 1.2842, Error 0.0341(best: 0.0361)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1790 | Time 10.6854(10.8536) | Bit/dim 1.2234(1.2440) | Xent 0.1026(0.1139) | Loss 1.2747(1.3010) | Error 0.0322(0.0356) Steps 506(512.02) | Grad Norm 3.7269(8.5493) | Total Time 10.00(10.00)\n",
      "Iter 1800 | Time 10.5855(10.8821) | Bit/dim 1.4794(1.2566) | Xent 0.1192(0.1160) | Loss 1.5390(1.3146) | Error 0.0433(0.0364) Steps 500(512.93) | Grad Norm 26.9976(11.1970) | Total Time 10.00(10.00)\n",
      "Iter 1810 | Time 11.0555(10.9650) | Bit/dim 1.3259(1.2872) | Xent 0.1411(0.1202) | Loss 1.3964(1.3472) | Error 0.0356(0.0372) Steps 536(517.03) | Grad Norm 7.9884(11.6402) | Total Time 10.00(10.00)\n",
      "Iter 1820 | Time 10.8026(10.9438) | Bit/dim 1.2611(1.2847) | Xent 0.1225(0.1192) | Loss 1.3223(1.3443) | Error 0.0456(0.0371) Steps 512(515.93) | Grad Norm 4.9867(10.6159) | Total Time 10.00(10.00)\n",
      "Iter 1830 | Time 10.6027(10.9133) | Bit/dim 1.2464(1.2746) | Xent 0.1251(0.1161) | Loss 1.3090(1.3326) | Error 0.0411(0.0361) Steps 512(514.61) | Grad Norm 4.4406(9.1140) | Total Time 10.00(10.00)\n",
      "Iter 1840 | Time 10.7856(10.8848) | Bit/dim 1.2488(1.2652) | Xent 0.1194(0.1152) | Loss 1.3085(1.3228) | Error 0.0433(0.0360) Steps 518(514.13) | Grad Norm 5.6797(7.8501) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0028 | Time 47.4712, Epoch Time 783.1866(669.0634), Bit/dim 1.2319(best: 1.2328), Xent 0.0982, Loss 1.2810, Error 0.0312(best: 0.0341)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1850 | Time 10.9484(10.9160) | Bit/dim 1.2239(1.2579) | Xent 0.1125(0.1102) | Loss 1.2802(1.3130) | Error 0.0300(0.0344) Steps 518(514.84) | Grad Norm 4.2520(7.2907) | Total Time 10.00(10.00)\n",
      "Iter 1860 | Time 10.7411(10.9118) | Bit/dim 1.2346(1.2500) | Xent 0.0805(0.1047) | Loss 1.2749(1.3023) | Error 0.0244(0.0322) Steps 512(514.37) | Grad Norm 6.1757(6.5046) | Total Time 10.00(10.00)\n",
      "Iter 1870 | Time 10.7187(10.9097) | Bit/dim 1.2368(1.2458) | Xent 0.0826(0.1011) | Loss 1.2781(1.2964) | Error 0.0267(0.0315) Steps 512(514.50) | Grad Norm 7.0059(6.6841) | Total Time 10.00(10.00)\n",
      "Iter 1880 | Time 10.7112(10.9069) | Bit/dim 1.2274(1.2412) | Xent 0.0903(0.0966) | Loss 1.2726(1.2895) | Error 0.0300(0.0304) Steps 512(514.46) | Grad Norm 6.1925(6.3341) | Total Time 10.00(10.00)\n",
      "Iter 1890 | Time 10.6107(10.8832) | Bit/dim 1.2281(1.2376) | Xent 0.1055(0.0959) | Loss 1.2809(1.2855) | Error 0.0333(0.0303) Steps 518(514.34) | Grad Norm 14.0691(6.1496) | Total Time 10.00(10.00)\n",
      "Iter 1900 | Time 11.3140(10.8739) | Bit/dim 1.2918(1.2374) | Xent 0.0781(0.0953) | Loss 1.3309(1.2851) | Error 0.0289(0.0303) Steps 530(514.60) | Grad Norm 22.1686(7.8746) | Total Time 10.00(10.00)\n",
      "Iter 1910 | Time 11.8975(10.9652) | Bit/dim 1.4953(1.2841) | Xent 0.1498(0.0994) | Loss 1.5702(1.3338) | Error 0.0489(0.0316) Steps 548(517.53) | Grad Norm 6.8518(10.2765) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0029 | Time 47.8340, Epoch Time 784.5537(672.5281), Bit/dim 1.3753(best: 1.2319), Xent 0.1171, Loss 1.4338, Error 0.0352(best: 0.0312)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1920 | Time 11.0908(11.0405) | Bit/dim 1.3405(1.3089) | Xent 0.0835(0.1016) | Loss 1.3823(1.3597) | Error 0.0278(0.0322) Steps 518(519.70) | Grad Norm 10.6118(10.2691) | Total Time 10.00(10.00)\n",
      "Iter 1930 | Time 10.8600(11.0214) | Bit/dim 1.2607(1.2994) | Xent 0.1144(0.0996) | Loss 1.3179(1.3493) | Error 0.0356(0.0311) Steps 512(519.67) | Grad Norm 6.4114(10.1755) | Total Time 10.00(10.00)\n",
      "Iter 1940 | Time 10.9297(10.9810) | Bit/dim 1.2303(1.2839) | Xent 0.0930(0.0965) | Loss 1.2768(1.3322) | Error 0.0267(0.0304) Steps 518(518.01) | Grad Norm 2.3069(8.7147) | Total Time 10.00(10.00)\n",
      "Iter 1950 | Time 10.8526(10.9449) | Bit/dim 1.2230(1.2693) | Xent 0.0853(0.0949) | Loss 1.2656(1.3168) | Error 0.0289(0.0295) Steps 512(516.43) | Grad Norm 1.5339(7.0245) | Total Time 10.00(10.00)\n",
      "Iter 1960 | Time 10.8365(10.9303) | Bit/dim 1.2234(1.2571) | Xent 0.1030(0.0928) | Loss 1.2749(1.3035) | Error 0.0256(0.0288) Steps 512(515.27) | Grad Norm 3.0109(5.8137) | Total Time 10.00(10.00)\n",
      "Iter 1970 | Time 10.9040(10.9356) | Bit/dim 1.2231(1.2481) | Xent 0.0686(0.0906) | Loss 1.2574(1.2934) | Error 0.0200(0.0281) Steps 512(516.16) | Grad Norm 1.8562(5.8290) | Total Time 10.00(10.00)\n",
      "Iter 1980 | Time 11.9067(10.9589) | Bit/dim 1.2994(1.2455) | Xent 0.0971(0.0874) | Loss 1.3479(1.2892) | Error 0.0333(0.0271) Steps 554(517.93) | Grad Norm 42.0849(7.7742) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0030 | Time 45.9826, Epoch Time 784.8080(675.8965), Bit/dim 1.3861(best: 1.2319), Xent 0.0849, Loss 1.4285, Error 0.0256(best: 0.0312)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1990 | Time 11.3572(11.0424) | Bit/dim 1.3512(1.2659) | Xent 0.0952(0.0894) | Loss 1.3987(1.3106) | Error 0.0300(0.0276) Steps 536(521.03) | Grad Norm 14.4076(10.5062) | Total Time 10.00(10.00)\n",
      "Iter 2000 | Time 10.8151(11.0778) | Bit/dim 1.2382(1.2650) | Xent 0.1124(0.0897) | Loss 1.2944(1.3099) | Error 0.0389(0.0280) Steps 518(521.25) | Grad Norm 12.3485(10.9363) | Total Time 10.00(10.00)\n",
      "Iter 2010 | Time 11.1517(11.0520) | Bit/dim 1.2322(1.2574) | Xent 0.0867(0.0891) | Loss 1.2755(1.3019) | Error 0.0344(0.0278) Steps 524(520.29) | Grad Norm 2.7494(9.8256) | Total Time 10.00(10.00)\n",
      "Iter 2020 | Time 10.8752(11.0350) | Bit/dim 1.2254(1.2485) | Xent 0.0703(0.0853) | Loss 1.2606(1.2911) | Error 0.0222(0.0270) Steps 524(521.26) | Grad Norm 6.6809(9.0134) | Total Time 10.00(10.00)\n",
      "Iter 2030 | Time 10.9324(10.9843) | Bit/dim 1.2185(1.2406) | Xent 0.0798(0.0821) | Loss 1.2584(1.2816) | Error 0.0267(0.0265) Steps 518(520.44) | Grad Norm 2.0499(8.0169) | Total Time 10.00(10.00)\n",
      "Iter 2040 | Time 10.5335(10.9730) | Bit/dim 1.2085(1.2335) | Xent 0.0801(0.0808) | Loss 1.2485(1.2739) | Error 0.0244(0.0259) Steps 518(519.82) | Grad Norm 1.9033(6.9728) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0031 | Time 47.0337, Epoch Time 789.7102(679.3109), Bit/dim 1.2057(best: 1.2319), Xent 0.0765, Loss 1.2439, Error 0.0249(best: 0.0256)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2050 | Time 10.8585(10.9697) | Bit/dim 1.2122(1.2285) | Xent 0.0574(0.0760) | Loss 1.2409(1.2665) | Error 0.0200(0.0245) Steps 518(519.36) | Grad Norm 4.0706(6.1581) | Total Time 10.00(10.00)\n",
      "Iter 2060 | Time 10.9388(10.9956) | Bit/dim 1.2086(1.2232) | Xent 0.0905(0.0754) | Loss 1.2539(1.2609) | Error 0.0278(0.0240) Steps 512(519.90) | Grad Norm 3.5462(5.5263) | Total Time 10.00(10.00)\n",
      "Iter 2070 | Time 10.8069(10.9586) | Bit/dim 1.2242(1.2208) | Xent 0.0509(0.0735) | Loss 1.2497(1.2575) | Error 0.0133(0.0235) Steps 524(521.12) | Grad Norm 9.0098(5.5659) | Total Time 10.00(10.00)\n",
      "Iter 2080 | Time 12.2911(11.0360) | Bit/dim 1.2452(1.2210) | Xent 0.0635(0.0721) | Loss 1.2769(1.2571) | Error 0.0289(0.0226) Steps 548(523.29) | Grad Norm 28.9206(7.1867) | Total Time 10.00(10.00)\n",
      "Iter 2090 | Time 10.9033(11.1872) | Bit/dim 1.4585(1.2548) | Xent 0.1019(0.0744) | Loss 1.5094(1.2919) | Error 0.0233(0.0230) Steps 518(526.99) | Grad Norm 15.3650(11.1597) | Total Time 10.00(10.00)\n",
      "Iter 2100 | Time 11.7237(11.2549) | Bit/dim 1.2751(1.2764) | Xent 0.0918(0.0794) | Loss 1.3210(1.3161) | Error 0.0322(0.0244) Steps 542(528.76) | Grad Norm 14.6276(11.1410) | Total Time 10.00(10.00)\n",
      "Iter 2110 | Time 11.4547(11.2255) | Bit/dim 1.2345(1.2687) | Xent 0.0825(0.0791) | Loss 1.2757(1.3083) | Error 0.0222(0.0241) Steps 524(528.20) | Grad Norm 5.8852(9.9615) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0032 | Time 47.6456, Epoch Time 802.4510(683.0051), Bit/dim 1.2192(best: 1.2057), Xent 0.0737, Loss 1.2561, Error 0.0232(best: 0.0249)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2120 | Time 11.4554(11.1855) | Bit/dim 1.2075(1.2550) | Xent 0.0779(0.0764) | Loss 1.2465(1.2932) | Error 0.0211(0.0234) Steps 530(527.59) | Grad Norm 2.2849(8.2788) | Total Time 10.00(10.00)\n",
      "Iter 2130 | Time 11.0451(11.1526) | Bit/dim 1.2058(1.2449) | Xent 0.0681(0.0742) | Loss 1.2399(1.2820) | Error 0.0167(0.0226) Steps 530(527.30) | Grad Norm 7.9670(7.1201) | Total Time 10.00(10.00)\n",
      "Iter 2140 | Time 11.0057(11.1223) | Bit/dim 1.2102(1.2351) | Xent 0.0528(0.0716) | Loss 1.2366(1.2709) | Error 0.0178(0.0221) Steps 518(526.53) | Grad Norm 3.0115(6.0787) | Total Time 10.00(10.00)\n",
      "Iter 2150 | Time 10.8531(11.0885) | Bit/dim 1.2146(1.2267) | Xent 0.0556(0.0707) | Loss 1.2424(1.2621) | Error 0.0144(0.0220) Steps 530(526.46) | Grad Norm 3.2006(5.2977) | Total Time 10.00(10.00)\n",
      "Iter 2160 | Time 11.0104(11.0728) | Bit/dim 1.2000(1.2204) | Xent 0.0516(0.0692) | Loss 1.2258(1.2550) | Error 0.0122(0.0214) Steps 530(526.47) | Grad Norm 3.9138(4.7304) | Total Time 10.00(10.00)\n",
      "Iter 2170 | Time 11.4112(11.0620) | Bit/dim 1.1938(1.2157) | Xent 0.0695(0.0676) | Loss 1.2285(1.2495) | Error 0.0167(0.0209) Steps 530(526.44) | Grad Norm 1.9333(4.7064) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0033 | Time 48.4030, Epoch Time 791.3518(686.2555), Bit/dim 1.1918(best: 1.2057), Xent 0.0688, Loss 1.2262, Error 0.0206(best: 0.0232)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2180 | Time 11.2116(11.0537) | Bit/dim 1.1847(1.2117) | Xent 0.0958(0.0655) | Loss 1.2326(1.2444) | Error 0.0222(0.0203) Steps 530(526.75) | Grad Norm 1.2404(4.7756) | Total Time 10.00(10.00)\n",
      "Iter 2190 | Time 10.8943(11.0806) | Bit/dim 1.1991(1.2090) | Xent 0.0626(0.0630) | Loss 1.2305(1.2405) | Error 0.0211(0.0198) Steps 530(527.43) | Grad Norm 6.7607(4.6962) | Total Time 10.00(10.00)\n",
      "Iter 2200 | Time 11.1612(11.0602) | Bit/dim 1.1914(1.2062) | Xent 0.0606(0.0616) | Loss 1.2217(1.2370) | Error 0.0178(0.0193) Steps 524(527.02) | Grad Norm 5.2412(5.5939) | Total Time 10.00(10.00)\n",
      "Iter 2210 | Time 11.0112(11.0669) | Bit/dim 1.2041(1.2051) | Xent 0.0534(0.0611) | Loss 1.2308(1.2357) | Error 0.0178(0.0192) Steps 524(527.01) | Grad Norm 7.6691(5.8938) | Total Time 10.00(10.00)\n",
      "Iter 2220 | Time 11.1158(11.0604) | Bit/dim 1.2026(1.2038) | Xent 0.0725(0.0599) | Loss 1.2388(1.2338) | Error 0.0244(0.0187) Steps 530(527.33) | Grad Norm 10.4524(6.2711) | Total Time 10.00(10.00)\n",
      "Iter 2230 | Time 10.9139(11.0957) | Bit/dim 1.1925(1.2025) | Xent 0.0471(0.0599) | Loss 1.2160(1.2324) | Error 0.0133(0.0183) Steps 530(527.72) | Grad Norm 11.8351(6.8782) | Total Time 10.00(10.00)\n",
      "Iter 2240 | Time 12.0668(11.2086) | Bit/dim 1.4132(1.2225) | Xent 0.0826(0.0632) | Loss 1.4545(1.2541) | Error 0.0333(0.0200) Steps 542(530.29) | Grad Norm 22.7428(10.8042) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0034 | Time 48.6754, Epoch Time 802.0640(689.7298), Bit/dim 1.2545(best: 1.1918), Xent 0.0968, Loss 1.3030, Error 0.0302(best: 0.0206)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2250 | Time 11.6348(11.2740) | Bit/dim 1.4125(1.2536) | Xent 0.0549(0.0649) | Loss 1.4400(1.2860) | Error 0.0167(0.0205) Steps 536(531.69) | Grad Norm 12.2009(12.4373) | Total Time 10.00(10.00)\n",
      "Iter 2260 | Time 11.5667(11.3013) | Bit/dim 1.3129(1.2660) | Xent 0.0493(0.0705) | Loss 1.3376(1.3012) | Error 0.0178(0.0225) Steps 530(531.49) | Grad Norm 13.2302(12.7921) | Total Time 10.00(10.00)\n",
      "Iter 2270 | Time 10.8076(11.2667) | Bit/dim 1.2277(1.2626) | Xent 0.0514(0.0699) | Loss 1.2534(1.2975) | Error 0.0167(0.0223) Steps 518(530.27) | Grad Norm 15.1273(12.7517) | Total Time 10.00(10.00)\n",
      "Iter 2280 | Time 11.4117(11.2362) | Bit/dim 1.2052(1.2506) | Xent 0.0540(0.0679) | Loss 1.2322(1.2845) | Error 0.0167(0.0212) Steps 524(528.63) | Grad Norm 8.0827(11.1590) | Total Time 10.00(10.00)\n",
      "Iter 2290 | Time 10.6368(11.1288) | Bit/dim 1.1864(1.2368) | Xent 0.0444(0.0639) | Loss 1.2086(1.2688) | Error 0.0133(0.0197) Steps 524(527.87) | Grad Norm 1.8807(9.4970) | Total Time 10.00(10.00)\n",
      "Iter 2300 | Time 10.7013(11.0649) | Bit/dim 1.1796(1.2267) | Xent 0.0600(0.0625) | Loss 1.2097(1.2579) | Error 0.0167(0.0189) Steps 524(526.85) | Grad Norm 3.4001(7.8628) | Total Time 10.00(10.00)\n",
      "Iter 2310 | Time 11.0092(11.0881) | Bit/dim 1.2037(1.2182) | Xent 0.0490(0.0623) | Loss 1.2282(1.2494) | Error 0.0189(0.0192) Steps 530(526.76) | Grad Norm 1.4815(6.7959) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0035 | Time 48.3520, Epoch Time 797.7758(692.9712), Bit/dim 1.1874(best: 1.1918), Xent 0.0611, Loss 1.2180, Error 0.0188(best: 0.0206)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2320 | Time 10.9256(11.0662) | Bit/dim 1.1892(1.2106) | Xent 0.0458(0.0575) | Loss 1.2121(1.2393) | Error 0.0122(0.0180) Steps 524(526.67) | Grad Norm 6.7102(6.0138) | Total Time 10.00(10.00)\n",
      "Iter 2330 | Time 11.0472(11.0555) | Bit/dim 1.1831(1.2059) | Xent 0.0345(0.0543) | Loss 1.2004(1.2330) | Error 0.0111(0.0168) Steps 530(527.37) | Grad Norm 1.2112(5.2081) | Total Time 10.00(10.00)\n",
      "Iter 2340 | Time 10.9749(11.0535) | Bit/dim 1.1983(1.2007) | Xent 0.0677(0.0541) | Loss 1.2322(1.2277) | Error 0.0233(0.0166) Steps 524(527.57) | Grad Norm 7.4374(4.8903) | Total Time 10.00(10.00)\n",
      "Iter 2350 | Time 11.1055(11.0602) | Bit/dim 1.1860(1.1975) | Xent 0.0595(0.0522) | Loss 1.2157(1.2236) | Error 0.0178(0.0159) Steps 530(527.73) | Grad Norm 6.4298(5.4703) | Total Time 10.00(10.00)\n",
      "Iter 2360 | Time 11.0739(11.0366) | Bit/dim 1.1911(1.1939) | Xent 0.0520(0.0517) | Loss 1.2171(1.2197) | Error 0.0156(0.0157) Steps 530(528.02) | Grad Norm 7.8855(5.6036) | Total Time 10.00(10.00)\n",
      "Iter 2370 | Time 10.9820(11.0370) | Bit/dim 1.2151(1.1938) | Xent 0.0614(0.0506) | Loss 1.2458(1.2191) | Error 0.0200(0.0157) Steps 524(528.07) | Grad Norm 15.6619(6.4386) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0036 | Time 46.1654, Epoch Time 790.1325(695.8860), Bit/dim 1.4560(best: 1.1874), Xent 0.0522, Loss 1.4821, Error 0.0163(best: 0.0188)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2380 | Time 11.8021(11.1200) | Bit/dim 1.3228(1.2253) | Xent 0.0781(0.0512) | Loss 1.3618(1.2509) | Error 0.0289(0.0159) Steps 542(529.11) | Grad Norm 7.3458(8.6652) | Total Time 10.00(10.00)\n",
      "Iter 2390 | Time 11.4538(11.1333) | Bit/dim 1.2115(1.2331) | Xent 0.0671(0.0565) | Loss 1.2451(1.2613) | Error 0.0222(0.0175) Steps 542(531.39) | Grad Norm 3.8155(7.6970) | Total Time 10.00(10.00)\n",
      "Iter 2400 | Time 11.0978(11.1388) | Bit/dim 1.2041(1.2283) | Xent 0.0550(0.0574) | Loss 1.2315(1.2570) | Error 0.0167(0.0177) Steps 530(531.77) | Grad Norm 3.8534(6.7330) | Total Time 10.00(10.00)\n",
      "Iter 2410 | Time 10.7708(11.0949) | Bit/dim 1.1868(1.2205) | Xent 0.0233(0.0550) | Loss 1.1984(1.2480) | Error 0.0111(0.0171) Steps 524(530.93) | Grad Norm 2.8349(5.8138) | Total Time 10.00(10.00)\n",
      "Iter 2420 | Time 11.2227(11.0708) | Bit/dim 1.1825(1.2119) | Xent 0.0231(0.0538) | Loss 1.1940(1.2388) | Error 0.0100(0.0167) Steps 530(529.94) | Grad Norm 4.9875(5.4249) | Total Time 10.00(10.00)\n",
      "Iter 2430 | Time 11.1317(11.0786) | Bit/dim 1.1846(1.2053) | Xent 0.0611(0.0533) | Loss 1.2151(1.2319) | Error 0.0244(0.0167) Steps 530(529.52) | Grad Norm 6.0283(5.2203) | Total Time 10.00(10.00)\n",
      "Iter 2440 | Time 11.1679(11.0899) | Bit/dim 1.1850(1.1994) | Xent 0.0683(0.0511) | Loss 1.2192(1.2249) | Error 0.0211(0.0163) Steps 530(529.34) | Grad Norm 5.2444(5.1590) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0037 | Time 48.0794, Epoch Time 796.7090(698.9107), Bit/dim 1.1737(best: 1.1874), Xent 0.0591, Loss 1.2032, Error 0.0172(best: 0.0163)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2450 | Time 11.0865(11.0801) | Bit/dim 1.1870(1.1949) | Xent 0.0390(0.0497) | Loss 1.2065(1.2198) | Error 0.0122(0.0160) Steps 530(529.20) | Grad Norm 2.0225(5.5495) | Total Time 10.00(10.00)\n",
      "Iter 2460 | Time 11.0184(11.0790) | Bit/dim 1.1810(1.1922) | Xent 0.0338(0.0481) | Loss 1.1979(1.2162) | Error 0.0111(0.0153) Steps 524(528.93) | Grad Norm 8.2730(5.8703) | Total Time 10.00(10.00)\n",
      "Iter 2470 | Time 10.5660(11.1290) | Bit/dim 1.6007(1.2339) | Xent 0.0309(0.0484) | Loss 1.6161(1.2581) | Error 0.0089(0.0152) Steps 524(529.17) | Grad Norm 6.5066(9.6031) | Total Time 10.00(10.00)\n",
      "Iter 2480 | Time 11.7773(11.2719) | Bit/dim 1.5148(1.3117) | Xent 0.0462(0.0480) | Loss 1.5379(1.3357) | Error 0.0144(0.0154) Steps 554(533.57) | Grad Norm 2.8114(8.4389) | Total Time 10.00(10.00)\n",
      "Iter 2490 | Time 11.5916(11.2868) | Bit/dim 1.2914(1.3335) | Xent 0.0894(0.0498) | Loss 1.3361(1.3584) | Error 0.0233(0.0158) Steps 548(533.84) | Grad Norm 4.5583(7.1069) | Total Time 10.00(10.00)\n",
      "Iter 2500 | Time 10.6145(11.3223) | Bit/dim 1.3648(1.3302) | Xent 0.0535(0.0544) | Loss 1.3916(1.3573) | Error 0.0233(0.0174) Steps 506(534.61) | Grad Norm 15.7589(9.9682) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0038 | Time 45.7102, Epoch Time 808.8670(702.2094), Bit/dim 1.2508(best: 1.1737), Xent 0.0634, Loss 1.2825, Error 0.0195(best: 0.0163)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2510 | Time 11.3548(11.3401) | Bit/dim 1.2199(1.3112) | Xent 0.0533(0.0520) | Loss 1.2466(1.3372) | Error 0.0189(0.0166) Steps 536(533.21) | Grad Norm 3.9464(9.9648) | Total Time 10.00(10.00)\n",
      "Iter 2520 | Time 11.0784(11.2523) | Bit/dim 1.2214(1.2862) | Xent 0.0366(0.0491) | Loss 1.2397(1.3108) | Error 0.0122(0.0156) Steps 530(531.88) | Grad Norm 9.8371(9.2319) | Total Time 10.00(10.00)\n",
      "Iter 2530 | Time 11.1383(11.2203) | Bit/dim 1.1986(1.2638) | Xent 0.0401(0.0468) | Loss 1.2186(1.2872) | Error 0.0167(0.0151) Steps 524(530.44) | Grad Norm 4.3627(8.1756) | Total Time 10.00(10.00)\n",
      "Iter 2540 | Time 11.2905(11.1774) | Bit/dim 1.1969(1.2445) | Xent 0.0417(0.0461) | Loss 1.2178(1.2675) | Error 0.0156(0.0151) Steps 524(529.85) | Grad Norm 4.5995(7.0277) | Total Time 10.00(10.00)\n",
      "Iter 2550 | Time 11.1562(11.1409) | Bit/dim 1.1830(1.2289) | Xent 0.0602(0.0456) | Loss 1.2131(1.2517) | Error 0.0200(0.0150) Steps 524(528.94) | Grad Norm 2.4060(6.1901) | Total Time 10.00(10.00)\n",
      "Iter 2560 | Time 10.9658(11.1061) | Bit/dim 1.1819(1.2149) | Xent 0.0274(0.0445) | Loss 1.1956(1.2371) | Error 0.0067(0.0146) Steps 530(528.75) | Grad Norm 2.9521(5.2346) | Total Time 10.00(10.00)\n",
      "Iter 2570 | Time 11.4058(11.1263) | Bit/dim 1.1764(1.2054) | Xent 0.0514(0.0420) | Loss 1.2021(1.2264) | Error 0.0144(0.0133) Steps 530(529.08) | Grad Norm 2.5716(4.7271) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0039 | Time 48.2574, Epoch Time 794.0585(704.9649), Bit/dim 1.1753(best: 1.1737), Xent 0.0529, Loss 1.2018, Error 0.0166(best: 0.0163)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2580 | Time 11.1445(11.1219) | Bit/dim 1.1616(1.1977) | Xent 0.0328(0.0407) | Loss 1.1780(1.2181) | Error 0.0133(0.0133) Steps 530(529.00) | Grad Norm 11.1448(5.0292) | Total Time 10.00(10.00)\n",
      "Iter 2590 | Time 11.5926(11.2043) | Bit/dim 1.2547(1.2025) | Xent 0.0360(0.0393) | Loss 1.2727(1.2222) | Error 0.0122(0.0129) Steps 536(529.74) | Grad Norm 37.4865(7.9580) | Total Time 10.00(10.00)\n",
      "Iter 2600 | Time 11.0498(11.2999) | Bit/dim 1.2397(1.2171) | Xent 0.0498(0.0394) | Loss 1.2646(1.2368) | Error 0.0200(0.0130) Steps 530(531.95) | Grad Norm 10.6847(9.3669) | Total Time 10.00(10.00)\n",
      "Iter 2610 | Time 11.1079(11.2958) | Bit/dim 1.1757(1.2155) | Xent 0.0381(0.0406) | Loss 1.1948(1.2358) | Error 0.0111(0.0133) Steps 530(532.20) | Grad Norm 3.2043(9.8413) | Total Time 10.00(10.00)\n",
      "Iter 2620 | Time 11.4466(11.3188) | Bit/dim 1.2188(1.2168) | Xent 0.0324(0.0431) | Loss 1.2350(1.2384) | Error 0.0089(0.0131) Steps 530(532.76) | Grad Norm 11.0722(11.1157) | Total Time 10.00(10.00)\n",
      "Iter 2630 | Time 11.9076(11.3392) | Bit/dim 1.2278(1.2172) | Xent 0.0462(0.0422) | Loss 1.2509(1.2383) | Error 0.0144(0.0132) Steps 536(532.20) | Grad Norm 12.2516(11.4645) | Total Time 10.00(10.00)\n",
      "Iter 2640 | Time 11.2724(11.2951) | Bit/dim 1.1905(1.2109) | Xent 0.0388(0.0424) | Loss 1.2099(1.2321) | Error 0.0111(0.0133) Steps 530(531.31) | Grad Norm 12.2977(10.6171) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0040 | Time 48.2631, Epoch Time 811.9403(708.1741), Bit/dim 1.1747(best: 1.1737), Xent 0.0545, Loss 1.2019, Error 0.0162(best: 0.0163)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2650 | Time 11.0166(11.2601) | Bit/dim 1.1882(1.2021) | Xent 0.0388(0.0393) | Loss 1.2077(1.2218) | Error 0.0133(0.0124) Steps 524(530.65) | Grad Norm 3.5217(9.1107) | Total Time 10.00(10.00)\n",
      "Iter 2660 | Time 11.4276(11.2472) | Bit/dim 1.1785(1.1954) | Xent 0.0416(0.0381) | Loss 1.1993(1.2144) | Error 0.0122(0.0119) Steps 530(530.48) | Grad Norm 3.2619(7.4609) | Total Time 10.00(10.00)\n",
      "Iter 2670 | Time 11.6463(11.2781) | Bit/dim 1.1805(1.1900) | Xent 0.0464(0.0367) | Loss 1.2037(1.2083) | Error 0.0144(0.0118) Steps 530(530.35) | Grad Norm 2.2574(5.9744) | Total Time 10.00(10.00)\n",
      "Iter 2680 | Time 11.2842(11.2410) | Bit/dim 1.1607(1.1850) | Xent 0.0292(0.0372) | Loss 1.1753(1.2035) | Error 0.0100(0.0117) Steps 530(530.26) | Grad Norm 3.8598(5.1604) | Total Time 10.00(10.00)\n",
      "Iter 2690 | Time 11.0324(11.2226) | Bit/dim 1.1619(1.1810) | Xent 0.0299(0.0373) | Loss 1.1769(1.1997) | Error 0.0133(0.0118) Steps 530(530.19) | Grad Norm 2.3105(4.9867) | Total Time 10.00(10.00)\n",
      "Iter 2700 | Time 11.0921(11.2245) | Bit/dim 1.1972(1.1817) | Xent 0.0701(0.0375) | Loss 1.2322(1.2004) | Error 0.0156(0.0118) Steps 530(530.14) | Grad Norm 19.8500(6.8825) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0041 | Time 49.2224, Epoch Time 803.5631(711.0358), Bit/dim 1.1914(best: 1.1737), Xent 0.0589, Loss 1.2209, Error 0.0177(best: 0.0162)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2710 | Time 10.9869(11.2115) | Bit/dim 1.1937(1.1827) | Xent 0.0401(0.0370) | Loss 1.2138(1.2012) | Error 0.0144(0.0118) Steps 530(529.95) | Grad Norm 14.3307(7.9924) | Total Time 10.00(10.00)\n",
      "Iter 2720 | Time 10.9008(11.1755) | Bit/dim 1.1970(1.1812) | Xent 0.0385(0.0368) | Loss 1.2163(1.1996) | Error 0.0100(0.0113) Steps 530(529.82) | Grad Norm 19.8248(8.4360) | Total Time 10.00(10.00)\n",
      "Iter 2730 | Time 11.1660(11.1682) | Bit/dim 1.1527(1.1820) | Xent 0.0404(0.0364) | Loss 1.1728(1.2002) | Error 0.0122(0.0113) Steps 530(529.73) | Grad Norm 1.6121(8.9598) | Total Time 10.00(10.00)\n",
      "Iter 2740 | Time 11.0128(11.1488) | Bit/dim 1.1856(1.1779) | Xent 0.0275(0.0352) | Loss 1.1994(1.1955) | Error 0.0089(0.0111) Steps 530(529.80) | Grad Norm 6.6845(7.8509) | Total Time 10.00(10.00)\n",
      "Iter 2750 | Time 11.2765(11.1525) | Bit/dim 1.2117(1.1802) | Xent 0.0320(0.0357) | Loss 1.2277(1.1981) | Error 0.0089(0.0113) Steps 530(529.87) | Grad Norm 15.2550(8.7104) | Total Time 10.00(10.00)\n",
      "Iter 2760 | Time 11.3397(11.2186) | Bit/dim 1.2429(1.1869) | Xent 0.0439(0.0369) | Loss 1.2649(1.2053) | Error 0.0200(0.0121) Steps 542(531.44) | Grad Norm 18.3960(10.3607) | Total Time 10.00(10.00)\n",
      "Iter 2770 | Time 11.2423(11.2942) | Bit/dim 1.2294(1.2005) | Xent 0.0552(0.0377) | Loss 1.2570(1.2193) | Error 0.0222(0.0120) Steps 536(534.15) | Grad Norm 9.7233(11.9285) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0042 | Time 48.3510, Epoch Time 804.3210(713.8344), Bit/dim 1.2123(best: 1.1737), Xent 0.0492, Loss 1.2369, Error 0.0117(best: 0.0162)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2780 | Time 11.4572(11.2949) | Bit/dim 1.2148(1.2006) | Xent 0.0489(0.0361) | Loss 1.2393(1.2186) | Error 0.0200(0.0119) Steps 530(534.03) | Grad Norm 22.9114(11.9301) | Total Time 10.00(10.00)\n",
      "Iter 2790 | Time 11.2790(11.3250) | Bit/dim 1.2757(1.2050) | Xent 0.0337(0.0368) | Loss 1.2926(1.2234) | Error 0.0133(0.0123) Steps 530(534.52) | Grad Norm 13.6851(12.7911) | Total Time 10.00(10.00)\n",
      "Iter 2800 | Time 11.4129(11.3334) | Bit/dim 1.2244(1.2079) | Xent 0.0192(0.0366) | Loss 1.2340(1.2262) | Error 0.0067(0.0124) Steps 524(533.88) | Grad Norm 13.1456(12.8377) | Total Time 10.00(10.00)\n",
      "Iter 2810 | Time 11.2541(11.2672) | Bit/dim 1.1678(1.2049) | Xent 0.0261(0.0355) | Loss 1.1809(1.2226) | Error 0.0167(0.0121) Steps 530(533.06) | Grad Norm 4.0026(12.2188) | Total Time 10.00(10.00)\n",
      "Iter 2820 | Time 10.9231(11.2086) | Bit/dim 1.1601(1.1994) | Xent 0.0358(0.0347) | Loss 1.1780(1.2168) | Error 0.0100(0.0115) Steps 530(532.42) | Grad Norm 3.1077(11.4716) | Total Time 10.00(10.00)\n",
      "Iter 2830 | Time 11.2020(11.1770) | Bit/dim 1.1736(1.1921) | Xent 0.0277(0.0340) | Loss 1.1875(1.2092) | Error 0.0089(0.0113) Steps 530(531.92) | Grad Norm 3.6258(10.2514) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0043 | Time 48.8313, Epoch Time 803.5189(716.5249), Bit/dim 1.1573(best: 1.1737), Xent 0.0524, Loss 1.1835, Error 0.0157(best: 0.0117)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2840 | Time 11.0012(11.1799) | Bit/dim 1.1711(1.1849) | Xent 0.0190(0.0334) | Loss 1.1806(1.2016) | Error 0.0056(0.0108) Steps 530(531.27) | Grad Norm 5.4053(9.0341) | Total Time 10.00(10.00)\n",
      "Iter 2850 | Time 11.0489(11.1581) | Bit/dim 1.1574(1.1791) | Xent 0.0269(0.0331) | Loss 1.1709(1.1957) | Error 0.0089(0.0108) Steps 530(530.93) | Grad Norm 3.0003(7.6348) | Total Time 10.00(10.00)\n",
      "Iter 2860 | Time 11.1486(11.1219) | Bit/dim 1.1443(1.1752) | Xent 0.0349(0.0315) | Loss 1.1617(1.1909) | Error 0.0100(0.0103) Steps 530(530.69) | Grad Norm 3.7647(6.6249) | Total Time 10.00(10.00)\n",
      "Iter 2870 | Time 11.4973(11.1539) | Bit/dim 1.1544(1.1705) | Xent 0.0211(0.0308) | Loss 1.1650(1.1859) | Error 0.0067(0.0103) Steps 530(530.51) | Grad Norm 2.4047(5.7872) | Total Time 10.00(10.00)\n",
      "Iter 2880 | Time 11.1046(11.1397) | Bit/dim 1.1776(1.1683) | Xent 0.0391(0.0314) | Loss 1.1971(1.1840) | Error 0.0189(0.0107) Steps 530(530.37) | Grad Norm 1.8070(5.0523) | Total Time 10.00(10.00)\n",
      "Iter 2890 | Time 11.1820(11.1112) | Bit/dim 1.1436(1.1655) | Xent 0.0401(0.0311) | Loss 1.1636(1.1811) | Error 0.0111(0.0105) Steps 530(530.44) | Grad Norm 1.7921(4.4726) | Total Time 10.00(10.00)\n",
      "Iter 2900 | Time 10.9892(11.0891) | Bit/dim 1.1687(1.1633) | Xent 0.0185(0.0312) | Loss 1.1779(1.1789) | Error 0.0078(0.0102) Steps 530(530.32) | Grad Norm 3.3056(3.9002) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0044 | Time 46.9697, Epoch Time 793.5233(718.8348), Bit/dim 1.1485(best: 1.1573), Xent 0.0491, Loss 1.1730, Error 0.0155(best: 0.0117)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2910 | Time 10.9807(11.0895) | Bit/dim 1.1513(1.1619) | Xent 0.0320(0.0300) | Loss 1.1673(1.1769) | Error 0.0111(0.0097) Steps 530(530.24) | Grad Norm 3.2172(3.8585) | Total Time 10.00(10.00)\n",
      "Iter 2920 | Time 10.9568(11.1061) | Bit/dim 1.1471(1.1590) | Xent 0.0115(0.0292) | Loss 1.1529(1.1736) | Error 0.0044(0.0094) Steps 530(530.18) | Grad Norm 2.1022(3.5991) | Total Time 10.00(10.00)\n",
      "Iter 2930 | Time 11.0017(11.0813) | Bit/dim 1.1503(1.1594) | Xent 0.0189(0.0285) | Loss 1.1598(1.1737) | Error 0.0089(0.0093) Steps 530(530.13) | Grad Norm 4.8288(4.0252) | Total Time 10.00(10.00)\n",
      "Iter 2940 | Time 11.2561(11.0954) | Bit/dim 1.1688(1.1573) | Xent 0.0394(0.0275) | Loss 1.1885(1.1711) | Error 0.0133(0.0090) Steps 530(530.10) | Grad Norm 4.5457(4.1820) | Total Time 10.00(10.00)\n",
      "Iter 2950 | Time 11.1136(11.0970) | Bit/dim 1.1616(1.1582) | Xent 0.0235(0.0283) | Loss 1.1734(1.1723) | Error 0.0067(0.0091) Steps 530(530.07) | Grad Norm 7.8815(4.7229) | Total Time 10.00(10.00)\n",
      "Iter 2960 | Time 11.0709(11.0920) | Bit/dim 1.1571(1.1594) | Xent 0.0164(0.0275) | Loss 1.1653(1.1732) | Error 0.0056(0.0092) Steps 530(530.23) | Grad Norm 4.2169(5.7813) | Total Time 10.00(10.00)\n",
      "Iter 2970 | Time 11.7973(11.2259) | Bit/dim 1.2464(1.1893) | Xent 0.0806(0.0293) | Loss 1.2867(1.2039) | Error 0.0256(0.0098) Steps 542(531.66) | Grad Norm 8.3215(8.3748) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0045 | Time 49.0916, Epoch Time 801.5791(721.3172), Bit/dim 1.2393(best: 1.1485), Xent 0.0526, Loss 1.2656, Error 0.0150(best: 0.0117)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2980 | Time 11.2149(11.2144) | Bit/dim 1.1805(1.1932) | Xent 0.0535(0.0322) | Loss 1.2072(1.2093) | Error 0.0178(0.0110) Steps 542(532.69) | Grad Norm 5.8166(7.6393) | Total Time 10.00(10.00)\n",
      "Iter 2990 | Time 10.9968(11.1838) | Bit/dim 1.1715(1.1873) | Xent 0.0419(0.0326) | Loss 1.1924(1.2036) | Error 0.0122(0.0111) Steps 530(533.24) | Grad Norm 4.8267(6.8139) | Total Time 10.00(10.00)\n",
      "Iter 3000 | Time 11.0350(11.1950) | Bit/dim 1.1870(1.1888) | Xent 0.0340(0.0325) | Loss 1.2040(1.2050) | Error 0.0133(0.0112) Steps 530(532.07) | Grad Norm 6.8998(8.2344) | Total Time 10.00(10.00)\n",
      "Iter 3010 | Time 11.2135(11.2716) | Bit/dim 1.2274(1.2018) | Xent 0.0331(0.0334) | Loss 1.2439(1.2185) | Error 0.0111(0.0111) Steps 536(533.82) | Grad Norm 21.0428(10.7355) | Total Time 10.00(10.00)\n",
      "Iter 3020 | Time 11.4531(11.2797) | Bit/dim 1.2158(1.2049) | Xent 0.0363(0.0329) | Loss 1.2339(1.2214) | Error 0.0122(0.0109) Steps 530(534.20) | Grad Norm 12.1364(11.8999) | Total Time 10.00(10.00)\n",
      "Iter 3030 | Time 11.3320(11.2578) | Bit/dim 1.1805(1.1972) | Xent 0.0342(0.0319) | Loss 1.1976(1.2131) | Error 0.0100(0.0105) Steps 530(533.39) | Grad Norm 15.4714(11.3901) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0046 | Time 48.4037, Epoch Time 806.1747(723.8629), Bit/dim 1.1654(best: 1.1485), Xent 0.0487, Loss 1.1898, Error 0.0151(best: 0.0117)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3040 | Time 11.1026(11.2713) | Bit/dim 1.1420(1.1876) | Xent 0.0379(0.0311) | Loss 1.1609(1.2032) | Error 0.0122(0.0102) Steps 530(532.34) | Grad Norm 7.1230(10.5417) | Total Time 10.00(10.00)\n",
      "Iter 3050 | Time 11.2943(11.2244) | Bit/dim 1.1562(1.1793) | Xent 0.0119(0.0295) | Loss 1.1621(1.1940) | Error 0.0056(0.0098) Steps 530(531.73) | Grad Norm 2.0946(8.5402) | Total Time 10.00(10.00)\n",
      "Iter 3060 | Time 10.8850(11.1778) | Bit/dim 1.1560(1.1716) | Xent 0.0459(0.0283) | Loss 1.1789(1.1857) | Error 0.0111(0.0093) Steps 530(531.27) | Grad Norm 4.7294(6.9092) | Total Time 10.00(10.00)\n",
      "Iter 3070 | Time 11.4495(11.1670) | Bit/dim 1.1553(1.1673) | Xent 0.0213(0.0276) | Loss 1.1660(1.1811) | Error 0.0089(0.0091) Steps 530(530.94) | Grad Norm 11.9317(6.9370) | Total Time 10.00(10.00)\n",
      "Iter 3080 | Time 11.7367(11.2473) | Bit/dim 1.2435(1.1818) | Xent 0.0418(0.0303) | Loss 1.2644(1.1969) | Error 0.0122(0.0100) Steps 554(532.30) | Grad Norm 21.0284(9.7712) | Total Time 10.00(10.00)\n",
      "Iter 3090 | Time 11.4409(11.2312) | Bit/dim 1.2096(1.1868) | Xent 0.0215(0.0292) | Loss 1.2203(1.2014) | Error 0.0078(0.0097) Steps 530(531.86) | Grad Norm 13.7279(10.7919) | Total Time 10.00(10.00)\n",
      "Iter 3100 | Time 11.0214(11.3115) | Bit/dim 1.1748(1.1886) | Xent 0.0316(0.0274) | Loss 1.1906(1.2023) | Error 0.0100(0.0091) Steps 530(533.20) | Grad Norm 4.7778(11.2265) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0047 | Time 49.1627, Epoch Time 806.6435(726.3463), Bit/dim 1.1634(best: 1.1485), Xent 0.0507, Loss 1.1888, Error 0.0137(best: 0.0117)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3110 | Time 11.1215(11.2755) | Bit/dim 1.1529(1.1810) | Xent 0.0198(0.0254) | Loss 1.1629(1.1937) | Error 0.0089(0.0087) Steps 530(532.36) | Grad Norm 7.8131(10.3132) | Total Time 10.00(10.00)\n",
      "Iter 3120 | Time 11.1225(11.2435) | Bit/dim 1.1599(1.1746) | Xent 0.0225(0.0250) | Loss 1.1711(1.1871) | Error 0.0067(0.0083) Steps 530(531.74) | Grad Norm 7.8195(9.6114) | Total Time 10.00(10.00)\n",
      "Iter 3130 | Time 11.3340(11.2350) | Bit/dim 1.1501(1.1693) | Xent 0.0243(0.0236) | Loss 1.1623(1.1812) | Error 0.0067(0.0075) Steps 530(531.28) | Grad Norm 3.5060(8.4572) | Total Time 10.00(10.00)\n",
      "Iter 3140 | Time 10.9885(11.1950) | Bit/dim 1.1505(1.1642) | Xent 0.0274(0.0237) | Loss 1.1643(1.1760) | Error 0.0067(0.0077) Steps 530(530.95) | Grad Norm 3.4465(7.2691) | Total Time 10.00(10.00)\n",
      "Iter 3150 | Time 11.3941(11.1720) | Bit/dim 1.1545(1.1602) | Xent 0.0278(0.0228) | Loss 1.1684(1.1715) | Error 0.0111(0.0073) Steps 530(530.70) | Grad Norm 4.1528(6.2122) | Total Time 10.00(10.00)\n",
      "Iter 3160 | Time 11.2843(11.1534) | Bit/dim 1.1515(1.1567) | Xent 0.0147(0.0229) | Loss 1.1589(1.1682) | Error 0.0067(0.0072) Steps 530(530.51) | Grad Norm 3.6100(5.2503) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0048 | Time 47.8454, Epoch Time 797.6855(728.4865), Bit/dim 1.1412(best: 1.1485), Xent 0.0398, Loss 1.1610, Error 0.0137(best: 0.0117)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3170 | Time 11.0756(11.1406) | Bit/dim 1.1629(1.1541) | Xent 0.0155(0.0229) | Loss 1.1707(1.1656) | Error 0.0067(0.0074) Steps 530(530.38) | Grad Norm 3.4833(4.4858) | Total Time 10.00(10.00)\n",
      "Iter 3180 | Time 11.2250(11.1179) | Bit/dim 1.1348(1.1509) | Xent 0.0193(0.0222) | Loss 1.1445(1.1620) | Error 0.0078(0.0074) Steps 530(530.28) | Grad Norm 2.0099(4.2341) | Total Time 10.00(10.00)\n",
      "Iter 3190 | Time 11.3828(11.1364) | Bit/dim 1.1649(1.1501) | Xent 0.0282(0.0234) | Loss 1.1790(1.1618) | Error 0.0133(0.0075) Steps 530(530.21) | Grad Norm 8.5254(4.8989) | Total Time 10.00(10.00)\n",
      "Iter 3200 | Time 11.0573(11.1232) | Bit/dim 1.1379(1.1485) | Xent 0.0404(0.0227) | Loss 1.1581(1.1599) | Error 0.0122(0.0074) Steps 530(530.15) | Grad Norm 6.2679(4.8168) | Total Time 10.00(10.00)\n",
      "Iter 3210 | Time 10.9734(11.1412) | Bit/dim 1.1454(1.1478) | Xent 0.0189(0.0216) | Loss 1.1548(1.1586) | Error 0.0067(0.0072) Steps 530(530.11) | Grad Norm 2.4873(4.4392) | Total Time 10.00(10.00)\n",
      "Iter 3220 | Time 10.5204(11.1672) | Bit/dim 1.4130(1.1656) | Xent 0.0309(0.0227) | Loss 1.4285(1.1769) | Error 0.0089(0.0073) Steps 518(530.68) | Grad Norm 12.8954(7.1990) | Total Time 10.00(10.00)\n",
      "Iter 3230 | Time 11.4290(11.2184) | Bit/dim 1.2139(1.1952) | Xent 0.0401(0.0271) | Loss 1.2340(1.2088) | Error 0.0122(0.0085) Steps 530(531.54) | Grad Norm 4.0639(6.9521) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0049 | Time 47.5997, Epoch Time 800.7462(730.6543), Bit/dim 1.1816(best: 1.1412), Xent 0.0496, Loss 1.2064, Error 0.0143(best: 0.0117)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3240 | Time 10.7545(11.1585) | Bit/dim 1.1873(1.1924) | Xent 0.0453(0.0284) | Loss 1.2099(1.2066) | Error 0.0100(0.0090) Steps 524(530.16) | Grad Norm 6.9819(6.3746) | Total Time 10.00(10.00)\n",
      "Iter 3250 | Time 11.0545(11.1205) | Bit/dim 1.1505(1.1832) | Xent 0.0154(0.0276) | Loss 1.1582(1.1970) | Error 0.0044(0.0089) Steps 530(529.82) | Grad Norm 1.5955(5.5371) | Total Time 10.00(10.00)\n",
      "Iter 3260 | Time 11.4054(11.1067) | Bit/dim 1.1447(1.1741) | Xent 0.0199(0.0257) | Loss 1.1547(1.1870) | Error 0.0067(0.0086) Steps 530(529.86) | Grad Norm 1.7308(4.7090) | Total Time 10.00(10.00)\n",
      "Iter 3270 | Time 10.9998(11.0934) | Bit/dim 1.1214(1.1657) | Xent 0.0140(0.0256) | Loss 1.1283(1.1785) | Error 0.0056(0.0085) Steps 530(529.90) | Grad Norm 2.1877(4.2540) | Total Time 10.00(10.00)\n",
      "Iter 3280 | Time 11.0920(11.1009) | Bit/dim 1.1444(1.1603) | Xent 0.0184(0.0245) | Loss 1.1536(1.1725) | Error 0.0089(0.0081) Steps 530(529.77) | Grad Norm 3.1998(4.0826) | Total Time 10.00(10.00)\n",
      "Iter 3290 | Time 11.1436(11.0952) | Bit/dim 1.1453(1.1575) | Xent 0.0109(0.0231) | Loss 1.1507(1.1690) | Error 0.0033(0.0077) Steps 530(529.52) | Grad Norm 9.7924(4.8232) | Total Time 10.00(10.00)\n",
      "Iter 3300 | Time 10.8397(11.1434) | Bit/dim 1.3917(1.1666) | Xent 0.0404(0.0240) | Loss 1.4119(1.1786) | Error 0.0100(0.0078) Steps 518(530.30) | Grad Norm 19.8625(7.6548) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0050 | Time 47.1027, Epoch Time 793.6054(732.5428), Bit/dim 1.4673(best: 1.1412), Xent 0.0394, Loss 1.4870, Error 0.0099(best: 0.0117)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3310 | Time 11.0324(11.1566) | Bit/dim 1.3137(1.2200) | Xent 0.0402(0.0251) | Loss 1.3338(1.2325) | Error 0.0111(0.0082) Steps 530(530.75) | Grad Norm 5.1019(7.2927) | Total Time 10.00(10.00)\n",
      "Iter 3320 | Time 11.7791(11.1544) | Bit/dim 1.2078(1.2177) | Xent 0.0212(0.0244) | Loss 1.2183(1.2299) | Error 0.0056(0.0078) Steps 536(531.84) | Grad Norm 16.8238(7.3656) | Total Time 10.00(10.00)\n",
      "Iter 3330 | Time 10.6713(11.1716) | Bit/dim 1.2698(1.2292) | Xent 0.0205(0.0264) | Loss 1.2800(1.2424) | Error 0.0056(0.0085) Steps 512(531.14) | Grad Norm 13.4841(10.4470) | Total Time 10.00(10.00)\n",
      "Iter 3340 | Time 11.0671(11.1940) | Bit/dim 1.2050(1.2269) | Xent 0.0213(0.0256) | Loss 1.2157(1.2398) | Error 0.0067(0.0081) Steps 530(531.27) | Grad Norm 10.0706(10.8252) | Total Time 10.00(10.00)\n",
      "Iter 3350 | Time 10.8930(11.1763) | Bit/dim 1.1696(1.2131) | Xent 0.0339(0.0236) | Loss 1.1865(1.2249) | Error 0.0078(0.0075) Steps 530(531.85) | Grad Norm 16.1344(10.5001) | Total Time 10.00(10.00)\n",
      "Iter 3360 | Time 10.9731(11.1594) | Bit/dim 1.1547(1.1967) | Xent 0.0156(0.0232) | Loss 1.1625(1.2083) | Error 0.0078(0.0075) Steps 530(531.64) | Grad Norm 5.3219(9.3671) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0051 | Time 49.7393, Epoch Time 802.4128(734.6389), Bit/dim 1.1436(best: 1.1412), Xent 0.0455, Loss 1.1663, Error 0.0135(best: 0.0099)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3370 | Time 11.0544(11.1404) | Bit/dim 1.1381(1.1838) | Xent 0.0136(0.0215) | Loss 1.1448(1.1946) | Error 0.0056(0.0070) Steps 530(531.35) | Grad Norm 5.4696(8.1733) | Total Time 10.00(10.00)\n",
      "Iter 3380 | Time 10.8109(11.1043) | Bit/dim 1.1420(1.1719) | Xent 0.0182(0.0211) | Loss 1.1511(1.1825) | Error 0.0067(0.0069) Steps 530(530.83) | Grad Norm 5.0086(7.0307) | Total Time 10.00(10.00)\n",
      "Iter 3390 | Time 11.0119(11.0949) | Bit/dim 1.1348(1.1642) | Xent 0.0322(0.0209) | Loss 1.1509(1.1746) | Error 0.0111(0.0069) Steps 530(530.95) | Grad Norm 2.7279(5.7856) | Total Time 10.00(10.00)\n",
      "Iter 3400 | Time 11.0777(11.1017) | Bit/dim 1.1405(1.1579) | Xent 0.0150(0.0202) | Loss 1.1479(1.1680) | Error 0.0067(0.0069) Steps 530(530.70) | Grad Norm 2.0924(4.7540) | Total Time 10.00(10.00)\n",
      "Iter 3410 | Time 11.0069(11.0781) | Bit/dim 1.1407(1.1541) | Xent 0.0358(0.0204) | Loss 1.1586(1.1643) | Error 0.0089(0.0067) Steps 530(530.52) | Grad Norm 5.3837(4.3613) | Total Time 10.00(10.00)\n",
      "Iter 3420 | Time 11.4950(11.1017) | Bit/dim 1.1188(1.1495) | Xent 0.0245(0.0194) | Loss 1.1310(1.1591) | Error 0.0067(0.0065) Steps 530(530.38) | Grad Norm 3.3840(4.3622) | Total Time 10.00(10.00)\n",
      "Iter 3430 | Time 11.0424(11.1171) | Bit/dim 1.1330(1.1462) | Xent 0.0160(0.0189) | Loss 1.1410(1.1556) | Error 0.0056(0.0065) Steps 530(530.72) | Grad Norm 2.1586(4.1571) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0052 | Time 48.8597, Epoch Time 795.7315(736.4717), Bit/dim 1.1300(best: 1.1412), Xent 0.0404, Loss 1.1502, Error 0.0117(best: 0.0099)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3440 | Time 11.1543(11.1362) | Bit/dim 1.1181(1.1437) | Xent 0.0139(0.0174) | Loss 1.1251(1.1524) | Error 0.0033(0.0059) Steps 530(530.67) | Grad Norm 2.3663(3.5313) | Total Time 10.00(10.00)\n",
      "Iter 3450 | Time 11.1851(11.1309) | Bit/dim 1.1417(1.1415) | Xent 0.0157(0.0174) | Loss 1.1495(1.1502) | Error 0.0044(0.0058) Steps 530(530.50) | Grad Norm 10.4297(3.7915) | Total Time 10.00(10.00)\n",
      "Iter 3460 | Time 11.2358(11.1302) | Bit/dim 1.1221(1.1396) | Xent 0.0200(0.0174) | Loss 1.1321(1.1483) | Error 0.0078(0.0056) Steps 536(531.22) | Grad Norm 1.8575(4.1770) | Total Time 10.00(10.00)\n",
      "Iter 3470 | Time 12.4834(11.2127) | Bit/dim 1.1869(1.1491) | Xent 0.0284(0.0177) | Loss 1.2011(1.1579) | Error 0.0089(0.0057) Steps 554(532.18) | Grad Norm 30.5235(7.0722) | Total Time 10.00(10.00)\n",
      "Iter 3480 | Time 11.6204(11.2081) | Bit/dim 1.2021(1.1747) | Xent 0.0451(0.0205) | Loss 1.2247(1.1850) | Error 0.0222(0.0070) Steps 536(532.47) | Grad Norm 12.4494(9.3597) | Total Time 10.00(10.00)\n",
      "Iter 3490 | Time 11.3493(11.1923) | Bit/dim 1.2338(1.1941) | Xent 0.0323(0.0231) | Loss 1.2500(1.2057) | Error 0.0111(0.0079) Steps 536(531.63) | Grad Norm 26.9024(10.9006) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0053 | Time 49.4214, Epoch Time 800.0737(738.3798), Bit/dim 1.1663(best: 1.1300), Xent 0.0510, Loss 1.1918, Error 0.0135(best: 0.0099)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3500 | Time 11.9322(11.1270) | Bit/dim 1.2980(1.2068) | Xent 0.0469(0.0257) | Loss 1.3215(1.2197) | Error 0.0189(0.0088) Steps 554(530.69) | Grad Norm 41.7168(12.3154) | Total Time 10.00(10.00)\n",
      "Iter 3510 | Time 10.9544(11.1104) | Bit/dim 1.1833(1.2114) | Xent 0.0246(0.0260) | Loss 1.1956(1.2244) | Error 0.0067(0.0089) Steps 524(530.89) | Grad Norm 4.6150(11.5376) | Total Time 10.00(10.00)\n",
      "Iter 3520 | Time 11.4704(11.1283) | Bit/dim 1.1587(1.2023) | Xent 0.0218(0.0257) | Loss 1.1696(1.2151) | Error 0.0056(0.0086) Steps 530(529.87) | Grad Norm 10.4332(11.2471) | Total Time 10.00(10.00)\n",
      "Iter 3530 | Time 11.6002(11.1448) | Bit/dim 1.1667(1.1930) | Xent 0.0165(0.0243) | Loss 1.1750(1.2052) | Error 0.0056(0.0082) Steps 524(528.84) | Grad Norm 11.5868(10.9881) | Total Time 10.00(10.00)\n",
      "Iter 3540 | Time 11.1782(11.1165) | Bit/dim 1.1621(1.1867) | Xent 0.0220(0.0226) | Loss 1.1731(1.1980) | Error 0.0078(0.0075) Steps 530(528.80) | Grad Norm 12.2642(10.9931) | Total Time 10.00(10.00)\n",
      "Iter 3550 | Time 10.8385(11.0966) | Bit/dim 1.1398(1.1756) | Xent 0.0257(0.0217) | Loss 1.1526(1.1865) | Error 0.0078(0.0071) Steps 524(527.99) | Grad Norm 4.9438(9.6939) | Total Time 10.00(10.00)\n",
      "Iter 3560 | Time 11.3142(11.1212) | Bit/dim 1.1352(1.1667) | Xent 0.0174(0.0204) | Loss 1.1439(1.1770) | Error 0.0056(0.0068) Steps 530(527.90) | Grad Norm 2.5167(8.1842) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0054 | Time 49.5267, Epoch Time 799.0868(740.2010), Bit/dim 1.1296(best: 1.1300), Xent 0.0415, Loss 1.1503, Error 0.0106(best: 0.0099)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3570 | Time 11.0949(11.1316) | Bit/dim 1.1240(1.1586) | Xent 0.0099(0.0187) | Loss 1.1289(1.1679) | Error 0.0033(0.0062) Steps 530(529.07) | Grad Norm 3.5943(6.6530) | Total Time 10.00(10.00)\n",
      "Iter 3580 | Time 10.9141(11.1177) | Bit/dim 1.1251(1.1527) | Xent 0.0140(0.0180) | Loss 1.1322(1.1617) | Error 0.0033(0.0059) Steps 530(529.31) | Grad Norm 2.5117(5.5384) | Total Time 10.00(10.00)\n",
      "Iter 3590 | Time 11.0437(11.1025) | Bit/dim 1.1402(1.1466) | Xent 0.0105(0.0168) | Loss 1.1455(1.1550) | Error 0.0022(0.0056) Steps 530(529.49) | Grad Norm 2.2167(4.9077) | Total Time 10.00(10.00)\n",
      "Iter 3600 | Time 11.2045(11.0973) | Bit/dim 1.1450(1.1442) | Xent 0.0108(0.0164) | Loss 1.1504(1.1524) | Error 0.0022(0.0054) Steps 524(529.45) | Grad Norm 11.0359(4.9903) | Total Time 10.00(10.00)\n",
      "Iter 3610 | Time 11.0382(11.1416) | Bit/dim 1.1428(1.1445) | Xent 0.0254(0.0165) | Loss 1.1555(1.1528) | Error 0.0078(0.0054) Steps 530(530.07) | Grad Norm 6.2381(6.5967) | Total Time 10.00(10.00)\n",
      "Iter 3620 | Time 11.4721(11.1472) | Bit/dim 1.1587(1.1448) | Xent 0.0161(0.0163) | Loss 1.1668(1.1530) | Error 0.0056(0.0053) Steps 536(530.84) | Grad Norm 26.8553(7.7386) | Total Time 10.00(10.00)\n",
      "Iter 3630 | Time 10.8838(11.1795) | Bit/dim 1.1940(1.1628) | Xent 0.0213(0.0183) | Loss 1.2047(1.1719) | Error 0.0089(0.0059) Steps 542(531.44) | Grad Norm 10.4524(9.7267) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0055 | Time 46.8341, Epoch Time 797.6831(741.9254), Bit/dim 1.2160(best: 1.1296), Xent 0.0526, Loss 1.2423, Error 0.0146(best: 0.0099)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3640 | Time 11.1784(11.1634) | Bit/dim 1.2344(1.1783) | Xent 0.0182(0.0188) | Loss 1.2435(1.1877) | Error 0.0078(0.0060) Steps 536(531.28) | Grad Norm 13.7967(10.7329) | Total Time 10.00(10.00)\n",
      "Iter 3650 | Time 10.3299(11.1536) | Bit/dim 1.2413(1.1886) | Xent 0.0093(0.0200) | Loss 1.2460(1.1986) | Error 0.0022(0.0063) Steps 512(530.45) | Grad Norm 11.9855(11.7780) | Total Time 10.00(10.00)\n",
      "Iter 3660 | Time 10.2864(11.1113) | Bit/dim 1.2070(1.1885) | Xent 0.0223(0.0200) | Loss 1.2181(1.1986) | Error 0.0089(0.0066) Steps 518(529.55) | Grad Norm 11.0390(11.6792) | Total Time 10.00(10.00)\n",
      "Iter 3670 | Time 11.4798(11.1530) | Bit/dim 1.1716(1.1853) | Xent 0.0183(0.0195) | Loss 1.1808(1.1950) | Error 0.0078(0.0066) Steps 524(529.80) | Grad Norm 10.4667(11.3947) | Total Time 10.00(10.00)\n",
      "Iter 3680 | Time 11.4444(11.1515) | Bit/dim 1.1592(1.1768) | Xent 0.0243(0.0184) | Loss 1.1714(1.1860) | Error 0.0056(0.0063) Steps 530(530.67) | Grad Norm 14.7191(10.7411) | Total Time 10.00(10.00)\n",
      "Iter 3690 | Time 10.7622(11.1450) | Bit/dim 1.1547(1.1683) | Xent 0.0130(0.0174) | Loss 1.1613(1.1770) | Error 0.0067(0.0060) Steps 524(529.71) | Grad Norm 6.9617(9.5043) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0056 | Time 48.9604, Epoch Time 797.9952(743.6075), Bit/dim 1.1319(best: 1.1296), Xent 0.0432, Loss 1.1534, Error 0.0118(best: 0.0099)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3700 | Time 11.1116(11.1167) | Bit/dim 1.1313(1.1588) | Xent 0.0138(0.0160) | Loss 1.1382(1.1668) | Error 0.0044(0.0058) Steps 524(529.15) | Grad Norm 6.2627(8.4375) | Total Time 10.00(10.00)\n",
      "Iter 3710 | Time 10.9428(11.1182) | Bit/dim 1.1346(1.1522) | Xent 0.0157(0.0153) | Loss 1.1425(1.1598) | Error 0.0067(0.0055) Steps 530(529.37) | Grad Norm 4.4525(7.4513) | Total Time 10.00(10.00)\n",
      "Iter 3720 | Time 11.1473(11.1163) | Bit/dim 1.1415(1.1467) | Xent 0.0142(0.0147) | Loss 1.1486(1.1541) | Error 0.0056(0.0052) Steps 542(530.56) | Grad Norm 4.8773(6.4276) | Total Time 10.00(10.00)\n",
      "Iter 3730 | Time 10.9741(11.0991) | Bit/dim 1.1261(1.1428) | Xent 0.0087(0.0141) | Loss 1.1304(1.1498) | Error 0.0011(0.0049) Steps 536(531.85) | Grad Norm 4.8103(6.3774) | Total Time 10.00(10.00)\n",
      "Iter 3740 | Time 11.2618(11.1064) | Bit/dim 1.1210(1.1420) | Xent 0.0259(0.0142) | Loss 1.1340(1.1491) | Error 0.0022(0.0048) Steps 530(531.83) | Grad Norm 11.6417(7.5463) | Total Time 10.00(10.00)\n",
      "Iter 3750 | Time 11.1733(11.1147) | Bit/dim 1.1284(1.1384) | Xent 0.0235(0.0154) | Loss 1.1402(1.1461) | Error 0.0089(0.0051) Steps 530(531.35) | Grad Norm 2.6352(6.5085) | Total Time 10.00(10.00)\n",
      "Iter 3760 | Time 11.0505(11.0918) | Bit/dim 1.1401(1.1361) | Xent 0.0124(0.0143) | Loss 1.1463(1.1432) | Error 0.0056(0.0050) Steps 530(531.00) | Grad Norm 1.7497(5.7192) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0057 | Time 48.5119, Epoch Time 795.6857(745.1699), Bit/dim 1.1194(best: 1.1296), Xent 0.0433, Loss 1.1411, Error 0.0117(best: 0.0099)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3770 | Time 11.3242(11.1202) | Bit/dim 1.1497(1.1361) | Xent 0.0173(0.0140) | Loss 1.1584(1.1431) | Error 0.0056(0.0049) Steps 530(531.42) | Grad Norm 21.8311(6.2790) | Total Time 10.00(10.00)\n",
      "Iter 3780 | Time 11.1772(11.1539) | Bit/dim 1.1451(1.1410) | Xent 0.0102(0.0144) | Loss 1.1502(1.1482) | Error 0.0044(0.0049) Steps 536(531.92) | Grad Norm 4.8953(7.5816) | Total Time 10.00(10.00)\n",
      "Iter 3790 | Time 11.0094(11.1151) | Bit/dim 1.2412(1.1686) | Xent 0.0066(0.0141) | Loss 1.2445(1.1756) | Error 0.0022(0.0049) Steps 536(531.04) | Grad Norm 6.4637(9.5979) | Total Time 10.00(10.00)\n",
      "Iter 3800 | Time 11.4730(11.1411) | Bit/dim 1.1736(1.1756) | Xent 0.0093(0.0151) | Loss 1.1782(1.1831) | Error 0.0056(0.0053) Steps 542(532.12) | Grad Norm 3.0717(8.7382) | Total Time 10.00(10.00)\n",
      "Iter 3810 | Time 10.8732(11.1265) | Bit/dim 1.1665(1.1732) | Xent 0.0078(0.0154) | Loss 1.1704(1.1809) | Error 0.0033(0.0055) Steps 518(532.71) | Grad Norm 12.2521(9.6955) | Total Time 10.00(10.00)\n",
      "Iter 3820 | Time 10.8550(11.1285) | Bit/dim 1.1418(1.1662) | Xent 0.0090(0.0165) | Loss 1.1463(1.1745) | Error 0.0033(0.0055) Steps 530(532.77) | Grad Norm 4.4372(9.2180) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0058 | Time 49.4023, Epoch Time 799.4426(746.7981), Bit/dim 1.1268(best: 1.1194), Xent 0.0398, Loss 1.1467, Error 0.0107(best: 0.0099)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3830 | Time 10.9423(11.0926) | Bit/dim 1.1395(1.1581) | Xent 0.0125(0.0162) | Loss 1.1457(1.1662) | Error 0.0044(0.0052) Steps 530(532.56) | Grad Norm 1.5772(8.1161) | Total Time 10.00(10.00)\n",
      "Iter 3840 | Time 11.0878(11.0912) | Bit/dim 1.1350(1.1507) | Xent 0.0113(0.0147) | Loss 1.1407(1.1581) | Error 0.0067(0.0047) Steps 530(531.88) | Grad Norm 1.0708(6.3808) | Total Time 10.00(10.00)\n",
      "Iter 3850 | Time 11.2714(11.0997) | Bit/dim 1.1410(1.1439) | Xent 0.0046(0.0137) | Loss 1.1432(1.1507) | Error 0.0022(0.0045) Steps 536(531.91) | Grad Norm 1.3783(5.1568) | Total Time 10.00(10.00)\n",
      "Iter 3860 | Time 11.0275(11.1000) | Bit/dim 1.1255(1.1388) | Xent 0.0032(0.0128) | Loss 1.1271(1.1451) | Error 0.0011(0.0042) Steps 530(531.69) | Grad Norm 1.0823(4.2931) | Total Time 10.00(10.00)\n",
      "Iter 3870 | Time 11.1172(11.0929) | Bit/dim 1.1178(1.1333) | Xent 0.0126(0.0125) | Loss 1.1241(1.1395) | Error 0.0033(0.0041) Steps 536(531.56) | Grad Norm 1.2559(3.4852) | Total Time 10.00(10.00)\n",
      "Iter 3880 | Time 11.0349(11.0829) | Bit/dim 1.1405(1.1317) | Xent 0.0053(0.0124) | Loss 1.1431(1.1379) | Error 0.0011(0.0040) Steps 530(531.44) | Grad Norm 8.9318(4.2679) | Total Time 10.00(10.00)\n",
      "Iter 3890 | Time 11.1631(11.0795) | Bit/dim 1.1243(1.1308) | Xent 0.0203(0.0126) | Loss 1.1345(1.1371) | Error 0.0044(0.0038) Steps 530(531.04) | Grad Norm 5.2077(5.0163) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0059 | Time 48.6725, Epoch Time 794.4174(748.2266), Bit/dim 1.2319(best: 1.1194), Xent 0.0440, Loss 1.2539, Error 0.0115(best: 0.0099)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3900 | Time 10.9604(11.0616) | Bit/dim 1.2136(1.1577) | Xent 0.0334(0.0139) | Loss 1.2303(1.1646) | Error 0.0089(0.0044) Steps 530(530.85) | Grad Norm 29.3009(8.0597) | Total Time 10.00(10.00)\n",
      "Iter 3910 | Time 12.3320(11.0502) | Bit/dim 1.2293(1.1753) | Xent 0.0512(0.0142) | Loss 1.2549(1.1824) | Error 0.0178(0.0046) Steps 572(532.12) | Grad Norm 26.1252(9.8305) | Total Time 10.00(10.00)\n",
      "Iter 3920 | Time 11.2316(11.0617) | Bit/dim 1.1588(1.1752) | Xent 0.0182(0.0149) | Loss 1.1678(1.1827) | Error 0.0067(0.0048) Steps 530(531.83) | Grad Norm 9.0938(10.0421) | Total Time 10.00(10.00)\n",
      "Iter 3930 | Time 11.3892(11.1242) | Bit/dim 1.1203(1.1675) | Xent 0.0108(0.0149) | Loss 1.1257(1.1750) | Error 0.0033(0.0048) Steps 542(532.41) | Grad Norm 5.0830(9.3225) | Total Time 10.00(10.00)\n",
      "Iter 3940 | Time 10.9509(11.1233) | Bit/dim 1.1148(1.1576) | Xent 0.0139(0.0143) | Loss 1.1217(1.1647) | Error 0.0067(0.0047) Steps 530(532.84) | Grad Norm 2.4695(8.1444) | Total Time 10.00(10.00)\n",
      "Iter 3950 | Time 11.1415(11.1578) | Bit/dim 1.1129(1.1495) | Xent 0.0066(0.0141) | Loss 1.1162(1.1566) | Error 0.0022(0.0045) Steps 536(533.52) | Grad Norm 4.3917(6.9160) | Total Time 10.00(10.00)\n",
      "Iter 3960 | Time 11.0530(11.1373) | Bit/dim 1.1228(1.1442) | Xent 0.0166(0.0135) | Loss 1.1311(1.1509) | Error 0.0056(0.0044) Steps 530(533.64) | Grad Norm 4.3882(6.6176) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0060 | Time 48.2667, Epoch Time 798.3522(749.7304), Bit/dim 1.1353(best: 1.1194), Xent 0.0455, Loss 1.1580, Error 0.0124(best: 0.0099)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3970 | Time 11.1387(11.1052) | Bit/dim 1.1247(1.1404) | Xent 0.0107(0.0132) | Loss 1.1301(1.1470) | Error 0.0056(0.0044) Steps 530(533.22) | Grad Norm 8.1702(7.4976) | Total Time 10.00(10.00)\n",
      "Iter 3980 | Time 10.9647(11.0764) | Bit/dim 1.1767(1.1395) | Xent 0.0188(0.0125) | Loss 1.1861(1.1458) | Error 0.0067(0.0042) Steps 524(532.37) | Grad Norm 15.0454(8.1485) | Total Time 10.00(10.00)\n",
      "Iter 3990 | Time 11.0550(11.0982) | Bit/dim 1.1792(1.1655) | Xent 0.0178(0.0147) | Loss 1.1881(1.1728) | Error 0.0089(0.0052) Steps 536(533.80) | Grad Norm 8.2104(9.5238) | Total Time 10.00(10.00)\n",
      "Iter 4000 | Time 11.3689(11.0885) | Bit/dim 1.1813(1.1719) | Xent 0.0231(0.0150) | Loss 1.1929(1.1794) | Error 0.0089(0.0054) Steps 536(532.62) | Grad Norm 12.8102(9.8327) | Total Time 10.00(10.00)\n",
      "Iter 4010 | Time 11.1955(11.0565) | Bit/dim 1.2047(1.1738) | Xent 0.0302(0.0167) | Loss 1.2198(1.1821) | Error 0.0100(0.0057) Steps 536(532.04) | Grad Norm 26.5030(10.9945) | Total Time 10.00(10.00)\n",
      "Iter 4020 | Time 10.9703(11.0708) | Bit/dim 1.1490(1.1694) | Xent 0.0300(0.0169) | Loss 1.1640(1.1778) | Error 0.0156(0.0059) Steps 530(530.72) | Grad Norm 7.0617(10.6889) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0061 | Time 49.1483, Epoch Time 793.5611(751.0453), Bit/dim 1.1502(best: 1.1194), Xent 0.0477, Loss 1.1741, Error 0.0140(best: 0.0099)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 4030 | Time 11.1756(11.0733) | Bit/dim 1.1429(1.1644) | Xent 0.0164(0.0156) | Loss 1.1511(1.1722) | Error 0.0044(0.0054) Steps 536(530.96) | Grad Norm 1.3947(10.3838) | Total Time 10.00(10.00)\n",
      "Iter 4040 | Time 11.1392(11.0603) | Bit/dim 1.1479(1.1584) | Xent 0.0035(0.0148) | Loss 1.1496(1.1658) | Error 0.0000(0.0050) Steps 518(530.14) | Grad Norm 10.4755(10.6414) | Total Time 10.00(10.00)\n",
      "Iter 4050 | Time 10.9846(11.0418) | Bit/dim 1.1300(1.1562) | Xent 0.0178(0.0148) | Loss 1.1389(1.1636) | Error 0.0078(0.0049) Steps 530(529.51) | Grad Norm 7.3700(10.7201) | Total Time 10.00(10.00)\n",
      "Iter 4060 | Time 11.0179(11.0682) | Bit/dim 1.1174(1.1488) | Xent 0.0159(0.0131) | Loss 1.1253(1.1554) | Error 0.0056(0.0045) Steps 530(529.78) | Grad Norm 4.7779(9.5111) | Total Time 10.00(10.00)\n",
      "Iter 4070 | Time 10.8583(11.0596) | Bit/dim 1.1327(1.1429) | Xent 0.0165(0.0129) | Loss 1.1409(1.1494) | Error 0.0044(0.0044) Steps 536(530.83) | Grad Norm 1.5505(7.9997) | Total Time 10.00(10.00)\n",
      "Iter 4080 | Time 11.2917(11.1211) | Bit/dim 1.1236(1.1377) | Xent 0.0314(0.0133) | Loss 1.1393(1.1444) | Error 0.0111(0.0045) Steps 536(532.19) | Grad Norm 1.8084(6.6593) | Total Time 10.00(10.00)\n",
      "Iter 4090 | Time 11.3017(11.1318) | Bit/dim 1.1174(1.1337) | Xent 0.0086(0.0126) | Loss 1.1217(1.1400) | Error 0.0044(0.0043) Steps 530(532.50) | Grad Norm 2.0311(5.5691) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0062 | Time 49.2675, Epoch Time 797.5884(752.4416), Bit/dim 1.1135(best: 1.1194), Xent 0.0452, Loss 1.1361, Error 0.0107(best: 0.0099)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 4100 | Time 11.1372(11.1157) | Bit/dim 1.1115(1.1303) | Xent 0.0080(0.0121) | Loss 1.1156(1.1363) | Error 0.0033(0.0041) Steps 536(532.20) | Grad Norm 2.5858(4.7285) | Total Time 10.00(10.00)\n",
      "Iter 4110 | Time 11.2532(11.1056) | Bit/dim 1.1223(1.1271) | Xent 0.0057(0.0117) | Loss 1.1252(1.1330) | Error 0.0022(0.0042) Steps 536(532.92) | Grad Norm 1.0347(4.0613) | Total Time 10.00(10.00)\n",
      "Iter 4120 | Time 11.1040(11.1042) | Bit/dim 1.1272(1.1241) | Xent 0.0092(0.0115) | Loss 1.1318(1.1298) | Error 0.0044(0.0041) Steps 530(532.15) | Grad Norm 1.9153(3.3785) | Total Time 10.00(10.00)\n",
      "Iter 4130 | Time 11.1916(11.0786) | Bit/dim 1.1187(1.1215) | Xent 0.0057(0.0105) | Loss 1.1216(1.1267) | Error 0.0022(0.0037) Steps 536(532.07) | Grad Norm 2.4505(2.9312) | Total Time 10.00(10.00)\n",
      "Iter 4140 | Time 11.0995(11.0859) | Bit/dim 1.1170(1.1196) | Xent 0.0110(0.0106) | Loss 1.1225(1.1249) | Error 0.0067(0.0040) Steps 530(532.45) | Grad Norm 5.0323(2.9512) | Total Time 10.00(10.00)\n",
      "Iter 4150 | Time 11.7108(11.1137) | Bit/dim 1.1087(1.1175) | Xent 0.0102(0.0104) | Loss 1.1137(1.1227) | Error 0.0022(0.0037) Steps 542(533.75) | Grad Norm 5.5062(3.4611) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0063 | Time 49.1831, Epoch Time 796.6031(753.7665), Bit/dim 1.1077(best: 1.1135), Xent 0.0427, Loss 1.1290, Error 0.0108(best: 0.0099)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 4160 | Time 11.3881(11.1393) | Bit/dim 1.1173(1.1173) | Xent 0.0090(0.0116) | Loss 1.1218(1.1230) | Error 0.0022(0.0040) Steps 536(534.03) | Grad Norm 4.4729(3.6550) | Total Time 10.00(10.00)\n",
      "Iter 4170 | Time 10.8506(11.1704) | Bit/dim 1.2027(1.1217) | Xent 0.0114(0.0116) | Loss 1.2084(1.1275) | Error 0.0056(0.0041) Steps 530(534.63) | Grad Norm 17.9585(5.6154) | Total Time 10.00(10.00)\n",
      "Iter 4180 | Time 10.4782(11.1524) | Bit/dim 1.1275(1.1255) | Xent 0.0128(0.0118) | Loss 1.1339(1.1315) | Error 0.0067(0.0043) Steps 512(533.92) | Grad Norm 9.9625(6.8147) | Total Time 10.00(10.00)\n",
      "Iter 4190 | Time 11.2733(11.1541) | Bit/dim 1.1680(1.1454) | Xent 0.0110(0.0120) | Loss 1.1735(1.1514) | Error 0.0033(0.0043) Steps 542(533.60) | Grad Norm 7.0067(8.9836) | Total Time 10.00(10.00)\n",
      "Iter 4200 | Time 11.3128(11.1028) | Bit/dim 1.2926(1.1936) | Xent 0.0228(0.0136) | Loss 1.3040(1.2004) | Error 0.0089(0.0045) Steps 536(533.25) | Grad Norm 5.6586(9.4771) | Total Time 10.00(10.00)\n",
      "Iter 4210 | Time 11.4574(11.1199) | Bit/dim 1.1593(1.1948) | Xent 0.0121(0.0136) | Loss 1.1653(1.2016) | Error 0.0044(0.0047) Steps 536(532.43) | Grad Norm 3.3806(8.1046) | Total Time 10.00(10.00)\n",
      "Iter 4220 | Time 11.5865(11.1182) | Bit/dim 1.1601(1.1840) | Xent 0.0057(0.0137) | Loss 1.1630(1.1908) | Error 0.0011(0.0048) Steps 530(532.46) | Grad Norm 9.2187(7.5017) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0064 | Time 48.3935, Epoch Time 797.8835(755.0900), Bit/dim 1.1276(best: 1.1077), Xent 0.0454, Loss 1.1503, Error 0.0108(best: 0.0099)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 4230 | Time 10.9020(11.0937) | Bit/dim 1.1226(1.1701) | Xent 0.0131(0.0141) | Loss 1.1292(1.1772) | Error 0.0056(0.0048) Steps 524(530.84) | Grad Norm 2.9114(7.5129) | Total Time 10.00(10.00)\n",
      "Iter 4240 | Time 11.3949(11.0928) | Bit/dim 1.1250(1.1574) | Xent 0.0073(0.0133) | Loss 1.1286(1.1640) | Error 0.0022(0.0046) Steps 530(530.33) | Grad Norm 4.5397(6.8269) | Total Time 10.00(10.00)\n",
      "Iter 4250 | Time 10.4851(11.0523) | Bit/dim 1.1636(1.1494) | Xent 0.0076(0.0125) | Loss 1.1674(1.1557) | Error 0.0022(0.0043) Steps 512(529.66) | Grad Norm 12.0900(7.5574) | Total Time 10.00(10.00)\n",
      "Iter 4260 | Time 10.8322(11.0148) | Bit/dim 1.1181(1.1440) | Xent 0.0074(0.0120) | Loss 1.1218(1.1500) | Error 0.0022(0.0041) Steps 524(529.24) | Grad Norm 7.0609(7.8821) | Total Time 10.00(10.00)\n",
      "Iter 4270 | Time 11.1633(11.0423) | Bit/dim 1.1273(1.1383) | Xent 0.0165(0.0113) | Loss 1.1356(1.1439) | Error 0.0044(0.0038) Steps 536(530.85) | Grad Norm 6.6723(6.9261) | Total Time 10.00(10.00)\n",
      "Iter 4280 | Time 11.5011(11.0549) | Bit/dim 1.1275(1.1364) | Xent 0.0057(0.0109) | Loss 1.1304(1.1419) | Error 0.0022(0.0035) Steps 530(530.57) | Grad Norm 10.6394(7.8479) | Total Time 10.00(10.00)\n",
      "Iter 4290 | Time 11.1969(11.1082) | Bit/dim 1.1232(1.1321) | Xent 0.0114(0.0106) | Loss 1.1289(1.1374) | Error 0.0022(0.0034) Steps 542(532.52) | Grad Norm 4.4334(7.4362) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0065 | Time 50.3334, Epoch Time 795.7436(756.3096), Bit/dim 1.1113(best: 1.1077), Xent 0.0426, Loss 1.1326, Error 0.0099(best: 0.0099)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 4300 | Time 11.0361(11.1384) | Bit/dim 1.1117(1.1262) | Xent 0.0047(0.0099) | Loss 1.1140(1.1311) | Error 0.0011(0.0032) Steps 542(534.71) | Grad Norm 2.1462(6.2419) | Total Time 10.00(10.00)\n",
      "Iter 4310 | Time 11.5078(11.1695) | Bit/dim 1.1103(1.1221) | Xent 0.0125(0.0094) | Loss 1.1165(1.1268) | Error 0.0078(0.0032) Steps 536(535.18) | Grad Norm 3.9632(5.6849) | Total Time 10.00(10.00)\n",
      "Iter 4320 | Time 11.3653(11.1532) | Bit/dim 1.1233(1.1194) | Xent 0.0032(0.0090) | Loss 1.1249(1.1239) | Error 0.0011(0.0031) Steps 536(535.40) | Grad Norm 3.8838(5.1614) | Total Time 10.00(10.00)\n",
      "Iter 4330 | Time 11.1607(11.1506) | Bit/dim 1.1135(1.1164) | Xent 0.0122(0.0085) | Loss 1.1196(1.1207) | Error 0.0044(0.0029) Steps 536(535.56) | Grad Norm 3.7496(4.5991) | Total Time 10.00(10.00)\n",
      "Iter 4340 | Time 11.0994(11.1678) | Bit/dim 1.1130(1.1166) | Xent 0.0068(0.0089) | Loss 1.1164(1.1210) | Error 0.0022(0.0030) Steps 536(535.33) | Grad Norm 3.7605(4.3827) | Total Time 10.00(10.00)\n",
      "Iter 4350 | Time 11.2624(11.1705) | Bit/dim 1.1290(1.1161) | Xent 0.0222(0.0097) | Loss 1.1401(1.1209) | Error 0.0089(0.0034) Steps 530(535.33) | Grad Norm 7.9045(5.3260) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0066 | Time 47.5574, Epoch Time 800.1792(757.6257), Bit/dim 1.2131(best: 1.1077), Xent 0.0373, Loss 1.2318, Error 0.0101(best: 0.0099)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 4360 | Time 10.1210(11.1264) | Bit/dim 1.1941(1.1306) | Xent 0.0067(0.0108) | Loss 1.1975(1.1360) | Error 0.0022(0.0039) Steps 518(534.39) | Grad Norm 15.2021(7.7933) | Total Time 10.00(10.00)\n",
      "Iter 4370 | Time 10.5163(11.0578) | Bit/dim 1.3911(1.1970) | Xent 0.0041(0.0118) | Loss 1.3931(1.2029) | Error 0.0011(0.0042) Steps 518(532.61) | Grad Norm 3.4836(9.2286) | Total Time 10.00(10.00)\n",
      "Iter 4380 | Time 11.2327(11.0385) | Bit/dim 1.3101(1.2370) | Xent 0.0128(0.0123) | Loss 1.3164(1.2431) | Error 0.0044(0.0042) Steps 536(531.59) | Grad Norm 1.7793(7.6809) | Total Time 10.00(10.00)\n",
      "Iter 4390 | Time 10.7004(11.1506) | Bit/dim 1.1954(1.2369) | Xent 0.0122(0.0130) | Loss 1.2015(1.2434) | Error 0.0044(0.0046) Steps 518(532.97) | Grad Norm 1.5828(6.1881) | Total Time 10.00(10.00)\n",
      "Iter 4400 | Time 10.5648(11.0796) | Bit/dim 1.1404(1.2170) | Xent 0.0145(0.0128) | Loss 1.1476(1.2234) | Error 0.0078(0.0044) Steps 524(530.67) | Grad Norm 1.9449(5.0093) | Total Time 10.00(10.00)\n",
      "Iter 4410 | Time 11.0230(11.1016) | Bit/dim 1.1310(1.1950) | Xent 0.0038(0.0119) | Loss 1.1329(1.2009) | Error 0.0011(0.0042) Steps 542(533.02) | Grad Norm 1.3930(4.4223) | Total Time 10.00(10.00)\n",
      "Iter 4420 | Time 11.0578(11.1158) | Bit/dim 1.1223(1.1760) | Xent 0.0115(0.0118) | Loss 1.1281(1.1819) | Error 0.0056(0.0042) Steps 530(532.91) | Grad Norm 1.2500(3.9563) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0067 | Time 49.7396, Epoch Time 796.1659(758.7819), Bit/dim 1.1130(best: 1.1077), Xent 0.0422, Loss 1.1341, Error 0.0115(best: 0.0099)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 4430 | Time 10.9597(11.1067) | Bit/dim 1.0992(1.1591) | Xent 0.0066(0.0107) | Loss 1.1025(1.1645) | Error 0.0022(0.0038) Steps 530(532.30) | Grad Norm 1.2950(3.6463) | Total Time 10.00(10.00)\n",
      "Iter 4440 | Time 11.2523(11.1329) | Bit/dim 1.1202(1.1458) | Xent 0.0073(0.0102) | Loss 1.1238(1.1509) | Error 0.0011(0.0037) Steps 530(532.81) | Grad Norm 6.0406(3.3453) | Total Time 10.00(10.00)\n",
      "Iter 4450 | Time 10.6619(11.1186) | Bit/dim 1.3857(1.1655) | Xent 0.0019(0.0102) | Loss 1.3866(1.1705) | Error 0.0000(0.0037) Steps 524(532.22) | Grad Norm 8.1973(6.5223) | Total Time 10.00(10.00)\n",
      "Iter 4460 | Time 10.7541(11.1125) | Bit/dim 1.2012(1.1883) | Xent 0.0113(0.0118) | Loss 1.2069(1.1942) | Error 0.0022(0.0039) Steps 518(532.83) | Grad Norm 3.8587(6.0083) | Total Time 10.00(10.00)\n",
      "Iter 4470 | Time 11.0919(11.0910) | Bit/dim 1.1465(1.1798) | Xent 0.0252(0.0132) | Loss 1.1591(1.1864) | Error 0.0067(0.0043) Steps 536(534.04) | Grad Norm 3.5515(5.1640) | Total Time 10.00(10.00)\n",
      "Iter 4480 | Time 11.2125(11.0803) | Bit/dim 1.1359(1.1680) | Xent 0.0049(0.0133) | Loss 1.1383(1.1746) | Error 0.0000(0.0045) Steps 530(533.45) | Grad Norm 3.3710(4.8100) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0068 | Time 47.9187, Epoch Time 794.3537(759.8490), Bit/dim 1.1119(best: 1.1077), Xent 0.0415, Loss 1.1326, Error 0.0106(best: 0.0099)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 4490 | Time 10.9091(11.0671) | Bit/dim 1.1150(1.1560) | Xent 0.0076(0.0124) | Loss 1.1188(1.1622) | Error 0.0033(0.0043) Steps 536(533.58) | Grad Norm 1.3513(4.4158) | Total Time 10.00(10.00)\n",
      "Iter 4500 | Time 10.8096(11.0663) | Bit/dim 1.1250(1.1447) | Xent 0.0079(0.0118) | Loss 1.1289(1.1506) | Error 0.0033(0.0041) Steps 530(534.62) | Grad Norm 2.9309(3.9304) | Total Time 10.00(10.00)\n",
      "Iter 4510 | Time 10.9815(11.0701) | Bit/dim 1.1283(1.1355) | Xent 0.0154(0.0110) | Loss 1.1360(1.1410) | Error 0.0044(0.0039) Steps 530(534.95) | Grad Norm 5.8209(3.9121) | Total Time 10.00(10.00)\n",
      "Iter 4520 | Time 10.9641(11.0568) | Bit/dim 1.1219(1.1295) | Xent 0.0055(0.0110) | Loss 1.1247(1.1350) | Error 0.0022(0.0040) Steps 518(534.40) | Grad Norm 10.2031(4.4050) | Total Time 10.00(10.00)\n",
      "Iter 4530 | Time 10.8242(11.0638) | Bit/dim 1.1398(1.1310) | Xent 0.0131(0.0111) | Loss 1.1463(1.1366) | Error 0.0022(0.0039) Steps 524(533.62) | Grad Norm 12.0366(6.0424) | Total Time 10.00(10.00)\n",
      "Iter 4540 | Time 10.9196(11.0331) | Bit/dim 1.2883(1.1846) | Xent 0.0130(0.0115) | Loss 1.2948(1.1903) | Error 0.0044(0.0040) Steps 542(533.56) | Grad Norm 3.9798(7.4527) | Total Time 10.00(10.00)\n",
      "Iter 4550 | Time 11.1472(11.0686) | Bit/dim 1.1739(1.1990) | Xent 0.0159(0.0138) | Loss 1.1819(1.2059) | Error 0.0056(0.0046) Steps 536(533.76) | Grad Norm 2.3447(6.4128) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0069 | Time 49.8520, Epoch Time 795.3401(760.9138), Bit/dim 1.1401(best: 1.1077), Xent 0.0576, Loss 1.1689, Error 0.0159(best: 0.0099)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 4560 | Time 11.2537(11.0843) | Bit/dim 1.1212(1.1848) | Xent 0.0132(0.0137) | Loss 1.1278(1.1916) | Error 0.0044(0.0045) Steps 530(534.09) | Grad Norm 1.9583(5.3539) | Total Time 10.00(10.00)\n",
      "Iter 4570 | Time 11.1550(11.0556) | Bit/dim 1.1229(1.1689) | Xent 0.0054(0.0133) | Loss 1.1255(1.1755) | Error 0.0011(0.0043) Steps 530(532.74) | Grad Norm 1.5535(4.3921) | Total Time 10.00(10.00)\n",
      "Iter 4580 | Time 11.2417(11.0507) | Bit/dim 1.0951(1.1550) | Xent 0.0129(0.0129) | Loss 1.1015(1.1615) | Error 0.0033(0.0043) Steps 536(532.53) | Grad Norm 1.5791(3.6976) | Total Time 10.00(10.00)\n",
      "Iter 4590 | Time 11.1267(11.0531) | Bit/dim 1.1167(1.1438) | Xent 0.0058(0.0118) | Loss 1.1196(1.1497) | Error 0.0022(0.0041) Steps 536(532.54) | Grad Norm 1.8221(3.1854) | Total Time 10.00(10.00)\n",
      "Iter 4600 | Time 10.5049(11.0528) | Bit/dim 1.2206(1.1496) | Xent 0.0043(0.0111) | Loss 1.2228(1.1552) | Error 0.0022(0.0038) Steps 518(532.25) | Grad Norm 9.3069(5.2356) | Total Time 10.00(10.00)\n",
      "Iter 4610 | Time 11.2705(11.0954) | Bit/dim 1.1475(1.1546) | Xent 0.0105(0.0120) | Loss 1.1527(1.1606) | Error 0.0033(0.0041) Steps 530(533.46) | Grad Norm 6.0512(6.3305) | Total Time 10.00(10.00)\n",
      "Iter 4620 | Time 11.2513(11.0691) | Bit/dim 1.1661(1.1508) | Xent 0.0081(0.0116) | Loss 1.1701(1.1567) | Error 0.0022(0.0039) Steps 542(533.38) | Grad Norm 34.9508(7.8598) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0070 | Time 47.8069, Epoch Time 792.8390(761.8715), Bit/dim 1.2226(best: 1.1077), Xent 0.0447, Loss 1.2449, Error 0.0097(best: 0.0099)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 4630 | Time 10.6795(11.0587) | Bit/dim 1.1931(1.1627) | Xent 0.0073(0.0112) | Loss 1.1968(1.1683) | Error 0.0033(0.0039) Steps 524(533.03) | Grad Norm 9.8106(9.0776) | Total Time 10.00(10.00)\n",
      "Iter 4640 | Time 10.7079(11.0168) | Bit/dim 1.2487(1.1690) | Xent 0.0196(0.0126) | Loss 1.2585(1.1753) | Error 0.0033(0.0042) Steps 530(532.62) | Grad Norm 9.4587(9.8767) | Total Time 10.00(10.00)\n",
      "Iter 4650 | Time 10.9341(11.0189) | Bit/dim 1.1963(1.1724) | Xent 0.0044(0.0128) | Loss 1.1985(1.1788) | Error 0.0011(0.0043) Steps 518(531.91) | Grad Norm 8.5587(9.8962) | Total Time 10.00(10.00)\n",
      "Iter 4660 | Time 11.1169(10.9792) | Bit/dim 1.1355(1.1669) | Xent 0.0101(0.0118) | Loss 1.1406(1.1728) | Error 0.0033(0.0039) Steps 536(530.57) | Grad Norm 4.8627(9.5485) | Total Time 10.00(10.00)\n",
      "Iter 4670 | Time 10.9285(10.9746) | Bit/dim 1.1465(1.1601) | Xent 0.0015(0.0104) | Loss 1.1473(1.1652) | Error 0.0000(0.0035) Steps 530(530.29) | Grad Norm 7.6945(9.6780) | Total Time 10.00(10.00)\n",
      "Iter 4680 | Time 10.8495(10.9460) | Bit/dim 1.1272(1.1537) | Xent 0.0018(0.0099) | Loss 1.1281(1.1587) | Error 0.0000(0.0032) Steps 530(528.78) | Grad Norm 5.1362(9.7361) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0071 | Time 46.1459, Epoch Time 782.4657(762.4893), Bit/dim 1.1151(best: 1.1077), Xent 0.0417, Loss 1.1360, Error 0.0116(best: 0.0097)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 4690 | Time 10.8373(10.9086) | Bit/dim 1.1130(1.1456) | Xent 0.0061(0.0096) | Loss 1.1161(1.1504) | Error 0.0011(0.0032) Steps 518(527.36) | Grad Norm 4.9845(8.9194) | Total Time 10.00(10.00)\n",
      "Iter 4700 | Time 10.9480(10.9168) | Bit/dim 1.1135(1.1379) | Xent 0.0039(0.0085) | Loss 1.1155(1.1421) | Error 0.0011(0.0027) Steps 524(527.28) | Grad Norm 1.5814(8.1742) | Total Time 10.00(10.00)\n",
      "Iter 4710 | Time 10.7832(10.9488) | Bit/dim 1.1004(1.1305) | Xent 0.0044(0.0086) | Loss 1.1026(1.1347) | Error 0.0011(0.0026) Steps 530(528.99) | Grad Norm 3.7661(6.9563) | Total Time 10.00(10.00)\n",
      "Iter 4720 | Time 10.9757(11.0012) | Bit/dim 1.1032(1.1239) | Xent 0.0080(0.0087) | Loss 1.1072(1.1282) | Error 0.0056(0.0027) Steps 536(530.83) | Grad Norm 1.2025(5.8630) | Total Time 10.00(10.00)\n",
      "Iter 4730 | Time 11.3729(11.0548) | Bit/dim 1.1030(1.1185) | Xent 0.0012(0.0083) | Loss 1.1036(1.1227) | Error 0.0000(0.0025) Steps 530(531.69) | Grad Norm 0.9723(4.8312) | Total Time 10.00(10.00)\n",
      "Iter 4740 | Time 11.1320(11.0710) | Bit/dim 1.1125(1.1151) | Xent 0.0151(0.0081) | Loss 1.1200(1.1191) | Error 0.0056(0.0027) Steps 542(533.63) | Grad Norm 1.4559(3.9631) | Total Time 10.00(10.00)\n",
      "Iter 4750 | Time 11.1085(11.1017) | Bit/dim 1.1098(1.1112) | Xent 0.0104(0.0079) | Loss 1.1151(1.1152) | Error 0.0056(0.0029) Steps 524(534.95) | Grad Norm 5.4158(3.8255) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0072 | Time 48.2261, Epoch Time 795.2939(763.4735), Bit/dim 1.1024(best: 1.1077), Xent 0.0432, Loss 1.1240, Error 0.0109(best: 0.0097)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 4760 | Time 11.6827(11.1260) | Bit/dim 1.0922(1.1098) | Xent 0.0068(0.0082) | Loss 1.0956(1.1139) | Error 0.0022(0.0031) Steps 542(536.20) | Grad Norm 3.4574(3.9037) | Total Time 10.00(10.00)\n",
      "Iter 4770 | Time 11.1615(11.1444) | Bit/dim 1.1102(1.1085) | Xent 0.0074(0.0079) | Loss 1.1140(1.1125) | Error 0.0033(0.0031) Steps 536(535.09) | Grad Norm 1.9065(4.3423) | Total Time 10.00(10.00)\n",
      "Iter 4780 | Time 11.0446(11.1636) | Bit/dim 1.1092(1.1060) | Xent 0.0043(0.0076) | Loss 1.1114(1.1098) | Error 0.0033(0.0029) Steps 542(536.46) | Grad Norm 3.7083(4.1806) | Total Time 10.00(10.00)\n",
      "Iter 4790 | Time 10.8244(11.1187) | Bit/dim 1.0898(1.1050) | Xent 0.0044(0.0078) | Loss 1.0920(1.1089) | Error 0.0011(0.0029) Steps 524(536.21) | Grad Norm 8.4104(4.6018) | Total Time 10.00(10.00)\n",
      "Iter 4800 | Time 11.2249(11.1060) | Bit/dim 1.4541(1.1374) | Xent 0.0003(0.0084) | Loss 1.4542(1.1416) | Error 0.0000(0.0030) Steps 542(535.88) | Grad Norm 6.8486(7.4190) | Total Time 10.00(10.00)\n",
      "Iter 4810 | Time 11.1848(11.0771) | Bit/dim 1.2696(1.1854) | Xent 0.0129(0.0092) | Loss 1.2761(1.1900) | Error 0.0033(0.0034) Steps 536(535.39) | Grad Norm 2.9205(6.6216) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0073 | Time 49.3864, Epoch Time 797.5007(764.4943), Bit/dim 1.1454(best: 1.1024), Xent 0.0537, Loss 1.1723, Error 0.0133(best: 0.0097)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 4820 | Time 11.6050(11.1070) | Bit/dim 1.1543(1.1857) | Xent 0.0145(0.0104) | Loss 1.1616(1.1909) | Error 0.0022(0.0036) Steps 542(536.49) | Grad Norm 2.2532(5.5723) | Total Time 10.00(10.00)\n",
      "Iter 4830 | Time 10.9140(11.0497) | Bit/dim 1.1482(1.1726) | Xent 0.0105(0.0106) | Loss 1.1534(1.1779) | Error 0.0033(0.0036) Steps 542(534.66) | Grad Norm 3.4928(4.9668) | Total Time 10.00(10.00)\n",
      "Iter 4840 | Time 10.9764(11.0232) | Bit/dim 1.1047(1.1570) | Xent 0.0057(0.0101) | Loss 1.1075(1.1621) | Error 0.0022(0.0035) Steps 530(533.21) | Grad Norm 1.2302(4.3538) | Total Time 10.00(10.00)\n",
      "Iter 4850 | Time 11.2880(11.0698) | Bit/dim 1.1173(1.1438) | Xent 0.0043(0.0096) | Loss 1.1194(1.1486) | Error 0.0022(0.0035) Steps 524(533.61) | Grad Norm 6.8887(4.1032) | Total Time 10.00(10.00)\n",
      "Iter 4860 | Time 10.7001(11.0669) | Bit/dim 1.1180(1.1354) | Xent 0.0075(0.0095) | Loss 1.1218(1.1402) | Error 0.0044(0.0036) Steps 524(533.54) | Grad Norm 8.2928(4.2275) | Total Time 10.00(10.00)\n",
      "Iter 4870 | Time 11.5134(11.0853) | Bit/dim 1.2880(1.1598) | Xent 0.0167(0.0099) | Loss 1.2963(1.1647) | Error 0.0067(0.0034) Steps 560(533.97) | Grad Norm 6.6340(6.2230) | Total Time 10.00(10.00)\n",
      "Iter 4880 | Time 10.7139(11.0635) | Bit/dim 1.1663(1.1692) | Xent 0.0150(0.0118) | Loss 1.1738(1.1751) | Error 0.0033(0.0040) Steps 524(533.31) | Grad Norm 10.5904(6.5726) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0074 | Time 50.0644, Epoch Time 793.9631(765.3784), Bit/dim 1.1421(best: 1.1024), Xent 0.0523, Loss 1.1683, Error 0.0125(best: 0.0097)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 4890 | Time 10.5854(11.0340) | Bit/dim 1.1128(1.1594) | Xent 0.0098(0.0112) | Loss 1.1177(1.1650) | Error 0.0022(0.0039) Steps 524(533.55) | Grad Norm 2.4128(6.1993) | Total Time 10.00(10.00)\n",
      "Iter 4900 | Time 10.9821(10.9956) | Bit/dim 1.1140(1.1481) | Xent 0.0012(0.0105) | Loss 1.1146(1.1533) | Error 0.0000(0.0036) Steps 530(532.05) | Grad Norm 3.0320(5.3917) | Total Time 10.00(10.00)\n",
      "Iter 4910 | Time 10.9338(11.0025) | Bit/dim 1.1078(1.1385) | Xent 0.0149(0.0108) | Loss 1.1152(1.1439) | Error 0.0044(0.0037) Steps 536(531.13) | Grad Norm 0.9448(4.6887) | Total Time 10.00(10.00)\n",
      "Iter 4920 | Time 11.1993(11.0451) | Bit/dim 1.1084(1.1294) | Xent 0.0047(0.0093) | Loss 1.1107(1.1340) | Error 0.0011(0.0032) Steps 542(532.46) | Grad Norm 2.0886(3.8402) | Total Time 10.00(10.00)\n",
      "Iter 4930 | Time 11.1415(11.0582) | Bit/dim 1.1062(1.1218) | Xent 0.0090(0.0087) | Loss 1.1107(1.1262) | Error 0.0044(0.0030) Steps 536(532.75) | Grad Norm 5.2495(3.9220) | Total Time 10.00(10.00)\n",
      "Iter 4940 | Time 11.3489(11.1400) | Bit/dim 1.5074(1.1638) | Xent 0.0044(0.0110) | Loss 1.5096(1.1693) | Error 0.0022(0.0038) Steps 560(535.14) | Grad Norm 6.3563(6.7876) | Total Time 10.00(10.00)\n",
      "Iter 4950 | Time 11.1489(11.0863) | Bit/dim 1.3210(1.2179) | Xent 0.0318(0.0126) | Loss 1.3369(1.2243) | Error 0.0078(0.0043) Steps 542(534.12) | Grad Norm 3.4783(5.9869) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0075 | Time 48.4207, Epoch Time 793.8301(766.2319), Bit/dim 1.2935(best: 1.1024), Xent 0.0511, Loss 1.3190, Error 0.0119(best: 0.0097)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 4960 | Time 11.3573(11.1291) | Bit/dim 1.1972(1.2241) | Xent 0.0038(0.0135) | Loss 1.1991(1.2308) | Error 0.0011(0.0046) Steps 554(536.13) | Grad Norm 3.3288(5.2320) | Total Time 10.00(10.00)\n",
      "Iter 4970 | Time 10.7152(11.1280) | Bit/dim 1.1445(1.2071) | Xent 0.0052(0.0135) | Loss 1.1471(1.2138) | Error 0.0033(0.0047) Steps 524(535.12) | Grad Norm 3.0378(4.5975) | Total Time 10.00(10.00)\n",
      "Iter 4980 | Time 11.3219(11.0901) | Bit/dim 1.1469(1.1884) | Xent 0.0094(0.0122) | Loss 1.1516(1.1945) | Error 0.0044(0.0043) Steps 536(535.20) | Grad Norm 10.5842(5.1044) | Total Time 10.00(10.00)\n",
      "Iter 4990 | Time 10.9445(11.0501) | Bit/dim 1.1175(1.1727) | Xent 0.0047(0.0116) | Loss 1.1198(1.1785) | Error 0.0022(0.0040) Steps 530(533.01) | Grad Norm 2.0810(6.3881) | Total Time 10.00(10.00)\n",
      "Iter 5000 | Time 11.7590(10.9909) | Bit/dim 1.1827(1.1629) | Xent 0.0229(0.0107) | Loss 1.1941(1.1682) | Error 0.0056(0.0038) Steps 548(531.38) | Grad Norm 36.3273(8.0472) | Total Time 10.00(10.00)\n",
      "Iter 5010 | Time 11.3507(11.0198) | Bit/dim 1.1964(1.1690) | Xent 0.0066(0.0104) | Loss 1.1997(1.1743) | Error 0.0044(0.0036) Steps 548(531.81) | Grad Norm 39.4614(9.4977) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0076 | Time 50.0966, Epoch Time 790.9959(766.9748), Bit/dim 1.1667(best: 1.1024), Xent 0.0457, Loss 1.1895, Error 0.0120(best: 0.0097)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 5020 | Time 10.5123(10.9743) | Bit/dim 1.1970(1.1819) | Xent 0.0047(0.0101) | Loss 1.1993(1.1870) | Error 0.0011(0.0034) Steps 530(532.24) | Grad Norm 6.9787(8.8299) | Total Time 10.00(10.00)\n",
      "Iter 5030 | Time 11.2604(10.9517) | Bit/dim 1.1291(1.1713) | Xent 0.0108(0.0095) | Loss 1.1346(1.1761) | Error 0.0022(0.0031) Steps 542(530.97) | Grad Norm 5.6884(7.9599) | Total Time 10.00(10.00)\n",
      "Iter 5040 | Time 11.0313(10.9407) | Bit/dim 1.1145(1.1581) | Xent 0.0115(0.0094) | Loss 1.1202(1.1628) | Error 0.0033(0.0031) Steps 518(530.51) | Grad Norm 1.6741(7.0550) | Total Time 10.00(10.00)\n",
      "Iter 5050 | Time 10.8365(10.9509) | Bit/dim 1.1053(1.1434) | Xent 0.0079(0.0084) | Loss 1.1093(1.1475) | Error 0.0033(0.0028) Steps 524(529.43) | Grad Norm 1.5357(5.6089) | Total Time 10.00(10.00)\n",
      "Iter 5060 | Time 11.0452(10.9688) | Bit/dim 1.0937(1.1329) | Xent 0.0056(0.0077) | Loss 1.0965(1.1368) | Error 0.0022(0.0027) Steps 524(528.39) | Grad Norm 1.5709(4.5403) | Total Time 10.00(10.00)\n",
      "Iter 5070 | Time 11.2770(10.9969) | Bit/dim 1.1157(1.1250) | Xent 0.0098(0.0072) | Loss 1.1206(1.1286) | Error 0.0022(0.0025) Steps 536(529.02) | Grad Norm 0.9740(3.6512) | Total Time 10.00(10.00)\n",
      "Iter 5080 | Time 11.1518(11.0285) | Bit/dim 1.1011(1.1177) | Xent 0.0142(0.0079) | Loss 1.1082(1.1217) | Error 0.0044(0.0027) Steps 518(530.00) | Grad Norm 6.3152(3.6623) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0077 | Time 48.9704, Epoch Time 790.4504(767.6791), Bit/dim 1.0950(best: 1.1024), Xent 0.0467, Loss 1.1183, Error 0.0107(best: 0.0097)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 5090 | Time 10.7053(11.0117) | Bit/dim 1.0982(1.1119) | Xent 0.0116(0.0079) | Loss 1.1040(1.1158) | Error 0.0044(0.0028) Steps 512(528.96) | Grad Norm 6.9436(3.5764) | Total Time 10.00(10.00)\n",
      "Iter 5100 | Time 10.7672(10.9977) | Bit/dim 1.1009(1.1088) | Xent 0.0016(0.0077) | Loss 1.1017(1.1127) | Error 0.0000(0.0027) Steps 506(528.04) | Grad Norm 9.3251(3.9503) | Total Time 10.00(10.00)\n",
      "Iter 5110 | Time 11.0008(10.9732) | Bit/dim 1.1111(1.1076) | Xent 0.0064(0.0078) | Loss 1.1143(1.1115) | Error 0.0022(0.0027) Steps 524(527.46) | Grad Norm 12.4149(4.7716) | Total Time 10.00(10.00)\n",
      "Iter 5120 | Time 10.7969(10.9706) | Bit/dim 1.2853(1.1584) | Xent 0.0111(0.0085) | Loss 1.2908(1.1627) | Error 0.0056(0.0029) Steps 530(527.67) | Grad Norm 3.5097(6.6113) | Total Time 10.00(10.00)\n",
      "Iter 5130 | Time 11.2505(11.0118) | Bit/dim 1.1867(1.1729) | Xent 0.0264(0.0111) | Loss 1.1999(1.1784) | Error 0.0067(0.0038) Steps 542(529.69) | Grad Norm 2.8903(5.7633) | Total Time 10.00(10.00)\n",
      "Iter 5140 | Time 10.6051(11.0827) | Bit/dim 1.1375(1.1644) | Xent 0.0254(0.0120) | Loss 1.1501(1.1704) | Error 0.0078(0.0043) Steps 524(531.66) | Grad Norm 4.2169(5.3310) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0078 | Time 48.8573, Epoch Time 791.0634(768.3806), Bit/dim 1.1256(best: 1.0950), Xent 0.0480, Loss 1.1496, Error 0.0099(best: 0.0097)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 5150 | Time 11.3035(11.0302) | Bit/dim 1.1335(1.1562) | Xent 0.0062(0.0115) | Loss 1.1366(1.1620) | Error 0.0033(0.0042) Steps 518(529.69) | Grad Norm 10.2708(6.5520) | Total Time 10.00(10.00)\n",
      "Iter 5160 | Time 10.2501(10.9883) | Bit/dim 1.1650(1.1604) | Xent 0.0016(0.0114) | Loss 1.1658(1.1661) | Error 0.0000(0.0040) Steps 512(528.87) | Grad Norm 6.7915(8.0628) | Total Time 10.00(10.00)\n",
      "Iter 5170 | Time 10.4712(10.9589) | Bit/dim 1.1413(1.1600) | Xent 0.0071(0.0105) | Loss 1.1449(1.1653) | Error 0.0022(0.0037) Steps 518(528.24) | Grad Norm 6.3641(8.3024) | Total Time 10.00(10.00)\n",
      "Iter 5180 | Time 10.9520(10.9356) | Bit/dim 1.1359(1.1522) | Xent 0.0228(0.0106) | Loss 1.1473(1.1575) | Error 0.0089(0.0036) Steps 536(528.39) | Grad Norm 14.6888(8.1018) | Total Time 10.00(10.00)\n",
      "Iter 5190 | Time 10.3668(10.8635) | Bit/dim 1.1314(1.1452) | Xent 0.0093(0.0100) | Loss 1.1360(1.1502) | Error 0.0056(0.0034) Steps 518(527.74) | Grad Norm 10.3879(8.5060) | Total Time 10.00(10.00)\n",
      "Iter 5200 | Time 10.4852(10.8274) | Bit/dim 1.1029(1.1397) | Xent 0.0026(0.0092) | Loss 1.1042(1.1443) | Error 0.0011(0.0030) Steps 524(526.56) | Grad Norm 2.0229(8.5786) | Total Time 10.00(10.00)\n",
      "Iter 5210 | Time 10.8672(10.8022) | Bit/dim 1.1039(1.1351) | Xent 0.0103(0.0086) | Loss 1.1091(1.1394) | Error 0.0011(0.0030) Steps 530(526.87) | Grad Norm 3.5107(8.7381) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0079 | Time 48.7840, Epoch Time 777.1255(768.6430), Bit/dim 1.1023(best: 1.0950), Xent 0.0425, Loss 1.1236, Error 0.0106(best: 0.0097)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 5220 | Time 10.7836(10.7883) | Bit/dim 1.1059(1.1277) | Xent 0.0012(0.0080) | Loss 1.1065(1.1317) | Error 0.0000(0.0028) Steps 524(526.02) | Grad Norm 0.6569(7.5869) | Total Time 10.00(10.00)\n",
      "Iter 5230 | Time 10.8956(10.7976) | Bit/dim 1.0894(1.1211) | Xent 0.0018(0.0073) | Loss 1.0903(1.1248) | Error 0.0000(0.0025) Steps 530(526.14) | Grad Norm 2.4008(6.7485) | Total Time 10.00(10.00)\n",
      "Iter 5240 | Time 11.1326(10.8060) | Bit/dim 1.0960(1.1153) | Xent 0.0105(0.0071) | Loss 1.1012(1.1189) | Error 0.0022(0.0025) Steps 536(526.71) | Grad Norm 8.4923(6.8506) | Total Time 10.00(10.00)\n",
      "Iter 5250 | Time 10.8636(10.7753) | Bit/dim 1.1215(1.1134) | Xent 0.0027(0.0065) | Loss 1.1229(1.1167) | Error 0.0000(0.0023) Steps 518(525.69) | Grad Norm 11.3722(7.2422) | Total Time 10.00(10.00)\n",
      "Iter 5260 | Time 11.7719(10.8108) | Bit/dim 1.1451(1.1155) | Xent 0.0238(0.0066) | Loss 1.1570(1.1188) | Error 0.0078(0.0022) Steps 548(526.12) | Grad Norm 28.3020(8.7164) | Total Time 10.00(10.00)\n",
      "Iter 5270 | Time 12.0129(10.8950) | Bit/dim 1.1794(1.1291) | Xent 0.0356(0.0079) | Loss 1.1973(1.1331) | Error 0.0111(0.0026) Steps 560(527.44) | Grad Norm 25.2415(9.8649) | Total Time 10.00(10.00)\n",
      "Iter 5280 | Time 11.2185(10.8733) | Bit/dim 1.1432(1.1312) | Xent 0.0052(0.0077) | Loss 1.1458(1.1351) | Error 0.0022(0.0026) Steps 542(528.04) | Grad Norm 12.6248(9.5368) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0080 | Time 49.4106, Epoch Time 779.6638(768.9736), Bit/dim 1.1096(best: 1.0950), Xent 0.0441, Loss 1.1316, Error 0.0108(best: 0.0097)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 5290 | Time 10.7028(10.8976) | Bit/dim 1.1252(1.1275) | Xent 0.0047(0.0079) | Loss 1.1275(1.1314) | Error 0.0011(0.0026) Steps 524(527.67) | Grad Norm 6.7476(8.6859) | Total Time 10.00(10.00)\n",
      "Iter 5300 | Time 11.0145(10.8881) | Bit/dim 1.0903(1.1221) | Xent 0.0093(0.0072) | Loss 1.0950(1.1257) | Error 0.0033(0.0024) Steps 530(527.09) | Grad Norm 6.1006(7.6530) | Total Time 10.00(10.00)\n",
      "Iter 5310 | Time 10.8959(10.8559) | Bit/dim 1.0811(1.1144) | Xent 0.0036(0.0066) | Loss 1.0829(1.1178) | Error 0.0022(0.0022) Steps 524(527.36) | Grad Norm 2.3789(6.3397) | Total Time 10.00(10.00)\n",
      "Iter 5320 | Time 10.8849(10.8349) | Bit/dim 1.1009(1.1096) | Xent 0.0044(0.0065) | Loss 1.1031(1.1129) | Error 0.0022(0.0023) Steps 518(526.19) | Grad Norm 3.0112(5.3292) | Total Time 10.00(10.00)\n",
      "Iter 5330 | Time 10.7108(10.8684) | Bit/dim 1.1336(1.1088) | Xent 0.0022(0.0068) | Loss 1.1347(1.1122) | Error 0.0011(0.0023) Steps 518(526.90) | Grad Norm 10.5892(5.9036) | Total Time 10.00(10.00)\n",
      "Iter 5340 | Time 10.7470(10.9039) | Bit/dim 1.1573(1.1136) | Xent 0.0039(0.0066) | Loss 1.1593(1.1169) | Error 0.0022(0.0023) Steps 518(528.12) | Grad Norm 11.8899(7.8008) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0081 | Time 47.6524, Epoch Time 782.0221(769.3651), Bit/dim 1.1324(best: 1.0950), Xent 0.0446, Loss 1.1548, Error 0.0099(best: 0.0097)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 5350 | Time 10.4118(10.9057) | Bit/dim 1.1409(1.1222) | Xent 0.0087(0.0067) | Loss 1.1452(1.1255) | Error 0.0044(0.0026) Steps 524(528.60) | Grad Norm 7.6863(8.7534) | Total Time 10.00(10.00)\n",
      "Iter 5360 | Time 10.5618(10.8453) | Bit/dim 1.1032(1.1204) | Xent 0.0010(0.0063) | Loss 1.1038(1.1235) | Error 0.0000(0.0023) Steps 524(527.57) | Grad Norm 5.5108(8.4650) | Total Time 10.00(10.00)\n",
      "Iter 5370 | Time 10.4444(10.8102) | Bit/dim 1.1098(1.1157) | Xent 0.0022(0.0063) | Loss 1.1109(1.1188) | Error 0.0000(0.0021) Steps 530(527.71) | Grad Norm 4.0569(7.4315) | Total Time 10.00(10.00)\n",
      "Iter 5380 | Time 10.6538(10.8173) | Bit/dim 1.0920(1.1096) | Xent 0.0052(0.0058) | Loss 1.0946(1.1125) | Error 0.0033(0.0020) Steps 518(527.15) | Grad Norm 2.2883(6.1049) | Total Time 10.00(10.00)\n",
      "Iter 5390 | Time 11.2038(10.8662) | Bit/dim 1.0911(1.1054) | Xent 0.0043(0.0054) | Loss 1.0932(1.1080) | Error 0.0011(0.0018) Steps 536(528.73) | Grad Norm 0.7179(4.7306) | Total Time 10.00(10.00)\n",
      "Iter 5400 | Time 10.5167(10.8514) | Bit/dim 1.0897(1.1008) | Xent 0.0011(0.0061) | Loss 1.0902(1.1038) | Error 0.0000(0.0020) Steps 524(528.10) | Grad Norm 2.9190(3.8699) | Total Time 10.00(10.00)\n",
      "Iter 5410 | Time 10.7795(10.8642) | Bit/dim 1.0934(1.0975) | Xent 0.0018(0.0053) | Loss 1.0943(1.1002) | Error 0.0011(0.0018) Steps 524(527.02) | Grad Norm 2.7027(3.4415) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0082 | Time 48.5323, Epoch Time 778.1684(769.6292), Bit/dim 1.0840(best: 1.0950), Xent 0.0439, Loss 1.1060, Error 0.0105(best: 0.0097)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 5420 | Time 10.5928(10.8665) | Bit/dim 1.0871(1.0945) | Xent 0.0071(0.0052) | Loss 1.0907(1.0971) | Error 0.0011(0.0018) Steps 524(526.53) | Grad Norm 0.7914(3.0412) | Total Time 10.00(10.00)\n",
      "Iter 5430 | Time 11.2014(10.8885) | Bit/dim 1.0922(1.0934) | Xent 0.0033(0.0048) | Loss 1.0938(1.0958) | Error 0.0011(0.0016) Steps 524(526.77) | Grad Norm 4.3116(3.1184) | Total Time 10.00(10.00)\n",
      "Iter 5440 | Time 11.2481(10.9519) | Bit/dim 1.0967(1.0924) | Xent 0.0126(0.0050) | Loss 1.1030(1.0949) | Error 0.0056(0.0018) Steps 536(526.86) | Grad Norm 4.5808(3.0925) | Total Time 10.00(10.00)\n",
      "Iter 5450 | Time 10.9956(10.9541) | Bit/dim 1.4130(1.1111) | Xent 0.0035(0.0051) | Loss 1.4148(1.1137) | Error 0.0011(0.0019) Steps 542(528.40) | Grad Norm 9.8736(5.8867) | Total Time 10.00(10.00)\n",
      "Iter 5460 | Time 11.5602(10.9645) | Bit/dim 1.2382(1.1629) | Xent 0.0084(0.0057) | Loss 1.2424(1.1658) | Error 0.0033(0.0020) Steps 536(529.26) | Grad Norm 4.1682(5.5256) | Total Time 10.00(10.00)\n",
      "Iter 5470 | Time 12.0942(11.1151) | Bit/dim 1.1370(1.1652) | Xent 0.0283(0.0088) | Loss 1.1512(1.1696) | Error 0.0056(0.0029) Steps 548(532.75) | Grad Norm 2.4679(4.7265) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0083 | Time 48.6039, Epoch Time 793.3325(770.3403), Bit/dim 1.1260(best: 1.0840), Xent 0.0468, Loss 1.1494, Error 0.0095(best: 0.0097)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 5480 | Time 10.9074(11.0676) | Bit/dim 1.1049(1.1555) | Xent 0.0081(0.0117) | Loss 1.1089(1.1613) | Error 0.0033(0.0039) Steps 518(531.01) | Grad Norm 2.8961(4.7333) | Total Time 10.00(10.00)\n",
      "Iter 5490 | Time 10.1287(11.0150) | Bit/dim 1.1323(1.1461) | Xent 0.0036(0.0110) | Loss 1.1341(1.1516) | Error 0.0011(0.0038) Steps 512(530.44) | Grad Norm 9.2963(5.9412) | Total Time 10.00(10.00)\n",
      "Iter 5500 | Time 10.6775(10.9610) | Bit/dim 1.1122(1.1381) | Xent 0.0046(0.0095) | Loss 1.1145(1.1428) | Error 0.0011(0.0033) Steps 518(529.17) | Grad Norm 7.1034(6.7327) | Total Time 10.00(10.00)\n",
      "Iter 5510 | Time 10.8135(10.9156) | Bit/dim 1.1354(1.1298) | Xent 0.0097(0.0085) | Loss 1.1403(1.1340) | Error 0.0033(0.0030) Steps 518(526.60) | Grad Norm 11.5919(7.2412) | Total Time 10.00(10.00)\n",
      "Iter 5520 | Time 10.9389(10.9476) | Bit/dim 1.1218(1.1310) | Xent 0.0018(0.0080) | Loss 1.1227(1.1350) | Error 0.0011(0.0027) Steps 530(527.94) | Grad Norm 8.6398(8.3538) | Total Time 10.00(10.00)\n",
      "Iter 5530 | Time 10.5588(10.8679) | Bit/dim 1.0960(1.1238) | Xent 0.0009(0.0074) | Loss 1.0964(1.1274) | Error 0.0000(0.0024) Steps 524(526.29) | Grad Norm 0.8360(7.6183) | Total Time 10.00(10.00)\n",
      "Iter 5540 | Time 10.7857(10.8413) | Bit/dim 1.0967(1.1173) | Xent 0.0110(0.0068) | Loss 1.1022(1.1207) | Error 0.0044(0.0023) Steps 524(526.35) | Grad Norm 1.2065(6.8726) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0084 | Time 48.7023, Epoch Time 776.5979(770.5280), Bit/dim 1.0853(best: 1.0840), Xent 0.0388, Loss 1.1047, Error 0.0094(best: 0.0095)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 5550 | Time 10.7343(10.7738) | Bit/dim 1.0862(1.1096) | Xent 0.0061(0.0061) | Loss 1.0892(1.1126) | Error 0.0033(0.0021) Steps 524(525.44) | Grad Norm 4.3535(5.7968) | Total Time 10.00(10.00)\n",
      "Iter 5560 | Time 10.9007(10.7976) | Bit/dim 1.0831(1.1048) | Xent 0.0059(0.0057) | Loss 1.0860(1.1076) | Error 0.0022(0.0020) Steps 530(525.39) | Grad Norm 2.4487(5.0435) | Total Time 10.00(10.00)\n",
      "Iter 5570 | Time 11.5937(10.7958) | Bit/dim 1.0911(1.1003) | Xent 0.0016(0.0050) | Loss 1.0919(1.1028) | Error 0.0000(0.0018) Steps 536(525.64) | Grad Norm 5.3291(4.7282) | Total Time 10.00(10.00)\n",
      "Iter 5580 | Time 11.0390(10.8331) | Bit/dim 1.0943(1.0970) | Xent 0.0067(0.0049) | Loss 1.0977(1.0994) | Error 0.0033(0.0017) Steps 536(526.27) | Grad Norm 5.9695(4.6361) | Total Time 10.00(10.00)\n",
      "Iter 5590 | Time 11.0278(10.8276) | Bit/dim 1.0895(1.0950) | Xent 0.0078(0.0053) | Loss 1.0934(1.0976) | Error 0.0033(0.0020) Steps 524(526.27) | Grad Norm 1.4580(4.1119) | Total Time 10.00(10.00)\n",
      "Iter 5600 | Time 11.3229(10.8776) | Bit/dim 1.0868(1.0927) | Xent 0.0033(0.0053) | Loss 1.0885(1.0954) | Error 0.0011(0.0020) Steps 542(526.51) | Grad Norm 5.7117(4.1663) | Total Time 10.00(10.00)\n",
      "Iter 5610 | Time 10.5677(10.8752) | Bit/dim 1.0953(1.0917) | Xent 0.0109(0.0061) | Loss 1.1007(1.0948) | Error 0.0044(0.0023) Steps 530(527.66) | Grad Norm 9.3104(5.0377) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0085 | Time 49.6295, Epoch Time 781.2893(770.8508), Bit/dim 1.0840(best: 1.0840), Xent 0.0488, Loss 1.1084, Error 0.0112(best: 0.0094)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 5620 | Time 11.2512(10.9023) | Bit/dim 1.0951(1.0919) | Xent 0.0117(0.0063) | Loss 1.1009(1.0950) | Error 0.0067(0.0024) Steps 542(528.08) | Grad Norm 7.2141(5.2187) | Total Time 10.00(10.00)\n",
      "Iter 5630 | Time 11.5208(10.9124) | Bit/dim 1.1654(1.0943) | Xent 0.0135(0.0065) | Loss 1.1722(1.0976) | Error 0.0056(0.0022) Steps 548(528.33) | Grad Norm 46.1630(6.9108) | Total Time 10.00(10.00)\n",
      "Iter 5640 | Time 11.5129(10.9388) | Bit/dim 1.2083(1.1463) | Xent 0.0103(0.0068) | Loss 1.2135(1.1497) | Error 0.0044(0.0024) Steps 554(531.11) | Grad Norm 6.6973(6.7846) | Total Time 10.00(10.00)\n",
      "Iter 5650 | Time 11.1101(10.9968) | Bit/dim 1.1338(1.1494) | Xent 0.0136(0.0074) | Loss 1.1406(1.1531) | Error 0.0044(0.0028) Steps 542(531.73) | Grad Norm 3.1997(5.8373) | Total Time 10.00(10.00)\n",
      "Iter 5660 | Time 10.6832(10.9299) | Bit/dim 1.1024(1.1399) | Xent 0.0095(0.0084) | Loss 1.1072(1.1441) | Error 0.0044(0.0031) Steps 524(530.63) | Grad Norm 3.3276(5.6889) | Total Time 10.00(10.00)\n",
      "Iter 5670 | Time 10.8634(10.8891) | Bit/dim 1.0925(1.1285) | Xent 0.0073(0.0084) | Loss 1.0962(1.1327) | Error 0.0033(0.0033) Steps 524(529.39) | Grad Norm 3.0202(5.4069) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0086 | Time 50.0322, Epoch Time 786.6053(771.3235), Bit/dim 1.0856(best: 1.0840), Xent 0.0421, Loss 1.1066, Error 0.0113(best: 0.0094)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 5680 | Time 11.2797(10.9095) | Bit/dim 1.0904(1.1191) | Xent 0.0027(0.0076) | Loss 1.0918(1.1229) | Error 0.0011(0.0028) Steps 524(527.98) | Grad Norm 1.0904(4.6232) | Total Time 10.00(10.00)\n",
      "Iter 5690 | Time 11.2569(10.9073) | Bit/dim 1.0827(1.1094) | Xent 0.0066(0.0072) | Loss 1.0861(1.1129) | Error 0.0033(0.0027) Steps 518(526.59) | Grad Norm 3.3951(3.9252) | Total Time 10.00(10.00)\n",
      "Iter 5700 | Time 10.8945(10.9336) | Bit/dim 1.4077(1.1317) | Xent 0.0158(0.0080) | Loss 1.4156(1.1357) | Error 0.0044(0.0029) Steps 536(528.98) | Grad Norm 12.1547(7.1608) | Total Time 10.00(10.00)\n",
      "Iter 5710 | Time 11.3772(10.9482) | Bit/dim 1.2312(1.1765) | Xent 0.0210(0.0083) | Loss 1.2417(1.1806) | Error 0.0100(0.0031) Steps 554(531.43) | Grad Norm 3.4227(6.3639) | Total Time 10.00(10.00)\n",
      "Iter 5720 | Time 11.8480(11.0876) | Bit/dim 1.1398(1.1770) | Xent 0.0095(0.0097) | Loss 1.1445(1.1819) | Error 0.0033(0.0035) Steps 560(535.85) | Grad Norm 2.3182(5.4540) | Total Time 10.00(10.00)\n",
      "Iter 5730 | Time 10.5884(11.0421) | Bit/dim 1.1253(1.1641) | Xent 0.0138(0.0113) | Loss 1.1323(1.1697) | Error 0.0033(0.0038) Steps 530(535.00) | Grad Norm 6.1573(4.9899) | Total Time 10.00(10.00)\n",
      "Iter 5740 | Time 11.0794(11.0232) | Bit/dim 1.1118(1.1504) | Xent 0.0113(0.0115) | Loss 1.1175(1.1561) | Error 0.0044(0.0038) Steps 524(534.89) | Grad Norm 4.9857(5.6901) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0087 | Time 48.3234, Epoch Time 791.7556(771.9364), Bit/dim 1.0950(best: 1.0840), Xent 0.0511, Loss 1.1205, Error 0.0124(best: 0.0094)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 5750 | Time 10.4216(10.9740) | Bit/dim 1.1011(1.1376) | Xent 0.0088(0.0106) | Loss 1.1055(1.1428) | Error 0.0033(0.0035) Steps 518(532.58) | Grad Norm 7.7191(6.2575) | Total Time 10.00(10.00)\n",
      "Iter 5760 | Time 11.1891(10.9611) | Bit/dim 1.1013(1.1287) | Xent 0.0183(0.0093) | Loss 1.1104(1.1333) | Error 0.0089(0.0032) Steps 542(531.30) | Grad Norm 13.4974(7.3147) | Total Time 10.00(10.00)\n",
      "Iter 5770 | Time 10.9201(10.9298) | Bit/dim 1.1033(1.1195) | Xent 0.0106(0.0082) | Loss 1.1087(1.1235) | Error 0.0011(0.0028) Steps 530(530.16) | Grad Norm 9.3877(7.1345) | Total Time 10.00(10.00)\n",
      "Iter 5780 | Time 10.8593(10.9497) | Bit/dim 1.1629(1.1178) | Xent 0.0037(0.0073) | Loss 1.1648(1.1215) | Error 0.0022(0.0025) Steps 530(531.13) | Grad Norm 12.6177(8.3964) | Total Time 10.00(10.00)\n",
      "Iter 5790 | Time 10.5442(10.9784) | Bit/dim 1.1984(1.1282) | Xent 0.0013(0.0073) | Loss 1.1990(1.1318) | Error 0.0000(0.0024) Steps 530(533.16) | Grad Norm 9.2051(9.3075) | Total Time 10.00(10.00)\n",
      "Iter 5800 | Time 10.2282(11.0128) | Bit/dim 1.1268(1.1303) | Xent 0.0028(0.0066) | Loss 1.1282(1.1336) | Error 0.0011(0.0022) Steps 524(534.69) | Grad Norm 6.2073(8.9866) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0088 | Time 48.1909, Epoch Time 784.8152(772.3228), Bit/dim 1.0998(best: 1.0840), Xent 0.0407, Loss 1.1201, Error 0.0102(best: 0.0094)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 5810 | Time 10.7194(10.9378) | Bit/dim 1.0986(1.1249) | Xent 0.0061(0.0066) | Loss 1.1016(1.1282) | Error 0.0011(0.0022) Steps 524(533.59) | Grad Norm 2.1458(8.1371) | Total Time 10.00(10.00)\n",
      "Iter 5820 | Time 10.4792(10.9155) | Bit/dim 1.0865(1.1187) | Xent 0.0053(0.0059) | Loss 1.0891(1.1217) | Error 0.0011(0.0021) Steps 536(533.49) | Grad Norm 6.2831(7.9053) | Total Time 10.00(10.00)\n",
      "Iter 5830 | Time 10.3264(10.8440) | Bit/dim 1.0658(1.1104) | Xent 0.0064(0.0059) | Loss 1.0690(1.1133) | Error 0.0022(0.0019) Steps 524(531.88) | Grad Norm 3.5344(6.9456) | Total Time 10.00(10.00)\n",
      "Iter 5840 | Time 10.5721(10.8811) | Bit/dim 1.1190(1.1064) | Xent 0.0026(0.0052) | Loss 1.1203(1.1090) | Error 0.0011(0.0017) Steps 530(531.73) | Grad Norm 8.7172(7.0981) | Total Time 10.00(10.00)\n",
      "Iter 5850 | Time 10.6068(10.8288) | Bit/dim 1.0936(1.1014) | Xent 0.0055(0.0052) | Loss 1.0964(1.1040) | Error 0.0022(0.0019) Steps 524(530.68) | Grad Norm 2.4242(6.4164) | Total Time 10.00(10.00)\n",
      "Iter 5860 | Time 10.4107(10.7618) | Bit/dim 1.0861(1.0964) | Xent 0.0114(0.0050) | Loss 1.0918(1.0989) | Error 0.0022(0.0018) Steps 530(529.93) | Grad Norm 1.6914(5.2817) | Total Time 10.00(10.00)\n",
      "Iter 5870 | Time 10.7976(10.7609) | Bit/dim 1.0765(1.0928) | Xent 0.0014(0.0049) | Loss 1.0771(1.0953) | Error 0.0000(0.0016) Steps 536(529.93) | Grad Norm 1.7190(4.5875) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0089 | Time 47.8894, Epoch Time 773.0922(772.3459), Bit/dim 1.0760(best: 1.0840), Xent 0.0367, Loss 1.0943, Error 0.0092(best: 0.0094)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 5880 | Time 10.7325(10.7962) | Bit/dim 1.0730(1.0898) | Xent 0.0043(0.0049) | Loss 1.0752(1.0922) | Error 0.0011(0.0015) Steps 524(529.34) | Grad Norm 2.6622(4.2870) | Total Time 10.00(10.00)\n",
      "Iter 5890 | Time 11.3818(10.8426) | Bit/dim 1.0803(1.0872) | Xent 0.0079(0.0050) | Loss 1.0842(1.0897) | Error 0.0011(0.0016) Steps 536(528.75) | Grad Norm 2.8813(3.9940) | Total Time 10.00(10.00)\n",
      "Iter 5900 | Time 11.2232(10.8590) | Bit/dim 1.0868(1.0863) | Xent 0.0025(0.0048) | Loss 1.0880(1.0887) | Error 0.0000(0.0016) Steps 536(529.08) | Grad Norm 4.7389(4.5599) | Total Time 10.00(10.00)\n",
      "Iter 5910 | Time 10.6523(10.9059) | Bit/dim 1.3304(1.1158) | Xent 0.0041(0.0047) | Loss 1.3324(1.1182) | Error 0.0022(0.0016) Steps 536(531.29) | Grad Norm 6.1264(6.6370) | Total Time 10.00(10.00)\n",
      "Iter 5920 | Time 11.5543(10.9213) | Bit/dim 1.1776(1.1386) | Xent 0.0055(0.0069) | Loss 1.1803(1.1421) | Error 0.0011(0.0023) Steps 566(532.71) | Grad Norm 2.9332(6.0186) | Total Time 10.00(10.00)\n",
      "Iter 5930 | Time 10.8497(11.0390) | Bit/dim 1.1046(1.1347) | Xent 0.0147(0.0092) | Loss 1.1119(1.1393) | Error 0.0067(0.0030) Steps 536(535.20) | Grad Norm 6.9539(5.6168) | Total Time 10.00(10.00)\n",
      "Iter 5940 | Time 10.9548(11.0382) | Bit/dim 1.1590(1.1325) | Xent 0.0066(0.0097) | Loss 1.1623(1.1374) | Error 0.0033(0.0033) Steps 536(535.34) | Grad Norm 11.3721(6.5235) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0090 | Time 48.0448, Epoch Time 791.1223(772.9092), Bit/dim 1.1595(best: 1.0760), Xent 0.0472, Loss 1.1831, Error 0.0113(best: 0.0092)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 5950 | Time 10.6242(11.0475) | Bit/dim 1.0921(1.1321) | Xent 0.0040(0.0093) | Loss 1.0941(1.1368) | Error 0.0022(0.0031) Steps 524(535.80) | Grad Norm 5.7613(7.8160) | Total Time 10.00(10.00)\n",
      "Iter 5960 | Time 10.3379(10.9610) | Bit/dim 1.0874(1.1240) | Xent 0.0081(0.0092) | Loss 1.0914(1.1286) | Error 0.0022(0.0032) Steps 524(534.15) | Grad Norm 5.2669(7.5267) | Total Time 10.00(10.00)\n",
      "Iter 5970 | Time 11.0445(10.9116) | Bit/dim 1.0855(1.1153) | Xent 0.0135(0.0086) | Loss 1.0923(1.1196) | Error 0.0033(0.0029) Steps 524(531.85) | Grad Norm 4.2161(6.8188) | Total Time 10.00(10.00)\n",
      "Iter 5980 | Time 10.9952(10.9294) | Bit/dim 1.1218(1.1101) | Xent 0.0018(0.0080) | Loss 1.1227(1.1141) | Error 0.0000(0.0029) Steps 536(531.41) | Grad Norm 10.4951(7.1708) | Total Time 10.00(10.00)\n",
      "Iter 5990 | Time 10.3587(10.9376) | Bit/dim 1.1126(1.1079) | Xent 0.0010(0.0074) | Loss 1.1131(1.1116) | Error 0.0000(0.0026) Steps 524(531.43) | Grad Norm 9.2867(7.4864) | Total Time 10.00(10.00)\n",
      "Iter 6000 | Time 10.9582(10.9223) | Bit/dim 1.0861(1.1062) | Xent 0.0035(0.0069) | Loss 1.0879(1.1097) | Error 0.0011(0.0025) Steps 536(532.04) | Grad Norm 6.2190(8.0876) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0091 | Time 50.1574, Epoch Time 784.4292(773.2548), Bit/dim 1.0805(best: 1.0760), Xent 0.0464, Loss 1.1037, Error 0.0102(best: 0.0092)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 6010 | Time 11.1268(10.8994) | Bit/dim 1.0951(1.1024) | Xent 0.0015(0.0063) | Loss 1.0958(1.1055) | Error 0.0000(0.0023) Steps 536(530.89) | Grad Norm 11.8464(7.9603) | Total Time 10.00(10.00)\n",
      "Iter 6020 | Time 10.4551(10.9080) | Bit/dim 1.0740(1.0991) | Xent 0.0019(0.0061) | Loss 1.0749(1.1022) | Error 0.0000(0.0022) Steps 524(531.36) | Grad Norm 4.7289(7.7120) | Total Time 10.00(10.00)\n",
      "Iter 6030 | Time 10.5756(10.8926) | Bit/dim 1.0903(1.0972) | Xent 0.0012(0.0054) | Loss 1.0909(1.0999) | Error 0.0000(0.0019) Steps 530(531.99) | Grad Norm 6.5908(8.1144) | Total Time 10.00(10.00)\n",
      "Iter 6040 | Time 11.1894(10.8826) | Bit/dim 1.0776(1.0928) | Xent 0.0053(0.0057) | Loss 1.0803(1.0956) | Error 0.0022(0.0019) Steps 554(532.63) | Grad Norm 16.8119(7.5658) | Total Time 10.00(10.00)\n",
      "Iter 6050 | Time 11.1271(10.8793) | Bit/dim 1.1109(1.0922) | Xent 0.0016(0.0065) | Loss 1.1117(1.0954) | Error 0.0011(0.0021) Steps 548(532.72) | Grad Norm 15.9308(7.8327) | Total Time 10.00(10.00)\n",
      "Iter 6060 | Time 12.1651(10.9486) | Bit/dim 1.1120(1.0942) | Xent 0.0065(0.0059) | Loss 1.1152(1.0972) | Error 0.0022(0.0019) Steps 566(533.43) | Grad Norm 21.6720(8.3503) | Total Time 10.00(10.00)\n",
      "Iter 6070 | Time 10.6432(10.9749) | Bit/dim 1.1134(1.1062) | Xent 0.0073(0.0060) | Loss 1.1170(1.1092) | Error 0.0033(0.0020) Steps 530(534.04) | Grad Norm 6.5500(9.3505) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0092 | Time 49.4909, Epoch Time 786.9778(773.6664), Bit/dim 1.0993(best: 1.0760), Xent 0.0473, Loss 1.1229, Error 0.0094(best: 0.0092)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 6080 | Time 11.4279(11.0353) | Bit/dim 1.0998(1.1127) | Xent 0.0048(0.0059) | Loss 1.1022(1.1157) | Error 0.0011(0.0020) Steps 548(536.12) | Grad Norm 3.3985(9.2650) | Total Time 10.00(10.00)\n",
      "Iter 6090 | Time 10.8008(10.9893) | Bit/dim 1.0944(1.1108) | Xent 0.0027(0.0051) | Loss 1.0957(1.1134) | Error 0.0011(0.0018) Steps 524(535.00) | Grad Norm 4.9944(8.3230) | Total Time 10.00(10.00)\n",
      "Iter 6100 | Time 10.9798(10.9591) | Bit/dim 1.0743(1.1041) | Xent 0.0032(0.0050) | Loss 1.0759(1.1066) | Error 0.0022(0.0017) Steps 530(533.20) | Grad Norm 4.2185(7.0908) | Total Time 10.00(10.00)\n",
      "Iter 6110 | Time 10.9486(10.9470) | Bit/dim 1.0782(1.0977) | Xent 0.0013(0.0042) | Loss 1.0788(1.0999) | Error 0.0011(0.0014) Steps 524(532.51) | Grad Norm 3.1415(6.4565) | Total Time 10.00(10.00)\n",
      "Iter 6120 | Time 10.6549(10.9453) | Bit/dim 1.0779(1.0937) | Xent 0.0058(0.0041) | Loss 1.0808(1.0958) | Error 0.0011(0.0013) Steps 530(532.52) | Grad Norm 6.5948(6.4694) | Total Time 10.00(10.00)\n",
      "Iter 6130 | Time 12.4959(11.0470) | Bit/dim 1.1753(1.1068) | Xent 0.0045(0.0044) | Loss 1.1776(1.1090) | Error 0.0011(0.0014) Steps 566(534.88) | Grad Norm 15.8175(8.1666) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0093 | Time 49.6378, Epoch Time 790.4921(774.1712), Bit/dim 1.1077(best: 1.0760), Xent 0.0504, Loss 1.1329, Error 0.0114(best: 0.0092)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 6140 | Time 10.8816(11.0264) | Bit/dim 1.0990(1.1074) | Xent 0.0013(0.0053) | Loss 1.0997(1.1100) | Error 0.0000(0.0015) Steps 536(534.90) | Grad Norm 2.3391(7.8835) | Total Time 10.00(10.00)\n",
      "Iter 6150 | Time 10.6415(10.9563) | Bit/dim 1.0924(1.1035) | Xent 0.0027(0.0052) | Loss 1.0937(1.1061) | Error 0.0022(0.0017) Steps 530(535.74) | Grad Norm 4.0562(7.1657) | Total Time 10.00(10.00)\n",
      "Iter 6160 | Time 11.2312(10.9380) | Bit/dim 1.0929(1.0991) | Xent 0.0069(0.0054) | Loss 1.0964(1.1018) | Error 0.0022(0.0016) Steps 542(535.07) | Grad Norm 10.1514(6.6364) | Total Time 10.00(10.00)\n",
      "Iter 6170 | Time 10.6938(10.9531) | Bit/dim 1.0848(1.0953) | Xent 0.0021(0.0053) | Loss 1.0858(1.0979) | Error 0.0011(0.0017) Steps 536(534.73) | Grad Norm 8.2352(6.7053) | Total Time 10.00(10.00)\n",
      "Iter 6180 | Time 10.8296(11.0276) | Bit/dim 1.1034(1.0956) | Xent 0.0027(0.0055) | Loss 1.1047(1.0983) | Error 0.0011(0.0017) Steps 536(536.81) | Grad Norm 9.2675(7.7071) | Total Time 10.00(10.00)\n",
      "Iter 6190 | Time 10.7460(11.0482) | Bit/dim 1.1209(1.1004) | Xent 0.0027(0.0054) | Loss 1.1223(1.1031) | Error 0.0022(0.0020) Steps 542(537.07) | Grad Norm 8.2371(8.5184) | Total Time 10.00(10.00)\n",
      "Iter 6200 | Time 11.1354(11.0303) | Bit/dim 1.0882(1.1020) | Xent 0.0059(0.0057) | Loss 1.0912(1.1049) | Error 0.0022(0.0021) Steps 530(536.95) | Grad Norm 5.7484(8.6231) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0094 | Time 49.4612, Epoch Time 789.5678(774.6331), Bit/dim 1.0871(best: 1.0760), Xent 0.0551, Loss 1.1146, Error 0.0124(best: 0.0092)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 6210 | Time 10.7856(10.9363) | Bit/dim 1.0837(1.0985) | Xent 0.0068(0.0064) | Loss 1.0872(1.1017) | Error 0.0011(0.0021) Steps 524(534.49) | Grad Norm 4.9278(7.8571) | Total Time 10.00(10.00)\n",
      "Iter 6220 | Time 11.1989(10.8901) | Bit/dim 1.0985(1.0955) | Xent 0.0019(0.0063) | Loss 1.0995(1.0987) | Error 0.0000(0.0021) Steps 524(532.34) | Grad Norm 2.7718(6.5920) | Total Time 10.00(10.00)\n",
      "Iter 6230 | Time 10.4848(10.8638) | Bit/dim 1.0749(1.0906) | Xent 0.0068(0.0065) | Loss 1.0783(1.0939) | Error 0.0011(0.0022) Steps 524(531.45) | Grad Norm 2.4406(5.8599) | Total Time 10.00(10.00)\n",
      "Iter 6240 | Time 10.9041(10.9354) | Bit/dim 1.1554(1.0982) | Xent 0.0090(0.0069) | Loss 1.1599(1.1017) | Error 0.0022(0.0025) Steps 536(533.77) | Grad Norm 11.5208(7.8032) | Total Time 10.00(10.00)\n",
      "Iter 6250 | Time 11.0938(10.9856) | Bit/dim 1.0942(1.1015) | Xent 0.0042(0.0062) | Loss 1.0963(1.1047) | Error 0.0011(0.0022) Steps 524(535.71) | Grad Norm 5.7867(8.2241) | Total Time 10.00(10.00)\n",
      "Iter 6260 | Time 10.7136(10.8875) | Bit/dim 1.0914(1.1000) | Xent 0.0144(0.0061) | Loss 1.0987(1.1031) | Error 0.0056(0.0021) Steps 530(533.58) | Grad Norm 11.6355(7.8453) | Total Time 10.00(10.00)\n",
      "Iter 6270 | Time 10.8077(10.8974) | Bit/dim 1.0729(1.0963) | Xent 0.0048(0.0059) | Loss 1.0753(1.0993) | Error 0.0022(0.0020) Steps 536(533.63) | Grad Norm 0.6852(7.3760) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0095 | Time 49.9820, Epoch Time 782.7200(774.8757), Bit/dim 1.0824(best: 1.0760), Xent 0.0472, Loss 1.1060, Error 0.0100(best: 0.0092)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 6280 | Time 10.8569(10.8788) | Bit/dim 1.0706(1.0941) | Xent 0.0123(0.0058) | Loss 1.0768(1.0969) | Error 0.0044(0.0020) Steps 530(534.37) | Grad Norm 4.3372(6.9993) | Total Time 10.00(10.00)\n",
      "Iter 6290 | Time 10.7389(10.8581) | Bit/dim 1.0901(1.0894) | Xent 0.0044(0.0055) | Loss 1.0923(1.0922) | Error 0.0022(0.0019) Steps 536(534.16) | Grad Norm 7.4937(6.7045) | Total Time 10.00(10.00)\n",
      "Iter 6300 | Time 10.9903(10.9105) | Bit/dim 1.0812(1.0896) | Xent 0.0041(0.0054) | Loss 1.0833(1.0923) | Error 0.0022(0.0019) Steps 524(534.66) | Grad Norm 8.9070(7.4737) | Total Time 10.00(10.00)\n",
      "Iter 6310 | Time 11.1522(10.9491) | Bit/dim 1.1197(1.0971) | Xent 0.0051(0.0054) | Loss 1.1222(1.0998) | Error 0.0033(0.0019) Steps 524(534.83) | Grad Norm 8.6045(8.5855) | Total Time 10.00(10.00)\n",
      "Iter 6320 | Time 11.0414(10.9813) | Bit/dim 1.0811(1.0995) | Xent 0.0055(0.0053) | Loss 1.0839(1.1022) | Error 0.0033(0.0019) Steps 524(535.65) | Grad Norm 4.8894(8.6673) | Total Time 10.00(10.00)\n",
      "Iter 6330 | Time 10.8426(10.9486) | Bit/dim 1.0938(1.0970) | Xent 0.0039(0.0048) | Loss 1.0957(1.0994) | Error 0.0011(0.0017) Steps 524(533.94) | Grad Norm 3.5295(7.8380) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0096 | Time 49.7478, Epoch Time 786.4119(775.2218), Bit/dim 1.0753(best: 1.0760), Xent 0.0442, Loss 1.0974, Error 0.0096(best: 0.0092)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 6340 | Time 11.0861(10.9098) | Bit/dim 1.0891(1.0933) | Xent 0.0089(0.0051) | Loss 1.0935(1.0958) | Error 0.0022(0.0016) Steps 530(533.54) | Grad Norm 6.9745(6.9610) | Total Time 10.00(10.00)\n",
      "Iter 6350 | Time 10.6164(10.8430) | Bit/dim 1.0850(1.0893) | Xent 0.0036(0.0047) | Loss 1.0868(1.0917) | Error 0.0022(0.0016) Steps 524(532.61) | Grad Norm 0.6693(5.8527) | Total Time 10.00(10.00)\n",
      "Iter 6360 | Time 11.1328(10.8596) | Bit/dim 1.0684(1.0846) | Xent 0.0010(0.0040) | Loss 1.0689(1.0866) | Error 0.0000(0.0014) Steps 530(531.62) | Grad Norm 2.2274(4.8760) | Total Time 10.00(10.00)\n",
      "Iter 6370 | Time 10.6211(10.8528) | Bit/dim 1.0669(1.0814) | Xent 0.0052(0.0040) | Loss 1.0695(1.0834) | Error 0.0011(0.0013) Steps 530(532.08) | Grad Norm 1.4511(4.4467) | Total Time 10.00(10.00)\n",
      "Iter 6380 | Time 11.0907(10.8660) | Bit/dim 1.0767(1.0790) | Xent 0.0055(0.0040) | Loss 1.0795(1.0810) | Error 0.0022(0.0014) Steps 542(533.39) | Grad Norm 7.0119(4.2445) | Total Time 10.00(10.00)\n",
      "Iter 6390 | Time 10.7496(10.8625) | Bit/dim 1.0785(1.0772) | Xent 0.0015(0.0042) | Loss 1.0792(1.0792) | Error 0.0000(0.0015) Steps 530(532.91) | Grad Norm 4.6122(4.1709) | Total Time 10.00(10.00)\n",
      "Iter 6400 | Time 10.6377(10.9243) | Bit/dim 1.1964(1.0864) | Xent 0.0172(0.0051) | Loss 1.2050(1.0890) | Error 0.0056(0.0015) Steps 536(535.20) | Grad Norm 13.8877(6.5274) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0097 | Time 49.1147, Epoch Time 781.8770(775.4215), Bit/dim 1.2063(best: 1.0753), Xent 0.0557, Loss 1.2341, Error 0.0119(best: 0.0092)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 6410 | Time 11.1262(11.0227) | Bit/dim 1.1696(1.1107) | Xent 0.0038(0.0059) | Loss 1.1715(1.1136) | Error 0.0011(0.0020) Steps 542(537.53) | Grad Norm 16.6866(7.2541) | Total Time 10.00(10.00)\n",
      "Iter 6420 | Time 11.2121(11.0687) | Bit/dim 1.0983(1.1120) | Xent 0.0035(0.0057) | Loss 1.1000(1.1149) | Error 0.0011(0.0019) Steps 524(539.08) | Grad Norm 6.1567(7.2470) | Total Time 10.00(10.00)\n",
      "Iter 6430 | Time 11.6077(11.1006) | Bit/dim 1.1154(1.1115) | Xent 0.0025(0.0058) | Loss 1.1167(1.1144) | Error 0.0011(0.0020) Steps 530(538.11) | Grad Norm 3.3860(7.6725) | Total Time 10.00(10.00)\n",
      "Iter 6440 | Time 11.7473(11.1349) | Bit/dim 1.1121(1.1096) | Xent 0.0079(0.0058) | Loss 1.1160(1.1125) | Error 0.0033(0.0020) Steps 560(539.95) | Grad Norm 13.8955(8.2205) | Total Time 10.00(10.00)\n",
      "Iter 6450 | Time 10.8566(11.0343) | Bit/dim 1.0818(1.1039) | Xent 0.0095(0.0055) | Loss 1.0866(1.1066) | Error 0.0056(0.0019) Steps 530(537.52) | Grad Norm 4.7637(7.3544) | Total Time 10.00(10.00)\n",
      "Iter 6460 | Time 10.7232(10.9414) | Bit/dim 1.0682(1.0974) | Xent 0.0111(0.0056) | Loss 1.0738(1.1002) | Error 0.0022(0.0019) Steps 536(535.56) | Grad Norm 1.8654(6.5573) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0098 | Time 49.9673, Epoch Time 792.8961(775.9457), Bit/dim 1.0768(best: 1.0753), Xent 0.0426, Loss 1.0981, Error 0.0095(best: 0.0092)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 6470 | Time 11.4975(10.9470) | Bit/dim 1.0990(1.0926) | Xent 0.0080(0.0050) | Loss 1.1030(1.0951) | Error 0.0033(0.0017) Steps 536(535.84) | Grad Norm 8.4541(6.2475) | Total Time 10.00(10.00)\n",
      "Iter 6480 | Time 10.9064(11.0044) | Bit/dim 1.1334(1.0927) | Xent 0.0014(0.0055) | Loss 1.1341(1.0954) | Error 0.0000(0.0018) Steps 536(538.21) | Grad Norm 11.3292(7.3441) | Total Time 10.00(10.00)\n",
      "Iter 6490 | Time 10.9939(11.0432) | Bit/dim 1.0893(1.0955) | Xent 0.0027(0.0049) | Loss 1.0906(1.0979) | Error 0.0011(0.0017) Steps 530(538.55) | Grad Norm 5.3126(8.0129) | Total Time 10.00(10.00)\n",
      "Iter 6500 | Time 11.2938(11.1277) | Bit/dim 1.0778(1.0916) | Xent 0.0016(0.0044) | Loss 1.0786(1.0938) | Error 0.0000(0.0015) Steps 548(539.18) | Grad Norm 6.8901(7.8195) | Total Time 10.00(10.00)\n",
      "Iter 6510 | Time 11.3046(11.1024) | Bit/dim 1.0741(1.0873) | Xent 0.0039(0.0041) | Loss 1.0761(1.0893) | Error 0.0022(0.0014) Steps 530(538.22) | Grad Norm 8.5129(7.3603) | Total Time 10.00(10.00)\n",
      "Iter 6520 | Time 10.8144(11.0758) | Bit/dim 1.0663(1.0848) | Xent 0.0037(0.0047) | Loss 1.0682(1.0872) | Error 0.0011(0.0016) Steps 530(537.36) | Grad Norm 3.2940(6.7159) | Total Time 10.00(10.00)\n",
      "Iter 6530 | Time 10.3591(11.0674) | Bit/dim 1.0742(1.0843) | Xent 0.0017(0.0048) | Loss 1.0750(1.0867) | Error 0.0000(0.0016) Steps 530(539.12) | Grad Norm 8.4065(6.8732) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0099 | Time 49.1386, Epoch Time 797.6636(776.5972), Bit/dim 1.0734(best: 1.0753), Xent 0.0432, Loss 1.0950, Error 0.0108(best: 0.0092)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 6540 | Time 11.3148(11.0259) | Bit/dim 1.0845(1.0833) | Xent 0.0012(0.0047) | Loss 1.0851(1.0856) | Error 0.0000(0.0016) Steps 548(538.32) | Grad Norm 5.5079(7.0405) | Total Time 10.00(10.00)\n",
      "Iter 6550 | Time 11.1988(11.0546) | Bit/dim 1.0912(1.0818) | Xent 0.0075(0.0046) | Loss 1.0950(1.0841) | Error 0.0011(0.0015) Steps 536(539.84) | Grad Norm 10.0973(7.5525) | Total Time 10.00(10.00)\n",
      "Iter 6560 | Time 11.3603(11.1387) | Bit/dim 1.0870(1.0805) | Xent 0.0003(0.0041) | Loss 1.0872(1.0826) | Error 0.0000(0.0013) Steps 548(541.17) | Grad Norm 11.6470(7.8245) | Total Time 10.00(10.00)\n",
      "Iter 6570 | Time 11.1854(11.2386) | Bit/dim 1.1232(1.0920) | Xent 0.0076(0.0044) | Loss 1.1270(1.0942) | Error 0.0022(0.0013) Steps 560(544.37) | Grad Norm 9.3847(8.5918) | Total Time 10.00(10.00)\n",
      "Iter 6580 | Time 11.8999(11.2866) | Bit/dim 1.0938(1.0937) | Xent 0.0017(0.0043) | Loss 1.0946(1.0959) | Error 0.0000(0.0013) Steps 542(545.44) | Grad Norm 2.0386(8.0229) | Total Time 10.00(10.00)\n",
      "Iter 6590 | Time 11.4082(11.2283) | Bit/dim 1.0774(1.0908) | Xent 0.0052(0.0045) | Loss 1.0799(1.0930) | Error 0.0022(0.0014) Steps 536(546.05) | Grad Norm 4.3146(7.5416) | Total Time 10.00(10.00)\n",
      "Iter 6600 | Time 11.6079(11.1552) | Bit/dim 1.0692(1.0865) | Xent 0.0032(0.0042) | Loss 1.0709(1.0886) | Error 0.0011(0.0013) Steps 536(545.11) | Grad Norm 1.3929(6.4558) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0100 | Time 49.6033, Epoch Time 804.3615(777.4302), Bit/dim 1.0671(best: 1.0734), Xent 0.0369, Loss 1.0856, Error 0.0081(best: 0.0092)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 6610 | Time 11.6473(11.1244) | Bit/dim 1.0889(1.0831) | Xent 0.0110(0.0042) | Loss 1.0944(1.0852) | Error 0.0022(0.0013) Steps 548(543.58) | Grad Norm 16.1219(6.3259) | Total Time 10.00(10.00)\n",
      "Iter 6620 | Time 11.7531(11.1514) | Bit/dim 1.0950(1.0830) | Xent 0.0068(0.0041) | Loss 1.0984(1.0850) | Error 0.0033(0.0013) Steps 554(543.02) | Grad Norm 18.4726(7.0611) | Total Time 10.00(10.00)\n",
      "Iter 6630 | Time 11.1006(11.1950) | Bit/dim 1.0829(1.0841) | Xent 0.0138(0.0050) | Loss 1.0898(1.0866) | Error 0.0056(0.0016) Steps 554(543.67) | Grad Norm 6.4822(7.6752) | Total Time 10.00(10.00)\n",
      "Iter 6640 | Time 11.1029(11.2115) | Bit/dim 1.0817(1.0850) | Xent 0.0098(0.0057) | Loss 1.0866(1.0879) | Error 0.0022(0.0018) Steps 536(544.20) | Grad Norm 7.2861(7.8227) | Total Time 10.00(10.00)\n",
      "Iter 6650 | Time 11.1984(11.2276) | Bit/dim 1.0716(1.0840) | Xent 0.0026(0.0054) | Loss 1.0728(1.0867) | Error 0.0011(0.0016) Steps 536(544.77) | Grad Norm 7.5648(7.8748) | Total Time 10.00(10.00)\n",
      "Iter 6660 | Time 11.3748(11.1548) | Bit/dim 1.0888(1.0820) | Xent 0.0152(0.0057) | Loss 1.0963(1.0849) | Error 0.0044(0.0019) Steps 542(541.92) | Grad Norm 1.7586(7.0515) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0101 | Time 50.8174, Epoch Time 802.6000(778.1853), Bit/dim 1.0740(best: 1.0671), Xent 0.0478, Loss 1.0979, Error 0.0104(best: 0.0081)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 6670 | Time 11.9000(11.1589) | Bit/dim 1.0793(1.0813) | Xent 0.0042(0.0053) | Loss 1.0814(1.0840) | Error 0.0011(0.0018) Steps 554(541.74) | Grad Norm 16.9127(7.0914) | Total Time 10.00(10.00)\n",
      "Iter 6680 | Time 12.2838(11.2017) | Bit/dim 1.0973(1.0836) | Xent 0.0027(0.0055) | Loss 1.0986(1.0864) | Error 0.0011(0.0019) Steps 572(543.26) | Grad Norm 17.1084(8.0377) | Total Time 10.00(10.00)\n",
      "Iter 6690 | Time 10.5292(11.2008) | Bit/dim 1.0734(1.0822) | Xent 0.0028(0.0054) | Loss 1.0748(1.0849) | Error 0.0022(0.0019) Steps 542(543.84) | Grad Norm 8.7685(8.0344) | Total Time 10.00(10.00)\n",
      "Iter 6700 | Time 11.1166(11.2113) | Bit/dim 1.0856(1.0801) | Xent 0.0027(0.0051) | Loss 1.0870(1.0826) | Error 0.0011(0.0018) Steps 542(544.52) | Grad Norm 7.6566(7.8708) | Total Time 10.00(10.00)\n",
      "Iter 6710 | Time 11.6603(11.2987) | Bit/dim 1.1111(1.0858) | Xent 0.0139(0.0054) | Loss 1.1180(1.0885) | Error 0.0044(0.0020) Steps 554(550.15) | Grad Norm 9.5507(8.5695) | Total Time 10.00(10.00)\n",
      "Iter 6720 | Time 12.1654(11.2966) | Bit/dim 1.0911(1.0871) | Xent 0.0114(0.0060) | Loss 1.0968(1.0901) | Error 0.0033(0.0021) Steps 566(551.14) | Grad Norm 15.7181(8.7317) | Total Time 10.00(10.00)\n",
      "Iter 6730 | Time 11.2905(11.3146) | Bit/dim 1.1093(1.0889) | Xent 0.0039(0.0059) | Loss 1.1112(1.0919) | Error 0.0011(0.0019) Steps 548(552.01) | Grad Norm 9.5118(8.8313) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0102 | Time 50.7974, Epoch Time 813.3200(779.2393), Bit/dim 1.0743(best: 1.0671), Xent 0.0429, Loss 1.0957, Error 0.0100(best: 0.0081)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 6740 | Time 10.9984(11.3253) | Bit/dim 1.0754(1.0876) | Xent 0.0147(0.0060) | Loss 1.0828(1.0906) | Error 0.0044(0.0021) Steps 554(553.44) | Grad Norm 11.0432(8.3900) | Total Time 10.00(10.00)\n",
      "Iter 6750 | Time 11.3220(11.3441) | Bit/dim 1.0656(1.0847) | Xent 0.0029(0.0054) | Loss 1.0670(1.0874) | Error 0.0011(0.0019) Steps 560(552.91) | Grad Norm 8.0709(7.9232) | Total Time 10.00(10.00)\n",
      "Iter 6760 | Time 11.4460(11.3580) | Bit/dim 1.0748(1.0838) | Xent 0.0036(0.0048) | Loss 1.0766(1.0862) | Error 0.0011(0.0017) Steps 548(553.10) | Grad Norm 2.6505(7.6842) | Total Time 10.00(10.00)\n",
      "Iter 6770 | Time 11.2524(11.3490) | Bit/dim 1.0741(1.0816) | Xent 0.0047(0.0048) | Loss 1.0764(1.0840) | Error 0.0011(0.0017) Steps 554(552.63) | Grad Norm 2.5043(7.1552) | Total Time 10.00(10.00)\n",
      "Iter 6780 | Time 11.3605(11.2511) | Bit/dim 1.0730(1.0784) | Xent 0.0028(0.0049) | Loss 1.0744(1.0809) | Error 0.0011(0.0017) Steps 560(549.00) | Grad Norm 4.6625(6.2412) | Total Time 10.00(10.00)\n",
      "Iter 6790 | Time 11.4041(11.2835) | Bit/dim 1.0604(1.0752) | Xent 0.0082(0.0052) | Loss 1.0646(1.0778) | Error 0.0022(0.0018) Steps 542(548.90) | Grad Norm 2.6802(5.0864) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0103 | Time 49.9299, Epoch Time 810.3036(780.1712), Bit/dim 1.0609(best: 1.0671), Xent 0.0496, Loss 1.0857, Error 0.0102(best: 0.0081)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 6800 | Time 11.2144(11.2689) | Bit/dim 1.0537(1.0730) | Xent 0.0082(0.0048) | Loss 1.0578(1.0754) | Error 0.0022(0.0016) Steps 554(549.31) | Grad Norm 1.3393(4.2081) | Total Time 10.00(10.00)\n",
      "Iter 6810 | Time 11.6458(11.2526) | Bit/dim 1.0721(1.0709) | Xent 0.0033(0.0045) | Loss 1.0737(1.0732) | Error 0.0022(0.0015) Steps 548(548.35) | Grad Norm 1.8793(4.2859) | Total Time 10.00(10.00)\n",
      "Iter 6820 | Time 11.9842(11.3256) | Bit/dim 1.0575(1.0702) | Xent 0.0031(0.0039) | Loss 1.0590(1.0722) | Error 0.0011(0.0013) Steps 554(549.44) | Grad Norm 7.0846(4.5588) | Total Time 10.00(10.00)\n",
      "Iter 6830 | Time 12.4548(11.3562) | Bit/dim 1.1286(1.0728) | Xent 0.0152(0.0043) | Loss 1.1363(1.0749) | Error 0.0078(0.0016) Steps 572(548.41) | Grad Norm 38.9887(6.0158) | Total Time 10.00(10.00)\n",
      "Iter 6840 | Time 11.4179(11.3629) | Bit/dim 1.1467(1.0997) | Xent 0.0065(0.0048) | Loss 1.1499(1.1021) | Error 0.0022(0.0017) Steps 554(549.95) | Grad Norm 8.0899(6.6078) | Total Time 10.00(10.00)\n",
      "Iter 6850 | Time 10.9244(11.3698) | Bit/dim 1.1020(1.1012) | Xent 0.0055(0.0054) | Loss 1.1048(1.1039) | Error 0.0011(0.0020) Steps 560(554.28) | Grad Norm 1.3769(6.0554) | Total Time 10.00(10.00)\n",
      "Iter 6860 | Time 11.7390(11.3735) | Bit/dim 1.1029(1.1003) | Xent 0.0039(0.0060) | Loss 1.1049(1.1033) | Error 0.0022(0.0022) Steps 548(556.68) | Grad Norm 9.0969(7.0354) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0104 | Time 49.6014, Epoch Time 814.2998(781.1951), Bit/dim 1.0751(best: 1.0609), Xent 0.0591, Loss 1.1047, Error 0.0121(best: 0.0081)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 6870 | Time 11.4423(11.3374) | Bit/dim 1.0982(1.0953) | Xent 0.0041(0.0062) | Loss 1.1002(1.0984) | Error 0.0022(0.0022) Steps 554(555.21) | Grad Norm 6.7380(7.2093) | Total Time 10.00(10.00)\n",
      "Iter 6880 | Time 11.4765(11.2856) | Bit/dim 1.0569(1.0883) | Xent 0.0062(0.0064) | Loss 1.0599(1.0915) | Error 0.0022(0.0022) Steps 530(552.95) | Grad Norm 0.8998(6.5583) | Total Time 10.00(10.00)\n",
      "Iter 6890 | Time 11.3567(11.3448) | Bit/dim 1.1530(1.0933) | Xent 0.0104(0.0063) | Loss 1.1582(1.0965) | Error 0.0044(0.0022) Steps 560(553.94) | Grad Norm 11.7898(7.9717) | Total Time 10.00(10.00)\n",
      "Iter 6900 | Time 11.0716(11.3846) | Bit/dim 1.1246(1.0995) | Xent 0.0023(0.0063) | Loss 1.1257(1.1026) | Error 0.0011(0.0021) Steps 554(557.54) | Grad Norm 8.7879(8.5537) | Total Time 10.00(10.00)\n",
      "Iter 6910 | Time 11.0201(11.3673) | Bit/dim 1.0958(1.0974) | Xent 0.0006(0.0063) | Loss 1.0961(1.1005) | Error 0.0000(0.0021) Steps 548(557.48) | Grad Norm 4.6644(7.9851) | Total Time 10.00(10.00)\n",
      "Iter 6920 | Time 11.0180(11.2825) | Bit/dim 1.0609(1.0926) | Xent 0.0189(0.0070) | Loss 1.0703(1.0961) | Error 0.0067(0.0024) Steps 548(556.17) | Grad Norm 3.7143(7.1993) | Total Time 10.00(10.00)\n",
      "Iter 6930 | Time 10.7809(11.2441) | Bit/dim 1.0673(1.0879) | Xent 0.0064(0.0068) | Loss 1.0706(1.0913) | Error 0.0033(0.0023) Steps 524(553.29) | Grad Norm 1.4076(6.4550) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0105 | Time 49.8428, Epoch Time 809.2451(782.0366), Bit/dim 1.0658(best: 1.0609), Xent 0.0534, Loss 1.0925, Error 0.0104(best: 0.0081)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 6940 | Time 11.1619(11.2178) | Bit/dim 1.0644(1.0823) | Xent 0.0006(0.0062) | Loss 1.0647(1.0854) | Error 0.0000(0.0021) Steps 554(550.24) | Grad Norm 0.9547(5.2747) | Total Time 10.00(10.00)\n",
      "Iter 6950 | Time 11.2720(11.2395) | Bit/dim 1.0751(1.0786) | Xent 0.0125(0.0060) | Loss 1.0814(1.0816) | Error 0.0056(0.0021) Steps 566(550.77) | Grad Norm 0.8991(4.4464) | Total Time 10.00(10.00)\n",
      "Iter 6960 | Time 10.6550(11.2960) | Bit/dim 1.1710(1.0793) | Xent 0.0030(0.0059) | Loss 1.1725(1.0823) | Error 0.0011(0.0019) Steps 536(552.57) | Grad Norm 12.8393(5.6743) | Total Time 10.00(10.00)\n",
      "Iter 6970 | Time 11.0359(11.3271) | Bit/dim 1.1564(1.0966) | Xent 0.0009(0.0060) | Loss 1.1568(1.0996) | Error 0.0000(0.0019) Steps 542(555.53) | Grad Norm 9.0083(6.9859) | Total Time 10.00(10.00)\n",
      "Iter 6980 | Time 11.4358(11.4194) | Bit/dim 1.0971(1.0984) | Xent 0.0044(0.0071) | Loss 1.0993(1.1019) | Error 0.0011(0.0023) Steps 566(560.23) | Grad Norm 6.5229(7.2633) | Total Time 10.00(10.00)\n",
      "Iter 6990 | Time 11.9678(11.4812) | Bit/dim 1.0715(1.0942) | Xent 0.0166(0.0074) | Loss 1.0798(1.0979) | Error 0.0056(0.0023) Steps 554(560.72) | Grad Norm 3.8633(6.6801) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0106 | Time 49.9525, Epoch Time 820.2281(783.1823), Bit/dim 1.0681(best: 1.0609), Xent 0.0573, Loss 1.0968, Error 0.0114(best: 0.0081)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 7000 | Time 11.7969(11.4655) | Bit/dim 1.0685(1.0897) | Xent 0.0026(0.0073) | Loss 1.0698(1.0934) | Error 0.0011(0.0024) Steps 554(560.20) | Grad Norm 3.6039(5.7896) | Total Time 10.00(10.00)\n",
      "Iter 7010 | Time 11.9697(11.4513) | Bit/dim 1.0656(1.0847) | Xent 0.0118(0.0073) | Loss 1.0715(1.0883) | Error 0.0044(0.0025) Steps 560(559.16) | Grad Norm 1.6932(5.0878) | Total Time 10.00(10.00)\n",
      "Iter 7020 | Time 11.4437(11.4704) | Bit/dim 1.0660(1.0802) | Xent 0.0133(0.0074) | Loss 1.0727(1.0840) | Error 0.0022(0.0024) Steps 566(557.48) | Grad Norm 3.3663(4.3138) | Total Time 10.00(10.00)\n",
      "Iter 7030 | Time 12.2042(11.5751) | Bit/dim 1.0559(1.0764) | Xent 0.0081(0.0070) | Loss 1.0600(1.0799) | Error 0.0022(0.0023) Steps 584(561.29) | Grad Norm 2.7401(4.1635) | Total Time 10.00(10.00)\n",
      "Iter 7040 | Time 11.5576(11.5472) | Bit/dim 1.0645(1.0726) | Xent 0.0036(0.0066) | Loss 1.0663(1.0759) | Error 0.0022(0.0022) Steps 560(560.20) | Grad Norm 5.2255(4.2177) | Total Time 10.00(10.00)\n",
      "Iter 7050 | Time 11.0312(11.5761) | Bit/dim 1.1591(1.0758) | Xent 0.0008(0.0064) | Loss 1.1595(1.0790) | Error 0.0000(0.0022) Steps 554(562.82) | Grad Norm 12.7594(5.5023) | Total Time 10.00(10.00)\n",
      "Iter 7060 | Time 11.6697(11.5993) | Bit/dim 1.1192(1.0966) | Xent 0.0091(0.0070) | Loss 1.1237(1.1001) | Error 0.0022(0.0023) Steps 566(564.08) | Grad Norm 3.9171(7.2218) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0107 | Time 49.7245, Epoch Time 828.9791(784.5562), Bit/dim 1.1114(best: 1.0609), Xent 0.0517, Loss 1.1373, Error 0.0092(best: 0.0081)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 7070 | Time 11.1078(11.5258) | Bit/dim 1.0910(1.0978) | Xent 0.0063(0.0073) | Loss 1.0942(1.1015) | Error 0.0022(0.0023) Steps 560(563.88) | Grad Norm 4.3179(6.9217) | Total Time 10.00(10.00)\n",
      "Iter 7080 | Time 11.7231(11.4953) | Bit/dim 1.0778(1.0937) | Xent 0.0032(0.0072) | Loss 1.0794(1.0973) | Error 0.0011(0.0023) Steps 566(561.62) | Grad Norm 7.6802(6.7100) | Total Time 10.00(10.00)\n",
      "Iter 7090 | Time 11.7691(11.4725) | Bit/dim 1.0863(1.0902) | Xent 0.0003(0.0063) | Loss 1.0864(1.0934) | Error 0.0000(0.0020) Steps 560(560.81) | Grad Norm 8.0949(6.7673) | Total Time 10.00(10.00)\n",
      "Iter 7100 | Time 11.2896(11.4711) | Bit/dim 1.0623(1.0839) | Xent 0.0046(0.0061) | Loss 1.0646(1.0869) | Error 0.0044(0.0021) Steps 542(559.23) | Grad Norm 4.8102(6.3114) | Total Time 10.00(10.00)\n",
      "Iter 7110 | Time 11.5054(11.4286) | Bit/dim 1.0655(1.0798) | Xent 0.0084(0.0060) | Loss 1.0697(1.0828) | Error 0.0022(0.0020) Steps 542(557.24) | Grad Norm 2.1533(5.4559) | Total Time 10.00(10.00)\n",
      "Iter 7120 | Time 11.9078(11.4024) | Bit/dim 1.0727(1.0760) | Xent 0.0073(0.0068) | Loss 1.0764(1.0794) | Error 0.0033(0.0022) Steps 572(556.66) | Grad Norm 6.8334(4.9785) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0108 | Time 49.2815, Epoch Time 815.3213(785.4792), Bit/dim 1.0973(best: 1.0609), Xent 0.0627, Loss 1.1286, Error 0.0127(best: 0.0081)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 7130 | Time 11.3875(11.4283) | Bit/dim 1.0658(1.0735) | Xent 0.0124(0.0071) | Loss 1.0720(1.0770) | Error 0.0044(0.0024) Steps 572(556.88) | Grad Norm 6.8755(5.4986) | Total Time 10.00(10.00)\n",
      "Iter 7140 | Time 11.5589(11.5108) | Bit/dim 1.0957(1.0944) | Xent 0.0034(0.0081) | Loss 1.0974(1.0984) | Error 0.0011(0.0026) Steps 572(560.65) | Grad Norm 2.4310(6.7999) | Total Time 10.00(10.00)\n",
      "Iter 7150 | Time 11.6114(11.5148) | Bit/dim 1.0872(1.0977) | Xent 0.0265(0.0087) | Loss 1.1005(1.1020) | Error 0.0056(0.0028) Steps 560(562.92) | Grad Norm 2.5630(7.1906) | Total Time 10.00(10.00)\n",
      "Iter 7160 | Time 11.5087(11.4674) | Bit/dim 1.0824(1.0936) | Xent 0.0061(0.0082) | Loss 1.0854(1.0977) | Error 0.0022(0.0027) Steps 566(561.51) | Grad Norm 4.3930(7.0484) | Total Time 10.00(10.00)\n",
      "Iter 7170 | Time 11.2030(11.4841) | Bit/dim 1.0695(1.0874) | Xent 0.0142(0.0077) | Loss 1.0766(1.0913) | Error 0.0044(0.0025) Steps 566(562.40) | Grad Norm 7.1331(7.1553) | Total Time 10.00(10.00)\n",
      "Iter 7180 | Time 11.7932(11.4868) | Bit/dim 1.0799(1.0830) | Xent 0.0030(0.0073) | Loss 1.0814(1.0867) | Error 0.0011(0.0023) Steps 554(559.49) | Grad Norm 1.7790(6.7573) | Total Time 10.00(10.00)\n",
      "Iter 7190 | Time 11.5417(11.5522) | Bit/dim 1.1437(1.0858) | Xent 0.0088(0.0065) | Loss 1.1481(1.0891) | Error 0.0011(0.0020) Steps 560(563.40) | Grad Norm 11.3060(7.9050) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0109 | Time 49.5876, Epoch Time 828.0875(786.7574), Bit/dim 1.0875(best: 1.0609), Xent 0.0476, Loss 1.1114, Error 0.0097(best: 0.0081)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 7200 | Time 11.1792(11.5815) | Bit/dim 1.0751(1.0873) | Xent 0.0022(0.0058) | Loss 1.0762(1.0901) | Error 0.0011(0.0018) Steps 560(564.38) | Grad Norm 3.6593(8.2965) | Total Time 10.00(10.00)\n",
      "Iter 7210 | Time 11.6080(11.5833) | Bit/dim 1.0644(1.0848) | Xent 0.0005(0.0051) | Loss 1.0646(1.0873) | Error 0.0000(0.0017) Steps 554(563.81) | Grad Norm 0.7840(7.7368) | Total Time 10.00(10.00)\n",
      "Iter 7220 | Time 11.2907(11.5526) | Bit/dim 1.0819(1.0808) | Xent 0.0019(0.0050) | Loss 1.0828(1.0833) | Error 0.0011(0.0016) Steps 566(562.25) | Grad Norm 5.3424(6.9027) | Total Time 10.00(10.00)\n",
      "Iter 7230 | Time 11.2173(11.5680) | Bit/dim 1.0731(1.0783) | Xent 0.0023(0.0053) | Loss 1.0743(1.0810) | Error 0.0000(0.0017) Steps 560(563.51) | Grad Norm 8.1581(7.1846) | Total Time 10.00(10.00)\n",
      "Iter 7240 | Time 11.2111(11.5950) | Bit/dim 1.1013(1.0821) | Xent 0.0028(0.0050) | Loss 1.1028(1.0846) | Error 0.0000(0.0016) Steps 566(565.68) | Grad Norm 7.1967(7.9925) | Total Time 10.00(10.00)\n",
      "Iter 7250 | Time 12.7457(11.6061) | Bit/dim 1.0972(1.0837) | Xent 0.0049(0.0051) | Loss 1.0997(1.0863) | Error 0.0033(0.0017) Steps 590(567.30) | Grad Norm 11.7409(8.3414) | Total Time 10.00(10.00)\n",
      "Iter 7260 | Time 11.4344(11.5997) | Bit/dim 1.0740(1.0829) | Xent 0.0003(0.0045) | Loss 1.0742(1.0852) | Error 0.0000(0.0015) Steps 560(567.66) | Grad Norm 2.8949(7.9623) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0110 | Time 49.7730, Epoch Time 830.2485(788.0622), Bit/dim 1.0748(best: 1.0609), Xent 0.0418, Loss 1.0957, Error 0.0096(best: 0.0081)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 7270 | Time 11.5623(11.6056) | Bit/dim 1.0723(1.0798) | Xent 0.0220(0.0054) | Loss 1.0833(1.0825) | Error 0.0056(0.0016) Steps 566(567.17) | Grad Norm 2.8139(7.6243) | Total Time 10.00(10.00)\n",
      "Iter 7280 | Time 11.6362(11.6194) | Bit/dim 1.0662(1.0779) | Xent 0.0054(0.0056) | Loss 1.0689(1.0807) | Error 0.0011(0.0018) Steps 578(568.86) | Grad Norm 7.4799(7.6387) | Total Time 10.00(10.00)\n",
      "Iter 7290 | Time 11.6156(11.6069) | Bit/dim 1.0595(1.0757) | Xent 0.0064(0.0054) | Loss 1.0626(1.0784) | Error 0.0033(0.0017) Steps 584(570.22) | Grad Norm 7.2420(7.1490) | Total Time 10.00(10.00)\n",
      "Iter 7300 | Time 11.7710(11.6139) | Bit/dim 1.0477(1.0721) | Xent 0.0035(0.0054) | Loss 1.0494(1.0748) | Error 0.0022(0.0017) Steps 578(571.53) | Grad Norm 1.4978(6.6453) | Total Time 10.00(10.00)\n",
      "Iter 7310 | Time 11.6275(11.5912) | Bit/dim 1.0704(1.0704) | Xent 0.0059(0.0053) | Loss 1.0733(1.0730) | Error 0.0011(0.0017) Steps 566(569.13) | Grad Norm 3.5516(5.9746) | Total Time 10.00(10.00)\n",
      "Iter 7320 | Time 12.8135(11.6387) | Bit/dim 1.1538(1.0829) | Xent 0.0156(0.0055) | Loss 1.1616(1.0857) | Error 0.0044(0.0017) Steps 602(571.05) | Grad Norm 24.3753(7.8948) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0111 | Time 49.7531, Epoch Time 832.7894(789.4040), Bit/dim 1.0990(best: 1.0609), Xent 0.0487, Loss 1.1233, Error 0.0098(best: 0.0081)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 7330 | Time 12.3285(11.6376) | Bit/dim 1.0922(1.0888) | Xent 0.0134(0.0056) | Loss 1.0989(1.0917) | Error 0.0067(0.0018) Steps 596(572.70) | Grad Norm 11.1009(7.9523) | Total Time 10.00(10.00)\n",
      "Iter 7340 | Time 11.3256(11.5932) | Bit/dim 1.0670(1.0862) | Xent 0.0012(0.0059) | Loss 1.0676(1.0891) | Error 0.0000(0.0019) Steps 560(570.94) | Grad Norm 2.5063(6.9426) | Total Time 10.00(10.00)\n",
      "Iter 7350 | Time 11.5505(11.5661) | Bit/dim 1.0578(1.0823) | Xent 0.0053(0.0053) | Loss 1.0605(1.0849) | Error 0.0033(0.0018) Steps 560(569.52) | Grad Norm 4.3489(6.0818) | Total Time 10.00(10.00)\n",
      "Iter 7360 | Time 12.3906(11.5450) | Bit/dim 1.0387(1.0761) | Xent 0.0127(0.0052) | Loss 1.0450(1.0787) | Error 0.0044(0.0017) Steps 584(569.52) | Grad Norm 1.1736(5.1600) | Total Time 10.00(10.00)\n",
      "Iter 7370 | Time 11.4412(11.5303) | Bit/dim 1.0607(1.0723) | Xent 0.0009(0.0053) | Loss 1.0612(1.0750) | Error 0.0000(0.0017) Steps 566(568.86) | Grad Norm 1.3561(4.8335) | Total Time 10.00(10.00)\n",
      "Iter 7380 | Time 11.4095(11.6042) | Bit/dim 1.1163(1.0827) | Xent 0.0021(0.0059) | Loss 1.1173(1.0857) | Error 0.0022(0.0020) Steps 578(570.94) | Grad Norm 8.9175(6.7638) | Total Time 10.00(10.00)\n",
      "Iter 7390 | Time 11.0730(11.6652) | Bit/dim 1.0789(1.0874) | Xent 0.0009(0.0059) | Loss 1.0794(1.0904) | Error 0.0000(0.0019) Steps 566(571.51) | Grad Norm 8.0692(7.2931) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0112 | Time 50.0895, Epoch Time 831.1288(790.6557), Bit/dim 1.0696(best: 1.0609), Xent 0.0562, Loss 1.0977, Error 0.0130(best: 0.0081)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 7400 | Time 11.1854(11.6673) | Bit/dim 1.0718(1.0851) | Xent 0.0037(0.0065) | Loss 1.0737(1.0883) | Error 0.0022(0.0020) Steps 578(571.46) | Grad Norm 5.8280(6.8218) | Total Time 10.00(10.00)\n",
      "Iter 7410 | Time 11.4511(11.6361) | Bit/dim 1.0673(1.0796) | Xent 0.0016(0.0053) | Loss 1.0681(1.0823) | Error 0.0000(0.0016) Steps 578(571.84) | Grad Norm 1.3993(5.7142) | Total Time 10.00(10.00)\n",
      "Iter 7420 | Time 11.6108(11.6534) | Bit/dim 1.0593(1.0746) | Xent 0.0034(0.0049) | Loss 1.0610(1.0770) | Error 0.0011(0.0015) Steps 578(572.28) | Grad Norm 6.3992(5.0060) | Total Time 10.00(10.00)\n",
      "Iter 7430 | Time 11.9213(11.6866) | Bit/dim 1.1303(1.0780) | Xent 0.0111(0.0046) | Loss 1.1358(1.0803) | Error 0.0033(0.0015) Steps 590(572.56) | Grad Norm 26.8789(6.8014) | Total Time 10.00(10.00)\n",
      "Iter 7440 | Time 11.6783(11.6828) | Bit/dim 1.0685(1.0810) | Xent 0.0009(0.0047) | Loss 1.0690(1.0834) | Error 0.0000(0.0015) Steps 584(573.86) | Grad Norm 6.6099(7.4327) | Total Time 10.00(10.00)\n",
      "Iter 7450 | Time 11.4201(11.6774) | Bit/dim 1.0566(1.0788) | Xent 0.0005(0.0040) | Loss 1.0568(1.0808) | Error 0.0000(0.0013) Steps 560(573.12) | Grad Norm 6.2341(7.1064) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0113 | Time 50.3099, Epoch Time 834.3750(791.9673), Bit/dim 1.0551(best: 1.0609), Xent 0.0423, Loss 1.0763, Error 0.0097(best: 0.0081)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 7460 | Time 11.2369(11.6535) | Bit/dim 1.0583(1.0747) | Xent 0.0046(0.0036) | Loss 1.0606(1.0765) | Error 0.0022(0.0013) Steps 578(572.56) | Grad Norm 5.9224(6.3613) | Total Time 10.00(10.00)\n",
      "Iter 7470 | Time 11.7649(11.6702) | Bit/dim 1.0629(1.0710) | Xent 0.0018(0.0032) | Loss 1.0638(1.0727) | Error 0.0000(0.0011) Steps 578(574.04) | Grad Norm 1.3943(5.3956) | Total Time 10.00(10.00)\n",
      "Iter 7480 | Time 11.3325(11.6477) | Bit/dim 1.0441(1.0662) | Xent 0.0053(0.0030) | Loss 1.0467(1.0678) | Error 0.0011(0.0011) Steps 572(573.19) | Grad Norm 2.0283(4.4018) | Total Time 10.00(10.00)\n",
      "Iter 7490 | Time 11.5296(11.6798) | Bit/dim 1.0611(1.0644) | Xent 0.0060(0.0034) | Loss 1.0641(1.0661) | Error 0.0022(0.0012) Steps 590(574.29) | Grad Norm 1.8063(4.1626) | Total Time 10.00(10.00)\n",
      "Iter 7500 | Time 12.5168(11.7393) | Bit/dim 1.1085(1.0792) | Xent 0.0055(0.0046) | Loss 1.1112(1.0815) | Error 0.0011(0.0015) Steps 596(574.91) | Grad Norm 13.7419(6.2734) | Total Time 10.00(10.00)\n",
      "Iter 7510 | Time 11.6647(11.7386) | Bit/dim 1.0888(1.0858) | Xent 0.0223(0.0070) | Loss 1.1000(1.0893) | Error 0.0056(0.0023) Steps 572(576.40) | Grad Norm 7.7667(6.6506) | Total Time 10.00(10.00)\n",
      "Iter 7520 | Time 11.8523(11.7366) | Bit/dim 1.0735(1.0850) | Xent 0.0058(0.0074) | Loss 1.0764(1.0887) | Error 0.0022(0.0023) Steps 578(577.90) | Grad Norm 6.0317(6.5714) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0114 | Time 50.0368, Epoch Time 838.8949(793.3751), Bit/dim 1.0789(best: 1.0551), Xent 0.0584, Loss 1.1081, Error 0.0107(best: 0.0081)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 7530 | Time 11.6477(11.7143) | Bit/dim 1.0680(1.0831) | Xent 0.0039(0.0079) | Loss 1.0700(1.0871) | Error 0.0011(0.0025) Steps 578(575.04) | Grad Norm 5.8660(6.5043) | Total Time 10.00(10.00)\n",
      "Iter 7540 | Time 11.8385(11.6928) | Bit/dim 1.0715(1.0788) | Xent 0.0005(0.0071) | Loss 1.0718(1.0824) | Error 0.0000(0.0023) Steps 554(575.00) | Grad Norm 1.8918(5.6313) | Total Time 10.00(10.00)\n",
      "Iter 7550 | Time 11.8395(11.6775) | Bit/dim 1.0794(1.0740) | Xent 0.0034(0.0065) | Loss 1.0811(1.0772) | Error 0.0011(0.0021) Steps 572(574.79) | Grad Norm 1.0605(4.5434) | Total Time 10.00(10.00)\n",
      "Iter 7560 | Time 11.1718(11.6839) | Bit/dim 1.0617(1.0698) | Xent 0.0049(0.0069) | Loss 1.0641(1.0732) | Error 0.0033(0.0021) Steps 572(575.51) | Grad Norm 3.8991(4.0578) | Total Time 10.00(10.00)\n",
      "Iter 7570 | Time 11.9904(11.6853) | Bit/dim 1.0490(1.0682) | Xent 0.0067(0.0063) | Loss 1.0523(1.0714) | Error 0.0022(0.0019) Steps 572(575.21) | Grad Norm 5.3059(4.5114) | Total Time 10.00(10.00)\n",
      "Iter 7580 | Time 13.0007(11.7487) | Bit/dim 1.1861(1.0802) | Xent 0.0470(0.0073) | Loss 1.2096(1.0839) | Error 0.0111(0.0020) Steps 596(577.52) | Grad Norm 33.7041(6.5555) | Total Time 10.00(10.00)\n",
      "Iter 7590 | Time 11.8367(11.7481) | Bit/dim 1.1311(1.1000) | Xent 0.0028(0.0070) | Loss 1.1325(1.1035) | Error 0.0011(0.0021) Steps 578(578.09) | Grad Norm 6.5823(6.9003) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0115 | Time 48.7865, Epoch Time 837.2413(794.6911), Bit/dim 1.1099(best: 1.0551), Xent 0.0542, Loss 1.1370, Error 0.0111(best: 0.0081)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 7600 | Time 11.7764(11.7610) | Bit/dim 1.0954(1.1008) | Xent 0.0002(0.0081) | Loss 1.0955(1.1048) | Error 0.0000(0.0024) Steps 584(578.27) | Grad Norm 6.2571(6.8950) | Total Time 10.00(10.00)\n",
      "Iter 7610 | Time 11.4202(11.7012) | Bit/dim 1.0642(1.0943) | Xent 0.0007(0.0070) | Loss 1.0646(1.0978) | Error 0.0000(0.0021) Steps 572(576.78) | Grad Norm 1.9379(6.4354) | Total Time 10.00(10.00)\n",
      "Iter 7620 | Time 12.0191(11.6773) | Bit/dim 1.0579(1.0867) | Xent 0.0012(0.0059) | Loss 1.0585(1.0896) | Error 0.0000(0.0018) Steps 566(575.44) | Grad Norm 2.6836(5.7967) | Total Time 10.00(10.00)\n",
      "Iter 7630 | Time 11.1216(11.6620) | Bit/dim 1.0600(1.0801) | Xent 0.0050(0.0053) | Loss 1.0625(1.0828) | Error 0.0033(0.0017) Steps 566(575.33) | Grad Norm 3.1222(4.9751) | Total Time 10.00(10.00)\n",
      "Iter 7640 | Time 11.8973(11.6715) | Bit/dim 1.0534(1.0751) | Xent 0.0035(0.0049) | Loss 1.0551(1.0775) | Error 0.0022(0.0016) Steps 590(576.48) | Grad Norm 2.2558(4.1720) | Total Time 10.00(10.00)\n",
      "Iter 7650 | Time 11.9170(11.7460) | Bit/dim 1.1347(1.0802) | Xent 0.0011(0.0050) | Loss 1.1353(1.0827) | Error 0.0011(0.0016) Steps 590(578.98) | Grad Norm 8.1764(5.6217) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0116 | Time 52.2833, Epoch Time 840.2665(796.0584), Bit/dim 1.1210(best: 1.0551), Xent 0.0635, Loss 1.1527, Error 0.0143(best: 0.0081)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 7660 | Time 11.8789(11.7550) | Bit/dim 1.1200(1.0911) | Xent 0.0113(0.0066) | Loss 1.1257(1.0944) | Error 0.0022(0.0021) Steps 566(578.80) | Grad Norm 7.6127(6.9786) | Total Time 10.00(10.00)\n",
      "Iter 7670 | Time 11.2638(11.7362) | Bit/dim 1.0816(1.0907) | Xent 0.0038(0.0066) | Loss 1.0835(1.0940) | Error 0.0022(0.0022) Steps 584(581.40) | Grad Norm 1.3810(6.8613) | Total Time 10.00(10.00)\n",
      "Iter 7680 | Time 11.7651(11.7429) | Bit/dim 1.0761(1.0871) | Xent 0.0059(0.0067) | Loss 1.0790(1.0904) | Error 0.0022(0.0022) Steps 596(581.77) | Grad Norm 5.0458(6.9386) | Total Time 10.00(10.00)\n",
      "Iter 7690 | Time 11.8131(11.7386) | Bit/dim 1.0620(1.0830) | Xent 0.0097(0.0071) | Loss 1.0668(1.0865) | Error 0.0033(0.0025) Steps 596(582.03) | Grad Norm 4.9239(6.7275) | Total Time 10.00(10.00)\n",
      "Iter 7700 | Time 11.7425(11.7836) | Bit/dim 1.0728(1.0796) | Xent 0.0022(0.0064) | Loss 1.0739(1.0828) | Error 0.0011(0.0022) Steps 572(581.23) | Grad Norm 1.3443(6.0214) | Total Time 10.00(10.00)\n",
      "Iter 7710 | Time 11.0701(11.7796) | Bit/dim 1.0719(1.0780) | Xent 0.0013(0.0062) | Loss 1.0726(1.0811) | Error 0.0000(0.0021) Steps 560(579.72) | Grad Norm 6.5334(6.6270) | Total Time 10.00(10.00)\n",
      "Iter 7720 | Time 11.0947(11.7891) | Bit/dim 1.0587(1.0747) | Xent 0.0070(0.0060) | Loss 1.0623(1.0777) | Error 0.0022(0.0020) Steps 578(580.10) | Grad Norm 3.6576(6.5395) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0117 | Time 50.1488, Epoch Time 843.9793(797.4960), Bit/dim 1.0570(best: 1.0551), Xent 0.0449, Loss 1.0794, Error 0.0096(best: 0.0081)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 7730 | Time 12.9112(11.8004) | Bit/dim 1.0728(1.0723) | Xent 0.0011(0.0051) | Loss 1.0734(1.0748) | Error 0.0000(0.0018) Steps 626(580.94) | Grad Norm 14.4036(7.0115) | Total Time 10.00(10.00)\n",
      "Iter 7740 | Time 11.2306(11.7328) | Bit/dim 1.0559(1.0691) | Xent 0.0038(0.0047) | Loss 1.0578(1.0715) | Error 0.0011(0.0016) Steps 566(579.17) | Grad Norm 4.7803(6.5663) | Total Time 10.00(10.00)\n",
      "Iter 7750 | Time 12.6264(11.7769) | Bit/dim 1.0764(1.0696) | Xent 0.0210(0.0048) | Loss 1.0869(1.0720) | Error 0.0056(0.0015) Steps 608(579.92) | Grad Norm 18.7774(7.3965) | Total Time 10.00(10.00)\n",
      "Iter 7760 | Time 11.0657(11.7772) | Bit/dim 1.0843(1.0747) | Xent 0.0046(0.0048) | Loss 1.0866(1.0771) | Error 0.0033(0.0015) Steps 578(581.64) | Grad Norm 7.4502(8.0382) | Total Time 10.00(10.00)\n",
      "Iter 7770 | Time 11.6616(11.7458) | Bit/dim 1.0775(1.0746) | Xent 0.0003(0.0043) | Loss 1.0777(1.0768) | Error 0.0000(0.0013) Steps 566(581.13) | Grad Norm 4.7200(7.8208) | Total Time 10.00(10.00)\n",
      "Iter 7780 | Time 11.8443(11.7432) | Bit/dim 1.0688(1.0712) | Xent 0.0004(0.0037) | Loss 1.0690(1.0730) | Error 0.0000(0.0011) Steps 572(579.81) | Grad Norm 4.3935(6.9781) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0118 | Time 50.0952, Epoch Time 839.1493(798.7456), Bit/dim 1.0510(best: 1.0551), Xent 0.0474, Loss 1.0747, Error 0.0103(best: 0.0081)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 7790 | Time 11.3876(11.7209) | Bit/dim 1.0510(1.0658) | Xent 0.0008(0.0039) | Loss 1.0513(1.0677) | Error 0.0000(0.0012) Steps 590(579.37) | Grad Norm 5.2313(6.1029) | Total Time 10.00(10.00)\n",
      "Iter 7800 | Time 11.2650(11.7844) | Bit/dim 1.0481(1.0651) | Xent 0.0004(0.0035) | Loss 1.0483(1.0669) | Error 0.0000(0.0010) Steps 578(583.13) | Grad Norm 6.4287(6.5379) | Total Time 10.00(10.00)\n",
      "Iter 7810 | Time 11.7507(11.7413) | Bit/dim 1.0467(1.0613) | Xent 0.0004(0.0031) | Loss 1.0469(1.0628) | Error 0.0000(0.0010) Steps 578(582.10) | Grad Norm 1.4927(5.6351) | Total Time 10.00(10.00)\n",
      "Iter 7820 | Time 11.3123(11.7628) | Bit/dim 1.1155(1.0628) | Xent 0.0035(0.0032) | Loss 1.1172(1.0644) | Error 0.0011(0.0011) Steps 566(582.81) | Grad Norm 12.7257(6.3706) | Total Time 10.00(10.00)\n",
      "Iter 7830 | Time 11.6412(11.8343) | Bit/dim 1.0995(1.0722) | Xent 0.0058(0.0037) | Loss 1.1024(1.0740) | Error 0.0011(0.0012) Steps 572(583.95) | Grad Norm 6.9490(7.3940) | Total Time 10.00(10.00)\n",
      "Iter 7840 | Time 12.6195(11.8487) | Bit/dim 1.0873(1.0735) | Xent 0.0012(0.0039) | Loss 1.0879(1.0755) | Error 0.0000(0.0013) Steps 602(584.63) | Grad Norm 13.5868(7.6919) | Total Time 10.00(10.00)\n",
      "Iter 7850 | Time 11.6807(11.7971) | Bit/dim 1.0702(1.0717) | Xent 0.0053(0.0037) | Loss 1.0729(1.0735) | Error 0.0022(0.0013) Steps 590(584.86) | Grad Norm 5.6758(7.0415) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0119 | Time 50.7899, Epoch Time 845.3607(800.1441), Bit/dim 1.0561(best: 1.0510), Xent 0.0463, Loss 1.0793, Error 0.0104(best: 0.0081)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 7860 | Time 11.2399(11.7436) | Bit/dim 1.0502(1.0681) | Xent 0.0011(0.0036) | Loss 1.0507(1.0699) | Error 0.0000(0.0012) Steps 572(583.44) | Grad Norm 4.3801(6.3906) | Total Time 10.00(10.00)\n",
      "Iter 7870 | Time 11.2246(11.7101) | Bit/dim 1.0435(1.0647) | Xent 0.0106(0.0039) | Loss 1.0488(1.0666) | Error 0.0033(0.0013) Steps 572(581.18) | Grad Norm 4.0988(5.4171) | Total Time 10.00(10.00)\n",
      "Iter 7880 | Time 11.0999(11.6606) | Bit/dim 1.0820(1.0638) | Xent 0.0006(0.0045) | Loss 1.0823(1.0661) | Error 0.0000(0.0013) Steps 572(579.28) | Grad Norm 10.5077(5.5159) | Total Time 10.00(10.00)\n",
      "Iter 7890 | Time 11.9362(11.7327) | Bit/dim 1.1152(1.0787) | Xent 0.0039(0.0051) | Loss 1.1172(1.0813) | Error 0.0011(0.0015) Steps 590(581.37) | Grad Norm 6.8528(6.7640) | Total Time 10.00(10.00)\n",
      "Iter 7900 | Time 12.1597(11.7593) | Bit/dim 1.0922(1.0782) | Xent 0.0025(0.0060) | Loss 1.0935(1.0812) | Error 0.0011(0.0019) Steps 584(581.91) | Grad Norm 7.9868(6.4462) | Total Time 10.00(10.00)\n",
      "Iter 7910 | Time 11.7919(11.8203) | Bit/dim 1.0566(1.0758) | Xent 0.0163(0.0072) | Loss 1.0647(1.0794) | Error 0.0033(0.0023) Steps 578(583.29) | Grad Norm 6.3276(6.8863) | Total Time 10.00(10.00)\n",
      "Iter 7920 | Time 11.9501(11.8316) | Bit/dim 1.0789(1.0752) | Xent 0.0040(0.0074) | Loss 1.0809(1.0789) | Error 0.0022(0.0023) Steps 596(585.10) | Grad Norm 6.5723(7.4145) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0120 | Time 49.1656, Epoch Time 841.3242(801.3795), Bit/dim 1.0699(best: 1.0510), Xent 0.0532, Loss 1.0965, Error 0.0093(best: 0.0081)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 7930 | Time 12.3159(11.8535) | Bit/dim 1.0562(1.0733) | Xent 0.0023(0.0073) | Loss 1.0574(1.0770) | Error 0.0022(0.0024) Steps 608(585.40) | Grad Norm 10.9417(7.4057) | Total Time 10.00(10.00)\n",
      "Iter 7940 | Time 12.4789(11.9175) | Bit/dim 1.0751(1.0701) | Xent 0.0029(0.0068) | Loss 1.0765(1.0735) | Error 0.0011(0.0022) Steps 620(586.62) | Grad Norm 11.0094(6.9861) | Total Time 10.00(10.00)\n",
      "Iter 7950 | Time 11.9667(11.9178) | Bit/dim 1.0714(1.0669) | Xent 0.0001(0.0060) | Loss 1.0714(1.0699) | Error 0.0000(0.0020) Steps 572(586.39) | Grad Norm 6.3812(6.5791) | Total Time 10.00(10.00)\n",
      "Iter 7960 | Time 11.5370(11.9361) | Bit/dim 1.1078(1.0719) | Xent 0.0001(0.0056) | Loss 1.1079(1.0747) | Error 0.0000(0.0020) Steps 566(586.02) | Grad Norm 10.3135(7.8582) | Total Time 10.00(10.00)\n",
      "Iter 7970 | Time 12.1215(11.9513) | Bit/dim 1.1090(1.0836) | Xent 0.0066(0.0058) | Loss 1.1123(1.0865) | Error 0.0022(0.0020) Steps 590(586.54) | Grad Norm 12.1095(7.9041) | Total Time 10.00(10.00)\n",
      "Iter 7980 | Time 11.7991(11.9132) | Bit/dim 1.0784(1.0825) | Xent 0.0158(0.0068) | Loss 1.0863(1.0859) | Error 0.0033(0.0021) Steps 578(585.92) | Grad Norm 7.2614(7.3868) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0121 | Time 50.0410, Epoch Time 852.9934(802.9279), Bit/dim 1.0563(best: 1.0510), Xent 0.0594, Loss 1.0860, Error 0.0124(best: 0.0081)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 7990 | Time 11.4170(11.8507) | Bit/dim 1.0564(1.0779) | Xent 0.0066(0.0061) | Loss 1.0597(1.0810) | Error 0.0033(0.0018) Steps 584(584.80) | Grad Norm 6.4116(6.6133) | Total Time 10.00(10.00)\n",
      "Iter 8000 | Time 11.4535(11.7910) | Bit/dim 1.0580(1.0732) | Xent 0.0068(0.0054) | Loss 1.0614(1.0760) | Error 0.0022(0.0017) Steps 596(584.96) | Grad Norm 2.2061(5.7849) | Total Time 10.00(10.00)\n",
      "Iter 8010 | Time 11.4750(11.8165) | Bit/dim 1.0700(1.0692) | Xent 0.0005(0.0048) | Loss 1.0703(1.0716) | Error 0.0000(0.0016) Steps 566(584.44) | Grad Norm 10.8488(5.7034) | Total Time 10.00(10.00)\n",
      "Iter 8020 | Time 11.5723(11.8112) | Bit/dim 1.0878(1.0783) | Xent 0.0006(0.0053) | Loss 1.0881(1.0809) | Error 0.0000(0.0016) Steps 572(582.48) | Grad Norm 7.5594(7.2950) | Total Time 10.00(10.00)\n",
      "Iter 8030 | Time 11.7501(11.8200) | Bit/dim 1.0836(1.0793) | Xent 0.0025(0.0048) | Loss 1.0848(1.0817) | Error 0.0011(0.0014) Steps 572(583.03) | Grad Norm 6.5304(7.2272) | Total Time 10.00(10.00)\n",
      "Iter 8040 | Time 12.4964(11.8128) | Bit/dim 1.0660(1.0761) | Xent 0.0051(0.0048) | Loss 1.0685(1.0786) | Error 0.0033(0.0015) Steps 608(584.87) | Grad Norm 8.5992(6.7521) | Total Time 10.00(10.00)\n",
      "Iter 8050 | Time 11.4259(11.7450) | Bit/dim 1.0539(1.0722) | Xent 0.0051(0.0046) | Loss 1.0565(1.0745) | Error 0.0011(0.0014) Steps 584(584.32) | Grad Norm 3.1291(5.9457) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0122 | Time 50.3010, Epoch Time 839.8799(804.0365), Bit/dim 1.0483(best: 1.0510), Xent 0.0462, Loss 1.0714, Error 0.0097(best: 0.0081)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 8060 | Time 11.5284(11.7297) | Bit/dim 1.0432(1.0657) | Xent 0.0003(0.0038) | Loss 1.0433(1.0676) | Error 0.0000(0.0012) Steps 596(584.54) | Grad Norm 3.8922(5.0114) | Total Time 10.00(10.00)\n",
      "Iter 8070 | Time 12.2780(11.7629) | Bit/dim 1.0356(1.0611) | Xent 0.0017(0.0032) | Loss 1.0365(1.0627) | Error 0.0011(0.0010) Steps 590(584.52) | Grad Norm 2.9349(4.5990) | Total Time 10.00(10.00)\n",
      "Iter 8080 | Time 11.9470(11.7705) | Bit/dim 1.0443(1.0580) | Xent 0.0023(0.0028) | Loss 1.0454(1.0594) | Error 0.0011(0.0009) Steps 584(585.35) | Grad Norm 3.1674(4.2912) | Total Time 10.00(10.00)\n",
      "Iter 8090 | Time 11.4086(11.8189) | Bit/dim 1.0624(1.0558) | Xent 0.0074(0.0029) | Loss 1.0661(1.0572) | Error 0.0022(0.0009) Steps 572(586.15) | Grad Norm 10.6925(4.7027) | Total Time 10.00(10.00)\n",
      "Iter 8100 | Time 12.1582(11.8614) | Bit/dim 1.1071(1.0707) | Xent 0.0001(0.0037) | Loss 1.1072(1.0725) | Error 0.0000(0.0010) Steps 602(587.20) | Grad Norm 7.0815(6.4486) | Total Time 10.00(10.00)\n",
      "Iter 8110 | Time 13.3761(11.9849) | Bit/dim 1.0925(1.0764) | Xent 0.0132(0.0041) | Loss 1.0992(1.0784) | Error 0.0056(0.0013) Steps 614(589.52) | Grad Norm 14.9143(6.9377) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0123 | Time 49.5742, Epoch Time 852.5978(805.4933), Bit/dim 1.0686(best: 1.0483), Xent 0.0472, Loss 1.0922, Error 0.0099(best: 0.0081)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 8120 | Time 11.3298(11.9562) | Bit/dim 1.0952(1.0773) | Xent 0.0028(0.0046) | Loss 1.0966(1.0796) | Error 0.0011(0.0016) Steps 584(589.38) | Grad Norm 8.7147(7.3069) | Total Time 10.00(10.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf.py --data mnist --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/cnf_conditional_bs900_run1 --seed 1 --conditional True --controlled_tol False --train_mode semisup --log_freq 10 --weight_y 0.5\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
