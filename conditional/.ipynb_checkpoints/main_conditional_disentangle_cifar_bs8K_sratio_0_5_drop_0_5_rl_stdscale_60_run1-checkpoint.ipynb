{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_rl.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "GATES = [\"cnn1\", \"cnn2\", \"rnn\"]\n",
      "\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--gate', type=str, default='cnn1', choices=GATES)\n",
      "parser.add_argument('--scale', type=float, default=1.0)\n",
      "parser.add_argument('--scale_fac', type=float, default=1.0)\n",
      "parser.add_argument('--scale_std', type=float, default=1.0)\n",
      "parser.add_argument('--eta', default=0.1, type=float,\n",
      "                        help='tuning parameter that allows us to trade-off the competing goals of' \n",
      "                                'minimizing the prediction loss and maximizing the gate rewards ')\n",
      "parser.add_argument('--rl-weight', default=0.01, type=float,\n",
      "                        help='rl weight')\n",
      "\n",
      "parser.add_argument('--gamma', default=0.99, type=float,\n",
      "                        help='discount factor, default: (0.99)')\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "import lib.odenvp_conditional_rl as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol, \"scale\": args.scale, \"scale_fac\": args.scale_fac, \"scale_std\": args.scale_std, \"gate\": args.gate},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            # re-weight the gate rewards\n",
      "            normalized_eta = args.eta / len(logp_actions)\n",
      "            \n",
      "            # collect cumulative future rewards\n",
      "            R = - loss\n",
      "            cum_rewards = []\n",
      "            for r in nfe[::-1]:\n",
      "                R = -normalized_eta * r.view(-1,1) + args.gamma * R\n",
      "                cum_rewards.insert(0,R)\n",
      "            \n",
      "            # apply REINFORCE\n",
      "            rl_loss = 0\n",
      "            for lpa, r in zip(logp_actions, cum_rewards):\n",
      "                rl_loss = rl_loss - lpa.view(-1,1) * args.rl_weight * r\n",
      "                \n",
      "            loss = loss + rl_loss.mean()\n",
      "            \n",
      "            loss.backward()\n",
      "            \n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "            \n",
      "            for idx in range(len(model.module.transforms)):\n",
      "                for layer in model.module.transforms[idx].chain:\n",
      "                    if hasattr(layer, 'atol'):\n",
      "                        layer.odefunc.after_odeint()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'train': atol[tol_indx].mean()}, itr)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'train': rtol[tol_indx].mean()}, itr)\n",
      "                    \n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples, atol, rtol, logp_actions, nfe = model(fixed_z, reverse=True)\n",
      "            generated_samples = generated_samples.view(-1, *data_shape)\n",
      "            for tol_indx in range(len(atol)):\n",
      "                writer.add_scalars('atol_gen_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                writer.add_scalars('rtol_gen_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=0.0001, autoencode=False, batch_norm=False, batch_size=8000, batch_size_schedule='', begin_epoch=1, condition_ratio=0.5, conditional=True, controlled_tol=True, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, eta=0.1, gamma=0.99, gate='cnn1', imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=1, lr=0.01, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume=None, rl_weight=0.01, rtol=0.0001, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_rl_stdscale_60_run1', scale=1.0, scale_fac=1.0, scale_std=60.0, seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=5000, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000.0, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1450732\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc_rl.py:291: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Iter 0001 | Time 95.4405(95.4405) | Bit/dim 8.9617(8.9617) | Xent 2.3026(2.3026) | Loss 18.1612(18.1612) | Error 0.8982(0.8982) Steps 0(0.00) | Grad Norm 24.6379(24.6379) | Total Time 0.00(0.00)\n",
      "Iter 0002 | Time 37.1771(93.6926) | Bit/dim 8.8752(8.9591) | Xent 2.2924(2.3023) | Loss 17.9311(18.1543) | Error 0.7732(0.8945) Steps 0(0.00) | Grad Norm 22.2107(24.5651) | Total Time 0.00(0.00)\n",
      "Iter 0003 | Time 37.9353(92.0198) | Bit/dim 8.7792(8.9537) | Xent 2.2788(2.3016) | Loss 17.8177(18.1442) | Error 0.7688(0.8907) Steps 0(0.00) | Grad Norm 18.6858(24.3887) | Total Time 0.00(0.00)\n",
      "Iter 0004 | Time 35.0803(90.3117) | Bit/dim 8.7007(8.9461) | Xent 2.2612(2.3004) | Loss 17.1266(18.1137) | Error 0.7576(0.8867) Steps 0(0.00) | Grad Norm 13.9285(24.0749) | Total Time 0.00(0.00)\n",
      "Iter 0005 | Time 35.9790(88.6817) | Bit/dim 8.6243(8.9365) | Xent 2.2403(2.2986) | Loss 17.3136(18.0897) | Error 0.7534(0.8827) Steps 0(0.00) | Grad Norm 10.0178(23.6532) | Total Time 0.00(0.00)\n",
      "Iter 0006 | Time 37.1198(87.1348) | Bit/dim 8.5359(8.9244) | Xent 2.2224(2.2963) | Loss 17.0626(18.0589) | Error 0.7529(0.8788) Steps 0(0.00) | Grad Norm 7.5129(23.1690) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0001 | Time 34.7309, Epoch Time 329.8297(329.8297), Bit/dim 8.4975(best: inf), Xent 2.1969, Loss 9.5960, Error 0.7393(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0007 | Time 44.8702(85.8669) | Bit/dim 8.5288(8.9126) | Xent 2.1993(2.2934) | Loss 18.8970(18.0840) | Error 0.7560(0.8752) Steps 0(0.00) | Grad Norm 7.3845(22.6954) | Total Time 0.00(0.00)\n",
      "Iter 0008 | Time 36.4444(84.3842) | Bit/dim 8.4836(8.8997) | Xent 2.1766(2.2899) | Loss 17.1510(18.0560) | Error 0.7486(0.8714) Steps 0(0.00) | Grad Norm 9.6564(22.3043) | Total Time 0.00(0.00)\n",
      "Iter 0009 | Time 35.2099(82.9090) | Bit/dim 8.4526(8.8863) | Xent 2.1703(2.2863) | Loss 16.6445(18.0137) | Error 0.7634(0.8681) Steps 0(0.00) | Grad Norm 11.6470(21.9846) | Total Time 0.00(0.00)\n",
      "Iter 0010 | Time 32.8479(81.4071) | Bit/dim 8.4082(8.8719) | Xent 2.1579(2.2824) | Loss 16.8664(17.9793) | Error 0.7611(0.8649) Steps 0(0.00) | Grad Norm 13.3553(21.7257) | Total Time 0.00(0.00)\n",
      "Iter 0011 | Time 35.4789(80.0293) | Bit/dim 8.3455(8.8562) | Xent 2.1509(2.2785) | Loss 16.7592(17.9427) | Error 0.7539(0.8616) Steps 0(0.00) | Grad Norm 13.3563(21.4746) | Total Time 0.00(0.00)\n",
      "Iter 0012 | Time 36.7774(78.7317) | Bit/dim 8.2960(8.8393) | Xent 2.1449(2.2745) | Loss 16.7731(17.9076) | Error 0.7540(0.8583) Steps 0(0.00) | Grad Norm 12.4155(21.2028) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0002 | Time 17.8839, Epoch Time 255.3860(327.5964), Bit/dim 8.2164(best: 8.4975), Xent 2.1144, Loss 9.2735, Error 0.7340(best: 0.7393)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0013 | Time 37.9550(77.5084) | Bit/dim 8.1963(8.8201) | Xent 2.1190(2.2698) | Loss 17.9545(17.9090) | Error 0.7410(0.8548) Steps 0(0.00) | Grad Norm 9.7095(20.8580) | Total Time 0.00(0.00)\n",
      "Iter 0014 | Time 32.3865(76.1548) | Bit/dim 8.1819(8.8009) | Xent 2.1052(2.2649) | Loss 16.2676(17.8597) | Error 0.7155(0.8506) Steps 0(0.00) | Grad Norm 7.9287(20.4701) | Total Time 0.00(0.00)\n",
      "Iter 0015 | Time 33.1698(74.8652) | Bit/dim 8.0847(8.7794) | Xent 2.0848(2.2595) | Loss 15.7194(17.7955) | Error 0.7240(0.8468) Steps 0(0.00) | Grad Norm 5.8676(20.0321) | Total Time 0.00(0.00)\n",
      "Iter 0016 | Time 35.5577(73.6860) | Bit/dim 8.0415(8.7573) | Xent 2.0780(2.2540) | Loss 16.2943(17.7505) | Error 0.7288(0.8433) Steps 0(0.00) | Grad Norm 7.2152(19.6476) | Total Time 0.00(0.00)\n",
      "Iter 0017 | Time 35.1591(72.5302) | Bit/dim 7.9861(8.7342) | Xent 2.0789(2.2488) | Loss 15.9380(17.6961) | Error 0.7316(0.8400) Steps 0(0.00) | Grad Norm 9.2860(19.3367) | Total Time 0.00(0.00)\n",
      "Iter 0018 | Time 37.0795(71.4667) | Bit/dim 7.9063(8.7093) | Xent 2.0749(2.2436) | Loss 16.1599(17.6500) | Error 0.7358(0.8368) Steps 0(0.00) | Grad Norm 10.1687(19.0617) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0003 | Time 17.3409, Epoch Time 244.7238(325.1103), Bit/dim 7.8081(best: 8.2164), Xent 2.0457, Loss 8.8310, Error 0.7144(best: 0.7340)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0019 | Time 36.6636(70.4226) | Bit/dim 7.8184(8.6826) | Xent 2.0537(2.2379) | Loss 17.7599(17.6533) | Error 0.7220(0.8334) Steps 0(0.00) | Grad Norm 9.6752(18.7801) | Total Time 0.00(0.00)\n",
      "Iter 0020 | Time 38.8296(69.4748) | Bit/dim 7.7166(8.6536) | Xent 2.0349(2.2318) | Loss 15.6392(17.5929) | Error 0.6997(0.8294) Steps 0(0.00) | Grad Norm 7.2294(18.4336) | Total Time 0.00(0.00)\n",
      "Iter 0021 | Time 37.6601(68.5204) | Bit/dim 7.6235(8.6227) | Xent 2.0422(2.2261) | Loss 14.9934(17.5149) | Error 0.6966(0.8254) Steps 0(0.00) | Grad Norm 4.4646(18.0145) | Total Time 0.00(0.00)\n",
      "Iter 0022 | Time 35.5449(67.5311) | Bit/dim 7.5406(8.5902) | Xent 2.0297(2.2202) | Loss 15.3611(17.4503) | Error 0.6935(0.8214) Steps 0(0.00) | Grad Norm 5.1852(17.6296) | Total Time 0.00(0.00)\n",
      "Iter 0023 | Time 31.7769(66.4585) | Bit/dim 7.4688(8.5566) | Xent 2.0342(2.2146) | Loss 14.6835(17.3673) | Error 0.7010(0.8178) Steps 0(0.00) | Grad Norm 7.3002(17.3197) | Total Time 0.00(0.00)\n",
      "Iter 0024 | Time 34.9097(65.5120) | Bit/dim 7.4087(8.5222) | Xent 2.0338(2.2092) | Loss 15.2119(17.3026) | Error 0.6983(0.8142) Steps 0(0.00) | Grad Norm 8.4343(17.0532) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0004 | Time 17.7129, Epoch Time 249.3218(322.8366), Bit/dim 7.3435(best: 7.8081), Xent 2.0260, Loss 8.3565, Error 0.6933(best: 0.7144)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0025 | Time 39.5176(64.7322) | Bit/dim 7.3447(8.4868) | Xent 2.0327(2.2039) | Loss 16.9597(17.2924) | Error 0.6990(0.8108) Steps 0(0.00) | Grad Norm 7.5084(16.7668) | Total Time 0.00(0.00)\n",
      "Iter 0026 | Time 35.7597(63.8630) | Bit/dim 7.2796(8.4506) | Xent 2.0279(2.1986) | Loss 14.9622(17.2224) | Error 0.6934(0.8073) Steps 0(0.00) | Grad Norm 4.3862(16.3954) | Total Time 0.00(0.00)\n",
      "Iter 0027 | Time 36.0634(63.0290) | Bit/dim 7.2267(8.4139) | Xent 2.0321(2.1936) | Loss 14.9089(17.1530) | Error 0.6766(0.8033) Steps 0(0.00) | Grad Norm 4.6656(16.0435) | Total Time 0.00(0.00)\n",
      "Iter 0028 | Time 35.4908(62.2029) | Bit/dim 7.1846(8.3770) | Xent 2.0306(2.1887) | Loss 14.7523(17.0810) | Error 0.6795(0.7996) Steps 0(0.00) | Grad Norm 6.5932(15.7600) | Total Time 0.00(0.00)\n",
      "Iter 0029 | Time 46.8472(61.7422) | Bit/dim 7.1470(8.3401) | Xent 2.0369(2.1842) | Loss 14.8918(17.0153) | Error 0.6827(0.7961) Steps 0(0.00) | Grad Norm 5.8918(15.4640) | Total Time 0.00(0.00)\n",
      "Iter 0030 | Time 37.3577(61.0107) | Bit/dim 7.1216(8.3036) | Xent 2.0332(2.1796) | Loss 14.5804(16.9423) | Error 0.6917(0.7930) Steps 0(0.00) | Grad Norm 3.6588(15.1098) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0005 | Time 17.9963, Epoch Time 264.8270(321.0963), Bit/dim 7.0979(best: 7.3435), Xent 2.0321, Loss 8.1139, Error 0.6794(best: 0.6933)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0031 | Time 36.1861(60.2659) | Bit/dim 7.0925(8.2672) | Xent 2.0411(2.1755) | Loss 16.2920(16.9228) | Error 0.6994(0.7902) Steps 0(0.00) | Grad Norm 4.5746(14.7937) | Total Time 0.00(0.00)\n",
      "Iter 0032 | Time 38.2054(59.6041) | Bit/dim 7.0771(8.2315) | Xent 2.0431(2.1715) | Loss 14.5381(16.8512) | Error 0.7125(0.7878) Steps 0(0.00) | Grad Norm 5.1460(14.5043) | Total Time 0.00(0.00)\n",
      "Iter 0033 | Time 33.5628(58.8229) | Bit/dim 7.0563(8.1963) | Xent 2.0516(2.1679) | Loss 14.2382(16.7729) | Error 0.7226(0.7859) Steps 0(0.00) | Grad Norm 3.0271(14.1600) | Total Time 0.00(0.00)\n",
      "Iter 0034 | Time 32.7168(58.0397) | Bit/dim 7.0568(8.1621) | Xent 2.0534(2.1645) | Loss 14.3826(16.7011) | Error 0.7191(0.7839) Steps 0(0.00) | Grad Norm 4.8552(13.8809) | Total Time 0.00(0.00)\n",
      "Iter 0035 | Time 38.7503(57.4610) | Bit/dim 7.0412(8.1285) | Xent 2.0499(2.1610) | Loss 14.5972(16.6380) | Error 0.7095(0.7817) Steps 0(0.00) | Grad Norm 3.6890(13.5751) | Total Time 0.00(0.00)\n",
      "Iter 0036 | Time 38.0260(56.8780) | Bit/dim 7.0305(8.0955) | Xent 2.0376(2.1573) | Loss 14.3841(16.5704) | Error 0.7063(0.7794) Steps 0(0.00) | Grad Norm 3.8322(13.2828) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0006 | Time 18.6097, Epoch Time 251.9846(319.0230), Bit/dim 7.0257(best: 7.0979), Xent 2.0390, Loss 8.0452, Error 0.7026(best: 0.6794)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0037 | Time 39.4807(56.3560) | Bit/dim 7.0197(8.0633) | Xent 2.0398(2.1538) | Loss 15.2737(16.5315) | Error 0.7110(0.7773) Steps 0(0.00) | Grad Norm 4.7687(13.0274) | Total Time 0.00(0.00)\n",
      "Iter 0038 | Time 39.0213(55.8360) | Bit/dim 7.0149(8.0318) | Xent 2.0490(2.1507) | Loss 14.2887(16.4642) | Error 0.7020(0.7751) Steps 0(0.00) | Grad Norm 4.1493(12.7610) | Total Time 0.00(0.00)\n",
      "Iter 0039 | Time 38.6040(55.3190) | Bit/dim 7.0033(8.0009) | Xent 2.0472(2.1476) | Loss 13.9409(16.3885) | Error 0.7043(0.7730) Steps 0(0.00) | Grad Norm 4.4187(12.5108) | Total Time 0.00(0.00)\n",
      "Iter 0040 | Time 39.1237(54.8332) | Bit/dim 7.0028(7.9710) | Xent 2.0401(2.1443) | Loss 14.3128(16.3263) | Error 0.7167(0.7713) Steps 0(0.00) | Grad Norm 4.5532(12.2721) | Total Time 0.00(0.00)\n",
      "Iter 0041 | Time 37.6066(54.3164) | Bit/dim 7.0010(7.9419) | Xent 2.0234(2.1407) | Loss 14.5424(16.2727) | Error 0.7079(0.7694) Steps 0(0.00) | Grad Norm 3.9596(12.0227) | Total Time 0.00(0.00)\n",
      "Iter 0042 | Time 42.0623(53.9487) | Bit/dim 7.0030(7.9137) | Xent 2.0472(2.1379) | Loss 14.1977(16.2105) | Error 0.7056(0.7675) Steps 0(0.00) | Grad Norm 2.2316(11.7289) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0007 | Time 18.3404, Epoch Time 269.9603(317.5511), Bit/dim 6.9928(best: 7.0257), Xent 2.0148, Loss 8.0002, Error 0.6830(best: 0.6794)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0043 | Time 39.7302(53.5222) | Bit/dim 6.9854(7.8859) | Xent 2.0205(2.1344) | Loss 16.0146(16.2046) | Error 0.6983(0.7654) Steps 0(0.00) | Grad Norm 2.9161(11.4646) | Total Time 0.00(0.00)\n",
      "Iter 0044 | Time 44.3482(53.2470) | Bit/dim 6.9819(7.8588) | Xent 2.0145(2.1308) | Loss 14.2239(16.1452) | Error 0.6955(0.7633) Steps 0(0.00) | Grad Norm 3.2175(11.2171) | Total Time 0.00(0.00)\n",
      "Iter 0045 | Time 50.1238(53.1533) | Bit/dim 6.9806(7.8324) | Xent 2.0189(2.1274) | Loss 14.6012(16.0989) | Error 0.7037(0.7615) Steps 0(0.00) | Grad Norm 8.1188(11.1242) | Total Time 0.00(0.00)\n",
      "Iter 0046 | Time 45.1383(52.9128) | Bit/dim 6.9719(7.8066) | Xent 2.0535(2.1252) | Loss 14.1896(16.0416) | Error 0.7243(0.7604) Steps 0(0.00) | Grad Norm 20.0589(11.3922) | Total Time 0.00(0.00)\n",
      "Iter 0047 | Time 46.7408(52.7277) | Bit/dim 6.9962(7.7823) | Xent 2.1506(2.1260) | Loss 14.4850(15.9949) | Error 0.7686(0.7606) Steps 0(0.00) | Grad Norm 41.7183(12.3020) | Total Time 0.00(0.00)\n",
      "Iter 0048 | Time 43.4380(52.4490) | Bit/dim 7.0113(7.7592) | Xent 2.2869(2.1308) | Loss 14.7757(15.9583) | Error 0.7987(0.7618) Steps 0(0.00) | Grad Norm 54.6148(13.5714) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0008 | Time 19.2490, Epoch Time 304.2406(317.1518), Bit/dim 6.9609(best: 6.9928), Xent 2.0329, Loss 7.9774, Error 0.7168(best: 0.6794)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0049 | Time 39.4014(52.0576) | Bit/dim 6.9559(7.7351) | Xent 2.0495(2.1284) | Loss 15.7017(15.9506) | Error 0.7290(0.7608) Steps 0(0.00) | Grad Norm 22.2154(13.8307) | Total Time 0.00(0.00)\n",
      "Iter 0050 | Time 44.1783(51.8212) | Bit/dim 6.9706(7.7121) | Xent 2.0864(2.1271) | Loss 14.4157(15.9046) | Error 0.7418(0.7602) Steps 0(0.00) | Grad Norm 30.6443(14.3351) | Total Time 0.00(0.00)\n",
      "Iter 0051 | Time 40.2601(51.4743) | Bit/dim 6.9543(7.6894) | Xent 2.0684(2.1253) | Loss 14.2253(15.8542) | Error 0.7424(0.7597) Steps 0(0.00) | Grad Norm 28.0598(14.7469) | Total Time 0.00(0.00)\n",
      "Iter 0052 | Time 39.5708(51.1172) | Bit/dim 6.9326(7.6667) | Xent 2.0245(2.1223) | Loss 13.4684(15.7826) | Error 0.7063(0.7581) Steps 0(0.00) | Grad Norm 17.0771(14.8168) | Total Time 0.00(0.00)\n",
      "Iter 0053 | Time 46.2701(50.9718) | Bit/dim 6.9356(7.6448) | Xent 2.0497(2.1201) | Loss 14.2703(15.7372) | Error 0.7300(0.7572) Steps 0(0.00) | Grad Norm 21.3771(15.0136) | Total Time 0.00(0.00)\n",
      "Iter 0054 | Time 41.6015(50.6907) | Bit/dim 6.9397(7.6236) | Xent 2.0057(2.1167) | Loss 14.1344(15.6892) | Error 0.7047(0.7557) Steps 0(0.00) | Grad Norm 15.1055(15.0163) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0009 | Time 18.4645, Epoch Time 285.0769(316.1895), Bit/dim 6.9101(best: 6.9609), Xent 1.9911, Loss 7.9056, Error 0.6767(best: 0.6794)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0055 | Time 48.0733(50.6122) | Bit/dim 6.9126(7.6023) | Xent 2.0114(2.1135) | Loss 16.0584(15.7002) | Error 0.6859(0.7536) Steps 0(0.00) | Grad Norm 10.9732(14.8951) | Total Time 0.00(0.00)\n",
      "Iter 0056 | Time 48.7774(50.5571) | Bit/dim 6.9216(7.5819) | Xent 2.0095(2.1104) | Loss 14.3221(15.6589) | Error 0.6885(0.7516) Steps 0(0.00) | Grad Norm 15.3366(14.9083) | Total Time 0.00(0.00)\n",
      "Iter 0057 | Time 41.8512(50.2960) | Bit/dim 6.8792(7.5608) | Xent 1.9888(2.1068) | Loss 13.7595(15.6019) | Error 0.6781(0.7494) Steps 0(0.00) | Grad Norm 3.0825(14.5535) | Total Time 0.00(0.00)\n",
      "Iter 0058 | Time 38.0370(49.9282) | Bit/dim 6.8991(7.5409) | Xent 1.9978(2.1035) | Loss 14.0861(15.5564) | Error 0.6983(0.7479) Steps 0(0.00) | Grad Norm 12.7016(14.4980) | Total Time 0.00(0.00)\n",
      "Iter 0059 | Time 39.2155(49.6068) | Bit/dim 6.8692(7.5208) | Xent 1.9933(2.1002) | Loss 14.1560(15.5144) | Error 0.6877(0.7461) Steps 0(0.00) | Grad Norm 6.8740(14.2692) | Total Time 0.00(0.00)\n",
      "Iter 0060 | Time 42.8723(49.4048) | Bit/dim 6.8606(7.5010) | Xent 1.9700(2.0963) | Loss 14.0756(15.4713) | Error 0.6646(0.7436) Steps 0(0.00) | Grad Norm 6.1812(14.0266) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0010 | Time 19.4858, Epoch Time 294.0265(315.5246), Bit/dim 6.8580(best: 6.9101), Xent 1.9740, Loss 7.8450, Error 0.6527(best: 0.6767)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0061 | Time 39.7304(49.1146) | Bit/dim 6.8489(7.4814) | Xent 1.9871(2.0930) | Loss 15.8001(15.4811) | Error 0.6579(0.7411) Steps 0(0.00) | Grad Norm 9.6501(13.8953) | Total Time 0.00(0.00)\n",
      "Iter 0062 | Time 45.0280(48.9920) | Bit/dim 6.8270(7.4618) | Xent 1.9751(2.0895) | Loss 13.7471(15.4291) | Error 0.6689(0.7389) Steps 0(0.00) | Grad Norm 3.1564(13.5731) | Total Time 0.00(0.00)\n",
      "Iter 0063 | Time 39.4208(48.7048) | Bit/dim 6.8219(7.4426) | Xent 1.9736(2.0860) | Loss 13.5149(15.3717) | Error 0.6711(0.7369) Steps 0(0.00) | Grad Norm 7.8208(13.4006) | Total Time 0.00(0.00)\n",
      "Iter 0064 | Time 45.5090(48.6089) | Bit/dim 6.8131(7.4237) | Xent 1.9677(2.0825) | Loss 14.0981(15.3335) | Error 0.6674(0.7348) Steps 0(0.00) | Grad Norm 7.1868(13.2142) | Total Time 0.00(0.00)\n",
      "Iter 0065 | Time 42.9810(48.4401) | Bit/dim 6.7871(7.4046) | Xent 1.9646(2.0789) | Loss 13.9954(15.2933) | Error 0.6615(0.7326) Steps 0(0.00) | Grad Norm 3.9034(12.9348) | Total Time 0.00(0.00)\n",
      "Iter 0066 | Time 43.1557(48.2816) | Bit/dim 6.7931(7.3863) | Xent 1.9576(2.0753) | Loss 13.9994(15.2545) | Error 0.6531(0.7302) Steps 0(0.00) | Grad Norm 5.5213(12.7124) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0011 | Time 18.7825, Epoch Time 290.0752(314.7611), Bit/dim 6.7526(best: 6.8580), Xent 1.9442, Loss 7.7247, Error 0.6464(best: 0.6527)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0067 | Time 40.0985(48.0361) | Bit/dim 6.7447(7.3670) | Xent 1.9532(2.0716) | Loss 15.4358(15.2599) | Error 0.6621(0.7282) Steps 0(0.00) | Grad Norm 4.5089(12.4663) | Total Time 0.00(0.00)\n",
      "Iter 0068 | Time 45.8556(47.9707) | Bit/dim 6.7339(7.3480) | Xent 1.9662(2.0685) | Loss 14.1292(15.2260) | Error 0.6820(0.7268) Steps 0(0.00) | Grad Norm 10.1766(12.3976) | Total Time 0.00(0.00)\n",
      "Iter 0069 | Time 36.3177(47.6211) | Bit/dim 6.7311(7.3295) | Xent 2.0048(2.0665) | Loss 13.9745(15.1885) | Error 0.7029(0.7260) Steps 0(0.00) | Grad Norm 18.8347(12.5907) | Total Time 0.00(0.00)\n",
      "Iter 0070 | Time 40.6074(47.4107) | Bit/dim 6.7294(7.3115) | Xent 2.1718(2.0697) | Loss 14.2295(15.1597) | Error 0.7748(0.7275) Steps 0(0.00) | Grad Norm 39.6115(13.4014) | Total Time 0.00(0.00)\n",
      "Iter 0071 | Time 40.4591(47.2021) | Bit/dim 6.7269(7.2940) | Xent 2.4166(2.0801) | Loss 14.1800(15.1303) | Error 0.7618(0.7285) Steps 0(0.00) | Grad Norm 37.0707(14.1114) | Total Time 0.00(0.00)\n",
      "Iter 0072 | Time 37.8732(46.9223) | Bit/dim 6.6719(7.2753) | Xent 2.0214(2.0783) | Loss 13.8614(15.0923) | Error 0.7083(0.7279) Steps 0(0.00) | Grad Norm 16.4108(14.1804) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0012 | Time 18.6462, Epoch Time 275.5067(313.5835), Bit/dim 6.6848(best: 6.7526), Xent 2.3304, Loss 7.8501, Error 0.8119(best: 0.6464)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0073 | Time 43.7294(46.8265) | Bit/dim 6.6785(7.2574) | Xent 2.3296(2.0859) | Loss 15.1661(15.0945) | Error 0.8206(0.7307) Steps 0(0.00) | Grad Norm 40.0645(14.9569) | Total Time 0.00(0.00)\n",
      "Iter 0074 | Time 36.6784(46.5220) | Bit/dim 6.6231(7.2384) | Xent 2.0396(2.0845) | Loss 13.8331(15.0566) | Error 0.7144(0.7302) Steps 0(0.00) | Grad Norm 29.0616(15.3801) | Total Time 0.00(0.00)\n",
      "Iter 0075 | Time 40.1983(46.3323) | Bit/dim 6.6296(7.2201) | Xent 2.0150(2.0824) | Loss 13.1934(15.0007) | Error 0.7424(0.7306) Steps 0(0.00) | Grad Norm 21.7086(15.5699) | Total Time 0.00(0.00)\n",
      "Iter 0076 | Time 38.4074(46.0946) | Bit/dim 6.5991(7.2015) | Xent 2.0106(2.0803) | Loss 13.7173(14.9622) | Error 0.7329(0.7307) Steps 0(0.00) | Grad Norm 13.2289(15.4997) | Total Time 0.00(0.00)\n",
      "Iter 0077 | Time 38.6672(45.8717) | Bit/dim 6.5465(7.1818) | Xent 2.0599(2.0796) | Loss 13.5229(14.9190) | Error 0.7299(0.7306) Steps 0(0.00) | Grad Norm 30.4588(15.9485) | Total Time 0.00(0.00)\n",
      "Iter 0078 | Time 38.0084(45.6358) | Bit/dim 6.5370(7.1625) | Xent 2.1305(2.0812) | Loss 13.7146(14.8829) | Error 0.7550(0.7314) Steps 0(0.00) | Grad Norm 44.2813(16.7985) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0013 | Time 18.6393, Epoch Time 270.0252(312.2768), Bit/dim 6.5259(best: 6.6848), Xent 2.1102, Loss 7.5810, Error 0.7669(best: 0.6464)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0079 | Time 40.1825(45.4722) | Bit/dim 6.5233(7.1433) | Xent 2.1422(2.0830) | Loss 15.5375(14.9026) | Error 0.7775(0.7327) Steps 0(0.00) | Grad Norm 50.9882(17.8242) | Total Time 0.00(0.00)\n",
      "Iter 0080 | Time 42.2944(45.3769) | Bit/dim 6.4805(7.1234) | Xent 2.1418(2.0848) | Loss 13.7795(14.8689) | Error 0.7710(0.7339) Steps 0(0.00) | Grad Norm 48.3199(18.7390) | Total Time 0.00(0.00)\n",
      "Iter 0081 | Time 39.1514(45.1901) | Bit/dim 6.4249(7.1025) | Xent 2.0546(2.0839) | Loss 13.4579(14.8265) | Error 0.7286(0.7337) Steps 0(0.00) | Grad Norm 30.1377(19.0810) | Total Time 0.00(0.00)\n",
      "Iter 0082 | Time 35.5719(44.9016) | Bit/dim 6.3639(7.0803) | Xent 2.0122(2.0817) | Loss 13.1055(14.7749) | Error 0.7027(0.7328) Steps 0(0.00) | Grad Norm 11.1027(18.8416) | Total Time 0.00(0.00)\n",
      "Iter 0083 | Time 32.9864(44.5441) | Bit/dim 6.3454(7.0583) | Xent 2.0873(2.0819) | Loss 13.1570(14.7264) | Error 0.7454(0.7332) Steps 0(0.00) | Grad Norm 34.7059(19.3176) | Total Time 0.00(0.00)\n",
      "Iter 0084 | Time 37.9933(44.3476) | Bit/dim 6.2396(7.0337) | Xent 2.0579(2.0812) | Loss 13.0402(14.6758) | Error 0.7310(0.7331) Steps 0(0.00) | Grad Norm 27.4193(19.5606) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0014 | Time 18.7122, Epoch Time 262.5202(310.7841), Bit/dim 6.1804(best: 6.5259), Xent 2.0169, Loss 7.1888, Error 0.6795(best: 0.6464)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0085 | Time 43.9116(44.3345) | Bit/dim 6.1820(7.0082) | Xent 2.0270(2.0795) | Loss 14.3237(14.6652) | Error 0.6885(0.7318) Steps 0(0.00) | Grad Norm 8.4294(19.2267) | Total Time 0.00(0.00)\n",
      "Iter 0086 | Time 41.0479(44.2359) | Bit/dim 6.1391(6.9821) | Xent 2.0291(2.0780) | Loss 12.8231(14.6100) | Error 0.7064(0.7310) Steps 0(0.00) | Grad Norm 30.5608(19.5667) | Total Time 0.00(0.00)\n",
      "Iter 0087 | Time 43.5643(44.2158) | Bit/dim 6.0670(6.9546) | Xent 2.0884(2.0783) | Loss 12.9987(14.5616) | Error 0.7326(0.7311) Steps 0(0.00) | Grad Norm 39.9581(20.1785) | Total Time 0.00(0.00)\n",
      "Iter 0088 | Time 40.2758(44.0976) | Bit/dim 6.0550(6.9276) | Xent 2.0540(2.0776) | Loss 12.7082(14.5060) | Error 0.7279(0.7310) Steps 0(0.00) | Grad Norm 34.9680(20.6221) | Total Time 0.00(0.00)\n",
      "Iter 0089 | Time 39.1812(43.9501) | Bit/dim 5.9838(6.8993) | Xent 2.0506(2.0768) | Loss 12.4667(14.4448) | Error 0.7398(0.7312) Steps 0(0.00) | Grad Norm 22.0328(20.6645) | Total Time 0.00(0.00)\n",
      "Iter 0090 | Time 39.6157(43.8201) | Bit/dim 5.9635(6.8712) | Xent 2.0298(2.0754) | Loss 12.5541(14.3881) | Error 0.7052(0.7305) Steps 0(0.00) | Grad Norm 17.4362(20.5676) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0015 | Time 19.8696, Epoch Time 283.2639(309.9585), Bit/dim 5.9548(best: 6.1804), Xent 2.0221, Loss 6.9659, Error 0.6981(best: 0.6464)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0091 | Time 39.8971(43.7024) | Bit/dim 5.9598(6.8439) | Xent 2.0632(2.0750) | Loss 13.8735(14.3727) | Error 0.7212(0.7302) Steps 0(0.00) | Grad Norm 34.9320(20.9985) | Total Time 0.00(0.00)\n",
      "Iter 0092 | Time 43.7234(43.7030) | Bit/dim 6.0638(6.8205) | Xent 2.0693(2.0748) | Loss 12.4838(14.3160) | Error 0.7491(0.7307) Steps 0(0.00) | Grad Norm 76.8892(22.6753) | Total Time 0.00(0.00)\n",
      "Iter 0093 | Time 35.8170(43.4664) | Bit/dim 6.3411(6.8061) | Xent 2.1542(2.0772) | Loss 13.3366(14.2866) | Error 0.7715(0.7320) Steps 0(0.00) | Grad Norm 100.9973(25.0249) | Total Time 0.00(0.00)\n",
      "Iter 0094 | Time 40.7267(43.3842) | Bit/dim 5.9487(6.7804) | Xent 2.0428(2.0762) | Loss 12.5412(14.2343) | Error 0.7245(0.7317) Steps 0(0.00) | Grad Norm 18.3468(24.8246) | Total Time 0.00(0.00)\n",
      "Iter 0095 | Time 36.9425(43.1910) | Bit/dim 6.0874(6.7596) | Xent 2.0867(2.0765) | Loss 12.8533(14.1928) | Error 0.7490(0.7323) Steps 0(0.00) | Grad Norm 50.5198(25.5954) | Total Time 0.00(0.00)\n",
      "Iter 0096 | Time 35.0510(42.9468) | Bit/dim 5.9259(6.7346) | Xent 2.0124(2.0746) | Loss 12.4192(14.1396) | Error 0.6970(0.7312) Steps 0(0.00) | Grad Norm 16.8851(25.3341) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0016 | Time 19.0401, Epoch Time 267.1997(308.6757), Bit/dim 5.8816(best: 5.9548), Xent 2.0844, Loss 6.9238, Error 0.7375(best: 0.6464)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0097 | Time 34.6724(42.6985) | Bit/dim 5.8763(6.7088) | Xent 2.0958(2.0752) | Loss 14.3393(14.1456) | Error 0.7438(0.7316) Steps 0(0.00) | Grad Norm 19.3308(25.1540) | Total Time 0.00(0.00)\n",
      "Iter 0098 | Time 36.3610(42.5084) | Bit/dim 5.8997(6.6846) | Xent 2.0797(2.0754) | Loss 12.3060(14.0904) | Error 0.7344(0.7317) Steps 0(0.00) | Grad Norm 19.4072(24.9816) | Total Time 0.00(0.00)\n",
      "Iter 0099 | Time 40.9872(42.4628) | Bit/dim 5.8477(6.6595) | Xent 2.0429(2.0744) | Loss 12.4184(14.0403) | Error 0.7395(0.7319) Steps 0(0.00) | Grad Norm 13.5537(24.6388) | Total Time 0.00(0.00)\n",
      "Iter 0100 | Time 35.0909(42.2416) | Bit/dim 5.8113(6.6340) | Xent 2.0229(2.0728) | Loss 12.2511(13.9866) | Error 0.7149(0.7314) Steps 0(0.00) | Grad Norm 11.4688(24.2437) | Total Time 0.00(0.00)\n",
      "Iter 0101 | Time 41.3788(42.2157) | Bit/dim 5.7669(6.6080) | Xent 2.0246(2.0714) | Loss 12.3946(13.9388) | Error 0.6960(0.7303) Steps 0(0.00) | Grad Norm 6.2108(23.7027) | Total Time 0.00(0.00)\n",
      "Iter 0102 | Time 36.9362(42.0574) | Bit/dim 5.7620(6.5826) | Xent 2.0791(2.0716) | Loss 11.9215(13.8783) | Error 0.7485(0.7309) Steps 0(0.00) | Grad Norm 14.2870(23.4202) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0017 | Time 17.7728, Epoch Time 258.9589(307.1842), Bit/dim 5.7204(best: 5.8816), Xent 2.0168, Loss 6.7287, Error 0.6899(best: 0.6464)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0103 | Time 36.2331(41.8826) | Bit/dim 5.7213(6.5568) | Xent 2.0377(2.0706) | Loss 13.7623(13.8748) | Error 0.7077(0.7302) Steps 0(0.00) | Grad Norm 7.7788(22.9510) | Total Time 0.00(0.00)\n",
      "Iter 0104 | Time 37.2983(41.7451) | Bit/dim 5.7109(6.5314) | Xent 2.0486(2.0699) | Loss 11.9482(13.8170) | Error 0.7151(0.7297) Steps 0(0.00) | Grad Norm 8.1010(22.5055) | Total Time 0.00(0.00)\n",
      "Iter 0105 | Time 39.8739(41.6890) | Bit/dim 5.6667(6.5055) | Xent 2.0522(2.0694) | Loss 12.0420(13.7638) | Error 0.7225(0.7295) Steps 0(0.00) | Grad Norm 9.9505(22.1288) | Total Time 0.00(0.00)\n",
      "Iter 0106 | Time 37.2938(41.5571) | Bit/dim 5.6429(6.4796) | Xent 2.0417(2.0686) | Loss 11.9619(13.7097) | Error 0.6977(0.7286) Steps 0(0.00) | Grad Norm 3.8307(21.5799) | Total Time 0.00(0.00)\n",
      "Iter 0107 | Time 38.7394(41.4726) | Bit/dim 5.6324(6.4542) | Xent 2.0407(2.0677) | Loss 12.0990(13.6614) | Error 0.7196(0.7283) Steps 0(0.00) | Grad Norm 9.0607(21.2043) | Total Time 0.00(0.00)\n",
      "Iter 0108 | Time 43.6922(41.5392) | Bit/dim 5.6051(6.4287) | Xent 2.0238(2.0664) | Loss 12.0832(13.6141) | Error 0.6985(0.7274) Steps 0(0.00) | Grad Norm 5.8757(20.7445) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0018 | Time 18.3465, Epoch Time 267.0763(305.9810), Bit/dim 5.5869(best: 5.7204), Xent 1.9954, Loss 6.5846, Error 0.6926(best: 0.6464)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0109 | Time 42.0221(41.5537) | Bit/dim 5.5739(6.4031) | Xent 2.0149(2.0649) | Loss 13.9876(13.6253) | Error 0.7014(0.7266) Steps 0(0.00) | Grad Norm 4.6655(20.2621) | Total Time 0.00(0.00)\n",
      "Iter 0110 | Time 39.7057(41.4982) | Bit/dim 5.5733(6.3782) | Xent 2.0280(2.0638) | Loss 11.4961(13.5614) | Error 0.7223(0.7265) Steps 0(0.00) | Grad Norm 7.1776(19.8696) | Total Time 0.00(0.00)\n",
      "Iter 0111 | Time 39.3417(41.4335) | Bit/dim 5.5599(6.3536) | Xent 1.9963(2.0617) | Loss 11.9751(13.5138) | Error 0.6977(0.7256) Steps 0(0.00) | Grad Norm 3.3103(19.3728) | Total Time 0.00(0.00)\n",
      "Iter 0112 | Time 37.0677(41.3025) | Bit/dim 5.5516(6.3296) | Xent 2.0131(2.0603) | Loss 12.0332(13.4694) | Error 0.6940(0.7247) Steps 0(0.00) | Grad Norm 6.5656(18.9886) | Total Time 0.00(0.00)\n",
      "Iter 0113 | Time 37.0567(41.1752) | Bit/dim 5.5262(6.3055) | Xent 1.9866(2.0581) | Loss 11.7172(13.4168) | Error 0.6865(0.7235) Steps 0(0.00) | Grad Norm 5.2238(18.5756) | Total Time 0.00(0.00)\n",
      "Iter 0114 | Time 37.3935(41.0617) | Bit/dim 5.4783(6.2806) | Xent 1.9920(2.0561) | Loss 11.7965(13.3682) | Error 0.6907(0.7225) Steps 0(0.00) | Grad Norm 4.1109(18.1417) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0019 | Time 18.6355, Epoch Time 266.9901(304.8112), Bit/dim 5.4753(best: 5.5869), Xent 1.9699, Loss 6.4603, Error 0.6794(best: 0.6464)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0115 | Time 39.1816(41.0053) | Bit/dim 5.4760(6.2565) | Xent 1.9943(2.0542) | Loss 13.8596(13.3829) | Error 0.6985(0.7218) Steps 0(0.00) | Grad Norm 5.9878(17.7771) | Total Time 0.00(0.00)\n",
      "Iter 0116 | Time 37.2097(40.8915) | Bit/dim 5.4510(6.2323) | Xent 1.9752(2.0519) | Loss 11.4728(13.3256) | Error 0.6761(0.7205) Steps 0(0.00) | Grad Norm 3.6059(17.3519) | Total Time 0.00(0.00)\n",
      "Iter 0117 | Time 38.2868(40.8133) | Bit/dim 5.4432(6.2087) | Xent 1.9791(2.0497) | Loss 11.5310(13.2718) | Error 0.6841(0.7194) Steps 0(0.00) | Grad Norm 6.0352(17.0124) | Total Time 0.00(0.00)\n",
      "Iter 0118 | Time 38.0942(40.7317) | Bit/dim 5.4176(6.1849) | Xent 1.9605(2.0470) | Loss 11.4424(13.2169) | Error 0.6719(0.7179) Steps 0(0.00) | Grad Norm 3.0965(16.5949) | Total Time 0.00(0.00)\n",
      "Iter 0119 | Time 37.7800(40.6432) | Bit/dim 5.4070(6.1616) | Xent 1.9496(2.0441) | Loss 11.6539(13.1700) | Error 0.6754(0.7167) Steps 0(0.00) | Grad Norm 6.1836(16.2826) | Total Time 0.00(0.00)\n",
      "Iter 0120 | Time 39.0228(40.5946) | Bit/dim 5.4162(6.1392) | Xent 1.9638(2.0417) | Loss 11.7182(13.1265) | Error 0.6823(0.7156) Steps 0(0.00) | Grad Norm 5.5751(15.9614) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0020 | Time 19.0289, Epoch Time 264.1471(303.5913), Bit/dim 5.3833(best: 5.4753), Xent 1.9278, Loss 6.3472, Error 0.6523(best: 0.6464)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0121 | Time 37.2021(40.4928) | Bit/dim 5.3705(6.1162) | Xent 1.9376(2.0386) | Loss 13.4202(13.1353) | Error 0.6646(0.7141) Steps 0(0.00) | Grad Norm 7.0330(15.6935) | Total Time 0.00(0.00)\n",
      "Iter 0122 | Time 43.0692(40.5701) | Bit/dim 5.3517(6.0932) | Xent 1.9538(2.0360) | Loss 11.6222(13.0899) | Error 0.6761(0.7130) Steps 0(0.00) | Grad Norm 7.6251(15.4515) | Total Time 0.00(0.00)\n",
      "Iter 0123 | Time 42.8389(40.6382) | Bit/dim 5.3520(6.0710) | Xent 1.9221(2.0326) | Loss 11.5790(13.0446) | Error 0.6684(0.7116) Steps 0(0.00) | Grad Norm 7.7605(15.2207) | Total Time 0.00(0.00)\n",
      "Iter 0124 | Time 42.7821(40.7025) | Bit/dim 5.3254(6.0486) | Xent 1.9356(2.0297) | Loss 11.3718(12.9944) | Error 0.6713(0.7104) Steps 0(0.00) | Grad Norm 6.8403(14.9693) | Total Time 0.00(0.00)\n",
      "Iter 0125 | Time 37.4189(40.6040) | Bit/dim 5.3252(6.0269) | Xent 1.9438(2.0271) | Loss 10.9294(12.9324) | Error 0.6743(0.7093) Steps 0(0.00) | Grad Norm 13.4287(14.9231) | Total Time 0.00(0.00)\n",
      "Iter 0126 | Time 37.8941(40.5227) | Bit/dim 5.3440(6.0064) | Xent 2.0927(2.0291) | Loss 11.5937(12.8923) | Error 0.7332(0.7100) Steps 0(0.00) | Grad Norm 36.6822(15.5759) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0021 | Time 17.7877, Epoch Time 274.5151(302.7190), Bit/dim 5.5562(best: 5.3833), Xent 2.3261, Loss 6.7192, Error 0.7457(best: 0.6464)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0127 | Time 38.0124(40.4474) | Bit/dim 5.5533(5.9928) | Xent 2.3541(2.0388) | Loss 14.0232(12.9262) | Error 0.7555(0.7114) Steps 0(0.00) | Grad Norm 33.1618(16.1035) | Total Time 0.00(0.00)\n",
      "Iter 0128 | Time 39.8885(40.4306) | Bit/dim 5.4408(5.9763) | Xent 2.0726(2.0398) | Loss 11.7270(12.8902) | Error 0.7354(0.7121) Steps 0(0.00) | Grad Norm 19.1471(16.1948) | Total Time 0.00(0.00)\n",
      "Iter 0129 | Time 41.6437(40.4670) | Bit/dim 5.2842(5.9555) | Xent 1.9987(2.0386) | Loss 11.3726(12.8447) | Error 0.7044(0.7119) Steps 0(0.00) | Grad Norm 9.0222(15.9796) | Total Time 0.00(0.00)\n",
      "Iter 0130 | Time 38.4676(40.4070) | Bit/dim 5.4116(5.9392) | Xent 2.0968(2.0404) | Loss 11.4586(12.8031) | Error 0.7441(0.7129) Steps 0(0.00) | Grad Norm 28.6116(16.3586) | Total Time 0.00(0.00)\n",
      "Iter 0131 | Time 41.2493(40.4323) | Bit/dim 5.3021(5.9201) | Xent 2.0018(2.0392) | Loss 11.5683(12.7661) | Error 0.7001(0.7125) Steps 0(0.00) | Grad Norm 11.2736(16.2060) | Total Time 0.00(0.00)\n",
      "Iter 0132 | Time 41.8149(40.4738) | Bit/dim 5.3826(5.9040) | Xent 1.9894(2.0377) | Loss 11.6778(12.7334) | Error 0.6899(0.7118) Steps 0(0.00) | Grad Norm 7.6295(15.9487) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0022 | Time 18.3102, Epoch Time 275.1711(301.8926), Bit/dim 5.3608(best: 5.3833), Xent 2.0076, Loss 6.3646, Error 0.7134(best: 0.6464)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0133 | Time 38.0097(40.3998) | Bit/dim 5.3565(5.8875) | Xent 2.0233(2.0373) | Loss 13.2903(12.7501) | Error 0.7235(0.7122) Steps 0(0.00) | Grad Norm 15.1156(15.9237) | Total Time 0.00(0.00)\n",
      "Iter 0134 | Time 41.0985(40.4208) | Bit/dim 5.2699(5.8690) | Xent 2.0371(2.0373) | Loss 11.4444(12.7110) | Error 0.7245(0.7125) Steps 0(0.00) | Grad Norm 5.1211(15.5996) | Total Time 0.00(0.00)\n",
      "Iter 0135 | Time 39.4627(40.3921) | Bit/dim 5.2956(5.8518) | Xent 2.1070(2.0394) | Loss 11.5308(12.6756) | Error 0.7420(0.7134) Steps 0(0.00) | Grad Norm 20.8549(15.7573) | Total Time 0.00(0.00)\n",
      "Iter 0136 | Time 38.3366(40.3304) | Bit/dim 5.2294(5.8331) | Xent 2.0519(2.0397) | Loss 11.3852(12.6368) | Error 0.7223(0.7137) Steps 0(0.00) | Grad Norm 15.5268(15.7504) | Total Time 0.00(0.00)\n",
      "Iter 0137 | Time 41.7339(40.3725) | Bit/dim 5.2412(5.8154) | Xent 2.0441(2.0399) | Loss 11.2853(12.5963) | Error 0.7359(0.7143) Steps 0(0.00) | Grad Norm 6.1989(15.4638) | Total Time 0.00(0.00)\n",
      "Iter 0138 | Time 40.8742(40.3875) | Bit/dim 5.2602(5.7987) | Xent 2.0394(2.0399) | Loss 11.3883(12.5601) | Error 0.7372(0.7150) Steps 0(0.00) | Grad Norm 6.1619(15.1848) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0023 | Time 19.0239, Epoch Time 274.1598(301.0606), Bit/dim 5.2222(best: 5.3608), Xent 1.9952, Loss 6.2198, Error 0.6854(best: 0.6464)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0139 | Time 44.9567(40.5246) | Bit/dim 5.2301(5.7817) | Xent 2.0103(2.0390) | Loss 13.2328(12.5802) | Error 0.7034(0.7147) Steps 0(0.00) | Grad Norm 8.5221(14.9849) | Total Time 0.00(0.00)\n",
      "Iter 0140 | Time 41.5320(40.5548) | Bit/dim 5.1891(5.7639) | Xent 2.0276(2.0386) | Loss 11.4579(12.5466) | Error 0.7043(0.7144) Steps 0(0.00) | Grad Norm 5.1396(14.6895) | Total Time 0.00(0.00)\n",
      "Iter 0141 | Time 44.4932(40.6730) | Bit/dim 5.1688(5.7460) | Xent 2.0202(2.0381) | Loss 11.0310(12.5011) | Error 0.6980(0.7139) Steps 0(0.00) | Grad Norm 11.3884(14.5905) | Total Time 0.00(0.00)\n",
      "Iter 0142 | Time 42.3894(40.7245) | Bit/dim 5.1543(5.7283) | Xent 2.0143(2.0374) | Loss 10.7487(12.4485) | Error 0.7064(0.7136) Steps 0(0.00) | Grad Norm 5.5641(14.3197) | Total Time 0.00(0.00)\n",
      "Iter 0143 | Time 46.6838(40.9033) | Bit/dim 5.1632(5.7113) | Xent 1.9674(2.0353) | Loss 11.3827(12.4166) | Error 0.6825(0.7127) Steps 0(0.00) | Grad Norm 7.0864(14.1027) | Total Time 0.00(0.00)\n",
      "Iter 0144 | Time 40.1121(40.8795) | Bit/dim 5.1515(5.6945) | Xent 1.9846(2.0337) | Loss 11.1103(12.3774) | Error 0.6914(0.7121) Steps 0(0.00) | Grad Norm 9.8684(13.9757) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0024 | Time 18.2348, Epoch Time 294.0308(300.8497), Bit/dim 5.1205(best: 5.2222), Xent 1.9480, Loss 6.0945, Error 0.6663(best: 0.6464)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0145 | Time 44.2518(40.9807) | Bit/dim 5.1190(5.6773) | Xent 1.9737(2.0319) | Loss 12.6822(12.3865) | Error 0.6930(0.7115) Steps 0(0.00) | Grad Norm 7.3522(13.7770) | Total Time 0.00(0.00)\n",
      "Iter 0146 | Time 37.7629(40.8842) | Bit/dim 5.1284(5.6608) | Xent 1.9670(2.0300) | Loss 10.8053(12.3391) | Error 0.6865(0.7108) Steps 0(0.00) | Grad Norm 5.7278(13.5355) | Total Time 0.00(0.00)\n",
      "Iter 0147 | Time 42.3747(40.9289) | Bit/dim 5.1088(5.6442) | Xent 1.9687(2.0281) | Loss 10.9506(12.2974) | Error 0.6875(0.7101) Steps 0(0.00) | Grad Norm 4.6447(13.2688) | Total Time 0.00(0.00)\n",
      "Iter 0148 | Time 44.9482(41.0495) | Bit/dim 5.0977(5.6278) | Xent 1.9512(2.0258) | Loss 11.2162(12.2650) | Error 0.6763(0.7090) Steps 0(0.00) | Grad Norm 7.8042(13.1048) | Total Time 0.00(0.00)\n",
      "Iter 0149 | Time 41.8705(41.0741) | Bit/dim 5.0866(5.6116) | Xent 1.9432(2.0234) | Loss 11.0760(12.2293) | Error 0.6696(0.7079) Steps 0(0.00) | Grad Norm 4.4683(12.8457) | Total Time 0.00(0.00)\n",
      "Iter 0150 | Time 44.5780(41.1792) | Bit/dim 5.0633(5.5952) | Xent 1.9296(2.0205) | Loss 10.8891(12.1891) | Error 0.6729(0.7068) Steps 0(0.00) | Grad Norm 4.4399(12.5936) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0025 | Time 19.6545, Epoch Time 291.0116(300.5546), Bit/dim 5.0565(best: 5.1205), Xent 1.9105, Loss 6.0118, Error 0.6508(best: 0.6464)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0151 | Time 41.7783(41.1972) | Bit/dim 5.0731(5.5795) | Xent 1.9367(2.0180) | Loss 13.1928(12.2192) | Error 0.6761(0.7059) Steps 0(0.00) | Grad Norm 4.0522(12.3373) | Total Time 0.00(0.00)\n",
      "Iter 0152 | Time 40.0282(41.1621) | Bit/dim 5.0564(5.5638) | Xent 1.9168(2.0150) | Loss 10.6647(12.1726) | Error 0.6633(0.7046) Steps 0(0.00) | Grad Norm 5.3754(12.1285) | Total Time 0.00(0.00)\n",
      "Iter 0153 | Time 43.2911(41.2260) | Bit/dim 5.0483(5.5483) | Xent 1.9184(2.0121) | Loss 10.8294(12.1323) | Error 0.6629(0.7034) Steps 0(0.00) | Grad Norm 3.4862(11.8692) | Total Time 0.00(0.00)\n",
      "Iter 0154 | Time 41.1239(41.2229) | Bit/dim 5.0294(5.5328) | Xent 1.9481(2.0102) | Loss 10.6468(12.0877) | Error 0.6809(0.7027) Steps 0(0.00) | Grad Norm 11.4668(11.8571) | Total Time 0.00(0.00)\n",
      "Iter 0155 | Time 41.9266(41.2440) | Bit/dim 5.0155(5.5173) | Xent 1.9825(2.0093) | Loss 10.5764(12.0424) | Error 0.6884(0.7023) Steps 0(0.00) | Grad Norm 23.8889(12.2181) | Total Time 0.00(0.00)\n",
      "Iter 0156 | Time 42.8587(41.2925) | Bit/dim 5.0427(5.5030) | Xent 2.1735(2.0143) | Loss 11.0946(12.0140) | Error 0.7660(0.7042) Steps 0(0.00) | Grad Norm 45.9030(13.2286) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0026 | Time 18.2619, Epoch Time 284.9232(300.0856), Bit/dim 5.1108(best: 5.0565), Xent 2.3481, Loss 6.2849, Error 0.7731(best: 0.6464)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0157 | Time 41.4677(41.2977) | Bit/dim 5.1168(5.4914) | Xent 2.3747(2.0251) | Loss 13.1119(12.0469) | Error 0.7710(0.7062) Steps 0(0.00) | Grad Norm 52.9525(14.4203) | Total Time 0.00(0.00)\n",
      "Iter 0158 | Time 37.7194(41.1904) | Bit/dim 5.0069(5.4769) | Xent 1.9132(2.0217) | Loss 10.8421(12.0107) | Error 0.6716(0.7051) Steps 0(0.00) | Grad Norm 9.1373(14.2619) | Total Time 0.00(0.00)\n",
      "Iter 0159 | Time 47.5925(41.3824) | Bit/dim 5.0643(5.4645) | Xent 2.0587(2.0228) | Loss 11.1506(11.9849) | Error 0.7356(0.7060) Steps 0(0.00) | Grad Norm 24.2512(14.5615) | Total Time 0.00(0.00)\n",
      "Iter 0160 | Time 39.4476(41.3244) | Bit/dim 5.0021(5.4506) | Xent 1.9496(2.0206) | Loss 10.6322(11.9444) | Error 0.6932(0.7057) Steps 0(0.00) | Grad Norm 3.2814(14.2231) | Total Time 0.00(0.00)\n",
      "Iter 0161 | Time 47.2972(41.5036) | Bit/dim 5.0145(5.4376) | Xent 1.9754(2.0193) | Loss 11.2034(11.9221) | Error 0.6899(0.7052) Steps 0(0.00) | Grad Norm 7.3662(14.0174) | Total Time 0.00(0.00)\n",
      "Iter 0162 | Time 45.5621(41.6253) | Bit/dim 5.0197(5.4250) | Xent 1.9679(2.0177) | Loss 10.5509(11.8810) | Error 0.6874(0.7047) Steps 0(0.00) | Grad Norm 6.9455(13.8053) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0027 | Time 20.0873, Epoch Time 294.4711(299.9172), Bit/dim 4.9912(best: 5.0565), Xent 1.9544, Loss 5.9684, Error 0.6776(best: 0.6464)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0163 | Time 41.3404(41.6168) | Bit/dim 4.9823(5.4117) | Xent 1.9895(2.0169) | Loss 12.8306(11.9095) | Error 0.6972(0.7044) Steps 0(0.00) | Grad Norm 7.2839(13.6096) | Total Time 0.00(0.00)\n",
      "Iter 0164 | Time 41.6889(41.6190) | Bit/dim 4.9599(5.3982) | Xent 1.9873(2.0160) | Loss 10.8770(11.8785) | Error 0.6946(0.7041) Steps 0(0.00) | Grad Norm 2.4253(13.2741) | Total Time 0.00(0.00)\n",
      "Iter 0165 | Time 42.9775(41.6597) | Bit/dim 4.9617(5.3851) | Xent 1.9719(2.0147) | Loss 10.7442(11.8445) | Error 0.6904(0.7037) Steps 0(0.00) | Grad Norm 6.5816(13.0733) | Total Time 0.00(0.00)\n",
      "Iter 0166 | Time 39.3411(41.5901) | Bit/dim 4.9482(5.3720) | Xent 1.9768(2.0135) | Loss 10.6684(11.8092) | Error 0.6977(0.7035) Steps 0(0.00) | Grad Norm 5.5981(12.8491) | Total Time 0.00(0.00)\n",
      "Iter 0167 | Time 41.8853(41.5990) | Bit/dim 4.9361(5.3589) | Xent 1.9707(2.0123) | Loss 10.8987(11.7819) | Error 0.6905(0.7032) Steps 0(0.00) | Grad Norm 4.2460(12.5910) | Total Time 0.00(0.00)\n",
      "Iter 0168 | Time 42.8241(41.6358) | Bit/dim 4.9362(5.3462) | Xent 1.9740(2.0111) | Loss 10.6252(11.7472) | Error 0.6936(0.7029) Steps 0(0.00) | Grad Norm 4.9434(12.3615) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0028 | Time 18.4008, Epoch Time 284.1696(299.4448), Bit/dim 4.9170(best: 4.9912), Xent 1.9495, Loss 5.8917, Error 0.6834(best: 0.6464)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0169 | Time 42.1460(41.6511) | Bit/dim 4.9292(5.3337) | Xent 1.9777(2.0101) | Loss 12.4905(11.7695) | Error 0.6930(0.7026) Steps 0(0.00) | Grad Norm 6.2911(12.1794) | Total Time 0.00(0.00)\n",
      "Iter 0170 | Time 48.4899(41.8562) | Bit/dim 4.9211(5.3213) | Xent 1.9513(2.0083) | Loss 10.6827(11.7369) | Error 0.6780(0.7018) Steps 0(0.00) | Grad Norm 3.2350(11.9111) | Total Time 0.00(0.00)\n",
      "Iter 0171 | Time 50.8595(42.1263) | Bit/dim 4.8959(5.3086) | Xent 1.9354(2.0062) | Loss 10.6897(11.7055) | Error 0.6814(0.7012) Steps 0(0.00) | Grad Norm 4.6522(11.6933) | Total Time 0.00(0.00)\n",
      "Iter 0172 | Time 47.2602(42.2803) | Bit/dim 4.8738(5.2955) | Xent 1.9430(2.0043) | Loss 10.8610(11.6801) | Error 0.6819(0.7006) Steps 0(0.00) | Grad Norm 5.0906(11.4952) | Total Time 0.00(0.00)\n",
      "Iter 0173 | Time 47.6931(42.4427) | Bit/dim 4.8800(5.2831) | Xent 1.9139(2.0016) | Loss 10.4113(11.6421) | Error 0.6695(0.6997) Steps 0(0.00) | Grad Norm 3.2683(11.2484) | Total Time 0.00(0.00)\n",
      "Iter 0174 | Time 46.8036(42.5736) | Bit/dim 4.8797(5.2710) | Xent 1.9324(1.9995) | Loss 10.5423(11.6091) | Error 0.6680(0.6988) Steps 0(0.00) | Grad Norm 2.8828(10.9975) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0029 | Time 18.5004, Epoch Time 317.9691(300.0005), Bit/dim 4.8769(best: 4.9170), Xent 1.8844, Loss 5.8191, Error 0.6562(best: 0.6464)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0175 | Time 55.2365(42.9534) | Bit/dim 4.8729(5.2590) | Xent 1.9175(1.9970) | Loss 12.4287(11.6337) | Error 0.6730(0.6980) Steps 0(0.00) | Grad Norm 3.7479(10.7800) | Total Time 0.00(0.00)\n",
      "Iter 0176 | Time 48.3805(43.1163) | Bit/dim 4.8696(5.2473) | Xent 1.9171(1.9946) | Loss 10.6806(11.6051) | Error 0.6643(0.6970) Steps 0(0.00) | Grad Norm 3.9060(10.5738) | Total Time 0.00(0.00)\n",
      "Iter 0177 | Time 52.7699(43.4059) | Bit/dim 4.8537(5.2355) | Xent 1.8890(1.9915) | Loss 10.6764(11.5772) | Error 0.6623(0.6959) Steps 0(0.00) | Grad Norm 3.5216(10.3622) | Total Time 0.00(0.00)\n",
      "Iter 0178 | Time 53.7929(43.7175) | Bit/dim 4.8464(5.2239) | Xent 1.9040(1.9888) | Loss 10.4213(11.5425) | Error 0.6664(0.6950) Steps 0(0.00) | Grad Norm 2.8203(10.1359) | Total Time 0.00(0.00)\n",
      "Iter 0179 | Time 49.2196(43.8825) | Bit/dim 4.8306(5.2121) | Xent 1.8924(1.9859) | Loss 10.3196(11.5058) | Error 0.6623(0.6941) Steps 0(0.00) | Grad Norm 2.6632(9.9118) | Total Time 0.00(0.00)\n",
      "Iter 0180 | Time 56.8571(44.2718) | Bit/dim 4.8230(5.2004) | Xent 1.8913(1.9831) | Loss 10.3529(11.4712) | Error 0.6658(0.6932) Steps 0(0.00) | Grad Norm 2.3729(9.6856) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0030 | Time 20.6289, Epoch Time 352.5120(301.5758), Bit/dim 4.8146(best: 4.8769), Xent 1.8547, Loss 5.7420, Error 0.6469(best: 0.6464)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0181 | Time 49.1198(44.4172) | Bit/dim 4.8112(5.1887) | Xent 1.8775(1.9799) | Loss 12.5461(11.5035) | Error 0.6633(0.6923) Steps 0(0.00) | Grad Norm 4.0082(9.5153) | Total Time 0.00(0.00)\n",
      "Iter 0182 | Time 64.8059(45.0289) | Bit/dim 4.8161(5.1775) | Xent 1.9090(1.9778) | Loss 10.7466(11.4808) | Error 0.6731(0.6917) Steps 0(0.00) | Grad Norm 9.0877(9.5024) | Total Time 0.00(0.00)\n",
      "Iter 0183 | Time 46.6432(45.0773) | Bit/dim 4.8283(5.1671) | Xent 2.0214(1.9791) | Loss 10.5421(11.4526) | Error 0.7199(0.6926) Steps 0(0.00) | Grad Norm 21.5979(9.8653) | Total Time 0.00(0.00)\n",
      "Iter 0184 | Time 50.9063(45.2522) | Bit/dim 4.8875(5.1587) | Xent 2.2462(1.9871) | Loss 11.0612(11.4409) | Error 0.7831(0.6953) Steps 0(0.00) | Grad Norm 26.9367(10.3775) | Total Time 0.00(0.00)\n",
      "Iter 0185 | Time 53.0356(45.4857) | Bit/dim 4.8369(5.1490) | Xent 1.9115(1.9848) | Loss 10.6862(11.4182) | Error 0.6738(0.6947) Steps 0(0.00) | Grad Norm 12.7483(10.4486) | Total Time 0.00(0.00)\n",
      "Iter 0186 | Time 54.7879(45.7647) | Bit/dim 4.9267(5.1424) | Xent 1.9824(1.9848) | Loss 10.8599(11.4015) | Error 0.7020(0.6949) Steps 0(0.00) | Grad Norm 10.2108(10.4414) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0031 | Time 20.7357, Epoch Time 355.7164(303.2000), Bit/dim 4.8370(best: 4.8146), Xent 1.9209, Loss 5.7974, Error 0.6739(best: 0.6464)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0187 | Time 50.1725(45.8970) | Bit/dim 4.8447(5.1334) | Xent 1.9347(1.9833) | Loss 12.4224(11.4321) | Error 0.6850(0.6946) Steps 0(0.00) | Grad Norm 6.7545(10.3308) | Total Time 0.00(0.00)\n",
      "Iter 0188 | Time 57.0650(46.2320) | Bit/dim 4.9069(5.1266) | Xent 1.9683(1.9828) | Loss 10.7689(11.4122) | Error 0.7032(0.6948) Steps 0(0.00) | Grad Norm 19.1544(10.5955) | Total Time 0.00(0.00)\n",
      "Iter 0189 | Time 44.8690(46.1911) | Bit/dim 5.0213(5.1235) | Xent 1.9640(1.9823) | Loss 11.1309(11.4038) | Error 0.7000(0.6950) Steps 0(0.00) | Grad Norm 13.7002(10.6887) | Total Time 0.00(0.00)\n",
      "Iter 0190 | Time 53.3685(46.4065) | Bit/dim 4.9853(5.1193) | Xent 1.9193(1.9804) | Loss 10.5394(11.3779) | Error 0.6759(0.6944) Steps 0(0.00) | Grad Norm 10.4111(10.6804) | Total Time 0.00(0.00)\n",
      "Iter 0191 | Time 45.4060(46.3764) | Bit/dim 4.8429(5.1110) | Xent 1.9306(1.9789) | Loss 10.3572(11.3472) | Error 0.6733(0.6938) Steps 0(0.00) | Grad Norm 8.1338(10.6040) | Total Time 0.00(0.00)\n",
      "Iter 0192 | Time 47.0233(46.3958) | Bit/dim 4.9300(5.1056) | Xent 1.9275(1.9773) | Loss 10.8782(11.3332) | Error 0.6727(0.6932) Steps 0(0.00) | Grad Norm 17.0647(10.7978) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0032 | Time 19.7665, Epoch Time 333.5180(304.1096), Bit/dim 4.8667(best: 4.8146), Xent 1.8654, Loss 5.7994, Error 0.6437(best: 0.6464)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0193 | Time 47.0315(46.4149) | Bit/dim 4.8667(5.0984) | Xent 1.9084(1.9753) | Loss 12.5734(11.3704) | Error 0.6636(0.6923) Steps 0(0.00) | Grad Norm 5.9217(10.6515) | Total Time 0.00(0.00)\n",
      "Iter 0194 | Time 48.8865(46.4891) | Bit/dim 4.9545(5.0941) | Xent 1.8951(1.9729) | Loss 10.8796(11.3556) | Error 0.6724(0.6917) Steps 0(0.00) | Grad Norm 11.7145(10.6834) | Total Time 0.00(0.00)\n",
      "Iter 0195 | Time 52.6507(46.6739) | Bit/dim 4.8669(5.0873) | Xent 1.8997(1.9707) | Loss 10.7404(11.3372) | Error 0.6714(0.6911) Steps 0(0.00) | Grad Norm 9.3227(10.6426) | Total Time 0.00(0.00)\n",
      "Iter 0196 | Time 49.5122(46.7591) | Bit/dim 4.8328(5.0797) | Xent 1.8792(1.9679) | Loss 10.5815(11.3145) | Error 0.6623(0.6902) Steps 0(0.00) | Grad Norm 8.1906(10.5690) | Total Time 0.00(0.00)\n",
      "Iter 0197 | Time 47.6619(46.7861) | Bit/dim 4.8318(5.0722) | Xent 1.8789(1.9652) | Loss 10.3838(11.2866) | Error 0.6579(0.6892) Steps 0(0.00) | Grad Norm 12.1530(10.6165) | Total Time 0.00(0.00)\n",
      "Iter 0198 | Time 52.9279(46.9704) | Bit/dim 4.8249(5.0648) | Xent 1.8561(1.9620) | Loss 10.5201(11.2636) | Error 0.6525(0.6881) Steps 0(0.00) | Grad Norm 5.9906(10.4777) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0033 | Time 19.8609, Epoch Time 333.9023(305.0034), Bit/dim 4.8181(best: 4.8146), Xent 1.8425, Loss 5.7393, Error 0.6439(best: 0.6437)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0199 | Time 52.9177(47.1488) | Bit/dim 4.8129(5.0572) | Xent 1.8607(1.9589) | Loss 12.4676(11.2997) | Error 0.6570(0.6872) Steps 0(0.00) | Grad Norm 6.9365(10.3715) | Total Time 0.00(0.00)\n",
      "Iter 0200 | Time 51.3366(47.2745) | Bit/dim 4.7980(5.0495) | Xent 1.8796(1.9566) | Loss 10.3930(11.2725) | Error 0.6556(0.6862) Steps 0(0.00) | Grad Norm 5.9943(10.2402) | Total Time 0.00(0.00)\n",
      "Iter 0201 | Time 50.1059(47.3594) | Bit/dim 4.7582(5.0407) | Xent 1.8594(1.9536) | Loss 10.4504(11.2479) | Error 0.6629(0.6855) Steps 0(0.00) | Grad Norm 4.8536(10.0786) | Total Time 0.00(0.00)\n",
      "Iter 0202 | Time 51.5281(47.4845) | Bit/dim 4.7513(5.0321) | Xent 1.8631(1.9509) | Loss 10.6509(11.2299) | Error 0.6546(0.6846) Steps 0(0.00) | Grad Norm 5.5545(9.9429) | Total Time 0.00(0.00)\n",
      "Iter 0203 | Time 51.5429(47.6062) | Bit/dim 4.7610(5.0239) | Xent 1.8373(1.9475) | Loss 10.2774(11.2014) | Error 0.6427(0.6834) Steps 0(0.00) | Grad Norm 4.0361(9.7657) | Total Time 0.00(0.00)\n",
      "Iter 0204 | Time 50.0485(47.6795) | Bit/dim 4.7592(5.0160) | Xent 1.8318(1.9440) | Loss 10.4087(11.1776) | Error 0.6436(0.6822) Steps 0(0.00) | Grad Norm 77.6558(11.8024) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0034 | Time 20.0697, Epoch Time 343.5095(306.1586), Bit/dim 4.7472(best: 4.8146), Xent 1.8179, Loss 5.6561, Error 0.6472(best: 0.6437)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0205 | Time 51.7954(47.8030) | Bit/dim 4.7471(5.0079) | Xent 1.8449(1.9411) | Loss 12.3718(11.2134) | Error 0.6558(0.6814) Steps 0(0.00) | Grad Norm 7.3796(11.6697) | Total Time 0.00(0.00)\n",
      "Iter 0206 | Time 67.1792(48.3842) | Bit/dim 4.7305(4.9996) | Xent 1.8791(1.9392) | Loss 10.5482(11.1935) | Error 0.6779(0.6813) Steps 0(0.00) | Grad Norm 8.2596(11.5674) | Total Time 0.00(0.00)\n",
      "Iter 0207 | Time 57.1979(48.6487) | Bit/dim 4.7269(4.9914) | Xent 1.8343(1.9361) | Loss 10.4406(11.1709) | Error 0.6560(0.6805) Steps 0(0.00) | Grad Norm 3.9295(11.3383) | Total Time 0.00(0.00)\n",
      "Iter 0208 | Time 63.5580(49.0959) | Bit/dim 4.7121(4.9830) | Xent 1.8476(1.9334) | Loss 10.3319(11.1457) | Error 0.6481(0.6795) Steps 0(0.00) | Grad Norm 7319.4133(230.5805) | Total Time 0.00(0.00)\n",
      "Iter 0209 | Time 64.0215(49.5437) | Bit/dim 4.7185(4.9751) | Xent 1.8429(1.9307) | Loss 10.2470(11.1187) | Error 0.6461(0.6785) Steps 0(0.00) | Grad Norm 6.5051(223.8582) | Total Time 0.00(0.00)\n",
      "Iter 0210 | Time 66.9912(50.0671) | Bit/dim 4.7049(4.9670) | Xent 1.8070(1.9270) | Loss 10.2308(11.0921) | Error 0.6421(0.6774) Steps 0(0.00) | Grad Norm 105941808463947.4531(3178254254135.5688) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0035 | Time 20.9044, Epoch Time 407.3054(309.1930), Bit/dim 4.7377(best: 4.7472), Xent 1.7788, Loss 5.6271, Error 0.6252(best: 0.6437)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_rl.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 8000 --test_batch_size 5000 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_rl_stdscale_60_run1 --resume ../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_rl_stdscale_60_run1/epoch_28_checkpt.pth --seed 1 --conditional True --controlled_tol True --train_mode semisup --lr 0.01 --warmup_iters 1000 --atol 1e-4  --rtol 1e-4 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5 --scale_fac 1.0 --scale_std 60.0 --max_grad_norm 20.0\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
