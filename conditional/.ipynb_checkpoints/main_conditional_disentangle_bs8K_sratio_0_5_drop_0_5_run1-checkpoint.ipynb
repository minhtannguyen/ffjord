{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "if args.controlled_tol:\n",
      "    import lib.odenvp_conditional_tol as odenvp\n",
      "else:\n",
      "    import lib.odenvp_conditional as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            loss.backward()\n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=0.0001, autoencode=False, batch_norm=False, batch_size=8000, batch_size_schedule='', begin_epoch=1, condition_ratio=0.25, conditional=True, controlled_tol=True, conv=True, data='mnist', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=1, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume='../experiments_published/cnf_conditional_disentangle_bs8K_sratio_0_25_drop_0_5_run1/epoch_125_checkpt.pth', rtol=0.0001, save='../experiments_published/cnf_conditional_disentangle_bs8K_sratio_0_25_drop_0_5_run1', seed=0, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=5000, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=113.0, weight_decay=0.0, weight_y=0.5)\n",
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=392, bias=True)\n",
      "  (project_class): LinearZeros(in_features=196, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 807722\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc.py:286: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "Iter 0876 | Time 69.8261(31.6701) | Bit/dim 1.2181(1.2396) | Xent 0.0929(0.1190) | Loss 1.2645(1.2991) | Error 0.0305(0.0366) Steps 422(419.44) | Grad Norm 1.7107(3.3612) | Total Time 10.00(10.00)\n",
      "Iter 0877 | Time 30.8582(31.6457) | Bit/dim 1.2080(1.2387) | Xent 0.1006(0.1184) | Loss 1.2584(1.2979) | Error 0.0300(0.0364) Steps 422(419.52) | Grad Norm 1.5385(3.3065) | Total Time 10.00(10.00)\n",
      "Iter 0878 | Time 31.7081(31.6476) | Bit/dim 1.2056(1.2377) | Xent 0.0970(0.1178) | Loss 1.2541(1.2966) | Error 0.0296(0.0362) Steps 422(419.59) | Grad Norm 1.1980(3.2432) | Total Time 10.00(10.00)\n",
      "Iter 0879 | Time 29.7231(31.5899) | Bit/dim 1.2045(1.2367) | Xent 0.0960(0.1171) | Loss 1.2525(1.2953) | Error 0.0315(0.0360) Steps 416(419.48) | Grad Norm 0.7201(3.1675) | Total Time 10.00(10.00)\n",
      "Iter 0880 | Time 29.9594(31.5409) | Bit/dim 1.1956(1.2355) | Xent 0.1153(0.1171) | Loss 1.2532(1.2940) | Error 0.0344(0.0360) Steps 416(419.38) | Grad Norm 0.4168(3.0850) | Total Time 10.00(10.00)\n",
      "Iter 0881 | Time 30.3425(31.5050) | Bit/dim 1.1891(1.2341) | Xent 0.0959(0.1164) | Loss 1.2371(1.2923) | Error 0.0308(0.0358) Steps 416(419.28) | Grad Norm 0.7058(3.0136) | Total Time 10.00(10.00)\n",
      "Iter 0882 | Time 29.9582(31.4586) | Bit/dim 1.1949(1.2329) | Xent 0.1138(0.1163) | Loss 1.2518(1.2911) | Error 0.0349(0.0358) Steps 416(419.18) | Grad Norm 1.0481(2.9547) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0126 | Time 26.6769, Epoch Time 291.8184(236.6442), Bit/dim 1.1884(best: inf), Xent 0.0519, Loss 1.2143, Error 0.0169(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0883 | Time 31.2411(31.4521) | Bit/dim 1.1967(1.2318) | Xent 0.1147(0.1163) | Loss 1.2540(1.2900) | Error 0.0353(0.0358) Steps 416(419.08) | Grad Norm 1.1521(2.9006) | Total Time 10.00(10.00)\n",
      "Iter 0884 | Time 29.5338(31.3945) | Bit/dim 1.1948(1.2307) | Xent 0.1135(0.1162) | Loss 1.2516(1.2888) | Error 0.0337(0.0357) Steps 416(418.99) | Grad Norm 0.9925(2.8434) | Total Time 10.00(10.00)\n",
      "Iter 0885 | Time 30.6365(31.3718) | Bit/dim 1.1900(1.2295) | Xent 0.1128(0.1161) | Loss 1.2463(1.2875) | Error 0.0351(0.0357) Steps 416(418.90) | Grad Norm 0.7954(2.7819) | Total Time 10.00(10.00)\n",
      "Iter 0886 | Time 30.6608(31.3504) | Bit/dim 1.1976(1.2285) | Xent 0.1044(0.1158) | Loss 1.2497(1.2864) | Error 0.0351(0.0357) Steps 416(418.81) | Grad Norm 0.7526(2.7210) | Total Time 10.00(10.00)\n",
      "Iter 0887 | Time 29.8707(31.3061) | Bit/dim 1.1911(1.2274) | Xent 0.1112(0.1156) | Loss 1.2468(1.2852) | Error 0.0341(0.0356) Steps 416(418.73) | Grad Norm 0.8732(2.6656) | Total Time 10.00(10.00)\n",
      "Iter 0888 | Time 30.1653(31.2718) | Bit/dim 1.1987(1.2265) | Xent 0.0994(0.1151) | Loss 1.2484(1.2841) | Error 0.0304(0.0355) Steps 416(418.65) | Grad Norm 0.9776(2.6150) | Total Time 10.00(10.00)\n",
      "Iter 0889 | Time 30.9244(31.2614) | Bit/dim 1.1929(1.2255) | Xent 0.1016(0.1147) | Loss 1.2437(1.2829) | Error 0.0314(0.0354) Steps 416(418.57) | Grad Norm 0.8072(2.5607) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0127 | Time 15.8093, Epoch Time 241.3948(236.7867), Bit/dim 1.1877(best: 1.1884), Xent 0.0502, Loss 1.2128, Error 0.0162(best: 0.0169)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0890 | Time 29.4719(31.2077) | Bit/dim 1.1979(1.2247) | Xent 0.0902(0.1140) | Loss 1.2430(1.2817) | Error 0.0281(0.0351) Steps 416(418.49) | Grad Norm 0.5849(2.5015) | Total Time 10.00(10.00)\n",
      "Iter 0891 | Time 30.4134(31.1839) | Bit/dim 1.1910(1.2237) | Xent 0.1099(0.1139) | Loss 1.2459(1.2806) | Error 0.0347(0.0351) Steps 416(418.42) | Grad Norm 0.4857(2.4410) | Total Time 10.00(10.00)\n",
      "Iter 0892 | Time 29.8280(31.1432) | Bit/dim 1.1926(1.2228) | Xent 0.0969(0.1134) | Loss 1.2411(1.2794) | Error 0.0289(0.0349) Steps 416(418.34) | Grad Norm 0.7519(2.3903) | Total Time 10.00(10.00)\n",
      "Iter 0893 | Time 30.4713(31.1231) | Bit/dim 1.1922(1.2218) | Xent 0.0982(0.1129) | Loss 1.2413(1.2783) | Error 0.0310(0.0348) Steps 416(418.27) | Grad Norm 0.9290(2.3465) | Total Time 10.00(10.00)\n",
      "Iter 0894 | Time 29.9347(31.0874) | Bit/dim 1.1951(1.2210) | Xent 0.1000(0.1125) | Loss 1.2450(1.2773) | Error 0.0305(0.0347) Steps 416(418.21) | Grad Norm 0.8632(2.3020) | Total Time 10.00(10.00)\n",
      "Iter 0895 | Time 30.7038(31.0759) | Bit/dim 1.1860(1.2200) | Xent 0.1048(0.1123) | Loss 1.2384(1.2761) | Error 0.0311(0.0346) Steps 416(418.14) | Grad Norm 0.5359(2.2490) | Total Time 10.00(10.00)\n",
      "Iter 0896 | Time 31.7446(31.0960) | Bit/dim 1.1955(1.2193) | Xent 0.0977(0.1118) | Loss 1.2443(1.2752) | Error 0.0295(0.0344) Steps 416(418.07) | Grad Norm 0.3258(2.1913) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0128 | Time 15.5009, Epoch Time 241.0434(236.9144), Bit/dim 1.1853(best: 1.1877), Xent 0.0496, Loss 1.2101, Error 0.0169(best: 0.0162)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0897 | Time 29.9764(31.0624) | Bit/dim 1.1964(1.2186) | Xent 0.1015(0.1115) | Loss 1.2471(1.2743) | Error 0.0302(0.0343) Steps 416(418.01) | Grad Norm 0.4594(2.1393) | Total Time 10.00(10.00)\n",
      "Iter 0898 | Time 29.9436(31.0288) | Bit/dim 1.1878(1.2176) | Xent 0.0992(0.1112) | Loss 1.2374(1.2732) | Error 0.0291(0.0341) Steps 416(417.95) | Grad Norm 0.6177(2.0937) | Total Time 10.00(10.00)\n",
      "Iter 0899 | Time 30.4434(31.0112) | Bit/dim 1.1922(1.2169) | Xent 0.0979(0.1108) | Loss 1.2411(1.2723) | Error 0.0320(0.0341) Steps 416(417.89) | Grad Norm 0.6700(2.0510) | Total Time 10.00(10.00)\n",
      "Iter 0900 | Time 30.1635(30.9858) | Bit/dim 1.1923(1.2161) | Xent 0.0947(0.1103) | Loss 1.2396(1.2713) | Error 0.0300(0.0340) Steps 416(417.84) | Grad Norm 0.6505(2.0090) | Total Time 10.00(10.00)\n",
      "Iter 0901 | Time 29.7181(30.9478) | Bit/dim 1.1877(1.2153) | Xent 0.1021(0.1100) | Loss 1.2387(1.2703) | Error 0.0314(0.0339) Steps 416(417.78) | Grad Norm 0.3941(1.9605) | Total Time 10.00(10.00)\n",
      "Iter 0902 | Time 30.4156(30.9318) | Bit/dim 1.1883(1.2145) | Xent 0.1047(0.1099) | Loss 1.2406(1.2694) | Error 0.0336(0.0339) Steps 416(417.73) | Grad Norm 0.3598(1.9125) | Total Time 10.00(10.00)\n",
      "Iter 0903 | Time 30.0114(30.9042) | Bit/dim 1.1887(1.2137) | Xent 0.1043(0.1097) | Loss 1.2408(1.2686) | Error 0.0330(0.0338) Steps 416(417.68) | Grad Norm 0.4614(1.8690) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0129 | Time 15.6419, Epoch Time 238.8869(236.9736), Bit/dim 1.1826(best: 1.1853), Xent 0.0500, Loss 1.2076, Error 0.0163(best: 0.0162)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0904 | Time 29.6461(30.8665) | Bit/dim 1.1871(1.2129) | Xent 0.0866(0.1090) | Loss 1.2304(1.2674) | Error 0.0256(0.0336) Steps 416(417.63) | Grad Norm 0.5267(1.8287) | Total Time 10.00(10.00)\n",
      "Iter 0905 | Time 29.7529(30.8331) | Bit/dim 1.1927(1.2123) | Xent 0.0963(0.1086) | Loss 1.2409(1.2666) | Error 0.0296(0.0335) Steps 416(417.58) | Grad Norm 0.3835(1.7853) | Total Time 10.00(10.00)\n",
      "Iter 0906 | Time 30.4732(30.8223) | Bit/dim 1.1877(1.2116) | Xent 0.0966(0.1083) | Loss 1.2360(1.2657) | Error 0.0286(0.0333) Steps 416(417.53) | Grad Norm 0.4103(1.7441) | Total Time 10.00(10.00)\n",
      "Iter 0907 | Time 30.3144(30.8070) | Bit/dim 1.1858(1.2108) | Xent 0.0918(0.1078) | Loss 1.2317(1.2647) | Error 0.0288(0.0332) Steps 416(417.48) | Grad Norm 0.2896(1.7005) | Total Time 10.00(10.00)\n",
      "Iter 0908 | Time 30.9369(30.8109) | Bit/dim 1.1885(1.2101) | Xent 0.1075(0.1078) | Loss 1.2423(1.2640) | Error 0.0335(0.0332) Steps 416(417.44) | Grad Norm 0.2719(1.6576) | Total Time 10.00(10.00)\n",
      "Iter 0909 | Time 30.1420(30.7909) | Bit/dim 1.1855(1.2094) | Xent 0.0964(0.1074) | Loss 1.2338(1.2631) | Error 0.0294(0.0331) Steps 416(417.40) | Grad Norm 0.4193(1.6205) | Total Time 10.00(10.00)\n",
      "Iter 0910 | Time 29.9563(30.7658) | Bit/dim 1.1905(1.2088) | Xent 0.1047(0.1073) | Loss 1.2428(1.2625) | Error 0.0357(0.0332) Steps 416(417.35) | Grad Norm 0.4008(1.5839) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0130 | Time 15.5791, Epoch Time 239.2897(237.0430), Bit/dim 1.1813(best: 1.1826), Xent 0.0481, Loss 1.2054, Error 0.0160(best: 0.0162)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0911 | Time 31.1609(30.7777) | Bit/dim 1.1919(1.2083) | Xent 0.1000(0.1071) | Loss 1.2419(1.2619) | Error 0.0301(0.0331) Steps 416(417.31) | Grad Norm 0.4354(1.5494) | Total Time 10.00(10.00)\n",
      "Iter 0912 | Time 29.9578(30.7531) | Bit/dim 1.1871(1.2077) | Xent 0.0950(0.1068) | Loss 1.2347(1.2611) | Error 0.0279(0.0329) Steps 416(417.27) | Grad Norm 0.3130(1.5123) | Total Time 10.00(10.00)\n",
      "Iter 0913 | Time 30.0063(30.7307) | Bit/dim 1.1873(1.2071) | Xent 0.1034(0.1067) | Loss 1.2390(1.2604) | Error 0.0310(0.0329) Steps 416(417.24) | Grad Norm 0.3280(1.4768) | Total Time 10.00(10.00)\n",
      "Iter 0914 | Time 29.8790(30.7051) | Bit/dim 1.1907(1.2066) | Xent 0.0933(0.1063) | Loss 1.2374(1.2597) | Error 0.0288(0.0327) Steps 416(417.20) | Grad Norm 0.2221(1.4391) | Total Time 10.00(10.00)\n",
      "Iter 0915 | Time 30.8397(30.7092) | Bit/dim 1.1817(1.2058) | Xent 0.1022(0.1061) | Loss 1.2329(1.2589) | Error 0.0312(0.0327) Steps 416(417.16) | Grad Norm 0.3489(1.4064) | Total Time 10.00(10.00)\n",
      "Iter 0916 | Time 30.2070(30.6941) | Bit/dim 1.1871(1.2053) | Xent 0.0872(0.1056) | Loss 1.2308(1.2581) | Error 0.0280(0.0326) Steps 416(417.13) | Grad Norm 0.2763(1.3725) | Total Time 10.00(10.00)\n",
      "Iter 0917 | Time 30.6289(30.6921) | Bit/dim 1.1841(1.2046) | Xent 0.0922(0.1052) | Loss 1.2302(1.2572) | Error 0.0308(0.0325) Steps 416(417.09) | Grad Norm 0.3863(1.3429) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0131 | Time 15.8342, Epoch Time 241.4206(237.1744), Bit/dim 1.1800(best: 1.1813), Xent 0.0515, Loss 1.2057, Error 0.0176(best: 0.0160)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0918 | Time 30.0904(30.6741) | Bit/dim 1.1859(1.2041) | Xent 0.1095(0.1053) | Loss 1.2407(1.2567) | Error 0.0331(0.0325) Steps 428(417.42) | Grad Norm 0.3795(1.3140) | Total Time 10.00(10.00)\n",
      "Iter 0919 | Time 30.5531(30.6705) | Bit/dim 1.1873(1.2036) | Xent 0.1036(0.1053) | Loss 1.2391(1.2562) | Error 0.0339(0.0326) Steps 416(417.38) | Grad Norm 0.2804(1.2830) | Total Time 10.00(10.00)\n",
      "Iter 0920 | Time 30.0110(30.6507) | Bit/dim 1.1805(1.2029) | Xent 0.0906(0.1048) | Loss 1.2258(1.2553) | Error 0.0265(0.0324) Steps 416(417.34) | Grad Norm 0.2801(1.2529) | Total Time 10.00(10.00)\n",
      "Iter 0921 | Time 29.5054(30.6163) | Bit/dim 1.1851(1.2023) | Xent 0.1058(0.1048) | Loss 1.2380(1.2548) | Error 0.0325(0.0324) Steps 416(417.30) | Grad Norm 0.3416(1.2256) | Total Time 10.00(10.00)\n",
      "Iter 0922 | Time 30.2490(30.6053) | Bit/dim 1.1855(1.2018) | Xent 0.0937(0.1045) | Loss 1.2323(1.2541) | Error 0.0300(0.0323) Steps 416(417.26) | Grad Norm 0.2744(1.1971) | Total Time 10.00(10.00)\n",
      "Iter 0923 | Time 30.0713(30.5893) | Bit/dim 1.1864(1.2014) | Xent 0.0997(0.1044) | Loss 1.2363(1.2536) | Error 0.0314(0.0323) Steps 416(417.22) | Grad Norm 0.3016(1.1702) | Total Time 10.00(10.00)\n",
      "Iter 0924 | Time 31.6285(30.6205) | Bit/dim 1.1887(1.2010) | Xent 0.0988(0.1042) | Loss 1.2381(1.2531) | Error 0.0308(0.0322) Steps 416(417.18) | Grad Norm 0.2468(1.1425) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0132 | Time 15.7548, Epoch Time 240.5042(237.2743), Bit/dim 1.1789(best: 1.1800), Xent 0.0492, Loss 1.2035, Error 0.0165(best: 0.0160)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0925 | Time 30.5753(30.6191) | Bit/dim 1.1831(1.2005) | Xent 0.0917(0.1038) | Loss 1.2289(1.2524) | Error 0.0309(0.0322) Steps 416(417.15) | Grad Norm 0.3370(1.1183) | Total Time 10.00(10.00)\n",
      "Iter 0926 | Time 30.9725(30.6297) | Bit/dim 1.1846(1.2000) | Xent 0.1002(0.1037) | Loss 1.2347(1.2518) | Error 0.0330(0.0322) Steps 428(417.47) | Grad Norm 0.2251(1.0915) | Total Time 10.00(10.00)\n",
      "Iter 0927 | Time 30.8693(30.6369) | Bit/dim 1.1886(1.1996) | Xent 0.0948(0.1034) | Loss 1.2360(1.2514) | Error 0.0294(0.0321) Steps 428(417.79) | Grad Norm 0.3188(1.0684) | Total Time 10.00(10.00)\n",
      "Iter 0928 | Time 31.2599(30.6556) | Bit/dim 1.1815(1.1991) | Xent 0.1047(0.1035) | Loss 1.2339(1.2508) | Error 0.0329(0.0322) Steps 416(417.74) | Grad Norm 0.3003(1.0453) | Total Time 10.00(10.00)\n",
      "Iter 0929 | Time 30.8477(30.6613) | Bit/dim 1.1879(1.1988) | Xent 0.0959(0.1033) | Loss 1.2359(1.2504) | Error 0.0286(0.0321) Steps 428(418.04) | Grad Norm 0.2291(1.0208) | Total Time 10.00(10.00)\n",
      "Iter 0930 | Time 31.4669(30.6855) | Bit/dim 1.1854(1.1984) | Xent 0.0968(0.1031) | Loss 1.2338(1.2499) | Error 0.0282(0.0319) Steps 428(418.34) | Grad Norm 0.3167(0.9997) | Total Time 10.00(10.00)\n",
      "Iter 0931 | Time 30.7836(30.6884) | Bit/dim 1.1813(1.1978) | Xent 0.0999(0.1030) | Loss 1.2312(1.2493) | Error 0.0296(0.0319) Steps 428(418.63) | Grad Norm 0.2041(0.9758) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0133 | Time 15.9574, Epoch Time 245.4094(237.5183), Bit/dim 1.1780(best: 1.1789), Xent 0.0489, Loss 1.2024, Error 0.0164(best: 0.0160)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0932 | Time 31.7300(30.7197) | Bit/dim 1.1836(1.1974) | Xent 0.0998(0.1029) | Loss 1.2335(1.2489) | Error 0.0311(0.0318) Steps 416(418.55) | Grad Norm 0.2570(0.9543) | Total Time 10.00(10.00)\n",
      "Iter 0933 | Time 30.7247(30.7198) | Bit/dim 1.1852(1.1971) | Xent 0.0925(0.1026) | Loss 1.2314(1.2483) | Error 0.0279(0.0317) Steps 428(418.84) | Grad Norm 0.2548(0.9333) | Total Time 10.00(10.00)\n",
      "Iter 0934 | Time 31.4432(30.7415) | Bit/dim 1.1796(1.1965) | Xent 0.0898(0.1022) | Loss 1.2245(1.2476) | Error 0.0282(0.0316) Steps 428(419.11) | Grad Norm 0.2909(0.9140) | Total Time 10.00(10.00)\n",
      "Iter 0935 | Time 31.1534(30.7539) | Bit/dim 1.1806(1.1961) | Xent 0.0968(0.1020) | Loss 1.2290(1.2471) | Error 0.0309(0.0316) Steps 428(419.38) | Grad Norm 0.2749(0.8948) | Total Time 10.00(10.00)\n",
      "Iter 0936 | Time 31.0177(30.7618) | Bit/dim 1.1843(1.1957) | Xent 0.1013(0.1020) | Loss 1.2350(1.2467) | Error 0.0315(0.0316) Steps 428(419.64) | Grad Norm 0.2048(0.8741) | Total Time 10.00(10.00)\n",
      "Iter 0937 | Time 30.4081(30.7512) | Bit/dim 1.1828(1.1953) | Xent 0.1062(0.1021) | Loss 1.2359(1.2464) | Error 0.0315(0.0316) Steps 428(419.89) | Grad Norm 0.2572(0.8556) | Total Time 10.00(10.00)\n",
      "Iter 0938 | Time 31.3032(30.7678) | Bit/dim 1.1844(1.1950) | Xent 0.0904(0.1018) | Loss 1.2296(1.2459) | Error 0.0282(0.0315) Steps 428(420.13) | Grad Norm 0.1793(0.8353) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0134 | Time 16.4806, Epoch Time 247.4403(237.8160), Bit/dim 1.1764(best: 1.1780), Xent 0.0493, Loss 1.2010, Error 0.0155(best: 0.0160)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0939 | Time 30.1221(30.7484) | Bit/dim 1.1779(1.1945) | Xent 0.0976(0.1016) | Loss 1.2267(1.2453) | Error 0.0305(0.0315) Steps 428(420.37) | Grad Norm 0.2096(0.8166) | Total Time 10.00(10.00)\n",
      "Iter 0940 | Time 31.0557(30.7576) | Bit/dim 1.1862(1.1942) | Xent 0.0970(0.1015) | Loss 1.2347(1.2450) | Error 0.0291(0.0314) Steps 428(420.60) | Grad Norm 0.1935(0.7979) | Total Time 10.00(10.00)\n",
      "Iter 0941 | Time 32.1428(30.7992) | Bit/dim 1.1798(1.1938) | Xent 0.1059(0.1016) | Loss 1.2328(1.2446) | Error 0.0325(0.0314) Steps 428(420.82) | Grad Norm 0.2082(0.7802) | Total Time 10.00(10.00)\n",
      "Iter 0942 | Time 31.5370(30.8213) | Bit/dim 1.1821(1.1934) | Xent 0.0935(0.1014) | Loss 1.2289(1.2441) | Error 0.0304(0.0314) Steps 428(421.03) | Grad Norm 0.2359(0.7639) | Total Time 10.00(10.00)\n",
      "Iter 0943 | Time 30.7209(30.8183) | Bit/dim 1.1863(1.1932) | Xent 0.0950(0.1012) | Loss 1.2339(1.2438) | Error 0.0270(0.0313) Steps 428(421.24) | Grad Norm 0.2025(0.7470) | Total Time 10.00(10.00)\n",
      "Iter 0944 | Time 32.1606(30.8586) | Bit/dim 1.1811(1.1929) | Xent 0.1083(0.1014) | Loss 1.2352(1.2436) | Error 0.0340(0.0313) Steps 428(421.45) | Grad Norm 0.2648(0.7326) | Total Time 10.00(10.00)\n",
      "Iter 0945 | Time 30.2641(30.8407) | Bit/dim 1.1823(1.1925) | Xent 0.0950(0.1012) | Loss 1.2298(1.2432) | Error 0.0300(0.0313) Steps 428(421.64) | Grad Norm 0.2753(0.7188) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0135 | Time 16.3140, Epoch Time 246.7698(238.0846), Bit/dim 1.1761(best: 1.1764), Xent 0.0500, Loss 1.2011, Error 0.0167(best: 0.0155)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0946 | Time 30.8255(30.8403) | Bit/dim 1.1786(1.1921) | Xent 0.1089(0.1015) | Loss 1.2331(1.2429) | Error 0.0341(0.0314) Steps 428(421.83) | Grad Norm 0.2403(0.7045) | Total Time 10.00(10.00)\n",
      "Iter 0947 | Time 31.4282(30.8579) | Bit/dim 1.1795(1.1918) | Xent 0.1009(0.1014) | Loss 1.2300(1.2425) | Error 0.0320(0.0314) Steps 428(422.02) | Grad Norm 0.2990(0.6923) | Total Time 10.00(10.00)\n",
      "Iter 0948 | Time 31.6231(30.8809) | Bit/dim 1.1840(1.1915) | Xent 0.0963(0.1013) | Loss 1.2322(1.2422) | Error 0.0301(0.0314) Steps 428(422.20) | Grad Norm 0.1831(0.6770) | Total Time 10.00(10.00)\n",
      "Iter 0949 | Time 30.5994(30.8724) | Bit/dim 1.1793(1.1912) | Xent 0.1153(0.1017) | Loss 1.2370(1.2420) | Error 0.0346(0.0315) Steps 428(422.37) | Grad Norm 0.2374(0.6639) | Total Time 10.00(10.00)\n",
      "Iter 0950 | Time 30.8662(30.8722) | Bit/dim 1.1853(1.1910) | Xent 0.0961(0.1015) | Loss 1.2333(1.2417) | Error 0.0304(0.0314) Steps 428(422.54) | Grad Norm 0.2393(0.6511) | Total Time 10.00(10.00)\n",
      "Iter 0951 | Time 30.2799(30.8545) | Bit/dim 1.1852(1.1908) | Xent 0.1020(0.1016) | Loss 1.2362(1.2416) | Error 0.0317(0.0314) Steps 428(422.70) | Grad Norm 0.2582(0.6393) | Total Time 10.00(10.00)\n",
      "Iter 0952 | Time 31.5615(30.8757) | Bit/dim 1.1771(1.1904) | Xent 0.0963(0.1014) | Loss 1.2253(1.2411) | Error 0.0314(0.0314) Steps 428(422.86) | Grad Norm 0.2257(0.6269) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0136 | Time 15.9806, Epoch Time 245.5560(238.3087), Bit/dim 1.1752(best: 1.1761), Xent 0.0460, Loss 1.1982, Error 0.0152(best: 0.0155)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0953 | Time 30.5866(30.8670) | Bit/dim 1.1791(1.1901) | Xent 0.0969(0.1013) | Loss 1.2276(1.2407) | Error 0.0300(0.0314) Steps 428(423.02) | Grad Norm 0.1789(0.6135) | Total Time 10.00(10.00)\n",
      "Iter 0954 | Time 32.4735(30.9152) | Bit/dim 1.1823(1.1898) | Xent 0.1046(0.1014) | Loss 1.2345(1.2405) | Error 0.0299(0.0314) Steps 428(423.17) | Grad Norm 0.3247(0.6048) | Total Time 10.00(10.00)\n",
      "Iter 0955 | Time 31.0182(30.9183) | Bit/dim 1.1829(1.1896) | Xent 0.1097(0.1016) | Loss 1.2378(1.2404) | Error 0.0339(0.0314) Steps 428(423.31) | Grad Norm 0.2912(0.5954) | Total Time 10.00(10.00)\n",
      "Iter 0956 | Time 31.4396(30.9339) | Bit/dim 1.1837(1.1894) | Xent 0.1027(0.1016) | Loss 1.2351(1.2403) | Error 0.0340(0.0315) Steps 428(423.45) | Grad Norm 0.2524(0.5851) | Total Time 10.00(10.00)\n",
      "Iter 0957 | Time 31.5040(30.9510) | Bit/dim 1.1785(1.1891) | Xent 0.1033(0.1017) | Loss 1.2301(1.2400) | Error 0.0323(0.0315) Steps 428(423.59) | Grad Norm 0.2508(0.5751) | Total Time 10.00(10.00)\n",
      "Iter 0958 | Time 30.5546(30.9391) | Bit/dim 1.1808(1.1889) | Xent 0.0926(0.1014) | Loss 1.2271(1.2396) | Error 0.0289(0.0314) Steps 428(423.72) | Grad Norm 0.1851(0.5634) | Total Time 10.00(10.00)\n",
      "Iter 0959 | Time 32.2245(30.9777) | Bit/dim 1.1807(1.1886) | Xent 0.1004(0.1014) | Loss 1.2308(1.2393) | Error 0.0330(0.0315) Steps 428(423.85) | Grad Norm 0.2275(0.5533) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0137 | Time 16.1360, Epoch Time 248.5511(238.6160), Bit/dim 1.1744(best: 1.1752), Xent 0.0482, Loss 1.1984, Error 0.0154(best: 0.0152)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0960 | Time 30.5793(30.9657) | Bit/dim 1.1829(1.1884) | Xent 0.1017(0.1014) | Loss 1.2338(1.2391) | Error 0.0336(0.0316) Steps 428(423.97) | Grad Norm 0.3032(0.5458) | Total Time 10.00(10.00)\n",
      "Iter 0961 | Time 31.0826(30.9692) | Bit/dim 1.1785(1.1881) | Xent 0.0951(0.1012) | Loss 1.2260(1.2387) | Error 0.0290(0.0315) Steps 428(424.09) | Grad Norm 0.1830(0.5349) | Total Time 10.00(10.00)\n",
      "Iter 0962 | Time 30.9142(30.9676) | Bit/dim 1.1771(1.1878) | Xent 0.0945(0.1010) | Loss 1.2244(1.2383) | Error 0.0300(0.0314) Steps 428(424.21) | Grad Norm 0.1775(0.5242) | Total Time 10.00(10.00)\n",
      "Iter 0963 | Time 31.2687(30.9766) | Bit/dim 1.1833(1.1877) | Xent 0.1081(0.1012) | Loss 1.2373(1.2383) | Error 0.0321(0.0315) Steps 428(424.33) | Grad Norm 0.3002(0.5175) | Total Time 10.00(10.00)\n",
      "Iter 0964 | Time 31.1326(30.9813) | Bit/dim 1.1789(1.1874) | Xent 0.1034(0.1013) | Loss 1.2306(1.2381) | Error 0.0308(0.0314) Steps 428(424.44) | Grad Norm 0.2176(0.5085) | Total Time 10.00(10.00)\n",
      "Iter 0965 | Time 30.7082(30.9731) | Bit/dim 1.1797(1.1872) | Xent 0.0978(0.1012) | Loss 1.2286(1.2378) | Error 0.0309(0.0314) Steps 428(424.54) | Grad Norm 0.2461(0.5006) | Total Time 10.00(10.00)\n",
      "Iter 0966 | Time 30.7412(30.9662) | Bit/dim 1.1805(1.1870) | Xent 0.1034(0.1012) | Loss 1.2323(1.2376) | Error 0.0327(0.0315) Steps 428(424.65) | Grad Norm 0.1939(0.4914) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0138 | Time 16.1102, Epoch Time 245.1945(238.8134), Bit/dim 1.1729(best: 1.1744), Xent 0.0493, Loss 1.1975, Error 0.0156(best: 0.0152)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0967 | Time 30.9117(30.9645) | Bit/dim 1.1773(1.1867) | Xent 0.1063(0.1014) | Loss 1.2305(1.2374) | Error 0.0319(0.0315) Steps 428(424.75) | Grad Norm 0.1614(0.4815) | Total Time 10.00(10.00)\n",
      "Iter 0968 | Time 30.6053(30.9537) | Bit/dim 1.1796(1.1865) | Xent 0.0955(0.1012) | Loss 1.2273(1.2371) | Error 0.0291(0.0314) Steps 428(424.84) | Grad Norm 0.2443(0.4744) | Total Time 10.00(10.00)\n",
      "Iter 0969 | Time 31.0763(30.9574) | Bit/dim 1.1798(1.1863) | Xent 0.0904(0.1009) | Loss 1.2250(1.2367) | Error 0.0281(0.0313) Steps 410(424.40) | Grad Norm 0.2577(0.4679) | Total Time 10.00(10.00)\n",
      "Iter 0970 | Time 30.9522(30.9573) | Bit/dim 1.1812(1.1861) | Xent 0.0922(0.1006) | Loss 1.2273(1.2364) | Error 0.0286(0.0312) Steps 428(424.51) | Grad Norm 0.2210(0.4605) | Total Time 10.00(10.00)\n",
      "Iter 0971 | Time 32.0792(30.9909) | Bit/dim 1.1747(1.1858) | Xent 0.1086(0.1009) | Loss 1.2290(1.2362) | Error 0.0321(0.0313) Steps 428(424.61) | Grad Norm 0.2189(0.4532) | Total Time 10.00(10.00)\n",
      "Iter 0972 | Time 31.0485(30.9927) | Bit/dim 1.1825(1.1857) | Xent 0.0968(0.1007) | Loss 1.2309(1.2361) | Error 0.0298(0.0312) Steps 428(424.71) | Grad Norm 0.2036(0.4458) | Total Time 10.00(10.00)\n",
      "Iter 0973 | Time 33.1020(31.0559) | Bit/dim 1.1791(1.1855) | Xent 0.0952(0.1006) | Loss 1.2267(1.2358) | Error 0.0294(0.0312) Steps 428(424.81) | Grad Norm 0.2526(0.4400) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0139 | Time 16.2187, Epoch Time 248.4541(239.1026), Bit/dim 1.1735(best: 1.1729), Xent 0.0492, Loss 1.1981, Error 0.0167(best: 0.0152)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0974 | Time 30.6653(31.0442) | Bit/dim 1.1812(1.1854) | Xent 0.0909(0.1003) | Loss 1.2266(1.2355) | Error 0.0274(0.0310) Steps 410(424.37) | Grad Norm 0.2374(0.4339) | Total Time 10.00(10.00)\n",
      "Iter 0975 | Time 29.7736(31.0061) | Bit/dim 1.1756(1.1851) | Xent 0.0907(0.1000) | Loss 1.2210(1.2351) | Error 0.0290(0.0310) Steps 410(423.94) | Grad Norm 0.2428(0.4281) | Total Time 10.00(10.00)\n",
      "Iter 0976 | Time 30.4618(30.9898) | Bit/dim 1.1779(1.1849) | Xent 0.0961(0.0999) | Loss 1.2259(1.2348) | Error 0.0306(0.0310) Steps 422(423.88) | Grad Norm 0.2425(0.4226) | Total Time 10.00(10.00)\n",
      "Iter 0977 | Time 30.5865(30.9777) | Bit/dim 1.1799(1.1847) | Xent 0.1043(0.1000) | Loss 1.2320(1.2347) | Error 0.0299(0.0309) Steps 428(424.00) | Grad Norm 0.1662(0.4149) | Total Time 10.00(10.00)\n",
      "Iter 0978 | Time 30.9099(30.9756) | Bit/dim 1.1818(1.1846) | Xent 0.0967(0.0999) | Loss 1.2301(1.2346) | Error 0.0304(0.0309) Steps 422(423.94) | Grad Norm 0.2116(0.4088) | Total Time 10.00(10.00)\n",
      "Iter 0979 | Time 30.7274(30.9682) | Bit/dim 1.1742(1.1843) | Xent 0.1054(0.1001) | Loss 1.2268(1.2343) | Error 0.0330(0.0310) Steps 428(424.06) | Grad Norm 0.2071(0.4027) | Total Time 10.00(10.00)\n",
      "Iter 0980 | Time 30.0709(30.9413) | Bit/dim 1.1809(1.1842) | Xent 0.1138(0.1005) | Loss 1.2378(1.2344) | Error 0.0361(0.0311) Steps 422(424.00) | Grad Norm 0.2103(0.3970) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0140 | Time 16.0821, Epoch Time 241.8974(239.1864), Bit/dim 1.1717(best: 1.1729), Xent 0.0498, Loss 1.1966, Error 0.0171(best: 0.0152)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0981 | Time 30.2462(30.9204) | Bit/dim 1.1798(1.1841) | Xent 0.1023(0.1005) | Loss 1.2309(1.2343) | Error 0.0302(0.0311) Steps 422(423.94) | Grad Norm 0.2484(0.3925) | Total Time 10.00(10.00)\n",
      "Iter 0982 | Time 30.2454(30.9002) | Bit/dim 1.1777(1.1839) | Xent 0.0959(0.1004) | Loss 1.2256(1.2341) | Error 0.0304(0.0311) Steps 422(423.88) | Grad Norm 0.2102(0.3870) | Total Time 10.00(10.00)\n",
      "Iter 0983 | Time 30.7272(30.8950) | Bit/dim 1.1736(1.1836) | Xent 0.1007(0.1004) | Loss 1.2239(1.2338) | Error 0.0309(0.0311) Steps 422(423.83) | Grad Norm 0.2426(0.3827) | Total Time 10.00(10.00)\n",
      "Iter 0984 | Time 29.7911(30.8619) | Bit/dim 1.1813(1.1835) | Xent 0.0894(0.1001) | Loss 1.2260(1.2335) | Error 0.0268(0.0309) Steps 422(423.77) | Grad Norm 0.2094(0.3775) | Total Time 10.00(10.00)\n",
      "Iter 0985 | Time 31.1700(30.8711) | Bit/dim 1.1784(1.1833) | Xent 0.0970(0.1000) | Loss 1.2269(1.2333) | Error 0.0310(0.0310) Steps 422(423.72) | Grad Norm 0.2700(0.3743) | Total Time 10.00(10.00)\n",
      "Iter 0986 | Time 30.1117(30.8483) | Bit/dim 1.1760(1.1831) | Xent 0.1163(0.1005) | Loss 1.2342(1.2334) | Error 0.0345(0.0311) Steps 422(423.67) | Grad Norm 0.2947(0.3719) | Total Time 10.00(10.00)\n",
      "Iter 0987 | Time 30.4537(30.8365) | Bit/dim 1.1798(1.1830) | Xent 0.1069(0.1007) | Loss 1.2333(1.2334) | Error 0.0350(0.0312) Steps 422(423.62) | Grad Norm 0.2579(0.3685) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0141 | Time 15.9784, Epoch Time 241.1814(239.2463), Bit/dim 1.1713(best: 1.1717), Xent 0.0473, Loss 1.1950, Error 0.0153(best: 0.0152)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0988 | Time 30.8359(30.8365) | Bit/dim 1.1769(1.1828) | Xent 0.1077(0.1009) | Loss 1.2307(1.2333) | Error 0.0334(0.0312) Steps 422(423.57) | Grad Norm 0.3399(0.3676) | Total Time 10.00(10.00)\n",
      "Iter 0989 | Time 30.7023(30.8324) | Bit/dim 1.1757(1.1826) | Xent 0.1050(0.1010) | Loss 1.2282(1.2331) | Error 0.0296(0.0312) Steps 422(423.52) | Grad Norm 0.1702(0.3617) | Total Time 10.00(10.00)\n",
      "Iter 0990 | Time 30.7331(30.8295) | Bit/dim 1.1778(1.1825) | Xent 0.1023(0.1011) | Loss 1.2290(1.2330) | Error 0.0312(0.0312) Steps 422(423.48) | Grad Norm 0.3188(0.3604) | Total Time 10.00(10.00)\n",
      "Iter 0991 | Time 31.0451(30.8359) | Bit/dim 1.1729(1.1822) | Xent 0.0919(0.1008) | Loss 1.2189(1.2326) | Error 0.0289(0.0311) Steps 422(423.43) | Grad Norm 0.4132(0.3620) | Total Time 10.00(10.00)\n",
      "Iter 0992 | Time 30.1891(30.8165) | Bit/dim 1.1828(1.1822) | Xent 0.0971(0.1007) | Loss 1.2313(1.2325) | Error 0.0296(0.0311) Steps 422(423.39) | Grad Norm 0.2299(0.3580) | Total Time 10.00(10.00)\n",
      "Iter 0993 | Time 30.5246(30.8078) | Bit/dim 1.1751(1.1820) | Xent 0.0972(0.1006) | Loss 1.2237(1.2323) | Error 0.0289(0.0310) Steps 422(423.35) | Grad Norm 0.4037(0.3594) | Total Time 10.00(10.00)\n",
      "Iter 0994 | Time 30.9905(30.8132) | Bit/dim 1.1764(1.1818) | Xent 0.1027(0.1006) | Loss 1.2277(1.2321) | Error 0.0324(0.0311) Steps 422(423.31) | Grad Norm 0.2336(0.3556) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0142 | Time 16.0224, Epoch Time 243.6385(239.3780), Bit/dim 1.1712(best: 1.1713), Xent 0.0491, Loss 1.1958, Error 0.0172(best: 0.0152)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0995 | Time 31.1683(30.8239) | Bit/dim 1.1793(1.1818) | Xent 0.0984(0.1006) | Loss 1.2285(1.2320) | Error 0.0309(0.0310) Steps 422(423.27) | Grad Norm 0.2394(0.3521) | Total Time 10.00(10.00)\n",
      "Iter 0996 | Time 30.7742(30.8224) | Bit/dim 1.1800(1.1817) | Xent 0.1045(0.1007) | Loss 1.2322(1.2320) | Error 0.0320(0.0311) Steps 422(423.23) | Grad Norm 0.2059(0.3478) | Total Time 10.00(10.00)\n",
      "Iter 0997 | Time 30.5731(30.8149) | Bit/dim 1.1753(1.1815) | Xent 0.0936(0.1005) | Loss 1.2222(1.2317) | Error 0.0285(0.0310) Steps 422(423.19) | Grad Norm 0.3185(0.3469) | Total Time 10.00(10.00)\n",
      "Iter 0998 | Time 32.5215(30.8661) | Bit/dim 1.1766(1.1814) | Xent 0.1010(0.1005) | Loss 1.2271(1.2316) | Error 0.0320(0.0310) Steps 422(423.16) | Grad Norm 0.2797(0.3449) | Total Time 10.00(10.00)\n",
      "Iter 0999 | Time 30.0838(30.8427) | Bit/dim 1.1770(1.1812) | Xent 0.1064(0.1007) | Loss 1.2301(1.2316) | Error 0.0329(0.0311) Steps 422(423.12) | Grad Norm 0.2740(0.3427) | Total Time 10.00(10.00)\n",
      "Iter 1000 | Time 30.2446(30.8247) | Bit/dim 1.1748(1.1810) | Xent 0.1067(0.1008) | Loss 1.2281(1.2315) | Error 0.0344(0.0312) Steps 422(423.09) | Grad Norm 0.2472(0.3399) | Total Time 10.00(10.00)\n",
      "Iter 1001 | Time 30.2485(30.8074) | Bit/dim 1.1754(1.1809) | Xent 0.0991(0.1008) | Loss 1.2250(1.2313) | Error 0.0334(0.0313) Steps 422(423.06) | Grad Norm 0.2409(0.3369) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0143 | Time 16.3016, Epoch Time 244.4211(239.5293), Bit/dim 1.1713(best: 1.1712), Xent 0.0476, Loss 1.1951, Error 0.0157(best: 0.0152)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1002 | Time 30.0031(30.7833) | Bit/dim 1.1777(1.1808) | Xent 0.1008(0.1008) | Loss 1.2281(1.2312) | Error 0.0324(0.0313) Steps 422(423.02) | Grad Norm 0.2825(0.3353) | Total Time 10.00(10.00)\n",
      "Iter 1003 | Time 30.0025(30.7599) | Bit/dim 1.1778(1.1807) | Xent 0.1074(0.1010) | Loss 1.2315(1.2312) | Error 0.0310(0.0313) Steps 422(422.99) | Grad Norm 0.3573(0.3359) | Total Time 10.00(10.00)\n",
      "Iter 1004 | Time 30.5592(30.7539) | Bit/dim 1.1786(1.1806) | Xent 0.0946(0.1008) | Loss 1.2259(1.2310) | Error 0.0281(0.0312) Steps 422(422.96) | Grad Norm 0.2842(0.3344) | Total Time 10.00(10.00)\n",
      "Iter 1005 | Time 31.9480(30.7897) | Bit/dim 1.1704(1.1803) | Xent 0.0971(0.1007) | Loss 1.2189(1.2307) | Error 0.0315(0.0312) Steps 422(422.93) | Grad Norm 0.3271(0.3342) | Total Time 10.00(10.00)\n",
      "Iter 1006 | Time 31.1976(30.8019) | Bit/dim 1.1757(1.1802) | Xent 0.0917(0.1004) | Loss 1.2215(1.2304) | Error 0.0306(0.0312) Steps 416(422.73) | Grad Norm 0.2003(0.3301) | Total Time 10.00(10.00)\n",
      "Iter 1007 | Time 30.4794(30.7922) | Bit/dim 1.1788(1.1801) | Xent 0.0979(0.1003) | Loss 1.2277(1.2303) | Error 0.0300(0.0311) Steps 422(422.71) | Grad Norm 0.3024(0.3293) | Total Time 10.00(10.00)\n",
      "Iter 1008 | Time 31.7098(30.8198) | Bit/dim 1.1743(1.1800) | Xent 0.0989(0.1003) | Loss 1.2237(1.2301) | Error 0.0319(0.0312) Steps 422(422.68) | Grad Norm 0.3649(0.3304) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0144 | Time 16.0994, Epoch Time 244.5717(239.6806), Bit/dim 1.1693(best: 1.1712), Xent 0.0465, Loss 1.1925, Error 0.0163(best: 0.0152)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1009 | Time 31.4084(30.8374) | Bit/dim 1.1762(1.1798) | Xent 0.1030(0.1004) | Loss 1.2277(1.2300) | Error 0.0316(0.0312) Steps 422(422.66) | Grad Norm 0.2194(0.3271) | Total Time 10.00(10.00)\n",
      "Iter 1010 | Time 31.2886(30.8510) | Bit/dim 1.1815(1.1799) | Xent 0.1051(0.1005) | Loss 1.2341(1.2302) | Error 0.0319(0.0312) Steps 422(422.64) | Grad Norm 0.2777(0.3256) | Total Time 10.00(10.00)\n",
      "Iter 1011 | Time 31.2103(30.8617) | Bit/dim 1.1784(1.1798) | Xent 0.1041(0.1006) | Loss 1.2304(1.2302) | Error 0.0335(0.0313) Steps 422(422.62) | Grad Norm 0.2865(0.3244) | Total Time 10.00(10.00)\n",
      "Iter 1012 | Time 30.2586(30.8437) | Bit/dim 1.1729(1.1796) | Xent 0.1020(0.1007) | Loss 1.2239(1.2300) | Error 0.0312(0.0313) Steps 422(422.61) | Grad Norm 0.3454(0.3250) | Total Time 10.00(10.00)\n",
      "Iter 1013 | Time 30.6339(30.8374) | Bit/dim 1.1712(1.1794) | Xent 0.0960(0.1005) | Loss 1.2192(1.2297) | Error 0.0288(0.0312) Steps 422(422.59) | Grad Norm 0.3033(0.3244) | Total Time 10.00(10.00)\n",
      "Iter 1014 | Time 30.5372(30.8284) | Bit/dim 1.1743(1.1792) | Xent 0.0989(0.1005) | Loss 1.2237(1.2295) | Error 0.0306(0.0312) Steps 422(422.57) | Grad Norm 0.3152(0.3241) | Total Time 10.00(10.00)\n",
      "Iter 1015 | Time 30.5262(30.8193) | Bit/dim 1.1789(1.1792) | Xent 0.0887(0.1001) | Loss 1.2232(1.2293) | Error 0.0264(0.0310) Steps 422(422.55) | Grad Norm 0.3744(0.3256) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0145 | Time 16.1760, Epoch Time 244.6880(239.8308), Bit/dim 1.1684(best: 1.1693), Xent 0.0494, Loss 1.1931, Error 0.0162(best: 0.0152)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1016 | Time 30.5518(30.8113) | Bit/dim 1.1716(1.1790) | Xent 0.0965(0.1000) | Loss 1.2198(1.2290) | Error 0.0320(0.0311) Steps 422(422.54) | Grad Norm 0.2575(0.3236) | Total Time 10.00(10.00)\n",
      "Iter 1017 | Time 31.7937(30.8407) | Bit/dim 1.1766(1.1789) | Xent 0.1019(0.1001) | Loss 1.2276(1.2290) | Error 0.0317(0.0311) Steps 422(422.52) | Grad Norm 0.4145(0.3263) | Total Time 10.00(10.00)\n",
      "Iter 1018 | Time 30.3651(30.8265) | Bit/dim 1.1725(1.1787) | Xent 0.1028(0.1002) | Loss 1.2239(1.2288) | Error 0.0314(0.0311) Steps 422(422.50) | Grad Norm 0.3264(0.3263) | Total Time 10.00(10.00)\n",
      "Iter 1019 | Time 31.0423(30.8329) | Bit/dim 1.1804(1.1788) | Xent 0.1061(0.1003) | Loss 1.2335(1.2289) | Error 0.0334(0.0312) Steps 422(422.49) | Grad Norm 0.2534(0.3241) | Total Time 10.00(10.00)\n",
      "Iter 1020 | Time 31.8499(30.8635) | Bit/dim 1.1782(1.1788) | Xent 0.0873(0.0999) | Loss 1.2219(1.2287) | Error 0.0268(0.0310) Steps 422(422.47) | Grad Norm 0.3333(0.3244) | Total Time 10.00(10.00)\n",
      "Iter 1021 | Time 30.2471(30.8450) | Bit/dim 1.1743(1.1786) | Xent 0.1051(0.1001) | Loss 1.2269(1.2287) | Error 0.0316(0.0310) Steps 422(422.46) | Grad Norm 0.2852(0.3232) | Total Time 10.00(10.00)\n",
      "Iter 1022 | Time 30.5240(30.8353) | Bit/dim 1.1726(1.1784) | Xent 0.0973(0.1000) | Loss 1.2213(1.2285) | Error 0.0298(0.0310) Steps 422(422.45) | Grad Norm 0.2781(0.3219) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0146 | Time 15.9443, Epoch Time 244.9747(239.9851), Bit/dim 1.1693(best: 1.1684), Xent 0.0500, Loss 1.1943, Error 0.0162(best: 0.0152)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1023 | Time 30.8036(30.8344) | Bit/dim 1.1812(1.1785) | Xent 0.0947(0.0999) | Loss 1.2285(1.2285) | Error 0.0301(0.0310) Steps 422(422.43) | Grad Norm 0.2639(0.3201) | Total Time 10.00(10.00)\n",
      "Iter 1024 | Time 30.2494(30.8168) | Bit/dim 1.1737(1.1784) | Xent 0.0979(0.0998) | Loss 1.2227(1.2283) | Error 0.0305(0.0310) Steps 422(422.42) | Grad Norm 0.3723(0.3217) | Total Time 10.00(10.00)\n",
      "Iter 1025 | Time 31.2644(30.8303) | Bit/dim 1.1752(1.1783) | Xent 0.0956(0.0997) | Loss 1.2231(1.2281) | Error 0.0282(0.0309) Steps 422(422.41) | Grad Norm 0.2585(0.3198) | Total Time 10.00(10.00)\n",
      "Iter 1026 | Time 30.2801(30.8138) | Bit/dim 1.1691(1.1780) | Xent 0.0982(0.0996) | Loss 1.2181(1.2278) | Error 0.0309(0.0309) Steps 422(422.40) | Grad Norm 0.2075(0.3164) | Total Time 10.00(10.00)\n",
      "Iter 1027 | Time 32.5493(30.8658) | Bit/dim 1.1798(1.1781) | Xent 0.0898(0.0993) | Loss 1.2247(1.2277) | Error 0.0280(0.0308) Steps 422(422.38) | Grad Norm 0.1720(0.3121) | Total Time 10.00(10.00)\n",
      "Iter 1028 | Time 31.5888(30.8875) | Bit/dim 1.1760(1.1780) | Xent 0.0962(0.0992) | Loss 1.2241(1.2276) | Error 0.0270(0.0307) Steps 416(422.19) | Grad Norm 0.2145(0.3092) | Total Time 10.00(10.00)\n",
      "Iter 1029 | Time 30.5470(30.8773) | Bit/dim 1.1693(1.1777) | Xent 0.1005(0.0993) | Loss 1.2195(1.2274) | Error 0.0333(0.0308) Steps 422(422.19) | Grad Norm 0.2494(0.3074) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0147 | Time 15.6549, Epoch Time 245.6199(240.1542), Bit/dim 1.1690(best: 1.1684), Xent 0.0503, Loss 1.1941, Error 0.0166(best: 0.0152)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1030 | Time 30.2796(30.8594) | Bit/dim 1.1779(1.1777) | Xent 0.1052(0.0995) | Loss 1.2305(1.2275) | Error 0.0311(0.0308) Steps 422(422.18) | Grad Norm 0.2758(0.3064) | Total Time 10.00(10.00)\n",
      "Iter 1031 | Time 31.5502(30.8801) | Bit/dim 1.1716(1.1776) | Xent 0.0932(0.0993) | Loss 1.2182(1.2272) | Error 0.0286(0.0307) Steps 422(422.18) | Grad Norm 0.3376(0.3074) | Total Time 10.00(10.00)\n",
      "Iter 1032 | Time 31.1558(30.8884) | Bit/dim 1.1754(1.1775) | Xent 0.0961(0.0992) | Loss 1.2234(1.2271) | Error 0.0298(0.0307) Steps 422(422.17) | Grad Norm 0.2281(0.3050) | Total Time 10.00(10.00)\n",
      "Iter 1033 | Time 29.9869(30.8613) | Bit/dim 1.1741(1.1774) | Xent 0.1016(0.0992) | Loss 1.2249(1.2270) | Error 0.0300(0.0307) Steps 422(422.16) | Grad Norm 0.2332(0.3028) | Total Time 10.00(10.00)\n",
      "Iter 1034 | Time 31.2796(30.8739) | Bit/dim 1.1715(1.1772) | Xent 0.0929(0.0991) | Loss 1.2179(1.2267) | Error 0.0290(0.0306) Steps 422(422.16) | Grad Norm 0.2498(0.3012) | Total Time 10.00(10.00)\n",
      "Iter 1035 | Time 30.1439(30.8520) | Bit/dim 1.1710(1.1770) | Xent 0.0936(0.0989) | Loss 1.2178(1.2265) | Error 0.0300(0.0306) Steps 422(422.16) | Grad Norm 0.3494(0.3027) | Total Time 10.00(10.00)\n",
      "Iter 1036 | Time 30.3488(30.8369) | Bit/dim 1.1761(1.1770) | Xent 0.0987(0.0989) | Loss 1.2254(1.2264) | Error 0.0296(0.0306) Steps 422(422.15) | Grad Norm 0.2558(0.3013) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0148 | Time 16.1442, Epoch Time 243.7795(240.2629), Bit/dim 1.1678(best: 1.1684), Xent 0.0490, Loss 1.1923, Error 0.0158(best: 0.0152)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1037 | Time 30.4976(30.8267) | Bit/dim 1.1771(1.1770) | Xent 0.0978(0.0989) | Loss 1.2260(1.2264) | Error 0.0290(0.0305) Steps 422(422.15) | Grad Norm 0.2905(0.3009) | Total Time 10.00(10.00)\n",
      "Iter 1038 | Time 30.9593(30.8307) | Bit/dim 1.1742(1.1769) | Xent 0.0929(0.0987) | Loss 1.2206(1.2263) | Error 0.0311(0.0305) Steps 422(422.14) | Grad Norm 0.3169(0.3014) | Total Time 10.00(10.00)\n",
      "Iter 1039 | Time 30.1136(30.8092) | Bit/dim 1.1739(1.1768) | Xent 0.0996(0.0987) | Loss 1.2236(1.2262) | Error 0.0299(0.0305) Steps 422(422.14) | Grad Norm 0.2904(0.3011) | Total Time 10.00(10.00)\n",
      "Iter 1040 | Time 31.4019(30.8269) | Bit/dim 1.1722(1.1767) | Xent 0.1038(0.0989) | Loss 1.2241(1.2261) | Error 0.0327(0.0306) Steps 422(422.13) | Grad Norm 0.3319(0.3020) | Total Time 10.00(10.00)\n",
      "Iter 1041 | Time 30.2688(30.8102) | Bit/dim 1.1750(1.1766) | Xent 0.0987(0.0989) | Loss 1.2243(1.2261) | Error 0.0326(0.0306) Steps 422(422.13) | Grad Norm 0.2294(0.2998) | Total Time 10.00(10.00)\n",
      "Iter 1042 | Time 31.2475(30.8233) | Bit/dim 1.1701(1.1764) | Xent 0.1078(0.0991) | Loss 1.2240(1.2260) | Error 0.0341(0.0307) Steps 422(422.13) | Grad Norm 0.3499(0.3013) | Total Time 10.00(10.00)\n",
      "Iter 1043 | Time 30.0010(30.7986) | Bit/dim 1.1726(1.1763) | Xent 0.0902(0.0989) | Loss 1.2177(1.2257) | Error 0.0279(0.0307) Steps 422(422.12) | Grad Norm 0.2390(0.2995) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0149 | Time 16.0688, Epoch Time 243.1804(240.3505), Bit/dim 1.1668(best: 1.1678), Xent 0.0479, Loss 1.1907, Error 0.0155(best: 0.0152)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1044 | Time 30.1787(30.7800) | Bit/dim 1.1659(1.1760) | Xent 0.0975(0.0988) | Loss 1.2146(1.2254) | Error 0.0296(0.0306) Steps 422(422.12) | Grad Norm 0.2347(0.2975) | Total Time 10.00(10.00)\n",
      "Iter 1045 | Time 31.1415(30.7909) | Bit/dim 1.1756(1.1760) | Xent 0.1061(0.0990) | Loss 1.2286(1.2255) | Error 0.0349(0.0308) Steps 422(422.11) | Grad Norm 0.2066(0.2948) | Total Time 10.00(10.00)\n",
      "Iter 1046 | Time 31.1834(30.8027) | Bit/dim 1.1782(1.1761) | Xent 0.0909(0.0988) | Loss 1.2237(1.2255) | Error 0.0270(0.0306) Steps 422(422.11) | Grad Norm 0.1572(0.2907) | Total Time 10.00(10.00)\n",
      "Iter 1047 | Time 29.4031(30.7607) | Bit/dim 1.1709(1.1759) | Xent 0.1074(0.0990) | Loss 1.2246(1.2254) | Error 0.0316(0.0307) Steps 422(422.11) | Grad Norm 0.3254(0.2917) | Total Time 10.00(10.00)\n",
      "Iter 1048 | Time 30.1992(30.7438) | Bit/dim 1.1764(1.1759) | Xent 0.0945(0.0989) | Loss 1.2236(1.2254) | Error 0.0301(0.0307) Steps 422(422.10) | Grad Norm 0.2232(0.2897) | Total Time 10.00(10.00)\n",
      "Iter 1049 | Time 31.7832(30.7750) | Bit/dim 1.1726(1.1758) | Xent 0.1004(0.0990) | Loss 1.2227(1.2253) | Error 0.0340(0.0308) Steps 422(422.10) | Grad Norm 0.3219(0.2906) | Total Time 10.00(10.00)\n",
      "Iter 1050 | Time 30.0000(30.7518) | Bit/dim 1.1742(1.1758) | Xent 0.1001(0.0990) | Loss 1.2242(1.2253) | Error 0.0325(0.0308) Steps 422(422.10) | Grad Norm 0.3480(0.2923) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0150 | Time 16.1307, Epoch Time 242.6964(240.4208), Bit/dim 1.1665(best: 1.1668), Xent 0.0480, Loss 1.1905, Error 0.0154(best: 0.0152)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1051 | Time 31.6278(30.7780) | Bit/dim 1.1708(1.1756) | Xent 0.1005(0.0990) | Loss 1.2211(1.2251) | Error 0.0288(0.0307) Steps 422(422.10) | Grad Norm 0.3364(0.2937) | Total Time 10.00(10.00)\n",
      "Iter 1052 | Time 29.8446(30.7500) | Bit/dim 1.1723(1.1755) | Xent 0.0944(0.0989) | Loss 1.2195(1.2250) | Error 0.0276(0.0307) Steps 422(422.09) | Grad Norm 0.2251(0.2916) | Total Time 10.00(10.00)\n",
      "Iter 1053 | Time 31.5479(30.7740) | Bit/dim 1.1760(1.1755) | Xent 0.0972(0.0988) | Loss 1.2246(1.2250) | Error 0.0295(0.0306) Steps 422(422.09) | Grad Norm 0.3552(0.2935) | Total Time 10.00(10.00)\n",
      "Iter 1054 | Time 29.7970(30.7447) | Bit/dim 1.1736(1.1755) | Xent 0.1027(0.0990) | Loss 1.2249(1.2250) | Error 0.0296(0.0306) Steps 422(422.09) | Grad Norm 0.3158(0.2942) | Total Time 10.00(10.00)\n",
      "Iter 1055 | Time 30.9914(30.7521) | Bit/dim 1.1776(1.1755) | Xent 0.0999(0.0990) | Loss 1.2276(1.2250) | Error 0.0300(0.0306) Steps 422(422.08) | Grad Norm 0.2959(0.2942) | Total Time 10.00(10.00)\n",
      "Iter 1056 | Time 31.5159(30.7750) | Bit/dim 1.1747(1.1755) | Xent 0.1028(0.0991) | Loss 1.2261(1.2251) | Error 0.0324(0.0306) Steps 416(421.90) | Grad Norm 0.2631(0.2933) | Total Time 10.00(10.00)\n",
      "Iter 1057 | Time 29.9762(30.7510) | Bit/dim 1.1679(1.1753) | Xent 0.0982(0.0991) | Loss 1.2170(1.2248) | Error 0.0308(0.0306) Steps 422(421.90) | Grad Norm 0.2481(0.2919) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0151 | Time 15.9080, Epoch Time 243.9945(240.5280), Bit/dim 1.1669(best: 1.1665), Xent 0.0474, Loss 1.1906, Error 0.0150(best: 0.0152)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1058 | Time 31.4176(30.7710) | Bit/dim 1.1725(1.1752) | Xent 0.1078(0.0993) | Loss 1.2264(1.2249) | Error 0.0333(0.0307) Steps 422(421.91) | Grad Norm 0.2439(0.2905) | Total Time 10.00(10.00)\n",
      "Iter 1059 | Time 30.7099(30.7692) | Bit/dim 1.1736(1.1752) | Xent 0.1050(0.0995) | Loss 1.2261(1.2249) | Error 0.0308(0.0307) Steps 422(421.91) | Grad Norm 0.2437(0.2891) | Total Time 10.00(10.00)\n",
      "Iter 1060 | Time 31.5482(30.7925) | Bit/dim 1.1741(1.1751) | Xent 0.0966(0.0994) | Loss 1.2224(1.2248) | Error 0.0312(0.0307) Steps 416(421.73) | Grad Norm 0.2140(0.2868) | Total Time 10.00(10.00)\n",
      "Iter 1061 | Time 30.3974(30.7807) | Bit/dim 1.1727(1.1751) | Xent 0.1008(0.0995) | Loss 1.2231(1.2248) | Error 0.0304(0.0307) Steps 422(421.74) | Grad Norm 0.2870(0.2869) | Total Time 10.00(10.00)\n",
      "Iter 1062 | Time 31.0445(30.7886) | Bit/dim 1.1724(1.1750) | Xent 0.0947(0.0993) | Loss 1.2198(1.2246) | Error 0.0306(0.0307) Steps 422(421.75) | Grad Norm 0.2227(0.2849) | Total Time 10.00(10.00)\n",
      "Iter 1063 | Time 29.7660(30.7579) | Bit/dim 1.1745(1.1750) | Xent 0.0947(0.0992) | Loss 1.2218(1.2246) | Error 0.0304(0.0307) Steps 422(421.76) | Grad Norm 0.2472(0.2838) | Total Time 10.00(10.00)\n",
      "Iter 1064 | Time 31.9733(30.7944) | Bit/dim 1.1674(1.1747) | Xent 0.0924(0.0990) | Loss 1.2136(1.2242) | Error 0.0290(0.0306) Steps 416(421.58) | Grad Norm 0.3949(0.2871) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0152 | Time 15.9734, Epoch Time 245.2309(240.6691), Bit/dim 1.1668(best: 1.1665), Xent 0.0500, Loss 1.1918, Error 0.0173(best: 0.0150)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1065 | Time 30.9032(30.7977) | Bit/dim 1.1705(1.1746) | Xent 0.0924(0.0988) | Loss 1.2167(1.2240) | Error 0.0295(0.0306) Steps 416(421.42) | Grad Norm 0.2368(0.2856) | Total Time 10.00(10.00)\n",
      "Iter 1066 | Time 31.0028(30.8038) | Bit/dim 1.1685(1.1744) | Xent 0.1045(0.0989) | Loss 1.2207(1.2239) | Error 0.0319(0.0307) Steps 416(421.25) | Grad Norm 0.3429(0.2873) | Total Time 10.00(10.00)\n",
      "Iter 1067 | Time 31.2717(30.8178) | Bit/dim 1.1756(1.1745) | Xent 0.1015(0.0990) | Loss 1.2264(1.2240) | Error 0.0291(0.0306) Steps 428(421.46) | Grad Norm 0.2811(0.2871) | Total Time 10.00(10.00)\n",
      "Iter 1068 | Time 31.4142(30.8357) | Bit/dim 1.1731(1.1744) | Xent 0.1044(0.0992) | Loss 1.2253(1.2240) | Error 0.0337(0.0307) Steps 422(421.47) | Grad Norm 0.3367(0.2886) | Total Time 10.00(10.00)\n",
      "Iter 1069 | Time 31.5851(30.8582) | Bit/dim 1.1738(1.1744) | Xent 0.0866(0.0988) | Loss 1.2172(1.2238) | Error 0.0270(0.0306) Steps 422(421.49) | Grad Norm 0.2593(0.2878) | Total Time 10.00(10.00)\n",
      "Iter 1070 | Time 32.0332(30.8935) | Bit/dim 1.1717(1.1743) | Xent 0.0953(0.0987) | Loss 1.2193(1.2237) | Error 0.0284(0.0305) Steps 422(421.50) | Grad Norm 0.2811(0.2876) | Total Time 10.00(10.00)\n",
      "Iter 1071 | Time 32.2230(30.9334) | Bit/dim 1.1781(1.1744) | Xent 0.1018(0.0988) | Loss 1.2290(1.2238) | Error 0.0309(0.0305) Steps 428(421.70) | Grad Norm 0.2824(0.2874) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0153 | Time 16.2451, Epoch Time 249.3471(240.9295), Bit/dim 1.1672(best: 1.1665), Xent 0.0487, Loss 1.1916, Error 0.0152(best: 0.0150)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1072 | Time 31.4129(30.9477) | Bit/dim 1.1731(1.1744) | Xent 0.0848(0.0984) | Loss 1.2155(1.2236) | Error 0.0264(0.0304) Steps 428(421.89) | Grad Norm 0.2587(0.2865) | Total Time 10.00(10.00)\n",
      "Iter 1073 | Time 31.8733(30.9755) | Bit/dim 1.1772(1.1745) | Xent 0.0951(0.0983) | Loss 1.2248(1.2236) | Error 0.0294(0.0304) Steps 422(421.89) | Grad Norm 0.3285(0.2878) | Total Time 10.00(10.00)\n",
      "Iter 1074 | Time 32.1170(31.0098) | Bit/dim 1.1715(1.1744) | Xent 0.0953(0.0982) | Loss 1.2191(1.2235) | Error 0.0295(0.0304) Steps 428(422.07) | Grad Norm 0.2532(0.2868) | Total Time 10.00(10.00)\n",
      "Iter 1075 | Time 32.9676(31.0685) | Bit/dim 1.1678(1.1742) | Xent 0.0953(0.0981) | Loss 1.2155(1.2232) | Error 0.0281(0.0303) Steps 428(422.25) | Grad Norm 0.2382(0.2853) | Total Time 10.00(10.00)\n",
      "Iter 1076 | Time 32.7018(31.1175) | Bit/dim 1.1731(1.1742) | Xent 0.1021(0.0982) | Loss 1.2241(1.2233) | Error 0.0316(0.0303) Steps 422(422.24) | Grad Norm 0.2725(0.2849) | Total Time 10.00(10.00)\n",
      "Iter 1077 | Time 32.5402(31.1602) | Bit/dim 1.1767(1.1742) | Xent 0.1121(0.0986) | Loss 1.2327(1.2236) | Error 0.0321(0.0304) Steps 428(422.42) | Grad Norm 0.3627(0.2873) | Total Time 10.00(10.00)\n",
      "Iter 1078 | Time 31.9223(31.1830) | Bit/dim 1.1702(1.1741) | Xent 0.0973(0.0986) | Loss 1.2189(1.2234) | Error 0.0295(0.0304) Steps 428(422.58) | Grad Norm 0.2973(0.2876) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0154 | Time 15.9676, Epoch Time 254.2927(241.3304), Bit/dim 1.1663(best: 1.1665), Xent 0.0512, Loss 1.1919, Error 0.0160(best: 0.0150)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1079 | Time 33.5020(31.2526) | Bit/dim 1.1723(1.1741) | Xent 0.1120(0.0990) | Loss 1.2283(1.2236) | Error 0.0327(0.0304) Steps 428(422.75) | Grad Norm 0.2547(0.2866) | Total Time 10.00(10.00)\n",
      "Iter 1080 | Time 33.3076(31.3143) | Bit/dim 1.1673(1.1739) | Xent 0.0987(0.0990) | Loss 1.2166(1.2233) | Error 0.0309(0.0304) Steps 422(422.72) | Grad Norm 0.3759(0.2892) | Total Time 10.00(10.00)\n",
      "Iter 1081 | Time 31.5131(31.3202) | Bit/dim 1.1738(1.1739) | Xent 0.0986(0.0990) | Loss 1.2231(1.2233) | Error 0.0305(0.0304) Steps 422(422.70) | Grad Norm 0.3933(0.2924) | Total Time 10.00(10.00)\n",
      "Iter 1082 | Time 33.1401(31.3748) | Bit/dim 1.1716(1.1738) | Xent 0.1011(0.0990) | Loss 1.2221(1.2233) | Error 0.0317(0.0305) Steps 428(422.86) | Grad Norm 0.3476(0.2940) | Total Time 10.00(10.00)\n",
      "Iter 1083 | Time 33.6998(31.4446) | Bit/dim 1.1771(1.1739) | Xent 0.0927(0.0988) | Loss 1.2234(1.2233) | Error 0.0275(0.0304) Steps 428(423.02) | Grad Norm 0.2484(0.2927) | Total Time 10.00(10.00)\n",
      "Iter 1084 | Time 32.8515(31.4868) | Bit/dim 1.1722(1.1738) | Xent 0.0910(0.0986) | Loss 1.2177(1.2231) | Error 0.0296(0.0304) Steps 428(423.17) | Grad Norm 0.3167(0.2934) | Total Time 10.00(10.00)\n",
      "Iter 1085 | Time 31.1247(31.4759) | Bit/dim 1.1694(1.1737) | Xent 0.0893(0.0983) | Loss 1.2140(1.2229) | Error 0.0260(0.0302) Steps 422(423.13) | Grad Norm 0.4899(0.2993) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0155 | Time 16.2023, Epoch Time 258.1004(241.8335), Bit/dim 1.1666(best: 1.1663), Xent 0.0498, Loss 1.1915, Error 0.0168(best: 0.0150)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1086 | Time 31.1259(31.4654) | Bit/dim 1.1752(1.1737) | Xent 0.0914(0.0981) | Loss 1.2209(1.2228) | Error 0.0299(0.0302) Steps 428(423.28) | Grad Norm 0.3524(0.3009) | Total Time 10.00(10.00)\n",
      "Iter 1087 | Time 31.6566(31.4711) | Bit/dim 1.1707(1.1737) | Xent 0.1004(0.0982) | Loss 1.2209(1.2227) | Error 0.0296(0.0302) Steps 422(423.24) | Grad Norm 0.3182(0.3014) | Total Time 10.00(10.00)\n",
      "Iter 1088 | Time 32.5175(31.5025) | Bit/dim 1.1703(1.1735) | Xent 0.0959(0.0981) | Loss 1.2182(1.2226) | Error 0.0272(0.0301) Steps 422(423.20) | Grad Norm 0.2298(0.2992) | Total Time 10.00(10.00)\n",
      "Iter 1089 | Time 31.3213(31.4971) | Bit/dim 1.1685(1.1734) | Xent 0.0864(0.0978) | Loss 1.2117(1.2223) | Error 0.0268(0.0300) Steps 428(423.35) | Grad Norm 0.2006(0.2963) | Total Time 10.00(10.00)\n",
      "Iter 1090 | Time 31.8546(31.5078) | Bit/dim 1.1715(1.1733) | Xent 0.1000(0.0978) | Loss 1.2215(1.2223) | Error 0.0311(0.0301) Steps 422(423.30) | Grad Norm 0.2809(0.2958) | Total Time 10.00(10.00)\n",
      "Iter 1091 | Time 31.7886(31.5162) | Bit/dim 1.1698(1.1732) | Xent 0.1041(0.0980) | Loss 1.2219(1.2222) | Error 0.0325(0.0301) Steps 428(423.45) | Grad Norm 0.2868(0.2956) | Total Time 10.00(10.00)\n",
      "Iter 1092 | Time 31.8763(31.5271) | Bit/dim 1.1766(1.1733) | Xent 0.1082(0.0983) | Loss 1.2307(1.2225) | Error 0.0329(0.0302) Steps 428(423.58) | Grad Norm 0.2351(0.2937) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0156 | Time 16.2278, Epoch Time 250.9161(242.1060), Bit/dim 1.1658(best: 1.1663), Xent 0.0501, Loss 1.1908, Error 0.0173(best: 0.0150)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1093 | Time 32.8924(31.5680) | Bit/dim 1.1698(1.1732) | Xent 0.0968(0.0983) | Loss 1.2182(1.2224) | Error 0.0300(0.0302) Steps 428(423.71) | Grad Norm 0.3348(0.2950) | Total Time 10.00(10.00)\n",
      "Iter 1094 | Time 31.8425(31.5762) | Bit/dim 1.1711(1.1732) | Xent 0.1046(0.0985) | Loss 1.2234(1.2224) | Error 0.0326(0.0303) Steps 428(423.84) | Grad Norm 0.2437(0.2934) | Total Time 10.00(10.00)\n",
      "Iter 1095 | Time 32.7346(31.6110) | Bit/dim 1.1720(1.1731) | Xent 0.1002(0.0985) | Loss 1.2221(1.2224) | Error 0.0319(0.0303) Steps 422(423.79) | Grad Norm 0.6089(0.3029) | Total Time 10.00(10.00)\n",
      "Iter 1096 | Time 32.8653(31.6486) | Bit/dim 1.1763(1.1732) | Xent 0.1071(0.0988) | Loss 1.2298(1.2226) | Error 0.0333(0.0304) Steps 422(423.73) | Grad Norm 0.2402(0.3010) | Total Time 10.00(10.00)\n",
      "Iter 1097 | Time 30.9358(31.6272) | Bit/dim 1.1690(1.1731) | Xent 0.0950(0.0987) | Loss 1.2165(1.2224) | Error 0.0321(0.0305) Steps 428(423.86) | Grad Norm 0.3495(0.3025) | Total Time 10.00(10.00)\n",
      "Iter 1098 | Time 32.4917(31.6532) | Bit/dim 1.1768(1.1732) | Xent 0.1002(0.0987) | Loss 1.2269(1.2226) | Error 0.0288(0.0304) Steps 428(423.99) | Grad Norm 0.5293(0.3093) | Total Time 10.00(10.00)\n",
      "Iter 1099 | Time 32.7006(31.6846) | Bit/dim 1.1735(1.1732) | Xent 0.0957(0.0986) | Loss 1.2214(1.2225) | Error 0.0295(0.0304) Steps 422(423.93) | Grad Norm 0.2622(0.3079) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0157 | Time 15.7853, Epoch Time 254.9485(242.4912), Bit/dim 1.1665(best: 1.1658), Xent 0.0472, Loss 1.1901, Error 0.0159(best: 0.0150)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1100 | Time 31.1548(31.6687) | Bit/dim 1.1722(1.1732) | Xent 0.0974(0.0986) | Loss 1.2209(1.2225) | Error 0.0304(0.0304) Steps 428(424.05) | Grad Norm 0.5408(0.3149) | Total Time 10.00(10.00)\n",
      "Iter 1101 | Time 33.9605(31.7375) | Bit/dim 1.1720(1.1731) | Xent 0.0932(0.0984) | Loss 1.2186(1.2224) | Error 0.0278(0.0303) Steps 428(424.17) | Grad Norm 0.2281(0.3123) | Total Time 10.00(10.00)\n",
      "Iter 1102 | Time 33.1977(31.7813) | Bit/dim 1.1726(1.1731) | Xent 0.0988(0.0984) | Loss 1.2220(1.2224) | Error 0.0295(0.0303) Steps 422(424.10) | Grad Norm 0.4316(0.3158) | Total Time 10.00(10.00)\n",
      "Iter 1103 | Time 31.5814(31.7753) | Bit/dim 1.1745(1.1732) | Xent 0.1058(0.0987) | Loss 1.2274(1.2225) | Error 0.0323(0.0303) Steps 428(424.22) | Grad Norm 0.3432(0.3167) | Total Time 10.00(10.00)\n",
      "Iter 1104 | Time 32.3769(31.7933) | Bit/dim 1.1734(1.1732) | Xent 0.0977(0.0986) | Loss 1.2222(1.2225) | Error 0.0294(0.0303) Steps 428(424.33) | Grad Norm 0.2577(0.3149) | Total Time 10.00(10.00)\n",
      "Iter 1105 | Time 31.2912(31.7783) | Bit/dim 1.1723(1.1732) | Xent 0.1048(0.0988) | Loss 1.2247(1.2226) | Error 0.0304(0.0303) Steps 428(424.44) | Grad Norm 0.3694(0.3165) | Total Time 10.00(10.00)\n",
      "Iter 1106 | Time 31.6836(31.7754) | Bit/dim 1.1714(1.1731) | Xent 0.0879(0.0985) | Loss 1.2153(1.2223) | Error 0.0264(0.0302) Steps 428(424.55) | Grad Norm 0.3769(0.3183) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0158 | Time 16.0074, Epoch Time 253.9592(242.8353), Bit/dim 1.1664(best: 1.1658), Xent 0.0447, Loss 1.1888, Error 0.0161(best: 0.0150)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1107 | Time 32.4389(31.7953) | Bit/dim 1.1678(1.1729) | Xent 0.0956(0.0984) | Loss 1.2156(1.2221) | Error 0.0279(0.0301) Steps 428(424.65) | Grad Norm 0.1890(0.3145) | Total Time 10.00(10.00)\n",
      "Iter 1108 | Time 31.6199(31.7901) | Bit/dim 1.1688(1.1728) | Xent 0.0985(0.0984) | Loss 1.2180(1.2220) | Error 0.0298(0.0301) Steps 422(424.57) | Grad Norm 0.4815(0.3195) | Total Time 10.00(10.00)\n",
      "Iter 1109 | Time 31.3344(31.7764) | Bit/dim 1.1694(1.1727) | Xent 0.0946(0.0983) | Loss 1.2167(1.2219) | Error 0.0296(0.0301) Steps 428(424.68) | Grad Norm 0.3677(0.3209) | Total Time 10.00(10.00)\n",
      "Iter 1110 | Time 31.8029(31.7772) | Bit/dim 1.1760(1.1728) | Xent 0.0937(0.0982) | Loss 1.2229(1.2219) | Error 0.0289(0.0301) Steps 428(424.78) | Grad Norm 0.3786(0.3226) | Total Time 10.00(10.00)\n",
      "Iter 1111 | Time 32.6220(31.8025) | Bit/dim 1.1793(1.1730) | Xent 0.0993(0.0982) | Loss 1.2290(1.2221) | Error 0.0305(0.0301) Steps 422(424.69) | Grad Norm 0.4613(0.3268) | Total Time 10.00(10.00)\n",
      "Iter 1112 | Time 31.4676(31.7925) | Bit/dim 1.1759(1.1731) | Xent 0.0986(0.0982) | Loss 1.2253(1.2222) | Error 0.0321(0.0301) Steps 422(424.61) | Grad Norm 0.4062(0.3292) | Total Time 10.00(10.00)\n",
      "Iter 1113 | Time 31.3301(31.7786) | Bit/dim 1.1723(1.1731) | Xent 0.0933(0.0981) | Loss 1.2190(1.2221) | Error 0.0299(0.0301) Steps 428(424.71) | Grad Norm 0.4374(0.3324) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0159 | Time 16.1565, Epoch Time 251.3243(243.0899), Bit/dim 1.1668(best: 1.1658), Xent 0.0477, Loss 1.1907, Error 0.0161(best: 0.0150)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1114 | Time 31.9614(31.7841) | Bit/dim 1.1785(1.1732) | Xent 0.1010(0.0981) | Loss 1.2290(1.2223) | Error 0.0305(0.0301) Steps 428(424.81) | Grad Norm 0.3793(0.3338) | Total Time 10.00(10.00)\n",
      "Iter 1115 | Time 32.3146(31.8000) | Bit/dim 1.1692(1.1731) | Xent 0.0988(0.0982) | Loss 1.2186(1.2222) | Error 0.0295(0.0301) Steps 428(424.91) | Grad Norm 0.4145(0.3363) | Total Time 10.00(10.00)\n",
      "Iter 1116 | Time 33.1113(31.8393) | Bit/dim 1.1734(1.1731) | Xent 0.0904(0.0979) | Loss 1.2186(1.2221) | Error 0.0274(0.0300) Steps 428(425.00) | Grad Norm 0.4171(0.3387) | Total Time 10.00(10.00)\n",
      "Iter 1117 | Time 33.1904(31.8799) | Bit/dim 1.1775(1.1733) | Xent 0.0910(0.0977) | Loss 1.2230(1.2221) | Error 0.0272(0.0300) Steps 440(425.45) | Grad Norm 0.4597(0.3423) | Total Time 10.00(10.00)\n",
      "Iter 1118 | Time 32.8646(31.9094) | Bit/dim 1.1793(1.1734) | Xent 0.0993(0.0978) | Loss 1.2289(1.2223) | Error 0.0308(0.0300) Steps 434(425.71) | Grad Norm 0.2821(0.3405) | Total Time 10.00(10.00)\n",
      "Iter 1119 | Time 33.1215(31.9458) | Bit/dim 1.1717(1.1734) | Xent 0.1000(0.0978) | Loss 1.2217(1.2223) | Error 0.0302(0.0300) Steps 434(425.96) | Grad Norm 0.2608(0.3381) | Total Time 10.00(10.00)\n",
      "Iter 1120 | Time 33.5777(31.9947) | Bit/dim 1.1744(1.1734) | Xent 0.0996(0.0979) | Loss 1.2242(1.2224) | Error 0.0291(0.0300) Steps 434(426.20) | Grad Norm 0.2989(0.3369) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0160 | Time 16.4930, Epoch Time 259.0919(243.5700), Bit/dim 1.1673(best: 1.1658), Xent 0.0494, Loss 1.1920, Error 0.0163(best: 0.0150)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1121 | Time 31.9687(31.9940) | Bit/dim 1.1770(1.1735) | Xent 0.0924(0.0977) | Loss 1.2232(1.2224) | Error 0.0290(0.0299) Steps 434(426.43) | Grad Norm 0.3101(0.3361) | Total Time 10.00(10.00)\n",
      "Iter 1122 | Time 32.5712(32.0113) | Bit/dim 1.1733(1.1735) | Xent 0.0971(0.0977) | Loss 1.2219(1.2224) | Error 0.0285(0.0299) Steps 440(426.84) | Grad Norm 0.2246(0.3328) | Total Time 10.00(10.00)\n",
      "Iter 1123 | Time 33.6571(32.0607) | Bit/dim 1.1747(1.1736) | Xent 0.0903(0.0975) | Loss 1.2199(1.2223) | Error 0.0274(0.0298) Steps 440(427.23) | Grad Norm 0.4918(0.3376) | Total Time 10.00(10.00)\n",
      "Iter 1124 | Time 32.8004(32.0828) | Bit/dim 1.1726(1.1735) | Xent 0.0998(0.0975) | Loss 1.2225(1.2223) | Error 0.0299(0.0298) Steps 416(426.90) | Grad Norm 0.2869(0.3360) | Total Time 10.00(10.00)\n",
      "Iter 1125 | Time 31.6701(32.0705) | Bit/dim 1.1699(1.1734) | Xent 0.0930(0.0974) | Loss 1.2164(1.2221) | Error 0.0280(0.0298) Steps 422(426.75) | Grad Norm 0.4559(0.3396) | Total Time 10.00(10.00)\n",
      "Iter 1126 | Time 32.8818(32.0948) | Bit/dim 1.1753(1.1735) | Xent 0.0974(0.0974) | Loss 1.2240(1.2222) | Error 0.0298(0.0298) Steps 434(426.97) | Grad Norm 0.2838(0.3380) | Total Time 10.00(10.00)\n",
      "Iter 1127 | Time 32.4342(32.1050) | Bit/dim 1.1793(1.1737) | Xent 0.0948(0.0973) | Loss 1.2266(1.2223) | Error 0.0298(0.0298) Steps 428(427.00) | Grad Norm 0.3339(0.3378) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0161 | Time 16.3494, Epoch Time 257.0645(243.9748), Bit/dim 1.1683(best: 1.1658), Xent 0.0489, Loss 1.1928, Error 0.0165(best: 0.0150)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1128 | Time 32.3371(32.1119) | Bit/dim 1.1755(1.1737) | Xent 0.1009(0.0974) | Loss 1.2260(1.2224) | Error 0.0302(0.0298) Steps 428(427.03) | Grad Norm 0.4214(0.3403) | Total Time 10.00(10.00)\n",
      "Iter 1129 | Time 32.9510(32.1371) | Bit/dim 1.1761(1.1738) | Xent 0.1087(0.0978) | Loss 1.2304(1.2227) | Error 0.0323(0.0298) Steps 434(427.24) | Grad Norm 0.2720(0.3383) | Total Time 10.00(10.00)\n",
      "Iter 1130 | Time 32.9368(32.1611) | Bit/dim 1.1759(1.1738) | Xent 0.0962(0.0977) | Loss 1.2240(1.2227) | Error 0.0306(0.0299) Steps 434(427.44) | Grad Norm 0.3879(0.3398) | Total Time 10.00(10.00)\n",
      "Iter 1131 | Time 33.8876(32.2129) | Bit/dim 1.1782(1.1740) | Xent 0.0967(0.0977) | Loss 1.2265(1.2228) | Error 0.0302(0.0299) Steps 434(427.64) | Grad Norm 0.2161(0.3361) | Total Time 10.00(10.00)\n",
      "Iter 1132 | Time 33.6329(32.2555) | Bit/dim 1.1755(1.1740) | Xent 0.0951(0.0976) | Loss 1.2231(1.2228) | Error 0.0296(0.0299) Steps 428(427.65) | Grad Norm 0.3361(0.3361) | Total Time 10.00(10.00)\n",
      "Iter 1133 | Time 31.9204(32.2455) | Bit/dim 1.1760(1.1741) | Xent 0.0963(0.0976) | Loss 1.2242(1.2229) | Error 0.0289(0.0298) Steps 428(427.66) | Grad Norm 0.2342(0.3330) | Total Time 10.00(10.00)\n",
      "Iter 1134 | Time 31.2286(32.2149) | Bit/dim 1.1707(1.1740) | Xent 0.0960(0.0975) | Loss 1.2187(1.2227) | Error 0.0311(0.0299) Steps 428(427.67) | Grad Norm 0.4970(0.3379) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0162 | Time 16.3736, Epoch Time 257.8959(244.3925), Bit/dim 1.1694(best: 1.1658), Xent 0.0481, Loss 1.1935, Error 0.0163(best: 0.0150)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1135 | Time 31.8817(32.2049) | Bit/dim 1.1734(1.1740) | Xent 0.0952(0.0975) | Loss 1.2210(1.2227) | Error 0.0306(0.0299) Steps 428(427.68) | Grad Norm 0.2808(0.3362) | Total Time 10.00(10.00)\n",
      "Iter 1136 | Time 32.5851(32.2164) | Bit/dim 1.1716(1.1739) | Xent 0.1019(0.0976) | Loss 1.2225(1.2227) | Error 0.0344(0.0300) Steps 428(427.69) | Grad Norm 0.3601(0.3369) | Total Time 10.00(10.00)\n",
      "Iter 1137 | Time 31.8268(32.2047) | Bit/dim 1.1742(1.1739) | Xent 0.0951(0.0975) | Loss 1.2218(1.2227) | Error 0.0308(0.0301) Steps 428(427.70) | Grad Norm 0.3255(0.3366) | Total Time 10.00(10.00)\n",
      "Iter 1138 | Time 31.9892(32.1982) | Bit/dim 1.1785(1.1740) | Xent 0.0918(0.0974) | Loss 1.2244(1.2227) | Error 0.0292(0.0300) Steps 428(427.71) | Grad Norm 0.3638(0.3374) | Total Time 10.00(10.00)\n",
      "Iter 1139 | Time 32.3925(32.2040) | Bit/dim 1.1821(1.1743) | Xent 0.1036(0.0975) | Loss 1.2339(1.2230) | Error 0.0298(0.0300) Steps 428(427.72) | Grad Norm 0.4459(0.3407) | Total Time 10.00(10.00)\n",
      "Iter 1140 | Time 32.4655(32.2119) | Bit/dim 1.1761(1.1743) | Xent 0.0949(0.0975) | Loss 1.2235(1.2231) | Error 0.0284(0.0300) Steps 428(427.72) | Grad Norm 0.3383(0.3406) | Total Time 10.00(10.00)\n",
      "Iter 1141 | Time 33.3258(32.2453) | Bit/dim 1.1760(1.1744) | Xent 0.0916(0.0973) | Loss 1.2218(1.2230) | Error 0.0286(0.0299) Steps 428(427.73) | Grad Norm 0.5322(0.3463) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0163 | Time 16.4610, Epoch Time 255.4472(244.7241), Bit/dim 1.1702(best: 1.1658), Xent 0.0465, Loss 1.1934, Error 0.0153(best: 0.0150)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1142 | Time 31.7070(32.2291) | Bit/dim 1.1762(1.1744) | Xent 0.0922(0.0971) | Loss 1.2223(1.2230) | Error 0.0269(0.0298) Steps 428(427.74) | Grad Norm 0.2280(0.3428) | Total Time 10.00(10.00)\n",
      "Iter 1143 | Time 31.2933(32.2011) | Bit/dim 1.1772(1.1745) | Xent 0.0817(0.0967) | Loss 1.2180(1.2228) | Error 0.0256(0.0297) Steps 428(427.75) | Grad Norm 0.5247(0.3482) | Total Time 10.00(10.00)\n",
      "Iter 1144 | Time 31.4350(32.1781) | Bit/dim 1.1775(1.1746) | Xent 0.0967(0.0967) | Loss 1.2259(1.2229) | Error 0.0291(0.0297) Steps 428(427.76) | Grad Norm 0.2594(0.3456) | Total Time 10.00(10.00)\n",
      "Iter 1145 | Time 31.3292(32.1526) | Bit/dim 1.1783(1.1747) | Xent 0.0987(0.0967) | Loss 1.2276(1.2231) | Error 0.0295(0.0297) Steps 428(427.76) | Grad Norm 0.3297(0.3451) | Total Time 10.00(10.00)\n",
      "Iter 1146 | Time 32.0511(32.1496) | Bit/dim 1.1774(1.1748) | Xent 0.0938(0.0966) | Loss 1.2244(1.2231) | Error 0.0276(0.0296) Steps 428(427.77) | Grad Norm 0.4842(0.3493) | Total Time 10.00(10.00)\n",
      "Iter 1147 | Time 32.1621(32.1500) | Bit/dim 1.1770(1.1749) | Xent 0.1011(0.0968) | Loss 1.2276(1.2233) | Error 0.0308(0.0297) Steps 428(427.78) | Grad Norm 0.3136(0.3482) | Total Time 10.00(10.00)\n",
      "Iter 1148 | Time 33.5512(32.1920) | Bit/dim 1.1778(1.1750) | Xent 0.1022(0.0969) | Loss 1.2289(1.2234) | Error 0.0326(0.0298) Steps 428(427.78) | Grad Norm 0.5023(0.3528) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0164 | Time 16.4064, Epoch Time 252.5189(244.9580), Bit/dim 1.1727(best: 1.1658), Xent 0.0474, Loss 1.1964, Error 0.0155(best: 0.0150)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1149 | Time 33.0270(32.2170) | Bit/dim 1.1759(1.1750) | Xent 0.0965(0.0969) | Loss 1.2242(1.2234) | Error 0.0294(0.0297) Steps 434(427.97) | Grad Norm 0.2700(0.3503) | Total Time 10.00(10.00)\n",
      "Iter 1150 | Time 34.1177(32.2741) | Bit/dim 1.1736(1.1749) | Xent 0.0839(0.0965) | Loss 1.2155(1.2232) | Error 0.0272(0.0297) Steps 434(428.15) | Grad Norm 0.4978(0.3548) | Total Time 10.00(10.00)\n",
      "Iter 1151 | Time 32.7427(32.2881) | Bit/dim 1.1776(1.1750) | Xent 0.0970(0.0966) | Loss 1.2261(1.2233) | Error 0.0315(0.0297) Steps 434(428.33) | Grad Norm 0.2325(0.3511) | Total Time 10.00(10.00)\n",
      "Iter 1152 | Time 32.9956(32.3093) | Bit/dim 1.1789(1.1751) | Xent 0.0974(0.0966) | Loss 1.2276(1.2234) | Error 0.0312(0.0298) Steps 434(428.50) | Grad Norm 0.3806(0.3520) | Total Time 10.00(10.00)\n",
      "Iter 1153 | Time 32.7433(32.3224) | Bit/dim 1.1813(1.1753) | Xent 0.0961(0.0966) | Loss 1.2293(1.2236) | Error 0.0295(0.0298) Steps 434(428.66) | Grad Norm 0.2643(0.3494) | Total Time 10.00(10.00)\n",
      "Iter 1154 | Time 32.9816(32.3421) | Bit/dim 1.1822(1.1755) | Xent 0.0970(0.0966) | Loss 1.2307(1.2238) | Error 0.0315(0.0298) Steps 434(428.82) | Grad Norm 0.2682(0.3469) | Total Time 10.00(10.00)\n",
      "Iter 1155 | Time 32.8963(32.3588) | Bit/dim 1.1799(1.1757) | Xent 0.0920(0.0964) | Loss 1.2259(1.2239) | Error 0.0305(0.0298) Steps 440(429.16) | Grad Norm 0.4100(0.3488) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0165 | Time 16.3555, Epoch Time 260.4034(245.4213), Bit/dim 1.1719(best: 1.1658), Xent 0.0482, Loss 1.1960, Error 0.0157(best: 0.0150)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1156 | Time 33.4328(32.3910) | Bit/dim 1.1798(1.1758) | Xent 0.0920(0.0963) | Loss 1.2258(1.2239) | Error 0.0296(0.0298) Steps 434(429.30) | Grad Norm 0.2230(0.3450) | Total Time 10.00(10.00)\n",
      "Iter 1157 | Time 32.6149(32.3977) | Bit/dim 1.1787(1.1759) | Xent 0.0958(0.0963) | Loss 1.2266(1.2240) | Error 0.0294(0.0298) Steps 434(429.44) | Grad Norm 0.2715(0.3428) | Total Time 10.00(10.00)\n",
      "Iter 1158 | Time 33.0469(32.4172) | Bit/dim 1.1828(1.1761) | Xent 0.0892(0.0961) | Loss 1.2274(1.2241) | Error 0.0291(0.0298) Steps 434(429.58) | Grad Norm 0.4238(0.3453) | Total Time 10.00(10.00)\n",
      "Iter 1159 | Time 32.1657(32.4096) | Bit/dim 1.1804(1.1762) | Xent 0.0986(0.0962) | Loss 1.2297(1.2243) | Error 0.0295(0.0298) Steps 434(429.71) | Grad Norm 0.3039(0.3440) | Total Time 10.00(10.00)\n",
      "Iter 1160 | Time 33.3408(32.4376) | Bit/dim 1.1751(1.1762) | Xent 0.0891(0.0959) | Loss 1.2196(1.2241) | Error 0.0298(0.0298) Steps 434(429.84) | Grad Norm 0.3673(0.3447) | Total Time 10.00(10.00)\n",
      "Iter 1161 | Time 33.7951(32.4783) | Bit/dim 1.1735(1.1761) | Xent 0.1066(0.0963) | Loss 1.2269(1.2242) | Error 0.0325(0.0299) Steps 434(429.97) | Grad Norm 0.3645(0.3453) | Total Time 10.00(10.00)\n",
      "Iter 1162 | Time 32.5473(32.4804) | Bit/dim 1.1754(1.1761) | Xent 0.0936(0.0962) | Loss 1.2222(1.2242) | Error 0.0290(0.0298) Steps 434(430.09) | Grad Norm 0.1718(0.3401) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0166 | Time 16.4463, Epoch Time 259.9475(245.8571), Bit/dim 1.1712(best: 1.1658), Xent 0.0465, Loss 1.1945, Error 0.0151(best: 0.0150)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1163 | Time 33.4898(32.5106) | Bit/dim 1.1724(1.1760) | Xent 0.0979(0.0962) | Loss 1.2214(1.2241) | Error 0.0295(0.0298) Steps 434(430.20) | Grad Norm 0.2552(0.3376) | Total Time 10.00(10.00)\n",
      "Iter 1164 | Time 33.1629(32.5302) | Bit/dim 1.1807(1.1761) | Xent 0.1003(0.0964) | Loss 1.2309(1.2243) | Error 0.0308(0.0299) Steps 434(430.32) | Grad Norm 0.2769(0.3357) | Total Time 10.00(10.00)\n",
      "Iter 1165 | Time 33.3929(32.5561) | Bit/dim 1.1752(1.1761) | Xent 0.0988(0.0964) | Loss 1.2246(1.2243) | Error 0.0299(0.0299) Steps 434(430.43) | Grad Norm 0.2933(0.3345) | Total Time 10.00(10.00)\n",
      "Iter 1166 | Time 32.7685(32.5625) | Bit/dim 1.1741(1.1760) | Xent 0.1030(0.0966) | Loss 1.2256(1.2243) | Error 0.0317(0.0299) Steps 434(430.54) | Grad Norm 0.3507(0.3350) | Total Time 10.00(10.00)\n",
      "Iter 1167 | Time 32.7038(32.5667) | Bit/dim 1.1792(1.1761) | Xent 0.0977(0.0967) | Loss 1.2281(1.2244) | Error 0.0288(0.0299) Steps 434(430.64) | Grad Norm 0.2422(0.3322) | Total Time 10.00(10.00)\n",
      "Iter 1168 | Time 32.8178(32.5742) | Bit/dim 1.1736(1.1760) | Xent 0.0981(0.0967) | Loss 1.2226(1.2244) | Error 0.0299(0.0299) Steps 434(430.74) | Grad Norm 0.2425(0.3295) | Total Time 10.00(10.00)\n",
      "Iter 1169 | Time 32.9395(32.5852) | Bit/dim 1.1798(1.1761) | Xent 0.0889(0.0965) | Loss 1.2242(1.2244) | Error 0.0269(0.0298) Steps 434(430.84) | Grad Norm 0.1865(0.3252) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0167 | Time 16.5885, Epoch Time 260.4050(246.2935), Bit/dim 1.1703(best: 1.1658), Xent 0.0474, Loss 1.1940, Error 0.0146(best: 0.0150)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1170 | Time 33.6213(32.6163) | Bit/dim 1.1775(1.1762) | Xent 0.0931(0.0964) | Loss 1.2240(1.2244) | Error 0.0275(0.0297) Steps 434(430.93) | Grad Norm 0.3264(0.3252) | Total Time 10.00(10.00)\n",
      "Iter 1171 | Time 34.0559(32.6595) | Bit/dim 1.1785(1.1763) | Xent 0.0867(0.0961) | Loss 1.2218(1.2243) | Error 0.0260(0.0296) Steps 434(431.03) | Grad Norm 0.2528(0.3231) | Total Time 10.00(10.00)\n",
      "Iter 1172 | Time 33.8358(32.6948) | Bit/dim 1.1759(1.1762) | Xent 0.0817(0.0956) | Loss 1.2168(1.2241) | Error 0.0265(0.0295) Steps 434(431.11) | Grad Norm 0.2413(0.3206) | Total Time 10.00(10.00)\n",
      "Iter 1173 | Time 34.4523(32.7475) | Bit/dim 1.1768(1.1763) | Xent 0.0970(0.0957) | Loss 1.2253(1.2241) | Error 0.0291(0.0295) Steps 434(431.20) | Grad Norm 0.3105(0.3203) | Total Time 10.00(10.00)\n",
      "Iter 1174 | Time 32.6000(32.7431) | Bit/dim 1.1736(1.1762) | Xent 0.0951(0.0957) | Loss 1.2212(1.2240) | Error 0.0298(0.0295) Steps 434(431.29) | Grad Norm 0.3155(0.3202) | Total Time 10.00(10.00)\n",
      "Iter 1175 | Time 32.9661(32.7498) | Bit/dim 1.1679(1.1759) | Xent 0.1000(0.0958) | Loss 1.2179(1.2238) | Error 0.0301(0.0295) Steps 434(431.37) | Grad Norm 0.2489(0.3180) | Total Time 10.00(10.00)\n",
      "Iter 1176 | Time 32.3204(32.7369) | Bit/dim 1.1817(1.1761) | Xent 0.0921(0.0957) | Loss 1.2278(1.2240) | Error 0.0266(0.0294) Steps 434(431.45) | Grad Norm 0.3045(0.3176) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0168 | Time 16.2186, Epoch Time 262.6051(246.7829), Bit/dim 1.1705(best: 1.1658), Xent 0.0473, Loss 1.1941, Error 0.0151(best: 0.0146)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1177 | Time 32.9104(32.7421) | Bit/dim 1.1765(1.1761) | Xent 0.0928(0.0956) | Loss 1.2229(1.2239) | Error 0.0299(0.0295) Steps 440(431.70) | Grad Norm 0.5317(0.3240) | Total Time 10.00(10.00)\n",
      "Iter 1178 | Time 33.1665(32.7548) | Bit/dim 1.1779(1.1762) | Xent 0.0842(0.0953) | Loss 1.2200(1.2238) | Error 0.0248(0.0293) Steps 434(431.77) | Grad Norm 0.3775(0.3256) | Total Time 10.00(10.00)\n",
      "Iter 1179 | Time 32.6013(32.7502) | Bit/dim 1.1747(1.1761) | Xent 0.1020(0.0955) | Loss 1.2257(1.2239) | Error 0.0327(0.0294) Steps 434(431.84) | Grad Norm 0.6091(0.3341) | Total Time 10.00(10.00)\n",
      "Iter 1180 | Time 32.1065(32.7309) | Bit/dim 1.1750(1.1761) | Xent 0.0967(0.0955) | Loss 1.2234(1.2239) | Error 0.0295(0.0294) Steps 434(431.90) | Grad Norm 0.4442(0.3374) | Total Time 10.00(10.00)\n",
      "Iter 1181 | Time 32.5264(32.7248) | Bit/dim 1.1785(1.1762) | Xent 0.0852(0.0952) | Loss 1.2211(1.2238) | Error 0.0290(0.0294) Steps 440(432.15) | Grad Norm 0.4546(0.3410) | Total Time 10.00(10.00)\n",
      "Iter 1182 | Time 33.8652(32.7590) | Bit/dim 1.1765(1.1762) | Xent 0.0943(0.0952) | Loss 1.2236(1.2238) | Error 0.0296(0.0294) Steps 440(432.38) | Grad Norm 0.3828(0.3422) | Total Time 10.00(10.00)\n",
      "Iter 1183 | Time 33.1699(32.7713) | Bit/dim 1.1775(1.1762) | Xent 0.1067(0.0955) | Loss 1.2308(1.2240) | Error 0.0340(0.0296) Steps 434(432.43) | Grad Norm 0.4508(0.3455) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0169 | Time 16.2992, Epoch Time 259.2805(247.1578), Bit/dim 1.1712(best: 1.1658), Xent 0.0458, Loss 1.1941, Error 0.0158(best: 0.0146)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1184 | Time 33.4235(32.7909) | Bit/dim 1.1766(1.1762) | Xent 0.0988(0.0956) | Loss 1.2260(1.2240) | Error 0.0324(0.0296) Steps 434(432.48) | Grad Norm 0.4374(0.3482) | Total Time 10.00(10.00)\n",
      "Iter 1185 | Time 32.4746(32.7814) | Bit/dim 1.1779(1.1763) | Xent 0.1052(0.0959) | Loss 1.2305(1.2242) | Error 0.0316(0.0297) Steps 434(432.52) | Grad Norm 0.5998(0.3558) | Total Time 10.00(10.00)\n",
      "Iter 1186 | Time 32.8820(32.7844) | Bit/dim 1.1777(1.1763) | Xent 0.0943(0.0959) | Loss 1.2248(1.2242) | Error 0.0286(0.0297) Steps 434(432.57) | Grad Norm 0.3604(0.3559) | Total Time 10.00(10.00)\n",
      "Iter 1187 | Time 32.8019(32.7849) | Bit/dim 1.1724(1.1762) | Xent 0.0844(0.0955) | Loss 1.2146(1.2240) | Error 0.0256(0.0295) Steps 434(432.61) | Grad Norm 0.4575(0.3590) | Total Time 10.00(10.00)\n",
      "Iter 1188 | Time 32.5181(32.7769) | Bit/dim 1.1818(1.1764) | Xent 0.0837(0.0952) | Loss 1.2237(1.2239) | Error 0.0268(0.0295) Steps 434(432.65) | Grad Norm 0.3711(0.3593) | Total Time 10.00(10.00)\n",
      "Iter 1189 | Time 33.7895(32.8073) | Bit/dim 1.1778(1.1764) | Xent 0.1033(0.0954) | Loss 1.2294(1.2241) | Error 0.0321(0.0295) Steps 434(432.69) | Grad Norm 0.2855(0.3571) | Total Time 10.00(10.00)\n",
      "Iter 1190 | Time 33.6238(32.8318) | Bit/dim 1.1769(1.1764) | Xent 0.1020(0.0956) | Loss 1.2279(1.2242) | Error 0.0309(0.0296) Steps 434(432.73) | Grad Norm 0.3347(0.3564) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0170 | Time 16.4941, Epoch Time 260.6086(247.5613), Bit/dim 1.1720(best: 1.1658), Xent 0.0489, Loss 1.1964, Error 0.0158(best: 0.0146)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1191 | Time 34.2796(32.8752) | Bit/dim 1.1742(1.1764) | Xent 0.0996(0.0957) | Loss 1.2240(1.2242) | Error 0.0305(0.0296) Steps 434(432.77) | Grad Norm 0.2895(0.3544) | Total Time 10.00(10.00)\n",
      "Iter 1192 | Time 33.3066(32.8882) | Bit/dim 1.1784(1.1764) | Xent 0.1027(0.0959) | Loss 1.2298(1.2244) | Error 0.0308(0.0296) Steps 440(432.99) | Grad Norm 0.3515(0.3543) | Total Time 10.00(10.00)\n",
      "Iter 1193 | Time 33.7161(32.9130) | Bit/dim 1.1800(1.1765) | Xent 0.0958(0.0959) | Loss 1.2279(1.2245) | Error 0.0305(0.0297) Steps 446(433.38) | Grad Norm 0.2731(0.3519) | Total Time 10.00(10.00)\n",
      "Iter 1194 | Time 33.9125(32.9430) | Bit/dim 1.1811(1.1767) | Xent 0.1006(0.0961) | Loss 1.2314(1.2247) | Error 0.0296(0.0297) Steps 440(433.58) | Grad Norm 0.2621(0.3492) | Total Time 10.00(10.00)\n",
      "Iter 1195 | Time 33.6654(32.9647) | Bit/dim 1.1787(1.1767) | Xent 0.0861(0.0958) | Loss 1.2218(1.2246) | Error 0.0268(0.0296) Steps 440(433.77) | Grad Norm 0.3379(0.3489) | Total Time 10.00(10.00)\n",
      "Iter 1196 | Time 33.5857(32.9833) | Bit/dim 1.1731(1.1766) | Xent 0.0904(0.0956) | Loss 1.2183(1.2244) | Error 0.0291(0.0296) Steps 440(433.96) | Grad Norm 0.2879(0.3470) | Total Time 10.00(10.00)\n",
      "Iter 1197 | Time 33.4283(32.9966) | Bit/dim 1.1794(1.1767) | Xent 0.0934(0.0955) | Loss 1.2262(1.2245) | Error 0.0301(0.0296) Steps 440(434.14) | Grad Norm 0.2474(0.3441) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0171 | Time 16.4692, Epoch Time 264.9297(248.0824), Bit/dim 1.1715(best: 1.1658), Xent 0.0472, Loss 1.1951, Error 0.0165(best: 0.0146)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1198 | Time 34.0099(33.0270) | Bit/dim 1.1813(1.1768) | Xent 0.1023(0.0957) | Loss 1.2324(1.2247) | Error 0.0336(0.0297) Steps 440(434.31) | Grad Norm 0.2845(0.3423) | Total Time 10.00(10.00)\n",
      "Iter 1199 | Time 34.3658(33.0672) | Bit/dim 1.1743(1.1768) | Xent 0.1016(0.0959) | Loss 1.2252(1.2247) | Error 0.0320(0.0298) Steps 440(434.48) | Grad Norm 0.5147(0.3474) | Total Time 10.00(10.00)\n",
      "Iter 1200 | Time 33.9945(33.0950) | Bit/dim 1.1793(1.1768) | Xent 0.0887(0.0957) | Loss 1.2237(1.2247) | Error 0.0272(0.0297) Steps 440(434.65) | Grad Norm 0.2372(0.3441) | Total Time 10.00(10.00)\n",
      "Iter 1201 | Time 33.2708(33.1003) | Bit/dim 1.1818(1.1770) | Xent 0.0961(0.0957) | Loss 1.2299(1.2249) | Error 0.0285(0.0297) Steps 440(434.81) | Grad Norm 0.3376(0.3439) | Total Time 10.00(10.00)\n",
      "Iter 1202 | Time 34.5631(33.1442) | Bit/dim 1.1728(1.1769) | Xent 0.0967(0.0957) | Loss 1.2212(1.2247) | Error 0.0278(0.0296) Steps 434(434.78) | Grad Norm 0.3007(0.3426) | Total Time 10.00(10.00)\n",
      "Iter 1203 | Time 33.4022(33.1519) | Bit/dim 1.1790(1.1769) | Xent 0.0962(0.0958) | Loss 1.2271(1.2248) | Error 0.0280(0.0296) Steps 434(434.76) | Grad Norm 0.4511(0.3459) | Total Time 10.00(10.00)\n",
      "Iter 1204 | Time 33.8839(33.1739) | Bit/dim 1.1729(1.1768) | Xent 0.0950(0.0957) | Loss 1.2204(1.2247) | Error 0.0302(0.0296) Steps 434(434.74) | Grad Norm 0.4958(0.3504) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0172 | Time 16.2501, Epoch Time 266.3991(248.6319), Bit/dim 1.1720(best: 1.1658), Xent 0.0486, Loss 1.1962, Error 0.0149(best: 0.0146)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1205 | Time 33.4510(33.1822) | Bit/dim 1.1760(1.1768) | Xent 0.0878(0.0955) | Loss 1.2199(1.2245) | Error 0.0262(0.0295) Steps 434(434.72) | Grad Norm 0.3906(0.3516) | Total Time 10.00(10.00)\n",
      "Iter 1206 | Time 33.2795(33.1851) | Bit/dim 1.1757(1.1768) | Xent 0.0925(0.0954) | Loss 1.2219(1.2245) | Error 0.0292(0.0295) Steps 434(434.69) | Grad Norm 0.5867(0.3587) | Total Time 10.00(10.00)\n",
      "Iter 1207 | Time 34.5376(33.2257) | Bit/dim 1.1789(1.1768) | Xent 0.0995(0.0955) | Loss 1.2286(1.2246) | Error 0.0301(0.0295) Steps 434(434.67) | Grad Norm 0.4000(0.3599) | Total Time 10.00(10.00)\n",
      "Iter 1208 | Time 33.8673(33.2449) | Bit/dim 1.1777(1.1768) | Xent 0.0862(0.0952) | Loss 1.2209(1.2245) | Error 0.0286(0.0295) Steps 434(434.65) | Grad Norm 0.2336(0.3561) | Total Time 10.00(10.00)\n",
      "Iter 1209 | Time 33.5660(33.2546) | Bit/dim 1.1792(1.1769) | Xent 0.1059(0.0956) | Loss 1.2321(1.2247) | Error 0.0292(0.0295) Steps 434(434.63) | Grad Norm 0.6828(0.3659) | Total Time 10.00(10.00)\n",
      "Iter 1210 | Time 33.7084(33.2682) | Bit/dim 1.1764(1.1769) | Xent 0.0884(0.0954) | Loss 1.2206(1.2246) | Error 0.0279(0.0294) Steps 434(434.62) | Grad Norm 0.2814(0.3634) | Total Time 10.00(10.00)\n",
      "Iter 1211 | Time 33.0549(33.2618) | Bit/dim 1.1766(1.1769) | Xent 0.0977(0.0954) | Loss 1.2255(1.2246) | Error 0.0298(0.0294) Steps 434(434.60) | Grad Norm 0.8916(0.3792) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0173 | Time 16.2461, Epoch Time 264.6165(249.1114), Bit/dim 1.1712(best: 1.1658), Xent 0.0464, Loss 1.1944, Error 0.0160(best: 0.0146)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1212 | Time 33.7418(33.2762) | Bit/dim 1.1792(1.1770) | Xent 0.0938(0.0954) | Loss 1.2261(1.2246) | Error 0.0281(0.0294) Steps 434(434.58) | Grad Norm 0.4353(0.3809) | Total Time 10.00(10.00)\n",
      "Iter 1213 | Time 33.4401(33.2811) | Bit/dim 1.1744(1.1769) | Xent 0.1021(0.0956) | Loss 1.2254(1.2247) | Error 0.0296(0.0294) Steps 434(434.56) | Grad Norm 0.5639(0.3864) | Total Time 10.00(10.00)\n",
      "Iter 1214 | Time 33.4604(33.2865) | Bit/dim 1.1761(1.1769) | Xent 0.0928(0.0955) | Loss 1.2225(1.2246) | Error 0.0290(0.0294) Steps 434(434.54) | Grad Norm 0.4810(0.3892) | Total Time 10.00(10.00)\n",
      "Iter 1215 | Time 33.6839(33.2984) | Bit/dim 1.1768(1.1769) | Xent 0.0954(0.0955) | Loss 1.2245(1.2246) | Error 0.0312(0.0294) Steps 434(434.53) | Grad Norm 0.3471(0.3880) | Total Time 10.00(10.00)\n",
      "Iter 1216 | Time 33.4924(33.3042) | Bit/dim 1.1780(1.1769) | Xent 0.0944(0.0955) | Loss 1.2252(1.2246) | Error 0.0296(0.0294) Steps 434(434.51) | Grad Norm 0.4741(0.3905) | Total Time 10.00(10.00)\n",
      "Iter 1217 | Time 33.8282(33.3199) | Bit/dim 1.1804(1.1770) | Xent 0.0959(0.0955) | Loss 1.2283(1.2247) | Error 0.0282(0.0294) Steps 440(434.68) | Grad Norm 0.3976(0.3908) | Total Time 10.00(10.00)\n",
      "Iter 1218 | Time 34.5363(33.3564) | Bit/dim 1.1762(1.1770) | Xent 0.0972(0.0955) | Loss 1.2248(1.2247) | Error 0.0276(0.0293) Steps 440(434.84) | Grad Norm 0.6655(0.3990) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0174 | Time 16.2765, Epoch Time 265.1716(249.5932), Bit/dim 1.1698(best: 1.1658), Xent 0.0469, Loss 1.1932, Error 0.0149(best: 0.0146)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1219 | Time 33.9383(33.3739) | Bit/dim 1.1820(1.1771) | Xent 0.0939(0.0955) | Loss 1.2290(1.2249) | Error 0.0281(0.0293) Steps 440(434.99) | Grad Norm 0.5328(0.4030) | Total Time 10.00(10.00)\n",
      "Iter 1220 | Time 35.0493(33.4241) | Bit/dim 1.1780(1.1771) | Xent 0.1036(0.0957) | Loss 1.2298(1.2250) | Error 0.0319(0.0294) Steps 440(435.14) | Grad Norm 0.7681(0.4140) | Total Time 10.00(10.00)\n",
      "Iter 1221 | Time 34.0565(33.4431) | Bit/dim 1.1771(1.1771) | Xent 0.0953(0.0957) | Loss 1.2247(1.2250) | Error 0.0294(0.0294) Steps 440(435.29) | Grad Norm 0.7292(0.4234) | Total Time 10.00(10.00)\n",
      "Iter 1222 | Time 34.7343(33.4819) | Bit/dim 1.1760(1.1771) | Xent 0.0868(0.0954) | Loss 1.2194(1.2248) | Error 0.0278(0.0293) Steps 446(435.61) | Grad Norm 0.5604(0.4275) | Total Time 10.00(10.00)\n",
      "Iter 1223 | Time 34.5281(33.5132) | Bit/dim 1.1746(1.1770) | Xent 0.0868(0.0952) | Loss 1.2180(1.2246) | Error 0.0285(0.0293) Steps 440(435.74) | Grad Norm 0.7188(0.4363) | Total Time 10.00(10.00)\n",
      "Iter 1224 | Time 34.7462(33.5502) | Bit/dim 1.1767(1.1770) | Xent 0.0860(0.0949) | Loss 1.2197(1.2245) | Error 0.0258(0.0292) Steps 446(436.05) | Grad Norm 0.6452(0.4425) | Total Time 10.00(10.00)\n",
      "Iter 1225 | Time 34.8951(33.5906) | Bit/dim 1.1808(1.1771) | Xent 0.0884(0.0947) | Loss 1.2249(1.2245) | Error 0.0282(0.0292) Steps 446(436.35) | Grad Norm 0.6796(0.4496) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0175 | Time 16.6621, Epoch Time 271.2111(250.2418), Bit/dim 1.1727(best: 1.1658), Xent 0.0436, Loss 1.1945, Error 0.0139(best: 0.0146)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1226 | Time 34.6244(33.6216) | Bit/dim 1.1800(1.1772) | Xent 0.0854(0.0944) | Loss 1.2227(1.2244) | Error 0.0236(0.0290) Steps 452(436.82) | Grad Norm 0.7241(0.4579) | Total Time 10.00(10.00)\n",
      "Iter 1227 | Time 34.1566(33.6376) | Bit/dim 1.1792(1.1773) | Xent 0.0921(0.0944) | Loss 1.2253(1.2245) | Error 0.0302(0.0290) Steps 446(437.09) | Grad Norm 0.6261(0.4629) | Total Time 10.00(10.00)\n",
      "Iter 1228 | Time 33.8013(33.6426) | Bit/dim 1.1779(1.1773) | Xent 0.0974(0.0944) | Loss 1.2266(1.2245) | Error 0.0296(0.0291) Steps 446(437.36) | Grad Norm 0.4887(0.4637) | Total Time 10.00(10.00)\n",
      "Iter 1229 | Time 34.6294(33.6722) | Bit/dim 1.1813(1.1774) | Xent 0.0976(0.0945) | Loss 1.2301(1.2247) | Error 0.0305(0.0291) Steps 446(437.62) | Grad Norm 0.7930(0.4736) | Total Time 10.00(10.00)\n",
      "Iter 1230 | Time 34.4738(33.6962) | Bit/dim 1.1758(1.1774) | Xent 0.1017(0.0948) | Loss 1.2266(1.2248) | Error 0.0329(0.0292) Steps 446(437.87) | Grad Norm 0.3320(0.4693) | Total Time 10.00(10.00)\n",
      "Iter 1231 | Time 36.1311(33.7693) | Bit/dim 1.1798(1.1774) | Xent 0.0952(0.0948) | Loss 1.2274(1.2248) | Error 0.0302(0.0293) Steps 446(438.11) | Grad Norm 0.4406(0.4685) | Total Time 10.00(10.00)\n",
      "Iter 1232 | Time 35.5684(33.8232) | Bit/dim 1.1792(1.1775) | Xent 0.0990(0.0949) | Loss 1.2288(1.2249) | Error 0.0285(0.0292) Steps 446(438.35) | Grad Norm 0.4080(0.4667) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0176 | Time 16.6468, Epoch Time 272.5781(250.9119), Bit/dim 1.1719(best: 1.1658), Xent 0.0472, Loss 1.1955, Error 0.0160(best: 0.0139)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1233 | Time 34.0942(33.8314) | Bit/dim 1.1796(1.1776) | Xent 0.0869(0.0947) | Loss 1.2230(1.2249) | Error 0.0281(0.0292) Steps 446(438.58) | Grad Norm 0.2941(0.4615) | Total Time 10.00(10.00)\n",
      "Iter 1234 | Time 33.9243(33.8341) | Bit/dim 1.1738(1.1774) | Xent 0.0915(0.0946) | Loss 1.2195(1.2247) | Error 0.0308(0.0292) Steps 446(438.80) | Grad Norm 0.3616(0.4585) | Total Time 10.00(10.00)\n",
      "Iter 1235 | Time 34.3591(33.8499) | Bit/dim 1.1770(1.1774) | Xent 0.1010(0.0948) | Loss 1.2275(1.2248) | Error 0.0305(0.0293) Steps 446(439.02) | Grad Norm 0.3106(0.4540) | Total Time 10.00(10.00)\n",
      "Iter 1236 | Time 34.0872(33.8570) | Bit/dim 1.1783(1.1775) | Xent 0.0935(0.0947) | Loss 1.2250(1.2248) | Error 0.0292(0.0293) Steps 446(439.23) | Grad Norm 0.4073(0.4526) | Total Time 10.00(10.00)\n",
      "Iter 1237 | Time 34.6940(33.8821) | Bit/dim 1.1815(1.1776) | Xent 0.0924(0.0946) | Loss 1.2276(1.2249) | Error 0.0294(0.0293) Steps 446(439.43) | Grad Norm 0.3292(0.4489) | Total Time 10.00(10.00)\n",
      "Iter 1238 | Time 35.1841(33.9212) | Bit/dim 1.1733(1.1775) | Xent 0.0903(0.0945) | Loss 1.2185(1.2247) | Error 0.0294(0.0293) Steps 446(439.63) | Grad Norm 0.4935(0.4503) | Total Time 10.00(10.00)\n",
      "Iter 1239 | Time 33.9866(33.9231) | Bit/dim 1.1796(1.1775) | Xent 0.1067(0.0949) | Loss 1.2329(1.2250) | Error 0.0319(0.0294) Steps 446(439.82) | Grad Norm 0.3034(0.4459) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0177 | Time 16.9094, Epoch Time 269.9428(251.4828), Bit/dim 1.1722(best: 1.1658), Xent 0.0455, Loss 1.1950, Error 0.0152(best: 0.0139)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1240 | Time 36.1988(33.9914) | Bit/dim 1.1800(1.1776) | Xent 0.0890(0.0947) | Loss 1.2245(1.2249) | Error 0.0299(0.0294) Steps 446(440.00) | Grad Norm 0.5554(0.4492) | Total Time 10.00(10.00)\n",
      "Iter 1241 | Time 34.5454(34.0080) | Bit/dim 1.1763(1.1776) | Xent 0.0943(0.0947) | Loss 1.2234(1.2249) | Error 0.0302(0.0294) Steps 446(440.18) | Grad Norm 0.3431(0.4460) | Total Time 10.00(10.00)\n",
      "Iter 1242 | Time 34.7461(34.0302) | Bit/dim 1.1783(1.1776) | Xent 0.0915(0.0946) | Loss 1.2241(1.2249) | Error 0.0276(0.0294) Steps 446(440.36) | Grad Norm 1.1070(0.4658) | Total Time 10.00(10.00)\n",
      "Iter 1243 | Time 34.3528(34.0399) | Bit/dim 1.1792(1.1776) | Xent 0.0911(0.0945) | Loss 1.2247(1.2249) | Error 0.0284(0.0293) Steps 446(440.53) | Grad Norm 0.5986(0.4698) | Total Time 10.00(10.00)\n",
      "Iter 1244 | Time 34.3763(34.0499) | Bit/dim 1.1757(1.1776) | Xent 0.0983(0.0946) | Loss 1.2248(1.2249) | Error 0.0299(0.0293) Steps 446(440.69) | Grad Norm 0.8030(0.4798) | Total Time 10.00(10.00)\n",
      "Iter 1245 | Time 34.3814(34.0599) | Bit/dim 1.1752(1.1775) | Xent 0.1008(0.0948) | Loss 1.2256(1.2249) | Error 0.0299(0.0294) Steps 446(440.85) | Grad Norm 0.4647(0.4793) | Total Time 10.00(10.00)\n",
      "Iter 1246 | Time 34.5124(34.0735) | Bit/dim 1.1747(1.1774) | Xent 0.0968(0.0949) | Loss 1.2231(1.2248) | Error 0.0276(0.0293) Steps 446(441.01) | Grad Norm 0.5507(0.4815) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0178 | Time 16.9462, Epoch Time 272.8532(252.1239), Bit/dim 1.1712(best: 1.1658), Xent 0.0465, Loss 1.1944, Error 0.0154(best: 0.0139)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1247 | Time 35.3036(34.1104) | Bit/dim 1.1756(1.1774) | Xent 0.0901(0.0947) | Loss 1.2207(1.2247) | Error 0.0286(0.0293) Steps 446(441.16) | Grad Norm 0.2996(0.4760) | Total Time 10.00(10.00)\n",
      "Iter 1248 | Time 33.7226(34.0987) | Bit/dim 1.1789(1.1774) | Xent 0.0917(0.0946) | Loss 1.2247(1.2247) | Error 0.0301(0.0293) Steps 446(441.30) | Grad Norm 0.8261(0.4865) | Total Time 10.00(10.00)\n",
      "Iter 1249 | Time 35.3325(34.1357) | Bit/dim 1.1771(1.1774) | Xent 0.0890(0.0944) | Loss 1.2216(1.2246) | Error 0.0268(0.0292) Steps 446(441.44) | Grad Norm 0.4306(0.4848) | Total Time 10.00(10.00)\n",
      "Iter 1250 | Time 34.8460(34.1571) | Bit/dim 1.1793(1.1775) | Xent 0.0876(0.0942) | Loss 1.2231(1.2246) | Error 0.0280(0.0292) Steps 446(441.58) | Grad Norm 0.7215(0.4919) | Total Time 10.00(10.00)\n",
      "Iter 1251 | Time 34.5340(34.1684) | Bit/dim 1.1776(1.1775) | Xent 0.0993(0.0944) | Loss 1.2272(1.2247) | Error 0.0292(0.0292) Steps 446(441.71) | Grad Norm 0.5108(0.4925) | Total Time 10.00(10.00)\n",
      "Iter 1252 | Time 35.7431(34.2156) | Bit/dim 1.1790(1.1775) | Xent 0.1031(0.0947) | Loss 1.2305(1.2248) | Error 0.0323(0.0293) Steps 446(441.84) | Grad Norm 0.6024(0.4958) | Total Time 10.00(10.00)\n",
      "Iter 1253 | Time 34.6271(34.2280) | Bit/dim 1.1737(1.1774) | Xent 0.0964(0.0947) | Loss 1.2220(1.2247) | Error 0.0285(0.0293) Steps 446(441.97) | Grad Norm 0.4164(0.4934) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0179 | Time 16.9549, Epoch Time 273.9638(252.7791), Bit/dim 1.1717(best: 1.1658), Xent 0.0498, Loss 1.1966, Error 0.0176(best: 0.0139)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1254 | Time 34.0998(34.2241) | Bit/dim 1.1790(1.1774) | Xent 0.0880(0.0945) | Loss 1.2230(1.2247) | Error 0.0296(0.0293) Steps 446(442.09) | Grad Norm 0.7168(0.5001) | Total Time 10.00(10.00)\n",
      "Iter 1255 | Time 34.6670(34.2374) | Bit/dim 1.1789(1.1775) | Xent 0.0879(0.0943) | Loss 1.2228(1.2246) | Error 0.0281(0.0292) Steps 446(442.20) | Grad Norm 0.5776(0.5024) | Total Time 10.00(10.00)\n",
      "Iter 1256 | Time 34.2893(34.2389) | Bit/dim 1.1825(1.1776) | Xent 0.0956(0.0944) | Loss 1.2303(1.2248) | Error 0.0316(0.0293) Steps 446(442.32) | Grad Norm 0.5682(0.5044) | Total Time 10.00(10.00)\n",
      "Iter 1257 | Time 33.9576(34.2305) | Bit/dim 1.1781(1.1776) | Xent 0.0944(0.0944) | Loss 1.2253(1.2248) | Error 0.0300(0.0293) Steps 446(442.43) | Grad Norm 0.7704(0.5124) | Total Time 10.00(10.00)\n",
      "Iter 1258 | Time 33.8880(34.2202) | Bit/dim 1.1770(1.1776) | Xent 0.0965(0.0944) | Loss 1.2252(1.2248) | Error 0.0314(0.0294) Steps 446(442.54) | Grad Norm 0.4672(0.5110) | Total Time 10.00(10.00)\n",
      "Iter 1259 | Time 35.1523(34.2482) | Bit/dim 1.1753(1.1776) | Xent 0.0888(0.0943) | Loss 1.2197(1.2247) | Error 0.0254(0.0293) Steps 446(442.64) | Grad Norm 1.2443(0.5330) | Total Time 10.00(10.00)\n",
      "Iter 1260 | Time 34.8316(34.2657) | Bit/dim 1.1782(1.1776) | Xent 0.0881(0.0941) | Loss 1.2223(1.2246) | Error 0.0276(0.0292) Steps 446(442.74) | Grad Norm 0.6398(0.5362) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0180 | Time 16.4856, Epoch Time 269.8295(253.2906), Bit/dim 1.1716(best: 1.1658), Xent 0.0465, Loss 1.1949, Error 0.0162(best: 0.0139)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1261 | Time 34.5574(34.2744) | Bit/dim 1.1775(1.1776) | Xent 0.1021(0.0943) | Loss 1.2286(1.2247) | Error 0.0316(0.0293) Steps 446(442.84) | Grad Norm 0.5199(0.5358) | Total Time 10.00(10.00)\n",
      "Iter 1262 | Time 34.5578(34.2829) | Bit/dim 1.1829(1.1777) | Xent 0.0927(0.0943) | Loss 1.2293(1.2249) | Error 0.0295(0.0293) Steps 446(442.93) | Grad Norm 0.9319(0.5476) | Total Time 10.00(10.00)\n",
      "Iter 1263 | Time 33.6892(34.2651) | Bit/dim 1.1743(1.1776) | Xent 0.0917(0.0942) | Loss 1.2201(1.2247) | Error 0.0302(0.0293) Steps 446(443.02) | Grad Norm 0.5560(0.5479) | Total Time 10.00(10.00)\n",
      "Iter 1264 | Time 34.6050(34.2753) | Bit/dim 1.1818(1.1778) | Xent 0.0964(0.0942) | Loss 1.2300(1.2249) | Error 0.0296(0.0293) Steps 446(443.11) | Grad Norm 1.0008(0.5615) | Total Time 10.00(10.00)\n",
      "Iter 1265 | Time 34.7608(34.2899) | Bit/dim 1.1806(1.1778) | Xent 0.0924(0.0942) | Loss 1.2268(1.2249) | Error 0.0306(0.0294) Steps 446(443.20) | Grad Norm 0.6678(0.5647) | Total Time 10.00(10.00)\n",
      "Iter 1266 | Time 34.6328(34.3002) | Bit/dim 1.1760(1.1778) | Xent 0.0906(0.0941) | Loss 1.2213(1.2248) | Error 0.0290(0.0294) Steps 446(443.28) | Grad Norm 0.7374(0.5698) | Total Time 10.00(10.00)\n",
      "Iter 1267 | Time 34.4295(34.3041) | Bit/dim 1.1774(1.1778) | Xent 0.1025(0.0943) | Loss 1.2286(1.2249) | Error 0.0316(0.0294) Steps 446(443.37) | Grad Norm 0.7282(0.5746) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0181 | Time 16.9383, Epoch Time 270.9417(253.8201), Bit/dim 1.1727(best: 1.1658), Xent 0.0462, Loss 1.1958, Error 0.0149(best: 0.0139)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1268 | Time 34.4603(34.3087) | Bit/dim 1.1768(1.1777) | Xent 0.0916(0.0943) | Loss 1.2226(1.2249) | Error 0.0295(0.0294) Steps 446(443.44) | Grad Norm 0.3252(0.5671) | Total Time 10.00(10.00)\n",
      "Iter 1269 | Time 34.4518(34.3130) | Bit/dim 1.1714(1.1776) | Xent 0.0970(0.0943) | Loss 1.2199(1.2247) | Error 0.0290(0.0294) Steps 446(443.52) | Grad Norm 0.9462(0.5785) | Total Time 10.00(10.00)\n",
      "Iter 1270 | Time 34.9984(34.3336) | Bit/dim 1.1811(1.1777) | Xent 0.0927(0.0943) | Loss 1.2275(1.2248) | Error 0.0284(0.0294) Steps 446(443.60) | Grad Norm 0.8402(0.5863) | Total Time 10.00(10.00)\n",
      "Iter 1271 | Time 36.4417(34.3968) | Bit/dim 1.1847(1.1779) | Xent 0.0976(0.0944) | Loss 1.2335(1.2251) | Error 0.0305(0.0294) Steps 446(443.67) | Grad Norm 0.3912(0.5805) | Total Time 10.00(10.00)\n",
      "Iter 1272 | Time 35.1510(34.4195) | Bit/dim 1.1816(1.1780) | Xent 0.0924(0.0943) | Loss 1.2278(1.2251) | Error 0.0290(0.0294) Steps 446(443.74) | Grad Norm 0.5309(0.5790) | Total Time 10.00(10.00)\n",
      "Iter 1273 | Time 34.3320(34.4168) | Bit/dim 1.1763(1.1779) | Xent 0.0846(0.0940) | Loss 1.2186(1.2250) | Error 0.0258(0.0293) Steps 440(443.63) | Grad Norm 0.5454(0.5780) | Total Time 10.00(10.00)\n",
      "Iter 1274 | Time 34.3801(34.4157) | Bit/dim 1.1839(1.1781) | Xent 0.0931(0.0940) | Loss 1.2304(1.2251) | Error 0.0274(0.0292) Steps 440(443.52) | Grad Norm 0.6678(0.5807) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0182 | Time 16.5577, Epoch Time 273.3342(254.4056), Bit/dim 1.1745(best: 1.1658), Xent 0.0468, Loss 1.1978, Error 0.0161(best: 0.0139)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1275 | Time 34.8478(34.4287) | Bit/dim 1.1804(1.1782) | Xent 0.0895(0.0939) | Loss 1.2252(1.2251) | Error 0.0259(0.0291) Steps 446(443.59) | Grad Norm 0.4586(0.5770) | Total Time 10.00(10.00)\n",
      "Iter 1276 | Time 34.3685(34.4269) | Bit/dim 1.1756(1.1781) | Xent 0.0946(0.0939) | Loss 1.2229(1.2251) | Error 0.0296(0.0292) Steps 446(443.66) | Grad Norm 0.4566(0.5734) | Total Time 10.00(10.00)\n",
      "Iter 1277 | Time 34.9165(34.4416) | Bit/dim 1.1820(1.1782) | Xent 0.0931(0.0939) | Loss 1.2285(1.2252) | Error 0.0294(0.0292) Steps 446(443.73) | Grad Norm 0.5282(0.5721) | Total Time 10.00(10.00)\n",
      "Iter 1278 | Time 35.1084(34.4616) | Bit/dim 1.1792(1.1782) | Xent 0.0918(0.0938) | Loss 1.2250(1.2252) | Error 0.0276(0.0291) Steps 446(443.80) | Grad Norm 0.3642(0.5658) | Total Time 10.00(10.00)\n",
      "Iter 1279 | Time 34.3160(34.4572) | Bit/dim 1.1817(1.1784) | Xent 0.0946(0.0938) | Loss 1.2290(1.2253) | Error 0.0284(0.0291) Steps 446(443.87) | Grad Norm 0.3285(0.5587) | Total Time 10.00(10.00)\n",
      "Iter 1280 | Time 34.6158(34.4620) | Bit/dim 1.1796(1.1784) | Xent 0.0898(0.0937) | Loss 1.2245(1.2252) | Error 0.0269(0.0290) Steps 446(443.93) | Grad Norm 0.2281(0.5488) | Total Time 10.00(10.00)\n",
      "Iter 1281 | Time 34.9509(34.4767) | Bit/dim 1.1827(1.1785) | Xent 0.0931(0.0937) | Loss 1.2293(1.2254) | Error 0.0295(0.0290) Steps 446(443.99) | Grad Norm 0.6214(0.5510) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0183 | Time 16.4358, Epoch Time 272.0770(254.9357), Bit/dim 1.1744(best: 1.1658), Xent 0.0455, Loss 1.1972, Error 0.0156(best: 0.0139)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1282 | Time 34.5874(34.4800) | Bit/dim 1.1829(1.1787) | Xent 0.1047(0.0940) | Loss 1.2352(1.2257) | Error 0.0301(0.0291) Steps 446(444.05) | Grad Norm 0.6694(0.5545) | Total Time 10.00(10.00)\n",
      "Iter 1283 | Time 34.5091(34.4808) | Bit/dim 1.1745(1.1785) | Xent 0.0963(0.0941) | Loss 1.2227(1.2256) | Error 0.0281(0.0290) Steps 446(444.11) | Grad Norm 0.3210(0.5475) | Total Time 10.00(10.00)\n",
      "Iter 1284 | Time 35.4726(34.5106) | Bit/dim 1.1786(1.1785) | Xent 0.0971(0.0942) | Loss 1.2272(1.2256) | Error 0.0295(0.0291) Steps 446(444.17) | Grad Norm 0.8408(0.5563) | Total Time 10.00(10.00)\n",
      "Iter 1285 | Time 35.0321(34.5262) | Bit/dim 1.1775(1.1785) | Xent 0.0971(0.0943) | Loss 1.2261(1.2256) | Error 0.0299(0.0291) Steps 446(444.22) | Grad Norm 0.2486(0.5471) | Total Time 10.00(10.00)\n",
      "Iter 1286 | Time 35.3923(34.5522) | Bit/dim 1.1841(1.1787) | Xent 0.0908(0.0942) | Loss 1.2296(1.2258) | Error 0.0271(0.0290) Steps 446(444.28) | Grad Norm 1.2091(0.5669) | Total Time 10.00(10.00)\n",
      "Iter 1287 | Time 35.2853(34.5742) | Bit/dim 1.1777(1.1786) | Xent 0.1024(0.0944) | Loss 1.2289(1.2258) | Error 0.0309(0.0291) Steps 446(444.33) | Grad Norm 1.3497(0.5904) | Total Time 10.00(10.00)\n",
      "Iter 1288 | Time 34.6108(34.5753) | Bit/dim 1.1807(1.1787) | Xent 0.0962(0.0945) | Loss 1.2288(1.2259) | Error 0.0305(0.0291) Steps 446(444.38) | Grad Norm 0.3688(0.5838) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0184 | Time 16.8529, Epoch Time 274.6296(255.5265), Bit/dim 1.1733(best: 1.1658), Xent 0.0463, Loss 1.1964, Error 0.0159(best: 0.0139)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1289 | Time 35.3101(34.5974) | Bit/dim 1.1791(1.1787) | Xent 0.0921(0.0944) | Loss 1.2252(1.2259) | Error 0.0298(0.0291) Steps 446(444.43) | Grad Norm 0.9956(0.5961) | Total Time 10.00(10.00)\n",
      "Iter 1290 | Time 34.5657(34.5964) | Bit/dim 1.1814(1.1788) | Xent 0.0966(0.0945) | Loss 1.2297(1.2260) | Error 0.0301(0.0292) Steps 446(444.47) | Grad Norm 0.6512(0.5978) | Total Time 10.00(10.00)\n",
      "Iter 1291 | Time 34.3376(34.5886) | Bit/dim 1.1771(1.1787) | Xent 0.0993(0.0946) | Loss 1.2267(1.2260) | Error 0.0300(0.0292) Steps 446(444.52) | Grad Norm 0.6845(0.6004) | Total Time 10.00(10.00)\n",
      "Iter 1292 | Time 34.5380(34.5871) | Bit/dim 1.1750(1.1786) | Xent 0.0902(0.0945) | Loss 1.2201(1.2259) | Error 0.0291(0.0292) Steps 446(444.56) | Grad Norm 1.0582(0.6141) | Total Time 10.00(10.00)\n",
      "Iter 1293 | Time 35.3544(34.6101) | Bit/dim 1.1761(1.1786) | Xent 0.1037(0.0948) | Loss 1.2279(1.2259) | Error 0.0314(0.0293) Steps 446(444.61) | Grad Norm 0.4722(0.6099) | Total Time 10.00(10.00)\n",
      "Iter 1294 | Time 35.4405(34.6351) | Bit/dim 1.1780(1.1785) | Xent 0.0867(0.0945) | Loss 1.2213(1.2258) | Error 0.0261(0.0292) Steps 446(444.65) | Grad Norm 0.6676(0.6116) | Total Time 10.00(10.00)\n",
      "Iter 1295 | Time 36.0492(34.6775) | Bit/dim 1.1828(1.1787) | Xent 0.0969(0.0946) | Loss 1.2313(1.2260) | Error 0.0294(0.0292) Steps 446(444.69) | Grad Norm 0.4639(0.6072) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0185 | Time 16.5108, Epoch Time 274.6059(256.0989), Bit/dim 1.1717(best: 1.1658), Xent 0.0454, Loss 1.1944, Error 0.0157(best: 0.0139)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1296 | Time 35.0255(34.6879) | Bit/dim 1.1766(1.1786) | Xent 0.0853(0.0943) | Loss 1.2192(1.2258) | Error 0.0270(0.0291) Steps 446(444.73) | Grad Norm 0.2881(0.5976) | Total Time 10.00(10.00)\n",
      "Iter 1297 | Time 35.1386(34.7014) | Bit/dim 1.1754(1.1785) | Xent 0.0918(0.0942) | Loss 1.2213(1.2256) | Error 0.0271(0.0290) Steps 446(444.77) | Grad Norm 0.6368(0.5988) | Total Time 10.00(10.00)\n",
      "Iter 1298 | Time 34.7598(34.7032) | Bit/dim 1.1750(1.1784) | Xent 0.1022(0.0945) | Loss 1.2261(1.2256) | Error 0.0316(0.0291) Steps 446(444.80) | Grad Norm 0.5900(0.5985) | Total Time 10.00(10.00)\n",
      "Iter 1299 | Time 34.8756(34.7084) | Bit/dim 1.1814(1.1785) | Xent 0.0859(0.0942) | Loss 1.2244(1.2256) | Error 0.0260(0.0290) Steps 446(444.84) | Grad Norm 0.4541(0.5942) | Total Time 10.00(10.00)\n",
      "Iter 1300 | Time 35.1792(34.7225) | Bit/dim 1.1758(1.1784) | Xent 0.0988(0.0943) | Loss 1.2252(1.2256) | Error 0.0285(0.0290) Steps 446(444.88) | Grad Norm 0.9371(0.6045) | Total Time 10.00(10.00)\n",
      "Iter 1301 | Time 34.2731(34.7090) | Bit/dim 1.1795(1.1784) | Xent 0.0931(0.0943) | Loss 1.2261(1.2256) | Error 0.0271(0.0290) Steps 446(444.91) | Grad Norm 0.6869(0.6069) | Total Time 10.00(10.00)\n",
      "Iter 1302 | Time 36.0476(34.7492) | Bit/dim 1.1787(1.1785) | Xent 0.0934(0.0943) | Loss 1.2254(1.2256) | Error 0.0294(0.0290) Steps 446(444.94) | Grad Norm 0.5381(0.6049) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0186 | Time 16.7941, Epoch Time 274.8787(256.6623), Bit/dim 1.1705(best: 1.1658), Xent 0.0470, Loss 1.1940, Error 0.0159(best: 0.0139)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1303 | Time 34.1674(34.7317) | Bit/dim 1.1760(1.1784) | Xent 0.0893(0.0941) | Loss 1.2206(1.2254) | Error 0.0286(0.0290) Steps 446(444.97) | Grad Norm 0.9493(0.6152) | Total Time 10.00(10.00)\n",
      "Iter 1304 | Time 34.3756(34.7210) | Bit/dim 1.1782(1.1784) | Xent 0.1006(0.0943) | Loss 1.2285(1.2255) | Error 0.0300(0.0290) Steps 446(445.00) | Grad Norm 0.6152(0.6152) | Total Time 10.00(10.00)\n",
      "Iter 1305 | Time 35.3730(34.7406) | Bit/dim 1.1789(1.1784) | Xent 0.0835(0.0940) | Loss 1.2207(1.2254) | Error 0.0272(0.0289) Steps 446(445.03) | Grad Norm 0.4886(0.6114) | Total Time 10.00(10.00)\n",
      "Iter 1306 | Time 34.2295(34.7253) | Bit/dim 1.1749(1.1783) | Xent 0.0776(0.0935) | Loss 1.2137(1.2250) | Error 0.0252(0.0288) Steps 446(445.06) | Grad Norm 0.7311(0.6150) | Total Time 10.00(10.00)\n",
      "Iter 1307 | Time 35.2627(34.7414) | Bit/dim 1.1747(1.1782) | Xent 0.0971(0.0936) | Loss 1.2233(1.2250) | Error 0.0280(0.0288) Steps 446(445.09) | Grad Norm 0.4860(0.6111) | Total Time 10.00(10.00)\n",
      "Iter 1308 | Time 35.9354(34.7772) | Bit/dim 1.1768(1.1781) | Xent 0.0985(0.0938) | Loss 1.2261(1.2250) | Error 0.0299(0.0288) Steps 446(445.12) | Grad Norm 0.4136(0.6052) | Total Time 10.00(10.00)\n",
      "Iter 1309 | Time 35.4752(34.7981) | Bit/dim 1.1749(1.1780) | Xent 0.0870(0.0936) | Loss 1.2184(1.2248) | Error 0.0299(0.0289) Steps 446(445.14) | Grad Norm 0.8945(0.6139) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0187 | Time 16.5326, Epoch Time 273.9020(257.1795), Bit/dim 1.1708(best: 1.1658), Xent 0.0465, Loss 1.1940, Error 0.0168(best: 0.0139)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1310 | Time 35.5542(34.8208) | Bit/dim 1.1725(1.1779) | Xent 0.0852(0.0933) | Loss 1.2151(1.2245) | Error 0.0275(0.0288) Steps 446(445.17) | Grad Norm 0.3322(0.6054) | Total Time 10.00(10.00)\n",
      "Iter 1311 | Time 34.9838(34.8257) | Bit/dim 1.1737(1.1777) | Xent 0.1033(0.0936) | Loss 1.2253(1.2246) | Error 0.0315(0.0289) Steps 446(445.20) | Grad Norm 0.9609(0.6161) | Total Time 10.00(10.00)\n",
      "Iter 1312 | Time 35.8310(34.8559) | Bit/dim 1.1817(1.1779) | Xent 0.0885(0.0935) | Loss 1.2260(1.2246) | Error 0.0261(0.0288) Steps 446(445.22) | Grad Norm 0.8639(0.6235) | Total Time 10.00(10.00)\n",
      "Iter 1313 | Time 34.8077(34.8544) | Bit/dim 1.1762(1.1778) | Xent 0.0845(0.0932) | Loss 1.2185(1.2244) | Error 0.0255(0.0287) Steps 446(445.24) | Grad Norm 0.4197(0.6174) | Total Time 10.00(10.00)\n",
      "Iter 1314 | Time 35.5135(34.8742) | Bit/dim 1.1747(1.1777) | Xent 0.0948(0.0932) | Loss 1.2221(1.2243) | Error 0.0288(0.0287) Steps 446(445.27) | Grad Norm 0.7299(0.6208) | Total Time 10.00(10.00)\n",
      "Iter 1315 | Time 35.1400(34.8822) | Bit/dim 1.1789(1.1778) | Xent 0.0938(0.0933) | Loss 1.2258(1.2244) | Error 0.0286(0.0287) Steps 446(445.29) | Grad Norm 0.6310(0.6211) | Total Time 10.00(10.00)\n",
      "Iter 1316 | Time 34.9041(34.8828) | Bit/dim 1.1776(1.1778) | Xent 0.0957(0.0933) | Loss 1.2254(1.2244) | Error 0.0298(0.0288) Steps 446(445.31) | Grad Norm 0.4172(0.6150) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0188 | Time 16.8387, Epoch Time 276.0714(257.7462), Bit/dim 1.1686(best: 1.1658), Xent 0.0462, Loss 1.1917, Error 0.0148(best: 0.0139)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1317 | Time 34.8683(34.8824) | Bit/dim 1.1767(1.1777) | Xent 0.0920(0.0933) | Loss 1.2227(1.2244) | Error 0.0290(0.0288) Steps 446(445.33) | Grad Norm 0.5482(0.6130) | Total Time 10.00(10.00)\n",
      "Iter 1318 | Time 34.3620(34.8668) | Bit/dim 1.1731(1.1776) | Xent 0.0959(0.0934) | Loss 1.2211(1.2243) | Error 0.0298(0.0288) Steps 446(445.35) | Grad Norm 0.6133(0.6130) | Total Time 10.00(10.00)\n",
      "Iter 1319 | Time 35.0405(34.8720) | Bit/dim 1.1717(1.1774) | Xent 0.0845(0.0931) | Loss 1.2140(1.2240) | Error 0.0244(0.0287) Steps 446(445.37) | Grad Norm 0.6842(0.6151) | Total Time 10.00(10.00)\n",
      "Iter 1320 | Time 35.1969(34.8817) | Bit/dim 1.1783(1.1774) | Xent 0.1054(0.0935) | Loss 1.2311(1.2242) | Error 0.0320(0.0288) Steps 446(445.39) | Grad Norm 0.3334(0.6067) | Total Time 10.00(10.00)\n",
      "Iter 1321 | Time 34.9742(34.8845) | Bit/dim 1.1823(1.1776) | Xent 0.0854(0.0932) | Loss 1.2250(1.2242) | Error 0.0268(0.0287) Steps 446(445.41) | Grad Norm 0.7913(0.6122) | Total Time 10.00(10.00)\n",
      "Iter 1322 | Time 36.0395(34.9192) | Bit/dim 1.1761(1.1775) | Xent 0.0986(0.0934) | Loss 1.2254(1.2242) | Error 0.0292(0.0287) Steps 446(445.42) | Grad Norm 1.0942(0.6267) | Total Time 10.00(10.00)\n",
      "Iter 1323 | Time 35.2610(34.9294) | Bit/dim 1.1748(1.1775) | Xent 0.0960(0.0935) | Loss 1.2228(1.2242) | Error 0.0295(0.0287) Steps 446(445.44) | Grad Norm 0.9452(0.6362) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0189 | Time 16.7480, Epoch Time 275.1301(258.2678), Bit/dim 1.1688(best: 1.1658), Xent 0.0451, Loss 1.1913, Error 0.0163(best: 0.0139)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1324 | Time 35.2690(34.9396) | Bit/dim 1.1770(1.1774) | Xent 0.1003(0.0937) | Loss 1.2272(1.2243) | Error 0.0274(0.0287) Steps 446(445.46) | Grad Norm 0.5422(0.6334) | Total Time 10.00(10.00)\n",
      "Iter 1325 | Time 34.0486(34.9129) | Bit/dim 1.1781(1.1775) | Xent 0.0993(0.0938) | Loss 1.2277(1.2244) | Error 0.0309(0.0288) Steps 446(445.47) | Grad Norm 1.1813(0.6498) | Total Time 10.00(10.00)\n",
      "Iter 1326 | Time 34.9491(34.9140) | Bit/dim 1.1743(1.1774) | Xent 0.0857(0.0936) | Loss 1.2171(1.2242) | Error 0.0292(0.0288) Steps 446(445.49) | Grad Norm 1.2028(0.6664) | Total Time 10.00(10.00)\n",
      "Iter 1327 | Time 35.6549(34.9362) | Bit/dim 1.1760(1.1773) | Xent 0.1104(0.0941) | Loss 1.2312(1.2244) | Error 0.0339(0.0289) Steps 446(445.51) | Grad Norm 0.2981(0.6554) | Total Time 10.00(10.00)\n",
      "Iter 1328 | Time 34.6218(34.9268) | Bit/dim 1.1793(1.1774) | Xent 0.0988(0.0942) | Loss 1.2287(1.2245) | Error 0.0314(0.0290) Steps 446(445.52) | Grad Norm 0.9477(0.6641) | Total Time 10.00(10.00)\n",
      "Iter 1329 | Time 34.1498(34.9034) | Bit/dim 1.1750(1.1773) | Xent 0.0997(0.0944) | Loss 1.2249(1.2245) | Error 0.0301(0.0290) Steps 446(445.54) | Grad Norm 1.0559(0.6759) | Total Time 10.00(10.00)\n",
      "Iter 1330 | Time 35.2318(34.9133) | Bit/dim 1.1738(1.1772) | Xent 0.0844(0.0941) | Loss 1.2160(1.2243) | Error 0.0274(0.0290) Steps 446(445.55) | Grad Norm 0.3563(0.6663) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0190 | Time 16.7709, Epoch Time 273.1844(258.7153), Bit/dim 1.1687(best: 1.1658), Xent 0.0478, Loss 1.1925, Error 0.0167(best: 0.0139)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1331 | Time 35.0414(34.9171) | Bit/dim 1.1732(1.1771) | Xent 0.0896(0.0940) | Loss 1.2180(1.2241) | Error 0.0274(0.0289) Steps 446(445.56) | Grad Norm 1.0428(0.6776) | Total Time 10.00(10.00)\n",
      "Iter 1332 | Time 34.4443(34.9030) | Bit/dim 1.1757(1.1771) | Xent 0.0932(0.0940) | Loss 1.2223(1.2240) | Error 0.0281(0.0289) Steps 446(445.58) | Grad Norm 1.7457(0.7096) | Total Time 10.00(10.00)\n",
      "Iter 1333 | Time 37.1606(34.9707) | Bit/dim 1.1726(1.1769) | Xent 0.0966(0.0940) | Loss 1.2209(1.2239) | Error 0.0279(0.0289) Steps 446(445.59) | Grad Norm 1.5577(0.7351) | Total Time 10.00(10.00)\n",
      "Iter 1334 | Time 35.9206(34.9992) | Bit/dim 1.1762(1.1769) | Xent 0.1016(0.0943) | Loss 1.2270(1.2240) | Error 0.0300(0.0289) Steps 446(445.60) | Grad Norm 0.5651(0.7300) | Total Time 10.00(10.00)\n",
      "Iter 1335 | Time 36.2666(35.0372) | Bit/dim 1.1809(1.1770) | Xent 0.0938(0.0942) | Loss 1.2279(1.2241) | Error 0.0305(0.0290) Steps 446(445.61) | Grad Norm 1.1446(0.7424) | Total Time 10.00(10.00)\n",
      "Iter 1336 | Time 35.3356(35.0462) | Bit/dim 1.1770(1.1770) | Xent 0.0966(0.0943) | Loss 1.2253(1.2242) | Error 0.0290(0.0290) Steps 452(445.80) | Grad Norm 1.7583(0.7729) | Total Time 10.00(10.00)\n",
      "Iter 1337 | Time 35.9215(35.0724) | Bit/dim 1.1763(1.1770) | Xent 0.0872(0.0941) | Loss 1.2199(1.2240) | Error 0.0260(0.0289) Steps 452(445.99) | Grad Norm 1.0286(0.7806) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0191 | Time 16.9713, Epoch Time 280.2648(259.3617), Bit/dim 1.1697(best: 1.1658), Xent 0.0461, Loss 1.1928, Error 0.0153(best: 0.0139)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1338 | Time 34.9256(35.0680) | Bit/dim 1.1736(1.1769) | Xent 0.0801(0.0937) | Loss 1.2136(1.2237) | Error 0.0260(0.0288) Steps 452(446.17) | Grad Norm 0.8532(0.7827) | Total Time 10.00(10.00)\n",
      "Iter 1339 | Time 34.4856(35.0505) | Bit/dim 1.1787(1.1769) | Xent 0.0869(0.0935) | Loss 1.2221(1.2237) | Error 0.0266(0.0287) Steps 452(446.35) | Grad Norm 1.9939(0.8191) | Total Time 10.00(10.00)\n",
      "Iter 1340 | Time 35.5988(35.0670) | Bit/dim 1.1802(1.1770) | Xent 0.0880(0.0933) | Loss 1.2243(1.2237) | Error 0.0276(0.0287) Steps 452(446.52) | Grad Norm 1.9880(0.8541) | Total Time 10.00(10.00)\n",
      "Iter 1341 | Time 35.5509(35.0815) | Bit/dim 1.1764(1.1770) | Xent 0.1039(0.0936) | Loss 1.2284(1.2238) | Error 0.0309(0.0288) Steps 452(446.68) | Grad Norm 0.9772(0.8578) | Total Time 10.00(10.00)\n",
      "Iter 1342 | Time 35.7163(35.1005) | Bit/dim 1.1766(1.1770) | Xent 0.0928(0.0936) | Loss 1.2230(1.2238) | Error 0.0272(0.0287) Steps 452(446.84) | Grad Norm 0.4936(0.8469) | Total Time 10.00(10.00)\n",
      "Iter 1343 | Time 35.3640(35.1085) | Bit/dim 1.1773(1.1770) | Xent 0.0850(0.0934) | Loss 1.2199(1.2237) | Error 0.0284(0.0287) Steps 452(446.99) | Grad Norm 1.8056(0.8757) | Total Time 10.00(10.00)\n",
      "Iter 1344 | Time 35.2063(35.1114) | Bit/dim 1.1748(1.1770) | Xent 0.1008(0.0936) | Loss 1.2252(1.2237) | Error 0.0298(0.0287) Steps 452(447.14) | Grad Norm 2.1867(0.9150) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0192 | Time 16.8056, Epoch Time 276.1484(259.8653), Bit/dim 1.1703(best: 1.1658), Xent 0.0451, Loss 1.1929, Error 0.0144(best: 0.0139)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1345 | Time 35.7702(35.1312) | Bit/dim 1.1803(1.1771) | Xent 0.0926(0.0935) | Loss 1.2266(1.2238) | Error 0.0301(0.0288) Steps 452(447.29) | Grad Norm 1.4910(0.9323) | Total Time 10.00(10.00)\n",
      "Iter 1346 | Time 34.9779(35.1266) | Bit/dim 1.1826(1.1772) | Xent 0.0966(0.0936) | Loss 1.2309(1.2240) | Error 0.0292(0.0288) Steps 452(447.43) | Grad Norm 0.3432(0.9146) | Total Time 10.00(10.00)\n",
      "Iter 1347 | Time 34.7312(35.1147) | Bit/dim 1.1740(1.1771) | Xent 0.0949(0.0937) | Loss 1.2215(1.2240) | Error 0.0288(0.0288) Steps 452(447.57) | Grad Norm 1.1278(0.9210) | Total Time 10.00(10.00)\n",
      "Iter 1348 | Time 36.3081(35.1505) | Bit/dim 1.1781(1.1772) | Xent 0.0910(0.0936) | Loss 1.2236(1.2240) | Error 0.0285(0.0288) Steps 452(447.70) | Grad Norm 0.9365(0.9215) | Total Time 10.00(10.00)\n",
      "Iter 1349 | Time 35.8286(35.1708) | Bit/dim 1.1745(1.1771) | Xent 0.0931(0.0936) | Loss 1.2210(1.2239) | Error 0.0282(0.0288) Steps 458(448.01) | Grad Norm 0.4378(0.9070) | Total Time 10.00(10.00)\n",
      "Iter 1350 | Time 35.7224(35.1874) | Bit/dim 1.1731(1.1770) | Xent 0.1015(0.0938) | Loss 1.2238(1.2239) | Error 0.0319(0.0289) Steps 458(448.31) | Grad Norm 0.8180(0.9043) | Total Time 10.00(10.00)\n",
      "Iter 1351 | Time 35.7507(35.2043) | Bit/dim 1.1744(1.1769) | Xent 0.0843(0.0935) | Loss 1.2166(1.2236) | Error 0.0264(0.0288) Steps 452(448.42) | Grad Norm 1.0100(0.9075) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0193 | Time 17.3087, Epoch Time 279.2263(260.4462), Bit/dim 1.1707(best: 1.1658), Xent 0.0454, Loss 1.1934, Error 0.0146(best: 0.0139)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1352 | Time 36.3385(35.2383) | Bit/dim 1.1727(1.1768) | Xent 0.0939(0.0935) | Loss 1.2197(1.2235) | Error 0.0311(0.0289) Steps 452(448.53) | Grad Norm 0.8835(0.9067) | Total Time 10.00(10.00)\n",
      "Iter 1353 | Time 35.6733(35.2514) | Bit/dim 1.1718(1.1766) | Xent 0.0914(0.0935) | Loss 1.2175(1.2233) | Error 0.0275(0.0288) Steps 452(448.63) | Grad Norm 0.8501(0.9050) | Total Time 10.00(10.00)\n",
      "Iter 1354 | Time 35.8716(35.2700) | Bit/dim 1.1802(1.1767) | Xent 0.0963(0.0936) | Loss 1.2284(1.2235) | Error 0.0288(0.0288) Steps 458(448.91) | Grad Norm 0.7127(0.8993) | Total Time 10.00(10.00)\n",
      "Iter 1355 | Time 35.2992(35.2708) | Bit/dim 1.1765(1.1767) | Xent 0.0905(0.0935) | Loss 1.2217(1.2234) | Error 0.0295(0.0288) Steps 458(449.19) | Grad Norm 0.6890(0.8930) | Total Time 10.00(10.00)\n",
      "Iter 1356 | Time 37.0025(35.3228) | Bit/dim 1.1775(1.1767) | Xent 0.0892(0.0933) | Loss 1.2221(1.2234) | Error 0.0280(0.0288) Steps 458(449.45) | Grad Norm 0.7908(0.8899) | Total Time 10.00(10.00)\n",
      "Iter 1357 | Time 36.0423(35.3444) | Bit/dim 1.1768(1.1767) | Xent 0.0930(0.0933) | Loss 1.2233(1.2234) | Error 0.0279(0.0288) Steps 458(449.71) | Grad Norm 0.5731(0.8804) | Total Time 10.00(10.00)\n",
      "Iter 1358 | Time 35.4163(35.3465) | Bit/dim 1.1823(1.1769) | Xent 0.0888(0.0932) | Loss 1.2266(1.2235) | Error 0.0264(0.0287) Steps 458(449.96) | Grad Norm 0.3488(0.8645) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0194 | Time 17.3989, Epoch Time 281.7414(261.0850), Bit/dim 1.1716(best: 1.1658), Xent 0.0454, Loss 1.1943, Error 0.0156(best: 0.0139)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1359 | Time 36.1420(35.3704) | Bit/dim 1.1784(1.1769) | Xent 0.0838(0.0929) | Loss 1.2203(1.2234) | Error 0.0258(0.0286) Steps 458(450.20) | Grad Norm 0.5368(0.8546) | Total Time 10.00(10.00)\n",
      "Iter 1360 | Time 36.0418(35.3905) | Bit/dim 1.1818(1.1771) | Xent 0.0924(0.0929) | Loss 1.2280(1.2235) | Error 0.0302(0.0287) Steps 458(450.43) | Grad Norm 1.0246(0.8597) | Total Time 10.00(10.00)\n",
      "Iter 1361 | Time 35.7552(35.4015) | Bit/dim 1.1751(1.1770) | Xent 0.1001(0.0931) | Loss 1.2252(1.2236) | Error 0.0296(0.0287) Steps 458(450.66) | Grad Norm 0.8419(0.8592) | Total Time 10.00(10.00)\n",
      "Iter 1362 | Time 35.9860(35.4190) | Bit/dim 1.1767(1.1770) | Xent 0.0883(0.0930) | Loss 1.2209(1.2235) | Error 0.0272(0.0287) Steps 458(450.88) | Grad Norm 0.2883(0.8421) | Total Time 10.00(10.00)\n",
      "Iter 1363 | Time 36.4919(35.4512) | Bit/dim 1.1803(1.1771) | Xent 0.0911(0.0929) | Loss 1.2258(1.2236) | Error 0.0285(0.0286) Steps 458(451.09) | Grad Norm 1.4532(0.8604) | Total Time 10.00(10.00)\n",
      "Iter 1364 | Time 35.6736(35.4579) | Bit/dim 1.1773(1.1771) | Xent 0.0891(0.0928) | Loss 1.2218(1.2235) | Error 0.0264(0.0286) Steps 458(451.30) | Grad Norm 2.4655(0.9085) | Total Time 10.00(10.00)\n",
      "Iter 1365 | Time 39.6548(35.5838) | Bit/dim 1.1775(1.1771) | Xent 0.0795(0.0924) | Loss 1.2173(1.2233) | Error 0.0258(0.0285) Steps 470(451.86) | Grad Norm 2.6574(0.9610) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0195 | Time 17.4346, Epoch Time 286.0522(261.8340), Bit/dim 1.1735(best: 1.1658), Xent 0.0434, Loss 1.1952, Error 0.0142(best: 0.0139)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1366 | Time 35.7977(35.5902) | Bit/dim 1.1795(1.1772) | Xent 0.0905(0.0923) | Loss 1.2247(1.2234) | Error 0.0259(0.0284) Steps 458(452.04) | Grad Norm 1.5950(0.9800) | Total Time 10.00(10.00)\n",
      "Iter 1367 | Time 36.3846(35.6140) | Bit/dim 1.1763(1.1772) | Xent 0.0849(0.0921) | Loss 1.2187(1.2232) | Error 0.0262(0.0284) Steps 458(452.22) | Grad Norm 0.7181(0.9722) | Total Time 10.00(10.00)\n",
      "Iter 1368 | Time 37.7148(35.6771) | Bit/dim 1.1762(1.1771) | Xent 0.0927(0.0921) | Loss 1.2225(1.2232) | Error 0.0299(0.0284) Steps 458(452.40) | Grad Norm 2.4742(1.0172) | Total Time 10.00(10.00)\n",
      "Iter 1369 | Time 38.4522(35.7603) | Bit/dim 1.1824(1.1773) | Xent 0.1012(0.0924) | Loss 1.2330(1.2235) | Error 0.0305(0.0285) Steps 464(452.74) | Grad Norm 3.7572(1.0994) | Total Time 10.00(10.00)\n",
      "Iter 1370 | Time 40.1339(35.8915) | Bit/dim 1.1816(1.1774) | Xent 0.0937(0.0925) | Loss 1.2285(1.2237) | Error 0.0302(0.0285) Steps 470(453.26) | Grad Norm 4.1328(1.1904) | Total Time 10.00(10.00)\n",
      "Iter 1371 | Time 37.8304(35.9497) | Bit/dim 1.1824(1.1776) | Xent 0.0869(0.0923) | Loss 1.2259(1.2237) | Error 0.0280(0.0285) Steps 464(453.58) | Grad Norm 2.7777(1.2381) | Total Time 10.00(10.00)\n",
      "Iter 1372 | Time 37.2095(35.9875) | Bit/dim 1.1801(1.1777) | Xent 0.0892(0.0922) | Loss 1.2247(1.2238) | Error 0.0262(0.0284) Steps 458(453.72) | Grad Norm 0.7092(1.2222) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0196 | Time 17.3730, Epoch Time 293.6954(262.7899), Bit/dim 1.1740(best: 1.1658), Xent 0.0438, Loss 1.1959, Error 0.0156(best: 0.0139)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1373 | Time 35.5039(35.9730) | Bit/dim 1.1825(1.1778) | Xent 0.0839(0.0919) | Loss 1.2244(1.2238) | Error 0.0261(0.0284) Steps 458(453.85) | Grad Norm 1.4568(1.2292) | Total Time 10.00(10.00)\n",
      "Iter 1374 | Time 36.6248(35.9925) | Bit/dim 1.1820(1.1779) | Xent 0.0881(0.0918) | Loss 1.2261(1.2238) | Error 0.0251(0.0283) Steps 464(454.15) | Grad Norm 2.5335(1.2684) | Total Time 10.00(10.00)\n",
      "Iter 1375 | Time 37.2314(36.0297) | Bit/dim 1.1797(1.1780) | Xent 0.0846(0.0916) | Loss 1.2220(1.2238) | Error 0.0268(0.0282) Steps 470(454.63) | Grad Norm 2.3733(1.3015) | Total Time 10.00(10.00)\n",
      "Iter 1376 | Time 37.7407(36.0810) | Bit/dim 1.1789(1.1780) | Xent 0.0874(0.0915) | Loss 1.2226(1.2238) | Error 0.0251(0.0281) Steps 464(454.91) | Grad Norm 0.7582(1.2852) | Total Time 10.00(10.00)\n",
      "Iter 1377 | Time 37.2596(36.1164) | Bit/dim 1.1704(1.1778) | Xent 0.1068(0.0919) | Loss 1.2237(1.2238) | Error 0.0325(0.0283) Steps 464(455.18) | Grad Norm 1.4724(1.2908) | Total Time 10.00(10.00)\n",
      "Iter 1378 | Time 36.7964(36.1368) | Bit/dim 1.1796(1.1778) | Xent 0.0962(0.0921) | Loss 1.2277(1.2239) | Error 0.0295(0.0283) Steps 470(455.62) | Grad Norm 2.6975(1.3330) | Total Time 10.00(10.00)\n",
      "Iter 1379 | Time 38.3194(36.2023) | Bit/dim 1.1820(1.1780) | Xent 0.0925(0.0921) | Loss 1.2283(1.2240) | Error 0.0291(0.0283) Steps 464(455.88) | Grad Norm 2.9189(1.3806) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0197 | Time 17.3576, Epoch Time 289.3615(263.5870), Bit/dim 1.1740(best: 1.1658), Xent 0.0441, Loss 1.1961, Error 0.0149(best: 0.0139)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1380 | Time 37.7472(36.2486) | Bit/dim 1.1815(1.1781) | Xent 0.0921(0.0921) | Loss 1.2276(1.2241) | Error 0.0286(0.0283) Steps 458(455.94) | Grad Norm 2.2653(1.4071) | Total Time 10.00(10.00)\n",
      "Iter 1381 | Time 37.1225(36.2748) | Bit/dim 1.1763(1.1780) | Xent 0.0907(0.0920) | Loss 1.2216(1.2240) | Error 0.0280(0.0283) Steps 464(456.18) | Grad Norm 0.3616(1.3758) | Total Time 10.00(10.00)\n",
      "Iter 1382 | Time 37.6661(36.3166) | Bit/dim 1.1756(1.1779) | Xent 0.0979(0.0922) | Loss 1.2246(1.2241) | Error 0.0320(0.0284) Steps 464(456.42) | Grad Norm 2.4209(1.4071) | Total Time 10.00(10.00)\n",
      "Iter 1383 | Time 37.4678(36.3511) | Bit/dim 1.1837(1.1781) | Xent 0.0854(0.0920) | Loss 1.2264(1.2241) | Error 0.0269(0.0284) Steps 470(456.82) | Grad Norm 4.2337(1.4919) | Total Time 10.00(10.00)\n",
      "Iter 1384 | Time 37.4465(36.3840) | Bit/dim 1.1795(1.1782) | Xent 0.0908(0.0920) | Loss 1.2249(1.2241) | Error 0.0266(0.0283) Steps 464(457.04) | Grad Norm 4.2098(1.5735) | Total Time 10.00(10.00)\n",
      "Iter 1385 | Time 38.4548(36.4461) | Bit/dim 1.1828(1.1783) | Xent 0.1018(0.0923) | Loss 1.2336(1.2244) | Error 0.0308(0.0284) Steps 470(457.43) | Grad Norm 2.6446(1.6056) | Total Time 10.00(10.00)\n",
      "Iter 1386 | Time 37.6130(36.4811) | Bit/dim 1.1747(1.1782) | Xent 0.0822(0.0920) | Loss 1.2158(1.2242) | Error 0.0252(0.0283) Steps 464(457.62) | Grad Norm 0.3970(1.5693) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0198 | Time 17.1611, Epoch Time 293.2082(264.4757), Bit/dim 1.1718(best: 1.1658), Xent 0.0452, Loss 1.1944, Error 0.0150(best: 0.0139)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1387 | Time 36.5022(36.4817) | Bit/dim 1.1760(1.1781) | Xent 0.0905(0.0919) | Loss 1.2213(1.2241) | Error 0.0275(0.0283) Steps 464(457.82) | Grad Norm 3.8295(1.6371) | Total Time 10.00(10.00)\n",
      "Iter 1388 | Time 37.8698(36.5234) | Bit/dim 1.1839(1.1783) | Xent 0.0973(0.0921) | Loss 1.2325(1.2243) | Error 0.0300(0.0283) Steps 470(458.18) | Grad Norm 6.1370(1.7721) | Total Time 10.00(10.00)\n",
      "Iter 1389 | Time 36.6730(36.5279) | Bit/dim 1.1845(1.1785) | Xent 0.0903(0.0920) | Loss 1.2297(1.2245) | Error 0.0294(0.0284) Steps 464(458.36) | Grad Norm 5.0112(1.8693) | Total Time 10.00(10.00)\n",
      "Iter 1390 | Time 36.7380(36.5342) | Bit/dim 1.1761(1.1784) | Xent 0.0891(0.0919) | Loss 1.2207(1.2244) | Error 0.0280(0.0284) Steps 464(458.53) | Grad Norm 2.2382(1.8804) | Total Time 10.00(10.00)\n",
      "Iter 1391 | Time 37.2998(36.5571) | Bit/dim 1.1838(1.1786) | Xent 0.0863(0.0918) | Loss 1.2269(1.2245) | Error 0.0254(0.0283) Steps 464(458.69) | Grad Norm 0.9425(1.8522) | Total Time 10.00(10.00)\n",
      "Iter 1392 | Time 36.6880(36.5611) | Bit/dim 1.1737(1.1784) | Xent 0.1085(0.0923) | Loss 1.2280(1.2246) | Error 0.0330(0.0284) Steps 464(458.85) | Grad Norm 3.9454(1.9150) | Total Time 10.00(10.00)\n",
      "Iter 1393 | Time 37.9645(36.6032) | Bit/dim 1.1849(1.1786) | Xent 0.0802(0.0919) | Loss 1.2249(1.2246) | Error 0.0251(0.0283) Steps 470(459.18) | Grad Norm 5.3852(2.0191) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0199 | Time 17.2920, Epoch Time 289.8604(265.2372), Bit/dim 1.1714(best: 1.1658), Xent 0.0460, Loss 1.1944, Error 0.0170(best: 0.0139)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1394 | Time 38.2362(36.6521) | Bit/dim 1.1779(1.1786) | Xent 0.0911(0.0919) | Loss 1.2235(1.2245) | Error 0.0274(0.0283) Steps 464(459.33) | Grad Norm 4.0107(2.0789) | Total Time 10.00(10.00)\n",
      "Iter 1395 | Time 37.7904(36.6863) | Bit/dim 1.1787(1.1786) | Xent 0.0946(0.0920) | Loss 1.2260(1.2246) | Error 0.0298(0.0283) Steps 464(459.47) | Grad Norm 1.8771(2.0728) | Total Time 10.00(10.00)\n",
      "Iter 1396 | Time 37.1969(36.7016) | Bit/dim 1.1700(1.1783) | Xent 0.0918(0.0920) | Loss 1.2159(1.2243) | Error 0.0280(0.0283) Steps 464(459.60) | Grad Norm 0.6323(2.0296) | Total Time 10.00(10.00)\n",
      "Iter 1397 | Time 37.0697(36.7127) | Bit/dim 1.1782(1.1783) | Xent 0.0867(0.0918) | Loss 1.2216(1.2242) | Error 0.0266(0.0283) Steps 470(459.92) | Grad Norm 2.5421(2.0450) | Total Time 10.00(10.00)\n",
      "Iter 1398 | Time 38.0534(36.7529) | Bit/dim 1.1801(1.1784) | Xent 0.0875(0.0917) | Loss 1.2238(1.2242) | Error 0.0288(0.0283) Steps 476(460.40) | Grad Norm 3.2887(2.0823) | Total Time 10.00(10.00)\n",
      "Iter 1399 | Time 37.4067(36.7725) | Bit/dim 1.1821(1.1785) | Xent 0.0939(0.0917) | Loss 1.2290(1.2244) | Error 0.0279(0.0283) Steps 464(460.51) | Grad Norm 2.6737(2.1000) | Total Time 10.00(10.00)\n",
      "Iter 1400 | Time 38.0861(36.8119) | Bit/dim 1.1760(1.1784) | Xent 0.0973(0.0919) | Loss 1.2246(1.2244) | Error 0.0285(0.0283) Steps 470(460.79) | Grad Norm 1.6546(2.0867) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0200 | Time 17.5497, Epoch Time 294.1263(266.1039), Bit/dim 1.1701(best: 1.1658), Xent 0.0459, Loss 1.1931, Error 0.0152(best: 0.0139)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1401 | Time 38.3469(36.8580) | Bit/dim 1.1785(1.1784) | Xent 0.0984(0.0921) | Loss 1.2277(1.2245) | Error 0.0312(0.0284) Steps 464(460.89) | Grad Norm 0.7256(2.0458) | Total Time 10.00(10.00)\n",
      "Iter 1402 | Time 36.8569(36.8579) | Bit/dim 1.1785(1.1784) | Xent 0.0976(0.0923) | Loss 1.2273(1.2246) | Error 0.0314(0.0285) Steps 464(460.98) | Grad Norm 0.6354(2.0035) | Total Time 10.00(10.00)\n",
      "Iter 1403 | Time 37.3492(36.8727) | Bit/dim 1.1775(1.1784) | Xent 0.0872(0.0921) | Loss 1.2211(1.2245) | Error 0.0274(0.0284) Steps 464(461.07) | Grad Norm 1.1237(1.9771) | Total Time 10.00(10.00)\n",
      "Iter 1404 | Time 36.4956(36.8613) | Bit/dim 1.1777(1.1784) | Xent 0.0847(0.0919) | Loss 1.2201(1.2243) | Error 0.0266(0.0284) Steps 464(461.16) | Grad Norm 1.1313(1.9518) | Total Time 10.00(10.00)\n",
      "Iter 1405 | Time 38.0926(36.8983) | Bit/dim 1.1774(1.1784) | Xent 0.0849(0.0917) | Loss 1.2198(1.2242) | Error 0.0255(0.0283) Steps 464(461.24) | Grad Norm 0.9234(1.9209) | Total Time 10.00(10.00)\n",
      "Iter 1406 | Time 38.6867(36.9519) | Bit/dim 1.1698(1.1781) | Xent 0.0920(0.0917) | Loss 1.2157(1.2239) | Error 0.0288(0.0283) Steps 464(461.33) | Grad Norm 0.7865(1.8869) | Total Time 10.00(10.00)\n",
      "Iter 1407 | Time 37.2564(36.9611) | Bit/dim 1.1760(1.1780) | Xent 0.0770(0.0913) | Loss 1.2145(1.2237) | Error 0.0240(0.0282) Steps 464(461.41) | Grad Norm 1.0749(1.8625) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0201 | Time 17.3058, Epoch Time 292.9617(266.9096), Bit/dim 1.1698(best: 1.1658), Xent 0.0456, Loss 1.1926, Error 0.0150(best: 0.0139)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1408 | Time 36.9006(36.9593) | Bit/dim 1.1731(1.1779) | Xent 0.0881(0.0912) | Loss 1.2172(1.2235) | Error 0.0282(0.0282) Steps 464(461.49) | Grad Norm 1.6091(1.8549) | Total Time 10.00(10.00)\n",
      "Iter 1409 | Time 36.5623(36.9474) | Bit/dim 1.1781(1.1779) | Xent 0.0992(0.0914) | Loss 1.2277(1.2236) | Error 0.0288(0.0282) Steps 470(461.74) | Grad Norm 1.9934(1.8591) | Total Time 10.00(10.00)\n",
      "Iter 1410 | Time 37.2810(36.9574) | Bit/dim 1.1737(1.1778) | Xent 0.0905(0.0914) | Loss 1.2190(1.2235) | Error 0.0275(0.0282) Steps 464(461.81) | Grad Norm 2.0430(1.8646) | Total Time 10.00(10.00)\n",
      "Iter 1411 | Time 37.9487(36.9871) | Bit/dim 1.1805(1.1779) | Xent 0.0836(0.0911) | Loss 1.2223(1.2234) | Error 0.0280(0.0282) Steps 470(462.05) | Grad Norm 1.6960(1.8595) | Total Time 10.00(10.00)\n",
      "Iter 1412 | Time 36.4182(36.9700) | Bit/dim 1.1750(1.1778) | Xent 0.0945(0.0912) | Loss 1.2223(1.2234) | Error 0.0298(0.0282) Steps 464(462.11) | Grad Norm 1.0272(1.8346) | Total Time 10.00(10.00)\n",
      "Iter 1413 | Time 36.9003(36.9679) | Bit/dim 1.1818(1.1779) | Xent 0.0998(0.0915) | Loss 1.2317(1.2236) | Error 0.0312(0.0283) Steps 464(462.17) | Grad Norm 0.4229(1.7922) | Total Time 10.00(10.00)\n",
      "Iter 1414 | Time 38.6041(37.0170) | Bit/dim 1.1740(1.1778) | Xent 0.0878(0.0914) | Loss 1.2178(1.2235) | Error 0.0269(0.0283) Steps 464(462.22) | Grad Norm 0.5650(1.7554) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0202 | Time 17.1890, Epoch Time 290.3430(267.6126), Bit/dim 1.1702(best: 1.1658), Xent 0.0447, Loss 1.1926, Error 0.0142(best: 0.0139)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1415 | Time 36.7507(37.0090) | Bit/dim 1.1783(1.1778) | Xent 0.0871(0.0913) | Loss 1.2219(1.2234) | Error 0.0282(0.0283) Steps 464(462.28) | Grad Norm 1.1764(1.7380) | Total Time 10.00(10.00)\n",
      "Iter 1416 | Time 37.2652(37.0167) | Bit/dim 1.1781(1.1778) | Xent 0.0939(0.0913) | Loss 1.2251(1.2235) | Error 0.0302(0.0283) Steps 464(462.33) | Grad Norm 1.5242(1.7316) | Total Time 10.00(10.00)\n",
      "Iter 1417 | Time 36.9379(37.0144) | Bit/dim 1.1738(1.1777) | Xent 0.0934(0.0914) | Loss 1.2204(1.2234) | Error 0.0298(0.0284) Steps 464(462.38) | Grad Norm 1.7190(1.7312) | Total Time 10.00(10.00)\n",
      "Iter 1418 | Time 37.6948(37.0348) | Bit/dim 1.1779(1.1777) | Xent 0.0908(0.0914) | Loss 1.2233(1.2234) | Error 0.0286(0.0284) Steps 470(462.61) | Grad Norm 2.2143(1.7457) | Total Time 10.00(10.00)\n",
      "Iter 1419 | Time 36.0339(37.0047) | Bit/dim 1.1764(1.1776) | Xent 0.0943(0.0915) | Loss 1.2235(1.2234) | Error 0.0296(0.0284) Steps 464(462.65) | Grad Norm 2.9520(1.7819) | Total Time 10.00(10.00)\n",
      "Iter 1420 | Time 37.8768(37.0309) | Bit/dim 1.1829(1.1778) | Xent 0.0757(0.0910) | Loss 1.2208(1.2233) | Error 0.0221(0.0282) Steps 476(463.05) | Grad Norm 3.9774(1.8478) | Total Time 10.00(10.00)\n",
      "Iter 1421 | Time 38.5668(37.0770) | Bit/dim 1.1780(1.1778) | Xent 0.0928(0.0910) | Loss 1.2244(1.2233) | Error 0.0284(0.0282) Steps 464(463.08) | Grad Norm 5.4711(1.9565) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0203 | Time 17.1074, Epoch Time 290.8113(268.3086), Bit/dim 1.1794(best: 1.1658), Xent 0.0440, Loss 1.2014, Error 0.0148(best: 0.0139)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1422 | Time 39.4310(37.1476) | Bit/dim 1.1847(1.1780) | Xent 0.0875(0.0909) | Loss 1.2285(1.2235) | Error 0.0265(0.0282) Steps 470(463.29) | Grad Norm 6.5956(2.0957) | Total Time 10.00(10.00)\n",
      "Iter 1423 | Time 37.1430(37.1475) | Bit/dim 1.1769(1.1780) | Xent 0.0936(0.0910) | Loss 1.2237(1.2235) | Error 0.0278(0.0282) Steps 464(463.31) | Grad Norm 6.2308(2.2197) | Total Time 10.00(10.00)\n",
      "Iter 1424 | Time 38.4675(37.1871) | Bit/dim 1.1818(1.1781) | Xent 0.0878(0.0909) | Loss 1.2257(1.2236) | Error 0.0284(0.0282) Steps 476(463.69) | Grad Norm 5.0587(2.3049) | Total Time 10.00(10.00)\n",
      "Iter 1425 | Time 37.0231(37.1821) | Bit/dim 1.1764(1.1780) | Xent 0.1074(0.0914) | Loss 1.2301(1.2238) | Error 0.0321(0.0283) Steps 464(463.70) | Grad Norm 2.8837(2.3222) | Total Time 10.00(10.00)\n",
      "Iter 1426 | Time 36.9582(37.1754) | Bit/dim 1.1796(1.1781) | Xent 0.0915(0.0914) | Loss 1.2254(1.2238) | Error 0.0265(0.0282) Steps 458(463.53) | Grad Norm 0.3671(2.2636) | Total Time 10.00(10.00)\n",
      "Iter 1427 | Time 39.0995(37.2332) | Bit/dim 1.1819(1.1782) | Xent 0.1051(0.0918) | Loss 1.2345(1.2241) | Error 0.0306(0.0283) Steps 470(463.72) | Grad Norm 1.6169(2.2442) | Total Time 10.00(10.00)\n",
      "Iter 1428 | Time 37.9455(37.2545) | Bit/dim 1.1817(1.1783) | Xent 0.0843(0.0916) | Loss 1.2239(1.2241) | Error 0.0252(0.0282) Steps 476(464.09) | Grad Norm 2.3999(2.2489) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0204 | Time 17.0362, Epoch Time 295.5209(269.1250), Bit/dim 1.1731(best: 1.1658), Xent 0.0461, Loss 1.1961, Error 0.0155(best: 0.0139)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1429 | Time 38.3966(37.2888) | Bit/dim 1.1760(1.1782) | Xent 0.0896(0.0915) | Loss 1.2208(1.2240) | Error 0.0284(0.0282) Steps 470(464.27) | Grad Norm 2.7095(2.2627) | Total Time 10.00(10.00)\n",
      "Iter 1430 | Time 40.1015(37.3732) | Bit/dim 1.1808(1.1783) | Xent 0.0905(0.0915) | Loss 1.2260(1.2241) | Error 0.0282(0.0282) Steps 470(464.44) | Grad Norm 2.9002(2.2818) | Total Time 10.00(10.00)\n",
      "Iter 1431 | Time 40.2047(37.4581) | Bit/dim 1.1798(1.1784) | Xent 0.0899(0.0915) | Loss 1.2248(1.2241) | Error 0.0281(0.0282) Steps 476(464.79) | Grad Norm 2.9361(2.3014) | Total Time 10.00(10.00)\n",
      "Iter 1432 | Time 37.7380(37.4665) | Bit/dim 1.1770(1.1783) | Xent 0.0935(0.0915) | Loss 1.2237(1.2241) | Error 0.0279(0.0282) Steps 470(464.94) | Grad Norm 2.4265(2.3052) | Total Time 10.00(10.00)\n",
      "Iter 1433 | Time 39.0474(37.5139) | Bit/dim 1.1784(1.1783) | Xent 0.0853(0.0913) | Loss 1.2210(1.2240) | Error 0.0260(0.0281) Steps 464(464.91) | Grad Norm 2.0364(2.2971) | Total Time 10.00(10.00)\n",
      "Iter 1434 | Time 38.0564(37.5302) | Bit/dim 1.1780(1.1783) | Xent 0.0935(0.0914) | Loss 1.2248(1.2240) | Error 0.0284(0.0281) Steps 470(465.07) | Grad Norm 2.2742(2.2964) | Total Time 10.00(10.00)\n",
      "Iter 1435 | Time 40.0743(37.6065) | Bit/dim 1.1778(1.1783) | Xent 0.0935(0.0915) | Loss 1.2246(1.2240) | Error 0.0274(0.0281) Steps 476(465.39) | Grad Norm 3.1267(2.3213) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0205 | Time 17.1301, Epoch Time 303.2652(270.1492), Bit/dim 1.1716(best: 1.1658), Xent 0.0464, Loss 1.1948, Error 0.0155(best: 0.0139)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1436 | Time 38.1822(37.6238) | Bit/dim 1.1772(1.1783) | Xent 0.0871(0.0913) | Loss 1.2208(1.2239) | Error 0.0261(0.0281) Steps 470(465.53) | Grad Norm 4.3446(2.3820) | Total Time 10.00(10.00)\n",
      "Iter 1437 | Time 38.7697(37.6582) | Bit/dim 1.1843(1.1785) | Xent 0.0849(0.0911) | Loss 1.2268(1.2240) | Error 0.0264(0.0280) Steps 476(465.85) | Grad Norm 5.2863(2.4692) | Total Time 10.00(10.00)\n",
      "Iter 1438 | Time 38.0360(37.6695) | Bit/dim 1.1762(1.1784) | Xent 0.0988(0.0914) | Loss 1.2256(1.2241) | Error 0.0308(0.0281) Steps 470(465.97) | Grad Norm 6.0513(2.5766) | Total Time 10.00(10.00)\n",
      "Iter 1439 | Time 40.0587(37.7412) | Bit/dim 1.1843(1.1786) | Xent 0.0827(0.0911) | Loss 1.2256(1.2241) | Error 0.0239(0.0280) Steps 476(466.27) | Grad Norm 5.9016(2.6764) | Total Time 10.00(10.00)\n",
      "Iter 1440 | Time 38.2482(37.7564) | Bit/dim 1.1773(1.1785) | Xent 0.0921(0.0911) | Loss 1.2234(1.2241) | Error 0.0266(0.0279) Steps 470(466.38) | Grad Norm 3.7921(2.7099) | Total Time 10.00(10.00)\n",
      "Iter 1441 | Time 38.7680(37.7867) | Bit/dim 1.1767(1.1785) | Xent 0.0822(0.0909) | Loss 1.2178(1.2239) | Error 0.0255(0.0279) Steps 476(466.67) | Grad Norm 1.1017(2.6616) | Total Time 10.00(10.00)\n",
      "Iter 1442 | Time 38.7879(37.8168) | Bit/dim 1.1765(1.1784) | Xent 0.0947(0.0910) | Loss 1.2238(1.2239) | Error 0.0296(0.0279) Steps 476(466.95) | Grad Norm 1.4415(2.6250) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0206 | Time 17.3245, Epoch Time 300.8252(271.0694), Bit/dim 1.1709(best: 1.1658), Xent 0.0456, Loss 1.1937, Error 0.0159(best: 0.0139)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1443 | Time 37.7467(37.8147) | Bit/dim 1.1806(1.1785) | Xent 0.0864(0.0909) | Loss 1.2238(1.2239) | Error 0.0265(0.0279) Steps 470(467.04) | Grad Norm 3.5470(2.6527) | Total Time 10.00(10.00)\n",
      "Iter 1444 | Time 40.4144(37.8927) | Bit/dim 1.1790(1.1785) | Xent 0.0845(0.0907) | Loss 1.2213(1.2238) | Error 0.0264(0.0278) Steps 476(467.31) | Grad Norm 4.6952(2.7139) | Total Time 10.00(10.00)\n",
      "Iter 1445 | Time 38.2723(37.9041) | Bit/dim 1.1733(1.1783) | Xent 0.0945(0.0908) | Loss 1.2205(1.2237) | Error 0.0270(0.0278) Steps 470(467.39) | Grad Norm 4.5085(2.7678) | Total Time 10.00(10.00)\n",
      "Iter 1446 | Time 39.3849(37.9485) | Bit/dim 1.1815(1.1784) | Xent 0.0929(0.0908) | Loss 1.2280(1.2238) | Error 0.0285(0.0278) Steps 476(467.65) | Grad Norm 4.0898(2.8074) | Total Time 10.00(10.00)\n",
      "Iter 1447 | Time 38.3377(37.9602) | Bit/dim 1.1690(1.1781) | Xent 0.0959(0.0910) | Loss 1.2169(1.2236) | Error 0.0294(0.0279) Steps 470(467.72) | Grad Norm 3.5966(2.8311) | Total Time 10.00(10.00)\n",
      "Iter 1448 | Time 39.9539(38.0200) | Bit/dim 1.1812(1.1782) | Xent 0.0798(0.0907) | Loss 1.2211(1.2236) | Error 0.0236(0.0277) Steps 482(468.15) | Grad Norm 3.3846(2.8477) | Total Time 10.00(10.00)\n",
      "Iter 1449 | Time 38.2820(38.0278) | Bit/dim 1.1796(1.1783) | Xent 0.0854(0.0905) | Loss 1.2223(1.2235) | Error 0.0249(0.0276) Steps 476(468.39) | Grad Norm 3.3409(2.8625) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0207 | Time 17.2278, Epoch Time 302.3840(272.0089), Bit/dim 1.1721(best: 1.1658), Xent 0.0451, Loss 1.1947, Error 0.0149(best: 0.0139)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1450 | Time 39.5562(38.0737) | Bit/dim 1.1797(1.1783) | Xent 0.0926(0.0906) | Loss 1.2260(1.2236) | Error 0.0271(0.0276) Steps 476(468.61) | Grad Norm 3.3178(2.8762) | Total Time 10.00(10.00)\n",
      "Iter 1451 | Time 38.0203(38.0721) | Bit/dim 1.1739(1.1782) | Xent 0.0880(0.0905) | Loss 1.2179(1.2234) | Error 0.0270(0.0276) Steps 476(468.84) | Grad Norm 2.9076(2.8771) | Total Time 10.00(10.00)\n",
      "Iter 1452 | Time 39.6323(38.1189) | Bit/dim 1.1769(1.1781) | Xent 0.0926(0.0906) | Loss 1.2232(1.2234) | Error 0.0289(0.0277) Steps 476(469.05) | Grad Norm 2.5218(2.8665) | Total Time 10.00(10.00)\n",
      "Iter 1453 | Time 39.0504(38.1468) | Bit/dim 1.1703(1.1779) | Xent 0.0928(0.0906) | Loss 1.2167(1.2232) | Error 0.0276(0.0277) Steps 470(469.08) | Grad Norm 2.2638(2.8484) | Total Time 10.00(10.00)\n",
      "Iter 1454 | Time 39.6236(38.1911) | Bit/dim 1.1791(1.1779) | Xent 0.0865(0.0905) | Loss 1.2223(1.2232) | Error 0.0246(0.0276) Steps 476(469.29) | Grad Norm 1.9161(2.8204) | Total Time 10.00(10.00)\n",
      "Iter 1455 | Time 37.9655(38.1844) | Bit/dim 1.1765(1.1779) | Xent 0.0887(0.0904) | Loss 1.2209(1.2231) | Error 0.0269(0.0275) Steps 476(469.49) | Grad Norm 1.6710(2.7859) | Total Time 10.00(10.00)\n",
      "Iter 1456 | Time 38.9299(38.2067) | Bit/dim 1.1741(1.1778) | Xent 0.0862(0.0903) | Loss 1.2172(1.2229) | Error 0.0260(0.0275) Steps 482(469.86) | Grad Norm 1.5202(2.7480) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0208 | Time 17.5273, Epoch Time 303.1643(272.9435), Bit/dim 1.1683(best: 1.1658), Xent 0.0443, Loss 1.1904, Error 0.0150(best: 0.0139)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1457 | Time 38.7546(38.2232) | Bit/dim 1.1779(1.1778) | Xent 0.0946(0.0904) | Loss 1.2252(1.2230) | Error 0.0302(0.0276) Steps 476(470.05) | Grad Norm 1.2644(2.7035) | Total Time 10.00(10.00)\n",
      "Iter 1458 | Time 38.8830(38.2430) | Bit/dim 1.1729(1.1776) | Xent 0.0881(0.0904) | Loss 1.2169(1.2228) | Error 0.0275(0.0276) Steps 482(470.41) | Grad Norm 1.2370(2.6595) | Total Time 10.00(10.00)\n",
      "Iter 1459 | Time 38.5229(38.2514) | Bit/dim 1.1741(1.1775) | Xent 0.0923(0.0904) | Loss 1.2203(1.2228) | Error 0.0270(0.0276) Steps 476(470.57) | Grad Norm 1.1040(2.6128) | Total Time 10.00(10.00)\n",
      "Iter 1460 | Time 40.8581(38.3296) | Bit/dim 1.1760(1.1775) | Xent 0.0838(0.0902) | Loss 1.2179(1.2226) | Error 0.0256(0.0275) Steps 482(470.92) | Grad Norm 0.6255(2.5532) | Total Time 10.00(10.00)\n",
      "Iter 1461 | Time 41.3559(38.4204) | Bit/dim 1.1783(1.1775) | Xent 0.0934(0.0903) | Loss 1.2250(1.2227) | Error 0.0288(0.0275) Steps 482(471.25) | Grad Norm 0.3731(2.4878) | Total Time 10.00(10.00)\n",
      "Iter 1462 | Time 39.5818(38.4552) | Bit/dim 1.1760(1.1775) | Xent 0.0948(0.0905) | Loss 1.2234(1.2227) | Error 0.0300(0.0276) Steps 476(471.39) | Grad Norm 1.2018(2.4492) | Total Time 10.00(10.00)\n",
      "Iter 1463 | Time 39.0039(38.4717) | Bit/dim 1.1728(1.1773) | Xent 0.0918(0.0905) | Loss 1.2187(1.2226) | Error 0.0282(0.0276) Steps 482(471.71) | Grad Norm 2.6514(2.4553) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0209 | Time 17.7706, Epoch Time 307.4062(273.9774), Bit/dim 1.1714(best: 1.1658), Xent 0.0450, Loss 1.1939, Error 0.0150(best: 0.0139)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1464 | Time 38.6256(38.4763) | Bit/dim 1.1784(1.1774) | Xent 0.0941(0.0906) | Loss 1.2255(1.2227) | Error 0.0280(0.0276) Steps 476(471.84) | Grad Norm 4.7009(2.5226) | Total Time 10.00(10.00)\n",
      "Iter 1465 | Time 40.1508(38.5265) | Bit/dim 1.1815(1.1775) | Xent 0.0760(0.0902) | Loss 1.2195(1.2226) | Error 0.0234(0.0275) Steps 482(472.14) | Grad Norm 6.6790(2.6473) | Total Time 10.00(10.00)\n",
      "Iter 1466 | Time 38.8261(38.5355) | Bit/dim 1.1806(1.1776) | Xent 0.1002(0.0905) | Loss 1.2307(1.2228) | Error 0.0315(0.0276) Steps 476(472.26) | Grad Norm 8.3850(2.8194) | Total Time 10.00(10.00)\n",
      "Iter 1467 | Time 39.3425(38.5597) | Bit/dim 1.1945(1.1781) | Xent 0.0790(0.0901) | Loss 1.2340(1.2232) | Error 0.0255(0.0276) Steps 482(472.55) | Grad Norm 8.1355(2.9789) | Total Time 10.00(10.00)\n",
      "Iter 1468 | Time 39.2087(38.5792) | Bit/dim 1.1783(1.1781) | Xent 0.0934(0.0902) | Loss 1.2250(1.2232) | Error 0.0291(0.0276) Steps 476(472.65) | Grad Norm 2.8068(2.9738) | Total Time 10.00(10.00)\n",
      "Iter 1469 | Time 38.8895(38.5885) | Bit/dim 1.1761(1.1780) | Xent 0.0911(0.0903) | Loss 1.2217(1.2232) | Error 0.0279(0.0276) Steps 476(472.76) | Grad Norm 5.3272(3.0444) | Total Time 10.00(10.00)\n",
      "Iter 1470 | Time 39.0876(38.6035) | Bit/dim 1.2003(1.1787) | Xent 0.0915(0.0903) | Loss 1.2461(1.2238) | Error 0.0270(0.0276) Steps 482(473.03) | Grad Norm 8.6814(3.2135) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0210 | Time 17.5583, Epoch Time 304.3577(274.8888), Bit/dim 1.1709(best: 1.1658), Xent 0.0448, Loss 1.1933, Error 0.0155(best: 0.0139)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1471 | Time 38.5555(38.6020) | Bit/dim 1.1752(1.1786) | Xent 0.0875(0.0902) | Loss 1.2190(1.2237) | Error 0.0265(0.0276) Steps 476(473.12) | Grad Norm 3.5324(3.2230) | Total Time 10.00(10.00)\n",
      "Iter 1472 | Time 38.3855(38.5955) | Bit/dim 1.1826(1.1787) | Xent 0.0930(0.0903) | Loss 1.2291(1.2239) | Error 0.0296(0.0276) Steps 476(473.21) | Grad Norm 5.0723(3.2785) | Total Time 10.00(10.00)\n",
      "Iter 1473 | Time 41.6195(38.6862) | Bit/dim 1.1948(1.1792) | Xent 0.0856(0.0901) | Loss 1.2376(1.2243) | Error 0.0256(0.0276) Steps 482(473.47) | Grad Norm 8.1814(3.4256) | Total Time 10.00(10.00)\n",
      "Iter 1474 | Time 39.6029(38.7137) | Bit/dim 1.1768(1.1791) | Xent 0.0920(0.0902) | Loss 1.2228(1.2242) | Error 0.0276(0.0276) Steps 476(473.55) | Grad Norm 2.3897(3.3945) | Total Time 10.00(10.00)\n",
      "Iter 1475 | Time 37.7732(38.6855) | Bit/dim 1.1815(1.1792) | Xent 0.0947(0.0903) | Loss 1.2288(1.2244) | Error 0.0310(0.0277) Steps 476(473.62) | Grad Norm 7.2606(3.5105) | Total Time 10.00(10.00)\n",
      "Iter 1476 | Time 39.0523(38.6965) | Bit/dim 1.2038(1.1799) | Xent 0.0904(0.0903) | Loss 1.2490(1.2251) | Error 0.0281(0.0277) Steps 482(473.87) | Grad Norm 9.4789(3.6896) | Total Time 10.00(10.00)\n",
      "Iter 1477 | Time 40.2906(38.7444) | Bit/dim 1.1785(1.1799) | Xent 0.0905(0.0903) | Loss 1.2238(1.2251) | Error 0.0269(0.0277) Steps 482(474.12) | Grad Norm 1.8344(3.6339) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0211 | Time 18.0499, Epoch Time 305.8860(275.8187), Bit/dim 1.2106(best: 1.1658), Xent 0.0490, Loss 1.2351, Error 0.0168(best: 0.0139)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1478 | Time 37.3822(38.7035) | Bit/dim 1.2148(1.1809) | Xent 0.1003(0.0906) | Loss 1.2649(1.2263) | Error 0.0309(0.0278) Steps 476(474.17) | Grad Norm 17.5783(4.0522) | Total Time 10.00(10.00)\n",
      "Iter 1479 | Time 41.9037(38.7995) | Bit/dim 1.2427(1.1828) | Xent 0.0913(0.0907) | Loss 1.2884(1.2281) | Error 0.0292(0.0278) Steps 488(474.59) | Grad Norm 10.8249(4.2554) | Total Time 10.00(10.00)\n",
      "Iter 1480 | Time 40.0068(38.8357) | Bit/dim 1.2469(1.1847) | Xent 0.0779(0.0903) | Loss 1.2859(1.2299) | Error 0.0230(0.0277) Steps 488(474.99) | Grad Norm 9.4487(4.4112) | Total Time 10.00(10.00)\n",
      "Iter 1481 | Time 41.4514(38.9142) | Bit/dim 1.1996(1.1852) | Xent 0.0950(0.0904) | Loss 1.2470(1.2304) | Error 0.0289(0.0277) Steps 488(475.38) | Grad Norm 2.8242(4.3636) | Total Time 10.00(10.00)\n",
      "Iter 1482 | Time 39.3058(38.9259) | Bit/dim 1.2672(1.1876) | Xent 0.1165(0.0912) | Loss 1.3254(1.2332) | Error 0.0363(0.0280) Steps 482(475.58) | Grad Norm 21.3224(4.8724) | Total Time 10.00(10.00)\n",
      "Iter 1483 | Time 39.5952(38.9460) | Bit/dim 1.2168(1.1885) | Xent 0.1145(0.0919) | Loss 1.2740(1.2345) | Error 0.0355(0.0282) Steps 488(475.95) | Grad Norm 7.4461(4.9496) | Total Time 10.00(10.00)\n",
      "Iter 1484 | Time 39.7204(38.9692) | Bit/dim 1.2561(1.1905) | Xent 0.0947(0.0920) | Loss 1.3035(1.2365) | Error 0.0274(0.0282) Steps 482(476.13) | Grad Norm 8.4009(5.0531) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0212 | Time 17.7034, Epoch Time 309.7787(276.8375), Bit/dim 1.2233(best: 1.1658), Xent 0.0422, Loss 1.2444, Error 0.0141(best: 0.0139)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1485 | Time 39.1887(38.9758) | Bit/dim 1.2325(1.1918) | Xent 0.0771(0.0915) | Loss 1.2710(1.2376) | Error 0.0244(0.0280) Steps 482(476.31) | Grad Norm 5.8634(5.0774) | Total Time 10.00(10.00)\n",
      "Iter 1486 | Time 39.0593(38.9783) | Bit/dim 1.2105(1.1924) | Xent 0.1067(0.0920) | Loss 1.2639(1.2383) | Error 0.0300(0.0281) Steps 476(476.30) | Grad Norm 5.4400(5.0883) | Total Time 10.00(10.00)\n",
      "Iter 1487 | Time 39.3831(38.9905) | Bit/dim 1.2267(1.1934) | Xent 0.1188(0.0928) | Loss 1.2861(1.2398) | Error 0.0355(0.0283) Steps 476(476.29) | Grad Norm 12.2579(5.3034) | Total Time 10.00(10.00)\n",
      "Iter 1488 | Time 39.1651(38.9957) | Bit/dim 1.2039(1.1937) | Xent 0.1036(0.0931) | Loss 1.2557(1.2403) | Error 0.0330(0.0285) Steps 482(476.46) | Grad Norm 4.6847(5.2848) | Total Time 10.00(10.00)\n",
      "Iter 1489 | Time 39.1452(39.0002) | Bit/dim 1.2142(1.1943) | Xent 0.0911(0.0931) | Loss 1.2597(1.2408) | Error 0.0296(0.0285) Steps 482(476.63) | Grad Norm 7.4399(5.3495) | Total Time 10.00(10.00)\n",
      "Iter 1490 | Time 39.7220(39.0219) | Bit/dim 1.1997(1.1945) | Xent 0.0807(0.0927) | Loss 1.2400(1.2408) | Error 0.0249(0.0284) Steps 482(476.79) | Grad Norm 3.9472(5.3074) | Total Time 10.00(10.00)\n",
      "Iter 1491 | Time 39.8669(39.0472) | Bit/dim 1.2099(1.1949) | Xent 0.1082(0.0932) | Loss 1.2640(1.2415) | Error 0.0319(0.0285) Steps 482(476.95) | Grad Norm 9.0427(5.4195) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0213 | Time 17.6105, Epoch Time 305.7559(277.7051), Bit/dim 1.1885(best: 1.1658), Xent 0.0466, Loss 1.2118, Error 0.0152(best: 0.0139)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1492 | Time 39.8890(39.0725) | Bit/dim 1.1959(1.1950) | Xent 0.0923(0.0931) | Loss 1.2420(1.2415) | Error 0.0298(0.0285) Steps 476(476.92) | Grad Norm 3.7669(5.3699) | Total Time 10.00(10.00)\n",
      "Iter 1493 | Time 40.5775(39.1176) | Bit/dim 1.1999(1.1951) | Xent 0.0922(0.0931) | Loss 1.2460(1.2417) | Error 0.0260(0.0285) Steps 482(477.07) | Grad Norm 5.6025(5.3769) | Total Time 10.00(10.00)\n",
      "Iter 1494 | Time 39.8328(39.1391) | Bit/dim 1.1946(1.1951) | Xent 0.0984(0.0933) | Loss 1.2438(1.2417) | Error 0.0306(0.0285) Steps 482(477.22) | Grad Norm 5.6118(5.3839) | Total Time 10.00(10.00)\n",
      "Iter 1495 | Time 38.5669(39.1219) | Bit/dim 1.1893(1.1949) | Xent 0.1029(0.0936) | Loss 1.2408(1.2417) | Error 0.0314(0.0286) Steps 482(477.36) | Grad Norm 4.2209(5.3490) | Total Time 10.00(10.00)\n",
      "Iter 1496 | Time 39.4718(39.1324) | Bit/dim 1.1886(1.1947) | Xent 0.0832(0.0932) | Loss 1.2302(1.2414) | Error 0.0259(0.0285) Steps 476(477.32) | Grad Norm 5.5576(5.3553) | Total Time 10.00(10.00)\n",
      "Iter 1497 | Time 40.5356(39.1745) | Bit/dim 1.1966(1.1948) | Xent 0.0862(0.0930) | Loss 1.2397(1.2413) | Error 0.0271(0.0285) Steps 488(477.64) | Grad Norm 3.8572(5.3103) | Total Time 10.00(10.00)\n",
      "Iter 1498 | Time 40.3149(39.2087) | Bit/dim 1.2009(1.1950) | Xent 0.0754(0.0925) | Loss 1.2386(1.2412) | Error 0.0231(0.0283) Steps 488(477.95) | Grad Norm 4.6673(5.2911) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0214 | Time 17.5270, Epoch Time 309.1651(278.6489), Bit/dim 1.1815(best: 1.1658), Xent 0.0439, Loss 1.2035, Error 0.0148(best: 0.0139)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1499 | Time 39.2576(39.2102) | Bit/dim 1.1842(1.1946) | Xent 0.0969(0.0926) | Loss 1.2326(1.2410) | Error 0.0329(0.0285) Steps 488(478.25) | Grad Norm 2.7149(5.2138) | Total Time 10.00(10.00)\n",
      "Iter 1500 | Time 38.4988(39.1888) | Bit/dim 1.1952(1.1947) | Xent 0.1025(0.0929) | Loss 1.2465(1.2411) | Error 0.0302(0.0285) Steps 482(478.37) | Grad Norm 5.4504(5.2209) | Total Time 10.00(10.00)\n",
      "Iter 1501 | Time 39.9195(39.2108) | Bit/dim 1.1841(1.1943) | Xent 0.0955(0.0930) | Loss 1.2318(1.2409) | Error 0.0294(0.0285) Steps 488(478.65) | Grad Norm 3.4998(5.1692) | Total Time 10.00(10.00)\n",
      "Iter 1502 | Time 41.2300(39.2713) | Bit/dim 1.1955(1.1944) | Xent 0.0888(0.0929) | Loss 1.2399(1.2408) | Error 0.0272(0.0285) Steps 488(478.93) | Grad Norm 4.2719(5.1423) | Total Time 10.00(10.00)\n",
      "Iter 1503 | Time 40.7603(39.3160) | Bit/dim 1.1822(1.1940) | Xent 0.0871(0.0927) | Loss 1.2258(1.2404) | Error 0.0284(0.0285) Steps 488(479.21) | Grad Norm 2.2241(5.0548) | Total Time 10.00(10.00)\n",
      "Iter 1504 | Time 41.6831(39.3870) | Bit/dim 1.1879(1.1938) | Xent 0.0847(0.0925) | Loss 1.2302(1.2401) | Error 0.0260(0.0284) Steps 494(479.65) | Grad Norm 4.8787(5.0495) | Total Time 10.00(10.00)\n",
      "Iter 1505 | Time 40.4237(39.4181) | Bit/dim 1.1867(1.1936) | Xent 0.0782(0.0920) | Loss 1.2258(1.2396) | Error 0.0246(0.0283) Steps 488(479.90) | Grad Norm 2.8848(4.9845) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0215 | Time 18.0220, Epoch Time 312.2174(279.6560), Bit/dim 1.1799(best: 1.1658), Xent 0.0414, Loss 1.2006, Error 0.0140(best: 0.0139)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1506 | Time 40.4783(39.4499) | Bit/dim 1.1828(1.1933) | Xent 0.0753(0.0915) | Loss 1.2204(1.2391) | Error 0.0241(0.0282) Steps 488(480.14) | Grad Norm 3.2874(4.9336) | Total Time 10.00(10.00)\n",
      "Iter 1507 | Time 40.1913(39.4722) | Bit/dim 1.1809(1.1929) | Xent 0.0938(0.0916) | Loss 1.2278(1.2387) | Error 0.0298(0.0282) Steps 488(480.38) | Grad Norm 2.3717(4.8568) | Total Time 10.00(10.00)\n",
      "Iter 1508 | Time 40.6172(39.5065) | Bit/dim 1.1849(1.1927) | Xent 0.0865(0.0915) | Loss 1.2281(1.2384) | Error 0.0278(0.0282) Steps 494(480.79) | Grad Norm 3.0017(4.8011) | Total Time 10.00(10.00)\n",
      "Iter 1509 | Time 39.9141(39.5187) | Bit/dim 1.1863(1.1925) | Xent 0.0919(0.0915) | Loss 1.2322(1.2382) | Error 0.0282(0.0282) Steps 482(480.82) | Grad Norm 3.0138(4.7475) | Total Time 10.00(10.00)\n",
      "Iter 1510 | Time 40.5807(39.5506) | Bit/dim 1.1860(1.1923) | Xent 0.0796(0.0911) | Loss 1.2258(1.2379) | Error 0.0238(0.0281) Steps 488(481.04) | Grad Norm 2.2615(4.6729) | Total Time 10.00(10.00)\n",
      "Iter 1511 | Time 40.9551(39.5927) | Bit/dim 1.1860(1.1921) | Xent 0.0834(0.0909) | Loss 1.2277(1.2375) | Error 0.0248(0.0280) Steps 494(481.43) | Grad Norm 3.6803(4.6431) | Total Time 10.00(10.00)\n",
      "Iter 1512 | Time 40.8826(39.6314) | Bit/dim 1.1847(1.1919) | Xent 0.0966(0.0910) | Loss 1.2330(1.2374) | Error 0.0286(0.0280) Steps 488(481.63) | Grad Norm 0.7543(4.5265) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0216 | Time 17.9896, Epoch Time 314.2620(280.6941), Bit/dim 1.1761(best: 1.1658), Xent 0.0418, Loss 1.1970, Error 0.0138(best: 0.0139)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1513 | Time 41.1355(39.6765) | Bit/dim 1.1842(1.1917) | Xent 0.0880(0.0910) | Loss 1.2281(1.2371) | Error 0.0272(0.0280) Steps 488(481.82) | Grad Norm 2.5916(4.4684) | Total Time 10.00(10.00)\n",
      "Iter 1514 | Time 40.0589(39.6880) | Bit/dim 1.1800(1.1913) | Xent 0.0944(0.0911) | Loss 1.2272(1.2368) | Error 0.0289(0.0280) Steps 482(481.82) | Grad Norm 0.9301(4.3623) | Total Time 10.00(10.00)\n",
      "Iter 1515 | Time 41.8753(39.7536) | Bit/dim 1.1800(1.1910) | Xent 0.0966(0.0912) | Loss 1.2283(1.2366) | Error 0.0284(0.0280) Steps 482(481.83) | Grad Norm 2.8358(4.3165) | Total Time 10.00(10.00)\n",
      "Iter 1516 | Time 39.3434(39.7413) | Bit/dim 1.1813(1.1907) | Xent 0.0906(0.0912) | Loss 1.2266(1.2363) | Error 0.0278(0.0280) Steps 488(482.01) | Grad Norm 1.5780(4.2343) | Total Time 10.00(10.00)\n",
      "Iter 1517 | Time 40.9956(39.7790) | Bit/dim 1.1886(1.1906) | Xent 0.0889(0.0911) | Loss 1.2331(1.2362) | Error 0.0265(0.0280) Steps 488(482.19) | Grad Norm 2.1422(4.1716) | Total Time 10.00(10.00)\n",
      "Iter 1518 | Time 43.1257(39.8794) | Bit/dim 1.1817(1.1903) | Xent 0.0881(0.0911) | Loss 1.2257(1.2359) | Error 0.0268(0.0279) Steps 488(482.37) | Grad Norm 2.2897(4.1151) | Total Time 10.00(10.00)\n",
      "Iter 1519 | Time 39.9572(39.8817) | Bit/dim 1.1800(1.1900) | Xent 0.0890(0.0910) | Loss 1.2245(1.2355) | Error 0.0281(0.0279) Steps 488(482.54) | Grad Norm 0.7111(4.0130) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0217 | Time 17.6595, Epoch Time 316.9305(281.7812), Bit/dim 1.1763(best: 1.1658), Xent 0.0431, Loss 1.1978, Error 0.0142(best: 0.0138)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1520 | Time 42.3878(39.9569) | Bit/dim 1.1849(1.1899) | Xent 0.0900(0.0910) | Loss 1.2299(1.2354) | Error 0.0279(0.0279) Steps 488(482.70) | Grad Norm 1.9536(3.9512) | Total Time 10.00(10.00)\n",
      "Iter 1521 | Time 41.0432(39.9895) | Bit/dim 1.1776(1.1895) | Xent 0.0870(0.0908) | Loss 1.2210(1.2349) | Error 0.0268(0.0279) Steps 488(482.86) | Grad Norm 0.5369(3.8488) | Total Time 10.00(10.00)\n",
      "Iter 1522 | Time 40.4046(40.0019) | Bit/dim 1.1874(1.1894) | Xent 0.0854(0.0907) | Loss 1.2301(1.2348) | Error 0.0275(0.0279) Steps 488(483.01) | Grad Norm 1.9439(3.7916) | Total Time 10.00(10.00)\n",
      "Iter 1523 | Time 39.8263(39.9967) | Bit/dim 1.1821(1.1892) | Xent 0.0867(0.0906) | Loss 1.2254(1.2345) | Error 0.0295(0.0279) Steps 482(482.98) | Grad Norm 1.5152(3.7233) | Total Time 10.00(10.00)\n",
      "Iter 1524 | Time 39.7194(39.9883) | Bit/dim 1.1813(1.1890) | Xent 0.0946(0.0907) | Loss 1.2287(1.2343) | Error 0.0278(0.0279) Steps 482(482.95) | Grad Norm 1.1360(3.6457) | Total Time 10.00(10.00)\n",
      "Iter 1525 | Time 39.2428(39.9660) | Bit/dim 1.1850(1.1889) | Xent 0.0818(0.0904) | Loss 1.2259(1.2341) | Error 0.0262(0.0279) Steps 482(482.92) | Grad Norm 2.1079(3.5996) | Total Time 10.00(10.00)\n",
      "Iter 1526 | Time 39.5937(39.9548) | Bit/dim 1.1803(1.1886) | Xent 0.0923(0.0905) | Loss 1.2265(1.2338) | Error 0.0291(0.0279) Steps 482(482.90) | Grad Norm 0.6951(3.5125) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0218 | Time 17.4332, Epoch Time 312.2365(282.6949), Bit/dim 1.1782(best: 1.1658), Xent 0.0400, Loss 1.1982, Error 0.0135(best: 0.0138)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1527 | Time 39.6288(39.9450) | Bit/dim 1.1830(1.1884) | Xent 0.0861(0.0903) | Loss 1.2260(1.2336) | Error 0.0256(0.0278) Steps 482(482.87) | Grad Norm 1.8824(3.4635) | Total Time 10.00(10.00)\n",
      "Iter 1528 | Time 39.9895(39.9464) | Bit/dim 1.1807(1.1882) | Xent 0.0904(0.0903) | Loss 1.2259(1.2334) | Error 0.0265(0.0278) Steps 488(483.02) | Grad Norm 1.9983(3.4196) | Total Time 10.00(10.00)\n",
      "Iter 1529 | Time 40.8197(39.9726) | Bit/dim 1.1842(1.1881) | Xent 0.0868(0.0902) | Loss 1.2276(1.2332) | Error 0.0265(0.0278) Steps 488(483.17) | Grad Norm 0.3364(3.3271) | Total Time 10.00(10.00)\n",
      "Iter 1530 | Time 39.5303(39.9593) | Bit/dim 1.1832(1.1879) | Xent 0.0830(0.0900) | Loss 1.2247(1.2330) | Error 0.0270(0.0277) Steps 482(483.14) | Grad Norm 2.3218(3.2969) | Total Time 10.00(10.00)\n",
      "Iter 1531 | Time 39.1452(39.9349) | Bit/dim 1.1852(1.1879) | Xent 0.0885(0.0900) | Loss 1.2295(1.2328) | Error 0.0268(0.0277) Steps 482(483.10) | Grad Norm 2.1831(3.2635) | Total Time 10.00(10.00)\n",
      "Iter 1532 | Time 40.2271(39.9436) | Bit/dim 1.1872(1.1878) | Xent 0.0892(0.0899) | Loss 1.2317(1.2328) | Error 0.0264(0.0277) Steps 476(482.89) | Grad Norm 0.3641(3.1765) | Total Time 10.00(10.00)\n",
      "Iter 1533 | Time 40.4789(39.9597) | Bit/dim 1.1850(1.1878) | Xent 0.0747(0.0895) | Loss 1.2224(1.2325) | Error 0.0235(0.0275) Steps 488(483.04) | Grad Norm 1.9695(3.1403) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0219 | Time 17.7805, Epoch Time 310.1079(283.5173), Bit/dim 1.1785(best: 1.1658), Xent 0.0439, Loss 1.2005, Error 0.0150(best: 0.0135)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1534 | Time 41.5625(40.0078) | Bit/dim 1.1891(1.1878) | Xent 0.0879(0.0894) | Loss 1.2330(1.2325) | Error 0.0275(0.0275) Steps 488(483.19) | Grad Norm 1.7959(3.1000) | Total Time 10.00(10.00)\n",
      "Iter 1535 | Time 40.3661(40.0185) | Bit/dim 1.1784(1.1875) | Xent 0.0956(0.0896) | Loss 1.2262(1.2323) | Error 0.0280(0.0276) Steps 488(483.34) | Grad Norm 0.4912(3.0217) | Total Time 10.00(10.00)\n",
      "Iter 1536 | Time 41.1256(40.0517) | Bit/dim 1.1896(1.1876) | Xent 0.0749(0.0892) | Loss 1.2270(1.2322) | Error 0.0255(0.0275) Steps 488(483.48) | Grad Norm 1.4395(2.9743) | Total Time 10.00(10.00)\n",
      "Iter 1537 | Time 41.2215(40.0868) | Bit/dim 1.1761(1.1872) | Xent 0.0888(0.0892) | Loss 1.2205(1.2318) | Error 0.0282(0.0275) Steps 494(483.79) | Grad Norm 1.7165(2.9365) | Total Time 10.00(10.00)\n",
      "Iter 1538 | Time 41.1881(40.1199) | Bit/dim 1.1858(1.1872) | Xent 0.0867(0.0891) | Loss 1.2291(1.2317) | Error 0.0272(0.0275) Steps 488(483.92) | Grad Norm 0.7773(2.8718) | Total Time 10.00(10.00)\n",
      "Iter 1539 | Time 41.1542(40.1509) | Bit/dim 1.1881(1.1872) | Xent 0.0855(0.0890) | Loss 1.2309(1.2317) | Error 0.0278(0.0275) Steps 488(484.04) | Grad Norm 0.6467(2.8050) | Total Time 10.00(10.00)\n",
      "Iter 1540 | Time 41.4011(40.1884) | Bit/dim 1.1809(1.1870) | Xent 0.0865(0.0889) | Loss 1.2242(1.2315) | Error 0.0264(0.0275) Steps 488(484.16) | Grad Norm 0.9111(2.7482) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0220 | Time 17.6257, Epoch Time 318.3051(284.5609), Bit/dim 1.1770(best: 1.1658), Xent 0.0446, Loss 1.1993, Error 0.0147(best: 0.0135)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1541 | Time 41.8669(40.2388) | Bit/dim 1.1891(1.1871) | Xent 0.0965(0.0891) | Loss 1.2374(1.2317) | Error 0.0302(0.0276) Steps 494(484.46) | Grad Norm 0.7342(2.6878) | Total Time 10.00(10.00)\n",
      "Iter 1542 | Time 41.3025(40.2707) | Bit/dim 1.1823(1.1869) | Xent 0.0911(0.0892) | Loss 1.2279(1.2315) | Error 0.0285(0.0276) Steps 494(484.74) | Grad Norm 0.7050(2.6283) | Total Time 10.00(10.00)\n",
      "Iter 1543 | Time 41.6649(40.3125) | Bit/dim 1.1886(1.1870) | Xent 0.0839(0.0890) | Loss 1.2305(1.2315) | Error 0.0255(0.0275) Steps 494(485.02) | Grad Norm 1.0211(2.5801) | Total Time 10.00(10.00)\n",
      "Iter 1544 | Time 41.9977(40.3631) | Bit/dim 1.1793(1.1868) | Xent 0.0875(0.0890) | Loss 1.2230(1.2313) | Error 0.0259(0.0275) Steps 488(485.11) | Grad Norm 1.4067(2.5449) | Total Time 10.00(10.00)\n",
      "Iter 1545 | Time 41.6984(40.4031) | Bit/dim 1.1819(1.1866) | Xent 0.0949(0.0892) | Loss 1.2293(1.2312) | Error 0.0281(0.0275) Steps 494(485.38) | Grad Norm 1.1693(2.5036) | Total Time 10.00(10.00)\n",
      "Iter 1546 | Time 41.5333(40.4370) | Bit/dim 1.1803(1.1864) | Xent 0.0831(0.0890) | Loss 1.2218(1.2309) | Error 0.0255(0.0274) Steps 494(485.63) | Grad Norm 0.5469(2.4449) | Total Time 10.00(10.00)\n",
      "Iter 1547 | Time 41.6648(40.4739) | Bit/dim 1.1804(1.1862) | Xent 0.0870(0.0889) | Loss 1.2239(1.2307) | Error 0.0281(0.0275) Steps 494(485.89) | Grad Norm 0.5370(2.3877) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0221 | Time 17.6807, Epoch Time 321.9889(285.6837), Bit/dim 1.1755(best: 1.1658), Xent 0.0454, Loss 1.1982, Error 0.0151(best: 0.0135)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1548 | Time 40.8642(40.4856) | Bit/dim 1.1844(1.1862) | Xent 0.0866(0.0889) | Loss 1.2277(1.2306) | Error 0.0272(0.0275) Steps 494(486.13) | Grad Norm 0.4564(2.3297) | Total Time 10.00(10.00)\n",
      "Iter 1549 | Time 40.9935(40.5008) | Bit/dim 1.1828(1.1861) | Xent 0.0842(0.0887) | Loss 1.2249(1.2305) | Error 0.0261(0.0274) Steps 494(486.36) | Grad Norm 0.6178(2.2784) | Total Time 10.00(10.00)\n",
      "Iter 1550 | Time 43.3773(40.5871) | Bit/dim 1.1890(1.1862) | Xent 0.0912(0.0888) | Loss 1.2346(1.2306) | Error 0.0272(0.0274) Steps 494(486.59) | Grad Norm 0.6473(2.2294) | Total Time 10.00(10.00)\n",
      "Iter 1551 | Time 40.5822(40.5870) | Bit/dim 1.1826(1.1861) | Xent 0.0998(0.0891) | Loss 1.2325(1.2306) | Error 0.0298(0.0275) Steps 494(486.82) | Grad Norm 0.9469(2.1910) | Total Time 10.00(10.00)\n",
      "Iter 1552 | Time 41.8636(40.6253) | Bit/dim 1.1757(1.1858) | Xent 0.0936(0.0893) | Loss 1.2225(1.2304) | Error 0.0299(0.0276) Steps 494(487.03) | Grad Norm 1.6558(2.1749) | Total Time 10.00(10.00)\n",
      "Iter 1553 | Time 41.6238(40.6552) | Bit/dim 1.1823(1.1857) | Xent 0.0905(0.0893) | Loss 1.2275(1.2303) | Error 0.0276(0.0276) Steps 494(487.24) | Grad Norm 1.8285(2.1645) | Total Time 10.00(10.00)\n",
      "Iter 1554 | Time 40.9034(40.6627) | Bit/dim 1.1850(1.1856) | Xent 0.0887(0.0893) | Loss 1.2294(1.2303) | Error 0.0296(0.0276) Steps 494(487.44) | Grad Norm 1.7286(2.1514) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0222 | Time 18.4086, Epoch Time 321.1299(286.7471), Bit/dim 1.1769(best: 1.1658), Xent 0.0439, Loss 1.1989, Error 0.0144(best: 0.0135)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1555 | Time 41.7408(40.6950) | Bit/dim 1.1833(1.1856) | Xent 0.0902(0.0893) | Loss 1.2284(1.2302) | Error 0.0292(0.0277) Steps 494(487.64) | Grad Norm 1.7934(2.1407) | Total Time 10.00(10.00)\n",
      "Iter 1556 | Time 41.1885(40.7098) | Bit/dim 1.1849(1.1855) | Xent 0.0851(0.0892) | Loss 1.2275(1.2301) | Error 0.0270(0.0276) Steps 494(487.83) | Grad Norm 1.6639(2.1264) | Total Time 10.00(10.00)\n",
      "Iter 1557 | Time 40.9765(40.7178) | Bit/dim 1.1850(1.1855) | Xent 0.0760(0.0888) | Loss 1.2230(1.2299) | Error 0.0239(0.0275) Steps 500(488.20) | Grad Norm 1.0125(2.0930) | Total Time 10.00(10.00)\n",
      "Iter 1558 | Time 41.4415(40.7395) | Bit/dim 1.1858(1.1855) | Xent 0.0927(0.0889) | Loss 1.2322(1.2300) | Error 0.0294(0.0276) Steps 500(488.55) | Grad Norm 0.4706(2.0443) | Total Time 10.00(10.00)\n",
      "Iter 1559 | Time 41.3185(40.7569) | Bit/dim 1.1790(1.1853) | Xent 0.0888(0.0889) | Loss 1.2235(1.2298) | Error 0.0270(0.0276) Steps 500(488.89) | Grad Norm 1.5605(2.0298) | Total Time 10.00(10.00)\n",
      "Iter 1560 | Time 41.4345(40.7772) | Bit/dim 1.1805(1.1852) | Xent 0.0970(0.0891) | Loss 1.2290(1.2298) | Error 0.0294(0.0276) Steps 500(489.23) | Grad Norm 2.0547(2.0305) | Total Time 10.00(10.00)\n",
      "Iter 1561 | Time 43.0192(40.8445) | Bit/dim 1.1829(1.1851) | Xent 0.0829(0.0890) | Loss 1.2243(1.2296) | Error 0.0262(0.0276) Steps 500(489.55) | Grad Norm 2.0792(2.0320) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0223 | Time 18.0985, Epoch Time 322.1375(287.8088), Bit/dim 1.1772(best: 1.1658), Xent 0.0412, Loss 1.1978, Error 0.0144(best: 0.0135)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1562 | Time 42.3133(40.8885) | Bit/dim 1.1796(1.1850) | Xent 0.0896(0.0890) | Loss 1.2244(1.2295) | Error 0.0294(0.0276) Steps 506(490.04) | Grad Norm 2.0691(2.0331) | Total Time 10.00(10.00)\n",
      "Iter 1563 | Time 41.0248(40.8926) | Bit/dim 1.1859(1.1850) | Xent 0.0892(0.0890) | Loss 1.2305(1.2295) | Error 0.0268(0.0276) Steps 500(490.34) | Grad Norm 1.8512(2.0277) | Total Time 10.00(10.00)\n",
      "Iter 1564 | Time 43.5834(40.9734) | Bit/dim 1.1863(1.1850) | Xent 0.0957(0.0892) | Loss 1.2341(1.2296) | Error 0.0300(0.0277) Steps 506(490.81) | Grad Norm 1.5468(2.0132) | Total Time 10.00(10.00)\n",
      "Iter 1565 | Time 42.1845(41.0097) | Bit/dim 1.1866(1.1851) | Xent 0.0779(0.0888) | Loss 1.2256(1.2295) | Error 0.0222(0.0275) Steps 500(491.09) | Grad Norm 1.8310(2.0078) | Total Time 10.00(10.00)\n",
      "Iter 1566 | Time 43.1539(41.0740) | Bit/dim 1.1831(1.1850) | Xent 0.1004(0.0892) | Loss 1.2333(1.2296) | Error 0.0327(0.0277) Steps 506(491.53) | Grad Norm 2.1725(2.0127) | Total Time 10.00(10.00)\n",
      "Iter 1567 | Time 42.2426(41.1091) | Bit/dim 1.1850(1.1850) | Xent 0.0855(0.0891) | Loss 1.2278(1.2296) | Error 0.0279(0.0277) Steps 500(491.79) | Grad Norm 2.6480(2.0318) | Total Time 10.00(10.00)\n",
      "Iter 1568 | Time 43.0863(41.1684) | Bit/dim 1.1817(1.1849) | Xent 0.0835(0.0889) | Loss 1.2235(1.2294) | Error 0.0252(0.0276) Steps 506(492.22) | Grad Norm 3.3833(2.0723) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0224 | Time 18.2787, Epoch Time 328.4324(289.0275), Bit/dim 1.1795(best: 1.1658), Xent 0.0446, Loss 1.2017, Error 0.0150(best: 0.0135)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1569 | Time 40.6301(41.1522) | Bit/dim 1.1823(1.1848) | Xent 0.0918(0.0890) | Loss 1.2282(1.2293) | Error 0.0300(0.0277) Steps 500(492.45) | Grad Norm 3.5510(2.1167) | Total Time 10.00(10.00)\n",
      "Iter 1570 | Time 42.7183(41.1992) | Bit/dim 1.1841(1.1848) | Xent 0.0904(0.0890) | Loss 1.2293(1.2293) | Error 0.0261(0.0276) Steps 506(492.86) | Grad Norm 3.1994(2.1492) | Total Time 10.00(10.00)\n",
      "Iter 1571 | Time 42.9211(41.2509) | Bit/dim 1.1873(1.1849) | Xent 0.0884(0.0890) | Loss 1.2315(1.2294) | Error 0.0261(0.0276) Steps 506(493.25) | Grad Norm 2.3561(2.1554) | Total Time 10.00(10.00)\n",
      "Iter 1572 | Time 42.3904(41.2851) | Bit/dim 1.1825(1.1848) | Xent 0.0974(0.0893) | Loss 1.2312(1.2295) | Error 0.0299(0.0277) Steps 506(493.63) | Grad Norm 0.7329(2.1127) | Total Time 10.00(10.00)\n",
      "Iter 1573 | Time 41.8281(41.3014) | Bit/dim 1.1770(1.1846) | Xent 0.0971(0.0895) | Loss 1.2255(1.2293) | Error 0.0295(0.0277) Steps 506(494.00) | Grad Norm 1.3671(2.0903) | Total Time 10.00(10.00)\n",
      "Iter 1574 | Time 41.9062(41.3195) | Bit/dim 1.1866(1.1846) | Xent 0.0868(0.0894) | Loss 1.2300(1.2294) | Error 0.0291(0.0278) Steps 500(494.18) | Grad Norm 3.1412(2.1219) | Total Time 10.00(10.00)\n",
      "Iter 1575 | Time 42.7674(41.3629) | Bit/dim 1.1873(1.1847) | Xent 0.0842(0.0893) | Loss 1.2294(1.2294) | Error 0.0264(0.0277) Steps 506(494.54) | Grad Norm 5.4618(2.2221) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0225 | Time 18.2028, Epoch Time 325.8298(290.1316), Bit/dim 1.1860(best: 1.1658), Xent 0.0436, Loss 1.2078, Error 0.0150(best: 0.0135)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1576 | Time 41.8635(41.3779) | Bit/dim 1.1927(1.1850) | Xent 0.0738(0.0888) | Loss 1.2296(1.2294) | Error 0.0251(0.0276) Steps 500(494.70) | Grad Norm 6.5079(2.3506) | Total Time 10.00(10.00)\n",
      "Iter 1577 | Time 43.4874(41.4412) | Bit/dim 1.1834(1.1849) | Xent 0.0810(0.0886) | Loss 1.2239(1.2292) | Error 0.0255(0.0276) Steps 506(495.04) | Grad Norm 4.8711(2.4262) | Total Time 10.00(10.00)\n",
      "Iter 1578 | Time 42.6018(41.4761) | Bit/dim 1.1829(1.1849) | Xent 0.0973(0.0888) | Loss 1.2315(1.2293) | Error 0.0309(0.0277) Steps 506(495.37) | Grad Norm 2.4550(2.4271) | Total Time 10.00(10.00)\n",
      "Iter 1579 | Time 42.9918(41.5215) | Bit/dim 1.1828(1.1848) | Xent 0.0957(0.0890) | Loss 1.2307(1.2293) | Error 0.0298(0.0277) Steps 506(495.69) | Grad Norm 0.5684(2.3713) | Total Time 10.00(10.00)\n",
      "Iter 1580 | Time 42.9280(41.5637) | Bit/dim 1.1764(1.1845) | Xent 0.0929(0.0892) | Loss 1.2229(1.2291) | Error 0.0298(0.0278) Steps 506(496.00) | Grad Norm 2.9220(2.3879) | Total Time 10.00(10.00)\n",
      "Iter 1581 | Time 41.6800(41.5672) | Bit/dim 1.1897(1.1847) | Xent 0.0858(0.0891) | Loss 1.2326(1.2292) | Error 0.0272(0.0278) Steps 500(496.12) | Grad Norm 4.4896(2.4509) | Total Time 10.00(10.00)\n",
      "Iter 1582 | Time 42.5015(41.5952) | Bit/dim 1.1886(1.1848) | Xent 0.1018(0.0894) | Loss 1.2395(1.2295) | Error 0.0301(0.0278) Steps 506(496.41) | Grad Norm 5.2527(2.5350) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0226 | Time 18.2428, Epoch Time 328.9225(291.2953), Bit/dim 1.1798(best: 1.1658), Xent 0.0407, Loss 1.2001, Error 0.0139(best: 0.0135)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1583 | Time 42.0557(41.6091) | Bit/dim 1.1908(1.1850) | Xent 0.1040(0.0899) | Loss 1.2428(1.2299) | Error 0.0305(0.0279) Steps 500(496.52) | Grad Norm 4.7445(2.6013) | Total Time 10.00(10.00)\n",
      "Iter 1584 | Time 42.1659(41.6258) | Bit/dim 1.1829(1.1849) | Xent 0.0793(0.0896) | Loss 1.2225(1.2297) | Error 0.0249(0.0278) Steps 500(496.63) | Grad Norm 2.6559(2.6029) | Total Time 10.00(10.00)\n",
      "Iter 1585 | Time 42.0975(41.6399) | Bit/dim 1.1850(1.1849) | Xent 0.0789(0.0892) | Loss 1.2245(1.2296) | Error 0.0246(0.0277) Steps 500(496.73) | Grad Norm 0.4638(2.5387) | Total Time 10.00(10.00)\n",
      "Iter 1586 | Time 42.3767(41.6620) | Bit/dim 1.1852(1.1849) | Xent 0.0807(0.0890) | Loss 1.2255(1.2294) | Error 0.0248(0.0277) Steps 500(496.83) | Grad Norm 2.0538(2.5242) | Total Time 10.00(10.00)\n",
      "Iter 1587 | Time 43.3998(41.7141) | Bit/dim 1.1791(1.1848) | Xent 0.0860(0.0889) | Loss 1.2220(1.2292) | Error 0.0256(0.0276) Steps 500(496.92) | Grad Norm 3.7998(2.5624) | Total Time 10.00(10.00)\n",
      "Iter 1588 | Time 40.5880(41.6804) | Bit/dim 1.1864(1.1848) | Xent 0.0867(0.0888) | Loss 1.2297(1.2292) | Error 0.0260(0.0275) Steps 500(497.01) | Grad Norm 4.9126(2.6329) | Total Time 10.00(10.00)\n",
      "Iter 1589 | Time 42.3338(41.7000) | Bit/dim 1.1853(1.1848) | Xent 0.0933(0.0890) | Loss 1.2319(1.2293) | Error 0.0282(0.0276) Steps 500(497.10) | Grad Norm 5.2832(2.7125) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0227 | Time 18.1808, Epoch Time 325.8371(292.3316), Bit/dim 1.1827(best: 1.1658), Xent 0.0431, Loss 1.2043, Error 0.0145(best: 0.0135)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1590 | Time 43.0093(41.7392) | Bit/dim 1.1892(1.1850) | Xent 0.0855(0.0889) | Loss 1.2319(1.2294) | Error 0.0256(0.0275) Steps 500(497.19) | Grad Norm 4.8807(2.7775) | Total Time 10.00(10.00)\n",
      "Iter 1591 | Time 42.5849(41.7646) | Bit/dim 1.1803(1.1848) | Xent 0.0924(0.0890) | Loss 1.2265(1.2293) | Error 0.0284(0.0275) Steps 500(497.27) | Grad Norm 3.3610(2.7950) | Total Time 10.00(10.00)\n",
      "Iter 1592 | Time 42.7296(41.7936) | Bit/dim 1.1870(1.1849) | Xent 0.0892(0.0890) | Loss 1.2316(1.2294) | Error 0.0299(0.0276) Steps 500(497.36) | Grad Norm 1.5962(2.7590) | Total Time 10.00(10.00)\n",
      "Iter 1593 | Time 42.6486(41.8192) | Bit/dim 1.1820(1.1848) | Xent 0.0945(0.0891) | Loss 1.2293(1.2294) | Error 0.0292(0.0277) Steps 500(497.43) | Grad Norm 0.9046(2.7034) | Total Time 10.00(10.00)\n",
      "Iter 1594 | Time 41.3109(41.8040) | Bit/dim 1.1903(1.1850) | Xent 0.0957(0.0893) | Loss 1.2382(1.2296) | Error 0.0290(0.0277) Steps 494(497.33) | Grad Norm 2.1236(2.6860) | Total Time 10.00(10.00)\n",
      "Iter 1595 | Time 41.0677(41.7819) | Bit/dim 1.1938(1.1852) | Xent 0.0825(0.0891) | Loss 1.2351(1.2298) | Error 0.0244(0.0276) Steps 494(497.23) | Grad Norm 3.2278(2.7023) | Total Time 10.00(10.00)\n",
      "Iter 1596 | Time 42.2659(41.7964) | Bit/dim 1.1946(1.1855) | Xent 0.0815(0.0889) | Loss 1.2353(1.2300) | Error 0.0254(0.0275) Steps 500(497.31) | Grad Norm 3.8681(2.7372) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0228 | Time 18.3546, Epoch Time 326.5576(293.3584), Bit/dim 1.1868(best: 1.1658), Xent 0.0427, Loss 1.2081, Error 0.0148(best: 0.0135)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1597 | Time 41.3825(41.7840) | Bit/dim 1.1967(1.1858) | Xent 0.0983(0.0892) | Loss 1.2459(1.2304) | Error 0.0292(0.0276) Steps 500(497.40) | Grad Norm 3.6112(2.7635) | Total Time 10.00(10.00)\n",
      "Iter 1598 | Time 42.3073(41.7997) | Bit/dim 1.1864(1.1859) | Xent 0.0875(0.0891) | Loss 1.2301(1.2304) | Error 0.0261(0.0275) Steps 500(497.47) | Grad Norm 1.7300(2.7325) | Total Time 10.00(10.00)\n",
      "Iter 1599 | Time 41.9336(41.8037) | Bit/dim 1.1880(1.1859) | Xent 0.0871(0.0891) | Loss 1.2316(1.2305) | Error 0.0270(0.0275) Steps 494(497.37) | Grad Norm 1.3104(2.6898) | Total Time 10.00(10.00)\n",
      "Iter 1600 | Time 40.5905(41.7673) | Bit/dim 1.1888(1.1860) | Xent 0.0739(0.0886) | Loss 1.2258(1.2303) | Error 0.0229(0.0274) Steps 494(497.27) | Grad Norm 2.6007(2.6871) | Total Time 10.00(10.00)\n",
      "Iter 1601 | Time 41.9806(41.7737) | Bit/dim 1.1889(1.1861) | Xent 0.0765(0.0883) | Loss 1.2272(1.2302) | Error 0.0229(0.0272) Steps 500(497.35) | Grad Norm 5.3561(2.7672) | Total Time 10.00(10.00)\n",
      "Iter 1602 | Time 41.6356(41.7696) | Bit/dim 1.2051(1.1867) | Xent 0.0856(0.0882) | Loss 1.2479(1.2308) | Error 0.0270(0.0272) Steps 500(497.43) | Grad Norm 6.8721(2.8903) | Total Time 10.00(10.00)\n",
      "Iter 1603 | Time 41.9896(41.7762) | Bit/dim 1.1926(1.1868) | Xent 0.0885(0.0882) | Loss 1.2369(1.2309) | Error 0.0281(0.0273) Steps 500(497.51) | Grad Norm 4.4499(2.9371) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0229 | Time 18.3788, Epoch Time 322.6731(294.2378), Bit/dim 1.1833(best: 1.1658), Xent 0.0454, Loss 1.2060, Error 0.0154(best: 0.0135)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1604 | Time 40.9685(41.7519) | Bit/dim 1.1919(1.1870) | Xent 0.0913(0.0883) | Loss 1.2375(1.2311) | Error 0.0282(0.0273) Steps 500(497.58) | Grad Norm 1.5722(2.8962) | Total Time 10.00(10.00)\n",
      "Iter 1605 | Time 41.4229(41.7421) | Bit/dim 1.1900(1.1871) | Xent 0.0770(0.0879) | Loss 1.2285(1.2311) | Error 0.0251(0.0272) Steps 500(497.65) | Grad Norm 1.4958(2.8542) | Total Time 10.00(10.00)\n",
      "Iter 1606 | Time 42.7185(41.7714) | Bit/dim 1.1903(1.1872) | Xent 0.0837(0.0878) | Loss 1.2322(1.2311) | Error 0.0255(0.0272) Steps 500(497.72) | Grad Norm 4.4956(2.9034) | Total Time 10.00(10.00)\n",
      "Iter 1607 | Time 42.5434(41.7945) | Bit/dim 1.2027(1.1877) | Xent 0.0849(0.0877) | Loss 1.2451(1.2315) | Error 0.0260(0.0271) Steps 494(497.61) | Grad Norm 6.6846(3.0168) | Total Time 10.00(10.00)\n",
      "Iter 1608 | Time 42.9786(41.8300) | Bit/dim 1.1902(1.1877) | Xent 0.0861(0.0877) | Loss 1.2333(1.2316) | Error 0.0264(0.0271) Steps 512(498.04) | Grad Norm 5.4956(3.0912) | Total Time 10.00(10.00)\n",
      "Iter 1609 | Time 41.9394(41.8333) | Bit/dim 1.1935(1.1879) | Xent 0.0907(0.0878) | Loss 1.2388(1.2318) | Error 0.0269(0.0271) Steps 506(498.28) | Grad Norm 3.5516(3.1050) | Total Time 10.00(10.00)\n",
      "Iter 1610 | Time 43.7875(41.8919) | Bit/dim 1.1962(1.1882) | Xent 0.0942(0.0880) | Loss 1.2433(1.2321) | Error 0.0276(0.0271) Steps 512(498.69) | Grad Norm 2.0184(3.0724) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0230 | Time 18.5546, Epoch Time 327.4449(295.2340), Bit/dim 1.1849(best: 1.1658), Xent 0.0422, Loss 1.2060, Error 0.0142(best: 0.0135)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1611 | Time 43.3578(41.9359) | Bit/dim 1.1916(1.1883) | Xent 0.0960(0.0882) | Loss 1.2396(1.2324) | Error 0.0300(0.0272) Steps 506(498.91) | Grad Norm 1.0774(3.0126) | Total Time 10.00(10.00)\n",
      "Iter 1612 | Time 43.4666(41.9818) | Bit/dim 1.1901(1.1883) | Xent 0.0791(0.0879) | Loss 1.2296(1.2323) | Error 0.0261(0.0272) Steps 512(499.31) | Grad Norm 1.3953(2.9640) | Total Time 10.00(10.00)\n",
      "Iter 1613 | Time 43.6716(42.0325) | Bit/dim 1.1984(1.1886) | Xent 0.0907(0.0880) | Loss 1.2437(1.2326) | Error 0.0299(0.0273) Steps 512(499.69) | Grad Norm 3.5190(2.9807) | Total Time 10.00(10.00)\n",
      "Iter 1614 | Time 42.1618(42.0364) | Bit/dim 1.1979(1.1889) | Xent 0.0841(0.0879) | Loss 1.2399(1.2328) | Error 0.0251(0.0272) Steps 506(499.88) | Grad Norm 5.0655(3.0432) | Total Time 10.00(10.00)\n",
      "Iter 1615 | Time 44.1412(42.0996) | Bit/dim 1.1958(1.1891) | Xent 0.0886(0.0879) | Loss 1.2401(1.2331) | Error 0.0275(0.0272) Steps 512(500.24) | Grad Norm 5.2933(3.1107) | Total Time 10.00(10.00)\n",
      "Iter 1616 | Time 43.4098(42.1389) | Bit/dim 1.2031(1.1895) | Xent 0.0821(0.0877) | Loss 1.2441(1.2334) | Error 0.0216(0.0270) Steps 512(500.59) | Grad Norm 4.7337(3.1594) | Total Time 10.00(10.00)\n",
      "Iter 1617 | Time 43.5348(42.1807) | Bit/dim 1.1906(1.1895) | Xent 0.0931(0.0879) | Loss 1.2371(1.2335) | Error 0.0291(0.0271) Steps 512(500.94) | Grad Norm 3.8672(3.1807) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0231 | Time 18.4991, Epoch Time 335.0694(296.4291), Bit/dim 1.1883(best: 1.1658), Xent 0.0450, Loss 1.2108, Error 0.0147(best: 0.0135)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1618 | Time 42.4592(42.1891) | Bit/dim 1.1947(1.1897) | Xent 0.0795(0.0876) | Loss 1.2345(1.2335) | Error 0.0232(0.0270) Steps 512(501.27) | Grad Norm 2.6317(3.1642) | Total Time 10.00(10.00)\n",
      "Iter 1619 | Time 43.6073(42.2316) | Bit/dim 1.1942(1.1898) | Xent 0.0835(0.0875) | Loss 1.2359(1.2336) | Error 0.0261(0.0270) Steps 512(501.59) | Grad Norm 1.0967(3.1022) | Total Time 10.00(10.00)\n",
      "Iter 1620 | Time 42.1133(42.2281) | Bit/dim 1.2017(1.1902) | Xent 0.0868(0.0875) | Loss 1.2451(1.2339) | Error 0.0256(0.0269) Steps 512(501.90) | Grad Norm 1.0182(3.0397) | Total Time 10.00(10.00)\n",
      "Iter 1621 | Time 44.2959(42.2901) | Bit/dim 1.1963(1.1904) | Xent 0.0920(0.0876) | Loss 1.2423(1.2342) | Error 0.0300(0.0270) Steps 518(502.38) | Grad Norm 2.0860(3.0110) | Total Time 10.00(10.00)\n",
      "Iter 1622 | Time 43.1365(42.3155) | Bit/dim 1.1925(1.1904) | Xent 0.0973(0.0879) | Loss 1.2411(1.2344) | Error 0.0319(0.0272) Steps 512(502.67) | Grad Norm 2.9243(3.0084) | Total Time 10.00(10.00)\n",
      "Iter 1623 | Time 43.4081(42.3483) | Bit/dim 1.1936(1.1905) | Xent 0.0832(0.0878) | Loss 1.2352(1.2344) | Error 0.0252(0.0271) Steps 506(502.77) | Grad Norm 3.8187(3.0328) | Total Time 10.00(10.00)\n",
      "Iter 1624 | Time 43.9129(42.3952) | Bit/dim 1.2003(1.1908) | Xent 0.0918(0.0879) | Loss 1.2462(1.2348) | Error 0.0266(0.0271) Steps 512(503.05) | Grad Norm 3.8684(3.0578) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0232 | Time 18.1053, Epoch Time 333.8666(297.5522), Bit/dim 1.1946(best: 1.1658), Xent 0.0412, Loss 1.2153, Error 0.0144(best: 0.0135)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1625 | Time 43.5138(42.4288) | Bit/dim 1.2043(1.1912) | Xent 0.0805(0.0877) | Loss 1.2445(1.2351) | Error 0.0248(0.0270) Steps 506(503.14) | Grad Norm 3.6294(3.0750) | Total Time 10.00(10.00)\n",
      "Iter 1626 | Time 45.1205(42.5095) | Bit/dim 1.1959(1.1914) | Xent 0.0848(0.0876) | Loss 1.2383(1.2352) | Error 0.0268(0.0270) Steps 518(503.58) | Grad Norm 2.8339(3.0677) | Total Time 10.00(10.00)\n",
      "Iter 1627 | Time 44.5236(42.5700) | Bit/dim 1.2033(1.1917) | Xent 0.0767(0.0873) | Loss 1.2416(1.2354) | Error 0.0252(0.0270) Steps 518(504.02) | Grad Norm 2.3784(3.0471) | Total Time 10.00(10.00)\n",
      "Iter 1628 | Time 43.9033(42.6100) | Bit/dim 1.1999(1.1920) | Xent 0.0814(0.0871) | Loss 1.2406(1.2355) | Error 0.0252(0.0269) Steps 518(504.44) | Grad Norm 1.9920(3.0154) | Total Time 10.00(10.00)\n",
      "Iter 1629 | Time 44.1552(42.6563) | Bit/dim 1.2036(1.1923) | Xent 0.0933(0.0873) | Loss 1.2503(1.2360) | Error 0.0285(0.0270) Steps 518(504.84) | Grad Norm 1.7273(2.9768) | Total Time 10.00(10.00)\n",
      "Iter 1630 | Time 44.1483(42.7011) | Bit/dim 1.1967(1.1925) | Xent 0.0891(0.0873) | Loss 1.2412(1.2361) | Error 0.0280(0.0270) Steps 512(505.06) | Grad Norm 1.4540(2.9311) | Total Time 10.00(10.00)\n",
      "Iter 1631 | Time 44.7267(42.7618) | Bit/dim 1.2025(1.1928) | Xent 0.0857(0.0873) | Loss 1.2454(1.2364) | Error 0.0264(0.0270) Steps 512(505.27) | Grad Norm 1.3428(2.8834) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0233 | Time 18.6170, Epoch Time 341.4150(298.8681), Bit/dim 1.1935(best: 1.1658), Xent 0.0440, Loss 1.2155, Error 0.0150(best: 0.0135)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1632 | Time 43.8749(42.7952) | Bit/dim 1.2027(1.1931) | Xent 0.0902(0.0874) | Loss 1.2478(1.2367) | Error 0.0281(0.0270) Steps 512(505.47) | Grad Norm 1.2974(2.8358) | Total Time 10.00(10.00)\n",
      "Iter 1633 | Time 44.4167(42.8439) | Bit/dim 1.2037(1.1934) | Xent 0.0855(0.0873) | Loss 1.2464(1.2370) | Error 0.0269(0.0270) Steps 512(505.66) | Grad Norm 1.2510(2.7883) | Total Time 10.00(10.00)\n",
      "Iter 1634 | Time 44.7931(42.9024) | Bit/dim 1.2034(1.1937) | Xent 0.0900(0.0874) | Loss 1.2484(1.2374) | Error 0.0274(0.0270) Steps 518(506.03) | Grad Norm 1.1888(2.7403) | Total Time 10.00(10.00)\n",
      "Iter 1635 | Time 44.2909(42.9440) | Bit/dim 1.2006(1.1939) | Xent 0.0889(0.0874) | Loss 1.2451(1.2376) | Error 0.0286(0.0271) Steps 518(506.39) | Grad Norm 1.8399(2.7133) | Total Time 10.00(10.00)\n",
      "Iter 1636 | Time 44.4625(42.9896) | Bit/dim 1.1974(1.1940) | Xent 0.0806(0.0872) | Loss 1.2377(1.2376) | Error 0.0245(0.0270) Steps 518(506.74) | Grad Norm 2.7591(2.7147) | Total Time 10.00(10.00)\n",
      "Iter 1637 | Time 43.9735(43.0191) | Bit/dim 1.2079(1.1944) | Xent 0.0898(0.0873) | Loss 1.2528(1.2381) | Error 0.0288(0.0270) Steps 518(507.08) | Grad Norm 4.0528(2.7548) | Total Time 10.00(10.00)\n",
      "Iter 1638 | Time 45.5098(43.0938) | Bit/dim 1.2072(1.1948) | Xent 0.0867(0.0873) | Loss 1.2506(1.2384) | Error 0.0262(0.0270) Steps 530(507.77) | Grad Norm 4.9926(2.8220) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0234 | Time 18.8947, Epoch Time 343.2391(300.1992), Bit/dim 1.2048(best: 1.1658), Xent 0.0438, Loss 1.2267, Error 0.0149(best: 0.0135)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1639 | Time 43.6519(43.1106) | Bit/dim 1.2082(1.1952) | Xent 0.0885(0.0873) | Loss 1.2525(1.2389) | Error 0.0261(0.0270) Steps 518(508.07) | Grad Norm 6.0346(2.9183) | Total Time 10.00(10.00)\n",
      "Iter 1640 | Time 46.8651(43.2232) | Bit/dim 1.2116(1.1957) | Xent 0.0873(0.0873) | Loss 1.2552(1.2393) | Error 0.0286(0.0270) Steps 536(508.91) | Grad Norm 6.4796(3.0252) | Total Time 10.00(10.00)\n",
      "Iter 1641 | Time 44.9522(43.2751) | Bit/dim 1.2182(1.1964) | Xent 0.0728(0.0869) | Loss 1.2546(1.2398) | Error 0.0244(0.0270) Steps 524(509.36) | Grad Norm 5.9404(3.1126) | Total Time 10.00(10.00)\n",
      "Iter 1642 | Time 44.4394(43.3100) | Bit/dim 1.2138(1.1969) | Xent 0.0989(0.0872) | Loss 1.2633(1.2405) | Error 0.0304(0.0271) Steps 518(509.62) | Grad Norm 3.5578(3.1260) | Total Time 10.00(10.00)\n",
      "Iter 1643 | Time 43.5800(43.3181) | Bit/dim 1.2137(1.1974) | Xent 0.0864(0.0872) | Loss 1.2569(1.2410) | Error 0.0268(0.0270) Steps 518(509.87) | Grad Norm 1.1891(3.0679) | Total Time 10.00(10.00)\n",
      "Iter 1644 | Time 44.4428(43.3518) | Bit/dim 1.2194(1.1981) | Xent 0.0930(0.0874) | Loss 1.2659(1.2418) | Error 0.0295(0.0271) Steps 524(510.30) | Grad Norm 1.7922(3.0296) | Total Time 10.00(10.00)\n",
      "Iter 1645 | Time 46.5482(43.4477) | Bit/dim 1.2180(1.1987) | Xent 0.0902(0.0875) | Loss 1.2631(1.2424) | Error 0.0284(0.0272) Steps 530(510.89) | Grad Norm 2.9690(3.0278) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0235 | Time 19.0687, Epoch Time 346.2010(301.5793), Bit/dim 1.2147(best: 1.1658), Xent 0.0414, Loss 1.2354, Error 0.0143(best: 0.0135)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1646 | Time 45.8992(43.5213) | Bit/dim 1.2210(1.1993) | Xent 0.0892(0.0875) | Loss 1.2655(1.2431) | Error 0.0262(0.0271) Steps 524(511.28) | Grad Norm 3.4245(3.0397) | Total Time 10.00(10.00)\n",
      "Iter 1647 | Time 45.9454(43.5940) | Bit/dim 1.2193(1.1999) | Xent 0.0974(0.0878) | Loss 1.2680(1.2438) | Error 0.0300(0.0272) Steps 530(511.84) | Grad Norm 3.6644(3.0584) | Total Time 10.00(10.00)\n",
      "Iter 1648 | Time 45.4001(43.6482) | Bit/dim 1.2278(1.2008) | Xent 0.0868(0.0878) | Loss 1.2712(1.2447) | Error 0.0278(0.0272) Steps 542(512.75) | Grad Norm 3.9793(3.0861) | Total Time 10.00(10.00)\n",
      "Iter 1649 | Time 46.7744(43.7420) | Bit/dim 1.2357(1.2018) | Xent 0.0812(0.0876) | Loss 1.2763(1.2456) | Error 0.0251(0.0272) Steps 542(513.63) | Grad Norm 4.9762(3.1428) | Total Time 10.00(10.00)\n",
      "Iter 1650 | Time 46.3381(43.8198) | Bit/dim 1.2349(1.2028) | Xent 0.0838(0.0875) | Loss 1.2769(1.2465) | Error 0.0259(0.0271) Steps 530(514.12) | Grad Norm 5.3242(3.2082) | Total Time 10.00(10.00)\n",
      "Iter 1651 | Time 47.5642(43.9322) | Bit/dim 1.2364(1.2038) | Xent 0.0992(0.0878) | Loss 1.2860(1.2477) | Error 0.0295(0.0272) Steps 536(514.77) | Grad Norm 4.7478(3.2544) | Total Time 10.00(10.00)\n",
      "Iter 1652 | Time 46.6506(44.0137) | Bit/dim 1.2415(1.2049) | Xent 0.0862(0.0878) | Loss 1.2846(1.2488) | Error 0.0270(0.0272) Steps 542(515.59) | Grad Norm 3.9907(3.2765) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0236 | Time 19.7470, Epoch Time 356.8481(303.2373), Bit/dim 1.2377(best: 1.1658), Xent 0.0472, Loss 1.2613, Error 0.0155(best: 0.0135)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1653 | Time 48.8261(44.1581) | Bit/dim 1.2434(1.2061) | Xent 0.0761(0.0874) | Loss 1.2814(1.2498) | Error 0.0252(0.0271) Steps 548(516.56) | Grad Norm 1.9907(3.2379) | Total Time 10.00(10.00)\n",
      "Iter 1654 | Time 46.7995(44.2373) | Bit/dim 1.2624(1.2078) | Xent 0.0975(0.0877) | Loss 1.3111(1.2516) | Error 0.0299(0.0272) Steps 548(517.51) | Grad Norm 1.4315(3.1837) | Total Time 10.00(10.00)\n",
      "Iter 1655 | Time 48.5096(44.3655) | Bit/dim 1.2651(1.2095) | Xent 0.0867(0.0877) | Loss 1.3085(1.2534) | Error 0.0265(0.0272) Steps 554(518.60) | Grad Norm 3.2261(3.1850) | Total Time 10.00(10.00)\n",
      "Iter 1656 | Time 48.9926(44.5043) | Bit/dim 1.2810(1.2116) | Xent 0.0911(0.0878) | Loss 1.3265(1.2555) | Error 0.0278(0.0272) Steps 566(520.02) | Grad Norm 3.5717(3.1966) | Total Time 10.00(10.00)\n",
      "Iter 1657 | Time 51.9423(44.7275) | Bit/dim 1.3130(1.2147) | Xent 0.0979(0.0881) | Loss 1.3620(1.2587) | Error 0.0296(0.0273) Steps 566(521.40) | Grad Norm 3.1676(3.1957) | Total Time 10.00(10.00)\n",
      "Iter 1658 | Time 53.6089(44.9939) | Bit/dim 1.3403(1.2185) | Xent 0.1018(0.0885) | Loss 1.3912(1.2627) | Error 0.0319(0.0274) Steps 596(523.64) | Grad Norm 2.0978(3.1628) | Total Time 10.00(10.00)\n",
      "Iter 1659 | Time 54.1719(45.2692) | Bit/dim 1.3829(1.2234) | Xent 0.1142(0.0893) | Loss 1.4400(1.2680) | Error 0.0339(0.0276) Steps 596(525.81) | Grad Norm 3.2024(3.1640) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0237 | Time 21.3771, Epoch Time 386.9315(305.7482), Bit/dim 1.4225(best: 1.1658), Xent 0.0650, Loss 1.4550, Error 0.0186(best: 0.0135)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1660 | Time 56.4240(45.6039) | Bit/dim 1.4293(1.2296) | Xent 0.1272(0.0904) | Loss 1.4929(1.2748) | Error 0.0406(0.0280) Steps 620(528.64) | Grad Norm 5.4114(3.2314) | Total Time 10.00(10.00)\n",
      "Iter 1661 | Time 59.1574(46.0105) | Bit/dim 1.6505(1.2422) | Xent 0.1867(0.0933) | Loss 1.7439(1.2889) | Error 0.0539(0.0288) Steps 638(531.92) | Grad Norm 19.6792(3.7248) | Total Time 10.00(10.00)\n",
      "Iter 1662 | Time 54.9876(46.2798) | Bit/dim 1.6537(1.2545) | Xent 1.1014(0.1236) | Loss 2.2044(1.3163) | Error 0.3516(0.0385) Steps 602(534.02) | Grad Norm 36.1607(4.6979) | Total Time 10.00(10.00)\n",
      "Iter 1663 | Time 59.1219(46.6651) | Bit/dim 1.7605(1.2697) | Xent 1.2612(0.1577) | Loss 2.3911(1.3486) | Error 0.2696(0.0454) Steps 626(536.78) | Grad Norm 40.9610(5.7858) | Total Time 10.00(10.00)\n",
      "Iter 1664 | Time 59.1104(47.0384) | Bit/dim 2.0089(1.2919) | Xent 0.6851(0.1735) | Loss 2.3514(1.3786) | Error 0.1295(0.0479) Steps 620(539.28) | Grad Norm 50.6586(7.1320) | Total Time 10.00(10.00)\n",
      "Iter 1665 | Time 59.3160(47.4068) | Bit/dim 1.7999(1.3071) | Xent 0.1994(0.1743) | Loss 1.8996(1.3943) | Error 0.0584(0.0482) Steps 620(541.70) | Grad Norm 10.1930(7.2238) | Total Time 10.00(10.00)\n",
      "Iter 1666 | Time 58.0878(47.7272) | Bit/dim 1.7781(1.3213) | Xent 0.2878(0.1777) | Loss 1.9221(1.4101) | Error 0.0945(0.0496) Steps 608(543.69) | Grad Norm 8.5205(7.2627) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0238 | Time 20.6558, Epoch Time 439.4790(309.7601), Bit/dim 1.7243(best: 1.1658), Xent 0.3189, Loss 1.8837, Error 0.0918(best: 0.0135)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1667 | Time 55.2217(47.9520) | Bit/dim 1.7295(1.3335) | Xent 0.5149(0.1878) | Loss 1.9870(1.4274) | Error 0.1695(0.0532) Steps 596(545.26) | Grad Norm 16.0782(7.5272) | Total Time 10.00(10.00)\n",
      "Iter 1668 | Time 56.3939(48.2053) | Bit/dim 1.6460(1.3429) | Xent 0.4639(0.1961) | Loss 1.8779(1.4409) | Error 0.1564(0.0563) Steps 584(546.42) | Grad Norm 10.3730(7.6126) | Total Time 10.00(10.00)\n",
      "Iter 1669 | Time 57.7937(48.4929) | Bit/dim 1.6415(1.3518) | Xent 0.3470(0.2006) | Loss 1.8150(1.4522) | Error 0.1160(0.0581) Steps 602(548.09) | Grad Norm 8.6103(7.6425) | Total Time 10.00(10.00)\n",
      "Iter 1670 | Time 55.8452(48.7135) | Bit/dim 1.6567(1.3610) | Xent 0.3146(0.2040) | Loss 1.8140(1.4630) | Error 0.1021(0.0594) Steps 608(549.88) | Grad Norm 8.0857(7.6558) | Total Time 10.00(10.00)\n",
      "Iter 1671 | Time 61.3945(49.0939) | Bit/dim 1.6953(1.3710) | Xent 0.3652(0.2089) | Loss 1.8779(1.4755) | Error 0.1099(0.0609) Steps 644(552.71) | Grad Norm 8.2778(7.6744) | Total Time 10.00(10.00)\n",
      "Iter 1672 | Time 63.9739(49.5403) | Bit/dim 1.8160(1.3844) | Xent 0.4662(0.2166) | Loss 2.0491(1.4927) | Error 0.1409(0.0633) Steps 674(556.35) | Grad Norm 8.7697(7.7073) | Total Time 10.00(10.00)\n",
      "Iter 1673 | Time 69.1283(50.1280) | Bit/dim 1.8751(1.3991) | Xent 0.5318(0.2260) | Loss 2.1410(1.5121) | Error 0.1685(0.0665) Steps 680(560.06) | Grad Norm 11.8936(7.8329) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0239 | Time 23.2688, Epoch Time 455.8453(314.1426), Bit/dim 1.7993(best: 1.1658), Xent 0.2048, Loss 1.9017, Error 0.0704(best: 0.0135)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1674 | Time 64.6349(50.5632) | Bit/dim 1.8082(1.4114) | Xent 0.3594(0.2300) | Loss 1.9879(1.5264) | Error 0.1160(0.0680) Steps 674(563.47) | Grad Norm 7.3150(7.8173) | Total Time 10.00(10.00)\n",
      "Iter 1675 | Time 68.2530(51.0939) | Bit/dim 1.8080(1.4233) | Xent 0.2940(0.2320) | Loss 1.9550(1.5392) | Error 0.0903(0.0686) Steps 692(567.33) | Grad Norm 5.7117(7.7542) | Total Time 10.00(10.00)\n",
      "Iter 1676 | Time 67.9254(51.5988) | Bit/dim 1.8566(1.4363) | Xent 0.3020(0.2341) | Loss 2.0076(1.5533) | Error 0.0945(0.0694) Steps 692(571.07) | Grad Norm 9.0643(7.7935) | Total Time 10.00(10.00)\n",
      "Iter 1677 | Time 69.5030(52.1359) | Bit/dim 1.8578(1.4489) | Xent 0.2794(0.2354) | Loss 1.9975(1.5666) | Error 0.0904(0.0701) Steps 710(575.24) | Grad Norm 6.8006(7.7637) | Total Time 10.00(10.00)\n",
      "Iter 1678 | Time 68.9990(52.6418) | Bit/dim 1.8483(1.4609) | Xent 0.2955(0.2372) | Loss 1.9960(1.5795) | Error 0.0951(0.0708) Steps 698(578.92) | Grad Norm 6.8246(7.7355) | Total Time 10.00(10.00)\n",
      "Iter 1679 | Time 71.5419(53.2088) | Bit/dim 1.8414(1.4723) | Xent 0.2696(0.2382) | Loss 1.9762(1.5914) | Error 0.0894(0.0714) Steps 698(582.49) | Grad Norm 4.9212(7.6511) | Total Time 10.00(10.00)\n",
      "Iter 1680 | Time 72.0686(53.7746) | Bit/dim 1.8630(1.4840) | Xent 0.3036(0.2402) | Loss 2.0148(1.6041) | Error 0.0972(0.0721) Steps 704(586.14) | Grad Norm 7.8992(7.6585) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0240 | Time 24.5697, Epoch Time 520.1843(320.3239), Bit/dim 1.8371(best: 1.1658), Xent 0.1264, Loss 1.9003, Error 0.0391(best: 0.0135)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1681 | Time 70.9798(54.2908) | Bit/dim 1.8511(1.4950) | Xent 0.2505(0.2405) | Loss 1.9764(1.6153) | Error 0.0804(0.0724) Steps 722(590.21) | Grad Norm 4.2395(7.5560) | Total Time 10.00(10.00)\n",
      "Iter 1682 | Time 74.3597(54.8928) | Bit/dim 1.8497(1.5057) | Xent 0.2427(0.2405) | Loss 1.9710(1.6259) | Error 0.0794(0.0726) Steps 728(594.35) | Grad Norm 4.6515(7.4688) | Total Time 10.00(10.00)\n",
      "Iter 1683 | Time 72.0760(55.4083) | Bit/dim 1.8629(1.5164) | Xent 0.2218(0.2400) | Loss 1.9738(1.6364) | Error 0.0674(0.0724) Steps 734(598.54) | Grad Norm 4.0648(7.3667) | Total Time 10.00(10.00)\n",
      "Iter 1684 | Time 73.0373(55.9372) | Bit/dim 1.8566(1.5266) | Xent 0.2032(0.2389) | Loss 1.9582(1.6460) | Error 0.0636(0.0722) Steps 728(602.42) | Grad Norm 5.0796(7.2981) | Total Time 10.00(10.00)\n",
      "Iter 1685 | Time 73.9723(56.4783) | Bit/dim 1.8238(1.5355) | Xent 0.2150(0.2382) | Loss 1.9313(1.6546) | Error 0.0641(0.0719) Steps 734(606.37) | Grad Norm 3.0190(7.1697) | Total Time 10.00(10.00)\n",
      "Iter 1686 | Time 73.7981(56.9979) | Bit/dim 1.8077(1.5437) | Xent 0.2163(0.2375) | Loss 1.9159(1.6624) | Error 0.0699(0.0719) Steps 740(610.38) | Grad Norm 4.3543(7.0853) | Total Time 10.00(10.00)\n",
      "Iter 1687 | Time 73.8881(57.5046) | Bit/dim 1.7897(1.5511) | Xent 0.2147(0.2368) | Loss 1.8971(1.6695) | Error 0.0677(0.0717) Steps 734(614.09) | Grad Norm 2.9247(6.9604) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0241 | Time 25.2492, Epoch Time 549.8875(327.2108), Bit/dim 1.7859(best: 1.1658), Xent 0.1028, Loss 1.8373, Error 0.0305(best: 0.0135)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1688 | Time 73.6845(57.9900) | Bit/dim 1.7957(1.5584) | Xent 0.1961(0.2356) | Loss 1.8938(1.6762) | Error 0.0580(0.0713) Steps 734(617.68) | Grad Norm 3.7391(6.8638) | Total Time 10.00(10.00)\n",
      "Iter 1689 | Time 75.0162(58.5008) | Bit/dim 1.7770(1.5650) | Xent 0.1811(0.2340) | Loss 1.8676(1.6819) | Error 0.0567(0.0709) Steps 746(621.53) | Grad Norm 2.5431(6.7342) | Total Time 10.00(10.00)\n",
      "Iter 1690 | Time 73.9716(58.9649) | Bit/dim 1.7634(1.5709) | Xent 0.1995(0.2329) | Loss 1.8632(1.6874) | Error 0.0613(0.0706) Steps 740(625.09) | Grad Norm 4.2800(6.6606) | Total Time 10.00(10.00)\n",
      "Iter 1691 | Time 76.4679(59.4900) | Bit/dim 1.7707(1.5769) | Xent 0.1797(0.2313) | Loss 1.8606(1.6926) | Error 0.0555(0.0702) Steps 752(628.89) | Grad Norm 1.9532(6.5193) | Total Time 10.00(10.00)\n",
      "Iter 1692 | Time 77.2607(60.0231) | Bit/dim 1.7634(1.5825) | Xent 0.1698(0.2295) | Loss 1.8483(1.6973) | Error 0.0529(0.0696) Steps 758(632.77) | Grad Norm 4.6224(6.4624) | Total Time 10.00(10.00)\n",
      "Iter 1693 | Time 79.5692(60.6095) | Bit/dim 1.7612(1.5879) | Xent 0.1880(0.2282) | Loss 1.8551(1.7020) | Error 0.0587(0.0693) Steps 764(636.70) | Grad Norm 3.9955(6.3884) | Total Time 10.00(10.00)\n",
      "Iter 1694 | Time 75.8066(61.0654) | Bit/dim 1.7603(1.5930) | Xent 0.1808(0.2268) | Loss 1.8507(1.7065) | Error 0.0557(0.0689) Steps 752(640.16) | Grad Norm 4.5164(6.3323) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0242 | Time 26.2293, Epoch Time 570.4792(334.5089), Bit/dim 1.7644(best: 1.1658), Xent 0.0929, Loss 1.8109, Error 0.0316(best: 0.0135)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1695 | Time 76.6806(61.5338) | Bit/dim 1.7714(1.5984) | Xent 0.1646(0.2250) | Loss 1.8537(1.7109) | Error 0.0554(0.0685) Steps 764(643.88) | Grad Norm 5.1279(6.2961) | Total Time 10.00(10.00)\n",
      "Iter 1696 | Time 77.0438(61.9991) | Bit/dim 1.7571(1.6032) | Xent 0.1700(0.2233) | Loss 1.8421(1.7148) | Error 0.0531(0.0680) Steps 764(647.48) | Grad Norm 2.7512(6.1898) | Total Time 10.00(10.00)\n",
      "Iter 1697 | Time 76.9961(62.4490) | Bit/dim 1.7504(1.6076) | Xent 0.1679(0.2216) | Loss 1.8343(1.7184) | Error 0.0495(0.0675) Steps 764(650.98) | Grad Norm 3.7231(6.1158) | Total Time 10.00(10.00)\n",
      "Iter 1698 | Time 76.7285(62.8774) | Bit/dim 1.7566(1.6120) | Xent 0.1462(0.2194) | Loss 1.8297(1.7217) | Error 0.0479(0.0669) Steps 764(654.37) | Grad Norm 4.4541(6.0659) | Total Time 10.00(10.00)\n",
      "Iter 1699 | Time 76.6163(63.2896) | Bit/dim 1.7356(1.6157) | Xent 0.1552(0.2175) | Loss 1.8132(1.7245) | Error 0.0489(0.0664) Steps 752(657.30) | Grad Norm 2.4598(5.9577) | Total Time 10.00(10.00)\n",
      "Iter 1700 | Time 75.3584(63.6517) | Bit/dim 1.7250(1.6190) | Xent 0.1592(0.2157) | Loss 1.8046(1.7269) | Error 0.0523(0.0659) Steps 746(659.96) | Grad Norm 4.6801(5.9194) | Total Time 10.00(10.00)\n",
      "Iter 1701 | Time 74.9269(63.9899) | Bit/dim 1.7279(1.6223) | Xent 0.1530(0.2138) | Loss 1.8044(1.7292) | Error 0.0497(0.0654) Steps 752(662.72) | Grad Norm 1.7258(5.7936) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0243 | Time 26.6964, Epoch Time 573.3900(341.6753), Bit/dim 1.7271(best: 1.1658), Xent 0.0769, Loss 1.7656, Error 0.0259(best: 0.0135)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1702 | Time 77.9024(64.4073) | Bit/dim 1.7382(1.6258) | Xent 0.1315(0.2114) | Loss 1.8040(1.7314) | Error 0.0424(0.0648) Steps 770(665.94) | Grad Norm 5.1312(5.7737) | Total Time 10.00(10.00)\n",
      "Iter 1703 | Time 78.8536(64.8407) | Bit/dim 1.7196(1.6286) | Xent 0.1449(0.2094) | Loss 1.7920(1.7333) | Error 0.0461(0.0642) Steps 764(668.88) | Grad Norm 3.8045(5.7147) | Total Time 10.00(10.00)\n",
      "Iter 1704 | Time 78.9774(65.2648) | Bit/dim 1.7239(1.6314) | Xent 0.1462(0.2075) | Loss 1.7970(1.7352) | Error 0.0457(0.0636) Steps 764(671.73) | Grad Norm 1.7419(5.5955) | Total Time 10.00(10.00)\n",
      "Iter 1705 | Time 78.4485(65.6603) | Bit/dim 1.7240(1.6342) | Xent 0.1460(0.2056) | Loss 1.7970(1.7370) | Error 0.0454(0.0631) Steps 764(674.50) | Grad Norm 4.0789(5.5500) | Total Time 10.00(10.00)\n",
      "Iter 1706 | Time 77.4601(66.0143) | Bit/dim 1.7109(1.6365) | Xent 0.1499(0.2039) | Loss 1.7858(1.7385) | Error 0.0449(0.0625) Steps 764(677.19) | Grad Norm 3.3182(5.4830) | Total Time 10.00(10.00)\n",
      "Iter 1707 | Time 77.5427(66.3601) | Bit/dim 1.7097(1.6387) | Xent 0.1425(0.2021) | Loss 1.7810(1.7398) | Error 0.0460(0.0620) Steps 752(679.43) | Grad Norm 1.7568(5.3712) | Total Time 10.00(10.00)\n",
      "Iter 1708 | Time 76.6793(66.6697) | Bit/dim 1.7105(1.6409) | Xent 0.1451(0.2004) | Loss 1.7830(1.7411) | Error 0.0434(0.0615) Steps 746(681.43) | Grad Norm 3.8234(5.3248) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0244 | Time 26.2500, Epoch Time 584.8259(348.9698), Bit/dim 1.6941(best: 1.1658), Xent 0.0678, Loss 1.7280, Error 0.0214(best: 0.0135)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1709 | Time 76.2200(66.9562) | Bit/dim 1.7016(1.6427) | Xent 0.1442(0.1987) | Loss 1.7737(1.7420) | Error 0.0464(0.0610) Steps 752(683.54) | Grad Norm 2.4880(5.2397) | Total Time 10.00(10.00)\n",
      "Iter 1710 | Time 78.1113(67.2909) | Bit/dim 1.7039(1.6445) | Xent 0.1460(0.1971) | Loss 1.7769(1.7431) | Error 0.0455(0.0606) Steps 758(685.78) | Grad Norm 1.8095(5.1368) | Total Time 10.00(10.00)\n",
      "Iter 1711 | Time 80.1404(67.6764) | Bit/dim 1.7030(1.6463) | Xent 0.1327(0.1952) | Loss 1.7693(1.7439) | Error 0.0443(0.0601) Steps 782(688.67) | Grad Norm 4.3408(5.1129) | Total Time 10.00(10.00)\n",
      "Iter 1712 | Time 82.0974(68.1090) | Bit/dim 1.6991(1.6479) | Xent 0.1501(0.1938) | Loss 1.7742(1.7448) | Error 0.0486(0.0597) Steps 794(691.83) | Grad Norm 5.8175(5.1341) | Total Time 10.00(10.00)\n",
      "Iter 1713 | Time 78.9857(68.4353) | Bit/dim 1.7024(1.6495) | Xent 0.1552(0.1927) | Loss 1.7800(1.7458) | Error 0.0466(0.0593) Steps 776(694.35) | Grad Norm 7.5745(5.2073) | Total Time 10.00(10.00)\n",
      "Iter 1714 | Time 82.1451(68.8466) | Bit/dim 1.6930(1.6508) | Xent 0.1489(0.1914) | Loss 1.7674(1.7465) | Error 0.0475(0.0590) Steps 782(696.98) | Grad Norm 6.4283(5.2439) | Total Time 10.00(10.00)\n",
      "Iter 1715 | Time 76.3354(69.0713) | Bit/dim 1.6806(1.6517) | Xent 0.1334(0.1896) | Loss 1.7473(1.7465) | Error 0.0425(0.0585) Steps 782(699.53) | Grad Norm 2.0779(5.1489) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0245 | Time 26.3126, Epoch Time 592.9592(356.2895), Bit/dim 1.6822(best: 1.1658), Xent 0.0655, Loss 1.7150, Error 0.0203(best: 0.0135)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1716 | Time 78.5599(69.3559) | Bit/dim 1.6849(1.6527) | Xent 0.1202(0.1875) | Loss 1.7450(1.7465) | Error 0.0371(0.0579) Steps 788(702.18) | Grad Norm 3.7545(5.1071) | Total Time 10.00(10.00)\n",
      "Iter 1717 | Time 78.9291(69.6431) | Bit/dim 1.6858(1.6537) | Xent 0.1479(0.1864) | Loss 1.7598(1.7469) | Error 0.0467(0.0575) Steps 782(704.58) | Grad Norm 5.8795(5.1303) | Total Time 10.00(10.00)\n",
      "Iter 1718 | Time 79.4992(69.9388) | Bit/dim 1.6942(1.6549) | Xent 0.1387(0.1849) | Loss 1.7635(1.7474) | Error 0.0425(0.0571) Steps 782(706.90) | Grad Norm 8.1940(5.2222) | Total Time 10.00(10.00)\n",
      "Iter 1719 | Time 82.2833(70.3091) | Bit/dim 1.6829(1.6557) | Xent 0.1535(0.1840) | Loss 1.7596(1.7477) | Error 0.0484(0.0568) Steps 800(709.69) | Grad Norm 7.3250(5.2853) | Total Time 10.00(10.00)\n",
      "Iter 1720 | Time 79.7852(70.5934) | Bit/dim 1.6714(1.6562) | Xent 0.1301(0.1824) | Loss 1.7364(1.7474) | Error 0.0395(0.0563) Steps 776(711.68) | Grad Norm 2.8633(5.2126) | Total Time 10.00(10.00)\n",
      "Iter 1721 | Time 78.5690(70.8327) | Bit/dim 1.6754(1.6568) | Xent 0.1276(0.1807) | Loss 1.7393(1.7471) | Error 0.0393(0.0558) Steps 776(713.61) | Grad Norm 3.1622(5.1511) | Total Time 10.00(10.00)\n",
      "Iter 1722 | Time 81.6542(71.1573) | Bit/dim 1.6725(1.6573) | Xent 0.1344(0.1793) | Loss 1.7396(1.7469) | Error 0.0417(0.0554) Steps 776(715.48) | Grad Norm 6.1736(5.1818) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0246 | Time 25.5959, Epoch Time 597.3785(363.5222), Bit/dim 1.6728(best: 1.1658), Xent 0.0668, Loss 1.7062, Error 0.0221(best: 0.0135)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1723 | Time 80.2623(71.4305) | Bit/dim 1.6799(1.6579) | Xent 0.1291(0.1778) | Loss 1.7444(1.7468) | Error 0.0389(0.0549) Steps 770(717.12) | Grad Norm 6.1840(5.2118) | Total Time 10.00(10.00)\n",
      "Iter 1724 | Time 79.5254(71.6733) | Bit/dim 1.6610(1.6580) | Xent 0.1167(0.1760) | Loss 1.7194(1.7460) | Error 0.0384(0.0544) Steps 794(719.43) | Grad Norm 2.7550(5.1381) | Total Time 10.00(10.00)\n",
      "Iter 1725 | Time 79.5578(71.9099) | Bit/dim 1.6610(1.6581) | Xent 0.1196(0.1743) | Loss 1.7207(1.7453) | Error 0.0380(0.0539) Steps 806(722.02) | Grad Norm 2.1246(5.0477) | Total Time 10.00(10.00)\n",
      "Iter 1726 | Time 80.1910(72.1583) | Bit/dim 1.6646(1.6583) | Xent 0.1352(0.1731) | Loss 1.7323(1.7449) | Error 0.0435(0.0536) Steps 770(723.46) | Grad Norm 5.3351(5.0563) | Total Time 10.00(10.00)\n",
      "Iter 1727 | Time 79.4931(72.3783) | Bit/dim 1.6591(1.6583) | Xent 0.1362(0.1720) | Loss 1.7272(1.7443) | Error 0.0415(0.0532) Steps 782(725.22) | Grad Norm 6.2296(5.0915) | Total Time 10.00(10.00)\n",
      "Iter 1728 | Time 80.1701(72.6121) | Bit/dim 1.6512(1.6581) | Xent 0.1279(0.1707) | Loss 1.7152(1.7435) | Error 0.0394(0.0528) Steps 776(726.74) | Grad Norm 2.9826(5.0283) | Total Time 10.00(10.00)\n",
      "Iter 1729 | Time 79.2352(72.8108) | Bit/dim 1.6535(1.6580) | Xent 0.1215(0.1692) | Loss 1.7142(1.7426) | Error 0.0381(0.0523) Steps 782(728.40) | Grad Norm 2.0661(4.9394) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0247 | Time 26.6329, Epoch Time 597.6409(370.5457), Bit/dim 1.6411(best: 1.1658), Xent 0.0603, Loss 1.6713, Error 0.0186(best: 0.0135)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1730 | Time 80.7387(73.0486) | Bit/dim 1.6501(1.6577) | Xent 0.1222(0.1678) | Loss 1.7113(1.7417) | Error 0.0395(0.0520) Steps 788(730.19) | Grad Norm 4.8749(4.9375) | Total Time 10.00(10.00)\n",
      "Iter 1731 | Time 79.0075(73.2274) | Bit/dim 1.6624(1.6579) | Xent 0.1313(0.1667) | Loss 1.7281(1.7412) | Error 0.0400(0.0516) Steps 776(731.56) | Grad Norm 6.7240(4.9911) | Total Time 10.00(10.00)\n",
      "Iter 1732 | Time 80.5910(73.4483) | Bit/dim 1.6526(1.6577) | Xent 0.1418(0.1660) | Loss 1.7235(1.7407) | Error 0.0439(0.0514) Steps 776(732.90) | Grad Norm 8.6940(5.1022) | Total Time 10.00(10.00)\n",
      "Iter 1733 | Time 80.7137(73.6662) | Bit/dim 1.6663(1.6580) | Xent 0.1375(0.1651) | Loss 1.7351(1.7405) | Error 0.0414(0.0511) Steps 776(734.19) | Grad Norm 11.3946(5.2909) | Total Time 10.00(10.00)\n",
      "Iter 1734 | Time 76.9398(73.7645) | Bit/dim 1.6467(1.6576) | Xent 0.1400(0.1644) | Loss 1.7167(1.7398) | Error 0.0445(0.0509) Steps 764(735.08) | Grad Norm 9.6648(5.4221) | Total Time 10.00(10.00)\n",
      "Iter 1735 | Time 80.3037(73.9606) | Bit/dim 1.6398(1.6571) | Xent 0.1189(0.1630) | Loss 1.6992(1.7386) | Error 0.0366(0.0504) Steps 764(735.95) | Grad Norm 1.5093(5.3048) | Total Time 10.00(10.00)\n",
      "Iter 1736 | Time 80.2671(74.1498) | Bit/dim 1.6462(1.6568) | Xent 0.1161(0.1616) | Loss 1.7042(1.7376) | Error 0.0379(0.0501) Steps 788(737.51) | Grad Norm 7.7697(5.3787) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0248 | Time 26.4677, Epoch Time 597.4665(377.3533), Bit/dim 1.6350(best: 1.1658), Xent 0.0606, Loss 1.6653, Error 0.0194(best: 0.0135)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1737 | Time 80.1012(74.3284) | Bit/dim 1.6412(1.6563) | Xent 0.1420(0.1610) | Loss 1.7122(1.7368) | Error 0.0443(0.0499) Steps 776(738.67) | Grad Norm 8.7102(5.4787) | Total Time 10.00(10.00)\n",
      "Iter 1738 | Time 77.7875(74.4321) | Bit/dim 1.6322(1.6556) | Xent 0.1150(0.1596) | Loss 1.6897(1.7354) | Error 0.0359(0.0495) Steps 770(739.61) | Grad Norm 2.7467(5.3967) | Total Time 10.00(10.00)\n",
      "Iter 1739 | Time 79.6289(74.5880) | Bit/dim 1.6365(1.6550) | Xent 0.1233(0.1585) | Loss 1.6982(1.7343) | Error 0.0387(0.0492) Steps 776(740.70) | Grad Norm 7.7163(5.4663) | Total Time 10.00(10.00)\n",
      "Iter 1740 | Time 79.8670(74.7464) | Bit/dim 1.6345(1.6544) | Xent 0.1365(0.1579) | Loss 1.7027(1.7333) | Error 0.0437(0.0490) Steps 770(741.58) | Grad Norm 8.7820(5.5658) | Total Time 10.00(10.00)\n",
      "Iter 1741 | Time 79.7852(74.8976) | Bit/dim 1.6298(1.6537) | Xent 0.1153(0.1566) | Loss 1.6874(1.7320) | Error 0.0350(0.0486) Steps 782(742.79) | Grad Norm 3.3035(5.4979) | Total Time 10.00(10.00)\n",
      "Iter 1742 | Time 79.2129(75.0270) | Bit/dim 1.6354(1.6531) | Xent 0.1188(0.1555) | Loss 1.6948(1.7308) | Error 0.0367(0.0482) Steps 794(744.33) | Grad Norm 5.3762(5.4942) | Total Time 10.00(10.00)\n",
      "Iter 1743 | Time 80.1800(75.1816) | Bit/dim 1.6360(1.6526) | Xent 0.1338(0.1548) | Loss 1.7029(1.7300) | Error 0.0413(0.0480) Steps 776(745.28) | Grad Norm 7.7587(5.5622) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0249 | Time 25.8882, Epoch Time 595.0035(383.8829), Bit/dim 1.6183(best: 1.1658), Xent 0.0590, Loss 1.6478, Error 0.0189(best: 0.0135)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1744 | Time 79.0462(75.2976) | Bit/dim 1.6258(1.6518) | Xent 0.1097(0.1535) | Loss 1.6806(1.7285) | Error 0.0329(0.0476) Steps 776(746.20) | Grad Norm 2.8107(5.4796) | Total Time 10.00(10.00)\n",
      "Iter 1745 | Time 77.9901(75.3783) | Bit/dim 1.6217(1.6509) | Xent 0.1177(0.1524) | Loss 1.6806(1.7271) | Error 0.0365(0.0472) Steps 776(747.09) | Grad Norm 6.1269(5.4990) | Total Time 10.00(10.00)\n",
      "Iter 1746 | Time 80.2080(75.5232) | Bit/dim 1.6290(1.6502) | Xent 0.1456(0.1522) | Loss 1.7018(1.7263) | Error 0.0441(0.0471) Steps 788(748.32) | Grad Norm 8.8314(5.5990) | Total Time 10.00(10.00)\n",
      "Iter 1747 | Time 81.0982(75.6905) | Bit/dim 1.6136(1.6491) | Xent 0.1174(0.1511) | Loss 1.6723(1.7247) | Error 0.0376(0.0468) Steps 788(749.51) | Grad Norm 2.4126(5.5034) | Total Time 10.00(10.00)\n",
      "Iter 1748 | Time 82.5590(75.8965) | Bit/dim 1.6293(1.6485) | Xent 0.1163(0.1501) | Loss 1.6875(1.7236) | Error 0.0359(0.0465) Steps 782(750.49) | Grad Norm 7.2710(5.5564) | Total Time 10.00(10.00)\n",
      "Iter 1749 | Time 78.1915(75.9654) | Bit/dim 1.6249(1.6478) | Xent 0.1368(0.1497) | Loss 1.6933(1.7227) | Error 0.0449(0.0465) Steps 770(751.07) | Grad Norm 9.2974(5.6687) | Total Time 10.00(10.00)\n",
      "Iter 1750 | Time 80.1532(76.0910) | Bit/dim 1.6189(1.6470) | Xent 0.1085(0.1485) | Loss 1.6732(1.7212) | Error 0.0346(0.0461) Steps 770(751.64) | Grad Norm 2.1454(5.5630) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0250 | Time 26.2112, Epoch Time 598.1640(390.3113), Bit/dim 1.6200(best: 1.1658), Xent 0.0615, Loss 1.6507, Error 0.0192(best: 0.0135)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1751 | Time 79.8086(76.2025) | Bit/dim 1.6307(1.6465) | Xent 0.1160(0.1475) | Loss 1.6887(1.7202) | Error 0.0346(0.0458) Steps 806(753.27) | Grad Norm 7.2532(5.6137) | Total Time 10.00(10.00)\n",
      "Iter 1752 | Time 82.6030(76.3946) | Bit/dim 1.6125(1.6455) | Xent 0.1254(0.1468) | Loss 1.6752(1.7189) | Error 0.0413(0.0456) Steps 776(753.95) | Grad Norm 7.8825(5.6818) | Total Time 10.00(10.00)\n",
      "Iter 1753 | Time 80.3948(76.5146) | Bit/dim 1.6103(1.6444) | Xent 0.1190(0.1460) | Loss 1.6698(1.7174) | Error 0.0364(0.0454) Steps 776(754.61) | Grad Norm 3.4442(5.6146) | Total Time 10.00(10.00)\n",
      "Iter 1754 | Time 78.7626(76.5820) | Bit/dim 1.6158(1.6435) | Xent 0.1185(0.1452) | Loss 1.6751(1.7161) | Error 0.0383(0.0451) Steps 794(755.79) | Grad Norm 8.1352(5.6902) | Total Time 10.00(10.00)\n",
      "Iter 1755 | Time 80.6000(76.7025) | Bit/dim 1.6137(1.6427) | Xent 0.1042(0.1439) | Loss 1.6658(1.7146) | Error 0.0319(0.0447) Steps 794(756.94) | Grad Norm 9.1865(5.7951) | Total Time 10.00(10.00)\n",
      "Iter 1756 | Time 80.3139(76.8109) | Bit/dim 1.6160(1.6419) | Xent 0.1181(0.1432) | Loss 1.6751(1.7134) | Error 0.0356(0.0445) Steps 782(757.69) | Grad Norm 5.0247(5.7720) | Total Time 10.00(10.00)\n",
      "Iter 1757 | Time 81.5099(76.9519) | Bit/dim 1.6164(1.6411) | Xent 0.1141(0.1423) | Loss 1.6734(1.7122) | Error 0.0345(0.0442) Steps 812(759.32) | Grad Norm 8.4070(5.8511) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0251 | Time 26.1189, Epoch Time 603.1847(396.6975), Bit/dim 1.6160(best: 1.1658), Xent 0.0550, Loss 1.6435, Error 0.0171(best: 0.0135)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1758 | Time 81.0054(77.0735) | Bit/dim 1.6177(1.6404) | Xent 0.1157(0.1415) | Loss 1.6756(1.7111) | Error 0.0334(0.0438) Steps 788(760.18) | Grad Norm 11.3577(6.0163) | Total Time 10.00(10.00)\n",
      "Iter 1759 | Time 81.2852(77.1998) | Bit/dim 1.6007(1.6392) | Xent 0.1145(0.1407) | Loss 1.6580(1.7095) | Error 0.0333(0.0435) Steps 794(761.20) | Grad Norm 3.2152(5.9322) | Total Time 10.00(10.00)\n",
      "Iter 1760 | Time 81.6923(77.3346) | Bit/dim 1.6211(1.6387) | Xent 0.0988(0.1394) | Loss 1.6705(1.7084) | Error 0.0315(0.0432) Steps 770(761.46) | Grad Norm 9.3050(6.0334) | Total Time 10.00(10.00)\n",
      "Iter 1761 | Time 79.1059(77.3877) | Bit/dim 1.5949(1.6373) | Xent 0.1222(0.1389) | Loss 1.6560(1.7068) | Error 0.0391(0.0430) Steps 794(762.44) | Grad Norm 4.7082(5.9937) | Total Time 10.00(10.00)\n",
      "Iter 1762 | Time 79.8187(77.4607) | Bit/dim 1.6082(1.6365) | Xent 0.1032(0.1378) | Loss 1.6598(1.7054) | Error 0.0306(0.0427) Steps 782(763.02) | Grad Norm 6.7504(6.0164) | Total Time 10.00(10.00)\n",
      "Iter 1763 | Time 82.3184(77.6064) | Bit/dim 1.6026(1.6354) | Xent 0.1109(0.1370) | Loss 1.6580(1.7040) | Error 0.0331(0.0424) Steps 788(763.77) | Grad Norm 6.1333(6.0199) | Total Time 10.00(10.00)\n",
      "Iter 1764 | Time 79.0712(77.6503) | Bit/dim 1.6083(1.6346) | Xent 0.1243(0.1366) | Loss 1.6704(1.7030) | Error 0.0379(0.0423) Steps 788(764.50) | Grad Norm 5.4823(6.0037) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0252 | Time 26.2990, Epoch Time 603.2086(402.8928), Bit/dim 1.5892(best: 1.1658), Xent 0.0544, Loss 1.6164, Error 0.0175(best: 0.0135)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1765 | Time 81.7089(77.7721) | Bit/dim 1.5960(1.6335) | Xent 0.1021(0.1356) | Loss 1.6471(1.7013) | Error 0.0325(0.0420) Steps 794(765.38) | Grad Norm 2.8830(5.9101) | Total Time 10.00(10.00)\n",
      "Iter 1766 | Time 83.7641(77.9518) | Bit/dim 1.5988(1.6324) | Xent 0.1030(0.1346) | Loss 1.6504(1.6998) | Error 0.0327(0.0417) Steps 788(766.06) | Grad Norm 4.8044(5.8770) | Total Time 10.00(10.00)\n",
      "Iter 1767 | Time 82.0079(78.0735) | Bit/dim 1.5867(1.6311) | Xent 0.1023(0.1337) | Loss 1.6379(1.6979) | Error 0.0333(0.0414) Steps 776(766.36) | Grad Norm 3.7799(5.8140) | Total Time 10.00(10.00)\n",
      "Iter 1768 | Time 80.9283(78.1592) | Bit/dim 1.6000(1.6301) | Xent 0.1161(0.1331) | Loss 1.6581(1.6967) | Error 0.0371(0.0413) Steps 788(767.01) | Grad Norm 7.0561(5.8513) | Total Time 10.00(10.00)\n",
      "Iter 1769 | Time 80.9085(78.2417) | Bit/dim 1.5908(1.6290) | Xent 0.1112(0.1325) | Loss 1.6464(1.6952) | Error 0.0347(0.0411) Steps 788(767.64) | Grad Norm 3.4670(5.7798) | Total Time 10.00(10.00)\n",
      "Iter 1770 | Time 83.1851(78.3900) | Bit/dim 1.5942(1.6279) | Xent 0.1071(0.1317) | Loss 1.6478(1.6938) | Error 0.0349(0.0409) Steps 776(767.89) | Grad Norm 3.8474(5.7218) | Total Time 10.00(10.00)\n",
      "Iter 1771 | Time 83.3175(78.5378) | Bit/dim 1.5867(1.6267) | Xent 0.1028(0.1309) | Loss 1.6381(1.6921) | Error 0.0309(0.0406) Steps 794(768.67) | Grad Norm 0.8530(5.5757) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0253 | Time 26.4568, Epoch Time 614.7576(409.2488), Bit/dim 1.5833(best: 1.1658), Xent 0.0525, Loss 1.6095, Error 0.0169(best: 0.0135)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1772 | Time 80.0993(78.5846) | Bit/dim 1.5883(1.6255) | Xent 0.1049(0.1301) | Loss 1.6408(1.6906) | Error 0.0336(0.0404) Steps 794(769.43) | Grad Norm 2.9730(5.4977) | Total Time 10.00(10.00)\n",
      "Iter 1773 | Time 84.8658(78.7731) | Bit/dim 1.5890(1.6244) | Xent 0.1034(0.1293) | Loss 1.6407(1.6891) | Error 0.0316(0.0401) Steps 794(770.17) | Grad Norm 2.3840(5.4042) | Total Time 10.00(10.00)\n",
      "Iter 1774 | Time 81.1001(78.8429) | Bit/dim 1.5834(1.6232) | Xent 0.1091(0.1287) | Loss 1.6379(1.6875) | Error 0.0334(0.0399) Steps 788(770.71) | Grad Norm 4.9416(5.3904) | Total Time 10.00(10.00)\n",
      "Iter 1775 | Time 82.4778(78.9519) | Bit/dim 1.5772(1.6218) | Xent 0.1086(0.1281) | Loss 1.6315(1.6859) | Error 0.0324(0.0397) Steps 788(771.22) | Grad Norm 3.0772(5.3210) | Total Time 10.00(10.00)\n",
      "Iter 1776 | Time 82.3776(79.0547) | Bit/dim 1.5879(1.6208) | Xent 0.1054(0.1274) | Loss 1.6406(1.6845) | Error 0.0349(0.0396) Steps 806(772.27) | Grad Norm 3.3733(5.2625) | Total Time 10.00(10.00)\n",
      "Iter 1777 | Time 80.7850(79.1066) | Bit/dim 1.5887(1.6198) | Xent 0.1029(0.1267) | Loss 1.6401(1.6832) | Error 0.0341(0.0394) Steps 794(772.92) | Grad Norm 6.4306(5.2976) | Total Time 10.00(10.00)\n",
      "Iter 1778 | Time 82.0320(79.1944) | Bit/dim 1.5990(1.6192) | Xent 0.1300(0.1268) | Loss 1.6640(1.6826) | Error 0.0387(0.0394) Steps 776(773.01) | Grad Norm 13.3675(5.5397) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0254 | Time 26.6503, Epoch Time 612.9917(415.3611), Bit/dim 1.6391(best: 1.1658), Xent 0.0808, Loss 1.6795, Error 0.0261(best: 0.0135)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1779 | Time 81.0059(79.2487) | Bit/dim 1.6459(1.6200) | Xent 0.1455(0.1273) | Loss 1.7187(1.6837) | Error 0.0446(0.0395) Steps 794(773.64) | Grad Norm 22.2638(6.0414) | Total Time 10.00(10.00)\n",
      "Iter 1780 | Time 83.2643(79.3692) | Bit/dim 1.7030(1.6225) | Xent 0.3156(0.1330) | Loss 1.8607(1.6890) | Error 0.0980(0.0413) Steps 812(774.79) | Grad Norm 31.7798(6.8136) | Total Time 10.00(10.00)\n",
      "Iter 1781 | Time 79.5680(79.3751) | Bit/dim 1.6904(1.6245) | Xent 0.1947(0.1348) | Loss 1.7878(1.6919) | Error 0.0620(0.0419) Steps 788(775.19) | Grad Norm 23.1558(7.3038) | Total Time 10.00(10.00)\n",
      "Iter 1782 | Time 80.8117(79.4182) | Bit/dim 1.6075(1.6240) | Xent 0.1148(0.1342) | Loss 1.6649(1.6911) | Error 0.0361(0.0417) Steps 776(775.21) | Grad Norm 6.0496(7.2662) | Total Time 10.00(10.00)\n",
      "Iter 1783 | Time 80.2083(79.4419) | Bit/dim 1.6173(1.6238) | Xent 0.1526(0.1348) | Loss 1.6937(1.6912) | Error 0.0487(0.0420) Steps 788(775.60) | Grad Norm 8.2600(7.2960) | Total Time 10.00(10.00)\n",
      "Iter 1784 | Time 79.6749(79.4489) | Bit/dim 1.6165(1.6236) | Xent 0.1623(0.1356) | Loss 1.6976(1.6914) | Error 0.0514(0.0422) Steps 776(775.61) | Grad Norm 7.6078(7.3054) | Total Time 10.00(10.00)\n",
      "Iter 1785 | Time 81.7645(79.5184) | Bit/dim 1.6083(1.6231) | Xent 0.1181(0.1351) | Loss 1.6674(1.6907) | Error 0.0350(0.0420) Steps 776(775.62) | Grad Norm 3.2299(7.1831) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0255 | Time 26.0374, Epoch Time 604.9560(421.0489), Bit/dim 1.6318(best: 1.1658), Xent 0.0555, Loss 1.6595, Error 0.0174(best: 0.0135)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1786 | Time 79.8686(79.5289) | Bit/dim 1.6352(1.6235) | Xent 0.1279(0.1349) | Loss 1.6991(1.6909) | Error 0.0383(0.0419) Steps 776(775.63) | Grad Norm 6.4424(7.1609) | Total Time 10.00(10.00)\n",
      "Iter 1787 | Time 81.0921(79.5758) | Bit/dim 1.6141(1.6232) | Xent 0.1246(0.1345) | Loss 1.6764(1.6905) | Error 0.0399(0.0418) Steps 782(775.82) | Grad Norm 4.0097(7.0663) | Total Time 10.00(10.00)\n",
      "Iter 1788 | Time 81.1003(79.6215) | Bit/dim 1.6038(1.6226) | Xent 0.1261(0.1343) | Loss 1.6668(1.6898) | Error 0.0411(0.0418) Steps 776(775.83) | Grad Norm 2.8852(6.9409) | Total Time 10.00(10.00)\n",
      "Iter 1789 | Time 81.4622(79.6768) | Bit/dim 1.6127(1.6223) | Xent 0.1256(0.1340) | Loss 1.6755(1.6894) | Error 0.0390(0.0417) Steps 782(776.01) | Grad Norm 3.0892(6.8254) | Total Time 10.00(10.00)\n",
      "Iter 1790 | Time 83.5176(79.7920) | Bit/dim 1.6153(1.6221) | Xent 0.1193(0.1336) | Loss 1.6750(1.6889) | Error 0.0366(0.0416) Steps 776(776.01) | Grad Norm 3.0594(6.7124) | Total Time 10.00(10.00)\n",
      "Iter 1791 | Time 82.8036(79.8823) | Bit/dim 1.6024(1.6215) | Xent 0.1221(0.1332) | Loss 1.6635(1.6882) | Error 0.0374(0.0415) Steps 800(776.73) | Grad Norm 2.5682(6.5881) | Total Time 10.00(10.00)\n",
      "Iter 1792 | Time 84.0857(80.0084) | Bit/dim 1.6025(1.6210) | Xent 0.1139(0.1327) | Loss 1.6594(1.6873) | Error 0.0360(0.0413) Steps 812(777.79) | Grad Norm 2.7000(6.4714) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0256 | Time 26.7202, Epoch Time 613.3628(426.8183), Bit/dim 1.5996(best: 1.1658), Xent 0.0551, Loss 1.6271, Error 0.0179(best: 0.0135)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1793 | Time 81.2416(80.0454) | Bit/dim 1.6029(1.6204) | Xent 0.1230(0.1324) | Loss 1.6644(1.6866) | Error 0.0367(0.0412) Steps 794(778.28) | Grad Norm 2.7261(6.3591) | Total Time 10.00(10.00)\n",
      "Iter 1794 | Time 79.0518(80.0156) | Bit/dim 1.5928(1.6196) | Xent 0.1046(0.1315) | Loss 1.6451(1.6854) | Error 0.0316(0.0409) Steps 800(778.93) | Grad Norm 2.4902(6.2430) | Total Time 10.00(10.00)\n",
      "Iter 1795 | Time 80.3608(80.0260) | Bit/dim 1.5935(1.6188) | Xent 0.1304(0.1315) | Loss 1.6587(1.6846) | Error 0.0380(0.0408) Steps 764(778.48) | Grad Norm 3.2185(6.1523) | Total Time 10.00(10.00)\n",
      "Iter 1796 | Time 83.4067(80.1274) | Bit/dim 1.5900(1.6180) | Xent 0.1107(0.1309) | Loss 1.6454(1.6834) | Error 0.0345(0.0406) Steps 800(779.13) | Grad Norm 2.7201(6.0493) | Total Time 10.00(10.00)\n",
      "Iter 1797 | Time 83.2488(80.2210) | Bit/dim 1.5857(1.6170) | Xent 0.1059(0.1301) | Loss 1.6387(1.6821) | Error 0.0354(0.0404) Steps 824(780.47) | Grad Norm 2.4430(5.9411) | Total Time 10.00(10.00)\n",
      "Iter 1798 | Time 82.4192(80.2870) | Bit/dim 1.5891(1.6162) | Xent 0.1011(0.1293) | Loss 1.6397(1.6808) | Error 0.0317(0.0402) Steps 812(781.42) | Grad Norm 3.6278(5.8717) | Total Time 10.00(10.00)\n",
      "Iter 1799 | Time 81.9796(80.3378) | Bit/dim 1.5834(1.6152) | Xent 0.0938(0.1282) | Loss 1.6303(1.6793) | Error 0.0298(0.0399) Steps 806(782.16) | Grad Norm 3.3994(5.7975) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0257 | Time 26.5543, Epoch Time 610.9833(432.3433), Bit/dim 1.5738(best: 1.1658), Xent 0.0514, Loss 1.5995, Error 0.0162(best: 0.0135)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1800 | Time 82.9006(80.4146) | Bit/dim 1.5822(1.6142) | Xent 0.1103(0.1277) | Loss 1.6374(1.6780) | Error 0.0349(0.0397) Steps 794(782.51) | Grad Norm 2.1852(5.6892) | Total Time 10.00(10.00)\n",
      "Iter 1801 | Time 82.5609(80.4790) | Bit/dim 1.5821(1.6132) | Xent 0.1100(0.1271) | Loss 1.6371(1.6768) | Error 0.0344(0.0396) Steps 800(783.04) | Grad Norm 4.3521(5.6490) | Total Time 10.00(10.00)\n",
      "Iter 1802 | Time 83.0061(80.5548) | Bit/dim 1.5801(1.6122) | Xent 0.1026(0.1264) | Loss 1.6314(1.6754) | Error 0.0314(0.0393) Steps 824(784.26) | Grad Norm 1.7986(5.5335) | Total Time 10.00(10.00)\n",
      "Iter 1803 | Time 83.0196(80.6288) | Bit/dim 1.5754(1.6111) | Xent 0.1080(0.1258) | Loss 1.6294(1.6740) | Error 0.0324(0.0391) Steps 812(785.10) | Grad Norm 3.3980(5.4695) | Total Time 10.00(10.00)\n",
      "Iter 1804 | Time 84.8933(80.7567) | Bit/dim 1.5742(1.6100) | Xent 0.1088(0.1253) | Loss 1.6286(1.6727) | Error 0.0324(0.0389) Steps 806(785.72) | Grad Norm 2.4614(5.3792) | Total Time 10.00(10.00)\n",
      "Iter 1805 | Time 82.6883(80.8147) | Bit/dim 1.5643(1.6086) | Xent 0.0969(0.1245) | Loss 1.6128(1.6709) | Error 0.0298(0.0386) Steps 812(786.51) | Grad Norm 2.2458(5.2852) | Total Time 10.00(10.00)\n",
      "Iter 1806 | Time 82.3724(80.8614) | Bit/dim 1.5710(1.6075) | Xent 0.0971(0.1237) | Loss 1.6195(1.6693) | Error 0.0311(0.0384) Steps 788(786.56) | Grad Norm 1.9386(5.1848) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0258 | Time 27.3080, Epoch Time 621.4014(438.0150), Bit/dim 1.5654(best: 1.1658), Xent 0.0501, Loss 1.5905, Error 0.0163(best: 0.0135)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1807 | Time 83.4941(80.9404) | Bit/dim 1.5701(1.6064) | Xent 0.1046(0.1231) | Loss 1.6224(1.6679) | Error 0.0319(0.0382) Steps 806(787.14) | Grad Norm 1.7221(5.0809) | Total Time 10.00(10.00)\n",
      "Iter 1808 | Time 83.3879(81.0138) | Bit/dim 1.5738(1.6054) | Xent 0.1011(0.1224) | Loss 1.6244(1.6666) | Error 0.0315(0.0380) Steps 830(788.43) | Grad Norm 1.3568(4.9692) | Total Time 10.00(10.00)\n",
      "Iter 1809 | Time 82.8118(81.0677) | Bit/dim 1.5661(1.6042) | Xent 0.1033(0.1219) | Loss 1.6177(1.6652) | Error 0.0327(0.0378) Steps 818(789.31) | Grad Norm 1.5550(4.8668) | Total Time 10.00(10.00)\n",
      "Iter 1810 | Time 82.9865(81.1253) | Bit/dim 1.5610(1.6029) | Xent 0.1047(0.1213) | Loss 1.6133(1.6636) | Error 0.0350(0.0378) Steps 812(789.99) | Grad Norm 1.7860(4.7744) | Total Time 10.00(10.00)\n",
      "Iter 1811 | Time 83.3306(81.1915) | Bit/dim 1.5617(1.6017) | Xent 0.1012(0.1207) | Loss 1.6123(1.6621) | Error 0.0298(0.0375) Steps 794(790.11) | Grad Norm 0.8363(4.6562) | Total Time 10.00(10.00)\n",
      "Iter 1812 | Time 81.9951(81.2156) | Bit/dim 1.5569(1.6004) | Xent 0.1095(0.1204) | Loss 1.6116(1.6606) | Error 0.0323(0.0374) Steps 818(790.95) | Grad Norm 1.8234(4.5712) | Total Time 10.00(10.00)\n",
      "Iter 1813 | Time 83.7355(81.2912) | Bit/dim 1.5682(1.5994) | Xent 0.0984(0.1197) | Loss 1.6174(1.6593) | Error 0.0330(0.0372) Steps 818(791.76) | Grad Norm 2.0333(4.4951) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0259 | Time 26.9339, Epoch Time 621.2852(443.5131), Bit/dim 1.5551(best: 1.1658), Xent 0.0506, Loss 1.5804, Error 0.0163(best: 0.0135)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1814 | Time 84.5066(81.3876) | Bit/dim 1.5638(1.5983) | Xent 0.1021(0.1192) | Loss 1.6148(1.6579) | Error 0.0316(0.0371) Steps 806(792.19) | Grad Norm 1.2574(4.3980) | Total Time 10.00(10.00)\n",
      "Iter 1815 | Time 83.5065(81.4512) | Bit/dim 1.5629(1.5973) | Xent 0.1079(0.1189) | Loss 1.6169(1.6567) | Error 0.0344(0.0370) Steps 812(792.78) | Grad Norm 1.6142(4.3145) | Total Time 10.00(10.00)\n",
      "Iter 1816 | Time 82.1320(81.4716) | Bit/dim 1.5599(1.5961) | Xent 0.1027(0.1184) | Loss 1.6113(1.6553) | Error 0.0323(0.0368) Steps 812(793.36) | Grad Norm 1.6435(4.2343) | Total Time 10.00(10.00)\n",
      "Iter 1817 | Time 82.4744(81.5017) | Bit/dim 1.5620(1.5951) | Xent 0.0952(0.1177) | Loss 1.6096(1.6540) | Error 0.0279(0.0366) Steps 818(794.10) | Grad Norm 2.5909(4.1850) | Total Time 10.00(10.00)\n",
      "Iter 1818 | Time 84.7904(81.6004) | Bit/dim 1.5635(1.5942) | Xent 0.1068(0.1174) | Loss 1.6169(1.6528) | Error 0.0314(0.0364) Steps 818(794.82) | Grad Norm 2.9347(4.1475) | Total Time 10.00(10.00)\n",
      "Iter 1819 | Time 84.2287(81.6792) | Bit/dim 1.5528(1.5929) | Xent 0.0886(0.1165) | Loss 1.5971(1.6512) | Error 0.0268(0.0361) Steps 830(795.87) | Grad Norm 2.6135(4.1015) | Total Time 10.00(10.00)\n",
      "Iter 1820 | Time 84.0708(81.7510) | Bit/dim 1.5530(1.5917) | Xent 0.0997(0.1160) | Loss 1.6029(1.6497) | Error 0.0325(0.0360) Steps 806(796.18) | Grad Norm 1.8636(4.0344) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0260 | Time 27.2700, Epoch Time 625.7799(448.9811), Bit/dim 1.5514(best: 1.1658), Xent 0.0492, Loss 1.5760, Error 0.0158(best: 0.0135)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1821 | Time 86.1504(81.8829) | Bit/dim 1.5528(1.5906) | Xent 0.1018(0.1156) | Loss 1.6037(1.6483) | Error 0.0302(0.0358) Steps 818(796.83) | Grad Norm 1.1218(3.9470) | Total Time 10.00(10.00)\n",
      "Iter 1822 | Time 85.9486(82.0049) | Bit/dim 1.5520(1.5894) | Xent 0.1039(0.1152) | Loss 1.6039(1.6470) | Error 0.0315(0.0357) Steps 806(797.11) | Grad Norm 1.2892(3.8673) | Total Time 10.00(10.00)\n",
      "Iter 1823 | Time 85.0767(82.0971) | Bit/dim 1.5544(1.5883) | Xent 0.0985(0.1147) | Loss 1.6036(1.6457) | Error 0.0306(0.0356) Steps 812(797.55) | Grad Norm 1.4882(3.7959) | Total Time 10.00(10.00)\n",
      "Iter 1824 | Time 82.3845(82.1057) | Bit/dim 1.5568(1.5874) | Xent 0.0903(0.1140) | Loss 1.6019(1.6444) | Error 0.0301(0.0354) Steps 806(797.81) | Grad Norm 1.4229(3.7247) | Total Time 10.00(10.00)\n",
      "Iter 1825 | Time 84.2037(82.1686) | Bit/dim 1.5574(1.5865) | Xent 0.0945(0.1134) | Loss 1.6046(1.6432) | Error 0.0302(0.0352) Steps 830(798.77) | Grad Norm 2.2332(3.6799) | Total Time 10.00(10.00)\n",
      "Iter 1826 | Time 83.0130(82.1940) | Bit/dim 1.5543(1.5855) | Xent 0.1021(0.1131) | Loss 1.6054(1.6421) | Error 0.0325(0.0352) Steps 800(798.81) | Grad Norm 3.3991(3.6715) | Total Time 10.00(10.00)\n",
      "Iter 1827 | Time 83.8943(82.2450) | Bit/dim 1.5541(1.5846) | Xent 0.1018(0.1127) | Loss 1.6050(1.6410) | Error 0.0296(0.0350) Steps 824(799.56) | Grad Norm 7.2943(3.7802) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0261 | Time 27.6237, Epoch Time 630.6733(454.4319), Bit/dim 1.5713(best: 1.1658), Xent 0.0528, Loss 1.5977, Error 0.0175(best: 0.0135)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1828 | Time 83.7336(82.2896) | Bit/dim 1.5739(1.5843) | Xent 0.1020(0.1124) | Loss 1.6249(1.6405) | Error 0.0301(0.0348) Steps 830(800.48) | Grad Norm 14.8652(4.1128) | Total Time 10.00(10.00)\n",
      "Iter 1829 | Time 87.1092(82.4342) | Bit/dim 1.6053(1.5849) | Xent 0.1228(0.1127) | Loss 1.6667(1.6413) | Error 0.0385(0.0350) Steps 830(801.36) | Grad Norm 22.2397(4.6566) | Total Time 10.00(10.00)\n",
      "Iter 1830 | Time 85.1483(82.5156) | Bit/dim 1.6127(1.5857) | Xent 0.0995(0.1123) | Loss 1.6625(1.6419) | Error 0.0300(0.0348) Steps 824(802.04) | Grad Norm 19.7526(5.1094) | Total Time 10.00(10.00)\n",
      "Iter 1831 | Time 86.4455(82.6335) | Bit/dim 1.5681(1.5852) | Xent 0.1153(0.1124) | Loss 1.6258(1.6414) | Error 0.0353(0.0348) Steps 818(802.52) | Grad Norm 6.9342(5.1642) | Total Time 10.00(10.00)\n",
      "Iter 1832 | Time 85.2987(82.7135) | Bit/dim 1.5837(1.5852) | Xent 0.1159(0.1125) | Loss 1.6417(1.6414) | Error 0.0346(0.0348) Steps 830(803.35) | Grad Norm 14.5455(5.4456) | Total Time 10.00(10.00)\n",
      "Iter 1833 | Time 84.6651(82.7720) | Bit/dim 1.5773(1.5849) | Xent 0.0890(0.1118) | Loss 1.6219(1.6408) | Error 0.0270(0.0346) Steps 806(803.42) | Grad Norm 11.7677(5.6353) | Total Time 10.00(10.00)\n",
      "Iter 1834 | Time 86.0021(82.8689) | Bit/dim 1.5624(1.5843) | Xent 0.1036(0.1116) | Loss 1.6142(1.6400) | Error 0.0325(0.0345) Steps 812(803.68) | Grad Norm 4.9123(5.6136) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0262 | Time 26.9035, Epoch Time 637.8880(459.9356), Bit/dim 1.5499(best: 1.1658), Xent 0.0489, Loss 1.5743, Error 0.0153(best: 0.0135)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1835 | Time 86.1032(82.9660) | Bit/dim 1.5533(1.5833) | Xent 0.1042(0.1113) | Loss 1.6054(1.6390) | Error 0.0308(0.0344) Steps 800(803.57) | Grad Norm 6.3128(5.6346) | Total Time 10.00(10.00)\n",
      "Iter 1836 | Time 82.1999(82.9430) | Bit/dim 1.5752(1.5831) | Xent 0.1080(0.1112) | Loss 1.6292(1.6387) | Error 0.0314(0.0343) Steps 818(804.00) | Grad Norm 6.7727(5.6687) | Total Time 10.00(10.00)\n",
      "Iter 1837 | Time 84.0772(82.9770) | Bit/dim 1.5533(1.5822) | Xent 0.0847(0.1105) | Loss 1.5957(1.6374) | Error 0.0268(0.0341) Steps 830(804.78) | Grad Norm 2.6818(5.5791) | Total Time 10.00(10.00)\n",
      "Iter 1838 | Time 83.5980(82.9956) | Bit/dim 1.5636(1.5816) | Xent 0.1047(0.1103) | Loss 1.6159(1.6368) | Error 0.0333(0.0341) Steps 806(804.82) | Grad Norm 6.6168(5.6102) | Total Time 10.00(10.00)\n",
      "Iter 1839 | Time 82.6319(82.9847) | Bit/dim 1.5574(1.5809) | Xent 0.1008(0.1100) | Loss 1.6078(1.6359) | Error 0.0299(0.0339) Steps 806(804.86) | Grad Norm 5.3029(5.6010) | Total Time 10.00(10.00)\n",
      "Iter 1840 | Time 87.3770(83.1165) | Bit/dim 1.5578(1.5802) | Xent 0.0973(0.1096) | Loss 1.6065(1.6350) | Error 0.0296(0.0338) Steps 830(805.61) | Grad Norm 4.3710(5.5641) | Total Time 10.00(10.00)\n",
      "Iter 1841 | Time 88.3733(83.2742) | Bit/dim 1.5523(1.5794) | Xent 0.1074(0.1095) | Loss 1.6059(1.6341) | Error 0.0326(0.0338) Steps 836(806.52) | Grad Norm 6.3400(5.5874) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0263 | Time 27.2814, Epoch Time 634.1947(465.1633), Bit/dim 1.5491(best: 1.1658), Xent 0.0480, Loss 1.5731, Error 0.0158(best: 0.0135)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1842 | Time 83.5323(83.2819) | Bit/dim 1.5549(1.5786) | Xent 0.0937(0.1091) | Loss 1.6018(1.6332) | Error 0.0296(0.0336) Steps 818(806.87) | Grad Norm 5.0579(5.5715) | Total Time 10.00(10.00)\n",
      "Iter 1843 | Time 82.9759(83.2728) | Bit/dim 1.5441(1.5776) | Xent 0.1047(0.1089) | Loss 1.5965(1.6321) | Error 0.0281(0.0335) Steps 812(807.02) | Grad Norm 2.5927(5.4822) | Total Time 10.00(10.00)\n",
      "Iter 1844 | Time 83.9740(83.2938) | Bit/dim 1.5470(1.5767) | Xent 0.1008(0.1087) | Loss 1.5974(1.6310) | Error 0.0290(0.0333) Steps 830(807.71) | Grad Norm 4.2554(5.4453) | Total Time 10.00(10.00)\n",
      "Iter 1845 | Time 82.4734(83.2692) | Bit/dim 1.5547(1.5760) | Xent 0.0978(0.1084) | Loss 1.6036(1.6302) | Error 0.0288(0.0332) Steps 818(808.02) | Grad Norm 4.1512(5.4065) | Total Time 10.00(10.00)\n",
      "Iter 1846 | Time 86.4863(83.3657) | Bit/dim 1.5393(1.5749) | Xent 0.0944(0.1079) | Loss 1.5864(1.6289) | Error 0.0284(0.0331) Steps 824(808.50) | Grad Norm 1.4213(5.2870) | Total Time 10.00(10.00)\n",
      "Iter 1847 | Time 83.4583(83.3685) | Bit/dim 1.5544(1.5743) | Xent 0.1015(0.1078) | Loss 1.6051(1.6282) | Error 0.0326(0.0331) Steps 812(808.60) | Grad Norm 4.3166(5.2579) | Total Time 10.00(10.00)\n",
      "Iter 1848 | Time 83.9322(83.3854) | Bit/dim 1.5550(1.5737) | Xent 0.0897(0.1072) | Loss 1.5998(1.6273) | Error 0.0275(0.0329) Steps 812(808.70) | Grad Norm 5.9041(5.2772) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0264 | Time 27.7410, Epoch Time 627.0058(470.0186), Bit/dim 1.5432(best: 1.1658), Xent 0.0470, Loss 1.5667, Error 0.0157(best: 0.0135)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle.py --data mnist --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 8000 --test_batch_size 5000 --save ../experiments_published/cnf_conditional_disentangle_bs8K_sratio_0_25_drop_0_5_run1 --resume ../experiments_published/cnf_conditional_disentangle_bs8K_sratio_0_25_drop_0_5_run1/epoch_125_checkpt.pth --seed 0 --conditional True --controlled_tol True --train_mode semisup --lr 0.001 --warmup_iters 113 --atol 1e-4  --rtol 1e-4 --weight_y 0.5 --condition_ratio 0.25 --dropout_rate 0.5\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
