{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_cifar.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "if args.controlled_tol:\n",
      "    import lib.odenvp_conditional_tol as odenvp\n",
      "else:\n",
      "    import lib.odenvp_conditional as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            loss.backward()\n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=0.0001, autoencode=False, batch_norm=False, batch_size=8000, batch_size_schedule='', begin_epoch=1, condition_ratio=0.25, conditional=True, controlled_tol=True, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=1, lr=0.01, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume=None, rtol=0.0001, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_25_drop_0_5_run1', seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=5000, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000.0, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=1536, bias=True)\n",
      "  (project_class): LinearZeros(in_features=768, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1386550\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc.py:286: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "Iter 0001 | Time 98.6605(98.6605) | Bit/dim 9.2394(9.2394) | Xent 2.3026(2.3026) | Loss 10.3907(10.3907) | Error 0.8944(0.8944) Steps 550(550.00) | Grad Norm 17.8497(17.8497) | Total Time 14.00(14.00)\n",
      "Iter 0002 | Time 47.0261(97.1114) | Bit/dim 9.1320(9.2361) | Xent 2.2982(2.3025) | Loss 10.2812(10.3874) | Error 0.7661(0.8905) Steps 550(550.00) | Grad Norm 15.7457(17.7866) | Total Time 14.00(14.00)\n",
      "Iter 0003 | Time 44.1433(95.5224) | Bit/dim 9.0122(9.2294) | Xent 2.2912(2.3021) | Loss 10.1577(10.3805) | Error 0.7525(0.8864) Steps 550(550.00) | Grad Norm 13.5789(17.6603) | Total Time 14.00(14.00)\n",
      "Iter 0004 | Time 45.5829(94.0242) | Bit/dim 8.8566(9.2182) | Xent 2.2834(2.3016) | Loss 9.9983(10.3690) | Error 0.7622(0.8827) Steps 550(550.00) | Grad Norm 10.1968(17.4364) | Total Time 14.00(14.00)\n",
      "Iter 0005 | Time 45.4242(92.5662) | Bit/dim 8.7138(9.2031) | Xent 2.2749(2.3008) | Loss 9.8513(10.3535) | Error 0.7680(0.8792) Steps 550(550.00) | Grad Norm 7.1476(17.1278) | Total Time 14.00(14.00)\n",
      "Iter 0006 | Time 42.9461(91.0776) | Bit/dim 8.6154(9.1855) | Xent 2.2617(2.2996) | Loss 9.7463(10.3353) | Error 0.7657(0.8758) Steps 550(550.00) | Grad Norm 5.1453(16.7683) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0001 | Time 34.5408, Epoch Time 374.1430(374.1430), Bit/dim 8.5362(best: inf), Xent 2.2496, Loss 9.6610, Error 0.7661(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0007 | Time 55.5892(90.0130) | Bit/dim 8.5368(9.1660) | Xent 2.2504(2.2981) | Loss 9.6620(10.3151) | Error 0.7677(0.8726) Steps 550(550.00) | Grad Norm 5.3891(16.4269) | Total Time 14.00(14.00)\n",
      "Iter 0008 | Time 49.0875(88.7852) | Bit/dim 8.5382(9.1472) | Xent 2.2387(2.2963) | Loss 9.6575(10.2953) | Error 0.7674(0.8694) Steps 556(550.18) | Grad Norm 6.6988(16.1351) | Total Time 14.00(14.00)\n",
      "Iter 0009 | Time 43.2369(87.4187) | Bit/dim 8.5102(9.1281) | Xent 2.2263(2.2942) | Loss 9.6233(10.2752) | Error 0.7669(0.8663) Steps 544(549.99) | Grad Norm 8.2327(15.8980) | Total Time 14.00(14.00)\n",
      "Iter 0010 | Time 41.5165(86.0417) | Bit/dim 8.4817(9.1087) | Xent 2.2122(2.2918) | Loss 9.5878(10.2546) | Error 0.7488(0.8628) Steps 556(550.17) | Grad Norm 8.6732(15.6812) | Total Time 14.00(14.00)\n",
      "Iter 0011 | Time 44.7041(84.8015) | Bit/dim 8.4539(9.0890) | Xent 2.1995(2.2890) | Loss 9.5537(10.2335) | Error 0.7463(0.8593) Steps 562(550.53) | Grad Norm 8.3410(15.4610) | Total Time 14.00(14.00)\n",
      "Iter 0012 | Time 46.3097(83.6468) | Bit/dim 8.3745(9.0676) | Xent 2.1921(2.2861) | Loss 9.4706(10.2107) | Error 0.7479(0.8560) Steps 556(550.69) | Grad Norm 7.1859(15.2128) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0002 | Time 21.2617, Epoch Time 317.0702(372.4308), Bit/dim 8.3165(best: 8.5362), Xent 2.1791, Loss 9.4061, Error 0.7470(best: 0.7661)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0013 | Time 45.6666(82.5074) | Bit/dim 8.3281(9.0454) | Xent 2.1819(2.2830) | Loss 9.4190(10.1869) | Error 0.7496(0.8528) Steps 556(550.85) | Grad Norm 6.4674(14.9504) | Total Time 14.00(14.00)\n",
      "Iter 0014 | Time 46.0275(81.4130) | Bit/dim 8.2599(9.0219) | Xent 2.1727(2.2797) | Loss 9.3462(10.1617) | Error 0.7622(0.8501) Steps 556(551.01) | Grad Norm 5.8622(14.6778) | Total Time 14.00(14.00)\n",
      "Iter 0015 | Time 44.5790(80.3080) | Bit/dim 8.1989(8.9972) | Xent 2.1616(2.2761) | Loss 9.2797(10.1352) | Error 0.7641(0.8475) Steps 550(550.98) | Grad Norm 5.8560(14.4131) | Total Time 14.00(14.00)\n",
      "Iter 0016 | Time 44.2749(79.2270) | Bit/dim 8.0921(8.9700) | Xent 2.1629(2.2727) | Loss 9.1736(10.1064) | Error 0.7598(0.8449) Steps 550(550.95) | Grad Norm 5.5753(14.1480) | Total Time 14.00(14.00)\n",
      "Iter 0017 | Time 43.8257(78.1649) | Bit/dim 8.0172(8.9414) | Xent 2.1517(2.2691) | Loss 9.0930(10.0760) | Error 0.7524(0.8421) Steps 538(550.56) | Grad Norm 5.3626(13.8844) | Total Time 14.00(14.00)\n",
      "Iter 0018 | Time 42.8476(77.1054) | Bit/dim 7.9375(8.9113) | Xent 2.1399(2.2652) | Loss 9.0075(10.0439) | Error 0.7265(0.8386) Steps 538(550.18) | Grad Norm 5.0057(13.6181) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0003 | Time 21.1406, Epoch Time 303.8770(370.3742), Bit/dim 7.8408(best: 8.3165), Xent 2.1226, Loss 8.9021, Error 0.7024(best: 0.7470)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0019 | Time 46.6972(76.1932) | Bit/dim 7.8563(8.8797) | Xent 2.1233(2.2610) | Loss 8.9179(10.0101) | Error 0.7101(0.8348) Steps 538(549.82) | Grad Norm 4.6437(13.3488) | Total Time 14.00(14.00)\n",
      "Iter 0020 | Time 45.9093(75.2847) | Bit/dim 7.7420(8.8455) | Xent 2.1204(2.2567) | Loss 8.8022(9.9739) | Error 0.7025(0.8308) Steps 538(549.46) | Grad Norm 4.0604(13.0702) | Total Time 14.00(14.00)\n",
      "Iter 0021 | Time 45.1945(74.3820) | Bit/dim 7.6584(8.8099) | Xent 2.1165(2.2525) | Loss 8.7166(9.9362) | Error 0.7031(0.8270) Steps 544(549.30) | Grad Norm 3.6590(12.7878) | Total Time 14.00(14.00)\n",
      "Iter 0022 | Time 46.5434(73.5468) | Bit/dim 7.5703(8.7727) | Xent 2.1107(2.2483) | Loss 8.6257(9.8969) | Error 0.7099(0.8235) Steps 544(549.14) | Grad Norm 3.5016(12.5093) | Total Time 14.00(14.00)\n",
      "Iter 0023 | Time 45.6224(72.7091) | Bit/dim 7.5000(8.7345) | Xent 2.1080(2.2441) | Loss 8.5540(9.8566) | Error 0.7124(0.8201) Steps 550(549.17) | Grad Norm 3.7777(12.2473) | Total Time 14.00(14.00)\n",
      "Iter 0024 | Time 43.8213(71.8424) | Bit/dim 7.4384(8.6957) | Xent 2.1197(2.2403) | Loss 8.4983(9.8158) | Error 0.7234(0.8172) Steps 562(549.55) | Grad Norm 4.1088(12.0032) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0004 | Time 22.0771, Epoch Time 311.5479(368.6094), Bit/dim 7.3659(best: 7.8408), Xent 2.1125, Loss 8.4221, Error 0.7096(best: 0.7024)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0025 | Time 45.4249(71.0499) | Bit/dim 7.3660(8.6558) | Xent 2.1208(2.2368) | Loss 8.4264(9.7741) | Error 0.7201(0.8143) Steps 562(549.92) | Grad Norm 4.0586(11.7648) | Total Time 14.00(14.00)\n",
      "Iter 0026 | Time 46.7674(70.3214) | Bit/dim 7.2928(8.6149) | Xent 2.1170(2.2332) | Loss 8.3513(9.7315) | Error 0.7151(0.8113) Steps 562(550.29) | Grad Norm 3.4911(11.5166) | Total Time 14.00(14.00)\n",
      "Iter 0027 | Time 46.9950(69.6216) | Bit/dim 7.2270(8.5732) | Xent 2.1191(2.2297) | Loss 8.2865(9.6881) | Error 0.7123(0.8084) Steps 562(550.64) | Grad Norm 2.6295(11.2500) | Total Time 14.00(14.00)\n",
      "Iter 0028 | Time 45.8894(68.9097) | Bit/dim 7.1731(8.5312) | Xent 2.1277(2.2267) | Loss 8.2369(9.6446) | Error 0.6989(0.8051) Steps 562(550.98) | Grad Norm 2.2017(10.9785) | Total Time 14.00(14.00)\n",
      "Iter 0029 | Time 44.2437(68.1697) | Bit/dim 7.1374(8.4894) | Xent 2.1277(2.2237) | Loss 8.2013(9.6013) | Error 0.7083(0.8022) Steps 556(551.13) | Grad Norm 2.2861(10.7178) | Total Time 14.00(14.00)\n",
      "Iter 0030 | Time 46.2492(67.5121) | Bit/dim 7.1188(8.4483) | Xent 2.1341(2.2210) | Loss 8.1858(9.5588) | Error 0.7329(0.8001) Steps 556(551.28) | Grad Norm 2.5202(10.4718) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0005 | Time 21.5291, Epoch Time 312.6232(366.9298), Bit/dim 7.1023(best: 7.3659), Xent 2.1445, Loss 8.1746, Error 0.7609(best: 0.7024)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0031 | Time 44.3624(66.8176) | Bit/dim 7.1020(8.4079) | Xent 2.1475(2.2188) | Loss 8.1758(9.5173) | Error 0.7629(0.7990) Steps 550(551.24) | Grad Norm 2.6173(10.2362) | Total Time 14.00(14.00)\n",
      "Iter 0032 | Time 47.5410(66.2393) | Bit/dim 7.0796(8.3681) | Xent 2.1547(2.2169) | Loss 8.1569(9.4765) | Error 0.7790(0.7984) Steps 544(551.02) | Grad Norm 2.5980(10.0071) | Total Time 14.00(14.00)\n",
      "Iter 0033 | Time 44.7937(65.5959) | Bit/dim 7.0607(8.3288) | Xent 2.1521(2.2149) | Loss 8.1367(9.4363) | Error 0.7695(0.7975) Steps 550(550.99) | Grad Norm 2.3023(9.7759) | Total Time 14.00(14.00)\n",
      "Iter 0034 | Time 46.2156(65.0145) | Bit/dim 7.0426(8.2903) | Xent 2.1510(2.2130) | Loss 8.1181(9.3968) | Error 0.7582(0.7963) Steps 550(550.96) | Grad Norm 2.0396(9.5438) | Total Time 14.00(14.00)\n",
      "Iter 0035 | Time 48.4501(64.5176) | Bit/dim 7.0307(8.2525) | Xent 2.1464(2.2110) | Loss 8.1039(9.3580) | Error 0.7394(0.7946) Steps 568(551.47) | Grad Norm 1.8812(9.3139) | Total Time 14.00(14.00)\n",
      "Iter 0036 | Time 49.3590(64.0628) | Bit/dim 7.0274(8.2157) | Xent 2.1460(2.2091) | Loss 8.1004(9.3203) | Error 0.7194(0.7924) Steps 568(551.97) | Grad Norm 1.7859(9.0881) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0006 | Time 22.2632, Epoch Time 318.5840(365.4794), Bit/dim 7.0137(best: 7.1023), Xent 2.1380, Loss 8.0828, Error 0.7134(best: 0.7024)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0037 | Time 48.8794(63.6073) | Bit/dim 7.0099(8.1795) | Xent 2.1439(2.2071) | Loss 8.0819(9.2831) | Error 0.7306(0.7905) Steps 568(552.45) | Grad Norm 1.8186(8.8700) | Total Time 14.00(14.00)\n",
      "Iter 0038 | Time 50.0656(63.2011) | Bit/dim 7.0001(8.1442) | Xent 2.1393(2.2051) | Loss 8.0697(9.2467) | Error 0.7335(0.7888) Steps 574(553.09) | Grad Norm 1.5486(8.6504) | Total Time 14.00(14.00)\n",
      "Iter 0039 | Time 48.8118(62.7694) | Bit/dim 7.0052(8.1100) | Xent 2.1346(2.2030) | Loss 8.0725(9.2115) | Error 0.7299(0.7870) Steps 574(553.72) | Grad Norm 1.7163(8.4424) | Total Time 14.00(14.00)\n",
      "Iter 0040 | Time 50.8066(62.4105) | Bit/dim 6.9981(8.0766) | Xent 2.1293(2.2008) | Loss 8.0628(9.1770) | Error 0.7339(0.7854) Steps 574(554.33) | Grad Norm 1.8095(8.2434) | Total Time 14.00(14.00)\n",
      "Iter 0041 | Time 48.1710(61.9833) | Bit/dim 6.9938(8.0442) | Xent 2.1189(2.1983) | Loss 8.0533(9.1433) | Error 0.7321(0.7838) Steps 568(554.74) | Grad Norm 1.8136(8.0505) | Total Time 14.00(14.00)\n",
      "Iter 0042 | Time 47.4039(61.5459) | Bit/dim 6.9851(8.0124) | Xent 2.1076(2.1956) | Loss 8.0389(9.1102) | Error 0.7232(0.7820) Steps 556(554.78) | Grad Norm 1.4848(7.8535) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0007 | Time 22.3787, Epoch Time 331.9929(364.4748), Bit/dim 6.9862(best: 7.0137), Xent 2.0941, Loss 8.0332, Error 0.7048(best: 0.7024)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0043 | Time 47.9686(61.1386) | Bit/dim 6.9779(7.9813) | Xent 2.0993(2.1927) | Loss 8.0275(9.0777) | Error 0.7191(0.7801) Steps 562(554.99) | Grad Norm 1.8456(7.6733) | Total Time 14.00(14.00)\n",
      "Iter 0044 | Time 53.4173(60.9070) | Bit/dim 6.9670(7.9509) | Xent 2.1086(2.1902) | Loss 8.0213(9.0460) | Error 0.7370(0.7788) Steps 580(555.74) | Grad Norm 4.2381(7.5702) | Total Time 14.00(14.00)\n",
      "Iter 0045 | Time 51.9730(60.6390) | Bit/dim 6.9944(7.9222) | Xent 2.1107(2.1878) | Loss 8.0498(9.0161) | Error 0.7430(0.7778) Steps 592(556.83) | Grad Norm 10.4451(7.6565) | Total Time 14.00(14.00)\n",
      "Iter 0046 | Time 54.1158(60.4433) | Bit/dim 6.9961(7.8944) | Xent 2.1690(2.1872) | Loss 8.0806(8.9881) | Error 0.7930(0.7782) Steps 592(557.89) | Grad Norm 15.3581(7.8875) | Total Time 14.00(14.00)\n",
      "Iter 0047 | Time 56.7588(60.3327) | Bit/dim 6.9750(7.8669) | Xent 2.0853(2.1842) | Loss 8.0177(8.9589) | Error 0.7384(0.7770) Steps 598(559.09) | Grad Norm 8.8369(7.9160) | Total Time 14.00(14.00)\n",
      "Iter 0048 | Time 57.1763(60.2380) | Bit/dim 6.9582(7.8396) | Xent 2.0780(2.1810) | Loss 7.9972(8.9301) | Error 0.7309(0.7756) Steps 598(560.26) | Grad Norm 7.2452(7.8959) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0008 | Time 22.8826, Epoch Time 359.7305(364.3325), Bit/dim 6.9799(best: 6.9862), Xent 2.1142, Loss 8.0370, Error 0.7614(best: 0.7024)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0049 | Time 57.2995(60.1499) | Bit/dim 6.9826(7.8139) | Xent 2.1272(2.1794) | Loss 8.0462(8.9036) | Error 0.7634(0.7753) Steps 598(561.39) | Grad Norm 15.0373(8.1101) | Total Time 14.00(14.00)\n",
      "Iter 0050 | Time 55.4462(60.0088) | Bit/dim 6.9417(7.7877) | Xent 2.0581(2.1757) | Loss 7.9707(8.8756) | Error 0.7081(0.7733) Steps 598(562.49) | Grad Norm 5.4085(8.0291) | Total Time 14.00(14.00)\n",
      "Iter 0051 | Time 55.1615(59.8634) | Bit/dim 6.9360(7.7622) | Xent 2.0834(2.1730) | Loss 7.9776(8.8486) | Error 0.7366(0.7722) Steps 598(563.55) | Grad Norm 9.2664(8.0662) | Total Time 14.00(14.00)\n",
      "Iter 0052 | Time 56.7004(59.7685) | Bit/dim 6.9169(7.7368) | Xent 2.0838(2.1703) | Loss 7.9588(8.8220) | Error 0.7426(0.7713) Steps 586(564.23) | Grad Norm 10.4365(8.1373) | Total Time 14.00(14.00)\n",
      "Iter 0053 | Time 54.9219(59.6231) | Bit/dim 6.9045(7.7118) | Xent 2.0451(2.1665) | Loss 7.9270(8.7951) | Error 0.6897(0.7688) Steps 580(564.70) | Grad Norm 2.4909(7.9679) | Total Time 14.00(14.00)\n",
      "Iter 0054 | Time 54.6230(59.4731) | Bit/dim 6.9134(7.6879) | Xent 2.0697(2.1636) | Loss 7.9482(8.7697) | Error 0.7349(0.7678) Steps 574(564.98) | Grad Norm 11.1308(8.0628) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0009 | Time 22.3227, Epoch Time 371.8086(364.5568), Bit/dim 6.8838(best: 6.9799), Xent 2.0374, Loss 7.9025, Error 0.6763(best: 0.7024)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0055 | Time 52.9552(59.2775) | Bit/dim 6.8814(7.6637) | Xent 2.0405(2.1599) | Loss 7.9016(8.7437) | Error 0.6917(0.7655) Steps 568(565.07) | Grad Norm 4.0090(7.9412) | Total Time 14.00(14.00)\n",
      "Iter 0056 | Time 50.9590(59.0280) | Bit/dim 6.8802(7.6402) | Xent 2.0544(2.1568) | Loss 7.9074(8.7186) | Error 0.7111(0.7639) Steps 562(564.98) | Grad Norm 8.1987(7.9489) | Total Time 14.00(14.00)\n",
      "Iter 0057 | Time 47.8512(58.6927) | Bit/dim 6.8587(7.6167) | Xent 2.0742(2.1543) | Loss 7.8958(8.6939) | Error 0.7425(0.7633) Steps 550(564.53) | Grad Norm 8.6046(7.9686) | Total Time 14.00(14.00)\n",
      "Iter 0058 | Time 48.8348(58.3969) | Bit/dim 6.8395(7.5934) | Xent 2.0391(2.1508) | Loss 7.8590(8.6688) | Error 0.6990(0.7613) Steps 556(564.27) | Grad Norm 4.7994(7.8735) | Total Time 14.00(14.00)\n",
      "Iter 0059 | Time 50.1689(58.1501) | Bit/dim 6.8358(7.5707) | Xent 2.0579(2.1480) | Loss 7.8647(8.6447) | Error 0.7151(0.7599) Steps 556(564.02) | Grad Norm 9.1654(7.9123) | Total Time 14.00(14.00)\n",
      "Iter 0060 | Time 47.7773(57.8389) | Bit/dim 6.8050(7.5477) | Xent 2.0426(2.1449) | Loss 7.8263(8.6202) | Error 0.7086(0.7584) Steps 556(563.78) | Grad Norm 3.7483(7.7873) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0010 | Time 21.5044, Epoch Time 335.4491(363.6836), Bit/dim 6.8082(best: 6.8838), Xent 2.0427, Loss 7.8296, Error 0.7163(best: 0.6763)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0061 | Time 47.5520(57.5303) | Bit/dim 6.8044(7.5254) | Xent 2.0638(2.1424) | Loss 7.8363(8.5966) | Error 0.7295(0.7575) Steps 556(563.55) | Grad Norm 10.1025(7.8568) | Total Time 14.00(14.00)\n",
      "Iter 0062 | Time 49.3097(57.2837) | Bit/dim 6.8017(7.5037) | Xent 2.0818(2.1406) | Loss 7.8426(8.5740) | Error 0.7500(0.7573) Steps 550(563.14) | Grad Norm 13.0751(8.0133) | Total Time 14.00(14.00)\n",
      "Iter 0063 | Time 50.1835(57.0707) | Bit/dim 6.7611(7.4814) | Xent 2.1323(2.1404) | Loss 7.8272(8.5516) | Error 0.7558(0.7573) Steps 556(562.93) | Grad Norm 13.9194(8.1905) | Total Time 14.00(14.00)\n",
      "Iter 0064 | Time 47.6085(56.7868) | Bit/dim 6.7580(7.4597) | Xent 2.1061(2.1393) | Loss 7.8110(8.5294) | Error 0.7625(0.7574) Steps 544(562.36) | Grad Norm 15.6075(8.4130) | Total Time 14.00(14.00)\n",
      "Iter 0065 | Time 44.9446(56.4315) | Bit/dim 6.7275(7.4378) | Xent 2.0321(2.1361) | Loss 7.7435(8.5058) | Error 0.7014(0.7557) Steps 544(561.81) | Grad Norm 7.3562(8.3813) | Total Time 14.00(14.00)\n",
      "Iter 0066 | Time 47.4803(56.1630) | Bit/dim 6.7077(7.4159) | Xent 2.0097(2.1323) | Loss 7.7125(8.4820) | Error 0.6950(0.7539) Steps 538(561.10) | Grad Norm 7.5885(8.3575) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0011 | Time 21.0816, Epoch Time 323.8245(362.4878), Bit/dim 6.6929(best: 6.8082), Xent 2.0624, Loss 7.7241, Error 0.7398(best: 0.6763)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0067 | Time 45.4023(55.8402) | Bit/dim 6.6879(7.3940) | Xent 2.0641(2.1303) | Loss 7.7199(8.4592) | Error 0.7458(0.7537) Steps 538(560.40) | Grad Norm 13.7313(8.5188) | Total Time 14.00(14.00)\n",
      "Iter 0068 | Time 46.3300(55.5549) | Bit/dim 6.6701(7.3723) | Xent 2.0891(2.1291) | Loss 7.7147(8.4368) | Error 0.7248(0.7528) Steps 538(559.73) | Grad Norm 13.5937(8.6710) | Total Time 14.00(14.00)\n",
      "Iter 0069 | Time 46.2255(55.2750) | Bit/dim 6.6833(7.3516) | Xent 2.0872(2.1278) | Loss 7.7269(8.4155) | Error 0.7411(0.7525) Steps 538(559.08) | Grad Norm 19.3808(8.9923) | Total Time 14.00(14.00)\n",
      "Iter 0070 | Time 45.2219(54.9734) | Bit/dim 6.6603(7.3309) | Xent 2.1129(2.1274) | Loss 7.7167(8.3946) | Error 0.7804(0.7533) Steps 538(558.45) | Grad Norm 24.8260(9.4673) | Total Time 14.00(14.00)\n",
      "Iter 0071 | Time 45.3615(54.6851) | Bit/dim 6.6112(7.3093) | Xent 2.0344(2.1246) | Loss 7.6284(8.3716) | Error 0.7190(0.7523) Steps 538(557.83) | Grad Norm 17.3115(9.7026) | Total Time 14.00(14.00)\n",
      "Iter 0072 | Time 46.2709(54.4326) | Bit/dim 6.5469(7.2864) | Xent 2.0319(2.1218) | Loss 7.5629(8.3473) | Error 0.7176(0.7512) Steps 538(557.24) | Grad Norm 9.3753(9.6928) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0012 | Time 20.9246, Epoch Time 311.6618(360.9630), Bit/dim 6.5616(best: 6.6929), Xent 2.0417, Loss 7.5825, Error 0.7125(best: 0.6763)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0073 | Time 48.2661(54.2476) | Bit/dim 6.5649(7.2648) | Xent 2.0646(2.1201) | Loss 7.5971(8.3248) | Error 0.7379(0.7508) Steps 538(556.66) | Grad Norm 15.3873(9.8636) | Total Time 14.00(14.00)\n",
      "Iter 0074 | Time 44.9920(53.9700) | Bit/dim 6.4802(7.2412) | Xent 1.9980(2.1164) | Loss 7.4791(8.2994) | Error 0.6705(0.7484) Steps 538(556.10) | Grad Norm 2.2493(9.6352) | Total Time 14.00(14.00)\n",
      "Iter 0075 | Time 45.3797(53.7123) | Bit/dim 6.4656(7.2180) | Xent 2.0427(2.1142) | Loss 7.4870(8.2751) | Error 0.7328(0.7479) Steps 538(555.56) | Grad Norm 12.4387(9.7193) | Total Time 14.00(14.00)\n",
      "Iter 0076 | Time 45.1509(53.4554) | Bit/dim 6.3984(7.1934) | Xent 2.0546(2.1124) | Loss 7.4256(8.2496) | Error 0.7223(0.7472) Steps 538(555.03) | Grad Norm 12.0750(9.7900) | Total Time 14.00(14.00)\n",
      "Iter 0077 | Time 47.6418(53.2810) | Bit/dim 6.4701(7.1717) | Xent 2.1893(2.1147) | Loss 7.5648(8.2290) | Error 0.7869(0.7484) Steps 538(554.52) | Grad Norm 42.5552(10.7729) | Total Time 14.00(14.00)\n",
      "Iter 0078 | Time 47.9305(53.1205) | Bit/dim 6.7945(7.1604) | Xent 3.2548(2.1489) | Loss 8.4220(8.2348) | Error 0.8839(0.7524) Steps 538(554.03) | Grad Norm 79.1715(12.8249) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0013 | Time 20.9510, Epoch Time 315.8644(359.6101), Bit/dim 6.3860(best: 6.5616), Xent 2.3042, Loss 7.5381, Error 0.8393(best: 0.6763)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0079 | Time 45.7510(52.8994) | Bit/dim 6.3816(7.1370) | Xent 2.2963(2.1533) | Loss 7.5298(8.2137) | Error 0.8371(0.7550) Steps 538(553.54) | Grad Norm 41.1057(13.6733) | Total Time 14.00(14.00)\n",
      "Iter 0080 | Time 49.9391(52.8106) | Bit/dim 6.4426(7.1162) | Xent 2.0945(2.1516) | Loss 7.4898(8.1920) | Error 0.7565(0.7550) Steps 544(553.26) | Grad Norm 28.8342(14.1282) | Total Time 14.00(14.00)\n",
      "Iter 0081 | Time 46.3064(52.6155) | Bit/dim 6.2563(7.0904) | Xent 2.1442(2.1513) | Loss 7.3284(8.1661) | Error 0.7585(0.7551) Steps 538(552.80) | Grad Norm 10.5767(14.0216) | Total Time 14.00(14.00)\n",
      "Iter 0082 | Time 47.0536(52.4486) | Bit/dim 6.4271(7.0705) | Xent 2.1144(2.1502) | Loss 7.4843(8.1456) | Error 0.7640(0.7554) Steps 538(552.36) | Grad Norm 25.6896(14.3717) | Total Time 14.00(14.00)\n",
      "Iter 0083 | Time 47.1034(52.2883) | Bit/dim 6.2358(7.0454) | Xent 2.0882(2.1484) | Loss 7.2799(8.1196) | Error 0.7469(0.7551) Steps 538(551.93) | Grad Norm 11.1382(14.2746) | Total Time 14.00(14.00)\n",
      "Iter 0084 | Time 50.0071(52.2198) | Bit/dim 6.2128(7.0205) | Xent 2.2427(2.1512) | Loss 7.3342(8.0961) | Error 0.7995(0.7565) Steps 538(551.51) | Grad Norm 25.8646(14.6223) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0014 | Time 21.0767, Epoch Time 322.8845(358.5083), Bit/dim 6.1789(best: 6.3860), Xent 2.4245, Loss 7.3912, Error 0.8500(best: 0.6763)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0085 | Time 46.1493(52.0377) | Bit/dim 6.1763(6.9951) | Xent 2.4269(2.1595) | Loss 7.3897(8.0749) | Error 0.8494(0.7592) Steps 544(551.28) | Grad Norm 33.6953(15.1945) | Total Time 14.00(14.00)\n",
      "Iter 0086 | Time 47.0458(51.8880) | Bit/dim 6.2145(6.9717) | Xent 2.3476(2.1651) | Loss 7.3883(8.0543) | Error 0.8260(0.7613) Steps 544(551.06) | Grad Norm 37.2388(15.8559) | Total Time 14.00(14.00)\n",
      "Iter 0087 | Time 48.0749(51.7736) | Bit/dim 6.0680(6.9446) | Xent 2.1277(2.1640) | Loss 7.1319(8.0266) | Error 0.7502(0.7609) Steps 544(550.85) | Grad Norm 11.5444(15.7265) | Total Time 14.00(14.00)\n",
      "Iter 0088 | Time 45.2543(51.5780) | Bit/dim 6.0607(6.9181) | Xent 2.1428(2.1634) | Loss 7.1321(7.9998) | Error 0.7624(0.7610) Steps 526(550.11) | Grad Norm 17.3925(15.7765) | Total Time 14.00(14.00)\n",
      "Iter 0089 | Time 47.2194(51.4472) | Bit/dim 6.0305(6.8915) | Xent 2.2076(2.1647) | Loss 7.1343(7.9738) | Error 0.7911(0.7619) Steps 538(549.74) | Grad Norm 15.6539(15.7728) | Total Time 14.00(14.00)\n",
      "Iter 0090 | Time 44.1973(51.2297) | Bit/dim 5.9939(6.8645) | Xent 2.1254(2.1635) | Loss 7.0566(7.9463) | Error 0.7659(0.7620) Steps 532(549.21) | Grad Norm 7.9278(15.5375) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0015 | Time 20.2851, Epoch Time 313.9616(357.1719), Bit/dim 5.9745(best: 6.1789), Xent 2.0968, Loss 7.0228, Error 0.7271(best: 0.6763)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0091 | Time 43.6593(51.0026) | Bit/dim 5.9763(6.8379) | Xent 2.1184(2.1622) | Loss 7.0355(7.9190) | Error 0.7450(0.7615) Steps 514(548.15) | Grad Norm 11.8513(15.4269) | Total Time 14.00(14.00)\n",
      "Iter 0092 | Time 46.5007(50.8676) | Bit/dim 5.9352(6.8108) | Xent 2.1046(2.1604) | Loss 6.9875(7.8910) | Error 0.7318(0.7606) Steps 508(546.95) | Grad Norm 7.5370(15.1902) | Total Time 14.00(14.00)\n",
      "Iter 0093 | Time 46.2248(50.7283) | Bit/dim 5.9238(6.7842) | Xent 2.0862(2.1582) | Loss 6.9669(7.8633) | Error 0.7311(0.7597) Steps 532(546.50) | Grad Norm 5.3375(14.8946) | Total Time 14.00(14.00)\n",
      "Iter 0094 | Time 44.1701(50.5315) | Bit/dim 5.9113(6.7580) | Xent 2.0906(2.1562) | Loss 6.9566(7.8361) | Error 0.7418(0.7592) Steps 526(545.89) | Grad Norm 7.0191(14.6583) | Total Time 14.00(14.00)\n",
      "Iter 0095 | Time 43.8101(50.3299) | Bit/dim 5.8476(6.7307) | Xent 2.0792(2.1539) | Loss 6.8872(7.8076) | Error 0.7261(0.7582) Steps 532(545.47) | Grad Norm 2.4687(14.2927) | Total Time 14.00(14.00)\n",
      "Iter 0096 | Time 46.1175(50.2035) | Bit/dim 5.8275(6.7036) | Xent 2.0891(2.1519) | Loss 6.8720(7.7796) | Error 0.7278(0.7573) Steps 520(544.71) | Grad Norm 4.3148(13.9933) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0016 | Time 20.6291, Epoch Time 306.9825(355.6662), Bit/dim 5.7907(best: 5.9745), Xent 2.0805, Loss 6.8309, Error 0.7129(best: 0.6763)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0097 | Time 46.5288(50.0933) | Bit/dim 5.8001(6.6765) | Xent 2.1008(2.1504) | Loss 6.8505(7.7517) | Error 0.7320(0.7565) Steps 526(544.14) | Grad Norm 4.3091(13.7028) | Total Time 14.00(14.00)\n",
      "Iter 0098 | Time 47.1451(50.0048) | Bit/dim 5.7760(6.6495) | Xent 2.0821(2.1483) | Loss 6.8171(7.7237) | Error 0.7263(0.7556) Steps 538(543.96) | Grad Norm 1.5683(13.3388) | Total Time 14.00(14.00)\n",
      "Iter 0099 | Time 45.1205(49.8583) | Bit/dim 5.7429(6.6223) | Xent 2.0868(2.1465) | Loss 6.7863(7.6955) | Error 0.7364(0.7550) Steps 538(543.78) | Grad Norm 3.9710(13.0577) | Total Time 14.00(14.00)\n",
      "Iter 0100 | Time 44.1427(49.6868) | Bit/dim 5.7240(6.5953) | Xent 2.0810(2.1445) | Loss 6.7645(7.6676) | Error 0.7389(0.7545) Steps 538(543.61) | Grad Norm 3.5348(12.7720) | Total Time 14.00(14.00)\n",
      "Iter 0101 | Time 44.8168(49.5407) | Bit/dim 5.6923(6.5682) | Xent 2.0795(2.1426) | Loss 6.7321(7.6395) | Error 0.7181(0.7534) Steps 526(543.08) | Grad Norm 1.5102(12.4342) | Total Time 14.00(14.00)\n",
      "Iter 0102 | Time 45.7618(49.4274) | Bit/dim 5.7049(6.5423) | Xent 2.0821(2.1408) | Loss 6.7459(7.6127) | Error 0.7243(0.7526) Steps 526(542.57) | Grad Norm 3.7113(12.1725) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0017 | Time 20.8074, Epoch Time 310.1286(354.3001), Bit/dim 5.6719(best: 5.7907), Xent 2.0499, Loss 6.6969, Error 0.7013(best: 0.6763)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0103 | Time 43.3526(49.2451) | Bit/dim 5.6687(6.5161) | Xent 2.0655(2.1385) | Loss 6.7015(7.5854) | Error 0.7230(0.7517) Steps 526(542.07) | Grad Norm 2.2204(11.8739) | Total Time 14.00(14.00)\n",
      "Iter 0104 | Time 45.4903(49.1325) | Bit/dim 5.6514(6.4902) | Xent 2.0692(2.1364) | Loss 6.6860(7.5584) | Error 0.7255(0.7509) Steps 520(541.41) | Grad Norm 2.7533(11.6003) | Total Time 14.00(14.00)\n",
      "Iter 0105 | Time 42.3814(48.9299) | Bit/dim 5.6417(6.4647) | Xent 2.0606(2.1342) | Loss 6.6720(7.5318) | Error 0.7191(0.7499) Steps 526(540.95) | Grad Norm 3.9826(11.3718) | Total Time 14.00(14.00)\n",
      "Iter 0106 | Time 42.6812(48.7425) | Bit/dim 5.6301(6.4397) | Xent 2.0648(2.1321) | Loss 6.6626(7.5057) | Error 0.7183(0.7490) Steps 520(540.32) | Grad Norm 1.5693(11.0777) | Total Time 14.00(14.00)\n",
      "Iter 0107 | Time 41.9429(48.5385) | Bit/dim 5.6129(6.4149) | Xent 2.0606(2.1299) | Loss 6.6432(7.4799) | Error 0.7154(0.7480) Steps 520(539.71) | Grad Norm 3.6757(10.8557) | Total Time 14.00(14.00)\n",
      "Iter 0108 | Time 42.5535(48.3589) | Bit/dim 5.5876(6.3901) | Xent 2.0541(2.1277) | Loss 6.6147(7.4539) | Error 0.7081(0.7468) Steps 520(539.12) | Grad Norm 2.1569(10.5947) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0018 | Time 20.2967, Epoch Time 294.3945(352.5029), Bit/dim 5.5725(best: 5.6719), Xent 2.0158, Loss 6.5804, Error 0.6830(best: 0.6763)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0109 | Time 42.3830(48.1797) | Bit/dim 5.5766(6.3657) | Xent 2.0302(2.1247) | Loss 6.5917(7.4280) | Error 0.7030(0.7455) Steps 520(538.54) | Grad Norm 3.2946(10.3757) | Total Time 14.00(14.00)\n",
      "Iter 0110 | Time 42.4466(48.0077) | Bit/dim 5.5452(6.3411) | Xent 2.0379(2.1221) | Loss 6.5642(7.4021) | Error 0.6991(0.7441) Steps 526(538.17) | Grad Norm 1.5697(10.1115) | Total Time 14.00(14.00)\n",
      "Iter 0111 | Time 42.5381(47.8436) | Bit/dim 5.5389(6.3170) | Xent 2.0480(2.1199) | Loss 6.5629(7.3769) | Error 0.7047(0.7429) Steps 526(537.80) | Grad Norm 3.9877(9.9278) | Total Time 14.00(14.00)\n",
      "Iter 0112 | Time 42.0881(47.6709) | Bit/dim 5.5157(6.2930) | Xent 2.0208(2.1169) | Loss 6.5261(7.3514) | Error 0.6994(0.7416) Steps 526(537.45) | Grad Norm 1.6918(9.6807) | Total Time 14.00(14.00)\n",
      "Iter 0113 | Time 46.8464(47.6462) | Bit/dim 5.5080(6.2694) | Xent 2.0175(2.1139) | Loss 6.5168(7.3264) | Error 0.6841(0.7399) Steps 520(536.92) | Grad Norm 3.7678(9.5033) | Total Time 14.00(14.00)\n",
      "Iter 0114 | Time 43.1966(47.5127) | Bit/dim 5.4903(6.2460) | Xent 2.0242(2.1113) | Loss 6.5024(7.3017) | Error 0.7047(0.7388) Steps 514(536.24) | Grad Norm 6.3153(9.4077) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0019 | Time 20.5910, Epoch Time 295.3268(350.7876), Bit/dim 5.4791(best: 5.5725), Xent 1.9851, Loss 6.4717, Error 0.6664(best: 0.6763)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0115 | Time 44.8128(47.4317) | Bit/dim 5.4967(6.2236) | Xent 2.0107(2.1082) | Loss 6.5021(7.2777) | Error 0.6910(0.7374) Steps 526(535.93) | Grad Norm 5.3385(9.2856) | Total Time 14.00(14.00)\n",
      "Iter 0116 | Time 46.9239(47.4165) | Bit/dim 5.4483(6.2003) | Xent 2.0093(2.1053) | Loss 6.4530(7.2529) | Error 0.6934(0.7361) Steps 520(535.45) | Grad Norm 3.2105(9.1034) | Total Time 14.00(14.00)\n",
      "Iter 0117 | Time 44.5045(47.3291) | Bit/dim 5.4571(6.1780) | Xent 2.0002(2.1021) | Loss 6.4572(7.2291) | Error 0.6811(0.7344) Steps 526(535.17) | Grad Norm 1.1975(8.8662) | Total Time 14.00(14.00)\n",
      "Iter 0118 | Time 42.8363(47.1943) | Bit/dim 5.4435(6.1560) | Xent 1.9918(2.0988) | Loss 6.4394(7.2054) | Error 0.6784(0.7327) Steps 526(534.89) | Grad Norm 4.1042(8.7233) | Total Time 14.00(14.00)\n",
      "Iter 0119 | Time 43.7236(47.0902) | Bit/dim 5.4024(6.1334) | Xent 2.0200(2.0964) | Loss 6.4124(7.1816) | Error 0.6992(0.7317) Steps 520(534.45) | Grad Norm 9.0997(8.7346) | Total Time 14.00(14.00)\n",
      "Iter 0120 | Time 47.2122(47.0939) | Bit/dim 5.4272(6.1122) | Xent 2.0601(2.0954) | Loss 6.4572(7.1599) | Error 0.7355(0.7318) Steps 544(534.73) | Grad Norm 22.7706(9.1557) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0020 | Time 20.6247, Epoch Time 306.1759(349.4493), Bit/dim 5.5556(best: 5.4791), Xent 2.5571, Loss 6.8341, Error 0.8051(best: 0.6664)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0121 | Time 43.7906(46.9948) | Bit/dim 5.5467(6.0952) | Xent 2.5657(2.1095) | Loss 6.8295(7.1499) | Error 0.8056(0.7341) Steps 520(534.29) | Grad Norm 57.2962(10.5999) | Total Time 14.00(14.00)\n",
      "Iter 0122 | Time 41.8307(46.8398) | Bit/dim 5.8352(6.0874) | Xent 2.6388(2.1253) | Loss 7.1546(7.1501) | Error 0.8279(0.7369) Steps 508(533.50) | Grad Norm 46.2162(11.6684) | Total Time 14.00(14.00)\n",
      "Iter 0123 | Time 42.7948(46.7185) | Bit/dim 5.7162(6.0763) | Xent 2.0552(2.1232) | Loss 6.7438(7.1379) | Error 0.7398(0.7370) Steps 514(532.92) | Grad Norm 10.8895(11.6450) | Total Time 14.00(14.00)\n",
      "Iter 0124 | Time 45.5921(46.6847) | Bit/dim 5.6428(6.0633) | Xent 2.0883(2.1222) | Loss 6.6869(7.1244) | Error 0.7586(0.7376) Steps 538(533.07) | Grad Norm 6.4081(11.4879) | Total Time 14.00(14.00)\n",
      "Iter 0125 | Time 42.5977(46.5621) | Bit/dim 5.5920(6.0491) | Xent 2.1196(2.1221) | Loss 6.6518(7.1102) | Error 0.7724(0.7387) Steps 520(532.68) | Grad Norm 5.6901(11.3140) | Total Time 14.00(14.00)\n",
      "Iter 0126 | Time 42.8675(46.4513) | Bit/dim 5.5199(6.0333) | Xent 2.1407(2.1227) | Loss 6.5902(7.0946) | Error 0.7677(0.7395) Steps 508(531.94) | Grad Norm 3.8766(11.0909) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0021 | Time 21.1440, Epoch Time 296.0604(347.8476), Bit/dim 5.5377(best: 5.4791), Xent 2.2328, Loss 6.6541, Error 0.7978(best: 0.6664)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0127 | Time 46.4609(46.4515) | Bit/dim 5.5595(6.0190) | Xent 2.2721(2.1272) | Loss 6.6956(7.0826) | Error 0.8046(0.7415) Steps 538(532.12) | Grad Norm 14.5590(11.1949) | Total Time 14.00(14.00)\n",
      "Iter 0128 | Time 43.2325(46.3550) | Bit/dim 5.6026(6.0065) | Xent 2.2280(2.1302) | Loss 6.7166(7.0716) | Error 0.8044(0.7434) Steps 508(531.40) | Grad Norm 10.6239(11.1778) | Total Time 14.00(14.00)\n",
      "Iter 0129 | Time 45.7248(46.3361) | Bit/dim 5.6317(5.9953) | Xent 2.1712(2.1314) | Loss 6.7173(7.0610) | Error 0.7821(0.7445) Steps 532(531.41) | Grad Norm 13.0515(11.2340) | Total Time 14.00(14.00)\n",
      "Iter 0130 | Time 44.7034(46.2871) | Bit/dim 5.4065(5.9776) | Xent 2.2159(2.1339) | Loss 6.5145(7.0446) | Error 0.7937(0.7460) Steps 532(531.43) | Grad Norm 7.9520(11.1355) | Total Time 14.00(14.00)\n",
      "Iter 0131 | Time 42.1679(46.1635) | Bit/dim 5.4694(5.9624) | Xent 2.1419(2.1342) | Loss 6.5404(7.0295) | Error 0.7641(0.7465) Steps 514(530.91) | Grad Norm 4.7088(10.9427) | Total Time 14.00(14.00)\n",
      "Iter 0132 | Time 45.7169(46.1501) | Bit/dim 5.4465(5.9469) | Xent 2.1661(2.1351) | Loss 6.5296(7.0145) | Error 0.7994(0.7481) Steps 532(530.94) | Grad Norm 9.4500(10.8979) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0022 | Time 21.1352, Epoch Time 304.6879(346.5528), Bit/dim 5.3907(best: 5.4791), Xent 2.0963, Loss 6.4389, Error 0.7650(best: 0.6664)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0133 | Time 44.7008(46.1066) | Bit/dim 5.3874(5.9301) | Xent 2.1414(2.1353) | Loss 6.4581(6.9978) | Error 0.7802(0.7491) Steps 538(531.15) | Grad Norm 4.4933(10.7058) | Total Time 14.00(14.00)\n",
      "Iter 0134 | Time 43.4172(46.0260) | Bit/dim 5.3898(5.9139) | Xent 2.1481(2.1357) | Loss 6.4639(6.9818) | Error 0.7775(0.7499) Steps 532(531.18) | Grad Norm 6.7056(10.5858) | Total Time 14.00(14.00)\n",
      "Iter 0135 | Time 46.0783(46.0275) | Bit/dim 5.3763(5.8978) | Xent 2.1222(2.1353) | Loss 6.4375(6.9655) | Error 0.7659(0.7504) Steps 562(532.10) | Grad Norm 5.2821(10.4267) | Total Time 14.00(14.00)\n",
      "Iter 0136 | Time 48.6629(46.1066) | Bit/dim 5.3518(5.8814) | Xent 2.1315(2.1352) | Loss 6.4175(6.9490) | Error 0.7649(0.7509) Steps 556(532.82) | Grad Norm 4.3285(10.2437) | Total Time 14.00(14.00)\n",
      "Iter 0137 | Time 44.4588(46.0571) | Bit/dim 5.3275(5.8648) | Xent 2.1179(2.1347) | Loss 6.3865(6.9321) | Error 0.7632(0.7512) Steps 562(533.70) | Grad Norm 5.5992(10.1044) | Total Time 14.00(14.00)\n",
      "Iter 0138 | Time 45.1032(46.0285) | Bit/dim 5.3087(5.8481) | Xent 2.0986(2.1336) | Loss 6.3580(6.9149) | Error 0.7434(0.7510) Steps 562(534.54) | Grad Norm 2.5347(9.8773) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0023 | Time 21.5677, Epoch Time 309.6178(345.4448), Bit/dim 5.2878(best: 5.3907), Xent 2.0791, Loss 6.3273, Error 0.7289(best: 0.6664)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0139 | Time 45.4658(46.0116) | Bit/dim 5.2898(5.8314) | Xent 2.1215(2.1332) | Loss 6.3505(6.8980) | Error 0.7640(0.7514) Steps 550(535.01) | Grad Norm 4.4050(9.7131) | Total Time 14.00(14.00)\n",
      "Iter 0140 | Time 42.8996(45.9183) | Bit/dim 5.2699(5.8145) | Xent 2.1028(2.1323) | Loss 6.3213(6.8807) | Error 0.7492(0.7513) Steps 538(535.10) | Grad Norm 3.5878(9.5294) | Total Time 14.00(14.00)\n",
      "Iter 0141 | Time 42.8431(45.8260) | Bit/dim 5.2386(5.7972) | Xent 2.0801(2.1308) | Loss 6.2787(6.8626) | Error 0.7309(0.7507) Steps 550(535.54) | Grad Norm 1.7156(9.2950) | Total Time 14.00(14.00)\n",
      "Iter 0142 | Time 46.7495(45.8537) | Bit/dim 5.2119(5.7797) | Xent 2.1063(2.1300) | Loss 6.2651(6.8447) | Error 0.7586(0.7509) Steps 538(535.62) | Grad Norm 3.8793(9.1325) | Total Time 14.00(14.00)\n",
      "Iter 0143 | Time 45.5606(45.8449) | Bit/dim 5.2085(5.7625) | Xent 2.0904(2.1288) | Loss 6.2537(6.8270) | Error 0.7325(0.7504) Steps 538(535.69) | Grad Norm 2.1619(8.9234) | Total Time 14.00(14.00)\n",
      "Iter 0144 | Time 43.5466(45.7760) | Bit/dim 5.1911(5.7454) | Xent 2.0905(2.1277) | Loss 6.2363(6.8092) | Error 0.7502(0.7504) Steps 532(535.58) | Grad Norm 3.0549(8.7473) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0024 | Time 21.3839, Epoch Time 303.9878(344.2011), Bit/dim 5.1745(best: 5.2878), Xent 2.0426, Loss 6.1959, Error 0.7169(best: 0.6664)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0145 | Time 45.7310(45.7746) | Bit/dim 5.1743(5.7283) | Xent 2.0726(2.1260) | Loss 6.2107(6.7913) | Error 0.7369(0.7500) Steps 538(535.65) | Grad Norm 2.6029(8.5630) | Total Time 14.00(14.00)\n",
      "Iter 0146 | Time 46.8129(45.8058) | Bit/dim 5.1601(5.7112) | Xent 2.0826(2.1247) | Loss 6.2014(6.7736) | Error 0.7335(0.7495) Steps 538(535.72) | Grad Norm 2.1405(8.3703) | Total Time 14.00(14.00)\n",
      "Iter 0147 | Time 48.5905(45.8893) | Bit/dim 5.1370(5.6940) | Xent 2.0660(2.1230) | Loss 6.1700(6.7555) | Error 0.7316(0.7490) Steps 538(535.79) | Grad Norm 3.4663(8.2232) | Total Time 14.00(14.00)\n",
      "Iter 0148 | Time 48.2752(45.9609) | Bit/dim 5.1399(5.6774) | Xent 2.0701(2.1214) | Loss 6.1749(6.7381) | Error 0.7330(0.7485) Steps 532(535.68) | Grad Norm 1.5944(8.0243) | Total Time 14.00(14.00)\n",
      "Iter 0149 | Time 43.9511(45.9006) | Bit/dim 5.1308(5.6610) | Xent 2.0682(2.1198) | Loss 6.1649(6.7209) | Error 0.7331(0.7480) Steps 496(534.49) | Grad Norm 3.2542(7.8812) | Total Time 14.00(14.00)\n",
      "Iter 0150 | Time 45.3940(45.8854) | Bit/dim 5.1178(5.6447) | Xent 2.0456(2.1176) | Loss 6.1406(6.7035) | Error 0.7235(0.7473) Steps 502(533.51) | Grad Norm 2.3166(7.7143) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0025 | Time 20.4076, Epoch Time 314.6040(343.3131), Bit/dim 5.1002(best: 5.1745), Xent 2.0189, Loss 6.1096, Error 0.6990(best: 0.6664)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0151 | Time 46.5616(45.9057) | Bit/dim 5.1092(5.6286) | Xent 2.0501(2.1155) | Loss 6.1342(6.6864) | Error 0.7163(0.7463) Steps 490(532.21) | Grad Norm 3.2489(7.5803) | Total Time 14.00(14.00)\n",
      "Iter 0152 | Time 46.7409(45.9308) | Bit/dim 5.1015(5.6128) | Xent 2.0322(2.1130) | Loss 6.1176(6.6693) | Error 0.7076(0.7452) Steps 490(530.94) | Grad Norm 1.8236(7.4076) | Total Time 14.00(14.00)\n",
      "Iter 0153 | Time 47.3111(45.9722) | Bit/dim 5.0882(5.5971) | Xent 2.0232(2.1103) | Loss 6.0998(6.6522) | Error 0.7064(0.7440) Steps 496(529.89) | Grad Norm 3.0196(7.2760) | Total Time 14.00(14.00)\n",
      "Iter 0154 | Time 44.6250(45.9318) | Bit/dim 5.0539(5.5808) | Xent 2.0354(2.1081) | Loss 6.0716(6.6348) | Error 0.7120(0.7431) Steps 496(528.88) | Grad Norm 1.7642(7.1106) | Total Time 14.00(14.00)\n",
      "Iter 0155 | Time 49.9312(46.0517) | Bit/dim 5.0657(5.5653) | Xent 2.0275(2.1057) | Loss 6.0794(6.6182) | Error 0.7103(0.7421) Steps 490(527.71) | Grad Norm 2.6083(6.9756) | Total Time 14.00(14.00)\n",
      "Iter 0156 | Time 47.2000(46.0862) | Bit/dim 5.0513(5.5499) | Xent 2.0299(2.1034) | Loss 6.0663(6.6016) | Error 0.7155(0.7413) Steps 490(526.58) | Grad Norm 3.7473(6.8787) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0026 | Time 20.0734, Epoch Time 318.2194(342.5603), Bit/dim 5.0468(best: 5.1002), Xent 1.9804, Loss 6.0370, Error 0.6732(best: 0.6664)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0157 | Time 44.6921(46.0444) | Bit/dim 5.0443(5.5347) | Xent 2.0151(2.1007) | Loss 6.0519(6.5851) | Error 0.7003(0.7400) Steps 502(525.84) | Grad Norm 4.3247(6.8021) | Total Time 14.00(14.00)\n",
      "Iter 0158 | Time 44.8045(46.0072) | Bit/dim 5.0501(5.5202) | Xent 2.0088(2.0980) | Loss 6.0545(6.5692) | Error 0.6946(0.7387) Steps 502(525.13) | Grad Norm 1.6828(6.6485) | Total Time 14.00(14.00)\n",
      "Iter 0159 | Time 44.1392(45.9511) | Bit/dim 5.0023(5.5047) | Xent 1.9829(2.0945) | Loss 5.9938(6.5519) | Error 0.6941(0.7373) Steps 502(524.43) | Grad Norm 1.4874(6.4937) | Total Time 14.00(14.00)\n",
      "Iter 0160 | Time 45.5535(45.9392) | Bit/dim 5.0152(5.4900) | Xent 1.9973(2.0916) | Loss 6.0138(6.5358) | Error 0.6969(0.7361) Steps 508(523.94) | Grad Norm 3.5773(6.4062) | Total Time 14.00(14.00)\n",
      "Iter 0161 | Time 46.5950(45.9589) | Bit/dim 5.0087(5.4755) | Xent 2.0183(2.0894) | Loss 6.0179(6.5202) | Error 0.7160(0.7355) Steps 508(523.46) | Grad Norm 6.6038(6.4121) | Total Time 14.00(14.00)\n",
      "Iter 0162 | Time 44.2052(45.9063) | Bit/dim 5.0099(5.4616) | Xent 2.0604(2.0885) | Loss 6.0401(6.5058) | Error 0.7308(0.7354) Steps 514(523.18) | Grad Norm 13.1061(6.6129) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0027 | Time 20.1257, Epoch Time 305.8325(341.4585), Bit/dim 5.0143(best: 5.0468), Xent 2.1970, Loss 6.1128, Error 0.7653(best: 0.6664)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0163 | Time 46.5737(45.9263) | Bit/dim 5.0290(5.4486) | Xent 2.2262(2.0927) | Loss 6.1421(6.4949) | Error 0.7734(0.7365) Steps 514(522.90) | Grad Norm 23.2172(7.1111) | Total Time 14.00(14.00)\n",
      "Iter 0164 | Time 45.6673(45.9185) | Bit/dim 5.0383(5.4363) | Xent 2.3412(2.1001) | Loss 6.2089(6.4863) | Error 0.8000(0.7384) Steps 520(522.81) | Grad Norm 23.5383(7.6039) | Total Time 14.00(14.00)\n",
      "Iter 0165 | Time 48.8533(46.0066) | Bit/dim 4.9911(5.4229) | Xent 1.9874(2.0968) | Loss 5.9848(6.4713) | Error 0.7066(0.7375) Steps 508(522.37) | Grad Norm 4.4417(7.5090) | Total Time 14.00(14.00)\n",
      "Iter 0166 | Time 46.1214(46.0100) | Bit/dim 4.9940(5.4101) | Xent 2.1365(2.0979) | Loss 6.0622(6.4590) | Error 0.7731(0.7385) Steps 490(521.40) | Grad Norm 12.1413(7.6480) | Total Time 14.00(14.00)\n",
      "Iter 0167 | Time 46.8766(46.0360) | Bit/dim 4.9536(5.3964) | Xent 2.0074(2.0952) | Loss 5.9573(6.4440) | Error 0.7132(0.7378) Steps 502(520.82) | Grad Norm 2.1369(7.4827) | Total Time 14.00(14.00)\n",
      "Iter 0168 | Time 47.7668(46.0879) | Bit/dim 4.9605(5.3833) | Xent 2.0455(2.0937) | Loss 5.9832(6.4302) | Error 0.7315(0.7376) Steps 484(519.71) | Grad Norm 5.1464(7.4126) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0028 | Time 19.9061, Epoch Time 317.1002(340.7278), Bit/dim 4.9613(best: 5.0143), Xent 1.9964, Loss 5.9595, Error 0.6984(best: 0.6664)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0169 | Time 47.1750(46.1205) | Bit/dim 4.9598(5.3706) | Xent 2.0244(2.0917) | Loss 5.9720(6.4164) | Error 0.7119(0.7368) Steps 490(518.82) | Grad Norm 4.0663(7.3122) | Total Time 14.00(14.00)\n",
      "Iter 0170 | Time 49.3943(46.2188) | Bit/dim 4.9448(5.3578) | Xent 2.0277(2.0897) | Loss 5.9586(6.4027) | Error 0.7100(0.7360) Steps 478(517.60) | Grad Norm 2.6157(7.1713) | Total Time 14.00(14.00)\n",
      "Iter 0171 | Time 46.9859(46.2418) | Bit/dim 4.9260(5.3448) | Xent 2.0255(2.0878) | Loss 5.9387(6.3888) | Error 0.7160(0.7354) Steps 484(516.59) | Grad Norm 2.2058(7.0223) | Total Time 14.00(14.00)\n",
      "Iter 0172 | Time 45.7707(46.2276) | Bit/dim 4.9254(5.3323) | Xent 2.0062(2.0854) | Loss 5.9285(6.3749) | Error 0.7130(0.7347) Steps 496(515.97) | Grad Norm 3.0842(6.9042) | Total Time 14.00(14.00)\n",
      "Iter 0173 | Time 46.5064(46.2360) | Bit/dim 4.9195(5.3199) | Xent 2.0203(2.0834) | Loss 5.9297(6.3616) | Error 0.7211(0.7343) Steps 484(515.01) | Grad Norm 2.4346(6.7701) | Total Time 14.00(14.00)\n",
      "Iter 0174 | Time 45.1270(46.2027) | Bit/dim 4.9310(5.3082) | Xent 1.9937(2.0807) | Loss 5.9279(6.3486) | Error 0.7020(0.7334) Steps 478(513.90) | Grad Norm 2.4168(6.6395) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0029 | Time 19.4332, Epoch Time 316.3616(339.9968), Bit/dim 4.9152(best: 4.9613), Xent 1.9575, Loss 5.8939, Error 0.6913(best: 0.6664)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0175 | Time 49.0659(46.2886) | Bit/dim 4.9145(5.2964) | Xent 1.9814(2.0777) | Loss 5.9053(6.3353) | Error 0.7031(0.7325) Steps 490(513.18) | Grad Norm 2.2415(6.5075) | Total Time 14.00(14.00)\n",
      "Iter 0176 | Time 44.8346(46.2450) | Bit/dim 4.9062(5.2847) | Xent 1.9757(2.0747) | Loss 5.8941(6.3220) | Error 0.6861(0.7311) Steps 508(513.03) | Grad Norm 1.4430(6.3556) | Total Time 14.00(14.00)\n",
      "Iter 0177 | Time 47.6893(46.2883) | Bit/dim 4.8853(5.2727) | Xent 1.9914(2.0722) | Loss 5.8810(6.3088) | Error 0.6955(0.7300) Steps 478(511.98) | Grad Norm 2.0836(6.2274) | Total Time 14.00(14.00)\n",
      "Iter 0178 | Time 48.0314(46.3406) | Bit/dim 4.8749(5.2608) | Xent 1.9801(2.0694) | Loss 5.8650(6.2955) | Error 0.6927(0.7289) Steps 478(510.96) | Grad Norm 2.5548(6.1173) | Total Time 14.00(14.00)\n",
      "Iter 0179 | Time 46.9820(46.3599) | Bit/dim 4.8663(5.2490) | Xent 1.9681(2.0664) | Loss 5.8504(6.2821) | Error 0.6909(0.7277) Steps 478(509.97) | Grad Norm 1.5668(5.9808) | Total Time 14.00(14.00)\n",
      "Iter 0180 | Time 47.0182(46.3796) | Bit/dim 4.8707(5.2376) | Xent 1.9607(2.0632) | Loss 5.8511(6.2692) | Error 0.6904(0.7266) Steps 484(509.19) | Grad Norm 2.0097(5.8616) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0030 | Time 19.4786, Epoch Time 318.7239(339.3586), Bit/dim 4.8518(best: 4.9152), Xent 1.9329, Loss 5.8182, Error 0.6626(best: 0.6664)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0181 | Time 45.1615(46.3431) | Bit/dim 4.8618(5.2263) | Xent 1.9683(2.0604) | Loss 5.8459(6.2565) | Error 0.6910(0.7256) Steps 478(508.26) | Grad Norm 2.9881(5.7754) | Total Time 14.00(14.00)\n",
      "Iter 0182 | Time 46.8996(46.3598) | Bit/dim 4.8514(5.2151) | Xent 1.9585(2.0573) | Loss 5.8307(6.2437) | Error 0.6894(0.7245) Steps 490(507.71) | Grad Norm 3.0503(5.6937) | Total Time 14.00(14.00)\n",
      "Iter 0183 | Time 47.7392(46.4012) | Bit/dim 4.8464(5.2040) | Xent 1.9299(2.0535) | Loss 5.8113(6.2308) | Error 0.6695(0.7228) Steps 484(507.00) | Grad Norm 2.1876(5.5885) | Total Time 14.00(14.00)\n",
      "Iter 0184 | Time 49.0976(46.4820) | Bit/dim 4.8308(5.1928) | Xent 1.9510(2.0504) | Loss 5.8063(6.2180) | Error 0.6850(0.7217) Steps 496(506.67) | Grad Norm 2.9548(5.5095) | Total Time 14.00(14.00)\n",
      "Iter 0185 | Time 45.8955(46.4644) | Bit/dim 4.8176(5.1816) | Xent 1.9501(2.0474) | Loss 5.7927(6.2053) | Error 0.6836(0.7205) Steps 502(506.53) | Grad Norm 3.2400(5.4414) | Total Time 14.00(14.00)\n",
      "Iter 0186 | Time 48.9497(46.5390) | Bit/dim 4.8054(5.1703) | Xent 1.9382(2.0441) | Loss 5.7745(6.1924) | Error 0.6783(0.7193) Steps 502(506.39) | Grad Norm 1.2768(5.3165) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0031 | Time 20.1099, Epoch Time 319.4087(338.7601), Bit/dim 4.8094(best: 4.8518), Xent 1.8999, Loss 5.7593, Error 0.6542(best: 0.6626)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0187 | Time 48.2210(46.5895) | Bit/dim 4.8052(5.1593) | Xent 1.9284(2.0407) | Loss 5.7694(6.1797) | Error 0.6736(0.7179) Steps 490(505.90) | Grad Norm 1.6949(5.2078) | Total Time 14.00(14.00)\n",
      "Iter 0188 | Time 47.4016(46.6138) | Bit/dim 4.7956(5.1484) | Xent 1.9234(2.0371) | Loss 5.7574(6.1670) | Error 0.6759(0.7166) Steps 490(505.42) | Grad Norm 1.1713(5.0867) | Total Time 14.00(14.00)\n",
      "Iter 0189 | Time 46.6349(46.6145) | Bit/dim 4.7788(5.1373) | Xent 1.9260(2.0338) | Loss 5.7418(6.1542) | Error 0.6795(0.7155) Steps 496(505.14) | Grad Norm 1.5280(4.9799) | Total Time 14.00(14.00)\n",
      "Iter 0190 | Time 47.4764(46.6403) | Bit/dim 4.7909(5.1269) | Xent 1.9142(2.0302) | Loss 5.7480(6.1420) | Error 0.6633(0.7140) Steps 502(505.04) | Grad Norm 2.9775(4.9199) | Total Time 14.00(14.00)\n",
      "Iter 0191 | Time 46.8395(46.6463) | Bit/dim 4.7899(5.1168) | Xent 1.9802(2.0287) | Loss 5.7801(6.1312) | Error 0.7007(0.7136) Steps 502(504.95) | Grad Norm 11.0644(5.1042) | Total Time 14.00(14.00)\n",
      "Iter 0192 | Time 43.9905(46.5666) | Bit/dim 4.9662(5.1123) | Xent 2.1649(2.0328) | Loss 6.0487(6.1287) | Error 0.7479(0.7146) Steps 514(505.23) | Grad Norm 18.2427(5.4984) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0032 | Time 20.2648, Epoch Time 316.0872(338.0799), Bit/dim 4.7924(best: 4.8094), Xent 1.8882, Loss 5.7365, Error 0.6542(best: 0.6542)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0193 | Time 43.9453(46.4880) | Bit/dim 4.7845(5.1025) | Xent 1.9288(2.0297) | Loss 5.7489(6.1173) | Error 0.6746(0.7134) Steps 508(505.31) | Grad Norm 5.6913(5.5042) | Total Time 14.00(14.00)\n",
      "Iter 0194 | Time 46.4010(46.4854) | Bit/dim 4.7987(5.0934) | Xent 1.9955(2.0287) | Loss 5.7965(6.1077) | Error 0.7169(0.7135) Steps 496(505.03) | Grad Norm 9.6066(5.6272) | Total Time 14.00(14.00)\n",
      "Iter 0195 | Time 47.2039(46.5069) | Bit/dim 4.8439(5.0859) | Xent 1.9378(2.0259) | Loss 5.8127(6.0988) | Error 0.6790(0.7125) Steps 526(505.66) | Grad Norm 4.8514(5.6040) | Total Time 14.00(14.00)\n",
      "Iter 0196 | Time 47.3584(46.5325) | Bit/dim 4.8051(5.0775) | Xent 1.9789(2.0245) | Loss 5.7946(6.0897) | Error 0.7025(0.7122) Steps 514(505.91) | Grad Norm 4.3612(5.5667) | Total Time 14.00(14.00)\n",
      "Iter 0197 | Time 49.1709(46.6116) | Bit/dim 4.7750(5.0684) | Xent 1.9693(2.0229) | Loss 5.7596(6.0798) | Error 0.6991(0.7118) Steps 496(505.61) | Grad Norm 4.1706(5.5248) | Total Time 14.00(14.00)\n",
      "Iter 0198 | Time 46.5962(46.6112) | Bit/dim 4.8083(5.0606) | Xent 1.9456(2.0205) | Loss 5.7811(6.0709) | Error 0.6786(0.7108) Steps 514(505.86) | Grad Norm 4.5898(5.4967) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0033 | Time 20.3631, Epoch Time 316.6069(337.4357), Bit/dim 4.7781(best: 4.7924), Xent 1.9208, Loss 5.7385, Error 0.6697(best: 0.6542)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0199 | Time 47.5579(46.6396) | Bit/dim 4.7744(5.0520) | Xent 1.9539(2.0185) | Loss 5.7514(6.0613) | Error 0.6910(0.7102) Steps 490(505.39) | Grad Norm 2.9018(5.4189) | Total Time 14.00(14.00)\n",
      "Iter 0200 | Time 48.8187(46.7049) | Bit/dim 4.7964(5.0443) | Xent 1.9799(2.0174) | Loss 5.7864(6.0530) | Error 0.7006(0.7099) Steps 502(505.29) | Grad Norm 7.6528(5.4859) | Total Time 14.00(14.00)\n",
      "Iter 0201 | Time 47.7205(46.7354) | Bit/dim 4.8151(5.0375) | Xent 1.9385(2.0150) | Loss 5.7843(6.0450) | Error 0.6869(0.7092) Steps 490(504.83) | Grad Norm 5.4896(5.4860) | Total Time 14.00(14.00)\n",
      "Iter 0202 | Time 46.5589(46.7301) | Bit/dim 4.7736(5.0295) | Xent 1.9381(2.0127) | Loss 5.7427(6.0359) | Error 0.6804(0.7083) Steps 490(504.38) | Grad Norm 4.0137(5.4418) | Total Time 14.00(14.00)\n",
      "Iter 0203 | Time 47.1838(46.7437) | Bit/dim 4.8334(5.0237) | Xent 1.9648(2.0113) | Loss 5.8157(6.0293) | Error 0.7014(0.7081) Steps 502(504.31) | Grad Norm 9.6845(5.5691) | Total Time 14.00(14.00)\n",
      "Iter 0204 | Time 45.6432(46.7107) | Bit/dim 4.8053(5.0171) | Xent 1.9303(2.0088) | Loss 5.7705(6.0215) | Error 0.6745(0.7071) Steps 520(504.78) | Grad Norm 4.4600(5.5359) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0034 | Time 20.1010, Epoch Time 319.6133(336.9010), Bit/dim 4.8256(best: 4.7781), Xent 1.8876, Loss 5.7694, Error 0.6483(best: 0.6542)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0205 | Time 45.8575(46.6851) | Bit/dim 4.8202(5.0112) | Xent 1.9155(2.0060) | Loss 5.7780(6.0142) | Error 0.6701(0.7060) Steps 490(504.34) | Grad Norm 5.5436(5.5361) | Total Time 14.00(14.00)\n",
      "Iter 0206 | Time 46.6356(46.6836) | Bit/dim 4.7975(5.0048) | Xent 1.9263(2.0036) | Loss 5.7607(6.0066) | Error 0.6830(0.7053) Steps 496(504.09) | Grad Norm 5.8224(5.5447) | Total Time 14.00(14.00)\n",
      "Iter 0207 | Time 46.9439(46.6914) | Bit/dim 4.7351(4.9967) | Xent 1.9334(2.0015) | Loss 5.7018(5.9975) | Error 0.6767(0.7045) Steps 490(503.66) | Grad Norm 8.4764(5.6326) | Total Time 14.00(14.00)\n",
      "Iter 0208 | Time 47.7833(46.7242) | Bit/dim 4.7432(4.9891) | Xent 1.9317(1.9994) | Loss 5.7091(5.9888) | Error 0.6834(0.7038) Steps 502(503.62) | Grad Norm 7.6405(5.6929) | Total Time 14.00(14.00)\n",
      "Iter 0209 | Time 49.5430(46.8088) | Bit/dim 4.7553(4.9821) | Xent 1.9022(1.9965) | Loss 5.7064(5.9803) | Error 0.6600(0.7025) Steps 508(503.75) | Grad Norm 4.0969(5.6450) | Total Time 14.00(14.00)\n",
      "Iter 0210 | Time 48.8717(46.8706) | Bit/dim 4.7304(4.9745) | Xent 1.9056(1.9938) | Loss 5.6832(5.9714) | Error 0.6663(0.7014) Steps 544(504.95) | Grad Norm 5.8637(5.6515) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0035 | Time 21.1552, Epoch Time 322.3227(336.4637), Bit/dim 4.7056(best: 4.7781), Xent 1.8374, Loss 5.6243, Error 0.6356(best: 0.6483)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0211 | Time 48.4739(46.9187) | Bit/dim 4.7020(4.9663) | Xent 1.8856(1.9906) | Loss 5.6448(5.9616) | Error 0.6644(0.7003) Steps 544(506.13) | Grad Norm 4.6696(5.6221) | Total Time 14.00(14.00)\n",
      "Iter 0212 | Time 46.8708(46.9173) | Bit/dim 4.7090(4.9586) | Xent 1.8738(1.9871) | Loss 5.6459(5.9522) | Error 0.6586(0.6991) Steps 496(505.82) | Grad Norm 2.7954(5.5373) | Total Time 14.00(14.00)\n",
      "Iter 0213 | Time 47.0087(46.9200) | Bit/dim 4.6801(4.9503) | Xent 1.8904(1.9842) | Loss 5.6253(5.9424) | Error 0.6650(0.6980) Steps 508(505.89) | Grad Norm 3.4441(5.4745) | Total Time 14.00(14.00)\n",
      "Iter 0214 | Time 51.0368(47.0436) | Bit/dim 4.6659(4.9417) | Xent 1.8732(1.9808) | Loss 5.6025(5.9322) | Error 0.6527(0.6967) Steps 508(505.95) | Grad Norm 6.9590(5.5190) | Total Time 14.00(14.00)\n",
      "Iter 0215 | Time 47.9649(47.0712) | Bit/dim 4.6617(4.9333) | Xent 1.8575(1.9771) | Loss 5.5904(5.9219) | Error 0.6458(0.6952) Steps 514(506.19) | Grad Norm 5.0049(5.5036) | Total Time 14.00(14.00)\n",
      "Iter 0216 | Time 47.6961(47.0899) | Bit/dim 4.6920(4.9261) | Xent 1.8581(1.9736) | Loss 5.6211(5.9129) | Error 0.6462(0.6937) Steps 526(506.79) | Grad Norm 5.2446(5.4958) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0036 | Time 21.7556, Epoch Time 326.1978(336.1557), Bit/dim 4.7079(best: 4.7056), Xent 1.8559, Loss 5.6359, Error 0.6568(best: 0.6356)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0217 | Time 44.9297(47.0251) | Bit/dim 4.7162(4.9198) | Xent 1.8983(1.9713) | Loss 5.6654(5.9055) | Error 0.6681(0.6929) Steps 544(507.90) | Grad Norm 13.7806(5.7444) | Total Time 14.00(14.00)\n",
      "Iter 0218 | Time 50.2814(47.1228) | Bit/dim 4.8101(4.9165) | Xent 1.9412(1.9704) | Loss 5.7807(5.9017) | Error 0.6903(0.6928) Steps 562(509.53) | Grad Norm 15.2851(6.0306) | Total Time 14.00(14.00)\n",
      "Iter 0219 | Time 49.0992(47.1821) | Bit/dim 4.6403(4.9082) | Xent 1.8805(1.9677) | Loss 5.5805(5.8921) | Error 0.6591(0.6918) Steps 532(510.20) | Grad Norm 5.0265(6.0005) | Total Time 14.00(14.00)\n",
      "Iter 0220 | Time 48.6632(47.2265) | Bit/dim 4.7545(4.9036) | Xent 1.8907(1.9654) | Loss 5.6998(5.8863) | Error 0.6711(0.6912) Steps 556(511.57) | Grad Norm 12.7822(6.2039) | Total Time 14.00(14.00)\n",
      "Iter 0221 | Time 47.6644(47.2397) | Bit/dim 4.7301(4.8984) | Xent 1.9277(1.9643) | Loss 5.6940(5.8805) | Error 0.6765(0.6908) Steps 550(512.73) | Grad Norm 7.9257(6.2556) | Total Time 14.00(14.00)\n",
      "Iter 0222 | Time 48.3969(47.2744) | Bit/dim 4.7448(4.8938) | Xent 1.9009(1.9624) | Loss 5.6953(5.8750) | Error 0.6724(0.6902) Steps 526(513.12) | Grad Norm 6.8230(6.2726) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0037 | Time 20.8546, Epoch Time 325.3489(335.8315), Bit/dim 4.6625(best: 4.7056), Xent 1.8843, Loss 5.6047, Error 0.6761(best: 0.6356)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0223 | Time 48.0977(47.2991) | Bit/dim 4.6658(4.8870) | Xent 1.9328(1.9615) | Loss 5.6321(5.8677) | Error 0.6867(0.6901) Steps 526(513.51) | Grad Norm 7.2103(6.3007) | Total Time 14.00(14.00)\n",
      "Iter 0224 | Time 48.3792(47.3315) | Bit/dim 4.7283(4.8822) | Xent 1.9301(1.9605) | Loss 5.6933(5.8625) | Error 0.6877(0.6900) Steps 526(513.89) | Grad Norm 10.7137(6.4331) | Total Time 14.00(14.00)\n",
      "Iter 0225 | Time 47.3486(47.3320) | Bit/dim 4.7078(4.8770) | Xent 1.8837(1.9582) | Loss 5.6496(5.8561) | Error 0.6540(0.6890) Steps 520(514.07) | Grad Norm 6.2679(6.4282) | Total Time 14.00(14.00)\n",
      "Iter 0226 | Time 50.6858(47.4326) | Bit/dim 4.6964(4.8715) | Xent 1.9013(1.9565) | Loss 5.6470(5.8498) | Error 0.6723(0.6885) Steps 526(514.43) | Grad Norm 4.5095(6.3706) | Total Time 14.00(14.00)\n",
      "Iter 0227 | Time 48.5071(47.4649) | Bit/dim 4.6608(4.8652) | Xent 1.8925(1.9546) | Loss 5.6070(5.8425) | Error 0.6839(0.6883) Steps 562(515.85) | Grad Norm 6.0190(6.3601) | Total Time 14.00(14.00)\n",
      "Iter 0228 | Time 48.9294(47.5088) | Bit/dim 4.6549(4.8589) | Xent 1.8835(1.9525) | Loss 5.5967(5.8351) | Error 0.6664(0.6877) Steps 544(516.70) | Grad Norm 6.1284(6.3531) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0038 | Time 20.2196, Epoch Time 327.3668(335.5776), Bit/dim 4.6353(best: 4.6625), Xent 1.8117, Loss 5.5412, Error 0.6327(best: 0.6356)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0229 | Time 45.9436(47.4618) | Bit/dim 4.6315(4.8521) | Xent 1.8613(1.9497) | Loss 5.5622(5.8270) | Error 0.6584(0.6868) Steps 490(515.90) | Grad Norm 5.7947(6.3364) | Total Time 14.00(14.00)\n",
      "Iter 0230 | Time 45.8923(47.4148) | Bit/dim 4.6598(4.8463) | Xent 1.8477(1.9467) | Loss 5.5836(5.8197) | Error 0.6504(0.6857) Steps 508(515.66) | Grad Norm 3.9957(6.2661) | Total Time 14.00(14.00)\n",
      "Iter 0231 | Time 49.1546(47.4669) | Bit/dim 4.6172(4.8395) | Xent 1.8317(1.9432) | Loss 5.5331(5.8111) | Error 0.6486(0.6846) Steps 496(515.07) | Grad Norm 4.1880(6.2038) | Total Time 14.00(14.00)\n",
      "Iter 0232 | Time 47.1198(47.4565) | Bit/dim 4.6382(4.8334) | Xent 1.8876(1.9415) | Loss 5.5820(5.8042) | Error 0.6645(0.6840) Steps 490(514.32) | Grad Norm 5.9238(6.1954) | Total Time 14.00(14.00)\n",
      "Iter 0233 | Time 50.2706(47.5410) | Bit/dim 4.5935(4.8262) | Xent 1.8164(1.9378) | Loss 5.5017(5.7951) | Error 0.6367(0.6826) Steps 538(515.03) | Grad Norm 4.1342(6.1336) | Total Time 14.00(14.00)\n",
      "Iter 0234 | Time 51.7663(47.6677) | Bit/dim 4.6098(4.8197) | Xent 1.8173(1.9342) | Loss 5.5185(5.7868) | Error 0.6429(0.6814) Steps 526(515.36) | Grad Norm 5.2098(6.1058) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0039 | Time 21.2070, Epoch Time 326.9145(335.3177), Bit/dim 4.5936(best: 4.6353), Xent 1.7788, Loss 5.4830, Error 0.6235(best: 0.6327)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0235 | Time 47.6454(47.6670) | Bit/dim 4.5879(4.8128) | Xent 1.8217(1.9308) | Loss 5.4987(5.7782) | Error 0.6431(0.6802) Steps 526(515.68) | Grad Norm 6.5273(6.1185) | Total Time 14.00(14.00)\n",
      "Iter 0236 | Time 51.5246(47.7828) | Bit/dim 4.5853(4.8059) | Xent 1.8409(1.9281) | Loss 5.5057(5.7700) | Error 0.6581(0.6796) Steps 556(516.89) | Grad Norm 7.7226(6.1666) | Total Time 14.00(14.00)\n",
      "Iter 0237 | Time 46.5467(47.7457) | Bit/dim 4.5602(4.7986) | Xent 1.8110(1.9246) | Loss 5.4656(5.7609) | Error 0.6355(0.6782) Steps 550(517.88) | Grad Norm 2.9312(6.0696) | Total Time 14.00(14.00)\n",
      "Iter 0238 | Time 48.8041(47.7774) | Bit/dim 4.5804(4.7920) | Xent 1.8046(1.9210) | Loss 5.4828(5.7525) | Error 0.6396(0.6771) Steps 514(517.76) | Grad Norm 6.9144(6.0949) | Total Time 14.00(14.00)\n",
      "Iter 0239 | Time 48.9967(47.8140) | Bit/dim 4.5468(4.7847) | Xent 1.8190(1.9179) | Loss 5.4563(5.7436) | Error 0.6347(0.6758) Steps 538(518.37) | Grad Norm 6.7548(6.1147) | Total Time 14.00(14.00)\n",
      "Iter 0240 | Time 49.8054(47.8738) | Bit/dim 4.5677(4.7782) | Xent 1.8128(1.9148) | Loss 5.4741(5.7356) | Error 0.6462(0.6749) Steps 514(518.24) | Grad Norm 5.8018(6.1053) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0040 | Time 20.6918, Epoch Time 329.5301(335.1440), Bit/dim 4.5679(best: 4.5936), Xent 1.7606, Loss 5.4482, Error 0.6081(best: 0.6235)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0241 | Time 47.4265(47.8603) | Bit/dim 4.5653(4.7718) | Xent 1.8031(1.9114) | Loss 5.4669(5.7275) | Error 0.6404(0.6739) Steps 538(518.83) | Grad Norm 7.2664(6.1401) | Total Time 14.00(14.00)\n",
      "Iter 0242 | Time 48.3340(47.8746) | Bit/dim 4.5558(4.7653) | Xent 1.8096(1.9084) | Loss 5.4606(5.7195) | Error 0.6431(0.6730) Steps 538(519.41) | Grad Norm 8.0477(6.1974) | Total Time 14.00(14.00)\n",
      "Iter 0243 | Time 48.5241(47.8940) | Bit/dim 4.5433(4.7586) | Xent 1.7681(1.9042) | Loss 5.4274(5.7107) | Error 0.6216(0.6714) Steps 538(519.97) | Grad Norm 5.2660(6.1694) | Total Time 14.00(14.00)\n",
      "Iter 0244 | Time 47.4267(47.8800) | Bit/dim 4.5180(4.7514) | Xent 1.8111(1.9014) | Loss 5.4236(5.7021) | Error 0.6421(0.6705) Steps 544(520.69) | Grad Norm 5.7636(6.1572) | Total Time 14.00(14.00)\n",
      "Iter 0245 | Time 48.7098(47.9049) | Bit/dim 4.5197(4.7445) | Xent 1.8254(1.8991) | Loss 5.4324(5.6940) | Error 0.6492(0.6699) Steps 544(521.39) | Grad Norm 9.6212(6.2612) | Total Time 14.00(14.00)\n",
      "Iter 0246 | Time 49.3302(47.9477) | Bit/dim 4.5357(4.7382) | Xent 1.8305(1.8970) | Loss 5.4509(5.6867) | Error 0.6544(0.6694) Steps 538(521.88) | Grad Norm 11.3116(6.4127) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0041 | Time 21.3580, Epoch Time 326.4453(334.8831), Bit/dim 4.5010(best: 4.5679), Xent 1.7835, Loss 5.3927, Error 0.6216(best: 0.6081)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0247 | Time 48.7595(47.9720) | Bit/dim 4.4982(4.7310) | Xent 1.8331(1.8951) | Loss 5.4148(5.6786) | Error 0.6492(0.6688) Steps 550(522.73) | Grad Norm 8.0755(6.4626) | Total Time 14.00(14.00)\n",
      "Iter 0248 | Time 50.0543(48.0345) | Bit/dim 4.5112(4.7244) | Xent 1.7855(1.8918) | Loss 5.4039(5.6703) | Error 0.6280(0.6676) Steps 544(523.37) | Grad Norm 4.4311(6.4016) | Total Time 14.00(14.00)\n",
      "Iter 0249 | Time 49.8572(48.0892) | Bit/dim 4.6112(4.7210) | Xent 1.8120(1.8894) | Loss 5.5172(5.6657) | Error 0.6324(0.6666) Steps 520(523.27) | Grad Norm 9.7659(6.5026) | Total Time 14.00(14.00)\n",
      "Iter 0250 | Time 51.2679(48.1845) | Bit/dim 4.7002(4.7204) | Xent 2.0502(1.8943) | Loss 5.7253(5.6675) | Error 0.7046(0.6677) Steps 544(523.89) | Grad Norm 26.3949(7.0993) | Total Time 14.00(14.00)\n",
      "Iter 0251 | Time 46.9289(48.1469) | Bit/dim 4.9598(4.7276) | Xent 2.1169(1.9009) | Loss 6.0182(5.6780) | Error 0.7531(0.6703) Steps 568(525.21) | Grad Norm 12.0719(7.2485) | Total Time 14.00(14.00)\n",
      "Iter 0252 | Time 50.7634(48.2254) | Bit/dim 4.9571(4.7345) | Xent 2.0429(1.9052) | Loss 5.9786(5.6871) | Error 0.7263(0.6719) Steps 598(527.39) | Grad Norm 7.0143(7.2415) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0042 | Time 22.1688, Epoch Time 335.2541(334.8942), Bit/dim 4.7012(best: 4.5010), Xent 1.9783, Loss 5.6903, Error 0.6941(best: 0.6081)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0253 | Time 50.1681(48.2836) | Bit/dim 4.7009(4.7335) | Xent 2.0318(1.9090) | Loss 5.7168(5.6880) | Error 0.7170(0.6733) Steps 592(529.33) | Grad Norm 3.9203(7.1418) | Total Time 14.00(14.00)\n",
      "Iter 0254 | Time 52.8033(48.4192) | Bit/dim 4.7529(4.7340) | Xent 2.0401(1.9129) | Loss 5.7729(5.6905) | Error 0.7219(0.6747) Steps 592(531.21) | Grad Norm 6.1814(7.1130) | Total Time 14.00(14.00)\n",
      "Iter 0255 | Time 51.0209(48.4973) | Bit/dim 4.7149(4.7335) | Xent 2.0099(1.9158) | Loss 5.7198(5.6914) | Error 0.7166(0.6760) Steps 574(532.50) | Grad Norm 4.5309(7.0356) | Total Time 14.00(14.00)\n",
      "Iter 0256 | Time 48.6802(48.5028) | Bit/dim 4.7045(4.7326) | Xent 1.9489(1.9168) | Loss 5.6789(5.6910) | Error 0.6870(0.6763) Steps 544(532.84) | Grad Norm 3.9987(6.9445) | Total Time 14.00(14.00)\n",
      "Iter 0257 | Time 47.0495(48.4592) | Bit/dim 4.7290(4.7325) | Xent 1.9242(1.9171) | Loss 5.6910(5.6910) | Error 0.6847(0.6766) Steps 544(533.18) | Grad Norm 3.6391(6.8453) | Total Time 14.00(14.00)\n",
      "Iter 0258 | Time 48.8694(48.4715) | Bit/dim 4.6806(4.7309) | Xent 1.9156(1.9170) | Loss 5.6384(5.6894) | Error 0.6835(0.6768) Steps 538(533.32) | Grad Norm 3.3234(6.7396) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0043 | Time 19.9486, Epoch Time 334.3012(334.8764), Bit/dim 4.6670(best: 4.5010), Xent 1.8600, Loss 5.5970, Error 0.6519(best: 0.6081)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0259 | Time 45.3893(48.3790) | Bit/dim 4.6616(4.7288) | Xent 1.9270(1.9173) | Loss 5.6251(5.6875) | Error 0.6889(0.6772) Steps 502(532.38) | Grad Norm 5.0874(6.6901) | Total Time 14.00(14.00)\n",
      "Iter 0260 | Time 45.2645(48.2856) | Bit/dim 4.6271(4.7258) | Xent 1.8993(1.9168) | Loss 5.5768(5.6842) | Error 0.6727(0.6770) Steps 514(531.83) | Grad Norm 3.0068(6.5796) | Total Time 14.00(14.00)\n",
      "Iter 0261 | Time 46.6094(48.2353) | Bit/dim 4.6335(4.7230) | Xent 1.8841(1.9158) | Loss 5.5756(5.6809) | Error 0.6619(0.6766) Steps 514(531.29) | Grad Norm 4.1163(6.5057) | Total Time 14.00(14.00)\n",
      "Iter 0262 | Time 47.3975(48.2102) | Bit/dim 4.6072(4.7196) | Xent 1.9143(1.9157) | Loss 5.5644(5.6774) | Error 0.6663(0.6763) Steps 526(531.14) | Grad Norm 3.0116(6.4008) | Total Time 14.00(14.00)\n",
      "Iter 0263 | Time 46.8364(48.1689) | Bit/dim 4.5938(4.7158) | Xent 1.9233(1.9160) | Loss 5.5555(5.6738) | Error 0.6798(0.6764) Steps 526(530.98) | Grad Norm 4.3273(6.3386) | Total Time 14.00(14.00)\n",
      "Iter 0264 | Time 51.2335(48.2609) | Bit/dim 4.5807(4.7117) | Xent 1.9200(1.9161) | Loss 5.5407(5.6698) | Error 0.6738(0.6763) Steps 550(531.55) | Grad Norm 3.3469(6.2489) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0044 | Time 20.7611, Epoch Time 318.8356(334.3952), Bit/dim 4.5624(best: 4.5010), Xent 1.8480, Loss 5.4864, Error 0.6350(best: 0.6081)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0265 | Time 50.0516(48.3146) | Bit/dim 4.5481(4.7068) | Xent 1.8926(1.9154) | Loss 5.4944(5.6645) | Error 0.6584(0.6757) Steps 532(531.57) | Grad Norm 2.6135(6.1398) | Total Time 14.00(14.00)\n",
      "Iter 0266 | Time 46.4479(48.2586) | Bit/dim 4.5541(4.7022) | Xent 1.8953(1.9148) | Loss 5.5018(5.6596) | Error 0.6653(0.6754) Steps 526(531.40) | Grad Norm 1.8962(6.0125) | Total Time 14.00(14.00)\n",
      "Iter 0267 | Time 48.2146(48.2573) | Bit/dim 4.5354(4.6972) | Xent 1.8939(1.9142) | Loss 5.4823(5.6543) | Error 0.6605(0.6750) Steps 526(531.24) | Grad Norm 2.9664(5.9211) | Total Time 14.00(14.00)\n",
      "Iter 0268 | Time 46.3596(48.2004) | Bit/dim 4.5279(4.6922) | Xent 1.8586(1.9125) | Loss 5.4572(5.6484) | Error 0.6646(0.6747) Steps 490(530.00) | Grad Norm 2.6462(5.8229) | Total Time 14.00(14.00)\n",
      "Iter 0269 | Time 46.9863(48.1639) | Bit/dim 4.5215(4.6870) | Xent 1.8471(1.9105) | Loss 5.4451(5.6423) | Error 0.6455(0.6738) Steps 490(528.80) | Grad Norm 1.8260(5.7030) | Total Time 14.00(14.00)\n",
      "Iter 0270 | Time 44.0977(48.0420) | Bit/dim 4.5075(4.6816) | Xent 1.8372(1.9083) | Loss 5.4261(5.6358) | Error 0.6370(0.6727) Steps 490(527.64) | Grad Norm 2.5360(5.6080) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0045 | Time 19.7836, Epoch Time 317.3387(333.8835), Bit/dim 4.4922(best: 4.5010), Xent 1.7979, Loss 5.3912, Error 0.6200(best: 0.6081)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0271 | Time 45.7706(47.9738) | Bit/dim 4.4850(4.6757) | Xent 1.8243(1.9058) | Loss 5.3971(5.6287) | Error 0.6411(0.6717) Steps 490(526.51) | Grad Norm 5.4674(5.6038) | Total Time 14.00(14.00)\n",
      "Iter 0272 | Time 48.0981(47.9775) | Bit/dim 4.5020(4.6705) | Xent 1.9157(1.9061) | Loss 5.4599(5.6236) | Error 0.6739(0.6718) Steps 502(525.77) | Grad Norm 12.1355(5.7997) | Total Time 14.00(14.00)\n",
      "Iter 0273 | Time 48.1461(47.9826) | Bit/dim 4.7459(4.6728) | Xent 2.2110(1.9153) | Loss 5.8514(5.6304) | Error 0.7619(0.6745) Steps 550(526.50) | Grad Norm 23.0352(6.3168) | Total Time 14.00(14.00)\n",
      "Iter 0274 | Time 48.6685(48.0032) | Bit/dim 4.6443(4.6719) | Xent 2.2764(1.9261) | Loss 5.7824(5.6350) | Error 0.7522(0.6768) Steps 550(527.20) | Grad Norm 21.5277(6.7731) | Total Time 14.00(14.00)\n",
      "Iter 0275 | Time 54.4463(48.1965) | Bit/dim 4.5762(4.6691) | Xent 1.9386(1.9265) | Loss 5.5455(5.6323) | Error 0.6979(0.6775) Steps 544(527.71) | Grad Norm 4.5402(6.7061) | Total Time 14.00(14.00)\n",
      "Iter 0276 | Time 50.6581(48.2703) | Bit/dim 4.5334(4.6650) | Xent 1.9570(1.9274) | Loss 5.5118(5.6287) | Error 0.6952(0.6780) Steps 574(529.10) | Grad Norm 4.7061(6.6461) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0046 | Time 21.9870, Epoch Time 333.0519(333.8586), Bit/dim 4.5391(best: 4.4922), Xent 1.9070, Loss 5.4926, Error 0.6736(best: 0.6081)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0277 | Time 47.4726(48.2464) | Bit/dim 4.5399(4.6612) | Xent 1.9502(1.9281) | Loss 5.5150(5.6253) | Error 0.6935(0.6785) Steps 574(530.44) | Grad Norm 4.3384(6.5769) | Total Time 14.00(14.00)\n",
      "Iter 0278 | Time 52.4345(48.3720) | Bit/dim 4.5448(4.6578) | Xent 1.9276(1.9280) | Loss 5.5086(5.6218) | Error 0.6834(0.6786) Steps 610(532.83) | Grad Norm 3.5138(6.4850) | Total Time 14.00(14.00)\n",
      "Iter 0279 | Time 50.9792(48.4502) | Bit/dim 4.5385(4.6542) | Xent 1.9190(1.9278) | Loss 5.4980(5.6181) | Error 0.6864(0.6789) Steps 580(534.25) | Grad Norm 4.0246(6.4112) | Total Time 14.00(14.00)\n",
      "Iter 0280 | Time 47.2641(48.4147) | Bit/dim 4.5414(4.6508) | Xent 2.0421(1.9312) | Loss 5.5625(5.6164) | Error 0.7324(0.6805) Steps 556(534.90) | Grad Norm 11.4346(6.5619) | Total Time 14.00(14.00)\n",
      "Iter 0281 | Time 51.3927(48.5040) | Bit/dim 4.5772(4.6486) | Xent 2.2932(1.9421) | Loss 5.7238(5.6196) | Error 0.7877(0.6837) Steps 574(536.07) | Grad Norm 20.1190(6.9686) | Total Time 14.00(14.00)\n",
      "Iter 0282 | Time 47.6932(48.4797) | Bit/dim 4.6173(4.6476) | Xent 2.1727(1.9490) | Loss 5.7036(5.6221) | Error 0.7812(0.6866) Steps 550(536.49) | Grad Norm 10.7527(7.0821) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0047 | Time 21.8167, Epoch Time 334.3162(333.8723), Bit/dim 4.5845(best: 4.4922), Xent 2.1409, Loss 5.6550, Error 0.7654(best: 0.6081)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0283 | Time 47.0128(48.4357) | Bit/dim 4.5779(4.6456) | Xent 2.1803(1.9559) | Loss 5.6681(5.6235) | Error 0.7903(0.6897) Steps 556(537.07) | Grad Norm 6.4826(7.0641) | Total Time 14.00(14.00)\n",
      "Iter 0284 | Time 48.8701(48.4487) | Bit/dim 4.5443(4.6425) | Xent 2.1013(1.9603) | Loss 5.5950(5.6227) | Error 0.7600(0.6918) Steps 574(538.18) | Grad Norm 3.2922(6.9510) | Total Time 14.00(14.00)\n",
      "Iter 0285 | Time 50.4046(48.5074) | Bit/dim 4.5798(4.6406) | Xent 2.0985(1.9644) | Loss 5.6290(5.6229) | Error 0.7572(0.6938) Steps 592(539.80) | Grad Norm 4.9223(6.8901) | Total Time 14.00(14.00)\n",
      "Iter 0286 | Time 51.9621(48.6110) | Bit/dim 4.5545(4.6380) | Xent 2.0418(1.9668) | Loss 5.5754(5.6214) | Error 0.7326(0.6950) Steps 574(540.82) | Grad Norm 2.9731(6.7726) | Total Time 14.00(14.00)\n",
      "Iter 0287 | Time 48.8608(48.6185) | Bit/dim 4.5342(4.6349) | Xent 2.0207(1.9684) | Loss 5.5445(5.6191) | Error 0.7298(0.6960) Steps 544(540.92) | Grad Norm 2.6915(6.6502) | Total Time 14.00(14.00)\n",
      "Iter 0288 | Time 46.3772(48.5513) | Bit/dim 4.5121(4.6312) | Xent 2.0083(1.9696) | Loss 5.5162(5.6160) | Error 0.7166(0.6966) Steps 544(541.01) | Grad Norm 3.2642(6.5486) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0048 | Time 20.8920, Epoch Time 329.6555(333.7458), Bit/dim 4.5026(best: 4.4922), Xent 1.9457, Loss 5.4754, Error 0.6950(best: 0.6081)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0289 | Time 46.7500(48.4972) | Bit/dim 4.4933(4.6271) | Xent 1.9812(1.9699) | Loss 5.4839(5.6121) | Error 0.7130(0.6971) Steps 526(540.56) | Grad Norm 2.9380(6.4403) | Total Time 14.00(14.00)\n",
      "Iter 0290 | Time 46.9166(48.4498) | Bit/dim 4.4857(4.6229) | Xent 1.9672(1.9698) | Loss 5.4693(5.6078) | Error 0.7007(0.6972) Steps 544(540.66) | Grad Norm 2.5594(6.3238) | Total Time 14.00(14.00)\n",
      "Iter 0291 | Time 47.1141(48.4097) | Bit/dim 4.4655(4.6181) | Xent 1.9330(1.9687) | Loss 5.4320(5.6025) | Error 0.6941(0.6971) Steps 550(540.94) | Grad Norm 1.9763(6.1934) | Total Time 14.00(14.00)\n",
      "Iter 0292 | Time 52.4303(48.5304) | Bit/dim 4.4741(4.6138) | Xent 1.9355(1.9677) | Loss 5.4418(5.5977) | Error 0.6932(0.6970) Steps 562(541.57) | Grad Norm 2.9408(6.0958) | Total Time 14.00(14.00)\n",
      "Iter 0293 | Time 50.2665(48.5824) | Bit/dim 4.4446(4.6087) | Xent 1.9132(1.9661) | Loss 5.4012(5.5918) | Error 0.6887(0.6968) Steps 574(542.55) | Grad Norm 2.1134(5.9764) | Total Time 14.00(14.00)\n",
      "Iter 0294 | Time 50.8308(48.6499) | Bit/dim 4.4466(4.6039) | Xent 1.9052(1.9643) | Loss 5.3992(5.5860) | Error 0.6825(0.6963) Steps 556(542.95) | Grad Norm 1.8758(5.8533) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0049 | Time 21.0490, Epoch Time 330.7037(333.6545), Bit/dim 4.4321(best: 4.4922), Xent 1.8381, Loss 5.3511, Error 0.6425(best: 0.6081)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0295 | Time 50.3950(48.7022) | Bit/dim 4.4257(4.5985) | Xent 1.8785(1.9617) | Loss 5.3649(5.5794) | Error 0.6556(0.6951) Steps 568(543.70) | Grad Norm 1.8843(5.7343) | Total Time 14.00(14.00)\n",
      "Iter 0296 | Time 51.5532(48.7878) | Bit/dim 4.4085(4.5928) | Xent 1.8779(1.9592) | Loss 5.3475(5.5724) | Error 0.6761(0.6945) Steps 562(544.25) | Grad Norm 2.1176(5.6258) | Total Time 14.00(14.00)\n",
      "Iter 0297 | Time 48.1541(48.7688) | Bit/dim 4.4259(4.5878) | Xent 1.8624(1.9563) | Loss 5.3571(5.5660) | Error 0.6655(0.6937) Steps 562(544.78) | Grad Norm 1.6031(5.5051) | Total Time 14.00(14.00)\n",
      "Iter 0298 | Time 48.2715(48.7538) | Bit/dim 4.4100(4.5825) | Xent 1.8500(1.9531) | Loss 5.3350(5.5590) | Error 0.6586(0.6926) Steps 538(544.58) | Grad Norm 1.8815(5.3964) | Total Time 14.00(14.00)\n",
      "Iter 0299 | Time 49.6339(48.7802) | Bit/dim 4.4004(4.5770) | Xent 1.8468(1.9499) | Loss 5.3239(5.5520) | Error 0.6574(0.6916) Steps 544(544.56) | Grad Norm 2.0745(5.2967) | Total Time 14.00(14.00)\n",
      "Iter 0300 | Time 48.7846(48.7804) | Bit/dim 4.3815(4.5712) | Xent 1.8405(1.9466) | Loss 5.3017(5.5445) | Error 0.6452(0.6902) Steps 544(544.55) | Grad Norm 2.4332(5.2108) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0050 | Time 20.5760, Epoch Time 332.9122(333.6322), Bit/dim 4.3821(best: 4.4321), Xent 1.7836, Loss 5.2739, Error 0.6252(best: 0.6081)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0301 | Time 50.5127(48.8323) | Bit/dim 4.3726(4.5652) | Xent 1.8227(1.9429) | Loss 5.2839(5.5367) | Error 0.6510(0.6890) Steps 538(544.35) | Grad Norm 1.5613(5.1013) | Total Time 14.00(14.00)\n",
      "Iter 0302 | Time 49.8716(48.8635) | Bit/dim 4.3717(4.5594) | Xent 1.8164(1.9391) | Loss 5.2799(5.5290) | Error 0.6410(0.6876) Steps 520(543.62) | Grad Norm 3.5104(5.0536) | Total Time 14.00(14.00)\n",
      "Iter 0303 | Time 46.3801(48.7890) | Bit/dim 4.3664(4.5536) | Xent 1.8348(1.9360) | Loss 5.2838(5.5216) | Error 0.6538(0.6865) Steps 550(543.81) | Grad Norm 5.9468(5.0804) | Total Time 14.00(14.00)\n",
      "Iter 0304 | Time 53.7661(48.9383) | Bit/dim 4.3735(4.5482) | Xent 1.8771(1.9342) | Loss 5.3120(5.5153) | Error 0.6713(0.6861) Steps 520(543.10) | Grad Norm 8.3821(5.1795) | Total Time 14.00(14.00)\n",
      "Iter 0305 | Time 45.3270(48.8300) | Bit/dim 4.4314(4.5447) | Xent 1.9327(1.9342) | Loss 5.3977(5.5118) | Error 0.6876(0.6861) Steps 538(542.94) | Grad Norm 11.3961(5.3660) | Total Time 14.00(14.00)\n",
      "Iter 0306 | Time 50.5773(48.8824) | Bit/dim 4.3947(4.5402) | Xent 1.9409(1.9344) | Loss 5.3651(5.5074) | Error 0.6932(0.6863) Steps 538(542.80) | Grad Norm 10.5885(5.5226) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0051 | Time 20.5279, Epoch Time 332.2785(333.5916), Bit/dim 4.3542(best: 4.3821), Xent 1.8005, Loss 5.2545, Error 0.6278(best: 0.6081)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0307 | Time 51.3185(48.9555) | Bit/dim 4.3484(4.5344) | Xent 1.8340(1.9314) | Loss 5.2654(5.5001) | Error 0.6438(0.6851) Steps 532(542.47) | Grad Norm 2.4668(5.4310) | Total Time 14.00(14.00)\n",
      "Iter 0308 | Time 47.9903(48.9265) | Bit/dim 4.3486(4.5289) | Xent 1.8687(1.9295) | Loss 5.2830(5.4936) | Error 0.6660(0.6845) Steps 538(542.34) | Grad Norm 4.3015(5.3971) | Total Time 14.00(14.00)\n",
      "Iter 0309 | Time 51.8073(49.0130) | Bit/dim 4.3587(4.5238) | Xent 1.8861(1.9282) | Loss 5.3017(5.4879) | Error 0.6709(0.6841) Steps 538(542.21) | Grad Norm 3.1149(5.3286) | Total Time 14.00(14.00)\n",
      "Iter 0310 | Time 48.6156(49.0010) | Bit/dim 4.3408(4.5183) | Xent 1.8350(1.9254) | Loss 5.2582(5.4810) | Error 0.6532(0.6832) Steps 526(541.72) | Grad Norm 2.9690(5.2578) | Total Time 14.00(14.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_cifar.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 8000 --test_batch_size 5000 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_25_drop_0_5_run1 --seed 1 --conditional True --controlled_tol True --train_mode semisup --lr 0.01 --warmup_iters 1000 --atol 1e-4  --rtol 1e-4 --weight_y 0.5 --condition_ratio 0.25 --dropout_rate 0.5\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
