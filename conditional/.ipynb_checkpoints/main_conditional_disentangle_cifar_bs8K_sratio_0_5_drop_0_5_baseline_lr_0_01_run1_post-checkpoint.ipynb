{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_cifar.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "if args.controlled_tol:\n",
      "    import lib.odenvp_conditional_tol as odenvp\n",
      "else:\n",
      "    import lib.odenvp_conditional as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            loss.backward()\n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=8000, batch_size_schedule='', begin_epoch=1, condition_ratio=0.5, conditional=True, controlled_tol=False, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=1, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume='../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_baseline_lr_0_01_run1/epoch_94_checkpt.pth', rtol=1e-05, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_baseline_lr_0_01_run1', seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=5000, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000.0, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1414198\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc.py:286: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "Iter 0565 | Time 142.3334(84.4813) | Bit/dim 3.9070(3.9959) | Xent 1.4066(1.5393) | Loss 4.6103(4.7656) | Error 0.4982(0.5495) Steps 922(900.50) | Grad Norm 1.2517(4.3167) | Total Time 14.00(14.00)\n",
      "Iter 0566 | Time 83.6282(84.4557) | Bit/dim 3.8954(3.9929) | Xent 1.4301(1.5360) | Loss 4.6105(4.7609) | Error 0.5071(0.5482) Steps 934(901.50) | Grad Norm 1.3191(4.2268) | Total Time 14.00(14.00)\n",
      "Iter 0567 | Time 83.6954(84.4329) | Bit/dim 3.8965(3.9900) | Xent 1.4205(1.5326) | Loss 4.6067(4.7563) | Error 0.5068(0.5470) Steps 922(902.12) | Grad Norm 1.0703(4.1321) | Total Time 14.00(14.00)\n",
      "Iter 0568 | Time 84.2893(84.4286) | Bit/dim 3.8856(3.9869) | Xent 1.4507(1.5301) | Loss 4.6109(4.7519) | Error 0.5148(0.5460) Steps 916(902.54) | Grad Norm 0.8888(4.0348) | Total Time 14.00(14.00)\n",
      "Iter 0569 | Time 85.2019(84.4518) | Bit/dim 3.8875(3.9839) | Xent 1.4127(1.5266) | Loss 4.5939(4.7472) | Error 0.5126(0.5450) Steps 928(903.30) | Grad Norm 0.6502(3.9332) | Total Time 14.00(14.00)\n",
      "Iter 0570 | Time 87.7391(84.5504) | Bit/dim 3.8817(3.9808) | Xent 1.4127(1.5232) | Loss 4.5881(4.7424) | Error 0.5057(0.5438) Steps 928(904.04) | Grad Norm 0.6758(3.8355) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0095 | Time 45.4494, Epoch Time 636.7484(491.7225), Bit/dim 3.8865(best: inf), Xent 1.3818, Loss 4.5774, Error 0.4934(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0571 | Time 95.9280(84.8918) | Bit/dim 3.8914(3.9781) | Xent 1.4395(1.5207) | Loss 4.6112(4.7385) | Error 0.5171(0.5430) Steps 916(904.40) | Grad Norm 0.8154(3.7449) | Total Time 14.00(14.00)\n",
      "Iter 0572 | Time 87.5353(84.9711) | Bit/dim 3.8814(3.9752) | Xent 1.4216(1.5177) | Loss 4.5922(4.7341) | Error 0.5076(0.5420) Steps 928(905.11) | Grad Norm 0.9480(3.6610) | Total Time 14.00(14.00)\n",
      "Iter 0573 | Time 86.9507(85.0305) | Bit/dim 3.8831(3.9725) | Xent 1.4203(1.5148) | Loss 4.5932(4.7299) | Error 0.5090(0.5410) Steps 928(905.79) | Grad Norm 0.7875(3.5748) | Total Time 14.00(14.00)\n",
      "Iter 0574 | Time 84.0878(85.0022) | Bit/dim 3.8869(3.9699) | Xent 1.4302(1.5122) | Loss 4.6020(4.7260) | Error 0.5155(0.5402) Steps 928(906.46) | Grad Norm 0.7381(3.4897) | Total Time 14.00(14.00)\n",
      "Iter 0575 | Time 85.2677(85.0101) | Bit/dim 3.8803(3.9672) | Xent 1.4153(1.5093) | Loss 4.5879(4.7219) | Error 0.5106(0.5393) Steps 946(907.65) | Grad Norm 0.7185(3.4066) | Total Time 14.00(14.00)\n",
      "Iter 0576 | Time 82.2462(84.9272) | Bit/dim 3.8788(3.9646) | Xent 1.3942(1.5059) | Loss 4.5759(4.7175) | Error 0.4999(0.5381) Steps 940(908.62) | Grad Norm 0.6338(3.3234) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0096 | Time 31.1912, Epoch Time 569.2433(494.0481), Bit/dim 3.8818(best: 3.8865), Xent 1.3779, Loss 4.5707, Error 0.4916(best: 0.4934)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0577 | Time 84.0772(84.9017) | Bit/dim 3.8925(3.9624) | Xent 1.3932(1.5025) | Loss 4.5891(4.7137) | Error 0.4958(0.5369) Steps 940(909.56) | Grad Norm 0.6598(3.2435) | Total Time 14.00(14.00)\n",
      "Iter 0578 | Time 86.9461(84.9630) | Bit/dim 3.8845(3.9601) | Xent 1.4307(1.5003) | Loss 4.5998(4.7102) | Error 0.5179(0.5363) Steps 934(910.29) | Grad Norm 0.7497(3.1686) | Total Time 14.00(14.00)\n",
      "Iter 0579 | Time 84.7169(84.9557) | Bit/dim 3.8726(3.9574) | Xent 1.4320(1.4983) | Loss 4.5886(4.7066) | Error 0.5176(0.5357) Steps 940(911.18) | Grad Norm 0.5463(3.0900) | Total Time 14.00(14.00)\n",
      "Iter 0580 | Time 84.9482(84.9554) | Bit/dim 3.8725(3.9549) | Xent 1.4250(1.4961) | Loss 4.5850(4.7029) | Error 0.5122(0.5350) Steps 928(911.69) | Grad Norm 0.5210(3.0129) | Total Time 14.00(14.00)\n",
      "Iter 0581 | Time 83.5119(84.9121) | Bit/dim 3.8747(3.9525) | Xent 1.3999(1.4932) | Loss 4.5746(4.6991) | Error 0.5010(0.5340) Steps 940(912.54) | Grad Norm 0.4936(2.9373) | Total Time 14.00(14.00)\n",
      "Iter 0582 | Time 84.3733(84.8960) | Bit/dim 3.8876(3.9505) | Xent 1.3992(1.4904) | Loss 4.5872(4.6957) | Error 0.4962(0.5329) Steps 934(913.18) | Grad Norm 0.6117(2.8676) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0097 | Time 30.6511, Epoch Time 555.1190(495.8802), Bit/dim 3.8794(best: 3.8818), Xent 1.3773, Loss 4.5680, Error 0.4946(best: 0.4916)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0583 | Time 85.9242(84.9268) | Bit/dim 3.8732(3.9482) | Xent 1.3922(1.4874) | Loss 4.5692(4.6919) | Error 0.4981(0.5318) Steps 940(913.99) | Grad Norm 0.6675(2.8016) | Total Time 14.00(14.00)\n",
      "Iter 0584 | Time 85.1510(84.9335) | Bit/dim 3.8851(3.9463) | Xent 1.3877(1.4844) | Loss 4.5790(4.6885) | Error 0.4942(0.5307) Steps 934(914.59) | Grad Norm 0.5646(2.7345) | Total Time 14.00(14.00)\n",
      "Iter 0585 | Time 83.1450(84.8799) | Bit/dim 3.8865(3.9445) | Xent 1.4223(1.4826) | Loss 4.5976(4.6858) | Error 0.5124(0.5302) Steps 934(915.17) | Grad Norm 0.5887(2.6701) | Total Time 14.00(14.00)\n",
      "Iter 0586 | Time 82.3040(84.8026) | Bit/dim 3.8630(3.9421) | Xent 1.4082(1.4803) | Loss 4.5671(4.6823) | Error 0.5071(0.5295) Steps 940(915.91) | Grad Norm 0.5062(2.6052) | Total Time 14.00(14.00)\n",
      "Iter 0587 | Time 86.1414(84.8428) | Bit/dim 3.8808(3.9402) | Xent 1.4090(1.4782) | Loss 4.5853(4.6794) | Error 0.5055(0.5287) Steps 946(916.82) | Grad Norm 0.4576(2.5407) | Total Time 14.00(14.00)\n",
      "Iter 0588 | Time 86.1641(84.8824) | Bit/dim 3.8730(3.9382) | Xent 1.4320(1.4768) | Loss 4.5890(4.6766) | Error 0.5145(0.5283) Steps 928(917.15) | Grad Norm 0.4184(2.4771) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0098 | Time 30.8053, Epoch Time 555.4433(497.6671), Bit/dim 3.8772(best: 3.8794), Xent 1.3723, Loss 4.5633, Error 0.4916(best: 0.4916)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0589 | Time 85.1555(84.8906) | Bit/dim 3.8702(3.9362) | Xent 1.4212(1.4752) | Loss 4.5808(4.6738) | Error 0.5099(0.5278) Steps 934(917.66) | Grad Norm 0.6096(2.4210) | Total Time 14.00(14.00)\n",
      "Iter 0590 | Time 86.1913(84.9296) | Bit/dim 3.8700(3.9342) | Xent 1.4160(1.4734) | Loss 4.5780(4.6709) | Error 0.5064(0.5271) Steps 928(917.97) | Grad Norm 0.5648(2.3654) | Total Time 14.00(14.00)\n",
      "Iter 0591 | Time 82.7605(84.8646) | Bit/dim 3.8796(3.9326) | Xent 1.3968(1.4711) | Loss 4.5780(4.6681) | Error 0.5015(0.5264) Steps 934(918.45) | Grad Norm 0.4531(2.3080) | Total Time 14.00(14.00)\n",
      "Iter 0592 | Time 83.3894(84.8203) | Bit/dim 3.8787(3.9309) | Xent 1.4173(1.4695) | Loss 4.5873(4.6657) | Error 0.5095(0.5258) Steps 934(918.91) | Grad Norm 0.4091(2.2510) | Total Time 14.00(14.00)\n",
      "Iter 0593 | Time 84.0569(84.7974) | Bit/dim 3.8743(3.9292) | Xent 1.4082(1.4676) | Loss 4.5784(4.6631) | Error 0.5021(0.5251) Steps 946(919.73) | Grad Norm 0.3714(2.1946) | Total Time 14.00(14.00)\n",
      "Iter 0594 | Time 83.2066(84.7497) | Bit/dim 3.8690(3.9274) | Xent 1.4158(1.4661) | Loss 4.5769(4.6605) | Error 0.5029(0.5245) Steps 934(920.16) | Grad Norm 0.4016(2.1408) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0099 | Time 30.6666, Epoch Time 551.5677(499.2841), Bit/dim 3.8739(best: 3.8772), Xent 1.3716, Loss 4.5597, Error 0.4912(best: 0.4916)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0595 | Time 85.8576(84.7829) | Bit/dim 3.8716(3.9258) | Xent 1.4025(1.4642) | Loss 4.5729(4.6578) | Error 0.5014(0.5238) Steps 934(920.57) | Grad Norm 0.4641(2.0905) | Total Time 14.00(14.00)\n",
      "Iter 0596 | Time 84.6978(84.7804) | Bit/dim 3.8758(3.9243) | Xent 1.3962(1.4621) | Loss 4.5739(4.6553) | Error 0.4976(0.5230) Steps 934(920.97) | Grad Norm 0.4654(2.0418) | Total Time 14.00(14.00)\n",
      "Iter 0597 | Time 86.2635(84.8249) | Bit/dim 3.8648(3.9225) | Xent 1.3966(1.4602) | Loss 4.5631(4.6526) | Error 0.4958(0.5222) Steps 934(921.36) | Grad Norm 0.3734(1.9917) | Total Time 14.00(14.00)\n",
      "Iter 0598 | Time 84.7024(84.8212) | Bit/dim 3.8722(3.9210) | Xent 1.4274(1.4592) | Loss 4.5859(4.6506) | Error 0.5129(0.5219) Steps 916(921.20) | Grad Norm 0.4539(1.9456) | Total Time 14.00(14.00)\n",
      "Iter 0599 | Time 85.8047(84.8507) | Bit/dim 3.8835(3.9198) | Xent 1.4100(1.4577) | Loss 4.5884(4.6487) | Error 0.5025(0.5213) Steps 934(921.59) | Grad Norm 0.3724(1.8984) | Total Time 14.00(14.00)\n",
      "Iter 0600 | Time 84.7988(84.8491) | Bit/dim 3.8659(3.9182) | Xent 1.4103(1.4563) | Loss 4.5710(4.6464) | Error 0.5026(0.5208) Steps 928(921.78) | Grad Norm 0.3840(1.8530) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0100 | Time 30.5933, Epoch Time 558.4602(501.0594), Bit/dim 3.8720(best: 3.8739), Xent 1.3689, Loss 4.5565, Error 0.4890(best: 0.4912)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0601 | Time 85.0336(84.8547) | Bit/dim 3.8708(3.9168) | Xent 1.4158(1.4551) | Loss 4.5787(4.6443) | Error 0.5124(0.5205) Steps 934(922.15) | Grad Norm 0.3903(1.8091) | Total Time 14.00(14.00)\n",
      "Iter 0602 | Time 88.0051(84.9492) | Bit/dim 3.8682(3.9153) | Xent 1.4088(1.4537) | Loss 4.5726(4.6422) | Error 0.5090(0.5202) Steps 940(922.68) | Grad Norm 0.3656(1.7658) | Total Time 14.00(14.00)\n",
      "Iter 0603 | Time 86.9684(85.0098) | Bit/dim 3.8798(3.9143) | Xent 1.4047(1.4522) | Loss 4.5821(4.6404) | Error 0.5009(0.5196) Steps 940(923.20) | Grad Norm 0.3592(1.7236) | Total Time 14.00(14.00)\n",
      "Iter 0604 | Time 84.4618(84.9933) | Bit/dim 3.8512(3.9124) | Xent 1.4093(1.4509) | Loss 4.5559(4.6378) | Error 0.5001(0.5190) Steps 934(923.53) | Grad Norm 0.4552(1.6855) | Total Time 14.00(14.00)\n",
      "Iter 0605 | Time 85.2329(85.0005) | Bit/dim 3.8744(3.9112) | Xent 1.4080(1.4496) | Loss 4.5784(4.6361) | Error 0.5014(0.5185) Steps 934(923.84) | Grad Norm 0.4447(1.6483) | Total Time 14.00(14.00)\n",
      "Iter 0606 | Time 82.9768(84.9398) | Bit/dim 3.8750(3.9102) | Xent 1.3951(1.4480) | Loss 4.5726(4.6342) | Error 0.4999(0.5179) Steps 934(924.14) | Grad Norm 0.2995(1.6078) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0101 | Time 31.0438, Epoch Time 559.7003(502.8186), Bit/dim 3.8692(best: 3.8720), Xent 1.3664, Loss 4.5524, Error 0.4907(best: 0.4890)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0607 | Time 83.7555(84.9043) | Bit/dim 3.8642(3.9088) | Xent 1.4185(1.4471) | Loss 4.5735(4.6323) | Error 0.5169(0.5179) Steps 940(924.62) | Grad Norm 0.3158(1.5691) | Total Time 14.00(14.00)\n",
      "Iter 0608 | Time 81.9614(84.8160) | Bit/dim 3.8651(3.9075) | Xent 1.4143(1.4461) | Loss 4.5722(4.6305) | Error 0.5071(0.5176) Steps 928(924.72) | Grad Norm 0.3119(1.5314) | Total Time 14.00(14.00)\n",
      "Iter 0609 | Time 86.9326(84.8795) | Bit/dim 3.8736(3.9065) | Xent 1.3994(1.4447) | Loss 4.5733(4.6288) | Error 0.4970(0.5169) Steps 922(924.64) | Grad Norm 0.5743(1.5027) | Total Time 14.00(14.00)\n",
      "Iter 0610 | Time 80.7507(84.7556) | Bit/dim 3.8716(3.9054) | Xent 1.4036(1.4435) | Loss 4.5734(4.6272) | Error 0.5051(0.5166) Steps 928(924.74) | Grad Norm 0.4968(1.4725) | Total Time 14.00(14.00)\n",
      "Iter 0611 | Time 86.2343(84.8000) | Bit/dim 3.8656(3.9042) | Xent 1.4066(1.4424) | Loss 4.5689(4.6254) | Error 0.5031(0.5162) Steps 934(925.02) | Grad Norm 0.3398(1.4385) | Total Time 14.00(14.00)\n",
      "Iter 0612 | Time 82.2483(84.7234) | Bit/dim 3.8719(3.9032) | Xent 1.4051(1.4413) | Loss 4.5745(4.6239) | Error 0.5037(0.5158) Steps 934(925.29) | Grad Norm 0.4628(1.4092) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0102 | Time 30.6131, Epoch Time 548.6326(504.1930), Bit/dim 3.8678(best: 3.8692), Xent 1.3642, Loss 4.5499, Error 0.4880(best: 0.4890)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0613 | Time 81.9050(84.6389) | Bit/dim 3.8595(3.9019) | Xent 1.4043(1.4402) | Loss 4.5617(4.6220) | Error 0.4940(0.5152) Steps 916(925.01) | Grad Norm 0.5636(1.3839) | Total Time 14.00(14.00)\n",
      "Iter 0614 | Time 84.7660(84.6427) | Bit/dim 3.8702(3.9010) | Xent 1.4050(1.4391) | Loss 4.5727(4.6205) | Error 0.5028(0.5148) Steps 922(924.92) | Grad Norm 0.6422(1.3616) | Total Time 14.00(14.00)\n",
      "Iter 0615 | Time 84.9665(84.6524) | Bit/dim 3.8647(3.8999) | Xent 1.3987(1.4379) | Loss 4.5640(4.6188) | Error 0.5042(0.5145) Steps 934(925.19) | Grad Norm 0.3520(1.3313) | Total Time 14.00(14.00)\n",
      "Iter 0616 | Time 82.8198(84.5974) | Bit/dim 3.8695(3.8990) | Xent 1.3861(1.4363) | Loss 4.5626(4.6172) | Error 0.4986(0.5140) Steps 922(925.10) | Grad Norm 0.5278(1.3072) | Total Time 14.00(14.00)\n",
      "Iter 0617 | Time 85.2360(84.6166) | Bit/dim 3.8741(3.8982) | Xent 1.4190(1.4358) | Loss 4.5836(4.6161) | Error 0.5117(0.5139) Steps 922(925.00) | Grad Norm 0.3892(1.2797) | Total Time 14.00(14.00)\n",
      "Iter 0618 | Time 83.5740(84.5853) | Bit/dim 3.8679(3.8973) | Xent 1.4092(1.4350) | Loss 4.5725(4.6148) | Error 0.5044(0.5136) Steps 922(924.91) | Grad Norm 0.3456(1.2517) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0103 | Time 30.5704, Epoch Time 549.5335(505.5533), Bit/dim 3.8663(best: 3.8678), Xent 1.3630, Loss 4.5478, Error 0.4905(best: 0.4880)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0619 | Time 87.1662(84.6627) | Bit/dim 3.8696(3.8965) | Xent 1.4167(1.4345) | Loss 4.5779(4.6137) | Error 0.5091(0.5135) Steps 934(925.19) | Grad Norm 0.2752(1.2224) | Total Time 14.00(14.00)\n",
      "Iter 0620 | Time 85.2343(84.6799) | Bit/dim 3.8752(3.8959) | Xent 1.4125(1.4338) | Loss 4.5815(4.6128) | Error 0.5064(0.5133) Steps 916(924.91) | Grad Norm 0.6205(1.2043) | Total Time 14.00(14.00)\n",
      "Iter 0621 | Time 82.0472(84.6009) | Bit/dim 3.8589(3.8947) | Xent 1.3768(1.4321) | Loss 4.5474(4.6108) | Error 0.4899(0.5126) Steps 928(925.00) | Grad Norm 0.4583(1.1819) | Total Time 14.00(14.00)\n",
      "Iter 0622 | Time 80.8186(84.4874) | Bit/dim 3.8672(3.8939) | Xent 1.4013(1.4312) | Loss 4.5679(4.6095) | Error 0.5030(0.5123) Steps 922(924.91) | Grad Norm 0.3211(1.1561) | Total Time 14.00(14.00)\n",
      "Iter 0623 | Time 82.6201(84.4314) | Bit/dim 3.8579(3.8928) | Xent 1.3942(1.4301) | Loss 4.5551(4.6079) | Error 0.4964(0.5118) Steps 934(925.19) | Grad Norm 0.5002(1.1364) | Total Time 14.00(14.00)\n",
      "Iter 0624 | Time 86.1753(84.4837) | Bit/dim 3.8641(3.8920) | Xent 1.3987(1.4291) | Loss 4.5635(4.6065) | Error 0.4970(0.5114) Steps 922(925.09) | Grad Norm 0.4486(1.1158) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0104 | Time 30.6632, Epoch Time 550.5082(506.9019), Bit/dim 3.8647(best: 3.8663), Xent 1.3609, Loss 4.5451, Error 0.4859(best: 0.4880)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0625 | Time 82.3156(84.4187) | Bit/dim 3.8518(3.8908) | Xent 1.4125(1.4286) | Loss 4.5580(4.6051) | Error 0.5044(0.5112) Steps 922(925.00) | Grad Norm 0.3818(1.0938) | Total Time 14.00(14.00)\n",
      "Iter 0626 | Time 83.4513(84.3897) | Bit/dim 3.8707(3.8902) | Xent 1.3980(1.4277) | Loss 4.5697(4.6040) | Error 0.4988(0.5108) Steps 916(924.73) | Grad Norm 0.5738(1.0782) | Total Time 14.00(14.00)\n",
      "Iter 0627 | Time 83.4831(84.3625) | Bit/dim 3.8767(3.8898) | Xent 1.3871(1.4265) | Loss 4.5703(4.6030) | Error 0.5006(0.5105) Steps 922(924.65) | Grad Norm 0.5663(1.0628) | Total Time 14.00(14.00)\n",
      "Iter 0628 | Time 83.7534(84.3442) | Bit/dim 3.8600(3.8889) | Xent 1.3838(1.4252) | Loss 4.5519(4.6015) | Error 0.4961(0.5101) Steps 910(924.21) | Grad Norm 0.3099(1.0402) | Total Time 14.00(14.00)\n",
      "Iter 0629 | Time 81.2401(84.2511) | Bit/dim 3.8682(3.8883) | Xent 1.4061(1.4246) | Loss 4.5712(4.6006) | Error 0.5060(0.5099) Steps 916(923.96) | Grad Norm 0.4991(1.0240) | Total Time 14.00(14.00)\n",
      "Iter 0630 | Time 80.0265(84.1243) | Bit/dim 3.8530(3.8872) | Xent 1.3982(1.4238) | Loss 4.5521(4.5991) | Error 0.5016(0.5097) Steps 922(923.90) | Grad Norm 0.4793(1.0077) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0105 | Time 30.7440, Epoch Time 540.8100(507.9192), Bit/dim 3.8643(best: 3.8647), Xent 1.3583, Loss 4.5434, Error 0.4884(best: 0.4859)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0631 | Time 84.4861(84.1352) | Bit/dim 3.8481(3.8860) | Xent 1.3936(1.4229) | Loss 4.5449(4.5975) | Error 0.5028(0.5095) Steps 928(924.02) | Grad Norm 0.5578(0.9942) | Total Time 14.00(14.00)\n",
      "Iter 0632 | Time 82.8554(84.0968) | Bit/dim 3.8626(3.8853) | Xent 1.3919(1.4220) | Loss 4.5585(4.5963) | Error 0.4999(0.5092) Steps 916(923.78) | Grad Norm 0.3986(0.9763) | Total Time 14.00(14.00)\n",
      "Iter 0633 | Time 86.9448(84.1822) | Bit/dim 3.8548(3.8844) | Xent 1.3991(1.4213) | Loss 4.5543(4.5951) | Error 0.5031(0.5090) Steps 916(923.55) | Grad Norm 0.2997(0.9560) | Total Time 14.00(14.00)\n",
      "Iter 0634 | Time 85.3449(84.2171) | Bit/dim 3.8664(3.8839) | Xent 1.4035(1.4208) | Loss 4.5682(4.5943) | Error 0.4959(0.5086) Steps 928(923.68) | Grad Norm 0.4806(0.9417) | Total Time 14.00(14.00)\n",
      "Iter 0635 | Time 82.6030(84.1687) | Bit/dim 3.8707(3.8835) | Xent 1.3877(1.4198) | Loss 4.5645(4.5934) | Error 0.4981(0.5083) Steps 922(923.63) | Grad Norm 0.3859(0.9251) | Total Time 14.00(14.00)\n",
      "Iter 0636 | Time 82.5746(84.1209) | Bit/dim 3.8701(3.8831) | Xent 1.4054(1.4194) | Loss 4.5729(4.5927) | Error 0.5059(0.5082) Steps 922(923.58) | Grad Norm 0.5200(0.9129) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0106 | Time 30.6305, Epoch Time 551.5476(509.2280), Bit/dim 3.8618(best: 3.8643), Xent 1.3577, Loss 4.5406, Error 0.4873(best: 0.4859)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0637 | Time 84.5025(84.1323) | Bit/dim 3.8637(3.8825) | Xent 1.3882(1.4184) | Loss 4.5578(4.5917) | Error 0.4971(0.5079) Steps 928(923.72) | Grad Norm 0.3172(0.8950) | Total Time 14.00(14.00)\n",
      "Iter 0638 | Time 82.3214(84.0780) | Bit/dim 3.8587(3.8818) | Xent 1.3797(1.4173) | Loss 4.5485(4.5904) | Error 0.4860(0.5072) Steps 904(923.12) | Grad Norm 0.4254(0.8809) | Total Time 14.00(14.00)\n",
      "Iter 0639 | Time 82.3554(84.0263) | Bit/dim 3.8662(3.8813) | Xent 1.3895(1.4164) | Loss 4.5609(4.5895) | Error 0.5008(0.5070) Steps 910(922.73) | Grad Norm 0.4389(0.8677) | Total Time 14.00(14.00)\n",
      "Iter 0640 | Time 85.7957(84.0794) | Bit/dim 3.8626(3.8807) | Xent 1.4000(1.4159) | Loss 4.5626(4.5887) | Error 0.5015(0.5069) Steps 910(922.35) | Grad Norm 0.4122(0.8540) | Total Time 14.00(14.00)\n",
      "Iter 0641 | Time 85.2154(84.1135) | Bit/dim 3.8544(3.8800) | Xent 1.4115(1.4158) | Loss 4.5602(4.5879) | Error 0.5092(0.5069) Steps 916(922.16) | Grad Norm 0.3279(0.8382) | Total Time 14.00(14.00)\n",
      "Iter 0642 | Time 85.9400(84.1683) | Bit/dim 3.8631(3.8794) | Xent 1.3981(1.4153) | Loss 4.5621(4.5871) | Error 0.5008(0.5068) Steps 928(922.33) | Grad Norm 0.4828(0.8276) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0107 | Time 30.4518, Epoch Time 552.6039(510.5293), Bit/dim 3.8608(best: 3.8618), Xent 1.3557, Loss 4.5387, Error 0.4864(best: 0.4859)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0643 | Time 81.7963(84.0971) | Bit/dim 3.8659(3.8790) | Xent 1.4053(1.4150) | Loss 4.5686(4.5865) | Error 0.5000(0.5066) Steps 910(921.96) | Grad Norm 0.3340(0.8128) | Total Time 14.00(14.00)\n",
      "Iter 0644 | Time 85.2006(84.1302) | Bit/dim 3.8507(3.8782) | Xent 1.3685(1.4136) | Loss 4.5350(4.5850) | Error 0.4895(0.5060) Steps 910(921.60) | Grad Norm 0.3640(0.7993) | Total Time 14.00(14.00)\n",
      "Iter 0645 | Time 81.8555(84.0620) | Bit/dim 3.8636(3.8778) | Xent 1.3971(1.4131) | Loss 4.5622(4.5843) | Error 0.4962(0.5058) Steps 904(921.08) | Grad Norm 0.4916(0.7901) | Total Time 14.00(14.00)\n",
      "Iter 0646 | Time 85.1050(84.0933) | Bit/dim 3.8481(3.8769) | Xent 1.3855(1.4123) | Loss 4.5409(4.5830) | Error 0.4884(0.5052) Steps 910(920.74) | Grad Norm 0.5280(0.7822) | Total Time 14.00(14.00)\n",
      "Iter 0647 | Time 85.6341(84.1395) | Bit/dim 3.8634(3.8765) | Xent 1.3865(1.4115) | Loss 4.5566(4.5822) | Error 0.5011(0.5051) Steps 916(920.60) | Grad Norm 0.7350(0.7808) | Total Time 14.00(14.00)\n",
      "Iter 0648 | Time 82.4916(84.0901) | Bit/dim 3.8591(3.8759) | Xent 1.4095(1.4114) | Loss 4.5639(4.5817) | Error 0.5086(0.5052) Steps 916(920.46) | Grad Norm 0.6434(0.7767) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0108 | Time 30.4815, Epoch Time 548.6592(511.6732), Bit/dim 3.8598(best: 3.8608), Xent 1.3543, Loss 4.5369, Error 0.4874(best: 0.4859)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0649 | Time 80.4624(83.9812) | Bit/dim 3.8604(3.8755) | Xent 1.3897(1.4108) | Loss 4.5553(4.5809) | Error 0.5008(0.5051) Steps 910(920.15) | Grad Norm 1.1086(0.7866) | Total Time 14.00(14.00)\n",
      "Iter 0650 | Time 83.7992(83.9758) | Bit/dim 3.8660(3.8752) | Xent 1.3921(1.4102) | Loss 4.5620(4.5803) | Error 0.5015(0.5050) Steps 916(920.03) | Grad Norm 0.5426(0.7793) | Total Time 14.00(14.00)\n",
      "Iter 0651 | Time 83.4372(83.9596) | Bit/dim 3.8573(3.8747) | Xent 1.4060(1.4101) | Loss 4.5604(4.5797) | Error 0.5042(0.5050) Steps 898(919.36) | Grad Norm 0.3431(0.7662) | Total Time 14.00(14.00)\n",
      "Iter 0652 | Time 81.6929(83.8916) | Bit/dim 3.8558(3.8741) | Xent 1.4038(1.4099) | Loss 4.5578(4.5790) | Error 0.4969(0.5047) Steps 910(919.08) | Grad Norm 0.9302(0.7711) | Total Time 14.00(14.00)\n",
      "Iter 0653 | Time 84.9373(83.9230) | Bit/dim 3.8620(3.8737) | Xent 1.3894(1.4093) | Loss 4.5566(4.5784) | Error 0.4942(0.5044) Steps 928(919.35) | Grad Norm 0.8413(0.7732) | Total Time 14.00(14.00)\n",
      "Iter 0654 | Time 82.5873(83.8829) | Bit/dim 3.8500(3.8730) | Xent 1.3864(1.4086) | Loss 4.5432(4.5773) | Error 0.4998(0.5043) Steps 910(919.07) | Grad Norm 0.4008(0.7621) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0109 | Time 30.2406, Epoch Time 543.0214(512.6136), Bit/dim 3.8576(best: 3.8598), Xent 1.3530, Loss 4.5340, Error 0.4838(best: 0.4859)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0655 | Time 81.9386(83.8246) | Bit/dim 3.8535(3.8724) | Xent 1.3924(1.4081) | Loss 4.5497(4.5765) | Error 0.4966(0.5040) Steps 916(918.98) | Grad Norm 1.0334(0.7702) | Total Time 14.00(14.00)\n",
      "Iter 0656 | Time 84.4405(83.8431) | Bit/dim 3.8545(3.8719) | Xent 1.3960(1.4077) | Loss 4.5525(4.5758) | Error 0.5062(0.5041) Steps 922(919.07) | Grad Norm 0.8984(0.7741) | Total Time 14.00(14.00)\n",
      "Iter 0657 | Time 86.2223(83.9144) | Bit/dim 3.8551(3.8714) | Xent 1.3838(1.4070) | Loss 4.5470(4.5749) | Error 0.4945(0.5038) Steps 910(918.80) | Grad Norm 0.6586(0.7706) | Total Time 14.00(14.00)\n",
      "Iter 0658 | Time 85.5670(83.9640) | Bit/dim 3.8718(3.8714) | Xent 1.4002(1.4068) | Loss 4.5719(4.5748) | Error 0.5044(0.5038) Steps 910(918.53) | Grad Norm 0.9456(0.7758) | Total Time 14.00(14.00)\n",
      "Iter 0659 | Time 86.7797(84.0485) | Bit/dim 3.8458(3.8706) | Xent 1.4028(1.4067) | Loss 4.5473(4.5740) | Error 0.5064(0.5039) Steps 916(918.46) | Grad Norm 1.2575(0.7903) | Total Time 14.00(14.00)\n",
      "Iter 0660 | Time 84.3142(84.0565) | Bit/dim 3.8505(3.8700) | Xent 1.3668(1.4055) | Loss 4.5339(4.5728) | Error 0.4896(0.5035) Steps 916(918.38) | Grad Norm 0.4205(0.7792) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0110 | Time 30.3394, Epoch Time 555.7674(513.9082), Bit/dim 3.8566(best: 3.8576), Xent 1.3520, Loss 4.5326, Error 0.4839(best: 0.4838)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0661 | Time 83.7002(84.0458) | Bit/dim 3.8556(3.8696) | Xent 1.3964(1.4052) | Loss 4.5538(4.5722) | Error 0.5051(0.5035) Steps 904(917.95) | Grad Norm 0.8356(0.7809) | Total Time 14.00(14.00)\n",
      "Iter 0662 | Time 83.6948(84.0352) | Bit/dim 3.8480(3.8689) | Xent 1.4011(1.4051) | Loss 4.5486(4.5715) | Error 0.5072(0.5036) Steps 910(917.71) | Grad Norm 0.9635(0.7864) | Total Time 14.00(14.00)\n",
      "Iter 0663 | Time 85.5393(84.0804) | Bit/dim 3.8587(3.8686) | Xent 1.3696(1.4040) | Loss 4.5435(4.5707) | Error 0.4969(0.5034) Steps 910(917.48) | Grad Norm 0.8047(0.7869) | Total Time 14.00(14.00)\n",
      "Iter 0664 | Time 86.7354(84.1600) | Bit/dim 3.8513(3.8681) | Xent 1.3687(1.4030) | Loss 4.5356(4.5696) | Error 0.4888(0.5030) Steps 916(917.44) | Grad Norm 0.7397(0.7855) | Total Time 14.00(14.00)\n",
      "Iter 0665 | Time 85.5170(84.2007) | Bit/dim 3.8650(3.8680) | Xent 1.3934(1.4027) | Loss 4.5617(4.5694) | Error 0.4966(0.5028) Steps 898(916.85) | Grad Norm 0.9591(0.7907) | Total Time 14.00(14.00)\n",
      "Iter 0666 | Time 83.3150(84.1741) | Bit/dim 3.8531(3.8676) | Xent 1.3665(1.4016) | Loss 4.5363(4.5684) | Error 0.4905(0.5024) Steps 910(916.65) | Grad Norm 0.7324(0.7890) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0111 | Time 30.5181, Epoch Time 554.8975(515.1379), Bit/dim 3.8545(best: 3.8566), Xent 1.3488, Loss 4.5289, Error 0.4867(best: 0.4838)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0667 | Time 83.5642(84.1558) | Bit/dim 3.8522(3.8671) | Xent 1.3860(1.4011) | Loss 4.5451(4.5677) | Error 0.4984(0.5023) Steps 910(916.45) | Grad Norm 0.3463(0.7757) | Total Time 14.00(14.00)\n",
      "Iter 0668 | Time 84.6317(84.1701) | Bit/dim 3.8479(3.8665) | Xent 1.3932(1.4009) | Loss 4.5445(4.5670) | Error 0.5035(0.5023) Steps 910(916.26) | Grad Norm 0.7838(0.7759) | Total Time 14.00(14.00)\n",
      "Iter 0669 | Time 86.4558(84.2387) | Bit/dim 3.8562(3.8662) | Xent 1.3786(1.4002) | Loss 4.5455(4.5663) | Error 0.4924(0.5020) Steps 910(916.07) | Grad Norm 0.9311(0.7806) | Total Time 14.00(14.00)\n",
      "Iter 0670 | Time 83.4279(84.2144) | Bit/dim 3.8486(3.8657) | Xent 1.3650(1.3992) | Loss 4.5312(4.5653) | Error 0.4919(0.5017) Steps 898(915.53) | Grad Norm 0.3791(0.7685) | Total Time 14.00(14.00)\n",
      "Iter 0671 | Time 85.6904(84.2587) | Bit/dim 3.8575(3.8655) | Xent 1.3936(1.3990) | Loss 4.5543(4.5650) | Error 0.5064(0.5019) Steps 886(914.64) | Grad Norm 0.4544(0.7591) | Total Time 14.00(14.00)\n",
      "Iter 0672 | Time 83.0620(84.2228) | Bit/dim 3.8622(3.8654) | Xent 1.3906(1.3988) | Loss 4.5575(4.5647) | Error 0.4990(0.5018) Steps 898(914.14) | Grad Norm 0.9479(0.7648) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0112 | Time 30.7813, Epoch Time 553.5467(516.2902), Bit/dim 3.8530(best: 3.8545), Xent 1.3494, Loss 4.5277, Error 0.4850(best: 0.4838)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0673 | Time 82.2909(84.1648) | Bit/dim 3.8643(3.8653) | Xent 1.4089(1.3991) | Loss 4.5687(4.5649) | Error 0.5030(0.5018) Steps 892(913.48) | Grad Norm 1.1706(0.7770) | Total Time 14.00(14.00)\n",
      "Iter 0674 | Time 81.7851(84.0934) | Bit/dim 3.8377(3.8645) | Xent 1.3742(1.3983) | Loss 4.5248(4.5637) | Error 0.4949(0.5016) Steps 892(912.83) | Grad Norm 0.4548(0.7673) | Total Time 14.00(14.00)\n",
      "Iter 0675 | Time 82.9748(84.0598) | Bit/dim 3.8419(3.8638) | Xent 1.4026(1.3984) | Loss 4.5432(4.5630) | Error 0.5019(0.5016) Steps 904(912.57) | Grad Norm 1.3241(0.7840) | Total Time 14.00(14.00)\n",
      "Iter 0676 | Time 83.9374(84.0562) | Bit/dim 3.8487(3.8634) | Xent 1.3756(1.3978) | Loss 4.5365(4.5622) | Error 0.4951(0.5014) Steps 904(912.31) | Grad Norm 0.8779(0.7868) | Total Time 14.00(14.00)\n",
      "Iter 0677 | Time 83.8702(84.0506) | Bit/dim 3.8661(3.8634) | Xent 1.3901(1.3975) | Loss 4.5612(4.5622) | Error 0.4958(0.5013) Steps 904(912.06) | Grad Norm 0.5795(0.7806) | Total Time 14.00(14.00)\n",
      "Iter 0678 | Time 84.0329(84.0501) | Bit/dim 3.8528(3.8631) | Xent 1.3766(1.3969) | Loss 4.5411(4.5616) | Error 0.4889(0.5009) Steps 904(911.82) | Grad Norm 0.6045(0.7753) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0113 | Time 30.5699, Epoch Time 545.6558(517.1712), Bit/dim 3.8527(best: 3.8530), Xent 1.3485, Loss 4.5269, Error 0.4839(best: 0.4838)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0679 | Time 81.2030(83.9647) | Bit/dim 3.8582(3.8630) | Xent 1.3981(1.3969) | Loss 4.5573(4.5614) | Error 0.5054(0.5010) Steps 892(911.22) | Grad Norm 0.5719(0.7692) | Total Time 14.00(14.00)\n",
      "Iter 0680 | Time 84.8389(83.9909) | Bit/dim 3.8511(3.8626) | Xent 1.3605(1.3958) | Loss 4.5313(4.5605) | Error 0.4859(0.5006) Steps 898(910.83) | Grad Norm 0.4286(0.7590) | Total Time 14.00(14.00)\n",
      "Iter 0681 | Time 85.6225(84.0398) | Bit/dim 3.8482(3.8622) | Xent 1.3803(1.3954) | Loss 4.5383(4.5599) | Error 0.4958(0.5004) Steps 892(910.26) | Grad Norm 0.5394(0.7524) | Total Time 14.00(14.00)\n",
      "Iter 0682 | Time 80.0088(83.9189) | Bit/dim 3.8461(3.8617) | Xent 1.3694(1.3946) | Loss 4.5308(4.5590) | Error 0.4960(0.5003) Steps 898(909.90) | Grad Norm 0.6767(0.7501) | Total Time 14.00(14.00)\n",
      "Iter 0683 | Time 82.3268(83.8711) | Bit/dim 3.8532(3.8615) | Xent 1.3834(1.3943) | Loss 4.5449(4.5586) | Error 0.4964(0.5002) Steps 892(909.36) | Grad Norm 0.3620(0.7385) | Total Time 14.00(14.00)\n",
      "Iter 0684 | Time 84.2139(83.8814) | Bit/dim 3.8497(3.8611) | Xent 1.3862(1.3940) | Loss 4.5428(4.5581) | Error 0.4945(0.5000) Steps 886(908.66) | Grad Norm 0.3724(0.7275) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0114 | Time 30.4676, Epoch Time 544.8412(518.0013), Bit/dim 3.8508(best: 3.8527), Xent 1.3461, Loss 4.5239, Error 0.4856(best: 0.4838)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0685 | Time 85.0950(83.9178) | Bit/dim 3.8583(3.8610) | Xent 1.3779(1.3935) | Loss 4.5473(4.5578) | Error 0.4974(0.4999) Steps 886(907.98) | Grad Norm 0.6635(0.7256) | Total Time 14.00(14.00)\n",
      "Iter 0686 | Time 79.9998(83.8003) | Bit/dim 3.8437(3.8605) | Xent 1.3837(1.3932) | Loss 4.5355(4.5571) | Error 0.4995(0.4999) Steps 886(907.32) | Grad Norm 0.6582(0.7236) | Total Time 14.00(14.00)\n",
      "Iter 0687 | Time 83.7591(83.7990) | Bit/dim 3.8492(3.8602) | Xent 1.3742(1.3927) | Loss 4.5363(4.5565) | Error 0.4918(0.4997) Steps 892(906.86) | Grad Norm 0.4697(0.7159) | Total Time 14.00(14.00)\n",
      "Iter 0688 | Time 84.9075(83.8323) | Bit/dim 3.8495(3.8598) | Xent 1.3789(1.3923) | Loss 4.5389(4.5560) | Error 0.4946(0.4995) Steps 880(906.05) | Grad Norm 0.6262(0.7133) | Total Time 14.00(14.00)\n",
      "Iter 0689 | Time 82.5919(83.7951) | Bit/dim 3.8520(3.8596) | Xent 1.3875(1.3921) | Loss 4.5457(4.5557) | Error 0.4904(0.4992) Steps 880(905.27) | Grad Norm 0.9760(0.7211) | Total Time 14.00(14.00)\n",
      "Iter 0690 | Time 80.6568(83.7009) | Bit/dim 3.8437(3.8591) | Xent 1.3677(1.3914) | Loss 4.5276(4.5548) | Error 0.4818(0.4987) Steps 892(904.87) | Grad Norm 0.4578(0.7132) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0115 | Time 29.5701, Epoch Time 542.4042(518.7333), Bit/dim 3.8500(best: 3.8508), Xent 1.3434, Loss 4.5216, Error 0.4822(best: 0.4838)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0691 | Time 82.6569(83.6696) | Bit/dim 3.8566(3.8591) | Xent 1.3747(1.3909) | Loss 4.5440(4.5545) | Error 0.4938(0.4986) Steps 886(904.31) | Grad Norm 1.1072(0.7251) | Total Time 14.00(14.00)\n",
      "Iter 0692 | Time 86.2494(83.7470) | Bit/dim 3.8524(3.8589) | Xent 1.3890(1.3908) | Loss 4.5469(4.5543) | Error 0.4945(0.4984) Steps 892(903.94) | Grad Norm 0.7131(0.7247) | Total Time 14.00(14.00)\n",
      "Iter 0693 | Time 86.5528(83.8312) | Bit/dim 3.8415(3.8583) | Xent 1.3667(1.3901) | Loss 4.5249(4.5534) | Error 0.4932(0.4983) Steps 910(904.12) | Grad Norm 0.6172(0.7215) | Total Time 14.00(14.00)\n",
      "Iter 0694 | Time 87.9506(83.9548) | Bit/dim 3.8543(3.8582) | Xent 1.3855(1.3900) | Loss 4.5470(4.5532) | Error 0.4924(0.4981) Steps 892(903.76) | Grad Norm 0.9623(0.7287) | Total Time 14.00(14.00)\n",
      "Iter 0695 | Time 82.7594(83.9189) | Bit/dim 3.8477(3.8579) | Xent 1.3828(1.3898) | Loss 4.5391(4.5528) | Error 0.4919(0.4979) Steps 886(903.22) | Grad Norm 0.4378(0.7200) | Total Time 14.00(14.00)\n",
      "Iter 0696 | Time 80.8827(83.8278) | Bit/dim 3.8449(3.8575) | Xent 1.3789(1.3894) | Loss 4.5344(4.5522) | Error 0.4990(0.4980) Steps 880(902.53) | Grad Norm 0.5976(0.7163) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0116 | Time 29.9391, Epoch Time 552.8234(519.7560), Bit/dim 3.8488(best: 3.8500), Xent 1.3414, Loss 4.5195, Error 0.4838(best: 0.4822)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0697 | Time 81.3538(83.7536) | Bit/dim 3.8530(3.8574) | Xent 1.3824(1.3892) | Loss 4.5442(4.5520) | Error 0.5002(0.4980) Steps 886(902.03) | Grad Norm 0.7766(0.7181) | Total Time 14.00(14.00)\n",
      "Iter 0698 | Time 79.9658(83.6400) | Bit/dim 3.8500(3.8571) | Xent 1.3687(1.3886) | Loss 4.5343(4.5514) | Error 0.4931(0.4979) Steps 886(901.55) | Grad Norm 0.6584(0.7163) | Total Time 14.00(14.00)\n",
      "Iter 0699 | Time 84.9053(83.6779) | Bit/dim 3.8513(3.8570) | Xent 1.3812(1.3884) | Loss 4.5419(4.5512) | Error 0.4979(0.4979) Steps 904(901.62) | Grad Norm 0.8534(0.7204) | Total Time 14.00(14.00)\n",
      "Iter 0700 | Time 81.6260(83.6164) | Bit/dim 3.8504(3.8568) | Xent 1.3784(1.3881) | Loss 4.5396(4.5508) | Error 0.4961(0.4978) Steps 892(901.34) | Grad Norm 0.5522(0.7154) | Total Time 14.00(14.00)\n",
      "Iter 0701 | Time 82.4688(83.5819) | Bit/dim 3.8418(3.8563) | Xent 1.3531(1.3870) | Loss 4.5184(4.5498) | Error 0.4815(0.4973) Steps 892(901.06) | Grad Norm 0.3698(0.7050) | Total Time 14.00(14.00)\n",
      "Iter 0702 | Time 83.0753(83.5667) | Bit/dim 3.8377(3.8558) | Xent 1.3785(1.3868) | Loss 4.5270(4.5492) | Error 0.4908(0.4971) Steps 886(900.60) | Grad Norm 0.7985(0.7078) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0117 | Time 30.0683, Epoch Time 539.1553(520.3380), Bit/dim 3.8477(best: 3.8488), Xent 1.3398, Loss 4.5176, Error 0.4806(best: 0.4822)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0703 | Time 79.9078(83.4570) | Bit/dim 3.8369(3.8552) | Xent 1.3884(1.3868) | Loss 4.5311(4.5486) | Error 0.4911(0.4970) Steps 886(900.17) | Grad Norm 0.4353(0.6996) | Total Time 14.00(14.00)\n",
      "Iter 0704 | Time 81.1770(83.3886) | Bit/dim 3.8401(3.8547) | Xent 1.3895(1.3869) | Loss 4.5349(4.5482) | Error 0.4984(0.4970) Steps 898(900.10) | Grad Norm 0.4283(0.6915) | Total Time 14.00(14.00)\n",
      "Iter 0705 | Time 81.5454(83.3333) | Bit/dim 3.8397(3.8543) | Xent 1.3829(1.3868) | Loss 4.5312(4.5477) | Error 0.4975(0.4970) Steps 892(899.86) | Grad Norm 0.6113(0.6891) | Total Time 14.00(14.00)\n",
      "Iter 0706 | Time 82.8005(83.3173) | Bit/dim 3.8554(3.8543) | Xent 1.3754(1.3864) | Loss 4.5431(4.5476) | Error 0.4894(0.4968) Steps 892(899.62) | Grad Norm 0.3841(0.6799) | Total Time 14.00(14.00)\n",
      "Iter 0707 | Time 83.5071(83.3230) | Bit/dim 3.8462(3.8541) | Xent 1.3468(1.3852) | Loss 4.5196(4.5467) | Error 0.4876(0.4965) Steps 892(899.39) | Grad Norm 0.5344(0.6756) | Total Time 14.00(14.00)\n",
      "Iter 0708 | Time 82.2483(83.2908) | Bit/dim 3.8504(3.8540) | Xent 1.3595(1.3845) | Loss 4.5301(4.5462) | Error 0.4875(0.4962) Steps 892(899.17) | Grad Norm 0.5728(0.6725) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0118 | Time 29.8928, Epoch Time 536.8455(520.8332), Bit/dim 3.8464(best: 3.8477), Xent 1.3397, Loss 4.5162, Error 0.4821(best: 0.4806)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0709 | Time 82.7563(83.2747) | Bit/dim 3.8476(3.8538) | Xent 1.3685(1.3840) | Loss 4.5318(4.5458) | Error 0.4945(0.4962) Steps 880(898.60) | Grad Norm 0.4022(0.6644) | Total Time 14.00(14.00)\n",
      "Iter 0710 | Time 83.3001(83.2755) | Bit/dim 3.8413(3.8534) | Xent 1.3962(1.3844) | Loss 4.5394(4.5456) | Error 0.5061(0.4965) Steps 880(898.04) | Grad Norm 0.6406(0.6637) | Total Time 14.00(14.00)\n",
      "Iter 0711 | Time 80.9622(83.2061) | Bit/dim 3.8431(3.8531) | Xent 1.3643(1.3838) | Loss 4.5253(4.5450) | Error 0.4875(0.4962) Steps 886(897.68) | Grad Norm 0.7598(0.6666) | Total Time 14.00(14.00)\n",
      "Iter 0712 | Time 84.5248(83.2456) | Bit/dim 3.8446(3.8528) | Xent 1.3623(1.3831) | Loss 4.5258(4.5444) | Error 0.4935(0.4961) Steps 886(897.33) | Grad Norm 0.5762(0.6639) | Total Time 14.00(14.00)\n",
      "Iter 0713 | Time 81.5152(83.1937) | Bit/dim 3.8454(3.8526) | Xent 1.3788(1.3830) | Loss 4.5348(4.5441) | Error 0.4894(0.4959) Steps 898(897.35) | Grad Norm 0.9047(0.6711) | Total Time 14.00(14.00)\n",
      "Iter 0714 | Time 82.7247(83.1797) | Bit/dim 3.8483(3.8525) | Xent 1.3736(1.3827) | Loss 4.5351(4.5438) | Error 0.4981(0.4960) Steps 892(897.19) | Grad Norm 0.4125(0.6633) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0119 | Time 30.1581, Epoch Time 542.2100(521.4745), Bit/dim 3.8451(best: 3.8464), Xent 1.3354, Loss 4.5128, Error 0.4792(best: 0.4806)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0715 | Time 82.7846(83.1678) | Bit/dim 3.8402(3.8521) | Xent 1.3835(1.3827) | Loss 4.5319(4.5435) | Error 0.4945(0.4960) Steps 880(896.67) | Grad Norm 0.3197(0.6530) | Total Time 14.00(14.00)\n",
      "Iter 0716 | Time 82.7152(83.1542) | Bit/dim 3.8350(3.8516) | Xent 1.3598(1.3820) | Loss 4.5149(4.5426) | Error 0.4902(0.4958) Steps 874(895.99) | Grad Norm 0.6812(0.6539) | Total Time 14.00(14.00)\n",
      "Iter 0717 | Time 84.4573(83.1933) | Bit/dim 3.8551(3.8517) | Xent 1.3631(1.3815) | Loss 4.5366(4.5425) | Error 0.4848(0.4955) Steps 898(896.05) | Grad Norm 0.5072(0.6495) | Total Time 14.00(14.00)\n",
      "Iter 0718 | Time 81.4254(83.1403) | Bit/dim 3.8417(3.8514) | Xent 1.3811(1.3815) | Loss 4.5322(4.5421) | Error 0.4982(0.4955) Steps 874(895.39) | Grad Norm 0.6642(0.6499) | Total Time 14.00(14.00)\n",
      "Iter 0719 | Time 80.6688(83.0661) | Bit/dim 3.8322(3.8508) | Xent 1.3636(1.3809) | Loss 4.5139(4.5413) | Error 0.4901(0.4954) Steps 886(895.11) | Grad Norm 1.0628(0.6623) | Total Time 14.00(14.00)\n",
      "Iter 0720 | Time 84.2460(83.1015) | Bit/dim 3.8576(3.8510) | Xent 1.3753(1.3808) | Loss 4.5452(4.5414) | Error 0.4919(0.4953) Steps 880(894.65) | Grad Norm 0.5118(0.6578) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0120 | Time 30.2034, Epoch Time 542.5676(522.1073), Bit/dim 3.8436(best: 3.8451), Xent 1.3350, Loss 4.5111, Error 0.4815(best: 0.4792)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0721 | Time 82.6209(83.0871) | Bit/dim 3.8462(3.8509) | Xent 1.3658(1.3803) | Loss 4.5291(4.5410) | Error 0.4909(0.4951) Steps 868(893.85) | Grad Norm 0.7956(0.6619) | Total Time 14.00(14.00)\n",
      "Iter 0722 | Time 84.3718(83.1257) | Bit/dim 3.8503(3.8509) | Xent 1.3664(1.3799) | Loss 4.5335(4.5408) | Error 0.4961(0.4952) Steps 892(893.80) | Grad Norm 0.6900(0.6627) | Total Time 14.00(14.00)\n",
      "Iter 0723 | Time 86.1669(83.2169) | Bit/dim 3.8300(3.8502) | Xent 1.3746(1.3797) | Loss 4.5173(4.5401) | Error 0.4905(0.4950) Steps 904(894.11) | Grad Norm 1.1558(0.6775) | Total Time 14.00(14.00)\n",
      "Iter 0724 | Time 84.0489(83.2419) | Bit/dim 3.8425(3.8500) | Xent 1.3774(1.3797) | Loss 4.5312(4.5398) | Error 0.4871(0.4948) Steps 898(894.22) | Grad Norm 0.5249(0.6730) | Total Time 14.00(14.00)\n",
      "Iter 0725 | Time 83.5842(83.2521) | Bit/dim 3.8463(3.8499) | Xent 1.3603(1.3791) | Loss 4.5265(4.5394) | Error 0.4872(0.4946) Steps 892(894.16) | Grad Norm 0.5234(0.6685) | Total Time 14.00(14.00)\n",
      "Iter 0726 | Time 82.9423(83.2428) | Bit/dim 3.8319(3.8494) | Xent 1.3607(1.3785) | Loss 4.5122(4.5386) | Error 0.4914(0.4945) Steps 892(894.09) | Grad Norm 0.9004(0.6754) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0121 | Time 29.7752, Epoch Time 549.5320(522.9301), Bit/dim 3.8418(best: 3.8436), Xent 1.3337, Loss 4.5086, Error 0.4814(best: 0.4792)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0727 | Time 85.1539(83.3002) | Bit/dim 3.8452(3.8492) | Xent 1.3551(1.3778) | Loss 4.5227(4.5382) | Error 0.4879(0.4943) Steps 892(894.03) | Grad Norm 0.6555(0.6748) | Total Time 14.00(14.00)\n",
      "Iter 0728 | Time 82.6162(83.2796) | Bit/dim 3.8467(3.8492) | Xent 1.3937(1.3783) | Loss 4.5435(4.5383) | Error 0.5082(0.4947) Steps 856(892.89) | Grad Norm 1.2817(0.6930) | Total Time 14.00(14.00)\n",
      "Iter 0729 | Time 82.1689(83.2463) | Bit/dim 3.8287(3.8486) | Xent 1.3680(1.3780) | Loss 4.5127(4.5375) | Error 0.4912(0.4946) Steps 880(892.50) | Grad Norm 0.5825(0.6897) | Total Time 14.00(14.00)\n",
      "Iter 0730 | Time 79.0159(83.1194) | Bit/dim 3.8334(3.8481) | Xent 1.3599(1.3774) | Loss 4.5134(4.5368) | Error 0.4905(0.4945) Steps 880(892.13) | Grad Norm 0.8868(0.6956) | Total Time 14.00(14.00)\n",
      "Iter 0731 | Time 81.5226(83.0715) | Bit/dim 3.8471(3.8481) | Xent 1.3621(1.3770) | Loss 4.5282(4.5366) | Error 0.4762(0.4939) Steps 880(891.76) | Grad Norm 0.7376(0.6969) | Total Time 14.00(14.00)\n",
      "Iter 0732 | Time 83.0732(83.0716) | Bit/dim 3.8363(3.8477) | Xent 1.3706(1.3768) | Loss 4.5215(4.5361) | Error 0.4891(0.4938) Steps 880(891.41) | Grad Norm 0.8933(0.7028) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0122 | Time 29.8929, Epoch Time 539.5636(523.4291), Bit/dim 3.8405(best: 3.8418), Xent 1.3319, Loss 4.5064, Error 0.4768(best: 0.4792)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0733 | Time 81.9580(83.0381) | Bit/dim 3.8453(3.8476) | Xent 1.3908(1.3772) | Loss 4.5407(4.5362) | Error 0.5024(0.4940) Steps 868(890.71) | Grad Norm 0.5089(0.6970) | Total Time 14.00(14.00)\n",
      "Iter 0734 | Time 80.9731(82.9762) | Bit/dim 3.8397(3.8474) | Xent 1.3728(1.3771) | Loss 4.5261(4.5359) | Error 0.4940(0.4940) Steps 874(890.21) | Grad Norm 0.5039(0.6912) | Total Time 14.00(14.00)\n",
      "Iter 0735 | Time 79.7235(82.8786) | Bit/dim 3.8412(3.8472) | Xent 1.3460(1.3762) | Loss 4.5142(4.5353) | Error 0.4846(0.4937) Steps 868(889.54) | Grad Norm 1.2111(0.7068) | Total Time 14.00(14.00)\n",
      "Iter 0736 | Time 79.9024(82.7893) | Bit/dim 3.8310(3.8467) | Xent 1.3593(1.3756) | Loss 4.5106(4.5346) | Error 0.4821(0.4934) Steps 868(888.89) | Grad Norm 0.6578(0.7053) | Total Time 14.00(14.00)\n",
      "Iter 0737 | Time 82.4187(82.7782) | Bit/dim 3.8378(3.8465) | Xent 1.3773(1.3757) | Loss 4.5265(4.5343) | Error 0.4941(0.4934) Steps 880(888.63) | Grad Norm 1.3597(0.7249) | Total Time 14.00(14.00)\n",
      "Iter 0738 | Time 80.3320(82.7048) | Bit/dim 3.8438(3.8464) | Xent 1.3740(1.3756) | Loss 4.5308(4.5342) | Error 0.4899(0.4933) Steps 874(888.19) | Grad Norm 0.7791(0.7266) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0123 | Time 29.6400, Epoch Time 530.8565(523.6519), Bit/dim 3.8399(best: 3.8405), Xent 1.3310, Loss 4.5054, Error 0.4799(best: 0.4768)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0739 | Time 83.1512(82.7182) | Bit/dim 3.8444(3.8463) | Xent 1.3806(1.3758) | Loss 4.5347(4.5342) | Error 0.4959(0.4934) Steps 880(887.94) | Grad Norm 0.8485(0.7302) | Total Time 14.00(14.00)\n",
      "Iter 0740 | Time 81.5145(82.6821) | Bit/dim 3.8414(3.8462) | Xent 1.3535(1.3751) | Loss 4.5181(4.5337) | Error 0.4889(0.4933) Steps 868(887.34) | Grad Norm 1.6678(0.7583) | Total Time 14.00(14.00)\n",
      "Iter 0741 | Time 82.9065(82.6888) | Bit/dim 3.8271(3.8456) | Xent 1.3610(1.3747) | Loss 4.5077(4.5330) | Error 0.4880(0.4931) Steps 874(886.94) | Grad Norm 1.4099(0.7779) | Total Time 14.00(14.00)\n",
      "Iter 0742 | Time 82.6163(82.6867) | Bit/dim 3.8443(3.8456) | Xent 1.3627(1.3743) | Loss 4.5257(4.5327) | Error 0.4856(0.4929) Steps 874(886.56) | Grad Norm 1.0517(0.7861) | Total Time 14.00(14.00)\n",
      "Iter 0743 | Time 83.8826(82.7225) | Bit/dim 3.8409(3.8454) | Xent 1.3597(1.3739) | Loss 4.5208(4.5324) | Error 0.4869(0.4927) Steps 868(886.00) | Grad Norm 0.8191(0.7871) | Total Time 14.00(14.00)\n",
      "Iter 0744 | Time 81.1613(82.6757) | Bit/dim 3.8314(3.8450) | Xent 1.3657(1.3737) | Loss 4.5143(4.5318) | Error 0.4825(0.4924) Steps 880(885.82) | Grad Norm 1.0849(0.7960) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0124 | Time 29.6659, Epoch Time 540.4596(524.1561), Bit/dim 3.8384(best: 3.8399), Xent 1.3273, Loss 4.5020, Error 0.4771(best: 0.4768)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0745 | Time 83.3443(82.6958) | Bit/dim 3.8443(3.8450) | Xent 1.3559(1.3731) | Loss 4.5223(4.5315) | Error 0.4810(0.4920) Steps 886(885.82) | Grad Norm 0.9665(0.8011) | Total Time 14.00(14.00)\n",
      "Iter 0746 | Time 82.4035(82.6870) | Bit/dim 3.8362(3.8447) | Xent 1.3677(1.3730) | Loss 4.5200(4.5312) | Error 0.4916(0.4920) Steps 868(885.29) | Grad Norm 1.6899(0.8278) | Total Time 14.00(14.00)\n",
      "Iter 0747 | Time 83.5896(82.7141) | Bit/dim 3.8257(3.8442) | Xent 1.3540(1.3724) | Loss 4.5027(4.5303) | Error 0.4864(0.4919) Steps 862(884.59) | Grad Norm 0.8901(0.8297) | Total Time 14.00(14.00)\n",
      "Iter 0748 | Time 81.2575(82.6704) | Bit/dim 3.8404(3.8440) | Xent 1.3584(1.3720) | Loss 4.5196(4.5300) | Error 0.4870(0.4917) Steps 880(884.45) | Grad Norm 0.9421(0.8331) | Total Time 14.00(14.00)\n",
      "Iter 0749 | Time 80.1157(82.5937) | Bit/dim 3.8388(3.8439) | Xent 1.3573(1.3715) | Loss 4.5174(4.5296) | Error 0.4865(0.4916) Steps 868(883.96) | Grad Norm 1.3596(0.8488) | Total Time 14.00(14.00)\n",
      "Iter 0750 | Time 81.2086(82.5522) | Bit/dim 3.8373(3.8437) | Xent 1.3617(1.3712) | Loss 4.5182(4.5293) | Error 0.4931(0.4916) Steps 880(883.84) | Grad Norm 0.6848(0.8439) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0125 | Time 29.9178, Epoch Time 537.8698(524.5675), Bit/dim 3.8363(best: 3.8384), Xent 1.3291, Loss 4.5008, Error 0.4802(best: 0.4768)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0751 | Time 82.4840(82.5501) | Bit/dim 3.8328(3.8434) | Xent 1.3586(1.3709) | Loss 4.5122(4.5288) | Error 0.4810(0.4913) Steps 880(883.73) | Grad Norm 1.9758(0.8779) | Total Time 14.00(14.00)\n",
      "Iter 0752 | Time 84.0306(82.5945) | Bit/dim 3.8475(3.8435) | Xent 1.3590(1.3705) | Loss 4.5270(4.5287) | Error 0.4930(0.4913) Steps 856(882.89) | Grad Norm 1.5267(0.8973) | Total Time 14.00(14.00)\n",
      "Iter 0753 | Time 80.6241(82.5354) | Bit/dim 3.8379(3.8433) | Xent 1.3497(1.3699) | Loss 4.5128(4.5283) | Error 0.4882(0.4912) Steps 880(882.81) | Grad Norm 1.2257(0.9072) | Total Time 14.00(14.00)\n",
      "Iter 0754 | Time 80.6580(82.4791) | Bit/dim 3.8349(3.8431) | Xent 1.3637(1.3697) | Loss 4.5168(4.5279) | Error 0.4900(0.4912) Steps 862(882.18) | Grad Norm 1.1473(0.9144) | Total Time 14.00(14.00)\n",
      "Iter 0755 | Time 80.5505(82.4212) | Bit/dim 3.8251(3.8425) | Xent 1.3809(1.3700) | Loss 4.5156(4.5275) | Error 0.4959(0.4914) Steps 862(881.58) | Grad Norm 1.4657(0.9309) | Total Time 14.00(14.00)\n",
      "Iter 0756 | Time 84.1331(82.4726) | Bit/dim 3.8353(3.8423) | Xent 1.3854(1.3705) | Loss 4.5280(4.5276) | Error 0.4928(0.4914) Steps 880(881.53) | Grad Norm 0.5399(0.9192) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0126 | Time 30.0018, Epoch Time 538.3983(524.9825), Bit/dim 3.8362(best: 3.8363), Xent 1.3262, Loss 4.4993, Error 0.4752(best: 0.4768)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0757 | Time 84.6353(82.5375) | Bit/dim 3.8381(3.8422) | Xent 1.3672(1.3704) | Loss 4.5217(4.5274) | Error 0.4965(0.4915) Steps 886(881.66) | Grad Norm 1.9929(0.9514) | Total Time 14.00(14.00)\n",
      "Iter 0758 | Time 83.3936(82.5632) | Bit/dim 3.8348(3.8420) | Xent 1.3501(1.3698) | Loss 4.5098(4.5269) | Error 0.4868(0.4914) Steps 892(881.97) | Grad Norm 0.6101(0.9412) | Total Time 14.00(14.00)\n",
      "Iter 0759 | Time 81.9864(82.5459) | Bit/dim 3.8344(3.8417) | Xent 1.3725(1.3699) | Loss 4.5207(4.5267) | Error 0.4992(0.4916) Steps 880(881.91) | Grad Norm 1.4729(0.9571) | Total Time 14.00(14.00)\n",
      "Iter 0760 | Time 78.7700(82.4326) | Bit/dim 3.8357(3.8416) | Xent 1.3493(1.3692) | Loss 4.5103(4.5262) | Error 0.4819(0.4913) Steps 874(881.68) | Grad Norm 0.5089(0.9437) | Total Time 14.00(14.00)\n",
      "Iter 0761 | Time 83.3693(82.4607) | Bit/dim 3.8373(3.8414) | Xent 1.3607(1.3690) | Loss 4.5176(4.5259) | Error 0.4911(0.4913) Steps 874(881.45) | Grad Norm 1.3072(0.9546) | Total Time 14.00(14.00)\n",
      "Iter 0762 | Time 79.4681(82.3709) | Bit/dim 3.8258(3.8410) | Xent 1.3675(1.3689) | Loss 4.5095(4.5254) | Error 0.4825(0.4911) Steps 874(881.22) | Grad Norm 0.9355(0.9540) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0127 | Time 29.7258, Epoch Time 537.0951(525.3458), Bit/dim 3.8349(best: 3.8362), Xent 1.3247, Loss 4.4973, Error 0.4776(best: 0.4752)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0763 | Time 82.4646(82.3737) | Bit/dim 3.8404(3.8409) | Xent 1.3668(1.3689) | Loss 4.5239(4.5254) | Error 0.4875(0.4910) Steps 886(881.37) | Grad Norm 0.6908(0.9461) | Total Time 14.00(14.00)\n",
      "Iter 0764 | Time 83.2643(82.4004) | Bit/dim 3.8355(3.8408) | Xent 1.3618(1.3687) | Loss 4.5164(4.5251) | Error 0.4898(0.4909) Steps 874(881.15) | Grad Norm 1.5236(0.9634) | Total Time 14.00(14.00)\n",
      "Iter 0765 | Time 82.3386(82.3986) | Bit/dim 3.8341(3.8406) | Xent 1.3378(1.3677) | Loss 4.5029(4.5244) | Error 0.4838(0.4907) Steps 880(881.11) | Grad Norm 1.1067(0.9677) | Total Time 14.00(14.00)\n",
      "Iter 0766 | Time 82.6615(82.4065) | Bit/dim 3.8316(3.8403) | Xent 1.3775(1.3680) | Loss 4.5203(4.5243) | Error 0.4941(0.4908) Steps 874(880.90) | Grad Norm 0.9049(0.9659) | Total Time 14.00(14.00)\n",
      "Iter 0767 | Time 82.1893(82.4000) | Bit/dim 3.8317(3.8400) | Xent 1.3667(1.3680) | Loss 4.5151(4.5240) | Error 0.4942(0.4909) Steps 868(880.51) | Grad Norm 0.7523(0.9595) | Total Time 14.00(14.00)\n",
      "Iter 0768 | Time 82.0362(82.3890) | Bit/dim 3.8280(3.8397) | Xent 1.3543(1.3676) | Loss 4.5051(4.5235) | Error 0.4842(0.4907) Steps 874(880.32) | Grad Norm 0.7085(0.9519) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0128 | Time 29.9256, Epoch Time 540.5668(525.8025), Bit/dim 3.8336(best: 3.8349), Xent 1.3215, Loss 4.4944, Error 0.4784(best: 0.4752)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0769 | Time 78.0456(82.2587) | Bit/dim 3.8320(3.8395) | Xent 1.3510(1.3671) | Loss 4.5075(4.5230) | Error 0.4888(0.4907) Steps 862(879.77) | Grad Norm 0.9643(0.9523) | Total Time 14.00(14.00)\n",
      "Iter 0770 | Time 79.7107(82.1823) | Bit/dim 3.8351(3.8393) | Xent 1.3512(1.3666) | Loss 4.5107(4.5226) | Error 0.4882(0.4906) Steps 868(879.41) | Grad Norm 0.7703(0.9468) | Total Time 14.00(14.00)\n",
      "Iter 0771 | Time 79.6254(82.1056) | Bit/dim 3.8236(3.8389) | Xent 1.3555(1.3663) | Loss 4.5013(4.5220) | Error 0.4830(0.4904) Steps 868(879.07) | Grad Norm 1.6029(0.9665) | Total Time 14.00(14.00)\n",
      "Iter 0772 | Time 79.5891(82.0301) | Bit/dim 3.8346(3.8387) | Xent 1.3674(1.3663) | Loss 4.5183(4.5219) | Error 0.4931(0.4904) Steps 856(878.38) | Grad Norm 0.8457(0.9629) | Total Time 14.00(14.00)\n",
      "Iter 0773 | Time 78.6953(81.9301) | Bit/dim 3.8364(3.8387) | Xent 1.3754(1.3666) | Loss 4.5241(4.5219) | Error 0.4930(0.4905) Steps 868(878.07) | Grad Norm 1.3613(0.9748) | Total Time 14.00(14.00)\n",
      "Iter 0774 | Time 83.9442(81.9905) | Bit/dim 3.8234(3.8382) | Xent 1.3513(1.3661) | Loss 4.4990(4.5213) | Error 0.4859(0.4904) Steps 868(877.77) | Grad Norm 1.1253(0.9794) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0129 | Time 30.0733, Epoch Time 525.2947(525.7872), Bit/dim 3.8324(best: 3.8336), Xent 1.3207, Loss 4.4927, Error 0.4772(best: 0.4752)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0775 | Time 81.5845(81.9783) | Bit/dim 3.8232(3.8377) | Xent 1.3473(1.3656) | Loss 4.4969(4.5205) | Error 0.4850(0.4902) Steps 868(877.47) | Grad Norm 1.7263(1.0018) | Total Time 14.00(14.00)\n",
      "Iter 0776 | Time 81.4274(81.9618) | Bit/dim 3.8258(3.8374) | Xent 1.3380(1.3647) | Loss 4.4948(4.5198) | Error 0.4802(0.4899) Steps 880(877.55) | Grad Norm 0.6703(0.9918) | Total Time 14.00(14.00)\n",
      "Iter 0777 | Time 79.8579(81.8987) | Bit/dim 3.8371(3.8374) | Xent 1.3650(1.3647) | Loss 4.5196(4.5197) | Error 0.4880(0.4899) Steps 868(877.26) | Grad Norm 1.2348(0.9991) | Total Time 14.00(14.00)\n",
      "Iter 0778 | Time 81.9947(81.9015) | Bit/dim 3.8361(3.8373) | Xent 1.3582(1.3645) | Loss 4.5152(4.5196) | Error 0.4905(0.4899) Steps 868(876.98) | Grad Norm 0.8183(0.9937) | Total Time 14.00(14.00)\n",
      "Iter 0779 | Time 79.6595(81.8343) | Bit/dim 3.8312(3.8372) | Xent 1.3626(1.3645) | Loss 4.5125(4.5194) | Error 0.4959(0.4901) Steps 862(876.53) | Grad Norm 0.7102(0.9852) | Total Time 14.00(14.00)\n",
      "Iter 0780 | Time 81.1470(81.8137) | Bit/dim 3.8305(3.8370) | Xent 1.3560(1.3642) | Loss 4.5085(4.5191) | Error 0.4832(0.4899) Steps 868(876.28) | Grad Norm 0.6068(0.9738) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0130 | Time 29.3410, Epoch Time 530.8654(525.9396), Bit/dim 3.8301(best: 3.8324), Xent 1.3180, Loss 4.4892, Error 0.4741(best: 0.4752)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0781 | Time 81.0144(81.7897) | Bit/dim 3.8394(3.8370) | Xent 1.3427(1.3636) | Loss 4.5107(4.5188) | Error 0.4869(0.4898) Steps 862(875.85) | Grad Norm 0.5208(0.9602) | Total Time 14.00(14.00)\n",
      "Iter 0782 | Time 81.2707(81.7741) | Bit/dim 3.8325(3.8369) | Xent 1.3606(1.3635) | Loss 4.5128(4.5186) | Error 0.4886(0.4897) Steps 850(875.07) | Grad Norm 0.7734(0.9546) | Total Time 14.00(14.00)\n",
      "Iter 0783 | Time 83.9624(81.8398) | Bit/dim 3.8226(3.8365) | Xent 1.3539(1.3632) | Loss 4.4996(4.5181) | Error 0.4842(0.4896) Steps 880(875.22) | Grad Norm 1.0297(0.9569) | Total Time 14.00(14.00)\n",
      "Iter 0784 | Time 78.6625(81.7444) | Bit/dim 3.8236(3.8361) | Xent 1.3383(1.3625) | Loss 4.4927(4.5173) | Error 0.4824(0.4894) Steps 862(874.83) | Grad Norm 0.5389(0.9444) | Total Time 14.00(14.00)\n",
      "Iter 0785 | Time 78.1897(81.6378) | Bit/dim 3.8309(3.8359) | Xent 1.3659(1.3626) | Loss 4.5139(4.5172) | Error 0.4862(0.4893) Steps 868(874.62) | Grad Norm 0.6483(0.9355) | Total Time 14.00(14.00)\n",
      "Iter 0786 | Time 78.9135(81.5561) | Bit/dim 3.8234(3.8355) | Xent 1.3446(1.3620) | Loss 4.4957(4.5166) | Error 0.4789(0.4889) Steps 868(874.42) | Grad Norm 0.6636(0.9273) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0131 | Time 29.7743, Epoch Time 536.5185(526.2570), Bit/dim 3.8286(best: 3.8301), Xent 1.3156, Loss 4.4864, Error 0.4746(best: 0.4741)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0787 | Time 81.1132(81.5428) | Bit/dim 3.8268(3.8353) | Xent 1.3589(1.3619) | Loss 4.5063(4.5163) | Error 0.4906(0.4890) Steps 856(873.87) | Grad Norm 0.6445(0.9188) | Total Time 14.00(14.00)\n",
      "Iter 0788 | Time 80.7929(81.5203) | Bit/dim 3.8312(3.8352) | Xent 1.3511(1.3616) | Loss 4.5067(4.5160) | Error 0.4791(0.4887) Steps 856(873.33) | Grad Norm 0.5814(0.9087) | Total Time 14.00(14.00)\n",
      "Iter 0789 | Time 79.8495(81.4702) | Bit/dim 3.8194(3.8347) | Xent 1.3558(1.3614) | Loss 4.4973(4.5154) | Error 0.4869(0.4886) Steps 868(873.17) | Grad Norm 0.5212(0.8971) | Total Time 14.00(14.00)\n",
      "Iter 0790 | Time 80.0129(81.4264) | Bit/dim 3.8338(3.8347) | Xent 1.3504(1.3611) | Loss 4.5090(4.5152) | Error 0.4851(0.4885) Steps 874(873.20) | Grad Norm 0.6620(0.8900) | Total Time 14.00(14.00)\n",
      "Iter 0791 | Time 79.0354(81.3547) | Bit/dim 3.8294(3.8345) | Xent 1.3468(1.3607) | Loss 4.5027(4.5148) | Error 0.4776(0.4882) Steps 856(872.68) | Grad Norm 0.5179(0.8789) | Total Time 14.00(14.00)\n",
      "Iter 0792 | Time 78.9558(81.2827) | Bit/dim 3.8307(3.8344) | Xent 1.3625(1.3607) | Loss 4.5119(4.5148) | Error 0.4921(0.4883) Steps 868(872.54) | Grad Norm 0.7834(0.8760) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0132 | Time 29.4918, Epoch Time 524.9314(526.2172), Bit/dim 3.8283(best: 3.8286), Xent 1.3123, Loss 4.4844, Error 0.4735(best: 0.4741)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0793 | Time 78.2451(81.1916) | Bit/dim 3.8239(3.8341) | Xent 1.3400(1.3601) | Loss 4.4939(4.5141) | Error 0.4836(0.4882) Steps 868(872.41) | Grad Norm 0.5916(0.8675) | Total Time 14.00(14.00)\n",
      "Iter 0794 | Time 80.0484(81.1573) | Bit/dim 3.8315(3.8340) | Xent 1.3403(1.3595) | Loss 4.5017(4.5138) | Error 0.4790(0.4879) Steps 862(872.09) | Grad Norm 0.5919(0.8592) | Total Time 14.00(14.00)\n",
      "Iter 0795 | Time 80.5776(81.1399) | Bit/dim 3.8280(3.8338) | Xent 1.3624(1.3596) | Loss 4.5092(4.5136) | Error 0.4925(0.4881) Steps 856(871.61) | Grad Norm 0.7085(0.8547) | Total Time 14.00(14.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_cifar.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 8000 --test_batch_size 5000 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_baseline_lr_0_01_run1_post --load_dir ../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_baseline_lr_0_01_run1 --seed 1 --conditional True --controlled_tol False --train_mode semisup --lr 0.001 --warmup_iters 1000 --atol 1e-5  --rtol 1e-5 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
