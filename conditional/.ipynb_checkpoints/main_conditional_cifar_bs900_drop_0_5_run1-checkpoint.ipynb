{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='4,5,6,7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_cifar.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "if args.controlled_tol:\n",
      "    import lib.odenvp_conditional_tol as odenvp\n",
      "else:\n",
      "    import lib.odenvp_conditional as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz = modules.GaussianDiag.logp(mean, logs, z).view(-1,1)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(z)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol},)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z = modules.GaussianDiag.sample(mean, logs)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            loss.backward()\n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, conditional=True, controlled_tol=False, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume=None, rtol=1e-05, save='../experiments_published/cnf_conditional_cifar10_bs900_run1', seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=6144, bias=True)\n",
      "  (project_class): LinearZeros(in_features=3072, out_features=10, bias=True)\n",
      ")\n",
      "Number of trainable parameters: 1469494\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc.py:286: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "Iter 0010 | Time 20.9860(46.1566) | Bit/dim 9.0495(9.2216) | Xent 2.2646(2.2985) | Loss 10.1818(10.3708) | Error 0.7589(0.8764) Steps 574(574.00) | Grad Norm 13.5161(17.7322) | Total Time 14.00(14.00)\n",
      "Iter 0020 | Time 21.2643(39.5750) | Bit/dim 8.6482(9.1044) | Xent 2.1813(2.2786) | Loss 9.7389(10.2437) | Error 0.7256(0.8430) Steps 574(574.00) | Grad Norm 5.4689(15.3244) | Total Time 14.00(14.00)\n",
      "Iter 0030 | Time 21.3996(34.7665) | Bit/dim 8.3936(8.9442) | Xent 2.1105(2.2423) | Loss 9.4489(10.0654) | Error 0.7144(0.8110) Steps 574(574.00) | Grad Norm 3.8204(12.4221) | Total Time 14.00(14.00)\n",
      "Iter 0040 | Time 21.2916(31.2639) | Bit/dim 8.2101(8.7685) | Xent 2.0773(2.2028) | Loss 9.2487(9.8699) | Error 0.6922(0.7814) Steps 574(574.00) | Grad Norm 3.6425(10.1032) | Total Time 14.00(14.00)\n",
      "Iter 0050 | Time 21.5044(28.6660) | Bit/dim 7.9455(8.5790) | Xent 2.0095(2.1590) | Loss 8.9502(9.6585) | Error 0.6633(0.7540) Steps 574(574.00) | Grad Norm 2.6935(8.2634) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0001 | Time 103.2895, Epoch Time 1323.5708(1323.5708), Bit/dim 7.7959(best: inf), Xent 1.9951, Loss 8.7934, Error 0.6563(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0060 | Time 21.3008(26.7709) | Bit/dim 7.6711(8.3750) | Xent 1.9633(2.1160) | Loss 8.6528(9.4330) | Error 0.6422(0.7293) Steps 574(574.00) | Grad Norm 2.7950(6.8543) | Total Time 14.00(14.00)\n",
      "Iter 0070 | Time 22.1563(25.3637) | Bit/dim 7.3697(8.1466) | Xent 1.9699(2.0825) | Loss 8.3546(9.1878) | Error 0.6356(0.7095) Steps 574(574.00) | Grad Norm 2.0309(5.7055) | Total Time 14.00(14.00)\n",
      "Iter 0080 | Time 20.4891(24.3003) | Bit/dim 7.1793(7.9122) | Xent 1.9865(2.0581) | Loss 8.1725(8.9412) | Error 0.6433(0.6929) Steps 574(574.00) | Grad Norm 1.6763(4.7123) | Total Time 14.00(14.00)\n",
      "Iter 0090 | Time 22.3966(23.6557) | Bit/dim 7.0833(7.7042) | Xent 2.0292(2.0469) | Loss 8.0979(8.7276) | Error 0.6800(0.6855) Steps 592(576.80) | Grad Norm 1.5295(3.9252) | Total Time 14.00(14.00)\n",
      "Iter 0100 | Time 24.6475(23.8054) | Bit/dim 7.0312(7.5316) | Xent 2.0010(2.0347) | Loss 8.0317(8.5489) | Error 0.6611(0.6786) Steps 610(583.78) | Grad Norm 1.8169(3.3055) | Total Time 14.00(14.00)\n",
      "Iter 0110 | Time 25.8610(24.2262) | Bit/dim 7.0006(7.3948) | Xent 2.0004(2.0257) | Loss 8.0008(8.4077) | Error 0.6700(0.6742) Steps 610(590.67) | Grad Norm 2.2625(2.9699) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0002 | Time 93.8850, Epoch Time 1362.1724(1324.7289), Bit/dim 6.9924(best: 7.7959), Xent 1.9927, Loss 7.9887, Error 0.6599(best: 0.6563)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0120 | Time 26.5333(24.5514) | Bit/dim 6.9400(7.2820) | Xent 2.0146(2.0164) | Loss 7.9473(8.2902) | Error 0.6867(0.6721) Steps 616(596.60) | Grad Norm 2.7268(2.9534) | Total Time 14.00(14.00)\n",
      "Iter 0130 | Time 26.0834(24.9794) | Bit/dim 6.9075(7.1924) | Xent 1.9834(2.0073) | Loss 7.8992(8.1961) | Error 0.6578(0.6677) Steps 628(604.26) | Grad Norm 4.5259(3.3723) | Total Time 14.00(14.00)\n",
      "Iter 0140 | Time 26.5259(25.5644) | Bit/dim 6.8911(7.1156) | Xent 1.9704(1.9995) | Loss 7.8763(8.1153) | Error 0.6389(0.6678) Steps 628(610.49) | Grad Norm 3.1897(3.8683) | Total Time 14.00(14.00)\n",
      "Iter 0150 | Time 27.0910(25.7396) | Bit/dim 6.8317(7.0471) | Xent 1.9514(1.9905) | Loss 7.8074(8.0424) | Error 0.6522(0.6666) Steps 628(615.09) | Grad Norm 7.9702(4.5334) | Total Time 14.00(14.00)\n",
      "Iter 0160 | Time 26.2596(25.9111) | Bit/dim 6.7614(6.9806) | Xent 1.9595(1.9808) | Loss 7.7411(7.9710) | Error 0.6544(0.6649) Steps 628(618.48) | Grad Norm 11.9288(5.4419) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0003 | Time 104.6190, Epoch Time 1570.1679(1332.0920), Bit/dim 6.7136(best: 6.9924), Xent 1.9277, Loss 7.6775, Error 0.6485(best: 0.6563)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0170 | Time 26.5201(26.0159) | Bit/dim 6.6795(6.9115) | Xent 1.9338(1.9728) | Loss 7.6464(7.8979) | Error 0.6533(0.6623) Steps 628(620.98) | Grad Norm 9.5255(6.7227) | Total Time 14.00(14.00)\n",
      "Iter 0180 | Time 28.1013(26.1608) | Bit/dim 6.5896(6.8342) | Xent 2.2085(1.9823) | Loss 7.6939(7.8254) | Error 0.7922(0.6725) Steps 634(623.00) | Grad Norm 45.2411(11.1657) | Total Time 14.00(14.00)\n",
      "Iter 0190 | Time 26.7628(26.3248) | Bit/dim 6.4512(6.7466) | Xent 1.9890(1.9874) | Loss 7.4457(7.7403) | Error 0.6733(0.6801) Steps 628(624.74) | Grad Norm 19.3252(13.9518) | Total Time 14.00(14.00)\n",
      "Iter 0200 | Time 27.3674(26.5900) | Bit/dim 6.3488(6.6515) | Xent 2.0265(1.9896) | Loss 7.3621(7.6463) | Error 0.7467(0.6841) Steps 640(626.64) | Grad Norm 49.0655(17.9635) | Total Time 14.00(14.00)\n",
      "Iter 0210 | Time 28.3720(26.8465) | Bit/dim 6.2135(6.5461) | Xent 1.9210(1.9800) | Loss 7.1740(7.5361) | Error 0.6589(0.6807) Steps 640(630.01) | Grad Norm 25.1870(19.8747) | Total Time 14.00(14.00)\n",
      "Iter 0220 | Time 25.7890(26.9137) | Bit/dim 6.0388(6.4272) | Xent 2.0123(1.9680) | Loss 7.0450(7.4112) | Error 0.7122(0.6734) Steps 634(631.94) | Grad Norm 40.4678(20.2176) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0004 | Time 102.7969, Epoch Time 1607.2424(1340.3466), Bit/dim 6.0066(best: 6.7136), Xent 1.9134, Loss 6.9633, Error 0.6344(best: 0.6485)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0230 | Time 26.9563(27.3471) | Bit/dim 5.9737(6.3078) | Xent 1.8990(1.9567) | Loss 6.9232(7.2862) | Error 0.6611(0.6700) Steps 640(635.51) | Grad Norm 36.2833(23.2592) | Total Time 14.00(14.00)\n",
      "Iter 0240 | Time 26.8957(27.2105) | Bit/dim 5.7918(6.1885) | Xent 1.8903(1.9418) | Loss 6.7370(7.1594) | Error 0.6000(0.6593) Steps 646(635.95) | Grad Norm 11.4463(21.5170) | Total Time 14.00(14.00)\n",
      "Iter 0250 | Time 28.6157(27.6181) | Bit/dim 5.7619(6.0835) | Xent 1.8242(1.9271) | Loss 6.6739(7.0471) | Error 0.5944(0.6520) Steps 652(638.96) | Grad Norm 10.7684(22.4474) | Total Time 14.00(14.00)\n",
      "Iter 0260 | Time 26.5687(27.5486) | Bit/dim 5.7115(5.9860) | Xent 1.8814(1.9171) | Loss 6.6521(6.9446) | Error 0.6189(0.6463) Steps 634(639.03) | Grad Norm 8.4150(20.3827) | Total Time 14.00(14.00)\n",
      "Iter 0270 | Time 27.8949(27.6148) | Bit/dim 5.8152(5.9056) | Xent 2.0686(1.9130) | Loss 6.8495(6.8621) | Error 0.7244(0.6461) Steps 652(640.97) | Grad Norm 92.9390(21.5676) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0005 | Time 100.2165, Epoch Time 1656.8782(1349.8425), Bit/dim 5.9542(best: 6.0066), Xent 2.0647, Loss 6.9866, Error 0.7180(best: 0.6344)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0280 | Time 25.1833(27.3914) | Bit/dim 5.7891(5.9063) | Xent 2.0245(1.9614) | Loss 6.8013(6.8870) | Error 0.7344(0.6701) Steps 610(637.40) | Grad Norm 10.6448(26.9526) | Total Time 14.00(14.00)\n",
      "Iter 0290 | Time 25.6273(26.7875) | Bit/dim 5.6529(5.8600) | Xent 1.8664(1.9561) | Loss 6.5861(6.8380) | Error 0.6256(0.6700) Steps 616(631.66) | Grad Norm 5.7551(21.9564) | Total Time 14.00(14.00)\n",
      "Iter 0300 | Time 25.2507(26.3244) | Bit/dim 5.6423(5.8021) | Xent 1.8922(1.9396) | Loss 6.5884(6.7719) | Error 0.6578(0.6605) Steps 622(627.82) | Grad Norm 17.8305(18.2772) | Total Time 14.00(14.00)\n",
      "Iter 0310 | Time 26.1161(26.2173) | Bit/dim 5.5979(5.7570) | Xent 1.9294(1.9299) | Loss 6.5626(6.7220) | Error 0.6478(0.6583) Steps 622(626.29) | Grad Norm 3.9788(17.1600) | Total Time 14.00(14.00)\n",
      "Iter 0320 | Time 24.8279(26.0160) | Bit/dim 5.5679(5.7104) | Xent 1.8429(1.9146) | Loss 6.4894(6.6677) | Error 0.6167(0.6519) Steps 622(625.17) | Grad Norm 8.7857(14.3114) | Total Time 14.00(14.00)\n",
      "Iter 0330 | Time 25.6250(25.8705) | Bit/dim 5.5586(5.6731) | Xent 1.8420(1.9041) | Loss 6.4796(6.6252) | Error 0.6178(0.6473) Steps 628(624.84) | Grad Norm 5.9242(12.2271) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0006 | Time 96.7139, Epoch Time 1511.8249(1354.7020), Bit/dim 5.5562(best: 5.9542), Xent 1.8818, Loss 6.4970, Error 0.6472(best: 0.6344)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0340 | Time 25.8345(25.8989) | Bit/dim 5.5106(5.6423) | Xent 1.8265(1.8830) | Loss 6.4239(6.5838) | Error 0.6322(0.6382) Steps 622(624.27) | Grad Norm 9.9641(10.6230) | Total Time 14.00(14.00)\n",
      "Iter 0350 | Time 26.3065(26.0354) | Bit/dim 5.5791(5.6175) | Xent 1.9298(1.9003) | Loss 6.5441(6.5677) | Error 0.6267(0.6460) Steps 628(624.93) | Grad Norm 5.4867(13.9256) | Total Time 14.00(14.00)\n",
      "Iter 0360 | Time 24.1746(25.6116) | Bit/dim 5.5109(5.5891) | Xent 1.8554(1.8911) | Loss 6.4386(6.5346) | Error 0.6333(0.6419) Steps 622(624.03) | Grad Norm 4.7066(11.5859) | Total Time 14.00(14.00)\n",
      "Iter 0370 | Time 25.8080(25.4554) | Bit/dim 5.4533(5.5605) | Xent 1.8373(1.8765) | Loss 6.3719(6.4988) | Error 0.6089(0.6336) Steps 622(623.49) | Grad Norm 5.9166(9.6648) | Total Time 14.00(14.00)\n",
      "Iter 0380 | Time 26.6292(25.4187) | Bit/dim 5.4304(5.5321) | Xent 1.8000(1.8616) | Loss 6.3304(6.4629) | Error 0.6022(0.6278) Steps 622(623.10) | Grad Norm 3.3964(8.3653) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0007 | Time 97.0524, Epoch Time 1510.6770(1359.3812), Bit/dim 5.4375(best: 5.5562), Xent 1.8607, Loss 6.3679, Error 0.6306(best: 0.6344)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0390 | Time 25.6688(25.2027) | Bit/dim 5.4243(5.5077) | Xent 1.7841(1.8435) | Loss 6.3164(6.4295) | Error 0.5756(0.6214) Steps 622(622.81) | Grad Norm 2.5042(7.4825) | Total Time 14.00(14.00)\n",
      "Iter 0400 | Time 25.4418(25.1611) | Bit/dim 5.3640(5.4834) | Xent 1.7943(1.8240) | Loss 6.2612(6.3954) | Error 0.6144(0.6127) Steps 622(622.44) | Grad Norm 13.8571(7.3125) | Total Time 14.00(14.00)\n",
      "Iter 0410 | Time 25.9531(25.3773) | Bit/dim 5.3673(5.4584) | Xent 1.8202(1.8140) | Loss 6.2774(6.3654) | Error 0.6444(0.6114) Steps 628(623.25) | Grad Norm 6.4394(8.0662) | Total Time 14.00(14.00)\n",
      "Iter 0420 | Time 25.3100(25.4936) | Bit/dim 5.4182(5.4333) | Xent 1.8128(1.8112) | Loss 6.3246(6.3389) | Error 0.6222(0.6103) Steps 634(625.36) | Grad Norm 3.7517(8.7663) | Total Time 14.00(14.00)\n",
      "Iter 0430 | Time 25.8830(25.5643) | Bit/dim 5.3089(5.4002) | Xent 1.8025(1.8046) | Loss 6.2101(6.3025) | Error 0.6156(0.6087) Steps 634(626.71) | Grad Norm 9.8067(8.8396) | Total Time 14.00(14.00)\n",
      "Iter 0440 | Time 24.7302(25.4270) | Bit/dim 5.2692(5.3680) | Xent 1.7814(1.7993) | Loss 6.1599(6.2676) | Error 0.6033(0.6077) Steps 622(626.05) | Grad Norm 11.5710(8.3405) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0008 | Time 97.7595, Epoch Time 1517.1262(1364.1136), Bit/dim 5.2586(best: 5.4375), Xent 1.8401, Loss 6.1786, Error 0.6352(best: 0.6306)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0450 | Time 25.6520(25.3354) | Bit/dim 5.2189(5.3351) | Xent 1.7263(1.7822) | Loss 6.0821(6.2262) | Error 0.5867(0.6002) Steps 628(625.33) | Grad Norm 14.4436(8.3206) | Total Time 14.00(14.00)\n",
      "Iter 0460 | Time 25.7038(25.3353) | Bit/dim 5.1696(5.3010) | Xent 1.7355(1.7754) | Loss 6.0374(6.1887) | Error 0.5889(0.6010) Steps 640(627.52) | Grad Norm 12.3447(10.6015) | Total Time 14.00(14.00)\n",
      "Iter 0470 | Time 25.0136(25.4189) | Bit/dim 5.1594(5.2651) | Xent 1.7582(1.7704) | Loss 6.0385(6.1503) | Error 0.6100(0.6004) Steps 646(630.51) | Grad Norm 7.4220(11.2257) | Total Time 14.00(14.00)\n",
      "Iter 0480 | Time 24.6413(25.3693) | Bit/dim 5.0963(5.2250) | Xent 1.7551(1.7639) | Loss 5.9739(6.1070) | Error 0.5822(0.5974) Steps 628(631.32) | Grad Norm 2.3612(10.0883) | Total Time 14.00(14.00)\n",
      "Iter 0490 | Time 26.6585(25.5672) | Bit/dim 5.0349(5.1846) | Xent 1.6760(1.7548) | Loss 5.8729(6.0620) | Error 0.5811(0.5953) Steps 634(632.23) | Grad Norm 13.9186(9.7134) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0009 | Time 101.4538, Epoch Time 1526.9189(1368.9977), Bit/dim 5.0456(best: 5.2586), Xent 1.8119, Loss 5.9515, Error 0.6302(best: 0.6306)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0500 | Time 27.3324(25.8207) | Bit/dim 5.0253(5.1512) | Xent 1.6096(1.7396) | Loss 5.8301(6.0210) | Error 0.5489(0.5902) Steps 652(635.73) | Grad Norm 4.6056(9.1467) | Total Time 14.00(14.00)\n",
      "Iter 0510 | Time 27.4593(26.2725) | Bit/dim 5.1070(5.1181) | Xent 1.8002(1.7300) | Loss 6.0070(5.9831) | Error 0.6333(0.5900) Steps 652(639.82) | Grad Norm 53.7517(11.3555) | Total Time 14.00(14.00)\n",
      "Iter 0520 | Time 26.9537(26.5317) | Bit/dim 5.0079(5.1010) | Xent 1.6914(1.7331) | Loss 5.8536(5.9675) | Error 0.5756(0.5911) Steps 646(643.85) | Grad Norm 7.1347(12.5256) | Total Time 14.00(14.00)\n",
      "Iter 0530 | Time 29.8747(27.0547) | Bit/dim 4.9538(5.0665) | Xent 1.7125(1.7297) | Loss 5.8100(5.9314) | Error 0.5822(0.5914) Steps 676(652.68) | Grad Norm 6.5249(12.1046) | Total Time 14.00(14.00)\n",
      "Iter 0540 | Time 28.4089(27.2251) | Bit/dim 4.8794(5.0294) | Xent 1.6941(1.7221) | Loss 5.7265(5.8905) | Error 0.5933(0.5908) Steps 676(657.55) | Grad Norm 8.5635(10.9026) | Total Time 14.00(14.00)\n",
      "Iter 0550 | Time 27.0923(27.2177) | Bit/dim 4.8545(4.9961) | Xent 1.6337(1.7066) | Loss 5.6713(5.8494) | Error 0.5533(0.5871) Steps 658(659.35) | Grad Norm 14.6881(10.0263) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0010 | Time 105.0325, Epoch Time 1638.5936(1377.0856), Bit/dim 4.8944(best: 5.0456), Xent 1.7830, Loss 5.7859, Error 0.6181(best: 0.6302)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0560 | Time 27.4415(27.2463) | Bit/dim 4.9187(4.9650) | Xent 1.7662(1.6879) | Loss 5.8018(5.8089) | Error 0.6222(0.5817) Steps 676(659.88) | Grad Norm 29.5946(10.7882) | Total Time 14.00(14.00)\n",
      "Iter 0570 | Time 27.3707(27.3659) | Bit/dim 4.8679(4.9459) | Xent 1.7100(1.6920) | Loss 5.7229(5.7919) | Error 0.5767(0.5847) Steps 664(663.30) | Grad Norm 19.2137(12.8169) | Total Time 14.00(14.00)\n",
      "Iter 0580 | Time 28.3467(27.5357) | Bit/dim 4.8407(4.9182) | Xent 1.6926(1.6829) | Loss 5.6870(5.7597) | Error 0.5878(0.5812) Steps 676(666.91) | Grad Norm 23.2612(12.1264) | Total Time 14.00(14.00)\n",
      "Iter 0590 | Time 28.3517(27.7828) | Bit/dim 4.7241(4.8830) | Xent 1.6226(1.6755) | Loss 5.5354(5.7207) | Error 0.5500(0.5782) Steps 670(668.89) | Grad Norm 5.0530(12.1039) | Total Time 14.00(14.00)\n",
      "Iter 0600 | Time 27.1877(27.7814) | Bit/dim 4.7702(4.8590) | Xent 1.6697(1.6755) | Loss 5.6051(5.6967) | Error 0.5889(0.5804) Steps 682(671.78) | Grad Norm 17.4172(12.3432) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0011 | Time 110.4191, Epoch Time 1661.0901(1385.6057), Bit/dim 4.7577(best: 4.8944), Xent 1.7642, Loss 5.6398, Error 0.6158(best: 0.6181)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0610 | Time 27.8147(27.8979) | Bit/dim 4.7016(4.8329) | Xent 1.6507(1.6701) | Loss 5.5270(5.6679) | Error 0.5622(0.5794) Steps 688(675.69) | Grad Norm 8.7683(12.1125) | Total Time 14.00(14.00)\n",
      "Iter 0620 | Time 28.3384(28.0766) | Bit/dim 4.7353(4.8060) | Xent 1.6731(1.6471) | Loss 5.5718(5.6295) | Error 0.5778(0.5705) Steps 700(679.24) | Grad Norm 20.5141(12.0385) | Total Time 14.00(14.00)\n",
      "Iter 0630 | Time 30.0461(28.2311) | Bit/dim 4.7086(4.7845) | Xent 1.5781(1.6334) | Loss 5.4976(5.6012) | Error 0.5567(0.5672) Steps 718(682.40) | Grad Norm 4.8180(11.9288) | Total Time 14.00(14.00)\n",
      "Iter 0640 | Time 28.5391(28.4900) | Bit/dim 4.6924(4.7567) | Xent 1.5852(1.6296) | Loss 5.4851(5.5715) | Error 0.5500(0.5670) Steps 700(685.64) | Grad Norm 5.7584(11.2180) | Total Time 14.00(14.00)\n",
      "Iter 0650 | Time 29.3941(28.7654) | Bit/dim 4.7038(4.7509) | Xent 1.6212(1.6545) | Loss 5.5144(5.5781) | Error 0.5722(0.5749) Steps 694(690.27) | Grad Norm 12.6295(14.1936) | Total Time 14.00(14.00)\n",
      "Iter 0660 | Time 29.0723(28.7457) | Bit/dim 4.6692(4.7295) | Xent 1.5865(1.6499) | Loss 5.4624(5.5545) | Error 0.5356(0.5755) Steps 688(690.85) | Grad Norm 3.3786(12.4858) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0012 | Time 110.9442, Epoch Time 1719.4878(1395.6222), Bit/dim 4.6545(best: 4.7577), Xent 1.7521, Loss 5.5306, Error 0.6096(best: 0.6158)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0670 | Time 28.5890(28.6762) | Bit/dim 4.6601(4.7085) | Xent 1.5176(1.6209) | Loss 5.4189(5.5189) | Error 0.5089(0.5640) Steps 700(689.28) | Grad Norm 10.1380(11.4925) | Total Time 14.00(14.00)\n",
      "Iter 0680 | Time 29.3977(28.8901) | Bit/dim 4.6130(4.6851) | Xent 1.5084(1.5956) | Loss 5.3673(5.4829) | Error 0.5400(0.5541) Steps 694(691.13) | Grad Norm 5.5578(10.3419) | Total Time 14.00(14.00)\n",
      "Iter 0690 | Time 29.3710(29.0680) | Bit/dim 4.6492(4.6677) | Xent 1.5440(1.5832) | Loss 5.4212(5.4593) | Error 0.5422(0.5509) Steps 712(695.35) | Grad Norm 12.2598(10.2756) | Total Time 14.00(14.00)\n",
      "Iter 0700 | Time 30.2224(29.1688) | Bit/dim 4.6313(4.6489) | Xent 1.6925(1.5882) | Loss 5.4775(5.4429) | Error 0.5911(0.5538) Steps 742(703.46) | Grad Norm 22.2858(11.2811) | Total Time 14.00(14.00)\n",
      "Iter 0710 | Time 29.4722(29.6858) | Bit/dim 4.5928(4.6373) | Xent 1.5998(1.5990) | Loss 5.3927(5.4368) | Error 0.5533(0.5574) Steps 730(712.04) | Grad Norm 13.7973(11.6313) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0013 | Time 112.9801, Epoch Time 1762.4732(1406.6277), Bit/dim 4.6027(best: 4.6545), Xent 1.6774, Loss 5.4414, Error 0.5939(best: 0.6096)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0720 | Time 30.7069(29.8174) | Bit/dim 4.5839(4.6261) | Xent 1.5020(1.5896) | Loss 5.3350(5.4209) | Error 0.5433(0.5552) Steps 718(713.85) | Grad Norm 7.9161(11.7810) | Total Time 14.00(14.00)\n",
      "Iter 0730 | Time 30.9632(30.1207) | Bit/dim 4.5951(4.6122) | Xent 1.5222(1.5675) | Loss 5.3561(5.3959) | Error 0.5167(0.5480) Steps 742(717.56) | Grad Norm 7.7768(11.2605) | Total Time 14.00(14.00)\n",
      "Iter 0740 | Time 30.6500(30.0963) | Bit/dim 4.5267(4.5983) | Xent 1.4979(1.5495) | Loss 5.2757(5.3730) | Error 0.5478(0.5421) Steps 742(721.40) | Grad Norm 9.9485(11.1228) | Total Time 14.00(14.00)\n",
      "Iter 0750 | Time 30.4659(30.3046) | Bit/dim 4.5339(4.5851) | Xent 1.4660(1.5384) | Loss 5.2669(5.3543) | Error 0.5422(0.5399) Steps 754(729.38) | Grad Norm 14.7076(11.3344) | Total Time 14.00(14.00)\n",
      "Iter 0760 | Time 32.0242(30.6083) | Bit/dim 4.5185(4.5722) | Xent 1.5040(1.5295) | Loss 5.2705(5.3370) | Error 0.5344(0.5361) Steps 760(737.83) | Grad Norm 4.0577(10.7745) | Total Time 14.00(14.00)\n",
      "Iter 0770 | Time 30.9424(30.7655) | Bit/dim 4.5195(4.5566) | Xent 1.5074(1.5204) | Loss 5.2732(5.3168) | Error 0.5489(0.5353) Steps 748(743.60) | Grad Norm 6.4908(9.8984) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0014 | Time 120.4680, Epoch Time 1838.2638(1419.5768), Bit/dim 4.5074(best: 4.6027), Xent 1.6235, Loss 5.3192, Error 0.5748(best: 0.5939)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0780 | Time 30.9101(30.9251) | Bit/dim 4.6350(4.5452) | Xent 1.5763(1.5014) | Loss 5.4232(5.2959) | Error 0.5467(0.5290) Steps 766(748.35) | Grad Norm 38.7412(11.1719) | Total Time 14.00(14.00)\n",
      "Iter 0790 | Time 32.3255(30.9329) | Bit/dim 4.6673(4.5729) | Xent 2.0253(1.5924) | Loss 5.6800(5.3691) | Error 0.6600(0.5528) Steps 802(757.35) | Grad Norm 19.4372(13.6171) | Total Time 14.00(14.00)\n",
      "Iter 0800 | Time 32.3423(31.2255) | Bit/dim 4.5531(4.5879) | Xent 1.6288(1.6102) | Loss 5.3675(5.3930) | Error 0.5856(0.5609) Steps 796(769.34) | Grad Norm 5.6713(11.9710) | Total Time 14.00(14.00)\n",
      "Iter 0810 | Time 30.9556(31.1380) | Bit/dim 4.5521(4.5782) | Xent 1.5259(1.5979) | Loss 5.3151(5.3771) | Error 0.5444(0.5593) Steps 754(770.71) | Grad Norm 5.1719(10.1526) | Total Time 14.00(14.00)\n",
      "Iter 0820 | Time 30.1876(31.3155) | Bit/dim 4.4930(4.5574) | Xent 1.4958(1.5802) | Loss 5.2409(5.3475) | Error 0.5344(0.5531) Steps 754(766.15) | Grad Norm 8.7574(8.9908) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0015 | Time 115.4591, Epoch Time 1858.6093(1432.7478), Bit/dim 4.4697(best: 4.5074), Xent 1.6395, Loss 5.2895, Error 0.5855(best: 0.5748)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0830 | Time 30.8430(31.1044) | Bit/dim 4.4484(4.5341) | Xent 1.4379(1.5495) | Loss 5.1674(5.3089) | Error 0.5078(0.5449) Steps 772(765.01) | Grad Norm 3.5939(7.7683) | Total Time 14.00(14.00)\n",
      "Iter 0840 | Time 30.5391(31.0967) | Bit/dim 4.4384(4.5119) | Xent 1.3845(1.5079) | Loss 5.1306(5.2659) | Error 0.4844(0.5297) Steps 754(767.94) | Grad Norm 6.0743(7.0237) | Total Time 14.00(14.00)\n",
      "Iter 0850 | Time 31.5184(31.1690) | Bit/dim 4.4627(4.4962) | Xent 1.4946(1.4940) | Loss 5.2101(5.2432) | Error 0.5300(0.5246) Steps 778(768.95) | Grad Norm 9.1330(7.8817) | Total Time 14.00(14.00)\n",
      "Iter 0860 | Time 31.3038(31.1398) | Bit/dim 4.4680(4.4813) | Xent 1.4266(1.4827) | Loss 5.1813(5.2227) | Error 0.4967(0.5209) Steps 778(770.60) | Grad Norm 6.2930(7.4064) | Total Time 14.00(14.00)\n",
      "Iter 0870 | Time 30.8345(31.1714) | Bit/dim 4.4139(4.4641) | Xent 1.4235(1.4700) | Loss 5.1256(5.1991) | Error 0.4933(0.5166) Steps 772(770.19) | Grad Norm 5.3622(6.7859) | Total Time 14.00(14.00)\n",
      "Iter 0880 | Time 31.7603(31.1810) | Bit/dim 4.3925(4.4468) | Xent 1.4213(1.4556) | Loss 5.1031(5.1746) | Error 0.5067(0.5130) Steps 772(773.91) | Grad Norm 6.1739(6.8418) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0016 | Time 119.0372, Epoch Time 1850.4889(1445.2800), Bit/dim 4.3992(best: 4.4697), Xent 1.5647, Loss 5.1815, Error 0.5568(best: 0.5748)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0890 | Time 31.4072(31.1670) | Bit/dim 4.4073(4.4348) | Xent 1.4046(1.4496) | Loss 5.1096(5.1596) | Error 0.4889(0.5110) Steps 772(776.14) | Grad Norm 15.3053(8.8448) | Total Time 14.00(14.00)\n",
      "Iter 0900 | Time 31.1547(31.1339) | Bit/dim 4.3954(4.4290) | Xent 1.4400(1.4375) | Loss 5.1154(5.1478) | Error 0.5011(0.5069) Steps 784(780.76) | Grad Norm 9.6706(8.7296) | Total Time 14.00(14.00)\n",
      "Iter 0910 | Time 29.6285(31.0744) | Bit/dim 4.3778(4.4193) | Xent 1.3733(1.4216) | Loss 5.0644(5.1301) | Error 0.4867(0.5004) Steps 772(780.31) | Grad Norm 3.4554(8.1658) | Total Time 14.00(14.00)\n",
      "Iter 0920 | Time 30.6584(30.9632) | Bit/dim 4.3685(4.4068) | Xent 1.3594(1.4103) | Loss 5.0482(5.1120) | Error 0.4644(0.4956) Steps 790(780.92) | Grad Norm 6.9033(7.6617) | Total Time 14.00(14.00)\n",
      "Iter 0930 | Time 30.3468(31.0033) | Bit/dim 4.3633(4.3936) | Xent 1.3652(1.3969) | Loss 5.0459(5.0920) | Error 0.4911(0.4919) Steps 796(780.84) | Grad Norm 6.4405(7.3739) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0017 | Time 118.1423, Epoch Time 1839.7310(1457.1136), Bit/dim 4.3738(best: 4.3992), Xent 1.6088, Loss 5.1782, Error 0.5593(best: 0.5568)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0940 | Time 31.2253(31.0640) | Bit/dim 4.4014(4.3896) | Xent 1.3680(1.3997) | Loss 5.0854(5.0895) | Error 0.4800(0.4914) Steps 760(782.13) | Grad Norm 9.9600(9.1329) | Total Time 14.00(14.00)\n",
      "Iter 0950 | Time 30.9617(31.2843) | Bit/dim 4.3538(4.3805) | Xent 1.3413(1.3794) | Loss 5.0244(5.0703) | Error 0.4711(0.4852) Steps 790(786.17) | Grad Norm 5.0433(8.1668) | Total Time 14.00(14.00)\n",
      "Iter 0960 | Time 33.2366(31.4842) | Bit/dim 4.3571(4.3707) | Xent 1.3291(1.3591) | Loss 5.0217(5.0503) | Error 0.4522(0.4784) Steps 802(792.42) | Grad Norm 5.5374(7.6682) | Total Time 14.00(14.00)\n",
      "Iter 0970 | Time 32.6576(31.5359) | Bit/dim 4.3279(4.3573) | Xent 1.4143(1.3610) | Loss 5.0351(5.0378) | Error 0.4989(0.4799) Steps 814(794.89) | Grad Norm 17.2206(8.3981) | Total Time 14.00(14.00)\n",
      "Iter 0980 | Time 33.6103(31.7953) | Bit/dim 4.3364(4.3494) | Xent 1.3926(1.3726) | Loss 5.0327(5.0357) | Error 0.4844(0.4826) Steps 802(799.39) | Grad Norm 5.5602(8.8771) | Total Time 14.00(14.00)\n",
      "Iter 0990 | Time 32.1645(31.7790) | Bit/dim 4.2890(4.3387) | Xent 1.3764(1.3738) | Loss 4.9772(5.0256) | Error 0.5044(0.4845) Steps 790(800.48) | Grad Norm 2.7507(7.9642) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0018 | Time 120.1684, Epoch Time 1896.1446(1470.2845), Bit/dim 4.3041(best: 4.3738), Xent 1.5231, Loss 5.0656, Error 0.5266(best: 0.5568)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1000 | Time 32.1452(31.8735) | Bit/dim 4.3216(4.3334) | Xent 1.2905(1.3506) | Loss 4.9668(5.0087) | Error 0.4489(0.4759) Steps 820(802.48) | Grad Norm 8.4544(8.4676) | Total Time 14.00(14.00)\n",
      "Iter 1010 | Time 31.9503(32.0859) | Bit/dim 4.2778(4.3247) | Xent 1.2746(1.3378) | Loss 4.9151(4.9936) | Error 0.4467(0.4707) Steps 790(800.71) | Grad Norm 4.5892(8.0315) | Total Time 14.00(14.00)\n",
      "Iter 1020 | Time 33.1222(32.3771) | Bit/dim 4.2751(4.3128) | Xent 1.2361(1.3196) | Loss 4.8932(4.9726) | Error 0.4611(0.4646) Steps 850(806.73) | Grad Norm 6.2504(7.5448) | Total Time 14.00(14.00)\n",
      "Iter 1030 | Time 32.1955(32.5142) | Bit/dim 4.2844(4.3068) | Xent 1.4136(1.3272) | Loss 4.9912(4.9704) | Error 0.5056(0.4670) Steps 838(810.05) | Grad Norm 10.7751(8.3800) | Total Time 14.00(14.00)\n",
      "Iter 1040 | Time 32.9694(32.8102) | Bit/dim 4.2392(4.2975) | Xent 1.2896(1.3243) | Loss 4.8840(4.9596) | Error 0.4567(0.4663) Steps 820(816.84) | Grad Norm 6.7243(7.9496) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0019 | Time 123.9130, Epoch Time 1955.1533(1484.8305), Bit/dim 4.2578(best: 4.3041), Xent 1.4534, Loss 4.9845, Error 0.5080(best: 0.5266)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1050 | Time 32.4171(32.8916) | Bit/dim 4.2097(4.2847) | Xent 1.1806(1.3038) | Loss 4.8000(4.9367) | Error 0.4233(0.4592) Steps 826(819.32) | Grad Norm 7.6568(7.6518) | Total Time 14.00(14.00)\n",
      "Iter 1060 | Time 31.9500(32.8363) | Bit/dim 4.2579(4.2781) | Xent 1.1668(1.2857) | Loss 4.8413(4.9209) | Error 0.4100(0.4531) Steps 838(819.52) | Grad Norm 6.3719(7.9416) | Total Time 14.00(14.00)\n",
      "Iter 1070 | Time 32.3341(32.9285) | Bit/dim 4.2263(4.2665) | Xent 1.2448(1.2628) | Loss 4.8487(4.8979) | Error 0.4356(0.4442) Steps 844(822.23) | Grad Norm 6.3212(7.3800) | Total Time 14.00(14.00)\n",
      "Iter 1080 | Time 34.0222(32.9475) | Bit/dim 4.2459(4.2578) | Xent 1.2695(1.2584) | Loss 4.8807(4.8870) | Error 0.4500(0.4431) Steps 838(823.16) | Grad Norm 10.4121(7.4500) | Total Time 14.00(14.00)\n",
      "Iter 1090 | Time 35.2873(33.1363) | Bit/dim 4.2573(4.2565) | Xent 1.2667(1.2891) | Loss 4.8906(4.9010) | Error 0.4433(0.4538) Steps 832(828.11) | Grad Norm 5.9283(8.9556) | Total Time 14.00(14.00)\n",
      "Iter 1100 | Time 33.7579(33.3066) | Bit/dim 4.2080(4.2485) | Xent 1.2342(1.2878) | Loss 4.8251(4.8923) | Error 0.4344(0.4537) Steps 820(825.43) | Grad Norm 3.2902(8.2075) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0020 | Time 119.4321, Epoch Time 1966.0662(1499.2676), Bit/dim 4.2187(best: 4.2578), Xent 1.4272, Loss 4.9324, Error 0.4948(best: 0.5080)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1110 | Time 34.6958(33.4731) | Bit/dim 4.1718(4.2376) | Xent 1.1262(1.2546) | Loss 4.7349(4.8649) | Error 0.3967(0.4419) Steps 820(823.67) | Grad Norm 3.9255(7.4178) | Total Time 14.00(14.00)\n",
      "Iter 1120 | Time 34.3703(33.4920) | Bit/dim 4.1558(4.2250) | Xent 1.1054(1.2301) | Loss 4.7085(4.8400) | Error 0.4022(0.4336) Steps 826(823.76) | Grad Norm 5.5774(7.2371) | Total Time 14.00(14.00)\n",
      "Iter 1130 | Time 34.1224(33.5905) | Bit/dim 4.1490(4.2133) | Xent 1.1492(1.2093) | Loss 4.7236(4.8180) | Error 0.4111(0.4241) Steps 808(820.37) | Grad Norm 3.5431(7.1137) | Total Time 14.00(14.00)\n",
      "Iter 1140 | Time 36.0096(33.8052) | Bit/dim 4.2180(4.2101) | Xent 1.2331(1.2209) | Loss 4.8345(4.8206) | Error 0.4489(0.4295) Steps 832(821.44) | Grad Norm 5.3793(7.5411) | Total Time 14.00(14.00)\n",
      "Iter 1150 | Time 33.7885(33.9355) | Bit/dim 4.1812(4.2041) | Xent 1.2407(1.2231) | Loss 4.8015(4.8157) | Error 0.4278(0.4303) Steps 820(822.69) | Grad Norm 7.9174(7.6225) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0021 | Time 123.5633, Epoch Time 2013.3410(1514.6898), Bit/dim 4.1830(best: 4.2187), Xent 1.4193, Loss 4.8926, Error 0.4844(best: 0.4948)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1160 | Time 35.4077(34.0941) | Bit/dim 4.1647(4.1974) | Xent 1.1330(1.2056) | Loss 4.7312(4.8003) | Error 0.4289(0.4246) Steps 850(825.59) | Grad Norm 2.2898(7.0977) | Total Time 14.00(14.00)\n",
      "Iter 1170 | Time 34.7129(34.1077) | Bit/dim 4.1565(4.1903) | Xent 1.1924(1.1939) | Loss 4.7528(4.7872) | Error 0.4178(0.4193) Steps 832(829.20) | Grad Norm 11.0480(8.0644) | Total Time 14.00(14.00)\n",
      "Iter 1180 | Time 36.1119(34.4072) | Bit/dim 4.1660(4.1817) | Xent 1.0997(1.1798) | Loss 4.7158(4.7716) | Error 0.3856(0.4139) Steps 832(832.13) | Grad Norm 5.3425(7.9636) | Total Time 14.00(14.00)\n",
      "Iter 1190 | Time 35.5967(34.3815) | Bit/dim 4.1213(4.1719) | Xent 1.1668(1.1651) | Loss 4.7047(4.7544) | Error 0.3978(0.4084) Steps 826(832.45) | Grad Norm 6.5917(7.6808) | Total Time 14.00(14.00)\n",
      "Iter 1200 | Time 34.7566(34.3419) | Bit/dim 4.1371(4.1668) | Xent 1.2885(1.1596) | Loss 4.7814(4.7466) | Error 0.4211(0.4058) Steps 838(833.55) | Grad Norm 7.8500(7.2372) | Total Time 14.00(14.00)\n",
      "Iter 1210 | Time 33.0454(34.2668) | Bit/dim 4.1213(4.1651) | Xent 1.2125(1.1753) | Loss 4.7276(4.7528) | Error 0.4222(0.4119) Steps 820(830.80) | Grad Norm 4.6043(7.5279) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0022 | Time 122.5115, Epoch Time 2035.3434(1530.3094), Bit/dim 4.1418(best: 4.1830), Xent 1.3891, Loss 4.8363, Error 0.4878(best: 0.4844)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1220 | Time 33.4532(33.9855) | Bit/dim 4.1400(4.1587) | Xent 1.1203(1.1550) | Loss 4.7001(4.7362) | Error 0.3822(0.4043) Steps 838(827.97) | Grad Norm 5.0597(7.4492) | Total Time 14.00(14.00)\n",
      "Iter 1230 | Time 33.5045(34.0673) | Bit/dim 4.1277(4.1531) | Xent 1.0740(1.1352) | Loss 4.6647(4.7207) | Error 0.3633(0.3966) Steps 814(834.44) | Grad Norm 12.6676(7.6154) | Total Time 14.00(14.00)\n",
      "Iter 1240 | Time 33.2652(34.0804) | Bit/dim 4.1610(4.1472) | Xent 1.0037(1.1239) | Loss 4.6628(4.7092) | Error 0.3544(0.3918) Steps 838(838.34) | Grad Norm 6.5744(7.3807) | Total Time 14.00(14.00)\n",
      "Iter 1250 | Time 32.9948(34.0379) | Bit/dim 4.1277(4.1356) | Xent 1.0327(1.1092) | Loss 4.6440(4.6902) | Error 0.3767(0.3880) Steps 820(837.39) | Grad Norm 5.7092(6.7019) | Total Time 14.00(14.00)\n",
      "Iter 1260 | Time 33.4638(33.9926) | Bit/dim 4.1364(4.1280) | Xent 1.2053(1.1158) | Loss 4.7390(4.6859) | Error 0.4156(0.3903) Steps 838(832.57) | Grad Norm 10.6142(7.0422) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0023 | Time 126.8034, Epoch Time 2011.6440(1544.7495), Bit/dim 4.1086(best: 4.1418), Xent 1.3224, Loss 4.7698, Error 0.4688(best: 0.4844)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1270 | Time 35.2259(34.0875) | Bit/dim 4.1550(4.1214) | Xent 1.0414(1.1135) | Loss 4.6757(4.6781) | Error 0.3633(0.3893) Steps 850(836.78) | Grad Norm 8.1507(7.3235) | Total Time 14.00(14.00)\n",
      "Iter 1280 | Time 34.4840(34.1686) | Bit/dim 4.1045(4.1133) | Xent 0.9798(1.0975) | Loss 4.5945(4.6621) | Error 0.3367(0.3827) Steps 844(838.86) | Grad Norm 5.6285(7.6026) | Total Time 14.00(14.00)\n",
      "Iter 1290 | Time 33.8281(34.0411) | Bit/dim 4.0715(4.1041) | Xent 1.0062(1.0831) | Loss 4.5746(4.6456) | Error 0.3733(0.3776) Steps 826(836.13) | Grad Norm 4.9486(7.3131) | Total Time 14.00(14.00)\n",
      "Iter 1300 | Time 36.5955(34.1628) | Bit/dim 4.1263(4.1017) | Xent 1.0749(1.0716) | Loss 4.6637(4.6375) | Error 0.3678(0.3740) Steps 856(838.66) | Grad Norm 9.3191(7.0314) | Total Time 14.00(14.00)\n",
      "Iter 1310 | Time 34.8898(34.3119) | Bit/dim 4.1032(4.0945) | Xent 1.0316(1.0738) | Loss 4.6190(4.6314) | Error 0.3667(0.3755) Steps 850(841.22) | Grad Norm 7.5772(7.2721) | Total Time 14.00(14.00)\n",
      "Iter 1320 | Time 33.9918(34.4194) | Bit/dim 4.1093(4.0920) | Xent 1.1335(1.0710) | Loss 4.6760(4.6275) | Error 0.4122(0.3758) Steps 838(841.77) | Grad Norm 4.7692(6.9503) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0024 | Time 124.9708, Epoch Time 2035.9513(1559.4855), Bit/dim 4.0668(best: 4.1086), Xent 1.2498, Loss 4.6917, Error 0.4379(best: 0.4688)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1330 | Time 35.5750(34.3130) | Bit/dim 4.0621(4.0854) | Xent 1.0018(1.0484) | Loss 4.5630(4.6096) | Error 0.3311(0.3680) Steps 850(840.05) | Grad Norm 7.6394(6.5777) | Total Time 14.00(14.00)\n",
      "Iter 1340 | Time 33.0465(34.1556) | Bit/dim 4.0704(4.0807) | Xent 1.0631(1.0356) | Loss 4.6019(4.5985) | Error 0.3733(0.3624) Steps 844(840.10) | Grad Norm 6.7428(7.1773) | Total Time 14.00(14.00)\n",
      "Iter 1350 | Time 32.6271(34.1418) | Bit/dim 4.0524(4.0762) | Xent 0.9521(1.0300) | Loss 4.5284(4.5912) | Error 0.3467(0.3600) Steps 826(839.78) | Grad Norm 4.5101(6.9920) | Total Time 14.00(14.00)\n",
      "Iter 1360 | Time 34.9504(34.1759) | Bit/dim 4.0956(4.0738) | Xent 1.0575(1.0423) | Loss 4.6244(4.5950) | Error 0.3656(0.3639) Steps 850(841.49) | Grad Norm 8.7132(7.7206) | Total Time 14.00(14.00)\n",
      "Iter 1370 | Time 32.8732(34.2057) | Bit/dim 4.0839(4.0725) | Xent 0.9542(1.0457) | Loss 4.5610(4.5953) | Error 0.3456(0.3664) Steps 814(839.85) | Grad Norm 4.0340(7.4254) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0025 | Time 122.1622, Epoch Time 2010.9191(1573.0285), Bit/dim 4.0585(best: 4.0668), Xent 1.2596, Loss 4.6883, Error 0.4425(best: 0.4379)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1380 | Time 32.6906(33.9250) | Bit/dim 4.0677(4.0678) | Xent 0.9958(1.0316) | Loss 4.5656(4.5836) | Error 0.3500(0.3617) Steps 802(834.15) | Grad Norm 5.2502(6.9387) | Total Time 14.00(14.00)\n",
      "Iter 1390 | Time 34.7654(33.7562) | Bit/dim 4.0417(4.0623) | Xent 1.0272(1.0177) | Loss 4.5553(4.5711) | Error 0.3578(0.3562) Steps 856(833.11) | Grad Norm 11.4478(7.3853) | Total Time 14.00(14.00)\n",
      "Iter 1400 | Time 33.3164(33.8883) | Bit/dim 4.0330(4.0555) | Xent 1.0075(1.0107) | Loss 4.5368(4.5608) | Error 0.3500(0.3527) Steps 820(835.11) | Grad Norm 3.5803(7.0253) | Total Time 14.00(14.00)\n",
      "Iter 1410 | Time 35.3851(34.0001) | Bit/dim 4.0320(4.0543) | Xent 0.9938(1.0068) | Loss 4.5290(4.5577) | Error 0.3633(0.3527) Steps 838(832.76) | Grad Norm 8.1104(6.8241) | Total Time 14.00(14.00)\n",
      "Iter 1420 | Time 34.2057(34.1750) | Bit/dim 4.0612(4.0480) | Xent 1.0555(1.0105) | Loss 4.5890(4.5532) | Error 0.3722(0.3545) Steps 820(833.69) | Grad Norm 12.1608(7.3785) | Total Time 14.00(14.00)\n",
      "Iter 1430 | Time 33.8443(34.2630) | Bit/dim 4.0741(4.0477) | Xent 1.0017(1.0156) | Loss 4.5749(4.5555) | Error 0.3478(0.3563) Steps 844(833.93) | Grad Norm 5.0610(7.3145) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0026 | Time 125.4333, Epoch Time 2020.0039(1586.4378), Bit/dim 4.0337(best: 4.0585), Xent 1.2479, Loss 4.6576, Error 0.4257(best: 0.4379)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1440 | Time 33.0833(33.9788) | Bit/dim 4.0421(4.0433) | Xent 0.9413(0.9978) | Loss 4.5128(4.5422) | Error 0.3267(0.3501) Steps 820(832.26) | Grad Norm 5.6657(6.9155) | Total Time 14.00(14.00)\n",
      "Iter 1450 | Time 33.6478(33.7349) | Bit/dim 4.0015(4.0368) | Xent 0.9050(0.9811) | Loss 4.4539(4.5274) | Error 0.3078(0.3434) Steps 808(827.21) | Grad Norm 5.7383(6.5213) | Total Time 14.00(14.00)\n",
      "Iter 1460 | Time 31.9933(33.6924) | Bit/dim 4.0243(4.0317) | Xent 0.9461(0.9720) | Loss 4.4973(4.5177) | Error 0.3022(0.3398) Steps 820(825.47) | Grad Norm 7.0476(6.3500) | Total Time 14.00(14.00)\n",
      "Iter 1470 | Time 33.1532(33.7191) | Bit/dim 3.9975(4.0240) | Xent 0.9613(0.9750) | Loss 4.4781(4.5115) | Error 0.3278(0.3400) Steps 832(828.21) | Grad Norm 8.2972(6.7888) | Total Time 14.00(14.00)\n",
      "Iter 1480 | Time 33.0443(33.8525) | Bit/dim 4.0503(4.0250) | Xent 1.0982(0.9909) | Loss 4.5994(4.5205) | Error 0.3944(0.3458) Steps 826(828.18) | Grad Norm 9.3066(7.9304) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0027 | Time 122.6810, Epoch Time 1988.2609(1598.4925), Bit/dim 4.0216(best: 4.0337), Xent 1.2377, Loss 4.6404, Error 0.4359(best: 0.4257)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1490 | Time 33.6948(33.7223) | Bit/dim 4.0263(4.0278) | Xent 0.9767(0.9885) | Loss 4.5146(4.5220) | Error 0.3478(0.3465) Steps 820(825.24) | Grad Norm 8.6070(7.8019) | Total Time 14.00(14.00)\n",
      "Iter 1500 | Time 34.7203(33.8264) | Bit/dim 4.0304(4.0243) | Xent 0.9128(0.9670) | Loss 4.4868(4.5078) | Error 0.3078(0.3402) Steps 808(823.40) | Grad Norm 8.0252(7.4378) | Total Time 14.00(14.00)\n",
      "Iter 1510 | Time 34.0413(33.8102) | Bit/dim 3.9960(4.0159) | Xent 0.9356(0.9586) | Loss 4.4638(4.4952) | Error 0.3244(0.3359) Steps 832(824.71) | Grad Norm 4.3364(6.5815) | Total Time 14.00(14.00)\n",
      "Iter 1520 | Time 33.5805(33.6883) | Bit/dim 4.0190(4.0127) | Xent 0.8933(0.9448) | Loss 4.4657(4.4851) | Error 0.3078(0.3305) Steps 814(824.25) | Grad Norm 4.5047(5.9655) | Total Time 14.00(14.00)\n",
      "Iter 1530 | Time 32.9489(33.7845) | Bit/dim 3.9947(4.0070) | Xent 1.0973(0.9465) | Loss 4.5433(4.4803) | Error 0.3633(0.3296) Steps 814(825.82) | Grad Norm 14.5531(6.4920) | Total Time 14.00(14.00)\n",
      "Iter 1540 | Time 34.0463(33.9211) | Bit/dim 3.9787(4.0047) | Xent 0.9858(0.9534) | Loss 4.4716(4.4814) | Error 0.3400(0.3328) Steps 850(830.68) | Grad Norm 7.6936(6.5286) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0028 | Time 123.4875, Epoch Time 2004.7464(1610.6801), Bit/dim 3.9949(best: 4.0216), Xent 1.2081, Loss 4.5989, Error 0.4201(best: 0.4257)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1550 | Time 32.3834(33.6779) | Bit/dim 3.9865(4.0002) | Xent 0.8611(0.9249) | Loss 4.4170(4.4626) | Error 0.2878(0.3227) Steps 820(827.21) | Grad Norm 3.8674(6.1041) | Total Time 14.00(14.00)\n",
      "Iter 1560 | Time 33.4868(33.6387) | Bit/dim 3.9977(3.9953) | Xent 0.9181(0.9130) | Loss 4.4568(4.4518) | Error 0.3067(0.3183) Steps 838(830.09) | Grad Norm 4.9516(5.6673) | Total Time 14.00(14.00)\n",
      "Iter 1570 | Time 35.0507(33.9132) | Bit/dim 3.9667(3.9912) | Xent 0.9066(0.9077) | Loss 4.4200(4.4451) | Error 0.3278(0.3187) Steps 850(833.63) | Grad Norm 5.8366(5.9332) | Total Time 14.00(14.00)\n",
      "Iter 1580 | Time 32.3622(33.8419) | Bit/dim 3.9259(3.9859) | Xent 0.9416(0.9059) | Loss 4.3967(4.4389) | Error 0.3322(0.3180) Steps 814(831.27) | Grad Norm 5.0043(5.8232) | Total Time 14.00(14.00)\n",
      "Iter 1590 | Time 32.9603(33.7706) | Bit/dim 4.0006(3.9854) | Xent 0.9464(0.9097) | Loss 4.4738(4.4402) | Error 0.3322(0.3212) Steps 796(827.28) | Grad Norm 4.8125(6.1618) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0029 | Time 123.5647, Epoch Time 1997.0198(1622.2703), Bit/dim 3.9750(best: 3.9949), Xent 1.1520, Loss 4.5510, Error 0.4023(best: 0.4201)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1600 | Time 31.9724(33.5777) | Bit/dim 3.9789(3.9819) | Xent 0.8308(0.9006) | Loss 4.3943(4.4322) | Error 0.2878(0.3174) Steps 814(823.35) | Grad Norm 4.3822(5.9379) | Total Time 14.00(14.00)\n",
      "Iter 1610 | Time 32.6227(33.5537) | Bit/dim 3.9632(3.9784) | Xent 0.8735(0.8799) | Loss 4.3999(4.4183) | Error 0.3067(0.3103) Steps 826(823.96) | Grad Norm 7.7027(5.9159) | Total Time 14.00(14.00)\n",
      "Iter 1620 | Time 33.1610(33.4204) | Bit/dim 3.9780(3.9753) | Xent 0.8560(0.8785) | Loss 4.4060(4.4145) | Error 0.3056(0.3096) Steps 832(824.20) | Grad Norm 7.0891(6.5056) | Total Time 14.00(14.00)\n",
      "Iter 1630 | Time 32.8465(33.2918) | Bit/dim 3.9656(3.9754) | Xent 0.9621(0.8843) | Loss 4.4467(4.4176) | Error 0.3200(0.3103) Steps 808(824.14) | Grad Norm 6.8560(6.6580) | Total Time 14.00(14.00)\n",
      "Iter 1640 | Time 35.4648(33.4561) | Bit/dim 3.9557(3.9763) | Xent 0.9146(0.8976) | Loss 4.4130(4.4251) | Error 0.3233(0.3159) Steps 832(824.53) | Grad Norm 6.3276(7.4633) | Total Time 14.00(14.00)\n",
      "Iter 1650 | Time 33.1635(33.4261) | Bit/dim 3.9595(3.9751) | Xent 0.9009(0.8947) | Loss 4.4100(4.4225) | Error 0.3322(0.3143) Steps 838(824.03) | Grad Norm 7.0381(7.1060) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0030 | Time 125.3442, Epoch Time 1972.9999(1632.7922), Bit/dim 3.9628(best: 3.9750), Xent 1.1671, Loss 4.5463, Error 0.4066(best: 0.4023)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1660 | Time 34.4109(33.4235) | Bit/dim 3.9631(3.9689) | Xent 0.7948(0.8681) | Loss 4.3605(4.4029) | Error 0.2689(0.3043) Steps 832(827.56) | Grad Norm 3.9294(6.6731) | Total Time 14.00(14.00)\n",
      "Iter 1670 | Time 32.5858(33.3627) | Bit/dim 3.9320(3.9643) | Xent 0.7981(0.8469) | Loss 4.3310(4.3877) | Error 0.2811(0.2964) Steps 826(828.37) | Grad Norm 5.9456(6.3377) | Total Time 14.00(14.00)\n",
      "Iter 1680 | Time 32.6819(33.5596) | Bit/dim 3.9617(3.9608) | Xent 0.8782(0.8405) | Loss 4.4007(4.3811) | Error 0.3011(0.2943) Steps 838(831.38) | Grad Norm 12.2581(6.3291) | Total Time 14.00(14.00)\n",
      "Iter 1690 | Time 34.9282(33.7334) | Bit/dim 3.9480(3.9583) | Xent 0.8325(0.8428) | Loss 4.3643(4.3797) | Error 0.2900(0.2944) Steps 850(830.46) | Grad Norm 6.1151(6.4687) | Total Time 14.00(14.00)\n",
      "Iter 1700 | Time 34.5745(33.9678) | Bit/dim 3.9408(3.9556) | Xent 0.9059(0.8512) | Loss 4.3938(4.3812) | Error 0.3322(0.2983) Steps 832(832.55) | Grad Norm 7.8408(6.5645) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0031 | Time 128.5715, Epoch Time 2011.3235(1644.1481), Bit/dim 3.9453(best: 3.9628), Xent 1.1683, Loss 4.5294, Error 0.3972(best: 0.4023)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1710 | Time 32.8252(33.8178) | Bit/dim 3.9497(3.9506) | Xent 0.7615(0.8446) | Loss 4.3304(4.3729) | Error 0.2700(0.2963) Steps 790(830.44) | Grad Norm 5.9797(6.3705) | Total Time 14.00(14.00)\n",
      "Iter 1720 | Time 33.2427(33.7330) | Bit/dim 3.9499(3.9493) | Xent 0.7988(0.8224) | Loss 4.3493(4.3605) | Error 0.2678(0.2875) Steps 832(830.83) | Grad Norm 8.7801(6.1360) | Total Time 14.00(14.00)\n",
      "Iter 1730 | Time 32.6953(33.6235) | Bit/dim 3.9156(3.9424) | Xent 0.8183(0.8088) | Loss 4.3247(4.3468) | Error 0.2878(0.2840) Steps 814(827.37) | Grad Norm 8.5488(6.1160) | Total Time 14.00(14.00)\n",
      "Iter 1740 | Time 33.4956(33.3498) | Bit/dim 3.9586(3.9424) | Xent 0.8068(0.8046) | Loss 4.3620(4.3447) | Error 0.2800(0.2838) Steps 826(824.40) | Grad Norm 7.0781(5.9408) | Total Time 14.00(14.00)\n",
      "Iter 1750 | Time 34.1663(33.2683) | Bit/dim 3.9229(3.9384) | Xent 0.7169(0.7991) | Loss 4.2813(4.3380) | Error 0.2667(0.2811) Steps 814(821.90) | Grad Norm 3.6714(5.8237) | Total Time 14.00(14.00)\n",
      "Iter 1760 | Time 33.5039(33.2640) | Bit/dim 3.9512(3.9382) | Xent 0.7866(0.8002) | Loss 4.3445(4.3383) | Error 0.2756(0.2822) Steps 856(824.42) | Grad Norm 4.6779(5.6697) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0032 | Time 123.1587, Epoch Time 1966.4886(1653.8183), Bit/dim 3.9306(best: 3.9453), Xent 1.1690, Loss 4.5151, Error 0.3878(best: 0.3972)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1770 | Time 32.6796(33.3334) | Bit/dim 3.9123(3.9372) | Xent 0.7369(0.7888) | Loss 4.2808(4.3316) | Error 0.2444(0.2768) Steps 826(824.92) | Grad Norm 6.5653(5.9423) | Total Time 14.00(14.00)\n",
      "Iter 1780 | Time 32.3536(33.2429) | Bit/dim 3.9194(3.9342) | Xent 0.7578(0.7799) | Loss 4.2983(4.3241) | Error 0.2900(0.2749) Steps 832(824.60) | Grad Norm 9.4170(6.4733) | Total Time 14.00(14.00)\n",
      "Iter 1790 | Time 33.5947(33.1481) | Bit/dim 3.9261(3.9333) | Xent 0.7401(0.7749) | Loss 4.2961(4.3208) | Error 0.2611(0.2729) Steps 820(824.39) | Grad Norm 9.7452(6.3622) | Total Time 14.00(14.00)\n",
      "Iter 1800 | Time 33.6585(33.1555) | Bit/dim 3.9025(3.9301) | Xent 0.6845(0.7691) | Loss 4.2447(4.3146) | Error 0.2444(0.2708) Steps 826(825.19) | Grad Norm 3.3566(6.2873) | Total Time 14.00(14.00)\n",
      "Iter 1810 | Time 33.1044(33.3694) | Bit/dim 3.9128(3.9280) | Xent 0.7664(0.7714) | Loss 4.2961(4.3137) | Error 0.2744(0.2718) Steps 838(827.05) | Grad Norm 8.8146(6.4191) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0033 | Time 122.2784, Epoch Time 1971.6257(1663.3526), Bit/dim 3.9266(best: 3.9306), Xent 1.1695, Loss 4.5114, Error 0.3901(best: 0.3878)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1820 | Time 33.9963(33.2525) | Bit/dim 3.9192(3.9245) | Xent 0.6298(0.7641) | Loss 4.2341(4.3066) | Error 0.2200(0.2686) Steps 832(825.29) | Grad Norm 6.4528(6.2870) | Total Time 14.00(14.00)\n",
      "Iter 1830 | Time 32.6107(33.2209) | Bit/dim 3.9098(3.9242) | Xent 0.6703(0.7482) | Loss 4.2450(4.2983) | Error 0.2422(0.2633) Steps 832(828.13) | Grad Norm 7.9970(6.2011) | Total Time 14.00(14.00)\n",
      "Iter 1840 | Time 31.5889(33.2830) | Bit/dim 3.9046(3.9188) | Xent 0.7172(0.7347) | Loss 4.2632(4.2861) | Error 0.2489(0.2599) Steps 838(831.13) | Grad Norm 6.0234(6.0370) | Total Time 14.00(14.00)\n",
      "Iter 1850 | Time 33.9664(33.3206) | Bit/dim 3.8891(3.9175) | Xent 0.7327(0.7334) | Loss 4.2555(4.2842) | Error 0.2589(0.2590) Steps 874(834.35) | Grad Norm 5.5699(6.1109) | Total Time 14.00(14.00)\n",
      "Iter 1860 | Time 35.4010(33.6180) | Bit/dim 3.8890(3.9136) | Xent 0.7389(0.7333) | Loss 4.2584(4.2803) | Error 0.2667(0.2589) Steps 820(833.98) | Grad Norm 4.1573(5.8467) | Total Time 14.00(14.00)\n",
      "Iter 1870 | Time 33.1373(33.6337) | Bit/dim 3.9419(3.9147) | Xent 0.7309(0.7403) | Loss 4.3074(4.2849) | Error 0.2622(0.2615) Steps 826(834.12) | Grad Norm 4.6341(6.1880) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0034 | Time 125.0006, Epoch Time 1990.7723(1673.1751), Bit/dim 3.9128(best: 3.9266), Xent 1.1978, Loss 4.5117, Error 0.3890(best: 0.3878)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1880 | Time 33.4984(33.9004) | Bit/dim 3.9355(3.9149) | Xent 0.5963(0.7115) | Loss 4.2336(4.2706) | Error 0.2144(0.2513) Steps 850(839.31) | Grad Norm 4.1829(5.6331) | Total Time 14.00(14.00)\n",
      "Iter 1890 | Time 33.8545(34.0266) | Bit/dim 3.8722(3.9131) | Xent 0.7039(0.7038) | Loss 4.2242(4.2650) | Error 0.2311(0.2473) Steps 850(844.04) | Grad Norm 4.7484(6.0329) | Total Time 14.00(14.00)\n",
      "Iter 1900 | Time 33.5081(33.9609) | Bit/dim 3.8681(3.9072) | Xent 0.6365(0.6946) | Loss 4.1863(4.2545) | Error 0.2144(0.2444) Steps 868(842.74) | Grad Norm 6.0352(6.1024) | Total Time 14.00(14.00)\n",
      "Iter 1910 | Time 32.6362(33.8615) | Bit/dim 3.9182(3.9075) | Xent 0.7071(0.7031) | Loss 4.2718(4.2591) | Error 0.2556(0.2483) Steps 832(843.70) | Grad Norm 6.2913(6.2164) | Total Time 14.00(14.00)\n",
      "Iter 1920 | Time 33.2792(33.8235) | Bit/dim 3.8938(3.9063) | Xent 0.7056(0.7086) | Loss 4.2466(4.2606) | Error 0.2678(0.2504) Steps 826(841.26) | Grad Norm 5.3290(6.5332) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0035 | Time 123.4872, Epoch Time 2009.9717(1683.2790), Bit/dim 3.9144(best: 3.9128), Xent 1.2524, Loss 4.5406, Error 0.3940(best: 0.3878)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1930 | Time 34.3427(33.6988) | Bit/dim 3.8813(3.9028) | Xent 0.5933(0.7059) | Loss 4.1779(4.2557) | Error 0.2211(0.2500) Steps 850(838.19) | Grad Norm 4.6021(6.4902) | Total Time 14.00(14.00)\n",
      "Iter 1940 | Time 32.2334(33.7974) | Bit/dim 3.8812(3.9017) | Xent 0.6384(0.6849) | Loss 4.2004(4.2442) | Error 0.2189(0.2409) Steps 820(840.67) | Grad Norm 9.2668(6.1428) | Total Time 14.00(14.00)\n",
      "Iter 1950 | Time 34.2896(33.6478) | Bit/dim 3.8936(3.8982) | Xent 0.5892(0.6669) | Loss 4.1882(4.2316) | Error 0.2244(0.2346) Steps 844(837.79) | Grad Norm 4.3605(5.7748) | Total Time 14.00(14.00)\n",
      "Iter 1960 | Time 32.7884(33.5467) | Bit/dim 3.9015(3.8969) | Xent 0.6294(0.6560) | Loss 4.2162(4.2249) | Error 0.2122(0.2312) Steps 820(834.68) | Grad Norm 4.3382(5.3123) | Total Time 14.00(14.00)\n",
      "Iter 1970 | Time 33.1388(33.8329) | Bit/dim 3.8895(3.8926) | Xent 0.5927(0.6479) | Loss 4.1858(4.2166) | Error 0.2067(0.2279) Steps 856(834.91) | Grad Norm 7.8860(5.2927) | Total Time 14.00(14.00)\n",
      "Iter 1980 | Time 35.6788(33.9856) | Bit/dim 3.9871(3.9052) | Xent 0.7123(0.7035) | Loss 4.3433(4.2569) | Error 0.2667(0.2459) Steps 886(840.28) | Grad Norm 8.2345(7.5730) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0036 | Time 126.4008, Epoch Time 2009.9011(1693.0777), Bit/dim 3.9735(best: 3.9128), Xent 1.3113, Loss 4.6292, Error 0.4032(best: 0.3878)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1990 | Time 35.6871(34.1209) | Bit/dim 3.9247(3.9116) | Xent 0.6658(0.7098) | Loss 4.2576(4.2665) | Error 0.2433(0.2479) Steps 856(841.87) | Grad Norm 6.0066(7.3677) | Total Time 14.00(14.00)\n",
      "Iter 2000 | Time 32.4557(33.8977) | Bit/dim 3.9403(3.9073) | Xent 0.6596(0.6939) | Loss 4.2700(4.2543) | Error 0.2300(0.2429) Steps 844(842.29) | Grad Norm 4.1421(6.7687) | Total Time 14.00(14.00)\n",
      "Iter 2010 | Time 34.3639(33.7334) | Bit/dim 3.8634(3.9011) | Xent 0.6588(0.6775) | Loss 4.1928(4.2399) | Error 0.2333(0.2380) Steps 826(841.24) | Grad Norm 4.2585(6.2240) | Total Time 14.00(14.00)\n",
      "Iter 2020 | Time 33.9381(33.6160) | Bit/dim 3.8743(3.8965) | Xent 0.6466(0.6600) | Loss 4.1976(4.2265) | Error 0.2389(0.2327) Steps 868(841.55) | Grad Norm 6.4744(5.9014) | Total Time 14.00(14.00)\n",
      "Iter 2030 | Time 34.8097(33.7403) | Bit/dim 3.8346(3.8893) | Xent 0.5950(0.6459) | Loss 4.1321(4.2123) | Error 0.1967(0.2281) Steps 856(843.27) | Grad Norm 4.7906(5.5717) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0037 | Time 123.7945, Epoch Time 1994.3541(1702.1160), Bit/dim 3.8791(best: 3.9128), Xent 1.2604, Loss 4.5092, Error 0.3747(best: 0.3878)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2040 | Time 33.0822(33.6212) | Bit/dim 3.8559(3.8876) | Xent 0.5959(0.6330) | Loss 4.1539(4.2041) | Error 0.2133(0.2233) Steps 832(843.97) | Grad Norm 9.0695(5.9285) | Total Time 14.00(14.00)\n",
      "Iter 2050 | Time 34.4322(33.6376) | Bit/dim 3.8314(3.8860) | Xent 0.5038(0.6093) | Loss 4.0833(4.1907) | Error 0.1856(0.2147) Steps 856(845.83) | Grad Norm 3.6563(5.7172) | Total Time 14.00(14.00)\n",
      "Iter 2060 | Time 35.8103(33.7906) | Bit/dim 3.8410(3.8814) | Xent 0.5029(0.5897) | Loss 4.0924(4.1762) | Error 0.1778(0.2079) Steps 844(846.53) | Grad Norm 2.8816(5.1456) | Total Time 14.00(14.00)\n",
      "Iter 2070 | Time 33.0333(33.7347) | Bit/dim 3.9062(3.8779) | Xent 0.6057(0.5855) | Loss 4.2091(4.1707) | Error 0.2067(0.2054) Steps 850(843.01) | Grad Norm 6.8164(5.1807) | Total Time 14.00(14.00)\n",
      "Iter 2080 | Time 32.3832(33.9067) | Bit/dim 3.8808(3.8776) | Xent 0.5913(0.5975) | Loss 4.1764(4.1763) | Error 0.2011(0.2102) Steps 844(847.53) | Grad Norm 7.8275(5.9657) | Total Time 14.00(14.00)\n",
      "Iter 2090 | Time 35.8181(33.8579) | Bit/dim 3.8612(3.8759) | Xent 0.5762(0.5984) | Loss 4.1493(4.1751) | Error 0.1956(0.2110) Steps 874(848.29) | Grad Norm 3.6568(5.9054) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0038 | Time 122.1568, Epoch Time 2002.5142(1711.1279), Bit/dim 3.8690(best: 3.8791), Xent 1.2954, Loss 4.5167, Error 0.3892(best: 0.3747)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2100 | Time 33.2029(33.9443) | Bit/dim 3.8820(3.8728) | Xent 0.5209(0.5781) | Loss 4.1425(4.1619) | Error 0.1811(0.2033) Steps 868(851.87) | Grad Norm 4.0267(5.8692) | Total Time 14.00(14.00)\n",
      "Iter 2110 | Time 32.4763(33.7748) | Bit/dim 3.9210(3.8722) | Xent 0.5320(0.5637) | Loss 4.1870(4.1540) | Error 0.1933(0.1997) Steps 814(847.57) | Grad Norm 6.6224(5.8946) | Total Time 14.00(14.00)\n",
      "Iter 2120 | Time 33.2928(33.7941) | Bit/dim 3.8603(3.8691) | Xent 0.5369(0.5573) | Loss 4.1288(4.1478) | Error 0.1922(0.1970) Steps 844(849.40) | Grad Norm 6.6668(5.9359) | Total Time 14.00(14.00)\n",
      "Iter 2130 | Time 35.7534(33.8231) | Bit/dim 3.9279(3.8680) | Xent 0.5279(0.5537) | Loss 4.1918(4.1449) | Error 0.1856(0.1950) Steps 844(848.09) | Grad Norm 5.2596(5.9342) | Total Time 14.00(14.00)\n",
      "Iter 2140 | Time 34.4282(34.1463) | Bit/dim 3.8761(3.8687) | Xent 0.6621(0.5651) | Loss 4.2071(4.1512) | Error 0.2367(0.1991) Steps 844(849.65) | Grad Norm 13.5954(6.5003) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0039 | Time 123.7084, Epoch Time 2018.4640(1720.3480), Bit/dim 3.8658(best: 3.8690), Xent 1.2847, Loss 4.5082, Error 0.3819(best: 0.3747)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2150 | Time 33.8309(34.2155) | Bit/dim 3.8541(3.8693) | Xent 0.4890(0.5576) | Loss 4.0986(4.1482) | Error 0.1889(0.1968) Steps 844(848.99) | Grad Norm 5.9812(6.4580) | Total Time 14.00(14.00)\n",
      "Iter 2160 | Time 34.1935(34.2015) | Bit/dim 3.8231(3.8655) | Xent 0.4355(0.5328) | Loss 4.0408(4.1320) | Error 0.1500(0.1872) Steps 862(849.92) | Grad Norm 3.0050(6.1032) | Total Time 14.00(14.00)\n",
      "Iter 2170 | Time 34.6757(34.2834) | Bit/dim 3.8557(3.8623) | Xent 0.4921(0.5229) | Loss 4.1017(4.1237) | Error 0.1767(0.1848) Steps 862(848.64) | Grad Norm 4.5400(5.7288) | Total Time 14.00(14.00)\n",
      "Iter 2180 | Time 36.6455(34.4186) | Bit/dim 3.8555(3.8587) | Xent 0.5180(0.5182) | Loss 4.1145(4.1178) | Error 0.1711(0.1830) Steps 880(851.00) | Grad Norm 5.7468(5.5988) | Total Time 14.00(14.00)\n",
      "Iter 2190 | Time 34.5667(34.7289) | Bit/dim 3.8376(3.8587) | Xent 0.5642(0.5211) | Loss 4.1197(4.1192) | Error 0.2067(0.1838) Steps 838(854.50) | Grad Norm 7.5123(5.6176) | Total Time 14.00(14.00)\n",
      "Iter 2200 | Time 34.6162(34.6994) | Bit/dim 3.8591(3.8567) | Xent 0.5855(0.5362) | Loss 4.1519(4.1248) | Error 0.2089(0.1889) Steps 862(857.38) | Grad Norm 3.9539(5.8988) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0040 | Time 122.3309, Epoch Time 2049.8778(1730.2339), Bit/dim 3.8568(best: 3.8658), Xent 1.3472, Loss 4.5304, Error 0.3877(best: 0.3747)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2210 | Time 33.2451(34.4792) | Bit/dim 3.8578(3.8560) | Xent 0.5002(0.5215) | Loss 4.1079(4.1167) | Error 0.1767(0.1823) Steps 850(858.73) | Grad Norm 6.4431(5.8733) | Total Time 14.00(14.00)\n",
      "Iter 2220 | Time 34.8383(34.4718) | Bit/dim 3.8641(3.8542) | Xent 0.5011(0.5054) | Loss 4.1147(4.1069) | Error 0.1733(0.1768) Steps 874(860.32) | Grad Norm 4.9321(5.6090) | Total Time 14.00(14.00)\n",
      "Iter 2230 | Time 33.3183(34.6229) | Bit/dim 3.8215(3.8542) | Xent 0.4805(0.4880) | Loss 4.0618(4.0982) | Error 0.1800(0.1715) Steps 844(859.53) | Grad Norm 5.0594(5.2463) | Total Time 14.00(14.00)\n",
      "Iter 2240 | Time 35.0163(34.6336) | Bit/dim 3.8457(3.8520) | Xent 0.5032(0.4942) | Loss 4.0973(4.0991) | Error 0.1844(0.1740) Steps 856(857.63) | Grad Norm 8.3290(5.8615) | Total Time 14.00(14.00)\n",
      "Iter 2250 | Time 33.3799(34.5197) | Bit/dim 3.8563(3.8521) | Xent 0.5001(0.4984) | Loss 4.1063(4.1013) | Error 0.1767(0.1762) Steps 856(857.51) | Grad Norm 4.2275(5.7706) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0041 | Time 128.5709, Epoch Time 2047.0915(1739.7396), Bit/dim 3.8508(best: 3.8568), Xent 1.3026, Loss 4.5021, Error 0.3706(best: 0.3747)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2260 | Time 34.8569(34.7233) | Bit/dim 3.8469(3.8501) | Xent 0.3776(0.4823) | Loss 4.0356(4.0912) | Error 0.1333(0.1708) Steps 880(860.80) | Grad Norm 3.6576(5.4934) | Total Time 14.00(14.00)\n",
      "Iter 2270 | Time 34.5213(34.6894) | Bit/dim 3.8096(3.8467) | Xent 0.4278(0.4608) | Loss 4.0235(4.0771) | Error 0.1589(0.1630) Steps 826(860.13) | Grad Norm 6.8819(5.2947) | Total Time 14.00(14.00)\n",
      "Iter 2280 | Time 32.8832(34.3589) | Bit/dim 3.8511(3.8467) | Xent 0.4943(0.4555) | Loss 4.0982(4.0744) | Error 0.1711(0.1606) Steps 832(855.20) | Grad Norm 9.1799(5.9410) | Total Time 14.00(14.00)\n",
      "Iter 2290 | Time 34.6716(34.3891) | Bit/dim 3.8734(3.8498) | Xent 0.4852(0.4684) | Loss 4.1160(4.0840) | Error 0.1744(0.1645) Steps 880(857.73) | Grad Norm 3.9612(6.5985) | Total Time 14.00(14.00)\n",
      "Iter 2300 | Time 36.9577(34.5628) | Bit/dim 3.8615(3.8511) | Xent 0.4799(0.4712) | Loss 4.1015(4.0867) | Error 0.1578(0.1656) Steps 868(856.97) | Grad Norm 4.5731(6.4255) | Total Time 14.00(14.00)\n",
      "Iter 2310 | Time 33.6792(34.6002) | Bit/dim 3.8413(3.8466) | Xent 0.5149(0.4765) | Loss 4.0988(4.0849) | Error 0.1922(0.1681) Steps 832(859.24) | Grad Norm 8.0192(6.7029) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0042 | Time 123.8760, Epoch Time 2042.5492(1748.8239), Bit/dim 3.8556(best: 3.8508), Xent 1.4614, Loss 4.5863, Error 0.3889(best: 0.3706)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2320 | Time 35.8851(34.5917) | Bit/dim 3.8759(3.8488) | Xent 0.4673(0.4688) | Loss 4.1095(4.0832) | Error 0.1544(0.1646) Steps 874(859.74) | Grad Norm 7.9287(6.6260) | Total Time 14.00(14.00)\n",
      "Iter 2330 | Time 33.9687(34.5279) | Bit/dim 3.8141(3.8480) | Xent 0.4480(0.4559) | Loss 4.0381(4.0759) | Error 0.1611(0.1603) Steps 868(862.41) | Grad Norm 7.5899(6.6741) | Total Time 14.00(14.00)\n",
      "Iter 2340 | Time 34.6447(34.5717) | Bit/dim 3.8134(3.8440) | Xent 0.3921(0.4451) | Loss 4.0095(4.0665) | Error 0.1422(0.1558) Steps 850(860.28) | Grad Norm 3.4665(6.2864) | Total Time 14.00(14.00)\n",
      "Iter 2350 | Time 34.4407(34.7580) | Bit/dim 3.8180(3.8420) | Xent 0.3871(0.4339) | Loss 4.0115(4.0590) | Error 0.1422(0.1519) Steps 844(856.68) | Grad Norm 3.7741(5.8446) | Total Time 14.00(14.00)\n",
      "Iter 2360 | Time 34.3757(34.7355) | Bit/dim 3.8089(3.8389) | Xent 0.4130(0.4277) | Loss 4.0154(4.0527) | Error 0.1511(0.1497) Steps 856(860.39) | Grad Norm 6.1192(5.7444) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0043 | Time 125.7178, Epoch Time 2055.2921(1758.0180), Bit/dim 3.8370(best: 3.8508), Xent 1.4553, Loss 4.5647, Error 0.3826(best: 0.3706)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2370 | Time 35.5504(34.8512) | Bit/dim 3.7860(3.8361) | Xent 0.3393(0.4206) | Loss 3.9557(4.0464) | Error 0.1344(0.1473) Steps 874(861.72) | Grad Norm 4.7234(5.8679) | Total Time 14.00(14.00)\n",
      "Iter 2380 | Time 34.2298(34.7504) | Bit/dim 3.8404(3.8380) | Xent 0.3467(0.4023) | Loss 4.0137(4.0391) | Error 0.1289(0.1410) Steps 844(862.05) | Grad Norm 5.9201(5.9434) | Total Time 14.00(14.00)\n",
      "Iter 2390 | Time 34.0552(34.6469) | Bit/dim 3.8192(3.8319) | Xent 0.4043(0.3974) | Loss 4.0214(4.0306) | Error 0.1378(0.1400) Steps 856(861.10) | Grad Norm 3.9977(5.9921) | Total Time 14.00(14.00)\n",
      "Iter 2400 | Time 35.3685(34.8204) | Bit/dim 3.8230(3.8307) | Xent 0.4064(0.3885) | Loss 4.0262(4.0250) | Error 0.1467(0.1362) Steps 850(861.41) | Grad Norm 5.6694(5.8676) | Total Time 14.00(14.00)\n",
      "Iter 2410 | Time 35.9477(34.8507) | Bit/dim 3.8203(3.8308) | Xent 0.3621(0.3842) | Loss 4.0013(4.0229) | Error 0.1189(0.1335) Steps 880(864.91) | Grad Norm 4.4130(5.3616) | Total Time 14.00(14.00)\n",
      "Iter 2420 | Time 35.2762(34.8801) | Bit/dim 3.8275(3.8287) | Xent 0.4718(0.3971) | Loss 4.0634(4.0273) | Error 0.1733(0.1389) Steps 898(866.85) | Grad Norm 10.2608(5.7838) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0044 | Time 127.7621, Epoch Time 2063.9984(1767.1974), Bit/dim 3.8413(best: 3.8370), Xent 1.4792, Loss 4.5809, Error 0.3814(best: 0.3706)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2430 | Time 35.8404(34.9903) | Bit/dim 3.7838(3.8263) | Xent 0.3358(0.3837) | Loss 3.9517(4.0181) | Error 0.1156(0.1333) Steps 886(867.72) | Grad Norm 4.2483(5.6951) | Total Time 14.00(14.00)\n",
      "Iter 2440 | Time 34.3155(34.7611) | Bit/dim 3.8383(3.8271) | Xent 0.3211(0.3624) | Loss 3.9988(4.0083) | Error 0.1067(0.1251) Steps 856(862.59) | Grad Norm 3.1980(5.2416) | Total Time 14.00(14.00)\n",
      "Iter 2450 | Time 34.0285(34.8145) | Bit/dim 3.7963(3.8244) | Xent 0.4079(0.3588) | Loss 4.0002(4.0038) | Error 0.1467(0.1243) Steps 856(862.83) | Grad Norm 4.9531(5.1572) | Total Time 14.00(14.00)\n",
      "Iter 2460 | Time 34.5520(34.9941) | Bit/dim 3.8078(3.8245) | Xent 0.3722(0.3617) | Loss 3.9939(4.0054) | Error 0.1244(0.1263) Steps 886(864.58) | Grad Norm 6.1173(5.3750) | Total Time 14.00(14.00)\n",
      "Iter 2470 | Time 35.7950(34.9888) | Bit/dim 3.8209(3.8241) | Xent 0.3570(0.3645) | Loss 3.9995(4.0063) | Error 0.1333(0.1281) Steps 886(868.57) | Grad Norm 3.4875(5.4988) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0045 | Time 125.7744, Epoch Time 2071.0300(1776.3124), Bit/dim 3.8180(best: 3.8370), Xent 1.4969, Loss 4.5665, Error 0.3750(best: 0.3706)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2480 | Time 34.6804(35.0776) | Bit/dim 3.8525(3.8216) | Xent 0.2669(0.3524) | Loss 3.9860(3.9978) | Error 0.0856(0.1233) Steps 874(870.95) | Grad Norm 3.6507(5.4458) | Total Time 14.00(14.00)\n",
      "Iter 2490 | Time 35.8107(35.0662) | Bit/dim 3.8000(3.8177) | Xent 0.2558(0.3410) | Loss 3.9278(3.9882) | Error 0.1000(0.1193) Steps 874(871.16) | Grad Norm 4.4745(5.4789) | Total Time 14.00(14.00)\n",
      "Iter 2500 | Time 35.2021(35.0781) | Bit/dim 3.8084(3.8178) | Xent 0.3740(0.3353) | Loss 3.9954(3.9854) | Error 0.1300(0.1173) Steps 880(870.41) | Grad Norm 10.5274(5.4055) | Total Time 14.00(14.00)\n",
      "Iter 2510 | Time 35.1840(35.2564) | Bit/dim 3.8017(3.8167) | Xent 0.3331(0.3328) | Loss 3.9683(3.9831) | Error 0.1111(0.1175) Steps 880(871.14) | Grad Norm 5.8102(5.5758) | Total Time 14.00(14.00)\n",
      "Iter 2520 | Time 34.6886(35.2930) | Bit/dim 3.8057(3.8170) | Xent 0.3311(0.3290) | Loss 3.9712(3.9815) | Error 0.1200(0.1177) Steps 874(870.57) | Grad Norm 6.2064(5.4982) | Total Time 14.00(14.00)\n",
      "Iter 2530 | Time 34.6192(35.2670) | Bit/dim 3.8001(3.8163) | Xent 0.3752(0.3344) | Loss 3.9877(3.9835) | Error 0.1444(0.1200) Steps 874(871.88) | Grad Norm 8.5342(5.7415) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0046 | Time 124.1481, Epoch Time 2084.0854(1785.5456), Bit/dim 3.8311(best: 3.8180), Xent 1.5403, Loss 4.6013, Error 0.3817(best: 0.3706)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2540 | Time 34.8974(35.1784) | Bit/dim 3.7870(3.8145) | Xent 0.2728(0.3210) | Loss 3.9233(3.9750) | Error 0.0978(0.1152) Steps 874(870.83) | Grad Norm 3.7261(5.8848) | Total Time 14.00(14.00)\n",
      "Iter 2550 | Time 35.5802(35.3158) | Bit/dim 3.8320(3.8145) | Xent 0.2648(0.3107) | Loss 3.9644(3.9698) | Error 0.0989(0.1113) Steps 874(872.09) | Grad Norm 5.1375(5.9685) | Total Time 14.00(14.00)\n",
      "Iter 2560 | Time 37.0084(35.4723) | Bit/dim 3.8067(3.8137) | Xent 0.3905(0.3128) | Loss 4.0020(3.9701) | Error 0.1411(0.1106) Steps 892(872.32) | Grad Norm 8.6081(5.9679) | Total Time 14.00(14.00)\n",
      "Iter 2570 | Time 34.9278(35.4074) | Bit/dim 3.8157(3.8138) | Xent 0.3173(0.3098) | Loss 3.9743(3.9687) | Error 0.1100(0.1097) Steps 880(871.47) | Grad Norm 4.3688(5.5867) | Total Time 14.00(14.00)\n",
      "Iter 2580 | Time 35.2272(35.2466) | Bit/dim 3.8399(3.8120) | Xent 0.3131(0.3089) | Loss 3.9965(3.9664) | Error 0.1022(0.1091) Steps 874(870.59) | Grad Norm 5.1715(5.4765) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0047 | Time 129.6278, Epoch Time 2086.9146(1794.5866), Bit/dim 3.8120(best: 3.8180), Xent 1.7178, Loss 4.6708, Error 0.3772(best: 0.3706)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2590 | Time 35.1376(35.2029) | Bit/dim 3.8025(3.8092) | Xent 0.2528(0.3009) | Loss 3.9289(3.9596) | Error 0.0933(0.1065) Steps 862(867.23) | Grad Norm 6.7785(5.8001) | Total Time 14.00(14.00)\n",
      "Iter 2600 | Time 36.6416(35.1663) | Bit/dim 3.8194(3.8076) | Xent 0.2586(0.2973) | Loss 3.9487(3.9563) | Error 0.0967(0.1053) Steps 886(866.44) | Grad Norm 3.6748(5.8975) | Total Time 14.00(14.00)\n",
      "Iter 2610 | Time 36.6294(35.3387) | Bit/dim 3.7878(3.8066) | Xent 0.2571(0.2854) | Loss 3.9164(3.9493) | Error 0.0878(0.1006) Steps 856(866.78) | Grad Norm 4.9121(5.5532) | Total Time 14.00(14.00)\n",
      "Iter 2620 | Time 36.4468(35.2678) | Bit/dim 3.7858(3.8037) | Xent 0.2659(0.2803) | Loss 3.9187(3.9438) | Error 0.0944(0.0993) Steps 898(865.43) | Grad Norm 3.6776(5.1642) | Total Time 14.00(14.00)\n",
      "Iter 2630 | Time 35.5882(35.2537) | Bit/dim 3.8486(3.8063) | Xent 0.2477(0.2821) | Loss 3.9724(3.9473) | Error 0.0911(0.1000) Steps 862(866.58) | Grad Norm 4.3419(5.0352) | Total Time 14.00(14.00)\n",
      "Iter 2640 | Time 35.4227(35.2875) | Bit/dim 3.7939(3.8064) | Xent 0.3346(0.2862) | Loss 3.9612(3.9495) | Error 0.1222(0.1020) Steps 916(867.04) | Grad Norm 7.4043(5.2602) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0048 | Time 130.1580, Epoch Time 2091.7729(1803.5022), Bit/dim 3.8115(best: 3.8120), Xent 1.7827, Loss 4.7029, Error 0.3901(best: 0.3706)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2650 | Time 33.7095(35.2238) | Bit/dim 3.8248(3.8087) | Xent 0.2461(0.2791) | Loss 3.9478(3.9483) | Error 0.0911(0.1002) Steps 838(867.94) | Grad Norm 5.1622(5.6252) | Total Time 14.00(14.00)\n",
      "Iter 2660 | Time 35.0805(35.2228) | Bit/dim 3.8119(3.8066) | Xent 0.2196(0.2671) | Loss 3.9217(3.9402) | Error 0.0800(0.0959) Steps 850(867.15) | Grad Norm 6.3588(5.5310) | Total Time 14.00(14.00)\n",
      "Iter 2670 | Time 36.0014(35.1937) | Bit/dim 3.7829(3.8056) | Xent 0.2292(0.2603) | Loss 3.8975(3.9357) | Error 0.0822(0.0936) Steps 862(862.98) | Grad Norm 4.6241(5.2851) | Total Time 14.00(14.00)\n",
      "Iter 2680 | Time 34.5743(35.2652) | Bit/dim 3.7971(3.8030) | Xent 0.2445(0.2539) | Loss 3.9194(3.9299) | Error 0.0844(0.0915) Steps 862(864.91) | Grad Norm 4.0544(5.1424) | Total Time 14.00(14.00)\n",
      "Iter 2690 | Time 33.8542(35.2005) | Bit/dim 3.7807(3.8020) | Xent 0.2437(0.2549) | Loss 3.9025(3.9295) | Error 0.0811(0.0913) Steps 892(866.60) | Grad Norm 5.5220(5.2114) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0049 | Time 124.3308, Epoch Time 2078.5651(1811.7541), Bit/dim 3.7960(best: 3.8115), Xent 1.6924, Loss 4.6422, Error 0.3703(best: 0.3706)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2700 | Time 36.4006(35.1518) | Bit/dim 3.8105(3.8008) | Xent 0.1789(0.2513) | Loss 3.8999(3.9265) | Error 0.0600(0.0898) Steps 892(868.18) | Grad Norm 3.5542(5.2368) | Total Time 14.00(14.00)\n",
      "Iter 2710 | Time 34.1929(35.2301) | Bit/dim 3.8286(3.7998) | Xent 0.2300(0.2438) | Loss 3.9436(3.9217) | Error 0.0711(0.0859) Steps 850(865.61) | Grad Norm 7.0560(5.2462) | Total Time 14.00(14.00)\n",
      "Iter 2720 | Time 35.3487(35.4452) | Bit/dim 3.8317(3.7969) | Xent 0.2564(0.2400) | Loss 3.9599(3.9169) | Error 0.0867(0.0842) Steps 868(866.92) | Grad Norm 6.0973(5.2409) | Total Time 14.00(14.00)\n",
      "Iter 2730 | Time 35.2592(35.4722) | Bit/dim 3.7850(3.7950) | Xent 0.2875(0.2446) | Loss 3.9287(3.9174) | Error 0.1044(0.0860) Steps 898(868.13) | Grad Norm 6.3352(5.6021) | Total Time 14.00(14.00)\n",
      "Iter 2740 | Time 36.4624(35.5492) | Bit/dim 3.8148(3.7961) | Xent 0.2862(0.2526) | Loss 3.9579(3.9224) | Error 0.1089(0.0898) Steps 880(869.78) | Grad Norm 7.7292(5.6659) | Total Time 14.00(14.00)\n",
      "Iter 2750 | Time 35.1340(35.5315) | Bit/dim 3.7733(3.7965) | Xent 0.2653(0.2575) | Loss 3.9060(3.9252) | Error 0.0989(0.0927) Steps 874(870.70) | Grad Norm 6.5539(5.7545) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0050 | Time 126.6725, Epoch Time 2103.4418(1820.5047), Bit/dim 3.7961(best: 3.7960), Xent 1.8091, Loss 4.7006, Error 0.3875(best: 0.3703)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2760 | Time 36.1777(35.4800) | Bit/dim 3.7967(3.7966) | Xent 0.2244(0.2431) | Loss 3.9089(3.9181) | Error 0.0789(0.0871) Steps 862(869.75) | Grad Norm 5.8513(5.6459) | Total Time 14.00(14.00)\n",
      "Iter 2770 | Time 35.6990(35.6850) | Bit/dim 3.7906(3.7939) | Xent 0.1503(0.2264) | Loss 3.8658(3.9071) | Error 0.0500(0.0803) Steps 868(870.64) | Grad Norm 3.0465(5.0320) | Total Time 14.00(14.00)\n",
      "Iter 2780 | Time 34.7093(35.6199) | Bit/dim 3.7924(3.7929) | Xent 0.1746(0.2180) | Loss 3.8797(3.9019) | Error 0.0589(0.0763) Steps 850(871.60) | Grad Norm 2.6060(4.9347) | Total Time 14.00(14.00)\n",
      "Iter 2790 | Time 35.5861(35.5824) | Bit/dim 3.8158(3.7905) | Xent 0.1918(0.2176) | Loss 3.9117(3.8993) | Error 0.0622(0.0754) Steps 862(870.67) | Grad Norm 4.4493(4.9198) | Total Time 14.00(14.00)\n",
      "Iter 2800 | Time 34.1804(35.4999) | Bit/dim 3.7802(3.7871) | Xent 0.2330(0.2234) | Loss 3.8967(3.8988) | Error 0.0878(0.0777) Steps 850(869.55) | Grad Norm 6.9414(5.5326) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0051 | Time 128.7410, Epoch Time 2106.4536(1829.0832), Bit/dim 3.7961(best: 3.7960), Xent 1.8499, Loss 4.7211, Error 0.3826(best: 0.3703)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2810 | Time 35.0633(35.6497) | Bit/dim 3.8145(3.7865) | Xent 0.1515(0.2176) | Loss 3.8903(3.8953) | Error 0.0578(0.0764) Steps 892(873.22) | Grad Norm 3.5604(5.5198) | Total Time 14.00(14.00)\n",
      "Iter 2820 | Time 37.4505(35.7305) | Bit/dim 3.7668(3.7873) | Xent 0.2060(0.2105) | Loss 3.8698(3.8926) | Error 0.0722(0.0739) Steps 886(875.62) | Grad Norm 7.5036(5.2963) | Total Time 14.00(14.00)\n",
      "Iter 2830 | Time 35.0440(35.7058) | Bit/dim 3.8036(3.7871) | Xent 0.2062(0.2059) | Loss 3.9067(3.8901) | Error 0.0778(0.0728) Steps 886(875.68) | Grad Norm 4.8992(5.0981) | Total Time 14.00(14.00)\n",
      "Iter 2840 | Time 35.2205(35.6967) | Bit/dim 3.8040(3.7856) | Xent 0.2069(0.2042) | Loss 3.9075(3.8877) | Error 0.0722(0.0725) Steps 886(875.63) | Grad Norm 3.9476(4.9646) | Total Time 14.00(14.00)\n",
      "Iter 2850 | Time 35.3650(35.6016) | Bit/dim 3.7598(3.7854) | Xent 0.2322(0.2058) | Loss 3.8759(3.8883) | Error 0.0789(0.0731) Steps 862(875.39) | Grad Norm 6.4813(4.9458) | Total Time 14.00(14.00)\n",
      "Iter 2860 | Time 34.6207(35.6041) | Bit/dim 3.7756(3.7842) | Xent 0.2776(0.2067) | Loss 3.9145(3.8876) | Error 0.0922(0.0732) Steps 868(873.18) | Grad Norm 6.6129(4.9547) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0052 | Time 126.7009, Epoch Time 2108.1504(1837.4552), Bit/dim 3.7801(best: 3.7960), Xent 1.9676, Loss 4.7639, Error 0.3909(best: 0.3703)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2870 | Time 35.8073(35.8089) | Bit/dim 3.7964(3.7814) | Xent 0.1923(0.2014) | Loss 3.8925(3.8821) | Error 0.0789(0.0708) Steps 868(872.45) | Grad Norm 4.1810(4.7706) | Total Time 14.00(14.00)\n",
      "Iter 2880 | Time 33.4870(35.6270) | Bit/dim 3.7801(3.7810) | Xent 0.1847(0.1931) | Loss 3.8724(3.8775) | Error 0.0611(0.0684) Steps 850(871.67) | Grad Norm 4.9104(4.7502) | Total Time 14.00(14.00)\n",
      "Iter 2890 | Time 36.0771(35.7345) | Bit/dim 3.7551(3.7810) | Xent 0.1753(0.1881) | Loss 3.8427(3.8751) | Error 0.0644(0.0658) Steps 874(872.44) | Grad Norm 6.7747(4.8042) | Total Time 14.00(14.00)\n",
      "Iter 2900 | Time 33.7276(35.6029) | Bit/dim 3.8088(3.7822) | Xent 0.2067(0.1868) | Loss 3.9122(3.8756) | Error 0.0644(0.0654) Steps 856(872.74) | Grad Norm 6.0268(5.2896) | Total Time 14.00(14.00)\n",
      "Iter 2910 | Time 34.6630(35.6303) | Bit/dim 3.7660(3.7815) | Xent 0.2278(0.1899) | Loss 3.8799(3.8764) | Error 0.0800(0.0660) Steps 862(873.14) | Grad Norm 6.8777(5.5132) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0053 | Time 129.4508, Epoch Time 2112.8767(1845.7179), Bit/dim 3.7878(best: 3.7801), Xent 2.0514, Loss 4.8135, Error 0.4021(best: 0.3703)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2920 | Time 36.6145(35.7028) | Bit/dim 3.7792(3.7808) | Xent 0.2144(0.1981) | Loss 3.8864(3.8799) | Error 0.0767(0.0692) Steps 922(875.39) | Grad Norm 4.7571(5.9435) | Total Time 14.00(14.00)\n",
      "Iter 2930 | Time 35.5034(35.8547) | Bit/dim 3.7981(3.7823) | Xent 0.1843(0.1952) | Loss 3.8902(3.8799) | Error 0.0644(0.0682) Steps 880(874.85) | Grad Norm 4.9702(5.5910) | Total Time 14.00(14.00)\n",
      "Iter 2940 | Time 36.3191(35.8026) | Bit/dim 3.7837(3.7797) | Xent 0.1330(0.1844) | Loss 3.8502(3.8719) | Error 0.0433(0.0643) Steps 880(873.45) | Grad Norm 3.2195(5.1832) | Total Time 14.00(14.00)\n",
      "Iter 2950 | Time 34.7833(35.6910) | Bit/dim 3.7879(3.7755) | Xent 0.1788(0.1814) | Loss 3.8773(3.8662) | Error 0.0656(0.0628) Steps 892(871.52) | Grad Norm 7.2536(4.9785) | Total Time 14.00(14.00)\n",
      "Iter 2960 | Time 35.8097(35.7108) | Bit/dim 3.8053(3.7745) | Xent 0.1809(0.1842) | Loss 3.8957(3.8665) | Error 0.0656(0.0642) Steps 886(872.62) | Grad Norm 6.3005(5.0272) | Total Time 14.00(14.00)\n",
      "Iter 2970 | Time 34.9201(35.6232) | Bit/dim 3.8197(3.7801) | Xent 0.1809(0.1867) | Loss 3.9102(3.8735) | Error 0.0644(0.0656) Steps 868(870.98) | Grad Norm 7.6449(5.3131) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0054 | Time 122.1400, Epoch Time 2106.4093(1853.5386), Bit/dim 3.7837(best: 3.7801), Xent 1.9639, Loss 4.7656, Error 0.3843(best: 0.3703)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2980 | Time 34.2758(35.6183) | Bit/dim 3.7475(3.7813) | Xent 0.1853(0.1809) | Loss 3.8402(3.8717) | Error 0.0622(0.0634) Steps 850(869.37) | Grad Norm 4.7261(5.3483) | Total Time 14.00(14.00)\n",
      "Iter 2990 | Time 34.7248(35.5262) | Bit/dim 3.7842(3.7776) | Xent 0.1825(0.1751) | Loss 3.8754(3.8652) | Error 0.0644(0.0610) Steps 868(868.27) | Grad Norm 5.8060(5.1206) | Total Time 14.00(14.00)\n",
      "Iter 3000 | Time 33.4576(35.2203) | Bit/dim 3.7798(3.7775) | Xent 0.1465(0.1736) | Loss 3.8530(3.8642) | Error 0.0500(0.0599) Steps 880(866.56) | Grad Norm 3.5069(5.2960) | Total Time 14.00(14.00)\n",
      "Iter 3010 | Time 34.8420(35.3013) | Bit/dim 3.7724(3.7761) | Xent 0.1690(0.1715) | Loss 3.8569(3.8618) | Error 0.0600(0.0598) Steps 850(865.98) | Grad Norm 7.0454(5.3042) | Total Time 14.00(14.00)\n",
      "Iter 3020 | Time 36.8914(35.3861) | Bit/dim 3.7518(3.7737) | Xent 0.2013(0.1727) | Loss 3.8524(3.8601) | Error 0.0767(0.0605) Steps 880(865.52) | Grad Norm 5.8057(5.2959) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0055 | Time 132.7225, Epoch Time 2089.9799(1860.6318), Bit/dim 3.7730(best: 3.7801), Xent 2.0720, Loss 4.8090, Error 0.3749(best: 0.3703)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3030 | Time 35.6383(35.3542) | Bit/dim 3.7620(3.7724) | Xent 0.1954(0.1692) | Loss 3.8597(3.8570) | Error 0.0700(0.0590) Steps 862(866.31) | Grad Norm 6.3622(5.2227) | Total Time 14.00(14.00)\n",
      "Iter 3040 | Time 36.0057(35.4987) | Bit/dim 3.7822(3.7682) | Xent 0.1281(0.1640) | Loss 3.8462(3.8502) | Error 0.0400(0.0573) Steps 868(868.22) | Grad Norm 2.9981(5.2901) | Total Time 14.00(14.00)\n",
      "Iter 3050 | Time 36.0579(35.5751) | Bit/dim 3.7825(3.7690) | Xent 0.1433(0.1582) | Loss 3.8542(3.8481) | Error 0.0444(0.0550) Steps 874(871.70) | Grad Norm 4.8418(5.2649) | Total Time 14.00(14.00)\n",
      "Iter 3060 | Time 35.2434(35.6189) | Bit/dim 3.7802(3.7668) | Xent 0.1192(0.1572) | Loss 3.8398(3.8454) | Error 0.0356(0.0541) Steps 862(872.66) | Grad Norm 3.6551(5.1650) | Total Time 14.00(14.00)\n",
      "Iter 3070 | Time 34.6239(35.4890) | Bit/dim 3.7526(3.7668) | Xent 0.1489(0.1516) | Loss 3.8271(3.8426) | Error 0.0500(0.0527) Steps 886(872.81) | Grad Norm 4.2591(4.8838) | Total Time 14.00(14.00)\n",
      "Iter 3080 | Time 37.9387(35.5658) | Bit/dim 3.7641(3.7681) | Xent 0.2229(0.1553) | Loss 3.8755(3.8458) | Error 0.0756(0.0543) Steps 844(869.60) | Grad Norm 5.3400(4.6872) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0056 | Time 130.5253, Epoch Time 2111.3435(1868.1532), Bit/dim 3.7612(best: 3.7730), Xent 1.9903, Loss 4.7564, Error 0.3869(best: 0.3703)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3090 | Time 36.1335(35.4033) | Bit/dim 3.7571(3.7651) | Xent 0.1496(0.1493) | Loss 3.8319(3.8398) | Error 0.0556(0.0524) Steps 868(866.92) | Grad Norm 5.1453(4.6960) | Total Time 14.00(14.00)\n",
      "Iter 3100 | Time 35.9350(35.3349) | Bit/dim 3.7873(3.7649) | Xent 0.1552(0.1450) | Loss 3.8649(3.8374) | Error 0.0589(0.0514) Steps 844(865.77) | Grad Norm 6.6867(4.6835) | Total Time 14.00(14.00)\n",
      "Iter 3110 | Time 34.5349(35.3168) | Bit/dim 3.7361(3.7609) | Xent 0.1169(0.1425) | Loss 3.7946(3.8322) | Error 0.0411(0.0503) Steps 850(865.94) | Grad Norm 3.2915(4.6353) | Total Time 14.00(14.00)\n",
      "Iter 3120 | Time 35.9226(35.5276) | Bit/dim 3.7550(3.7604) | Xent 0.1583(0.1483) | Loss 3.8342(3.8346) | Error 0.0633(0.0521) Steps 880(869.04) | Grad Norm 6.1834(5.0395) | Total Time 14.00(14.00)\n",
      "Iter 3130 | Time 37.9703(35.6184) | Bit/dim 3.7894(3.7635) | Xent 0.2531(0.1586) | Loss 3.9160(3.8428) | Error 0.0867(0.0552) Steps 868(867.98) | Grad Norm 9.9546(5.3467) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0057 | Time 128.2919, Epoch Time 2100.2566(1875.1163), Bit/dim 3.7736(best: 3.7612), Xent 2.1526, Loss 4.8499, Error 0.3811(best: 0.3703)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3140 | Time 36.4492(35.6424) | Bit/dim 3.7519(3.7630) | Xent 0.1241(0.1574) | Loss 3.8140(3.8417) | Error 0.0444(0.0549) Steps 868(869.18) | Grad Norm 4.6308(5.3689) | Total Time 14.00(14.00)\n",
      "Iter 3150 | Time 34.2890(35.6691) | Bit/dim 3.7854(3.7617) | Xent 0.1066(0.1491) | Loss 3.8388(3.8362) | Error 0.0400(0.0519) Steps 850(871.51) | Grad Norm 3.6187(5.1310) | Total Time 14.00(14.00)\n",
      "Iter 3160 | Time 35.7310(35.6954) | Bit/dim 3.7607(3.7607) | Xent 0.1495(0.1419) | Loss 3.8354(3.8316) | Error 0.0500(0.0488) Steps 868(872.32) | Grad Norm 4.3175(4.8830) | Total Time 14.00(14.00)\n",
      "Iter 3170 | Time 36.6819(35.7184) | Bit/dim 3.7641(3.7599) | Xent 0.1622(0.1437) | Loss 3.8452(3.8317) | Error 0.0589(0.0497) Steps 898(874.19) | Grad Norm 6.3642(5.1307) | Total Time 14.00(14.00)\n",
      "Iter 3180 | Time 36.3128(35.8925) | Bit/dim 3.7920(3.7622) | Xent 0.1618(0.1444) | Loss 3.8730(3.8344) | Error 0.0600(0.0507) Steps 874(876.56) | Grad Norm 3.8727(5.0478) | Total Time 14.00(14.00)\n",
      "Iter 3190 | Time 35.5251(35.9789) | Bit/dim 3.7654(3.7604) | Xent 0.2342(0.1489) | Loss 3.8825(3.8348) | Error 0.0900(0.0525) Steps 850(877.05) | Grad Norm 11.9078(5.3960) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0058 | Time 125.5943, Epoch Time 2123.2162(1882.5593), Bit/dim 3.7618(best: 3.7612), Xent 2.1496, Loss 4.8366, Error 0.3909(best: 0.3703)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3200 | Time 35.2118(35.7903) | Bit/dim 3.7745(3.7589) | Xent 0.1421(0.1486) | Loss 3.8456(3.8332) | Error 0.0478(0.0520) Steps 868(874.52) | Grad Norm 5.1759(5.7187) | Total Time 14.00(14.00)\n",
      "Iter 3210 | Time 35.9622(35.5903) | Bit/dim 3.7830(3.7609) | Xent 0.1537(0.1469) | Loss 3.8599(3.8343) | Error 0.0433(0.0511) Steps 868(874.91) | Grad Norm 6.7585(5.4710) | Total Time 14.00(14.00)\n",
      "Iter 3220 | Time 35.2584(35.4942) | Bit/dim 3.7495(3.7583) | Xent 0.2093(0.1460) | Loss 3.8542(3.8313) | Error 0.0667(0.0505) Steps 862(873.91) | Grad Norm 9.3016(5.3305) | Total Time 14.00(14.00)\n",
      "Iter 3230 | Time 35.5609(35.4758) | Bit/dim 3.7246(3.7572) | Xent 0.1480(0.1498) | Loss 3.7986(3.8321) | Error 0.0489(0.0520) Steps 868(872.35) | Grad Norm 4.5599(5.4349) | Total Time 14.00(14.00)\n",
      "Iter 3240 | Time 36.5770(35.7103) | Bit/dim 3.7433(3.7581) | Xent 0.1710(0.1542) | Loss 3.8287(3.8352) | Error 0.0633(0.0540) Steps 892(873.35) | Grad Norm 5.6082(5.2822) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0059 | Time 130.0265, Epoch Time 2099.4368(1889.0656), Bit/dim 3.7563(best: 3.7612), Xent 2.1519, Loss 4.8323, Error 0.3795(best: 0.3703)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3250 | Time 35.2361(35.5777) | Bit/dim 3.7503(3.7546) | Xent 0.1242(0.1489) | Loss 3.8124(3.8290) | Error 0.0533(0.0522) Steps 886(876.64) | Grad Norm 4.0561(4.9459) | Total Time 14.00(14.00)\n",
      "Iter 3260 | Time 34.7752(35.5567) | Bit/dim 3.7492(3.7516) | Xent 0.1055(0.1373) | Loss 3.8019(3.8203) | Error 0.0344(0.0485) Steps 880(875.96) | Grad Norm 3.5728(4.6223) | Total Time 14.00(14.00)\n",
      "Iter 3270 | Time 35.2417(35.5838) | Bit/dim 3.7534(3.7505) | Xent 0.1081(0.1304) | Loss 3.8075(3.8156) | Error 0.0389(0.0463) Steps 886(877.82) | Grad Norm 4.8027(4.4748) | Total Time 14.00(14.00)\n",
      "Iter 3280 | Time 36.0026(35.7210) | Bit/dim 3.7510(3.7506) | Xent 0.1096(0.1228) | Loss 3.8058(3.8119) | Error 0.0378(0.0435) Steps 892(878.73) | Grad Norm 4.0871(4.1580) | Total Time 14.00(14.00)\n",
      "Iter 3290 | Time 36.5893(35.8553) | Bit/dim 3.7203(3.7476) | Xent 0.1068(0.1216) | Loss 3.7737(3.8084) | Error 0.0344(0.0424) Steps 898(879.15) | Grad Norm 3.1486(4.1741) | Total Time 14.00(14.00)\n",
      "Iter 3300 | Time 36.0798(35.7736) | Bit/dim 3.7293(3.7490) | Xent 0.1675(0.1292) | Loss 3.8130(3.8136) | Error 0.0578(0.0448) Steps 874(875.88) | Grad Norm 6.1416(4.7017) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0060 | Time 129.0856, Epoch Time 2114.3315(1895.8236), Bit/dim 3.7473(best: 3.7563), Xent 2.2438, Loss 4.8692, Error 0.3746(best: 0.3703)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3310 | Time 36.8446(35.7260) | Bit/dim 3.7662(3.7508) | Xent 0.1168(0.1223) | Loss 3.8246(3.8120) | Error 0.0378(0.0422) Steps 880(874.74) | Grad Norm 3.5212(4.5290) | Total Time 14.00(14.00)\n",
      "Iter 3320 | Time 36.4137(35.6609) | Bit/dim 3.7341(3.7484) | Xent 0.1020(0.1211) | Loss 3.7851(3.8090) | Error 0.0378(0.0417) Steps 892(872.31) | Grad Norm 4.4192(4.6209) | Total Time 14.00(14.00)\n",
      "Iter 3330 | Time 34.8659(35.7332) | Bit/dim 3.7272(3.7502) | Xent 0.1127(0.1214) | Loss 3.7836(3.8109) | Error 0.0378(0.0416) Steps 880(874.19) | Grad Norm 4.5297(4.6738) | Total Time 14.00(14.00)\n",
      "Iter 3340 | Time 36.7360(36.0406) | Bit/dim 3.7346(3.7476) | Xent 0.1390(0.1202) | Loss 3.8041(3.8077) | Error 0.0511(0.0418) Steps 880(876.21) | Grad Norm 3.4092(4.4766) | Total Time 14.00(14.00)\n",
      "Iter 3350 | Time 33.8576(35.8111) | Bit/dim 3.7206(3.7456) | Xent 0.1434(0.1226) | Loss 3.7923(3.8069) | Error 0.0444(0.0429) Steps 868(874.45) | Grad Norm 3.8263(4.4001) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0061 | Time 130.4289, Epoch Time 2121.1390(1902.5831), Bit/dim 3.7452(best: 3.7473), Xent 2.1973, Loss 4.8438, Error 0.3762(best: 0.3703)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3360 | Time 36.1979(35.8658) | Bit/dim 3.7456(3.7451) | Xent 0.1208(0.1209) | Loss 3.8060(3.8056) | Error 0.0444(0.0417) Steps 898(880.19) | Grad Norm 3.7907(4.2602) | Total Time 14.00(14.00)\n",
      "Iter 3370 | Time 36.6390(36.0938) | Bit/dim 3.7379(3.7431) | Xent 0.1408(0.1171) | Loss 3.8084(3.8016) | Error 0.0556(0.0404) Steps 874(880.80) | Grad Norm 5.2487(4.2789) | Total Time 14.00(14.00)\n",
      "Iter 3380 | Time 35.0342(36.0758) | Bit/dim 3.7447(3.7421) | Xent 0.0971(0.1146) | Loss 3.7932(3.7994) | Error 0.0367(0.0395) Steps 892(881.71) | Grad Norm 2.8285(4.3368) | Total Time 14.00(14.00)\n",
      "Iter 3390 | Time 36.4986(36.2130) | Bit/dim 3.7452(3.7433) | Xent 0.1270(0.1132) | Loss 3.8087(3.7999) | Error 0.0456(0.0388) Steps 886(883.95) | Grad Norm 4.0037(4.0995) | Total Time 14.00(14.00)\n",
      "Iter 3400 | Time 36.0831(36.1675) | Bit/dim 3.7187(3.7421) | Xent 0.1323(0.1145) | Loss 3.7848(3.7993) | Error 0.0511(0.0397) Steps 892(883.28) | Grad Norm 5.2174(4.3344) | Total Time 14.00(14.00)\n",
      "Iter 3410 | Time 35.9358(36.1614) | Bit/dim 3.7373(3.7409) | Xent 0.0944(0.1123) | Loss 3.7845(3.7970) | Error 0.0344(0.0390) Steps 856(882.36) | Grad Norm 3.6561(4.2704) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0062 | Time 129.8386, Epoch Time 2144.8078(1909.8498), Bit/dim 3.7366(best: 3.7452), Xent 2.2939, Loss 4.8836, Error 0.3790(best: 0.3703)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3420 | Time 36.6717(36.1019) | Bit/dim 3.7187(3.7367) | Xent 0.0833(0.1048) | Loss 3.7604(3.7891) | Error 0.0278(0.0362) Steps 868(881.30) | Grad Norm 3.9804(4.0169) | Total Time 14.00(14.00)\n",
      "Iter 3430 | Time 37.4735(36.0593) | Bit/dim 3.7005(3.7357) | Xent 0.0610(0.0987) | Loss 3.7311(3.7851) | Error 0.0189(0.0343) Steps 862(878.10) | Grad Norm 2.9703(3.8451) | Total Time 14.00(14.00)\n",
      "Iter 3440 | Time 38.3681(36.2924) | Bit/dim 3.7349(3.7334) | Xent 0.0992(0.0962) | Loss 3.7845(3.7815) | Error 0.0356(0.0334) Steps 892(879.66) | Grad Norm 3.6029(3.8393) | Total Time 14.00(14.00)\n",
      "Iter 3450 | Time 36.2886(36.2796) | Bit/dim 3.7426(3.7334) | Xent 0.1023(0.0945) | Loss 3.7937(3.7806) | Error 0.0300(0.0321) Steps 892(883.20) | Grad Norm 4.6244(3.7675) | Total Time 14.00(14.00)\n",
      "Iter 3460 | Time 35.8575(36.1702) | Bit/dim 3.7077(3.7304) | Xent 0.1033(0.0948) | Loss 3.7593(3.7778) | Error 0.0367(0.0322) Steps 880(882.24) | Grad Norm 3.7353(3.8463) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0063 | Time 129.3555, Epoch Time 2137.3582(1916.6750), Bit/dim 3.7283(best: 3.7366), Xent 2.3286, Loss 4.8927, Error 0.3765(best: 0.3703)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3470 | Time 35.9855(36.1221) | Bit/dim 3.7238(3.7301) | Xent 0.0931(0.0921) | Loss 3.7703(3.7762) | Error 0.0267(0.0310) Steps 892(882.08) | Grad Norm 3.0647(3.6111) | Total Time 14.00(14.00)\n",
      "Iter 3480 | Time 36.7717(36.1450) | Bit/dim 3.7434(3.7302) | Xent 0.1024(0.0891) | Loss 3.7946(3.7747) | Error 0.0333(0.0301) Steps 892(883.61) | Grad Norm 4.0797(3.4838) | Total Time 14.00(14.00)\n",
      "Iter 3490 | Time 37.2906(36.3262) | Bit/dim 3.7116(3.7285) | Xent 0.0733(0.0883) | Loss 3.7483(3.7726) | Error 0.0267(0.0299) Steps 874(881.95) | Grad Norm 3.3011(3.5490) | Total Time 14.00(14.00)\n",
      "Iter 3500 | Time 38.0472(36.1422) | Bit/dim 3.7590(3.7283) | Xent 0.0689(0.0899) | Loss 3.7935(3.7733) | Error 0.0222(0.0306) Steps 898(880.44) | Grad Norm 2.9548(3.6775) | Total Time 14.00(14.00)\n",
      "Iter 3510 | Time 36.7074(36.1830) | Bit/dim 3.7239(3.7284) | Xent 0.0929(0.0886) | Loss 3.7704(3.7727) | Error 0.0367(0.0304) Steps 898(883.48) | Grad Norm 3.1318(3.5901) | Total Time 14.00(14.00)\n",
      "Iter 3520 | Time 37.0719(36.1174) | Bit/dim 3.7179(3.7278) | Xent 0.1141(0.0936) | Loss 3.7749(3.7746) | Error 0.0389(0.0320) Steps 904(881.46) | Grad Norm 3.1823(3.8540) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0064 | Time 131.0988, Epoch Time 2137.9517(1923.3133), Bit/dim 3.7336(best: 3.7283), Xent 2.2654, Loss 4.8663, Error 0.3813(best: 0.3703)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3530 | Time 37.7876(36.2675) | Bit/dim 3.7659(3.7286) | Xent 0.0913(0.0916) | Loss 3.8115(3.7744) | Error 0.0344(0.0318) Steps 898(882.52) | Grad Norm 3.7600(3.7320) | Total Time 14.00(14.00)\n",
      "Iter 3540 | Time 37.1569(36.4384) | Bit/dim 3.7098(3.7252) | Xent 0.0980(0.0892) | Loss 3.7588(3.7698) | Error 0.0367(0.0315) Steps 898(884.88) | Grad Norm 3.8683(3.6151) | Total Time 14.00(14.00)\n",
      "Iter 3550 | Time 36.2136(36.4661) | Bit/dim 3.7402(3.7241) | Xent 0.1007(0.0890) | Loss 3.7905(3.7686) | Error 0.0422(0.0313) Steps 868(885.14) | Grad Norm 4.1081(3.6220) | Total Time 14.00(14.00)\n",
      "Iter 3560 | Time 36.5824(36.3205) | Bit/dim 3.6998(3.7226) | Xent 0.0858(0.0863) | Loss 3.7426(3.7658) | Error 0.0278(0.0300) Steps 898(883.05) | Grad Norm 5.2220(3.5898) | Total Time 14.00(14.00)\n",
      "Iter 3570 | Time 35.7800(36.2382) | Bit/dim 3.7128(3.7222) | Xent 0.0959(0.0878) | Loss 3.7608(3.7660) | Error 0.0311(0.0302) Steps 868(881.97) | Grad Norm 2.5749(3.5492) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0065 | Time 131.4645, Epoch Time 2152.9045(1930.2011), Bit/dim 3.7234(best: 3.7283), Xent 2.2780, Loss 4.8624, Error 0.3832(best: 0.3703)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3580 | Time 35.8744(36.2547) | Bit/dim 3.7016(3.7221) | Xent 0.0828(0.0879) | Loss 3.7430(3.7660) | Error 0.0278(0.0299) Steps 886(883.85) | Grad Norm 4.1253(3.6183) | Total Time 14.00(14.00)\n",
      "Iter 3590 | Time 36.1529(36.3473) | Bit/dim 3.7277(3.7259) | Xent 0.1176(0.0879) | Loss 3.7865(3.7699) | Error 0.0433(0.0298) Steps 886(880.56) | Grad Norm 4.5834(3.8106) | Total Time 14.00(14.00)\n",
      "Iter 3600 | Time 36.4634(36.2043) | Bit/dim 3.7198(3.7270) | Xent 0.0858(0.0939) | Loss 3.7627(3.7739) | Error 0.0333(0.0325) Steps 892(878.95) | Grad Norm 4.3322(4.1690) | Total Time 14.00(14.00)\n",
      "Iter 3610 | Time 36.4850(36.0983) | Bit/dim 3.7318(3.7273) | Xent 0.1057(0.0935) | Loss 3.7846(3.7741) | Error 0.0378(0.0325) Steps 886(879.72) | Grad Norm 4.4506(4.2810) | Total Time 14.00(14.00)\n",
      "Iter 3620 | Time 36.6531(36.1713) | Bit/dim 3.7231(3.7258) | Xent 0.1803(0.0990) | Loss 3.8133(3.7753) | Error 0.0567(0.0347) Steps 904(883.39) | Grad Norm 6.6021(4.5187) | Total Time 14.00(14.00)\n",
      "Iter 3630 | Time 34.7815(36.0910) | Bit/dim 3.7343(3.7267) | Xent 0.1075(0.1026) | Loss 3.7881(3.7780) | Error 0.0344(0.0363) Steps 856(883.12) | Grad Norm 3.2458(4.5732) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0066 | Time 129.1368, Epoch Time 2134.7021(1936.3361), Bit/dim 3.7370(best: 3.7234), Xent 2.3065, Loss 4.8903, Error 0.3807(best: 0.3703)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3640 | Time 35.8540(36.2061) | Bit/dim 3.7012(3.7263) | Xent 0.0755(0.0995) | Loss 3.7389(3.7760) | Error 0.0256(0.0352) Steps 892(882.98) | Grad Norm 2.2343(4.3020) | Total Time 14.00(14.00)\n",
      "Iter 3650 | Time 36.7277(36.2321) | Bit/dim 3.7225(3.7234) | Xent 0.0815(0.0974) | Loss 3.7633(3.7721) | Error 0.0278(0.0347) Steps 898(885.31) | Grad Norm 4.2035(4.6005) | Total Time 14.00(14.00)\n",
      "Iter 3660 | Time 37.5575(36.2503) | Bit/dim 3.7196(3.7246) | Xent 0.0843(0.0991) | Loss 3.7618(3.7741) | Error 0.0267(0.0350) Steps 892(886.11) | Grad Norm 3.9554(4.7498) | Total Time 14.00(14.00)\n",
      "Iter 3670 | Time 36.8789(36.2060) | Bit/dim 3.7707(3.7258) | Xent 0.1097(0.1027) | Loss 3.8256(3.7771) | Error 0.0444(0.0366) Steps 886(886.99) | Grad Norm 4.1414(4.7877) | Total Time 14.00(14.00)\n",
      "Iter 3680 | Time 35.3273(36.1635) | Bit/dim 3.7458(3.7278) | Xent 0.1058(0.1009) | Loss 3.7987(3.7782) | Error 0.0333(0.0359) Steps 880(889.84) | Grad Norm 3.7008(4.6315) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0067 | Time 135.8188, Epoch Time 2146.4449(1942.6394), Bit/dim 3.7246(best: 3.7234), Xent 2.4158, Loss 4.9325, Error 0.3753(best: 0.3703)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3690 | Time 36.9010(36.3198) | Bit/dim 3.7623(3.7272) | Xent 0.0699(0.0993) | Loss 3.7973(3.7769) | Error 0.0244(0.0352) Steps 892(891.28) | Grad Norm 3.7382(4.6238) | Total Time 14.00(14.00)\n",
      "Iter 3700 | Time 36.5376(36.2797) | Bit/dim 3.6958(3.7258) | Xent 0.0665(0.0964) | Loss 3.7291(3.7740) | Error 0.0189(0.0339) Steps 874(888.09) | Grad Norm 2.6347(4.5270) | Total Time 14.00(14.00)\n",
      "Iter 3710 | Time 36.5396(36.3928) | Bit/dim 3.7272(3.7224) | Xent 0.1052(0.0944) | Loss 3.7798(3.7696) | Error 0.0356(0.0328) Steps 898(889.82) | Grad Norm 2.8947(4.4779) | Total Time 14.00(14.00)\n",
      "Iter 3720 | Time 36.2813(36.2623) | Bit/dim 3.7235(3.7229) | Xent 0.1012(0.0943) | Loss 3.7741(3.7700) | Error 0.0367(0.0329) Steps 922(889.75) | Grad Norm 6.9821(4.4962) | Total Time 14.00(14.00)\n",
      "Iter 3730 | Time 35.9116(36.2379) | Bit/dim 3.7020(3.7235) | Xent 0.0798(0.0946) | Loss 3.7419(3.7708) | Error 0.0233(0.0325) Steps 880(891.39) | Grad Norm 3.6645(4.5616) | Total Time 14.00(14.00)\n",
      "Iter 3740 | Time 37.9112(36.2889) | Bit/dim 3.7186(3.7211) | Xent 0.0926(0.0979) | Loss 3.7649(3.7700) | Error 0.0333(0.0339) Steps 898(887.85) | Grad Norm 3.6023(4.5787) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0068 | Time 134.8223, Epoch Time 2154.8629(1949.0061), Bit/dim 3.7205(best: 3.7234), Xent 2.5260, Loss 4.9835, Error 0.3914(best: 0.3703)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3750 | Time 34.9562(36.0718) | Bit/dim 3.7168(3.7185) | Xent 0.0930(0.0949) | Loss 3.7633(3.7659) | Error 0.0333(0.0327) Steps 868(884.93) | Grad Norm 6.1330(4.4506) | Total Time 14.00(14.00)\n",
      "Iter 3760 | Time 37.0095(36.1536) | Bit/dim 3.7114(3.7192) | Xent 0.0767(0.0927) | Loss 3.7497(3.7656) | Error 0.0322(0.0318) Steps 922(884.90) | Grad Norm 5.0034(4.3233) | Total Time 14.00(14.00)\n",
      "Iter 3770 | Time 36.9679(36.2793) | Bit/dim 3.7245(3.7177) | Xent 0.1108(0.0934) | Loss 3.7799(3.7643) | Error 0.0344(0.0321) Steps 910(887.52) | Grad Norm 5.0681(4.2563) | Total Time 14.00(14.00)\n",
      "Iter 3780 | Time 35.8351(36.2849) | Bit/dim 3.6878(3.7159) | Xent 0.0695(0.0922) | Loss 3.7225(3.7620) | Error 0.0278(0.0319) Steps 880(888.76) | Grad Norm 3.6672(4.1049) | Total Time 14.00(14.00)\n",
      "Iter 3790 | Time 38.2135(36.3181) | Bit/dim 3.7267(3.7186) | Xent 0.0839(0.0929) | Loss 3.7687(3.7650) | Error 0.0289(0.0322) Steps 880(888.79) | Grad Norm 4.8516(4.0937) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0069 | Time 129.1338, Epoch Time 2142.8977(1954.8228), Bit/dim 3.7214(best: 3.7205), Xent 2.3320, Loss 4.8874, Error 0.3762(best: 0.3703)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3800 | Time 36.8018(36.3715) | Bit/dim 3.7320(3.7175) | Xent 0.0768(0.0899) | Loss 3.7704(3.7625) | Error 0.0267(0.0308) Steps 880(887.02) | Grad Norm 2.9387(3.8842) | Total Time 14.00(14.00)\n",
      "Iter 3810 | Time 36.4759(36.2988) | Bit/dim 3.7163(3.7162) | Xent 0.0660(0.0856) | Loss 3.7493(3.7590) | Error 0.0200(0.0293) Steps 904(886.83) | Grad Norm 2.8804(3.6472) | Total Time 14.00(14.00)\n",
      "Iter 3820 | Time 36.0410(36.1697) | Bit/dim 3.7151(3.7143) | Xent 0.0773(0.0840) | Loss 3.7538(3.7563) | Error 0.0256(0.0286) Steps 892(885.23) | Grad Norm 4.8519(3.6059) | Total Time 14.00(14.00)\n",
      "Iter 3830 | Time 35.5728(36.1873) | Bit/dim 3.7236(3.7122) | Xent 0.0779(0.0836) | Loss 3.7625(3.7540) | Error 0.0289(0.0283) Steps 892(887.06) | Grad Norm 2.7121(3.5212) | Total Time 14.00(14.00)\n",
      "Iter 3840 | Time 37.8877(36.2993) | Bit/dim 3.6836(3.7122) | Xent 0.0941(0.0855) | Loss 3.7306(3.7550) | Error 0.0289(0.0290) Steps 904(887.31) | Grad Norm 4.6613(3.5269) | Total Time 14.00(14.00)\n",
      "Iter 3850 | Time 35.5610(36.3303) | Bit/dim 3.7138(3.7114) | Xent 0.0972(0.0863) | Loss 3.7624(3.7546) | Error 0.0367(0.0298) Steps 874(888.39) | Grad Norm 3.2172(3.5564) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0070 | Time 128.4414, Epoch Time 2140.4149(1960.3906), Bit/dim 3.7176(best: 3.7205), Xent 2.4659, Loss 4.9505, Error 0.3782(best: 0.3703)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3860 | Time 36.6888(36.3076) | Bit/dim 3.6940(3.7076) | Xent 0.0721(0.0830) | Loss 3.7301(3.7491) | Error 0.0256(0.0287) Steps 892(887.85) | Grad Norm 2.6157(3.5203) | Total Time 14.00(14.00)\n",
      "Iter 3870 | Time 36.1132(36.2916) | Bit/dim 3.7198(3.7058) | Xent 0.0757(0.0794) | Loss 3.7577(3.7455) | Error 0.0256(0.0276) Steps 874(888.18) | Grad Norm 4.2069(3.5379) | Total Time 14.00(14.00)\n",
      "Iter 3880 | Time 34.7031(36.1059) | Bit/dim 3.7133(3.7055) | Xent 0.0574(0.0754) | Loss 3.7420(3.7432) | Error 0.0200(0.0261) Steps 862(882.90) | Grad Norm 2.3151(3.3868) | Total Time 14.00(14.00)\n",
      "Iter 3890 | Time 37.9112(36.1181) | Bit/dim 3.7381(3.7071) | Xent 0.0832(0.0755) | Loss 3.7797(3.7448) | Error 0.0300(0.0261) Steps 898(882.52) | Grad Norm 3.2146(3.5065) | Total Time 14.00(14.00)\n",
      "Iter 3900 | Time 36.5608(36.2431) | Bit/dim 3.7171(3.7077) | Xent 0.0796(0.0762) | Loss 3.7569(3.7458) | Error 0.0289(0.0261) Steps 886(883.21) | Grad Norm 4.4905(3.5767) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0071 | Time 131.5558, Epoch Time 2142.3499(1965.8494), Bit/dim 3.7119(best: 3.7176), Xent 2.4586, Loss 4.9412, Error 0.3785(best: 0.3703)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3910 | Time 35.4264(36.1782) | Bit/dim 3.7321(3.7069) | Xent 0.0689(0.0744) | Loss 3.7666(3.7442) | Error 0.0211(0.0252) Steps 886(882.25) | Grad Norm 3.9503(3.4981) | Total Time 14.00(14.00)\n",
      "Iter 3920 | Time 35.5783(36.0620) | Bit/dim 3.6914(3.7055) | Xent 0.0796(0.0732) | Loss 3.7312(3.7421) | Error 0.0300(0.0250) Steps 880(880.05) | Grad Norm 3.9004(3.6129) | Total Time 14.00(14.00)\n",
      "Iter 3930 | Time 35.2073(35.8608) | Bit/dim 3.6740(3.7032) | Xent 0.0767(0.0722) | Loss 3.7124(3.7393) | Error 0.0256(0.0247) Steps 862(877.85) | Grad Norm 3.5328(3.5655) | Total Time 14.00(14.00)\n",
      "Iter 3940 | Time 36.9752(35.8230) | Bit/dim 3.7226(3.7050) | Xent 0.1048(0.0759) | Loss 3.7750(3.7429) | Error 0.0422(0.0259) Steps 886(877.86) | Grad Norm 3.5843(3.6673) | Total Time 14.00(14.00)\n",
      "Iter 3950 | Time 35.1080(35.8666) | Bit/dim 3.7214(3.7053) | Xent 0.0697(0.0809) | Loss 3.7562(3.7457) | Error 0.0256(0.0277) Steps 886(878.40) | Grad Norm 3.3940(3.9755) | Total Time 14.00(14.00)\n",
      "Iter 3960 | Time 34.9949(35.9611) | Bit/dim 3.7061(3.7069) | Xent 0.0950(0.0820) | Loss 3.7536(3.7479) | Error 0.0356(0.0284) Steps 874(877.87) | Grad Norm 4.1982(3.9783) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0072 | Time 129.9107, Epoch Time 2116.0419(1970.3551), Bit/dim 3.7112(best: 3.7119), Xent 2.5389, Loss 4.9807, Error 0.3941(best: 0.3703)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3970 | Time 35.4806(35.8807) | Bit/dim 3.6884(3.7081) | Xent 0.0464(0.0788) | Loss 3.7116(3.7475) | Error 0.0156(0.0272) Steps 862(880.18) | Grad Norm 2.4654(3.9407) | Total Time 14.00(14.00)\n",
      "Iter 3980 | Time 34.5157(35.8635) | Bit/dim 3.7219(3.7072) | Xent 0.0764(0.0777) | Loss 3.7601(3.7461) | Error 0.0278(0.0271) Steps 856(880.08) | Grad Norm 3.0187(3.7764) | Total Time 14.00(14.00)\n",
      "Iter 3990 | Time 35.0239(35.8415) | Bit/dim 3.7131(3.7022) | Xent 0.1354(0.0786) | Loss 3.7808(3.7414) | Error 0.0433(0.0273) Steps 874(878.61) | Grad Norm 7.2768(4.0062) | Total Time 14.00(14.00)\n",
      "Iter 4000 | Time 35.5090(35.9967) | Bit/dim 3.6916(3.7002) | Xent 0.0774(0.0812) | Loss 3.7303(3.7408) | Error 0.0267(0.0281) Steps 880(878.63) | Grad Norm 3.4473(4.1673) | Total Time 14.00(14.00)\n",
      "Iter 4010 | Time 34.7539(35.8097) | Bit/dim 3.6972(3.7018) | Xent 0.0805(0.0807) | Loss 3.7374(3.7421) | Error 0.0344(0.0284) Steps 856(876.26) | Grad Norm 5.0654(4.0678) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0073 | Time 130.4363, Epoch Time 2118.9353(1974.8126), Bit/dim 3.7087(best: 3.7112), Xent 2.6229, Loss 5.0201, Error 0.3855(best: 0.3703)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4020 | Time 35.4410(35.7633) | Bit/dim 3.6720(3.7040) | Xent 0.0724(0.0791) | Loss 3.7082(3.7435) | Error 0.0244(0.0274) Steps 868(875.77) | Grad Norm 3.7302(4.0577) | Total Time 14.00(14.00)\n",
      "Iter 4030 | Time 35.6387(35.8966) | Bit/dim 3.7029(3.7013) | Xent 0.0571(0.0761) | Loss 3.7315(3.7394) | Error 0.0178(0.0261) Steps 904(878.21) | Grad Norm 2.7124(3.7917) | Total Time 14.00(14.00)\n",
      "Iter 4040 | Time 34.9793(35.9868) | Bit/dim 3.6765(3.7000) | Xent 0.0817(0.0750) | Loss 3.7174(3.7376) | Error 0.0278(0.0257) Steps 868(881.46) | Grad Norm 2.1492(3.4858) | Total Time 14.00(14.00)\n",
      "Iter 4050 | Time 37.1165(36.0962) | Bit/dim 3.6752(3.6980) | Xent 0.0881(0.0731) | Loss 3.7193(3.7345) | Error 0.0278(0.0253) Steps 880(879.96) | Grad Norm 2.5239(3.3992) | Total Time 14.00(14.00)\n",
      "Iter 4060 | Time 35.8663(35.8821) | Bit/dim 3.6956(3.6990) | Xent 0.0589(0.0735) | Loss 3.7250(3.7357) | Error 0.0156(0.0255) Steps 874(876.72) | Grad Norm 2.5196(3.4720) | Total Time 14.00(14.00)\n",
      "Iter 4070 | Time 36.2052(36.1030) | Bit/dim 3.7046(3.6979) | Xent 0.0929(0.0771) | Loss 3.7511(3.7365) | Error 0.0389(0.0267) Steps 868(879.76) | Grad Norm 5.1543(3.5564) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0074 | Time 129.7791, Epoch Time 2136.7430(1979.6705), Bit/dim 3.7025(best: 3.7087), Xent 2.5848, Loss 4.9949, Error 0.3857(best: 0.3703)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4080 | Time 35.6238(35.9596) | Bit/dim 3.7113(3.7008) | Xent 0.1009(0.0798) | Loss 3.7617(3.7407) | Error 0.0367(0.0274) Steps 862(878.80) | Grad Norm 4.7640(4.0570) | Total Time 14.00(14.00)\n",
      "Iter 4090 | Time 34.9980(35.8069) | Bit/dim 3.6930(3.7011) | Xent 0.0740(0.0792) | Loss 3.7300(3.7407) | Error 0.0244(0.0274) Steps 874(878.45) | Grad Norm 4.2145(3.9764) | Total Time 14.00(14.00)\n",
      "Iter 4100 | Time 35.4590(35.8248) | Bit/dim 3.7292(3.7016) | Xent 0.0873(0.0750) | Loss 3.7729(3.7392) | Error 0.0300(0.0260) Steps 850(876.13) | Grad Norm 2.9074(3.6202) | Total Time 14.00(14.00)\n",
      "Iter 4110 | Time 34.9522(35.7090) | Bit/dim 3.6938(3.6985) | Xent 0.0696(0.0715) | Loss 3.7285(3.7343) | Error 0.0256(0.0249) Steps 862(874.89) | Grad Norm 3.1325(3.3515) | Total Time 14.00(14.00)\n",
      "Iter 4120 | Time 36.3037(35.6282) | Bit/dim 3.7046(3.6953) | Xent 0.0834(0.0742) | Loss 3.7463(3.7324) | Error 0.0322(0.0261) Steps 856(876.12) | Grad Norm 5.9583(3.6592) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0075 | Time 130.0543, Epoch Time 2102.2166(1983.3469), Bit/dim 3.7002(best: 3.7025), Xent 2.5541, Loss 4.9773, Error 0.3788(best: 0.3703)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4130 | Time 36.6657(35.5774) | Bit/dim 3.6860(3.6936) | Xent 0.0547(0.0741) | Loss 3.7134(3.7306) | Error 0.0133(0.0262) Steps 886(875.34) | Grad Norm 2.3887(3.6025) | Total Time 14.00(14.00)\n",
      "Iter 4140 | Time 35.5490(35.5272) | Bit/dim 3.6874(3.6930) | Xent 0.0773(0.0700) | Loss 3.7261(3.7280) | Error 0.0233(0.0244) Steps 874(876.48) | Grad Norm 2.7483(3.3600) | Total Time 14.00(14.00)\n",
      "Iter 4150 | Time 35.5170(35.6788) | Bit/dim 3.7118(3.6924) | Xent 0.0725(0.0701) | Loss 3.7481(3.7274) | Error 0.0244(0.0246) Steps 874(877.10) | Grad Norm 4.9303(3.4618) | Total Time 14.00(14.00)\n",
      "Iter 4160 | Time 35.2504(35.6660) | Bit/dim 3.7041(3.6891) | Xent 0.1029(0.0745) | Loss 3.7555(3.7263) | Error 0.0367(0.0263) Steps 862(880.17) | Grad Norm 3.2832(3.5092) | Total Time 14.00(14.00)\n",
      "Iter 4170 | Time 36.0124(35.6878) | Bit/dim 3.6886(3.6942) | Xent 0.1071(0.0768) | Loss 3.7422(3.7326) | Error 0.0400(0.0270) Steps 868(878.42) | Grad Norm 4.5189(3.5983) | Total Time 14.00(14.00)\n",
      "Iter 4180 | Time 34.8248(35.6521) | Bit/dim 3.7148(3.6977) | Xent 0.2792(0.0924) | Loss 3.8545(3.7439) | Error 0.0900(0.0322) Steps 832(878.59) | Grad Norm 12.4699(4.6333) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0076 | Time 129.5594, Epoch Time 2110.2243(1987.1532), Bit/dim 3.7163(best: 3.7002), Xent 2.6569, Loss 5.0447, Error 0.3771(best: 0.3703)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4190 | Time 34.9344(35.6051) | Bit/dim 3.6911(3.7019) | Xent 0.1311(0.1067) | Loss 3.7566(3.7553) | Error 0.0500(0.0374) Steps 880(882.06) | Grad Norm 4.3949(4.9433) | Total Time 14.00(14.00)\n",
      "Iter 4200 | Time 35.2542(35.8834) | Bit/dim 3.7718(3.7074) | Xent 0.1468(0.1120) | Loss 3.8452(3.7634) | Error 0.0522(0.0399) Steps 886(885.21) | Grad Norm 9.2277(5.2175) | Total Time 14.00(14.00)\n",
      "Iter 4210 | Time 36.0918(35.8342) | Bit/dim 3.7110(3.7069) | Xent 0.0923(0.1091) | Loss 3.7572(3.7615) | Error 0.0356(0.0390) Steps 886(886.63) | Grad Norm 4.2463(5.0057) | Total Time 14.00(14.00)\n",
      "Iter 4220 | Time 35.1309(35.8691) | Bit/dim 3.7031(3.7090) | Xent 0.1029(0.1068) | Loss 3.7546(3.7624) | Error 0.0433(0.0384) Steps 880(887.81) | Grad Norm 4.0580(4.7327) | Total Time 14.00(14.00)\n",
      "Iter 4230 | Time 35.3826(35.7808) | Bit/dim 3.6699(3.7078) | Xent 0.1078(0.1041) | Loss 3.7238(3.7598) | Error 0.0356(0.0372) Steps 874(885.55) | Grad Norm 3.4081(4.6193) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0077 | Time 130.5767, Epoch Time 2122.0869(1991.2012), Bit/dim 3.7055(best: 3.7002), Xent 2.7197, Loss 5.0653, Error 0.3837(best: 0.3703)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4240 | Time 36.1475(35.7518) | Bit/dim 3.6902(3.7042) | Xent 0.0863(0.0998) | Loss 3.7333(3.7540) | Error 0.0322(0.0360) Steps 874(885.20) | Grad Norm 4.7156(4.5212) | Total Time 14.00(14.00)\n",
      "Iter 4250 | Time 34.5849(35.8930) | Bit/dim 3.6661(3.7001) | Xent 0.0672(0.0930) | Loss 3.6997(3.7466) | Error 0.0222(0.0331) Steps 868(880.94) | Grad Norm 2.9954(4.1031) | Total Time 14.00(14.00)\n",
      "Iter 4260 | Time 35.7040(35.7501) | Bit/dim 3.7222(3.6989) | Xent 0.0663(0.0864) | Loss 3.7554(3.7421) | Error 0.0244(0.0305) Steps 862(876.43) | Grad Norm 3.2554(3.8165) | Total Time 14.00(14.00)\n",
      "Iter 4270 | Time 36.5695(35.6279) | Bit/dim 3.6700(3.6936) | Xent 0.0625(0.0815) | Loss 3.7013(3.7343) | Error 0.0189(0.0287) Steps 862(873.14) | Grad Norm 3.1821(3.5545) | Total Time 14.00(14.00)\n",
      "Iter 4280 | Time 35.0672(35.6042) | Bit/dim 3.6701(3.6892) | Xent 0.0964(0.0802) | Loss 3.7183(3.7293) | Error 0.0322(0.0280) Steps 874(874.63) | Grad Norm 2.7377(3.4123) | Total Time 14.00(14.00)\n",
      "Iter 4290 | Time 35.0864(35.5366) | Bit/dim 3.7213(3.6929) | Xent 0.0814(0.0802) | Loss 3.7620(3.7329) | Error 0.0267(0.0281) Steps 874(877.36) | Grad Norm 2.6901(3.4249) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0078 | Time 128.6382, Epoch Time 2103.9016(1994.5822), Bit/dim 3.6911(best: 3.7002), Xent 2.4647, Loss 4.9234, Error 0.3676(best: 0.3703)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4300 | Time 34.3444(35.7451) | Bit/dim 3.6949(3.6914) | Xent 0.0760(0.0766) | Loss 3.7329(3.7297) | Error 0.0267(0.0263) Steps 880(877.23) | Grad Norm 4.0520(3.3312) | Total Time 14.00(14.00)\n",
      "Iter 4310 | Time 36.0088(35.8586) | Bit/dim 3.6815(3.6891) | Xent 0.0719(0.0741) | Loss 3.7174(3.7261) | Error 0.0222(0.0255) Steps 886(876.20) | Grad Norm 2.5784(3.4001) | Total Time 14.00(14.00)\n",
      "Iter 4320 | Time 34.6499(35.7327) | Bit/dim 3.6804(3.6880) | Xent 0.0659(0.0732) | Loss 3.7133(3.7246) | Error 0.0211(0.0248) Steps 874(875.22) | Grad Norm 2.2084(3.1935) | Total Time 14.00(14.00)\n",
      "Iter 4330 | Time 35.1884(35.6596) | Bit/dim 3.6685(3.6878) | Xent 0.0635(0.0715) | Loss 3.7002(3.7235) | Error 0.0211(0.0244) Steps 868(875.11) | Grad Norm 3.4579(3.1370) | Total Time 14.00(14.00)\n",
      "Iter 4340 | Time 34.9871(35.6313) | Bit/dim 3.6899(3.6888) | Xent 0.0597(0.0731) | Loss 3.7197(3.7253) | Error 0.0256(0.0249) Steps 862(875.24) | Grad Norm 2.6568(3.1968) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0079 | Time 128.4619, Epoch Time 2112.8268(1998.1295), Bit/dim 3.6881(best: 3.6911), Xent 2.5622, Loss 4.9692, Error 0.3765(best: 0.3676)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4350 | Time 35.8530(35.7277) | Bit/dim 3.6708(3.6893) | Xent 0.0490(0.0693) | Loss 3.6953(3.7239) | Error 0.0156(0.0233) Steps 856(876.65) | Grad Norm 2.7408(3.1061) | Total Time 14.00(14.00)\n",
      "Iter 4360 | Time 36.0516(35.7411) | Bit/dim 3.6761(3.6873) | Xent 0.0556(0.0677) | Loss 3.7039(3.7212) | Error 0.0189(0.0229) Steps 868(877.60) | Grad Norm 2.5576(3.2776) | Total Time 14.00(14.00)\n",
      "Iter 4370 | Time 36.2098(35.9222) | Bit/dim 3.6767(3.6870) | Xent 0.0467(0.0676) | Loss 3.7001(3.7207) | Error 0.0167(0.0229) Steps 886(878.01) | Grad Norm 2.8150(3.3788) | Total Time 14.00(14.00)\n",
      "Iter 4380 | Time 36.5270(35.9729) | Bit/dim 3.6954(3.6877) | Xent 0.0444(0.0639) | Loss 3.7176(3.7197) | Error 0.0167(0.0218) Steps 874(877.88) | Grad Norm 2.0447(3.2091) | Total Time 14.00(14.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_cifar.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/cnf_conditional_cifar10_bs900_run1 --seed 1 --conditional True --controlled_tol False --train_mode semisup --log_freq 10 --weight_y 0.5\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
