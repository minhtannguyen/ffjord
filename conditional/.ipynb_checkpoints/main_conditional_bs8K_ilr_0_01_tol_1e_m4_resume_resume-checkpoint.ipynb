{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_conditional.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.odenvp_conditional as odenvp\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=True, choices=[True, False])\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"./data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"./data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"./data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"./data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"./data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"./data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            './data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            './data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz = modules.GaussianDiag.logp(mean, logs, z).view(-1,1)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(z)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol},)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    # fixed_y = torch.randint(high=10, size=(100,)).type(torch.long).to(device, non_blocking=True)\n",
      "    fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "    fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "    with torch.no_grad():\n",
      "        mean, logs = model.module._prior(fixed_y_onehot)\n",
      "        fixed_z = modules.GaussianDiag.sample(mean, logs)\n",
      "    \n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    if args.conditional: best_error_score = float(\"inf\")\n",
      "    \n",
      "    itr = 0\n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                loss =  loss_nll + args.weight_y * loss_xent\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            loss.backward()\n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            xent_meter.update(loss_xent.item())\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits/dim', {'train': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses_xent.append(loss_xent.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'validation': time.time() - start}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits/dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}, Xent {:.4f}, Loss {:.4f}, Error {:.4f}\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, loss_xent, loss, error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=8000, batch_size_schedule='', begin_epoch=1, conditional=True, conv=True, data='mnist', dims='64,64,64', divergence_fn='approximate', dl2int=None, imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=1, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume='experiments/cnf_cond_bs8K_ilr_0_01_tol_1e_m5_wy_0_5_dev/best_nll_checkpt.pth', rtol=1e-05, save='experiments/cnf_cond_bs8K_ilr_0_01_tol_1e_m5_wy_0_5_dev_cont_lr_0_001', solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=8000, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, val_freq=1, warmup_iters=113.0, weight_decay=0.0, weight_y=0.5)\n",
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=1568, bias=True)\n",
      "  (project_class): LinearZeros(in_features=784, out_features=10, bias=True)\n",
      ")\n",
      "Number of trainable parameters: 828890\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc.py:286: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "Iter 0000 | Time 85.0939(85.0939) | Bit/dim 1.1284(1.1284) | Xent 0.0414(0.0414) | Loss 1.1491(1.1491) | Error 0.0140(0.0140) Steps 590(590.00) | Grad Norm 2.3149(2.3149) | Total Time 10.00(10.00)\n",
      "Iter 0001 | Time 44.2436(83.8684) | Bit/dim 1.1366(1.1287) | Xent 0.0323(0.0411) | Loss 1.1528(1.1492) | Error 0.0108(0.0139) Steps 596(590.18) | Grad Norm 2.2920(2.3142) | Total Time 10.00(10.00)\n",
      "Iter 0002 | Time 41.7815(82.6058) | Bit/dim 1.1355(1.1289) | Xent 0.0362(0.0410) | Loss 1.1536(1.1494) | Error 0.0102(0.0138) Steps 596(590.35) | Grad Norm 2.2218(2.3114) | Total Time 10.00(10.00)\n",
      "Iter 0003 | Time 43.4331(81.4306) | Bit/dim 1.1374(1.1291) | Xent 0.0281(0.0406) | Loss 1.1515(1.1494) | Error 0.0096(0.0137) Steps 584(590.16) | Grad Norm 2.2065(2.3083) | Total Time 10.00(10.00)\n",
      "Iter 0004 | Time 43.6536(80.2973) | Bit/dim 1.1358(1.1293) | Xent 0.0373(0.0405) | Loss 1.1544(1.1496) | Error 0.0118(0.0136) Steps 590(590.16) | Grad Norm 2.0805(2.3015) | Total Time 10.00(10.00)\n",
      "Iter 0005 | Time 43.2278(79.1852) | Bit/dim 1.1332(1.1295) | Xent 0.0385(0.0404) | Loss 1.1525(1.1497) | Error 0.0126(0.0136) Steps 596(590.33) | Grad Norm 2.0440(2.2937) | Total Time 10.00(10.00)\n",
      "Iter 0006 | Time 43.3999(78.1117) | Bit/dim 1.1314(1.1295) | Xent 0.0352(0.0403) | Loss 1.1490(1.1496) | Error 0.0108(0.0135) Steps 590(590.32) | Grad Norm 1.7867(2.2785) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0001 | Time 12.0384, Epoch Time 369.9781(369.9781), Bit/dim 1.1170, Xent 0.0510, Loss 1.1425, Error 0.2130\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0007 | Time 45.6944(77.1392) | Bit/dim 1.1313(1.1296) | Xent 0.0405(0.0403) | Loss 1.1516(1.1497) | Error 0.0128(0.0135) Steps 596(590.49) | Grad Norm 1.6421(2.2594) | Total Time 10.00(10.00)\n",
      "Iter 0008 | Time 43.5425(76.1313) | Bit/dim 1.1248(1.1294) | Xent 0.0384(0.0402) | Loss 1.1440(1.1495) | Error 0.0122(0.0134) Steps 590(590.48) | Grad Norm 1.3149(2.2311) | Total Time 10.00(10.00)\n",
      "Iter 0009 | Time 43.5870(75.1549) | Bit/dim 1.1264(1.1293) | Xent 0.0348(0.0401) | Loss 1.1438(1.1494) | Error 0.0108(0.0134) Steps 590(590.47) | Grad Norm 1.0207(2.1948) | Total Time 10.00(10.00)\n",
      "Iter 0010 | Time 43.1094(74.1936) | Bit/dim 1.1279(1.1293) | Xent 0.0398(0.0400) | Loss 1.1478(1.1493) | Error 0.0129(0.0133) Steps 596(590.63) | Grad Norm 0.6537(2.1485) | Total Time 10.00(10.00)\n",
      "Iter 0011 | Time 43.0374(73.2589) | Bit/dim 1.1257(1.1292) | Xent 0.0384(0.0400) | Loss 1.1449(1.1492) | Error 0.0121(0.0133) Steps 596(590.79) | Grad Norm 0.4737(2.0983) | Total Time 10.00(10.00)\n",
      "Iter 0012 | Time 44.0837(72.3836) | Bit/dim 1.1288(1.1292) | Xent 0.0322(0.0398) | Loss 1.1449(1.1491) | Error 0.0111(0.0132) Steps 602(591.13) | Grad Norm 0.5282(2.0512) | Total Time 10.00(10.00)\n",
      "Iter 0013 | Time 43.4605(71.5159) | Bit/dim 1.1296(1.1292) | Xent 0.0309(0.0395) | Loss 1.1450(1.1489) | Error 0.0088(0.0131) Steps 596(591.27) | Grad Norm 0.8989(2.0166) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0002 | Time 11.8059, Epoch Time 330.9243(368.8065), Bit/dim 1.1128, Xent 0.0539, Loss 1.1398, Error 0.2136\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0014 | Time 42.7821(70.6539) | Bit/dim 1.1281(1.1292) | Xent 0.0395(0.0395) | Loss 1.1479(1.1489) | Error 0.0119(0.0131) Steps 596(591.42) | Grad Norm 1.1813(1.9916) | Total Time 10.00(10.00)\n",
      "Iter 0015 | Time 42.0429(69.7956) | Bit/dim 1.1234(1.1290) | Xent 0.0387(0.0395) | Loss 1.1428(1.1487) | Error 0.0128(0.0131) Steps 596(591.55) | Grad Norm 1.5574(1.9785) | Total Time 10.00(10.00)\n",
      "Iter 0016 | Time 42.9675(68.9907) | Bit/dim 1.1226(1.1288) | Xent 0.0390(0.0395) | Loss 1.1421(1.1485) | Error 0.0134(0.0131) Steps 596(591.69) | Grad Norm 1.7763(1.9725) | Total Time 10.00(10.00)\n",
      "Iter 0017 | Time 43.6403(68.2302) | Bit/dim 1.1256(1.1287) | Xent 0.0348(0.0393) | Loss 1.1430(1.1484) | Error 0.0112(0.0130) Steps 596(591.82) | Grad Norm 1.7761(1.9666) | Total Time 10.00(10.00)\n",
      "Iter 0018 | Time 42.6791(67.4637) | Bit/dim 1.1233(1.1285) | Xent 0.0426(0.0394) | Loss 1.1447(1.1482) | Error 0.0128(0.0130) Steps 596(591.94) | Grad Norm 1.5861(1.9552) | Total Time 10.00(10.00)\n",
      "Iter 0019 | Time 44.1525(66.7644) | Bit/dim 1.1262(1.1285) | Xent 0.0392(0.0394) | Loss 1.1458(1.1482) | Error 0.0115(0.0130) Steps 596(592.06) | Grad Norm 1.2825(1.9350) | Total Time 10.00(10.00)\n",
      "Iter 0020 | Time 44.3023(66.0905) | Bit/dim 1.1313(1.1286) | Xent 0.0317(0.0392) | Loss 1.1471(1.1481) | Error 0.0106(0.0129) Steps 596(592.18) | Grad Norm 0.9860(1.9065) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0003 | Time 12.0846, Epoch Time 327.4600(367.5661), Bit/dim 1.1108, Xent 0.0546, Loss 1.1381, Error 0.2130\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0021 | Time 44.0917(65.4305) | Bit/dim 1.1271(1.1285) | Xent 0.0373(0.0391) | Loss 1.1458(1.1481) | Error 0.0116(0.0129) Steps 596(592.30) | Grad Norm 0.4974(1.8643) | Total Time 10.00(10.00)\n",
      "Iter 0022 | Time 43.4728(64.7718) | Bit/dim 1.1252(1.1284) | Xent 0.0399(0.0391) | Loss 1.1451(1.1480) | Error 0.0139(0.0129) Steps 596(592.41) | Grad Norm 0.3205(1.8179) | Total Time 10.00(10.00)\n",
      "Iter 0023 | Time 44.0833(64.1512) | Bit/dim 1.1215(1.1282) | Xent 0.0346(0.0390) | Loss 1.1388(1.1477) | Error 0.0110(0.0128) Steps 602(592.70) | Grad Norm 0.3816(1.7748) | Total Time 10.00(10.00)\n",
      "Iter 0024 | Time 42.7325(63.5086) | Bit/dim 1.1255(1.1281) | Xent 0.0398(0.0390) | Loss 1.1454(1.1476) | Error 0.0131(0.0128) Steps 596(592.79) | Grad Norm 0.7498(1.7441) | Total Time 10.00(10.00)\n",
      "Iter 0025 | Time 43.4765(62.9076) | Bit/dim 1.1225(1.1279) | Xent 0.0426(0.0391) | Loss 1.1438(1.1475) | Error 0.0138(0.0129) Steps 590(592.71) | Grad Norm 0.9514(1.7203) | Total Time 10.00(10.00)\n",
      "Iter 0026 | Time 44.9632(62.3693) | Bit/dim 1.1273(1.1279) | Xent 0.0368(0.0391) | Loss 1.1457(1.1475) | Error 0.0132(0.0129) Steps 584(592.45) | Grad Norm 0.9975(1.6986) | Total Time 10.00(10.00)\n",
      "Iter 0027 | Time 44.3498(61.8287) | Bit/dim 1.1272(1.1279) | Xent 0.0354(0.0390) | Loss 1.1449(1.1474) | Error 0.0116(0.0128) Steps 596(592.56) | Grad Norm 1.0597(1.6795) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0004 | Time 12.0487, Epoch Time 332.0260(366.4999), Bit/dim 1.1110, Xent 0.0508, Loss 1.1364, Error 0.2130\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0028 | Time 43.7206(61.2855) | Bit/dim 1.1241(1.1278) | Xent 0.0378(0.0389) | Loss 1.1430(1.1473) | Error 0.0124(0.0128) Steps 590(592.48) | Grad Norm 1.0351(1.6601) | Total Time 10.00(10.00)\n",
      "Iter 0029 | Time 44.4790(60.7813) | Bit/dim 1.1280(1.1278) | Xent 0.0345(0.0388) | Loss 1.1452(1.1472) | Error 0.0128(0.0128) Steps 590(592.40) | Grad Norm 0.7482(1.6328) | Total Time 10.00(10.00)\n",
      "Iter 0030 | Time 44.6114(60.2962) | Bit/dim 1.1267(1.1278) | Xent 0.0307(0.0386) | Loss 1.1420(1.1470) | Error 0.0101(0.0127) Steps 590(592.33) | Grad Norm 0.4749(1.5980) | Total Time 10.00(10.00)\n",
      "Iter 0031 | Time 43.7920(59.8010) | Bit/dim 1.1273(1.1278) | Xent 0.0430(0.0387) | Loss 1.1488(1.1471) | Error 0.0132(0.0128) Steps 596(592.44) | Grad Norm 0.1863(1.5557) | Total Time 10.00(10.00)\n",
      "Iter 0032 | Time 45.2374(59.3641) | Bit/dim 1.1270(1.1277) | Xent 0.0337(0.0385) | Loss 1.1439(1.1470) | Error 0.0105(0.0127) Steps 596(592.55) | Grad Norm 0.2519(1.5166) | Total Time 10.00(10.00)\n",
      "Iter 0033 | Time 44.0534(58.9048) | Bit/dim 1.1175(1.1274) | Xent 0.0378(0.0385) | Loss 1.1364(1.1467) | Error 0.0112(0.0126) Steps 590(592.47) | Grad Norm 0.5004(1.4861) | Total Time 10.00(10.00)\n",
      "Iter 0034 | Time 43.4462(58.4411) | Bit/dim 1.1174(1.1271) | Xent 0.0400(0.0386) | Loss 1.1373(1.1464) | Error 0.0138(0.0127) Steps 590(592.40) | Grad Norm 0.7264(1.4633) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0005 | Time 12.0398, Epoch Time 334.1164(365.5284), Bit/dim 1.1098, Xent 0.0525, Loss 1.1361, Error 0.2139\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0035 | Time 43.8675(58.0039) | Bit/dim 1.1236(1.1270) | Xent 0.0384(0.0386) | Loss 1.1428(1.1463) | Error 0.0125(0.0127) Steps 590(592.33) | Grad Norm 0.7026(1.4405) | Total Time 10.00(10.00)\n",
      "Iter 0036 | Time 43.9563(57.5824) | Bit/dim 1.1238(1.1269) | Xent 0.0405(0.0386) | Loss 1.1441(1.1462) | Error 0.0124(0.0127) Steps 596(592.44) | Grad Norm 0.7657(1.4202) | Total Time 10.00(10.00)\n",
      "Iter 0037 | Time 45.4663(57.2189) | Bit/dim 1.1202(1.1267) | Xent 0.0376(0.0386) | Loss 1.1390(1.1460) | Error 0.0110(0.0126) Steps 596(592.54) | Grad Norm 0.5653(1.3946) | Total Time 10.00(10.00)\n",
      "Iter 0038 | Time 45.2227(56.8591) | Bit/dim 1.1269(1.1267) | Xent 0.0327(0.0384) | Loss 1.1433(1.1459) | Error 0.0112(0.0126) Steps 602(592.83) | Grad Norm 0.3191(1.3623) | Total Time 10.00(10.00)\n",
      "Iter 0039 | Time 45.4008(56.5153) | Bit/dim 1.1240(1.1266) | Xent 0.0333(0.0383) | Loss 1.1407(1.1458) | Error 0.0111(0.0125) Steps 602(593.10) | Grad Norm 0.2115(1.3278) | Total Time 10.00(10.00)\n",
      "Iter 0040 | Time 43.7605(56.1327) | Bit/dim 1.1244(1.1266) | Xent 0.0360(0.0382) | Loss 1.1424(1.1457) | Error 0.0131(0.0125) Steps 602(593.37) | Grad Norm 0.2857(1.2965) | Total Time 10.00(10.00)\n",
      "Iter 0041 | Time 44.6849(55.7892) | Bit/dim 1.1237(1.1265) | Xent 0.0331(0.0380) | Loss 1.1403(1.1455) | Error 0.0102(0.0125) Steps 590(593.27) | Grad Norm 0.5364(1.2737) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0006 | Time 11.8911, Epoch Time 336.9700(364.6717), Bit/dim 1.1096, Xent 0.0527, Loss 1.1359, Error 0.2132\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0042 | Time 43.2636(55.4135) | Bit/dim 1.1216(1.1263) | Xent 0.0312(0.0378) | Loss 1.1372(1.1453) | Error 0.0102(0.0124) Steps 602(593.53) | Grad Norm 0.6680(1.2556) | Total Time 10.00(10.00)\n",
      "Iter 0043 | Time 43.1245(55.0448) | Bit/dim 1.1229(1.1262) | Xent 0.0414(0.0379) | Loss 1.1436(1.1452) | Error 0.0116(0.0124) Steps 596(593.60) | Grad Norm 0.6243(1.2366) | Total Time 10.00(10.00)\n",
      "Iter 0044 | Time 43.5923(54.7012) | Bit/dim 1.1197(1.1260) | Xent 0.0348(0.0378) | Loss 1.1371(1.1450) | Error 0.0109(0.0123) Steps 602(593.86) | Grad Norm 0.4113(1.2119) | Total Time 10.00(10.00)\n",
      "Iter 0045 | Time 45.5081(54.4254) | Bit/dim 1.1214(1.1259) | Xent 0.0363(0.0378) | Loss 1.1396(1.1448) | Error 0.0114(0.0123) Steps 590(593.74) | Grad Norm 0.1677(1.1805) | Total Time 10.00(10.00)\n",
      "Iter 0046 | Time 45.2616(54.1505) | Bit/dim 1.1262(1.1259) | Xent 0.0379(0.0378) | Loss 1.1452(1.1448) | Error 0.0122(0.0123) Steps 590(593.63) | Grad Norm 0.1365(1.1492) | Total Time 10.00(10.00)\n",
      "Iter 0047 | Time 44.8019(53.8700) | Bit/dim 1.1254(1.1259) | Xent 0.0323(0.0376) | Loss 1.1416(1.1447) | Error 0.0101(0.0122) Steps 596(593.70) | Grad Norm 0.2263(1.1215) | Total Time 10.00(10.00)\n",
      "Iter 0048 | Time 44.9782(53.6033) | Bit/dim 1.1226(1.1258) | Xent 0.0321(0.0375) | Loss 1.1386(1.1445) | Error 0.0114(0.0122) Steps 590(593.59) | Grad Norm 0.4360(1.1010) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0007 | Time 11.9468, Epoch Time 335.3031(363.7906), Bit/dim 1.1092, Xent 0.0567, Loss 1.1376, Error 0.2134\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0049 | Time 43.9279(53.3130) | Bit/dim 1.1237(1.1257) | Xent 0.0382(0.0375) | Loss 1.1429(1.1445) | Error 0.0112(0.0122) Steps 590(593.48) | Grad Norm 0.5282(1.0838) | Total Time 10.00(10.00)\n",
      "Iter 0050 | Time 44.0106(53.0340) | Bit/dim 1.1291(1.1258) | Xent 0.0376(0.0375) | Loss 1.1478(1.1446) | Error 0.0126(0.0122) Steps 590(593.38) | Grad Norm 0.4090(1.0635) | Total Time 10.00(10.00)\n",
      "Iter 0051 | Time 42.5748(52.7202) | Bit/dim 1.1220(1.1257) | Xent 0.0341(0.0374) | Loss 1.1391(1.1444) | Error 0.0105(0.0122) Steps 590(593.28) | Grad Norm 0.1551(1.0363) | Total Time 10.00(10.00)\n",
      "Iter 0052 | Time 42.7212(52.4202) | Bit/dim 1.1226(1.1256) | Xent 0.0360(0.0373) | Loss 1.1406(1.1443) | Error 0.0118(0.0121) Steps 590(593.18) | Grad Norm 0.1667(1.0102) | Total Time 10.00(10.00)\n",
      "Iter 0053 | Time 45.4651(52.2116) | Bit/dim 1.1228(1.1255) | Xent 0.0388(0.0374) | Loss 1.1421(1.1442) | Error 0.0124(0.0121) Steps 590(593.08) | Grad Norm 0.2658(0.9879) | Total Time 10.00(10.00)\n",
      "Iter 0054 | Time 43.1973(51.9411) | Bit/dim 1.1194(1.1254) | Xent 0.0317(0.0372) | Loss 1.1353(1.1440) | Error 0.0095(0.0121) Steps 596(593.17) | Grad Norm 0.3318(0.9682) | Total Time 10.00(10.00)\n",
      "Iter 0055 | Time 44.2992(51.7119) | Bit/dim 1.1169(1.1251) | Xent 0.0339(0.0371) | Loss 1.1339(1.1437) | Error 0.0109(0.0120) Steps 596(593.25) | Grad Norm 0.4442(0.9525) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0008 | Time 11.8599, Epoch Time 330.8095(362.8012), Bit/dim 1.1085, Xent 0.0541, Loss 1.1356, Error 0.2132\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0056 | Time 44.4927(51.4953) | Bit/dim 1.1240(1.1251) | Xent 0.0355(0.0371) | Loss 1.1418(1.1436) | Error 0.0114(0.0120) Steps 596(593.34) | Grad Norm 0.2382(0.9310) | Total Time 10.00(10.00)\n",
      "Iter 0057 | Time 44.9489(51.2989) | Bit/dim 1.1221(1.1250) | Xent 0.0317(0.0369) | Loss 1.1380(1.1434) | Error 0.0099(0.0119) Steps 590(593.24) | Grad Norm 0.1355(0.9072) | Total Time 10.00(10.00)\n",
      "Iter 0058 | Time 45.2654(51.1179) | Bit/dim 1.1167(1.1247) | Xent 0.0360(0.0369) | Loss 1.1347(1.1432) | Error 0.0121(0.0120) Steps 596(593.32) | Grad Norm 0.1690(0.8850) | Total Time 10.00(10.00)\n",
      "Iter 0059 | Time 44.2335(50.9114) | Bit/dim 1.1206(1.1246) | Xent 0.0424(0.0371) | Loss 1.1418(1.1431) | Error 0.0121(0.0120) Steps 596(593.40) | Grad Norm 0.1757(0.8637) | Total Time 10.00(10.00)\n",
      "Iter 0060 | Time 45.5097(50.7493) | Bit/dim 1.1227(1.1246) | Xent 0.0386(0.0371) | Loss 1.1421(1.1431) | Error 0.0122(0.0120) Steps 590(593.30) | Grad Norm 0.3393(0.8480) | Total Time 10.00(10.00)\n",
      "Iter 0061 | Time 43.5412(50.5331) | Bit/dim 1.1197(1.1244) | Xent 0.0373(0.0371) | Loss 1.1384(1.1430) | Error 0.0122(0.0120) Steps 590(593.20) | Grad Norm 0.3517(0.8331) | Total Time 10.00(10.00)\n",
      "Iter 0062 | Time 43.1416(50.3113) | Bit/dim 1.1217(1.1243) | Xent 0.0411(0.0372) | Loss 1.1423(1.1429) | Error 0.0130(0.0120) Steps 596(593.28) | Grad Norm 0.1857(0.8137) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0009 | Time 11.9992, Epoch Time 335.9417(361.9954), Bit/dim 1.1070, Xent 0.0594, Loss 1.1367, Error 0.2145\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0063 | Time 42.9822(50.0915) | Bit/dim 1.1221(1.1243) | Xent 0.0391(0.0373) | Loss 1.1416(1.1429) | Error 0.0126(0.0120) Steps 596(593.36) | Grad Norm 0.1403(0.7935) | Total Time 10.00(10.00)\n",
      "Iter 0064 | Time 44.3894(49.9204) | Bit/dim 1.1174(1.1241) | Xent 0.0337(0.0372) | Loss 1.1342(1.1426) | Error 0.0106(0.0120) Steps 590(593.26) | Grad Norm 0.2412(0.7769) | Total Time 10.00(10.00)\n",
      "Iter 0065 | Time 42.9743(49.7120) | Bit/dim 1.1214(1.1240) | Xent 0.0360(0.0371) | Loss 1.1394(1.1425) | Error 0.0112(0.0120) Steps 596(593.35) | Grad Norm 0.2489(0.7611) | Total Time 10.00(10.00)\n",
      "Iter 0066 | Time 43.7048(49.5318) | Bit/dim 1.1242(1.1240) | Xent 0.0321(0.0370) | Loss 1.1403(1.1425) | Error 0.0109(0.0119) Steps 590(593.25) | Grad Norm 0.3084(0.7475) | Total Time 10.00(10.00)\n",
      "Iter 0067 | Time 45.2449(49.4032) | Bit/dim 1.1194(1.1238) | Xent 0.0400(0.0371) | Loss 1.1394(1.1424) | Error 0.0125(0.0119) Steps 590(593.15) | Grad Norm 0.1573(0.7298) | Total Time 10.00(10.00)\n",
      "Iter 0068 | Time 43.0717(49.2132) | Bit/dim 1.1258(1.1239) | Xent 0.0344(0.0370) | Loss 1.1430(1.1424) | Error 0.0098(0.0119) Steps 596(593.23) | Grad Norm 0.1473(0.7123) | Total Time 10.00(10.00)\n",
      "Iter 0069 | Time 42.7510(49.0194) | Bit/dim 1.1191(1.1238) | Xent 0.0361(0.0370) | Loss 1.1372(1.1422) | Error 0.0111(0.0119) Steps 596(593.32) | Grad Norm 0.2576(0.6987) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0010 | Time 11.9364, Epoch Time 329.8993(361.0325), Bit/dim 1.1065, Xent 0.0507, Loss 1.1319, Error 0.2126\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0070 | Time 43.9107(48.8661) | Bit/dim 1.1200(1.1236) | Xent 0.0365(0.0370) | Loss 1.1383(1.1421) | Error 0.0114(0.0118) Steps 590(593.22) | Grad Norm 0.2173(0.6842) | Total Time 10.00(10.00)\n",
      "Iter 0071 | Time 44.3953(48.7320) | Bit/dim 1.1212(1.1236) | Xent 0.0322(0.0368) | Loss 1.1374(1.1420) | Error 0.0102(0.0118) Steps 584(592.94) | Grad Norm 0.1520(0.6683) | Total Time 10.00(10.00)\n",
      "Iter 0072 | Time 44.8616(48.6159) | Bit/dim 1.1253(1.1236) | Xent 0.0317(0.0367) | Loss 1.1412(1.1420) | Error 0.0106(0.0118) Steps 596(593.03) | Grad Norm 0.1221(0.6519) | Total Time 10.00(10.00)\n",
      "Iter 0073 | Time 44.7489(48.4999) | Bit/dim 1.1206(1.1235) | Xent 0.0335(0.0366) | Loss 1.1374(1.1418) | Error 0.0108(0.0117) Steps 596(593.12) | Grad Norm 0.1535(0.6369) | Total Time 10.00(10.00)\n",
      "Iter 0074 | Time 42.9488(48.3333) | Bit/dim 1.1155(1.1233) | Xent 0.0352(0.0365) | Loss 1.1331(1.1416) | Error 0.0116(0.0117) Steps 602(593.39) | Grad Norm 0.2710(0.6260) | Total Time 10.00(10.00)\n",
      "Iter 0075 | Time 45.3799(48.2447) | Bit/dim 1.1165(1.1231) | Xent 0.0384(0.0366) | Loss 1.1357(1.1414) | Error 0.0126(0.0118) Steps 602(593.65) | Grad Norm 0.1479(0.6116) | Total Time 10.00(10.00)\n",
      "Iter 0076 | Time 44.3511(48.1279) | Bit/dim 1.1216(1.1230) | Xent 0.0335(0.0365) | Loss 1.1384(1.1413) | Error 0.0119(0.0118) Steps 596(593.72) | Grad Norm 0.2084(0.5995) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0011 | Time 11.8845, Epoch Time 335.3039(360.2607), Bit/dim 1.1066, Xent 0.0541, Loss 1.1336, Error 0.2133\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0077 | Time 42.4256(47.9569) | Bit/dim 1.1197(1.1229) | Xent 0.0364(0.0365) | Loss 1.1379(1.1412) | Error 0.0130(0.0118) Steps 596(593.79) | Grad Norm 0.2195(0.5881) | Total Time 10.00(10.00)\n",
      "Iter 0078 | Time 43.0684(47.8102) | Bit/dim 1.1242(1.1230) | Xent 0.0364(0.0365) | Loss 1.1424(1.1412) | Error 0.0124(0.0118) Steps 596(593.85) | Grad Norm 0.1182(0.5740) | Total Time 10.00(10.00)\n",
      "Iter 0079 | Time 43.2257(47.6727) | Bit/dim 1.1178(1.1228) | Xent 0.0362(0.0365) | Loss 1.1359(1.1411) | Error 0.0110(0.0118) Steps 596(593.92) | Grad Norm 0.1487(0.5613) | Total Time 10.00(10.00)\n",
      "Iter 0080 | Time 43.4334(47.5455) | Bit/dim 1.1164(1.1226) | Xent 0.0335(0.0364) | Loss 1.1332(1.1408) | Error 0.0118(0.0118) Steps 596(593.98) | Grad Norm 0.1428(0.5487) | Total Time 10.00(10.00)\n",
      "Iter 0081 | Time 42.9714(47.4083) | Bit/dim 1.1169(1.1225) | Xent 0.0364(0.0364) | Loss 1.1351(1.1407) | Error 0.0134(0.0118) Steps 596(594.04) | Grad Norm 0.1688(0.5373) | Total Time 10.00(10.00)\n",
      "Iter 0082 | Time 45.5059(47.3512) | Bit/dim 1.1242(1.1225) | Xent 0.0394(0.0365) | Loss 1.1439(1.1408) | Error 0.0105(0.0118) Steps 590(593.92) | Grad Norm 0.1359(0.5253) | Total Time 10.00(10.00)\n",
      "Iter 0083 | Time 45.7945(47.3045) | Bit/dim 1.1230(1.1225) | Xent 0.0355(0.0365) | Loss 1.1408(1.1408) | Error 0.0116(0.0118) Steps 596(593.98) | Grad Norm 0.1947(0.5154) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0012 | Time 11.8300, Epoch Time 331.1085(359.3861), Bit/dim 1.1062, Xent 0.0494, Loss 1.1309, Error 0.2129\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0084 | Time 43.4526(47.1889) | Bit/dim 1.1185(1.1224) | Xent 0.0387(0.0365) | Loss 1.1378(1.1407) | Error 0.0111(0.0118) Steps 596(594.04) | Grad Norm 0.1946(0.5057) | Total Time 10.00(10.00)\n",
      "Iter 0085 | Time 44.2798(47.1017) | Bit/dim 1.1223(1.1224) | Xent 0.0297(0.0363) | Loss 1.1371(1.1406) | Error 0.0102(0.0117) Steps 596(594.10) | Grad Norm 0.1720(0.4957) | Total Time 10.00(10.00)\n",
      "Iter 0086 | Time 44.1956(47.0145) | Bit/dim 1.1239(1.1225) | Xent 0.0331(0.0362) | Loss 1.1404(1.1406) | Error 0.0116(0.0117) Steps 596(594.16) | Grad Norm 0.1644(0.4858) | Total Time 10.00(10.00)\n",
      "Iter 0087 | Time 45.0300(46.9549) | Bit/dim 1.1181(1.1223) | Xent 0.0303(0.0360) | Loss 1.1333(1.1403) | Error 0.0111(0.0117) Steps 596(594.21) | Grad Norm 0.1547(0.4758) | Total Time 10.00(10.00)\n",
      "Iter 0088 | Time 42.5650(46.8232) | Bit/dim 1.1170(1.1222) | Xent 0.0383(0.0361) | Loss 1.1361(1.1402) | Error 0.0111(0.0117) Steps 596(594.27) | Grad Norm 0.1480(0.4660) | Total Time 10.00(10.00)\n",
      "Iter 0089 | Time 43.7087(46.7298) | Bit/dim 1.1174(1.1220) | Xent 0.0383(0.0362) | Loss 1.1366(1.1401) | Error 0.0132(0.0117) Steps 590(594.14) | Grad Norm 0.1517(0.4566) | Total Time 10.00(10.00)\n",
      "Iter 0090 | Time 43.6310(46.6368) | Bit/dim 1.1197(1.1219) | Xent 0.0393(0.0363) | Loss 1.1394(1.1401) | Error 0.0134(0.0118) Steps 596(594.19) | Grad Norm 0.1288(0.4467) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0013 | Time 11.9214, Epoch Time 331.5186(358.5501), Bit/dim 1.1051, Xent 0.0536, Loss 1.1319, Error 0.2148\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0091 | Time 44.2131(46.5641) | Bit/dim 1.1208(1.1219) | Xent 0.0375(0.0363) | Loss 1.1395(1.1401) | Error 0.0114(0.0118) Steps 590(594.07) | Grad Norm 0.1555(0.4380) | Total Time 10.00(10.00)\n",
      "Iter 0092 | Time 44.9951(46.5171) | Bit/dim 1.1213(1.1219) | Xent 0.0363(0.0363) | Loss 1.1394(1.1400) | Error 0.0104(0.0117) Steps 596(594.13) | Grad Norm 0.1773(0.4302) | Total Time 10.00(10.00)\n",
      "Iter 0093 | Time 44.8033(46.4656) | Bit/dim 1.1153(1.1217) | Xent 0.0350(0.0363) | Loss 1.1328(1.1398) | Error 0.0118(0.0117) Steps 596(594.18) | Grad Norm 0.1854(0.4228) | Total Time 10.00(10.00)\n",
      "Iter 0094 | Time 44.4303(46.4046) | Bit/dim 1.1177(1.1216) | Xent 0.0325(0.0362) | Loss 1.1339(1.1397) | Error 0.0106(0.0117) Steps 590(594.06) | Grad Norm 0.2155(0.4166) | Total Time 10.00(10.00)\n",
      "Iter 0095 | Time 43.0326(46.3034) | Bit/dim 1.1222(1.1216) | Xent 0.0372(0.0362) | Loss 1.1408(1.1397) | Error 0.0118(0.0117) Steps 596(594.11) | Grad Norm 0.1690(0.4092) | Total Time 10.00(10.00)\n",
      "Iter 0096 | Time 44.5441(46.2506) | Bit/dim 1.1213(1.1216) | Xent 0.0380(0.0362) | Loss 1.1403(1.1397) | Error 0.0128(0.0117) Steps 596(594.17) | Grad Norm 0.1366(0.4010) | Total Time 10.00(10.00)\n",
      "Iter 0097 | Time 42.6466(46.1425) | Bit/dim 1.1191(1.1215) | Xent 0.0345(0.0362) | Loss 1.1363(1.1396) | Error 0.0109(0.0117) Steps 596(594.23) | Grad Norm 0.1244(0.3927) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0014 | Time 11.9062, Epoch Time 333.6910(357.8043), Bit/dim 1.1059, Xent 0.0518, Loss 1.1318, Error 0.2141\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0098 | Time 44.4549(46.0919) | Bit/dim 1.1185(1.1214) | Xent 0.0353(0.0362) | Loss 1.1362(1.1395) | Error 0.0111(0.0117) Steps 584(593.92) | Grad Norm 0.1408(0.3852) | Total Time 10.00(10.00)\n",
      "Iter 0099 | Time 44.2661(46.0371) | Bit/dim 1.1204(1.1214) | Xent 0.0312(0.0360) | Loss 1.1360(1.1394) | Error 0.0115(0.0117) Steps 590(593.80) | Grad Norm 0.1535(0.3782) | Total Time 10.00(10.00)\n",
      "Iter 0100 | Time 43.8940(45.9728) | Bit/dim 1.1173(1.1213) | Xent 0.0447(0.0363) | Loss 1.1397(1.1394) | Error 0.0158(0.0118) Steps 596(593.87) | Grad Norm 0.1810(0.3723) | Total Time 10.00(10.00)\n",
      "Iter 0101 | Time 44.3337(45.9237) | Bit/dim 1.1158(1.1211) | Xent 0.0366(0.0363) | Loss 1.1341(1.1392) | Error 0.0111(0.0118) Steps 596(593.93) | Grad Norm 0.1943(0.3670) | Total Time 10.00(10.00)\n",
      "Iter 0102 | Time 43.1529(45.8405) | Bit/dim 1.1224(1.1211) | Xent 0.0375(0.0363) | Loss 1.1411(1.1393) | Error 0.0119(0.0118) Steps 590(593.81) | Grad Norm 0.1485(0.3604) | Total Time 10.00(10.00)\n",
      "Iter 0103 | Time 45.2520(45.8229) | Bit/dim 1.1208(1.1211) | Xent 0.0370(0.0363) | Loss 1.1393(1.1393) | Error 0.0110(0.0118) Steps 596(593.88) | Grad Norm 0.1520(0.3542) | Total Time 10.00(10.00)\n",
      "Iter 0104 | Time 44.0952(45.7710) | Bit/dim 1.1158(1.1210) | Xent 0.0346(0.0363) | Loss 1.1331(1.1391) | Error 0.0122(0.0118) Steps 596(593.94) | Grad Norm 0.1797(0.3489) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0015 | Time 12.0469, Epoch Time 334.4625(357.1040), Bit/dim 1.1048, Xent 0.0549, Loss 1.1322, Error 0.2133\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0105 | Time 43.4182(45.7005) | Bit/dim 1.1255(1.1211) | Xent 0.0324(0.0362) | Loss 1.1417(1.1392) | Error 0.0104(0.0117) Steps 596(594.00) | Grad Norm 0.1965(0.3444) | Total Time 10.00(10.00)\n",
      "Iter 0106 | Time 44.7567(45.6721) | Bit/dim 1.1141(1.1209) | Xent 0.0338(0.0361) | Loss 1.1310(1.1389) | Error 0.0108(0.0117) Steps 596(594.06) | Grad Norm 0.2312(0.3410) | Total Time 10.00(10.00)\n",
      "Iter 0107 | Time 43.5905(45.6097) | Bit/dim 1.1195(1.1209) | Xent 0.0425(0.0363) | Loss 1.1407(1.1390) | Error 0.0114(0.0117) Steps 596(594.12) | Grad Norm 0.1505(0.3352) | Total Time 10.00(10.00)\n",
      "Iter 0108 | Time 44.6046(45.5795) | Bit/dim 1.1153(1.1207) | Xent 0.0290(0.0361) | Loss 1.1298(1.1387) | Error 0.0106(0.0117) Steps 596(594.18) | Grad Norm 0.1578(0.3299) | Total Time 10.00(10.00)\n",
      "Iter 0109 | Time 44.1839(45.5377) | Bit/dim 1.1209(1.1207) | Xent 0.0399(0.0362) | Loss 1.1409(1.1388) | Error 0.0141(0.0117) Steps 590(594.05) | Grad Norm 0.4046(0.3322) | Total Time 10.00(10.00)\n",
      "Iter 0110 | Time 43.6233(45.4802) | Bit/dim 1.1159(1.1206) | Xent 0.0336(0.0361) | Loss 1.1327(1.1386) | Error 0.0108(0.0117) Steps 596(594.11) | Grad Norm 0.2186(0.3288) | Total Time 10.00(10.00)\n",
      "Iter 0111 | Time 45.1291(45.4697) | Bit/dim 1.1163(1.1204) | Xent 0.0370(0.0361) | Loss 1.1348(1.1385) | Error 0.0124(0.0117) Steps 596(594.17) | Grad Norm 0.1846(0.3244) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0016 | Time 11.9163, Epoch Time 334.0976(356.4138), Bit/dim 1.1054, Xent 0.0539, Loss 1.1324, Error 0.2140\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0112 | Time 44.9106(45.4529) | Bit/dim 1.1173(1.1203) | Xent 0.0394(0.0362) | Loss 1.1370(1.1384) | Error 0.0136(0.0118) Steps 602(594.40) | Grad Norm 0.4814(0.3291) | Total Time 10.00(10.00)\n",
      "Iter 0113 | Time 45.4387(45.4525) | Bit/dim 1.1155(1.1202) | Xent 0.0309(0.0361) | Loss 1.1309(1.1382) | Error 0.0092(0.0117) Steps 596(594.45) | Grad Norm 0.2978(0.3282) | Total Time 10.00(10.00)\n",
      "Iter 0114 | Time 44.0007(45.4090) | Bit/dim 1.1188(1.1201) | Xent 0.0423(0.0363) | Loss 1.1399(1.1383) | Error 0.0122(0.0117) Steps 596(594.50) | Grad Norm 0.2674(0.3264) | Total Time 10.00(10.00)\n",
      "Iter 0115 | Time 43.1980(45.3426) | Bit/dim 1.1199(1.1201) | Xent 0.0368(0.0363) | Loss 1.1383(1.1383) | Error 0.0115(0.0117) Steps 596(594.54) | Grad Norm 0.2845(0.3251) | Total Time 10.00(10.00)\n",
      "Iter 0116 | Time 44.3025(45.3114) | Bit/dim 1.1200(1.1201) | Xent 0.0321(0.0361) | Loss 1.1360(1.1382) | Error 0.0116(0.0117) Steps 602(594.77) | Grad Norm 0.3142(0.3248) | Total Time 10.00(10.00)\n",
      "Iter 0117 | Time 44.4350(45.2851) | Bit/dim 1.1152(1.1200) | Xent 0.0341(0.0361) | Loss 1.1322(1.1380) | Error 0.0111(0.0117) Steps 602(594.98) | Grad Norm 0.1448(0.3194) | Total Time 10.00(10.00)\n",
      "Iter 0118 | Time 45.2372(45.2837) | Bit/dim 1.1230(1.1201) | Xent 0.0393(0.0362) | Loss 1.1426(1.1382) | Error 0.0124(0.0117) Steps 602(595.19) | Grad Norm 0.2960(0.3187) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0017 | Time 11.7565, Epoch Time 336.2638(355.8093), Bit/dim 1.1038, Xent 0.0466, Loss 1.1271, Error 0.2122\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0119 | Time 44.5226(45.2609) | Bit/dim 1.1197(1.1201) | Xent 0.0305(0.0360) | Loss 1.1350(1.1381) | Error 0.0091(0.0116) Steps 602(595.40) | Grad Norm 0.3834(0.3206) | Total Time 10.00(10.00)\n",
      "Iter 0120 | Time 43.3822(45.2045) | Bit/dim 1.1187(1.1200) | Xent 0.0321(0.0359) | Loss 1.1347(1.1380) | Error 0.0106(0.0116) Steps 590(595.24) | Grad Norm 0.1287(0.3149) | Total Time 10.00(10.00)\n",
      "Iter 0121 | Time 44.3814(45.1798) | Bit/dim 1.1199(1.1200) | Xent 0.0355(0.0359) | Loss 1.1376(1.1380) | Error 0.0118(0.0116) Steps 596(595.26) | Grad Norm 0.2277(0.3123) | Total Time 10.00(10.00)\n",
      "Iter 0122 | Time 44.5961(45.1623) | Bit/dim 1.1163(1.1199) | Xent 0.0401(0.0360) | Loss 1.1363(1.1379) | Error 0.0124(0.0116) Steps 590(595.10) | Grad Norm 0.2718(0.3110) | Total Time 10.00(10.00)\n",
      "Iter 0123 | Time 45.6576(45.1772) | Bit/dim 1.1173(1.1198) | Xent 0.0379(0.0361) | Loss 1.1362(1.1379) | Error 0.0118(0.0116) Steps 596(595.13) | Grad Norm 0.2319(0.3087) | Total Time 10.00(10.00)\n",
      "Iter 0124 | Time 45.3542(45.1825) | Bit/dim 1.1157(1.1197) | Xent 0.0356(0.0361) | Loss 1.1334(1.1377) | Error 0.0119(0.0116) Steps 608(595.51) | Grad Norm 0.2020(0.3055) | Total Time 10.00(10.00)\n",
      "Iter 0125 | Time 45.0021(45.1771) | Bit/dim 1.1184(1.1197) | Xent 0.0341(0.0360) | Loss 1.1354(1.1377) | Error 0.0114(0.0116) Steps 608(595.89) | Grad Norm 0.2438(0.3036) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0018 | Time 11.7723, Epoch Time 337.4605(355.2589), Bit/dim 1.1039, Xent 0.0529, Loss 1.1304, Error 0.2131\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0126 | Time 44.3692(45.1528) | Bit/dim 1.1202(1.1197) | Xent 0.0332(0.0359) | Loss 1.1368(1.1376) | Error 0.0100(0.0116) Steps 596(595.89) | Grad Norm 0.1725(0.2997) | Total Time 10.00(10.00)\n",
      "Iter 0127 | Time 42.2975(45.0672) | Bit/dim 1.1150(1.1195) | Xent 0.0425(0.0361) | Loss 1.1363(1.1376) | Error 0.0132(0.0116) Steps 596(595.90) | Grad Norm 0.1879(0.2963) | Total Time 10.00(10.00)\n",
      "Iter 0128 | Time 42.8946(45.0020) | Bit/dim 1.1122(1.1193) | Xent 0.0299(0.0359) | Loss 1.1271(1.1373) | Error 0.0089(0.0116) Steps 590(595.72) | Grad Norm 0.2309(0.2944) | Total Time 10.00(10.00)\n",
      "Iter 0129 | Time 43.6082(44.9602) | Bit/dim 1.1182(1.1193) | Xent 0.0374(0.0360) | Loss 1.1369(1.1373) | Error 0.0128(0.0116) Steps 596(595.73) | Grad Norm 0.2940(0.2944) | Total Time 10.00(10.00)\n",
      "Iter 0130 | Time 43.0462(44.9028) | Bit/dim 1.1150(1.1192) | Xent 0.0363(0.0360) | Loss 1.1332(1.1371) | Error 0.0128(0.0116) Steps 596(595.74) | Grad Norm 0.2822(0.2940) | Total Time 10.00(10.00)\n",
      "Iter 0131 | Time 45.3271(44.9155) | Bit/dim 1.1209(1.1192) | Xent 0.0350(0.0360) | Loss 1.1384(1.1372) | Error 0.0104(0.0116) Steps 596(595.74) | Grad Norm 0.3734(0.2964) | Total Time 10.00(10.00)\n",
      "Iter 0132 | Time 44.8117(44.9124) | Bit/dim 1.1227(1.1193) | Xent 0.0367(0.0360) | Loss 1.1411(1.1373) | Error 0.0112(0.0116) Steps 596(595.75) | Grad Norm 0.3175(0.2970) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0019 | Time 11.9478, Epoch Time 330.9293(354.5290), Bit/dim 1.1033, Xent 0.0501, Loss 1.1283, Error 0.2134\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0133 | Time 44.9110(44.9123) | Bit/dim 1.1198(1.1193) | Xent 0.0410(0.0361) | Loss 1.1403(1.1374) | Error 0.0131(0.0116) Steps 596(595.76) | Grad Norm 0.1504(0.2926) | Total Time 10.00(10.00)\n",
      "Iter 0134 | Time 44.7268(44.9068) | Bit/dim 1.1162(1.1192) | Xent 0.0366(0.0361) | Loss 1.1345(1.1373) | Error 0.0119(0.0116) Steps 590(595.59) | Grad Norm 0.1882(0.2895) | Total Time 10.00(10.00)\n",
      "Iter 0135 | Time 44.3114(44.8889) | Bit/dim 1.1161(1.1191) | Xent 0.0360(0.0361) | Loss 1.1341(1.1372) | Error 0.0120(0.0116) Steps 602(595.78) | Grad Norm 0.3804(0.2922) | Total Time 10.00(10.00)\n",
      "Iter 0136 | Time 43.1609(44.8371) | Bit/dim 1.1160(1.1190) | Xent 0.0381(0.0362) | Loss 1.1351(1.1371) | Error 0.0130(0.0117) Steps 596(595.78) | Grad Norm 0.1768(0.2887) | Total Time 10.00(10.00)\n",
      "Iter 0137 | Time 45.2267(44.8488) | Bit/dim 1.1149(1.1189) | Xent 0.0369(0.0362) | Loss 1.1333(1.1370) | Error 0.0125(0.0117) Steps 596(595.79) | Grad Norm 0.2933(0.2889) | Total Time 10.00(10.00)\n",
      "Iter 0138 | Time 44.5880(44.8409) | Bit/dim 1.1145(1.1188) | Xent 0.0399(0.0363) | Loss 1.1345(1.1370) | Error 0.0116(0.0117) Steps 596(595.80) | Grad Norm 0.2318(0.2872) | Total Time 10.00(10.00)\n",
      "Iter 0139 | Time 45.3712(44.8568) | Bit/dim 1.1166(1.1187) | Xent 0.0369(0.0363) | Loss 1.1351(1.1369) | Error 0.0122(0.0117) Steps 596(595.80) | Grad Norm 0.2100(0.2848) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0020 | Time 11.8931, Epoch Time 336.7810(353.9966), Bit/dim 1.1029, Xent 0.0544, Loss 1.1301, Error 0.2151\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0140 | Time 43.3065(44.8103) | Bit/dim 1.1149(1.1186) | Xent 0.0358(0.0363) | Loss 1.1328(1.1368) | Error 0.0112(0.0117) Steps 596(595.81) | Grad Norm 0.2120(0.2827) | Total Time 10.00(10.00)\n",
      "Iter 0141 | Time 44.1105(44.7893) | Bit/dim 1.1191(1.1186) | Xent 0.0328(0.0362) | Loss 1.1354(1.1367) | Error 0.0116(0.0117) Steps 596(595.82) | Grad Norm 0.2411(0.2814) | Total Time 10.00(10.00)\n",
      "Iter 0142 | Time 44.9364(44.7937) | Bit/dim 1.1214(1.1187) | Xent 0.0314(0.0361) | Loss 1.1371(1.1367) | Error 0.0098(0.0116) Steps 596(595.82) | Grad Norm 0.1929(0.2788) | Total Time 10.00(10.00)\n",
      "Iter 0143 | Time 44.7267(44.7917) | Bit/dim 1.1164(1.1186) | Xent 0.0373(0.0361) | Loss 1.1351(1.1367) | Error 0.0106(0.0116) Steps 596(595.83) | Grad Norm 0.2073(0.2766) | Total Time 10.00(10.00)\n",
      "Iter 0144 | Time 44.7137(44.7894) | Bit/dim 1.1137(1.1185) | Xent 0.0338(0.0360) | Loss 1.1307(1.1365) | Error 0.0105(0.0116) Steps 596(595.83) | Grad Norm 0.2177(0.2748) | Total Time 10.00(10.00)\n",
      "Iter 0145 | Time 43.6109(44.7540) | Bit/dim 1.1141(1.1184) | Xent 0.0434(0.0363) | Loss 1.1358(1.1365) | Error 0.0144(0.0117) Steps 602(596.02) | Grad Norm 0.2506(0.2741) | Total Time 10.00(10.00)\n",
      "Iter 0146 | Time 42.8630(44.6973) | Bit/dim 1.1139(1.1182) | Xent 0.0380(0.0363) | Loss 1.1329(1.1364) | Error 0.0118(0.0117) Steps 584(595.66) | Grad Norm 0.2412(0.2731) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0021 | Time 11.8185, Epoch Time 332.7996(353.3606), Bit/dim 1.1014, Xent 0.0531, Loss 1.1279, Error 0.2128\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0147 | Time 44.3075(44.6856) | Bit/dim 1.1169(1.1182) | Xent 0.0380(0.0364) | Loss 1.1359(1.1364) | Error 0.0134(0.0117) Steps 596(595.67) | Grad Norm 0.2870(0.2735) | Total Time 10.00(10.00)\n",
      "Iter 0148 | Time 44.4692(44.6791) | Bit/dim 1.1108(1.1180) | Xent 0.0353(0.0363) | Loss 1.1285(1.1361) | Error 0.0116(0.0117) Steps 584(595.32) | Grad Norm 0.1451(0.2697) | Total Time 10.00(10.00)\n",
      "Iter 0149 | Time 45.2040(44.6949) | Bit/dim 1.1196(1.1180) | Xent 0.0391(0.0364) | Loss 1.1391(1.1362) | Error 0.0125(0.0117) Steps 596(595.34) | Grad Norm 0.2940(0.2704) | Total Time 10.00(10.00)\n",
      "Iter 0150 | Time 44.6208(44.6926) | Bit/dim 1.1191(1.1180) | Xent 0.0359(0.0364) | Loss 1.1371(1.1362) | Error 0.0119(0.0117) Steps 596(595.36) | Grad Norm 0.2666(0.2703) | Total Time 10.00(10.00)\n",
      "Iter 0151 | Time 44.1382(44.6760) | Bit/dim 1.1159(1.1180) | Xent 0.0324(0.0363) | Loss 1.1321(1.1361) | Error 0.0102(0.0117) Steps 596(595.38) | Grad Norm 0.1889(0.2679) | Total Time 10.00(10.00)\n",
      "Iter 0152 | Time 43.3379(44.6359) | Bit/dim 1.1170(1.1180) | Xent 0.0331(0.0362) | Loss 1.1336(1.1360) | Error 0.0109(0.0117) Steps 602(595.57) | Grad Norm 0.3249(0.2696) | Total Time 10.00(10.00)\n",
      "Iter 0153 | Time 45.0702(44.6489) | Bit/dim 1.1114(1.1178) | Xent 0.0381(0.0362) | Loss 1.1305(1.1359) | Error 0.0114(0.0117) Steps 602(595.77) | Grad Norm 0.1958(0.2674) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0022 | Time 11.7904, Epoch Time 335.7692(352.8329), Bit/dim 1.1026, Xent 0.0541, Loss 1.1296, Error 0.2147\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0154 | Time 44.6550(44.6491) | Bit/dim 1.1183(1.1178) | Xent 0.0393(0.0363) | Loss 1.1379(1.1359) | Error 0.0121(0.0117) Steps 596(595.77) | Grad Norm 0.1524(0.2639) | Total Time 10.00(10.00)\n",
      "Iter 0155 | Time 43.7408(44.6218) | Bit/dim 1.1162(1.1177) | Xent 0.0296(0.0361) | Loss 1.1310(1.1358) | Error 0.0101(0.0116) Steps 590(595.60) | Grad Norm 0.2870(0.2646) | Total Time 10.00(10.00)\n",
      "Iter 0156 | Time 44.3496(44.6137) | Bit/dim 1.1191(1.1178) | Xent 0.0402(0.0363) | Loss 1.1392(1.1359) | Error 0.0132(0.0117) Steps 590(595.43) | Grad Norm 0.1936(0.2625) | Total Time 10.00(10.00)\n",
      "Iter 0157 | Time 44.0466(44.5967) | Bit/dim 1.1151(1.1177) | Xent 0.0387(0.0363) | Loss 1.1345(1.1359) | Error 0.0130(0.0117) Steps 596(595.45) | Grad Norm 0.1997(0.2606) | Total Time 10.00(10.00)\n",
      "Iter 0158 | Time 44.4890(44.5934) | Bit/dim 1.1168(1.1177) | Xent 0.0339(0.0363) | Loss 1.1338(1.1358) | Error 0.0112(0.0117) Steps 596(595.47) | Grad Norm 0.1605(0.2576) | Total Time 10.00(10.00)\n",
      "Iter 0159 | Time 45.4707(44.6197) | Bit/dim 1.1103(1.1174) | Xent 0.0339(0.0362) | Loss 1.1272(1.1355) | Error 0.0115(0.0117) Steps 602(595.66) | Grad Norm 0.3185(0.2594) | Total Time 10.00(10.00)\n",
      "Iter 0160 | Time 45.8881(44.6578) | Bit/dim 1.1171(1.1174) | Xent 0.0368(0.0362) | Loss 1.1355(1.1355) | Error 0.0116(0.0117) Steps 596(595.67) | Grad Norm 0.1584(0.2564) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0023 | Time 11.8936, Epoch Time 337.4842(352.3724), Bit/dim 1.1026, Xent 0.0536, Loss 1.1294, Error 0.2128\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0161 | Time 44.6105(44.6564) | Bit/dim 1.1181(1.1174) | Xent 0.0323(0.0361) | Loss 1.1342(1.1355) | Error 0.0104(0.0117) Steps 596(595.68) | Grad Norm 0.1827(0.2542) | Total Time 10.00(10.00)\n",
      "Iter 0162 | Time 44.6416(44.6559) | Bit/dim 1.1202(1.1175) | Xent 0.0379(0.0361) | Loss 1.1391(1.1356) | Error 0.0125(0.0117) Steps 602(595.87) | Grad Norm 0.1744(0.2518) | Total Time 10.00(10.00)\n",
      "Iter 0163 | Time 45.9626(44.6951) | Bit/dim 1.1144(1.1174) | Xent 0.0361(0.0361) | Loss 1.1324(1.1355) | Error 0.0125(0.0117) Steps 602(596.06) | Grad Norm 0.1914(0.2500) | Total Time 10.00(10.00)\n",
      "Iter 0164 | Time 43.9295(44.6722) | Bit/dim 1.1116(1.1173) | Xent 0.0394(0.0362) | Loss 1.1313(1.1354) | Error 0.0122(0.0117) Steps 596(596.05) | Grad Norm 0.2851(0.2510) | Total Time 10.00(10.00)\n",
      "Iter 0165 | Time 44.9191(44.6796) | Bit/dim 1.1086(1.1170) | Xent 0.0374(0.0363) | Loss 1.1273(1.1351) | Error 0.0116(0.0117) Steps 596(596.05) | Grad Norm 0.3485(0.2540) | Total Time 10.00(10.00)\n",
      "Iter 0166 | Time 45.9234(44.7169) | Bit/dim 1.1182(1.1170) | Xent 0.0356(0.0363) | Loss 1.1360(1.1352) | Error 0.0120(0.0117) Steps 608(596.41) | Grad Norm 0.1838(0.2518) | Total Time 10.00(10.00)\n",
      "Iter 0167 | Time 43.8962(44.6923) | Bit/dim 1.1147(1.1170) | Xent 0.0382(0.0363) | Loss 1.1338(1.1351) | Error 0.0126(0.0118) Steps 602(596.58) | Grad Norm 0.1704(0.2494) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0024 | Time 11.9830, Epoch Time 338.6875(351.9619), Bit/dim 1.1019, Xent 0.0570, Loss 1.1304, Error 0.2122\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0168 | Time 44.0173(44.6720) | Bit/dim 1.1131(1.1169) | Xent 0.0366(0.0363) | Loss 1.1314(1.1350) | Error 0.0131(0.0118) Steps 596(596.56) | Grad Norm 0.1633(0.2468) | Total Time 10.00(10.00)\n",
      "Iter 0169 | Time 45.6266(44.7007) | Bit/dim 1.1158(1.1168) | Xent 0.0317(0.0362) | Loss 1.1317(1.1349) | Error 0.0100(0.0117) Steps 602(596.72) | Grad Norm 0.1591(0.2442) | Total Time 10.00(10.00)\n",
      "Iter 0170 | Time 45.0213(44.7103) | Bit/dim 1.1152(1.1168) | Xent 0.0359(0.0362) | Loss 1.1332(1.1349) | Error 0.0124(0.0118) Steps 602(596.88) | Grad Norm 0.1483(0.2413) | Total Time 10.00(10.00)\n",
      "Iter 0171 | Time 45.5325(44.7349) | Bit/dim 1.1100(1.1166) | Xent 0.0364(0.0362) | Loss 1.1282(1.1347) | Error 0.0122(0.0118) Steps 602(597.04) | Grad Norm 0.1647(0.2390) | Total Time 10.00(10.00)\n",
      "Iter 0172 | Time 45.4255(44.7557) | Bit/dim 1.1167(1.1166) | Xent 0.0382(0.0362) | Loss 1.1358(1.1347) | Error 0.0125(0.0118) Steps 596(597.01) | Grad Norm 0.1717(0.2370) | Total Time 10.00(10.00)\n",
      "Iter 0173 | Time 43.4786(44.7173) | Bit/dim 1.1171(1.1166) | Xent 0.0335(0.0362) | Loss 1.1339(1.1347) | Error 0.0115(0.0118) Steps 596(596.98) | Grad Norm 0.1474(0.2343) | Total Time 10.00(10.00)\n",
      "Iter 0174 | Time 45.2474(44.7332) | Bit/dim 1.1170(1.1166) | Xent 0.0343(0.0361) | Loss 1.1341(1.1347) | Error 0.0108(0.0118) Steps 596(596.95) | Grad Norm 0.1333(0.2313) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0025 | Time 11.9814, Epoch Time 338.9986(351.5730), Bit/dim 1.1007, Xent 0.0546, Loss 1.1280, Error 0.2138\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0175 | Time 44.6280(44.7301) | Bit/dim 1.1080(1.1163) | Xent 0.0317(0.0360) | Loss 1.1238(1.1343) | Error 0.0102(0.0117) Steps 596(596.92) | Grad Norm 0.1535(0.2289) | Total Time 10.00(10.00)\n",
      "Iter 0176 | Time 45.4507(44.7517) | Bit/dim 1.1136(1.1163) | Xent 0.0326(0.0359) | Loss 1.1299(1.1342) | Error 0.0109(0.0117) Steps 596(596.89) | Grad Norm 0.1772(0.2274) | Total Time 10.00(10.00)\n",
      "Iter 0177 | Time 45.0861(44.7617) | Bit/dim 1.1154(1.1162) | Xent 0.0350(0.0358) | Loss 1.1329(1.1342) | Error 0.0110(0.0117) Steps 596(596.86) | Grad Norm 0.2218(0.2272) | Total Time 10.00(10.00)\n",
      "Iter 0178 | Time 45.4896(44.7836) | Bit/dim 1.1141(1.1162) | Xent 0.0344(0.0358) | Loss 1.1314(1.1341) | Error 0.0111(0.0117) Steps 596(596.84) | Grad Norm 0.1643(0.2253) | Total Time 10.00(10.00)\n",
      "Iter 0179 | Time 44.9492(44.7885) | Bit/dim 1.1177(1.1162) | Xent 0.0378(0.0359) | Loss 1.1366(1.1342) | Error 0.0119(0.0117) Steps 596(596.81) | Grad Norm 0.1365(0.2227) | Total Time 10.00(10.00)\n",
      "Iter 0180 | Time 43.2213(44.7415) | Bit/dim 1.1151(1.1162) | Xent 0.0358(0.0359) | Loss 1.1330(1.1341) | Error 0.0126(0.0117) Steps 596(596.79) | Grad Norm 0.2136(0.2224) | Total Time 10.00(10.00)\n",
      "Iter 0181 | Time 44.5099(44.7346) | Bit/dim 1.1169(1.1162) | Xent 0.0377(0.0359) | Loss 1.1358(1.1342) | Error 0.0119(0.0117) Steps 596(596.76) | Grad Norm 0.1226(0.2194) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0026 | Time 12.0752, Epoch Time 338.5937(351.1836), Bit/dim 1.1006, Xent 0.0532, Loss 1.1272, Error 0.2142\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0182 | Time 44.7639(44.7355) | Bit/dim 1.1139(1.1161) | Xent 0.0384(0.0360) | Loss 1.1332(1.1341) | Error 0.0114(0.0117) Steps 590(596.56) | Grad Norm 0.1666(0.2178) | Total Time 10.00(10.00)\n",
      "Iter 0183 | Time 43.8601(44.7092) | Bit/dim 1.1148(1.1161) | Xent 0.0356(0.0360) | Loss 1.1326(1.1341) | Error 0.0116(0.0117) Steps 596(596.54) | Grad Norm 0.1955(0.2171) | Total Time 10.00(10.00)\n",
      "Iter 0184 | Time 44.5469(44.7043) | Bit/dim 1.1180(1.1162) | Xent 0.0355(0.0360) | Loss 1.1358(1.1341) | Error 0.0114(0.0117) Steps 602(596.71) | Grad Norm 0.1861(0.2162) | Total Time 10.00(10.00)\n",
      "Iter 0185 | Time 44.4475(44.6966) | Bit/dim 1.1138(1.1161) | Xent 0.0388(0.0361) | Loss 1.1332(1.1341) | Error 0.0122(0.0117) Steps 602(596.87) | Grad Norm 0.1457(0.2141) | Total Time 10.00(10.00)\n",
      "Iter 0186 | Time 43.2059(44.6519) | Bit/dim 1.1121(1.1160) | Xent 0.0387(0.0361) | Loss 1.1315(1.1340) | Error 0.0131(0.0117) Steps 590(596.66) | Grad Norm 0.1821(0.2131) | Total Time 10.00(10.00)\n",
      "Iter 0187 | Time 44.6297(44.6512) | Bit/dim 1.1136(1.1159) | Xent 0.0294(0.0359) | Loss 1.1283(1.1339) | Error 0.0099(0.0117) Steps 602(596.82) | Grad Norm 0.1722(0.2119) | Total Time 10.00(10.00)\n",
      "Iter 0188 | Time 44.9772(44.6610) | Bit/dim 1.1146(1.1159) | Xent 0.0405(0.0361) | Loss 1.1348(1.1339) | Error 0.0144(0.0118) Steps 608(597.16) | Grad Norm 0.1534(0.2102) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0027 | Time 11.8661, Epoch Time 335.4170(350.7106), Bit/dim 1.1003, Xent 0.0501, Loss 1.1253, Error 0.2125\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0189 | Time 45.6006(44.6892) | Bit/dim 1.1172(1.1159) | Xent 0.0383(0.0361) | Loss 1.1364(1.1340) | Error 0.0131(0.0118) Steps 608(597.48) | Grad Norm 0.1667(0.2089) | Total Time 10.00(10.00)\n",
      "Iter 0190 | Time 43.7608(44.6613) | Bit/dim 1.1100(1.1157) | Xent 0.0343(0.0361) | Loss 1.1271(1.1338) | Error 0.0115(0.0118) Steps 602(597.62) | Grad Norm 0.1431(0.2069) | Total Time 10.00(10.00)\n",
      "Iter 0191 | Time 44.4355(44.6546) | Bit/dim 1.1156(1.1157) | Xent 0.0362(0.0361) | Loss 1.1337(1.1338) | Error 0.0118(0.0118) Steps 590(597.39) | Grad Norm 0.1219(0.2043) | Total Time 10.00(10.00)\n",
      "Iter 0192 | Time 45.1046(44.6681) | Bit/dim 1.1132(1.1156) | Xent 0.0352(0.0361) | Loss 1.1308(1.1337) | Error 0.0124(0.0118) Steps 596(597.35) | Grad Norm 0.1455(0.2026) | Total Time 10.00(10.00)\n",
      "Iter 0193 | Time 44.9468(44.6764) | Bit/dim 1.1148(1.1156) | Xent 0.0406(0.0362) | Loss 1.1351(1.1337) | Error 0.0135(0.0119) Steps 602(597.49) | Grad Norm 0.1918(0.2022) | Total Time 10.00(10.00)\n",
      "Iter 0194 | Time 45.5145(44.7016) | Bit/dim 1.1140(1.1156) | Xent 0.0363(0.0362) | Loss 1.1321(1.1337) | Error 0.0110(0.0118) Steps 608(597.80) | Grad Norm 0.1483(0.2006) | Total Time 10.00(10.00)\n",
      "Iter 0195 | Time 43.9779(44.6799) | Bit/dim 1.1134(1.1155) | Xent 0.0305(0.0360) | Loss 1.1287(1.1335) | Error 0.0101(0.0118) Steps 596(597.75) | Grad Norm 0.2906(0.2033) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0028 | Time 11.9465, Epoch Time 337.9226(350.3270), Bit/dim 1.0997, Xent 0.0554, Loss 1.1274, Error 0.2138\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0196 | Time 44.5412(44.6757) | Bit/dim 1.1165(1.1155) | Xent 0.0383(0.0361) | Loss 1.1357(1.1336) | Error 0.0125(0.0118) Steps 602(597.88) | Grad Norm 0.1338(0.2012) | Total Time 10.00(10.00)\n",
      "Iter 0197 | Time 44.6898(44.6761) | Bit/dim 1.1137(1.1155) | Xent 0.0379(0.0361) | Loss 1.1326(1.1336) | Error 0.0131(0.0118) Steps 590(597.64) | Grad Norm 0.2535(0.2028) | Total Time 10.00(10.00)\n",
      "Iter 0198 | Time 44.7058(44.6770) | Bit/dim 1.1134(1.1154) | Xent 0.0354(0.0361) | Loss 1.1311(1.1335) | Error 0.0111(0.0118) Steps 596(597.59) | Grad Norm 0.1824(0.2022) | Total Time 10.00(10.00)\n",
      "Iter 0199 | Time 42.8360(44.6218) | Bit/dim 1.1114(1.1153) | Xent 0.0366(0.0361) | Loss 1.1297(1.1334) | Error 0.0125(0.0118) Steps 596(597.54) | Grad Norm 0.1500(0.2006) | Total Time 10.00(10.00)\n",
      "Iter 0200 | Time 45.2299(44.6400) | Bit/dim 1.1174(1.1154) | Xent 0.0323(0.0360) | Loss 1.1336(1.1334) | Error 0.0095(0.0118) Steps 590(597.32) | Grad Norm 0.2291(0.2015) | Total Time 10.00(10.00)\n",
      "Iter 0201 | Time 45.1539(44.6554) | Bit/dim 1.1115(1.1152) | Xent 0.0347(0.0360) | Loss 1.1288(1.1332) | Error 0.0115(0.0118) Steps 602(597.46) | Grad Norm 0.1577(0.2002) | Total Time 10.00(10.00)\n",
      "Iter 0202 | Time 44.7550(44.6584) | Bit/dim 1.1115(1.1151) | Xent 0.0408(0.0361) | Loss 1.1319(1.1332) | Error 0.0146(0.0118) Steps 596(597.41) | Grad Norm 0.1918(0.1999) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0029 | Time 11.9398, Epoch Time 336.9633(349.9261), Bit/dim 1.0997, Xent 0.0526, Loss 1.1260, Error 0.2138\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0203 | Time 44.6640(44.6586) | Bit/dim 1.1152(1.1151) | Xent 0.0359(0.0361) | Loss 1.1331(1.1332) | Error 0.0116(0.0118) Steps 596(597.37) | Grad Norm 0.1760(0.1992) | Total Time 10.00(10.00)\n",
      "Iter 0204 | Time 44.3412(44.6491) | Bit/dim 1.1154(1.1151) | Xent 0.0344(0.0361) | Loss 1.1326(1.1332) | Error 0.0111(0.0118) Steps 596(597.33) | Grad Norm 0.1826(0.1987) | Total Time 10.00(10.00)\n",
      "Iter 0205 | Time 44.6836(44.6501) | Bit/dim 1.1122(1.1151) | Xent 0.0356(0.0361) | Loss 1.1301(1.1331) | Error 0.0109(0.0118) Steps 602(597.47) | Grad Norm 0.1587(0.1975) | Total Time 10.00(10.00)\n",
      "Iter 0206 | Time 46.0112(44.6909) | Bit/dim 1.1085(1.1149) | Xent 0.0372(0.0361) | Loss 1.1271(1.1329) | Error 0.0112(0.0118) Steps 602(597.61) | Grad Norm 0.3070(0.2008) | Total Time 10.00(10.00)\n",
      "Iter 0207 | Time 45.1264(44.7040) | Bit/dim 1.1146(1.1148) | Xent 0.0366(0.0361) | Loss 1.1329(1.1329) | Error 0.0131(0.0118) Steps 602(597.74) | Grad Norm 0.1952(0.2006) | Total Time 10.00(10.00)\n",
      "Iter 0208 | Time 44.2201(44.6895) | Bit/dim 1.1139(1.1148) | Xent 0.0342(0.0360) | Loss 1.1310(1.1328) | Error 0.0104(0.0118) Steps 596(597.69) | Grad Norm 0.3343(0.2046) | Total Time 10.00(10.00)\n",
      "Iter 0209 | Time 46.7947(44.7526) | Bit/dim 1.1110(1.1147) | Xent 0.0378(0.0361) | Loss 1.1299(1.1328) | Error 0.0114(0.0118) Steps 590(597.45) | Grad Norm 0.3159(0.2080) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0030 | Time 11.9546, Epoch Time 340.4446(349.6416), Bit/dim 1.1000, Xent 0.0523, Loss 1.1261, Error 0.2135\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0210 | Time 45.1104(44.7634) | Bit/dim 1.1143(1.1147) | Xent 0.0367(0.0361) | Loss 1.1327(1.1328) | Error 0.0114(0.0117) Steps 602(597.59) | Grad Norm 0.3045(0.2109) | Total Time 10.00(10.00)\n",
      "Iter 0211 | Time 44.2299(44.7474) | Bit/dim 1.1106(1.1146) | Xent 0.0338(0.0360) | Loss 1.1275(1.1326) | Error 0.0108(0.0117) Steps 608(597.90) | Grad Norm 0.2343(0.2116) | Total Time 10.00(10.00)\n",
      "Iter 0212 | Time 44.4344(44.7380) | Bit/dim 1.1150(1.1146) | Xent 0.0316(0.0359) | Loss 1.1308(1.1325) | Error 0.0104(0.0117) Steps 596(597.85) | Grad Norm 0.1726(0.2104) | Total Time 10.00(10.00)\n",
      "Iter 0213 | Time 46.1122(44.7792) | Bit/dim 1.1124(1.1145) | Xent 0.0385(0.0360) | Loss 1.1317(1.1325) | Error 0.0116(0.0117) Steps 596(597.79) | Grad Norm 0.4029(0.2162) | Total Time 10.00(10.00)\n",
      "Iter 0214 | Time 44.4318(44.7688) | Bit/dim 1.1160(1.1146) | Xent 0.0322(0.0359) | Loss 1.1321(1.1325) | Error 0.0099(0.0116) Steps 608(598.10) | Grad Norm 0.1613(0.2145) | Total Time 10.00(10.00)\n",
      "Iter 0215 | Time 44.7451(44.7681) | Bit/dim 1.1092(1.1144) | Xent 0.0343(0.0358) | Loss 1.1264(1.1323) | Error 0.0108(0.0116) Steps 596(598.03) | Grad Norm 0.2362(0.2152) | Total Time 10.00(10.00)\n",
      "Iter 0216 | Time 44.1993(44.7510) | Bit/dim 1.1112(1.1143) | Xent 0.0358(0.0358) | Loss 1.1291(1.1322) | Error 0.0111(0.0116) Steps 596(597.97) | Grad Norm 0.1637(0.2136) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0031 | Time 11.9415, Epoch Time 338.0428(349.2937), Bit/dim 1.0992, Xent 0.0535, Loss 1.1259, Error 0.2137\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0217 | Time 44.5936(44.7463) | Bit/dim 1.1088(1.1141) | Xent 0.0377(0.0359) | Loss 1.1277(1.1321) | Error 0.0120(0.0116) Steps 602(598.09) | Grad Norm 0.1431(0.2115) | Total Time 10.00(10.00)\n",
      "Iter 0218 | Time 45.2354(44.7610) | Bit/dim 1.1136(1.1141) | Xent 0.0361(0.0359) | Loss 1.1316(1.1321) | Error 0.0116(0.0116) Steps 596(598.03) | Grad Norm 0.1952(0.2110) | Total Time 10.00(10.00)\n",
      "Iter 0219 | Time 44.6697(44.7582) | Bit/dim 1.1120(1.1141) | Xent 0.0331(0.0358) | Loss 1.1286(1.1320) | Error 0.0120(0.0116) Steps 596(597.97) | Grad Norm 0.3446(0.2150) | Total Time 10.00(10.00)\n",
      "Iter 0220 | Time 45.1393(44.7697) | Bit/dim 1.1138(1.1141) | Xent 0.0318(0.0357) | Loss 1.1297(1.1319) | Error 0.0108(0.0116) Steps 602(598.09) | Grad Norm 0.2647(0.2165) | Total Time 10.00(10.00)\n",
      "Iter 0221 | Time 46.0401(44.8078) | Bit/dim 1.1120(1.1140) | Xent 0.0395(0.0358) | Loss 1.1318(1.1319) | Error 0.0118(0.0116) Steps 596(598.03) | Grad Norm 0.3363(0.2201) | Total Time 10.00(10.00)\n",
      "Iter 0222 | Time 45.6750(44.8338) | Bit/dim 1.1123(1.1139) | Xent 0.0347(0.0358) | Loss 1.1297(1.1318) | Error 0.0125(0.0116) Steps 608(598.33) | Grad Norm 0.3031(0.2226) | Total Time 10.00(10.00)\n",
      "Iter 0223 | Time 44.2527(44.8164) | Bit/dim 1.1156(1.1140) | Xent 0.0383(0.0359) | Loss 1.1347(1.1319) | Error 0.0122(0.0116) Steps 602(598.44) | Grad Norm 0.1871(0.2215) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0032 | Time 12.1377, Epoch Time 340.2472(349.0223), Bit/dim 1.0980, Xent 0.0487, Loss 1.1224, Error 0.2134\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0224 | Time 44.7626(44.8147) | Bit/dim 1.1141(1.1140) | Xent 0.0360(0.0359) | Loss 1.1321(1.1319) | Error 0.0109(0.0116) Steps 590(598.18) | Grad Norm 0.3501(0.2254) | Total Time 10.00(10.00)\n",
      "Iter 0225 | Time 45.1935(44.8261) | Bit/dim 1.1088(1.1138) | Xent 0.0296(0.0357) | Loss 1.1236(1.1317) | Error 0.0090(0.0115) Steps 596(598.12) | Grad Norm 0.2745(0.2269) | Total Time 10.00(10.00)\n",
      "Iter 0226 | Time 45.7153(44.8528) | Bit/dim 1.1124(1.1138) | Xent 0.0358(0.0357) | Loss 1.1303(1.1316) | Error 0.0115(0.0115) Steps 596(598.06) | Grad Norm 0.3339(0.2301) | Total Time 10.00(10.00)\n",
      "Iter 0227 | Time 44.2915(44.8359) | Bit/dim 1.1110(1.1137) | Xent 0.0315(0.0356) | Loss 1.1268(1.1315) | Error 0.0100(0.0115) Steps 602(598.17) | Grad Norm 0.4033(0.2353) | Total Time 10.00(10.00)\n",
      "Iter 0228 | Time 45.6533(44.8605) | Bit/dim 1.1150(1.1138) | Xent 0.0327(0.0355) | Loss 1.1314(1.1315) | Error 0.0098(0.0114) Steps 596(598.11) | Grad Norm 0.1586(0.2330) | Total Time 10.00(10.00)\n",
      "Iter 0229 | Time 45.5013(44.8797) | Bit/dim 1.1132(1.1137) | Xent 0.0362(0.0355) | Loss 1.1313(1.1315) | Error 0.0104(0.0114) Steps 608(598.41) | Grad Norm 0.3307(0.2359) | Total Time 10.00(10.00)\n",
      "Iter 0230 | Time 45.4866(44.8979) | Bit/dim 1.1103(1.1136) | Xent 0.0389(0.0356) | Loss 1.1298(1.1314) | Error 0.0125(0.0114) Steps 602(598.51) | Grad Norm 0.1844(0.2344) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0033 | Time 11.7530, Epoch Time 341.1083(348.7848), Bit/dim 1.0985, Xent 0.0562, Loss 1.1266, Error 0.2141\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0231 | Time 44.5364(44.8870) | Bit/dim 1.1103(1.1135) | Xent 0.0339(0.0355) | Loss 1.1273(1.1313) | Error 0.0116(0.0114) Steps 596(598.44) | Grad Norm 0.1938(0.2331) | Total Time 10.00(10.00)\n",
      "Iter 0232 | Time 44.5060(44.8756) | Bit/dim 1.1129(1.1135) | Xent 0.0384(0.0356) | Loss 1.1321(1.1313) | Error 0.0134(0.0115) Steps 602(598.54) | Grad Norm 0.1847(0.2317) | Total Time 10.00(10.00)\n",
      "Iter 0233 | Time 43.1930(44.8251) | Bit/dim 1.1081(1.1133) | Xent 0.0349(0.0356) | Loss 1.1256(1.1312) | Error 0.0108(0.0115) Steps 602(598.65) | Grad Norm 0.3528(0.2353) | Total Time 10.00(10.00)\n",
      "Iter 0234 | Time 43.0764(44.7727) | Bit/dim 1.1147(1.1134) | Xent 0.0362(0.0356) | Loss 1.1328(1.1312) | Error 0.0122(0.0115) Steps 596(598.57) | Grad Norm 0.4331(0.2413) | Total Time 10.00(10.00)\n",
      "Iter 0235 | Time 44.6070(44.7677) | Bit/dim 1.1124(1.1134) | Xent 0.0370(0.0357) | Loss 1.1309(1.1312) | Error 0.0126(0.0115) Steps 590(598.31) | Grad Norm 0.4112(0.2464) | Total Time 10.00(10.00)\n",
      "Iter 0236 | Time 45.7890(44.7983) | Bit/dim 1.1175(1.1135) | Xent 0.0341(0.0356) | Loss 1.1346(1.1313) | Error 0.0111(0.0115) Steps 596(598.24) | Grad Norm 0.2929(0.2478) | Total Time 10.00(10.00)\n",
      "Iter 0237 | Time 45.0879(44.8070) | Bit/dim 1.1079(1.1133) | Xent 0.0337(0.0356) | Loss 1.1247(1.1311) | Error 0.0115(0.0115) Steps 584(597.82) | Grad Norm 0.1231(0.2440) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0034 | Time 11.9292, Epoch Time 335.4156(348.3838), Bit/dim 1.0978, Xent 0.0540, Loss 1.1248, Error 0.2137\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0238 | Time 45.0217(44.8135) | Bit/dim 1.1131(1.1133) | Xent 0.0410(0.0357) | Loss 1.1336(1.1312) | Error 0.0134(0.0116) Steps 596(597.76) | Grad Norm 0.4176(0.2492) | Total Time 10.00(10.00)\n",
      "Iter 0239 | Time 44.7758(44.8123) | Bit/dim 1.1153(1.1134) | Xent 0.0306(0.0356) | Loss 1.1305(1.1312) | Error 0.0105(0.0115) Steps 590(597.53) | Grad Norm 0.1307(0.2457) | Total Time 10.00(10.00)\n",
      "Iter 0240 | Time 45.2450(44.8253) | Bit/dim 1.1133(1.1134) | Xent 0.0326(0.0355) | Loss 1.1295(1.1311) | Error 0.0105(0.0115) Steps 596(597.48) | Grad Norm 0.4793(0.2527) | Total Time 10.00(10.00)\n",
      "Iter 0241 | Time 44.2640(44.8085) | Bit/dim 1.1108(1.1133) | Xent 0.0356(0.0355) | Loss 1.1285(1.1310) | Error 0.0121(0.0115) Steps 596(597.44) | Grad Norm 0.2861(0.2537) | Total Time 10.00(10.00)\n",
      "Iter 0242 | Time 45.4959(44.8291) | Bit/dim 1.1082(1.1131) | Xent 0.0359(0.0355) | Loss 1.1262(1.1309) | Error 0.0122(0.0116) Steps 590(597.21) | Grad Norm 0.4854(0.2606) | Total Time 10.00(10.00)\n",
      "Iter 0243 | Time 43.4899(44.7889) | Bit/dim 1.1133(1.1131) | Xent 0.0363(0.0355) | Loss 1.1315(1.1309) | Error 0.0135(0.0116) Steps 590(597.00) | Grad Norm 0.4255(0.2656) | Total Time 10.00(10.00)\n",
      "Iter 0244 | Time 44.2263(44.7720) | Bit/dim 1.1099(1.1130) | Xent 0.0331(0.0354) | Loss 1.1265(1.1308) | Error 0.0110(0.0116) Steps 596(596.97) | Grad Norm 0.3392(0.2678) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0035 | Time 11.9582, Epoch Time 337.5890(348.0599), Bit/dim 1.0975, Xent 0.0501, Loss 1.1225, Error 0.2138\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0245 | Time 44.2894(44.7576) | Bit/dim 1.1140(1.1131) | Xent 0.0350(0.0354) | Loss 1.1315(1.1308) | Error 0.0111(0.0116) Steps 590(596.76) | Grad Norm 0.5038(0.2749) | Total Time 10.00(10.00)\n",
      "Iter 0246 | Time 45.4388(44.7780) | Bit/dim 1.1125(1.1131) | Xent 0.0349(0.0354) | Loss 1.1299(1.1308) | Error 0.0116(0.0116) Steps 596(596.74) | Grad Norm 0.1749(0.2719) | Total Time 10.00(10.00)\n",
      "Iter 0247 | Time 43.7759(44.7479) | Bit/dim 1.1122(1.1130) | Xent 0.0366(0.0354) | Loss 1.1305(1.1308) | Error 0.0109(0.0116) Steps 584(596.35) | Grad Norm 0.6626(0.2836) | Total Time 10.00(10.00)\n",
      "Iter 0248 | Time 43.7367(44.7176) | Bit/dim 1.1130(1.1130) | Xent 0.0296(0.0353) | Loss 1.1278(1.1307) | Error 0.0116(0.0116) Steps 596(596.34) | Grad Norm 0.1524(0.2797) | Total Time 10.00(10.00)\n",
      "Iter 0249 | Time 45.8921(44.7528) | Bit/dim 1.1078(1.1129) | Xent 0.0408(0.0354) | Loss 1.1282(1.1306) | Error 0.0119(0.0116) Steps 596(596.33) | Grad Norm 0.3686(0.2823) | Total Time 10.00(10.00)\n",
      "Iter 0250 | Time 45.2976(44.7692) | Bit/dim 1.1105(1.1128) | Xent 0.0287(0.0352) | Loss 1.1248(1.1304) | Error 0.0099(0.0115) Steps 596(596.32) | Grad Norm 0.2276(0.2807) | Total Time 10.00(10.00)\n",
      "Iter 0251 | Time 44.0075(44.7463) | Bit/dim 1.1115(1.1128) | Xent 0.0389(0.0353) | Loss 1.1310(1.1304) | Error 0.0104(0.0115) Steps 596(596.31) | Grad Norm 0.1591(0.2770) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0036 | Time 12.0666, Epoch Time 337.1955(347.7340), Bit/dim 1.0971, Xent 0.0525, Loss 1.1234, Error 0.2132\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0252 | Time 45.3149(44.7634) | Bit/dim 1.1088(1.1126) | Xent 0.0379(0.0354) | Loss 1.1277(1.1304) | Error 0.0124(0.0115) Steps 584(595.94) | Grad Norm 0.1787(0.2741) | Total Time 10.00(10.00)\n",
      "Iter 0253 | Time 44.0811(44.7429) | Bit/dim 1.1085(1.1125) | Xent 0.0289(0.0352) | Loss 1.1230(1.1301) | Error 0.0096(0.0115) Steps 590(595.77) | Grad Norm 0.3519(0.2764) | Total Time 10.00(10.00)\n",
      "Iter 0254 | Time 43.7865(44.7142) | Bit/dim 1.1126(1.1125) | Xent 0.0360(0.0352) | Loss 1.1306(1.1301) | Error 0.0105(0.0114) Steps 590(595.59) | Grad Norm 0.1825(0.2736) | Total Time 10.00(10.00)\n",
      "Iter 0255 | Time 43.9615(44.6916) | Bit/dim 1.1100(1.1124) | Xent 0.0413(0.0354) | Loss 1.1306(1.1302) | Error 0.0131(0.0115) Steps 590(595.43) | Grad Norm 0.1890(0.2711) | Total Time 10.00(10.00)\n",
      "Iter 0256 | Time 45.4009(44.7129) | Bit/dim 1.1142(1.1125) | Xent 0.0353(0.0354) | Loss 1.1318(1.1302) | Error 0.0108(0.0115) Steps 596(595.44) | Grad Norm 0.2248(0.2697) | Total Time 10.00(10.00)\n",
      "Iter 0257 | Time 44.4962(44.7064) | Bit/dim 1.1096(1.1124) | Xent 0.0377(0.0355) | Loss 1.1284(1.1302) | Error 0.0122(0.0115) Steps 590(595.28) | Grad Norm 0.2285(0.2684) | Total Time 10.00(10.00)\n",
      "Iter 0258 | Time 44.0194(44.6858) | Bit/dim 1.1157(1.1125) | Xent 0.0391(0.0356) | Loss 1.1352(1.1303) | Error 0.0131(0.0115) Steps 596(595.30) | Grad Norm 0.2140(0.2668) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0037 | Time 11.9487, Epoch Time 335.5735(347.3692), Bit/dim 1.0976, Xent 0.0518, Loss 1.1235, Error 0.2134\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0259 | Time 43.3944(44.6471) | Bit/dim 1.1084(1.1124) | Xent 0.0319(0.0355) | Loss 1.1243(1.1301) | Error 0.0096(0.0115) Steps 590(595.14) | Grad Norm 0.3278(0.2686) | Total Time 10.00(10.00)\n",
      "Iter 0260 | Time 42.7007(44.5887) | Bit/dim 1.1146(1.1124) | Xent 0.0329(0.0354) | Loss 1.1310(1.1302) | Error 0.0122(0.0115) Steps 590(594.99) | Grad Norm 0.1959(0.2665) | Total Time 10.00(10.00)\n",
      "Iter 0261 | Time 43.4927(44.5558) | Bit/dim 1.1070(1.1123) | Xent 0.0338(0.0354) | Loss 1.1239(1.1300) | Error 0.0106(0.0115) Steps 584(594.66) | Grad Norm 0.2349(0.2655) | Total Time 10.00(10.00)\n",
      "Iter 0262 | Time 44.1717(44.5443) | Bit/dim 1.1106(1.1122) | Xent 0.0355(0.0354) | Loss 1.1283(1.1299) | Error 0.0106(0.0114) Steps 590(594.52) | Grad Norm 0.4352(0.2706) | Total Time 10.00(10.00)\n",
      "Iter 0263 | Time 44.2094(44.5342) | Bit/dim 1.1117(1.1122) | Xent 0.0333(0.0353) | Loss 1.1283(1.1299) | Error 0.0102(0.0114) Steps 590(594.38) | Grad Norm 0.2336(0.2695) | Total Time 10.00(10.00)\n",
      "Iter 0264 | Time 43.6023(44.5063) | Bit/dim 1.1156(1.1123) | Xent 0.0332(0.0352) | Loss 1.1322(1.1299) | Error 0.0100(0.0114) Steps 590(594.25) | Grad Norm 0.4009(0.2734) | Total Time 10.00(10.00)\n",
      "Iter 0265 | Time 45.5880(44.5387) | Bit/dim 1.1074(1.1122) | Xent 0.0338(0.0352) | Loss 1.1243(1.1298) | Error 0.0115(0.0114) Steps 596(594.30) | Grad Norm 0.1681(0.2703) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0038 | Time 11.7215, Epoch Time 331.4947(346.8929), Bit/dim 1.0958, Xent 0.0504, Loss 1.1210, Error 0.2125\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0266 | Time 44.5828(44.5400) | Bit/dim 1.1152(1.1123) | Xent 0.0311(0.0351) | Loss 1.1308(1.1298) | Error 0.0108(0.0114) Steps 578(593.81) | Grad Norm 0.4349(0.2752) | Total Time 10.00(10.00)\n",
      "Iter 0267 | Time 43.0613(44.4957) | Bit/dim 1.1100(1.1122) | Xent 0.0341(0.0350) | Loss 1.1271(1.1297) | Error 0.0099(0.0113) Steps 584(593.52) | Grad Norm 0.2071(0.2732) | Total Time 10.00(10.00)\n",
      "Iter 0268 | Time 44.1299(44.4847) | Bit/dim 1.1110(1.1122) | Xent 0.0337(0.0350) | Loss 1.1279(1.1297) | Error 0.0111(0.0113) Steps 596(593.59) | Grad Norm 0.2678(0.2730) | Total Time 10.00(10.00)\n",
      "Iter 0269 | Time 44.3342(44.4802) | Bit/dim 1.1075(1.1120) | Xent 0.0396(0.0351) | Loss 1.1272(1.1296) | Error 0.0115(0.0113) Steps 590(593.49) | Grad Norm 0.5321(0.2808) | Total Time 10.00(10.00)\n",
      "Iter 0270 | Time 44.1982(44.4717) | Bit/dim 1.1073(1.1119) | Xent 0.0420(0.0353) | Loss 1.1283(1.1296) | Error 0.0124(0.0113) Steps 584(593.20) | Grad Norm 0.4535(0.2860) | Total Time 10.00(10.00)\n",
      "Iter 0271 | Time 42.9250(44.4253) | Bit/dim 1.1078(1.1118) | Xent 0.0328(0.0353) | Loss 1.1242(1.1294) | Error 0.0086(0.0113) Steps 596(593.29) | Grad Norm 0.1408(0.2816) | Total Time 10.00(10.00)\n",
      "Iter 0272 | Time 44.7125(44.4339) | Bit/dim 1.1148(1.1118) | Xent 0.0356(0.0353) | Loss 1.1326(1.1295) | Error 0.0121(0.0113) Steps 596(593.37) | Grad Norm 0.1519(0.2777) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0039 | Time 12.1600, Epoch Time 332.9332(346.4742), Bit/dim 1.0971, Xent 0.0552, Loss 1.1247, Error 0.2145\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0273 | Time 44.0612(44.4228) | Bit/dim 1.1090(1.1118) | Xent 0.0411(0.0355) | Loss 1.1295(1.1295) | Error 0.0140(0.0114) Steps 596(593.45) | Grad Norm 0.1626(0.2743) | Total Time 10.00(10.00)\n",
      "Iter 0274 | Time 43.6582(44.3998) | Bit/dim 1.1110(1.1117) | Xent 0.0355(0.0355) | Loss 1.1287(1.1295) | Error 0.0112(0.0114) Steps 590(593.34) | Grad Norm 0.3353(0.2761) | Total Time 10.00(10.00)\n",
      "Iter 0275 | Time 44.4052(44.4000) | Bit/dim 1.1133(1.1118) | Xent 0.0352(0.0355) | Loss 1.1310(1.1295) | Error 0.0112(0.0114) Steps 596(593.42) | Grad Norm 0.2140(0.2742) | Total Time 10.00(10.00)\n",
      "Iter 0276 | Time 44.5982(44.4059) | Bit/dim 1.1090(1.1117) | Xent 0.0393(0.0356) | Loss 1.1287(1.1295) | Error 0.0118(0.0114) Steps 596(593.50) | Grad Norm 0.4085(0.2783) | Total Time 10.00(10.00)\n",
      "Iter 0277 | Time 44.1226(44.3974) | Bit/dim 1.1067(1.1116) | Xent 0.0325(0.0355) | Loss 1.1229(1.1293) | Error 0.0106(0.0113) Steps 590(593.39) | Grad Norm 0.2874(0.2785) | Total Time 10.00(10.00)\n",
      "Iter 0278 | Time 44.2462(44.3929) | Bit/dim 1.1102(1.1115) | Xent 0.0336(0.0354) | Loss 1.1270(1.1292) | Error 0.0115(0.0114) Steps 596(593.47) | Grad Norm 0.2987(0.2791) | Total Time 10.00(10.00)\n",
      "Iter 0279 | Time 43.9295(44.3790) | Bit/dim 1.1155(1.1116) | Xent 0.0308(0.0353) | Loss 1.1310(1.1293) | Error 0.0108(0.0113) Steps 590(593.37) | Grad Norm 0.2726(0.2789) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0040 | Time 12.0515, Epoch Time 334.0010(346.1000), Bit/dim 1.0972, Xent 0.0510, Loss 1.1227, Error 0.2123\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0280 | Time 44.6616(44.3875) | Bit/dim 1.1052(1.1114) | Xent 0.0314(0.0352) | Loss 1.1209(1.1290) | Error 0.0106(0.0113) Steps 590(593.27) | Grad Norm 0.2116(0.2769) | Total Time 10.00(10.00)\n",
      "Iter 0281 | Time 42.5108(44.3312) | Bit/dim 1.1132(1.1115) | Xent 0.0338(0.0351) | Loss 1.1301(1.1291) | Error 0.0111(0.0113) Steps 590(593.17) | Grad Norm 0.1706(0.2737) | Total Time 10.00(10.00)\n",
      "Iter 0282 | Time 43.3936(44.3030) | Bit/dim 1.1097(1.1114) | Xent 0.0311(0.0350) | Loss 1.1253(1.1289) | Error 0.0088(0.0112) Steps 590(593.07) | Grad Norm 0.1821(0.2710) | Total Time 10.00(10.00)\n",
      "Iter 0283 | Time 44.8035(44.3181) | Bit/dim 1.1137(1.1115) | Xent 0.0409(0.0352) | Loss 1.1342(1.1291) | Error 0.0131(0.0113) Steps 596(593.16) | Grad Norm 0.2252(0.2696) | Total Time 10.00(10.00)\n",
      "Iter 0284 | Time 44.8778(44.3349) | Bit/dim 1.1117(1.1115) | Xent 0.0313(0.0351) | Loss 1.1274(1.1290) | Error 0.0105(0.0113) Steps 590(593.07) | Grad Norm 0.2508(0.2690) | Total Time 10.00(10.00)\n",
      "Iter 0285 | Time 43.4392(44.3080) | Bit/dim 1.1081(1.1114) | Xent 0.0333(0.0350) | Loss 1.1248(1.1289) | Error 0.0106(0.0112) Steps 590(592.98) | Grad Norm 0.3897(0.2727) | Total Time 10.00(10.00)\n",
      "Iter 0286 | Time 45.3663(44.3397) | Bit/dim 1.1084(1.1113) | Xent 0.0368(0.0351) | Loss 1.1268(1.1289) | Error 0.0115(0.0113) Steps 584(592.71) | Grad Norm 0.2111(0.2708) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0041 | Time 12.2234, Epoch Time 334.0470(345.7384), Bit/dim 1.0957, Xent 0.0567, Loss 1.1241, Error 0.2138\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0287 | Time 43.8521(44.3251) | Bit/dim 1.1044(1.1111) | Xent 0.0380(0.0352) | Loss 1.1234(1.1287) | Error 0.0124(0.0113) Steps 596(592.81) | Grad Norm 0.6013(0.2807) | Total Time 10.00(10.00)\n",
      "Iter 0288 | Time 43.6554(44.3050) | Bit/dim 1.1076(1.1110) | Xent 0.0394(0.0353) | Loss 1.1273(1.1286) | Error 0.0126(0.0113) Steps 590(592.72) | Grad Norm 0.1929(0.2781) | Total Time 10.00(10.00)\n",
      "Iter 0289 | Time 44.6741(44.3161) | Bit/dim 1.1111(1.1110) | Xent 0.0338(0.0352) | Loss 1.1279(1.1286) | Error 0.0115(0.0113) Steps 590(592.64) | Grad Norm 0.7337(0.2918) | Total Time 10.00(10.00)\n",
      "Iter 0290 | Time 44.5265(44.3224) | Bit/dim 1.1163(1.1112) | Xent 0.0385(0.0353) | Loss 1.1356(1.1288) | Error 0.0108(0.0113) Steps 584(592.38) | Grad Norm 0.3287(0.2929) | Total Time 10.00(10.00)\n",
      "Iter 0291 | Time 44.0357(44.3138) | Bit/dim 1.1081(1.1111) | Xent 0.0304(0.0352) | Loss 1.1234(1.1287) | Error 0.0101(0.0113) Steps 578(591.95) | Grad Norm 0.4200(0.2967) | Total Time 10.00(10.00)\n",
      "Iter 0292 | Time 43.6114(44.2927) | Bit/dim 1.1124(1.1111) | Xent 0.0390(0.0353) | Loss 1.1318(1.1288) | Error 0.0139(0.0114) Steps 584(591.71) | Grad Norm 0.6379(0.3069) | Total Time 10.00(10.00)\n",
      "Iter 0293 | Time 43.4515(44.2675) | Bit/dim 1.1090(1.1111) | Xent 0.0294(0.0351) | Loss 1.1237(1.1286) | Error 0.0096(0.0113) Steps 584(591.48) | Grad Norm 0.1811(0.3032) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0042 | Time 12.2659, Epoch Time 332.8604(345.3520), Bit/dim 1.0957, Xent 0.0546, Loss 1.1230, Error 0.2135\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0294 | Time 44.6054(44.2776) | Bit/dim 1.1099(1.1110) | Xent 0.0327(0.0351) | Loss 1.1263(1.1285) | Error 0.0114(0.0113) Steps 584(591.25) | Grad Norm 0.6767(0.3144) | Total Time 10.00(10.00)\n",
      "Iter 0295 | Time 44.0165(44.2698) | Bit/dim 1.1138(1.1111) | Xent 0.0347(0.0350) | Loss 1.1312(1.1286) | Error 0.0105(0.0113) Steps 590(591.22) | Grad Norm 0.2832(0.3134) | Total Time 10.00(10.00)\n",
      "Iter 0296 | Time 43.8312(44.2566) | Bit/dim 1.1086(1.1110) | Xent 0.0358(0.0351) | Loss 1.1265(1.1286) | Error 0.0111(0.0113) Steps 578(590.82) | Grad Norm 0.7097(0.3253) | Total Time 10.00(10.00)\n",
      "Iter 0297 | Time 44.9971(44.2789) | Bit/dim 1.1095(1.1110) | Xent 0.0309(0.0349) | Loss 1.1250(1.1285) | Error 0.0112(0.0113) Steps 578(590.44) | Grad Norm 0.2693(0.3236) | Total Time 10.00(10.00)\n",
      "Iter 0298 | Time 44.4562(44.2842) | Bit/dim 1.1089(1.1109) | Xent 0.0348(0.0349) | Loss 1.1263(1.1284) | Error 0.0128(0.0113) Steps 590(590.42) | Grad Norm 0.5106(0.3292) | Total Time 10.00(10.00)\n",
      "Iter 0299 | Time 43.6636(44.2656) | Bit/dim 1.1098(1.1109) | Xent 0.0390(0.0351) | Loss 1.1293(1.1284) | Error 0.0125(0.0114) Steps 584(590.23) | Grad Norm 0.2685(0.3274) | Total Time 10.00(10.00)\n",
      "Iter 0300 | Time 42.7933(44.2214) | Bit/dim 1.1059(1.1107) | Xent 0.0356(0.0351) | Loss 1.1237(1.1283) | Error 0.0126(0.0114) Steps 572(589.68) | Grad Norm 0.2882(0.3262) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0043 | Time 11.9771, Epoch Time 333.1180(344.9850), Bit/dim 1.0958, Xent 0.0511, Loss 1.1213, Error 0.2131\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0301 | Time 43.7947(44.2086) | Bit/dim 1.1099(1.1107) | Xent 0.0310(0.0350) | Loss 1.1254(1.1282) | Error 0.0111(0.0114) Steps 584(589.51) | Grad Norm 0.2964(0.3253) | Total Time 10.00(10.00)\n",
      "Iter 0302 | Time 43.7119(44.1937) | Bit/dim 1.1110(1.1107) | Xent 0.0403(0.0351) | Loss 1.1311(1.1283) | Error 0.0136(0.0115) Steps 578(589.17) | Grad Norm 0.1394(0.3198) | Total Time 10.00(10.00)\n",
      "Iter 0303 | Time 43.4891(44.1726) | Bit/dim 1.1085(1.1107) | Xent 0.0361(0.0351) | Loss 1.1266(1.1282) | Error 0.0111(0.0114) Steps 578(588.83) | Grad Norm 0.5739(0.3274) | Total Time 10.00(10.00)\n",
      "Iter 0304 | Time 44.2664(44.1754) | Bit/dim 1.1043(1.1105) | Xent 0.0380(0.0352) | Loss 1.1233(1.1281) | Error 0.0125(0.0115) Steps 584(588.69) | Grad Norm 0.3615(0.3284) | Total Time 10.00(10.00)\n",
      "Iter 0305 | Time 42.7643(44.1330) | Bit/dim 1.1115(1.1105) | Xent 0.0325(0.0351) | Loss 1.1278(1.1281) | Error 0.0110(0.0115) Steps 590(588.73) | Grad Norm 0.1656(0.3235) | Total Time 10.00(10.00)\n",
      "Iter 0306 | Time 45.0774(44.1614) | Bit/dim 1.1082(1.1104) | Xent 0.0355(0.0352) | Loss 1.1259(1.1280) | Error 0.0110(0.0114) Steps 590(588.76) | Grad Norm 0.1850(0.3194) | Total Time 10.00(10.00)\n",
      "Iter 0307 | Time 43.8983(44.1535) | Bit/dim 1.1102(1.1104) | Xent 0.0322(0.0351) | Loss 1.1263(1.1280) | Error 0.0118(0.0115) Steps 584(588.62) | Grad Norm 0.1699(0.3149) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0044 | Time 11.9230, Epoch Time 331.6281(344.5843), Bit/dim 1.0949, Xent 0.0490, Loss 1.1194, Error 0.2122\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0308 | Time 44.1574(44.1536) | Bit/dim 1.1118(1.1105) | Xent 0.0364(0.0351) | Loss 1.1301(1.1280) | Error 0.0136(0.0115) Steps 584(588.48) | Grad Norm 0.2042(0.3116) | Total Time 10.00(10.00)\n",
      "Iter 0309 | Time 43.4709(44.1331) | Bit/dim 1.1042(1.1103) | Xent 0.0361(0.0351) | Loss 1.1222(1.1278) | Error 0.0126(0.0116) Steps 590(588.53) | Grad Norm 0.1618(0.3071) | Total Time 10.00(10.00)\n",
      "Iter 0310 | Time 43.0704(44.1012) | Bit/dim 1.1126(1.1103) | Xent 0.0375(0.0352) | Loss 1.1314(1.1280) | Error 0.0124(0.0116) Steps 584(588.39) | Grad Norm 0.2871(0.3065) | Total Time 10.00(10.00)\n",
      "Iter 0311 | Time 44.0173(44.0987) | Bit/dim 1.1098(1.1103) | Xent 0.0338(0.0352) | Loss 1.1267(1.1279) | Error 0.0100(0.0115) Steps 590(588.44) | Grad Norm 0.1590(0.3021) | Total Time 10.00(10.00)\n",
      "Iter 0312 | Time 44.2952(44.1046) | Bit/dim 1.1126(1.1104) | Xent 0.0335(0.0351) | Loss 1.1293(1.1280) | Error 0.0102(0.0115) Steps 590(588.49) | Grad Norm 0.1668(0.2980) | Total Time 10.00(10.00)\n",
      "Iter 0313 | Time 43.3561(44.0821) | Bit/dim 1.1063(1.1103) | Xent 0.0327(0.0350) | Loss 1.1227(1.1278) | Error 0.0104(0.0115) Steps 590(588.53) | Grad Norm 0.1649(0.2940) | Total Time 10.00(10.00)\n",
      "Iter 0314 | Time 44.4004(44.0917) | Bit/dim 1.1047(1.1101) | Xent 0.0354(0.0351) | Loss 1.1224(1.1276) | Error 0.0112(0.0115) Steps 590(588.58) | Grad Norm 0.1346(0.2892) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0045 | Time 11.8419, Epoch Time 331.4340(344.1898), Bit/dim 1.0952, Xent 0.0522, Loss 1.1213, Error 0.2142\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0315 | Time 44.0525(44.0905) | Bit/dim 1.1087(1.1101) | Xent 0.0291(0.0349) | Loss 1.1232(1.1275) | Error 0.0084(0.0114) Steps 590(588.62) | Grad Norm 0.3685(0.2916) | Total Time 10.00(10.00)\n",
      "Iter 0316 | Time 43.9140(44.0852) | Bit/dim 1.1053(1.1099) | Xent 0.0344(0.0349) | Loss 1.1225(1.1274) | Error 0.0128(0.0114) Steps 584(588.48) | Grad Norm 0.2966(0.2918) | Total Time 10.00(10.00)\n",
      "Iter 0317 | Time 43.8436(44.0780) | Bit/dim 1.1062(1.1098) | Xent 0.0351(0.0349) | Loss 1.1237(1.1272) | Error 0.0111(0.0114) Steps 590(588.53) | Grad Norm 0.3794(0.2944) | Total Time 10.00(10.00)\n",
      "Iter 0318 | Time 43.9393(44.0738) | Bit/dim 1.1107(1.1098) | Xent 0.0376(0.0350) | Loss 1.1295(1.1273) | Error 0.0120(0.0114) Steps 590(588.57) | Grad Norm 0.4244(0.2983) | Total Time 10.00(10.00)\n",
      "Iter 0319 | Time 44.6334(44.0906) | Bit/dim 1.1100(1.1098) | Xent 0.0329(0.0349) | Loss 1.1264(1.1273) | Error 0.0105(0.0114) Steps 584(588.43) | Grad Norm 0.1643(0.2943) | Total Time 10.00(10.00)\n",
      "Iter 0320 | Time 43.2705(44.0660) | Bit/dim 1.1142(1.1100) | Xent 0.0411(0.0351) | Loss 1.1347(1.1275) | Error 0.0135(0.0114) Steps 590(588.48) | Grad Norm 0.2506(0.2930) | Total Time 10.00(10.00)\n",
      "Iter 0321 | Time 43.5427(44.0503) | Bit/dim 1.1075(1.1099) | Xent 0.0323(0.0350) | Loss 1.1237(1.1274) | Error 0.0102(0.0114) Steps 590(588.53) | Grad Norm 0.2048(0.2903) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0046 | Time 11.9794, Epoch Time 331.9503(343.8226), Bit/dim 1.0943, Xent 0.0556, Loss 1.1221, Error 0.2141\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0322 | Time 42.5109(44.0041) | Bit/dim 1.1110(1.1099) | Xent 0.0303(0.0349) | Loss 1.1261(1.1274) | Error 0.0090(0.0113) Steps 590(588.57) | Grad Norm 0.3335(0.2916) | Total Time 10.00(10.00)\n",
      "Iter 0323 | Time 44.1027(44.0071) | Bit/dim 1.1068(1.1098) | Xent 0.0348(0.0349) | Loss 1.1241(1.1273) | Error 0.0118(0.0114) Steps 596(588.79) | Grad Norm 0.2467(0.2903) | Total Time 10.00(10.00)\n",
      "Iter 0324 | Time 43.6285(43.9957) | Bit/dim 1.1092(1.1098) | Xent 0.0351(0.0349) | Loss 1.1267(1.1272) | Error 0.0115(0.0114) Steps 584(588.65) | Grad Norm 0.1616(0.2864) | Total Time 10.00(10.00)\n",
      "Iter 0325 | Time 43.9673(43.9949) | Bit/dim 1.1050(1.1097) | Xent 0.0336(0.0348) | Loss 1.1218(1.1271) | Error 0.0101(0.0113) Steps 590(588.69) | Grad Norm 0.2266(0.2846) | Total Time 10.00(10.00)\n",
      "Iter 0326 | Time 43.8022(43.9891) | Bit/dim 1.1113(1.1097) | Xent 0.0445(0.0351) | Loss 1.1335(1.1273) | Error 0.0135(0.0114) Steps 590(588.73) | Grad Norm 0.2170(0.2826) | Total Time 10.00(10.00)\n",
      "Iter 0327 | Time 41.9372(43.9275) | Bit/dim 1.1075(1.1097) | Xent 0.0407(0.0353) | Loss 1.1279(1.1273) | Error 0.0135(0.0114) Steps 590(588.77) | Grad Norm 0.4082(0.2863) | Total Time 10.00(10.00)\n",
      "Iter 0328 | Time 44.1439(43.9340) | Bit/dim 1.1077(1.1096) | Xent 0.0338(0.0352) | Loss 1.1247(1.1272) | Error 0.0118(0.0115) Steps 590(588.80) | Grad Norm 0.1644(0.2827) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0047 | Time 12.1042, Epoch Time 328.9600(343.3767), Bit/dim 1.0945, Xent 0.0499, Loss 1.1194, Error 0.2128\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0329 | Time 43.2160(43.9125) | Bit/dim 1.1090(1.1096) | Xent 0.0379(0.0353) | Loss 1.1280(1.1272) | Error 0.0115(0.0115) Steps 584(588.66) | Grad Norm 0.1400(0.2784) | Total Time 10.00(10.00)\n",
      "Iter 0330 | Time 44.3208(43.9247) | Bit/dim 1.1092(1.1096) | Xent 0.0314(0.0352) | Loss 1.1249(1.1272) | Error 0.0101(0.0114) Steps 590(588.70) | Grad Norm 0.1478(0.2745) | Total Time 10.00(10.00)\n",
      "Iter 0331 | Time 44.8576(43.9527) | Bit/dim 1.1090(1.1095) | Xent 0.0364(0.0352) | Loss 1.1272(1.1272) | Error 0.0125(0.0115) Steps 590(588.74) | Grad Norm 0.2648(0.2742) | Total Time 10.00(10.00)\n",
      "Iter 0332 | Time 44.8470(43.9795) | Bit/dim 1.1051(1.1094) | Xent 0.0341(0.0352) | Loss 1.1221(1.1270) | Error 0.0112(0.0114) Steps 584(588.60) | Grad Norm 0.2632(0.2739) | Total Time 10.00(10.00)\n",
      "Iter 0333 | Time 43.4606(43.9640) | Bit/dim 1.1077(1.1094) | Xent 0.0347(0.0352) | Loss 1.1250(1.1270) | Error 0.0111(0.0114) Steps 584(588.46) | Grad Norm 0.1582(0.2704) | Total Time 10.00(10.00)\n",
      "Iter 0334 | Time 45.1307(43.9990) | Bit/dim 1.1102(1.1094) | Xent 0.0331(0.0351) | Loss 1.1268(1.1270) | Error 0.0115(0.0114) Steps 590(588.51) | Grad Norm 0.2089(0.2685) | Total Time 10.00(10.00)\n",
      "Iter 0335 | Time 43.6784(43.9894) | Bit/dim 1.1086(1.1094) | Xent 0.0321(0.0350) | Loss 1.1247(1.1269) | Error 0.0099(0.0114) Steps 584(588.37) | Grad Norm 0.1817(0.2659) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0048 | Time 12.0880, Epoch Time 334.9040(343.1225), Bit/dim 1.0952, Xent 0.0542, Loss 1.1223, Error 0.2130\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0336 | Time 43.5704(43.9768) | Bit/dim 1.1128(1.1095) | Xent 0.0369(0.0351) | Loss 1.1313(1.1270) | Error 0.0118(0.0114) Steps 590(588.42) | Grad Norm 0.3580(0.2687) | Total Time 10.00(10.00)\n",
      "Iter 0337 | Time 43.5253(43.9633) | Bit/dim 1.1158(1.1097) | Xent 0.0355(0.0351) | Loss 1.1335(1.1272) | Error 0.0119(0.0114) Steps 578(588.11) | Grad Norm 0.2767(0.2689) | Total Time 10.00(10.00)\n",
      "Iter 0338 | Time 43.5915(43.9521) | Bit/dim 1.1075(1.1096) | Xent 0.0311(0.0350) | Loss 1.1230(1.1271) | Error 0.0094(0.0114) Steps 584(587.98) | Grad Norm 0.2175(0.2674) | Total Time 10.00(10.00)\n",
      "Iter 0339 | Time 42.9418(43.9218) | Bit/dim 1.0992(1.1093) | Xent 0.0313(0.0349) | Loss 1.1149(1.1267) | Error 0.0104(0.0113) Steps 590(588.04) | Grad Norm 0.2275(0.2662) | Total Time 10.00(10.00)\n",
      "Iter 0340 | Time 43.2917(43.9029) | Bit/dim 1.1061(1.1092) | Xent 0.0349(0.0349) | Loss 1.1236(1.1266) | Error 0.0120(0.0113) Steps 578(587.74) | Grad Norm 0.2307(0.2651) | Total Time 10.00(10.00)\n",
      "Iter 0341 | Time 42.8118(43.8702) | Bit/dim 1.1076(1.1091) | Xent 0.0352(0.0349) | Loss 1.1252(1.1266) | Error 0.0105(0.0113) Steps 590(587.81) | Grad Norm 0.2615(0.2650) | Total Time 10.00(10.00)\n",
      "Iter 0342 | Time 44.6570(43.8938) | Bit/dim 1.1018(1.1089) | Xent 0.0365(0.0349) | Loss 1.1201(1.1264) | Error 0.0124(0.0114) Steps 590(587.88) | Grad Norm 0.1843(0.2626) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0049 | Time 12.0064, Epoch Time 329.3283(342.7087), Bit/dim 1.0942, Xent 0.0537, Loss 1.1211, Error 0.2138\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0343 | Time 43.7021(43.8880) | Bit/dim 1.1055(1.1088) | Xent 0.0413(0.0351) | Loss 1.1262(1.1264) | Error 0.0116(0.0114) Steps 584(587.76) | Grad Norm 0.1318(0.2587) | Total Time 10.00(10.00)\n",
      "Iter 0344 | Time 44.2350(43.8984) | Bit/dim 1.1101(1.1089) | Xent 0.0326(0.0350) | Loss 1.1264(1.1264) | Error 0.0099(0.0113) Steps 590(587.83) | Grad Norm 0.3438(0.2612) | Total Time 10.00(10.00)\n",
      "Iter 0345 | Time 42.9676(43.8705) | Bit/dim 1.1092(1.1089) | Xent 0.0328(0.0350) | Loss 1.1256(1.1264) | Error 0.0102(0.0113) Steps 584(587.71) | Grad Norm 0.1658(0.2584) | Total Time 10.00(10.00)\n",
      "Iter 0346 | Time 44.5907(43.8921) | Bit/dim 1.1083(1.1089) | Xent 0.0357(0.0350) | Loss 1.1262(1.1264) | Error 0.0129(0.0113) Steps 590(587.78) | Grad Norm 0.3331(0.2606) | Total Time 10.00(10.00)\n",
      "Iter 0347 | Time 43.5072(43.8806) | Bit/dim 1.1023(1.1087) | Xent 0.0355(0.0350) | Loss 1.1200(1.1262) | Error 0.0128(0.0114) Steps 590(587.85) | Grad Norm 0.1804(0.2582) | Total Time 10.00(10.00)\n",
      "Iter 0348 | Time 43.8463(43.8795) | Bit/dim 1.1084(1.1086) | Xent 0.0343(0.0350) | Loss 1.1255(1.1261) | Error 0.0109(0.0114) Steps 584(587.73) | Grad Norm 0.6296(0.2694) | Total Time 10.00(10.00)\n",
      "Iter 0349 | Time 43.8206(43.8778) | Bit/dim 1.1083(1.1086) | Xent 0.0345(0.0350) | Loss 1.1256(1.1261) | Error 0.0118(0.0114) Steps 578(587.44) | Grad Norm 0.1544(0.2659) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0050 | Time 11.9122, Epoch Time 331.4598(342.3713), Bit/dim 1.0938, Xent 0.0534, Loss 1.1205, Error 0.2135\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0350 | Time 43.3972(43.8633) | Bit/dim 1.1057(1.1086) | Xent 0.0369(0.0350) | Loss 1.1241(1.1261) | Error 0.0120(0.0114) Steps 596(587.70) | Grad Norm 0.6554(0.2776) | Total Time 10.00(10.00)\n",
      "Iter 0351 | Time 44.6116(43.8858) | Bit/dim 1.1107(1.1086) | Xent 0.0355(0.0351) | Loss 1.1284(1.1261) | Error 0.0115(0.0114) Steps 584(587.59) | Grad Norm 0.2346(0.2763) | Total Time 10.00(10.00)\n",
      "Iter 0352 | Time 42.6241(43.8479) | Bit/dim 1.1084(1.1086) | Xent 0.0352(0.0351) | Loss 1.1260(1.1261) | Error 0.0116(0.0114) Steps 584(587.48) | Grad Norm 0.9506(0.2965) | Total Time 10.00(10.00)\n",
      "Iter 0353 | Time 43.5634(43.8394) | Bit/dim 1.1085(1.1086) | Xent 0.0289(0.0349) | Loss 1.1230(1.1260) | Error 0.0098(0.0114) Steps 584(587.37) | Grad Norm 0.1312(0.2916) | Total Time 10.00(10.00)\n",
      "Iter 0354 | Time 44.6987(43.8652) | Bit/dim 1.1074(1.1086) | Xent 0.0367(0.0349) | Loss 1.1257(1.1260) | Error 0.0116(0.0114) Steps 584(587.27) | Grad Norm 0.8438(0.3081) | Total Time 10.00(10.00)\n",
      "Iter 0355 | Time 42.3101(43.8185) | Bit/dim 1.1094(1.1086) | Xent 0.0344(0.0349) | Loss 1.1266(1.1260) | Error 0.0106(0.0113) Steps 578(586.99) | Grad Norm 0.3310(0.3088) | Total Time 10.00(10.00)\n",
      "Iter 0356 | Time 42.8431(43.7893) | Bit/dim 1.1049(1.1085) | Xent 0.0407(0.0351) | Loss 1.1253(1.1260) | Error 0.0131(0.0114) Steps 590(587.08) | Grad Norm 0.3303(0.3095) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0051 | Time 11.9762, Epoch Time 328.7757(341.9634), Bit/dim 1.0931, Xent 0.0560, Loss 1.1212, Error 0.2135\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0357 | Time 44.1191(43.7992) | Bit/dim 1.1096(1.1085) | Xent 0.0423(0.0353) | Loss 1.1308(1.1262) | Error 0.0131(0.0114) Steps 590(587.17) | Grad Norm 0.1799(0.3056) | Total Time 10.00(10.00)\n",
      "Iter 0358 | Time 43.7911(43.7989) | Bit/dim 1.1070(1.1085) | Xent 0.0306(0.0352) | Loss 1.1223(1.1261) | Error 0.0105(0.0114) Steps 584(587.08) | Grad Norm 0.3495(0.3069) | Total Time 10.00(10.00)\n",
      "Iter 0359 | Time 42.4417(43.7582) | Bit/dim 1.1090(1.1085) | Xent 0.0296(0.0350) | Loss 1.1238(1.1260) | Error 0.0109(0.0114) Steps 584(586.98) | Grad Norm 0.1342(0.3017) | Total Time 10.00(10.00)\n",
      "Iter 0360 | Time 44.5343(43.7815) | Bit/dim 1.1043(1.1084) | Xent 0.0396(0.0351) | Loss 1.1241(1.1259) | Error 0.0128(0.0114) Steps 590(587.08) | Grad Norm 0.2427(0.2999) | Total Time 10.00(10.00)\n",
      "Iter 0361 | Time 42.9054(43.7552) | Bit/dim 1.1079(1.1084) | Xent 0.0313(0.0350) | Loss 1.1235(1.1259) | Error 0.0102(0.0114) Steps 584(586.98) | Grad Norm 0.1354(0.2950) | Total Time 10.00(10.00)\n",
      "Iter 0362 | Time 43.9012(43.7596) | Bit/dim 1.1057(1.1083) | Xent 0.0306(0.0349) | Loss 1.1211(1.1257) | Error 0.0096(0.0113) Steps 584(586.89) | Grad Norm 0.2735(0.2944) | Total Time 10.00(10.00)\n",
      "Iter 0363 | Time 42.0687(43.7089) | Bit/dim 1.1062(1.1082) | Xent 0.0331(0.0348) | Loss 1.1228(1.1256) | Error 0.0114(0.0114) Steps 578(586.63) | Grad Norm 0.2235(0.2922) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0052 | Time 11.9064, Epoch Time 328.4188(341.5570), Bit/dim 1.0933, Xent 0.0505, Loss 1.1186, Error 0.2123\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0364 | Time 42.8702(43.6837) | Bit/dim 1.1106(1.1083) | Xent 0.0322(0.0348) | Loss 1.1267(1.1257) | Error 0.0100(0.0113) Steps 584(586.55) | Grad Norm 0.3078(0.2927) | Total Time 10.00(10.00)\n",
      "Iter 0365 | Time 44.0648(43.6951) | Bit/dim 1.1116(1.1084) | Xent 0.0290(0.0346) | Loss 1.1261(1.1257) | Error 0.0096(0.0113) Steps 590(586.65) | Grad Norm 0.5238(0.2996) | Total Time 10.00(10.00)\n",
      "Iter 0366 | Time 42.4703(43.6584) | Bit/dim 1.1063(1.1083) | Xent 0.0356(0.0346) | Loss 1.1241(1.1256) | Error 0.0105(0.0112) Steps 584(586.57) | Grad Norm 0.1742(0.2959) | Total Time 10.00(10.00)\n",
      "Iter 0367 | Time 43.7342(43.6607) | Bit/dim 1.1056(1.1082) | Xent 0.0370(0.0347) | Loss 1.1242(1.1256) | Error 0.0120(0.0113) Steps 590(586.67) | Grad Norm 0.3824(0.2985) | Total Time 10.00(10.00)\n",
      "Iter 0368 | Time 43.6999(43.6618) | Bit/dim 1.1029(1.1081) | Xent 0.0333(0.0346) | Loss 1.1196(1.1254) | Error 0.0104(0.0112) Steps 590(586.77) | Grad Norm 0.1814(0.2950) | Total Time 10.00(10.00)\n",
      "Iter 0369 | Time 43.7524(43.6645) | Bit/dim 1.1037(1.1079) | Xent 0.0329(0.0346) | Loss 1.1202(1.1252) | Error 0.0109(0.0112) Steps 578(586.51) | Grad Norm 0.1529(0.2907) | Total Time 10.00(10.00)\n",
      "Iter 0370 | Time 43.4350(43.6577) | Bit/dim 1.1042(1.1078) | Xent 0.0282(0.0344) | Loss 1.1183(1.1250) | Error 0.0086(0.0111) Steps 590(586.62) | Grad Norm 0.2432(0.2893) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0053 | Time 11.8077, Epoch Time 328.4977(341.1653), Bit/dim 1.0930, Xent 0.0510, Loss 1.1185, Error 0.2117\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0371 | Time 42.7765(43.6312) | Bit/dim 1.1088(1.1079) | Xent 0.0351(0.0344) | Loss 1.1264(1.1251) | Error 0.0121(0.0112) Steps 584(586.54) | Grad Norm 0.3808(0.2920) | Total Time 10.00(10.00)\n",
      "Iter 0372 | Time 45.5379(43.6884) | Bit/dim 1.1097(1.1079) | Xent 0.0329(0.0344) | Loss 1.1261(1.1251) | Error 0.0109(0.0112) Steps 584(586.46) | Grad Norm 0.2258(0.2900) | Total Time 10.00(10.00)\n",
      "Iter 0373 | Time 45.0079(43.7280) | Bit/dim 1.1020(1.1077) | Xent 0.0369(0.0345) | Loss 1.1205(1.1250) | Error 0.0118(0.0112) Steps 584(586.39) | Grad Norm 0.3119(0.2907) | Total Time 10.00(10.00)\n",
      "Iter 0374 | Time 43.0159(43.7067) | Bit/dim 1.1013(1.1075) | Xent 0.0362(0.0345) | Loss 1.1194(1.1248) | Error 0.0125(0.0112) Steps 590(586.50) | Grad Norm 0.5687(0.2990) | Total Time 10.00(10.00)\n",
      "Iter 0375 | Time 44.2604(43.7233) | Bit/dim 1.1067(1.1075) | Xent 0.0385(0.0346) | Loss 1.1259(1.1248) | Error 0.0120(0.0112) Steps 578(586.24) | Grad Norm 0.2870(0.2987) | Total Time 10.00(10.00)\n",
      "Iter 0376 | Time 42.1304(43.6755) | Bit/dim 1.1122(1.1077) | Xent 0.0325(0.0346) | Loss 1.1285(1.1249) | Error 0.0114(0.0112) Steps 578(585.99) | Grad Norm 0.1879(0.2953) | Total Time 10.00(10.00)\n",
      "Iter 0377 | Time 42.2830(43.6337) | Bit/dim 1.1072(1.1076) | Xent 0.0290(0.0344) | Loss 1.1217(1.1248) | Error 0.0090(0.0112) Steps 578(585.75) | Grad Norm 0.2259(0.2933) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0054 | Time 12.0193, Epoch Time 329.7841(340.8238), Bit/dim 1.0927, Xent 0.0500, Loss 1.1176, Error 0.2128\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0378 | Time 43.7900(43.6384) | Bit/dim 1.1078(1.1076) | Xent 0.0336(0.0344) | Loss 1.1246(1.1248) | Error 0.0114(0.0112) Steps 590(585.88) | Grad Norm 0.1788(0.2898) | Total Time 10.00(10.00)\n",
      "Iter 0379 | Time 43.5976(43.6372) | Bit/dim 1.1061(1.1076) | Xent 0.0323(0.0343) | Loss 1.1223(1.1248) | Error 0.0106(0.0112) Steps 584(585.82) | Grad Norm 0.1717(0.2863) | Total Time 10.00(10.00)\n",
      "Iter 0380 | Time 43.3399(43.6283) | Bit/dim 1.1116(1.1077) | Xent 0.0334(0.0343) | Loss 1.1283(1.1249) | Error 0.0106(0.0112) Steps 584(585.77) | Grad Norm 0.1573(0.2824) | Total Time 10.00(10.00)\n",
      "Iter 0381 | Time 42.2687(43.5875) | Bit/dim 1.1040(1.1076) | Xent 0.0363(0.0343) | Loss 1.1222(1.1248) | Error 0.0116(0.0112) Steps 584(585.72) | Grad Norm 0.2320(0.2809) | Total Time 10.00(10.00)\n",
      "Iter 0382 | Time 43.0656(43.5718) | Bit/dim 1.1039(1.1075) | Xent 0.0349(0.0344) | Loss 1.1214(1.1247) | Error 0.0114(0.0112) Steps 584(585.67) | Grad Norm 0.2243(0.2792) | Total Time 10.00(10.00)\n",
      "Iter 0383 | Time 43.8869(43.5813) | Bit/dim 1.1045(1.1074) | Xent 0.0411(0.0346) | Loss 1.1250(1.1247) | Error 0.0126(0.0112) Steps 584(585.62) | Grad Norm 0.4277(0.2837) | Total Time 10.00(10.00)\n",
      "Iter 0384 | Time 42.7329(43.5558) | Bit/dim 1.1104(1.1075) | Xent 0.0361(0.0346) | Loss 1.1284(1.1248) | Error 0.0104(0.0112) Steps 590(585.75) | Grad Norm 0.3318(0.2851) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0055 | Time 11.9876, Epoch Time 327.4855(340.4237), Bit/dim 1.0922, Xent 0.0494, Loss 1.1170, Error 0.2127\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0385 | Time 44.0692(43.5712) | Bit/dim 1.1084(1.1075) | Xent 0.0344(0.0346) | Loss 1.1256(1.1248) | Error 0.0108(0.0112) Steps 590(585.87) | Grad Norm 0.6686(0.2966) | Total Time 10.00(10.00)\n",
      "Iter 0386 | Time 42.7799(43.5475) | Bit/dim 1.1052(1.1075) | Xent 0.0403(0.0348) | Loss 1.1253(1.1248) | Error 0.0122(0.0112) Steps 578(585.64) | Grad Norm 0.1832(0.2932) | Total Time 10.00(10.00)\n",
      "Iter 0387 | Time 43.0770(43.5334) | Bit/dim 1.1049(1.1074) | Xent 0.0410(0.0350) | Loss 1.1254(1.1249) | Error 0.0124(0.0112) Steps 584(585.59) | Grad Norm 0.5621(0.3013) | Total Time 10.00(10.00)\n",
      "Iter 0388 | Time 43.3330(43.5273) | Bit/dim 1.1040(1.1073) | Xent 0.0340(0.0349) | Loss 1.1210(1.1247) | Error 0.0111(0.0112) Steps 584(585.54) | Grad Norm 0.1654(0.2972) | Total Time 10.00(10.00)\n",
      "Iter 0389 | Time 44.7134(43.5629) | Bit/dim 1.1050(1.1072) | Xent 0.0349(0.0349) | Loss 1.1225(1.1247) | Error 0.0104(0.0112) Steps 590(585.68) | Grad Norm 0.4704(0.3024) | Total Time 10.00(10.00)\n",
      "Iter 0390 | Time 44.8073(43.6003) | Bit/dim 1.1084(1.1072) | Xent 0.0327(0.0349) | Loss 1.1247(1.1247) | Error 0.0106(0.0112) Steps 590(585.81) | Grad Norm 0.3022(0.3024) | Total Time 10.00(10.00)\n",
      "Iter 0391 | Time 44.2817(43.6207) | Bit/dim 1.1065(1.1072) | Xent 0.0336(0.0348) | Loss 1.1233(1.1246) | Error 0.0112(0.0112) Steps 590(585.93) | Grad Norm 0.7531(0.3159) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0056 | Time 12.0585, Epoch Time 331.9197(340.1686), Bit/dim 1.0923, Xent 0.0566, Loss 1.1206, Error 0.2145\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0392 | Time 43.3024(43.6112) | Bit/dim 1.1075(1.1072) | Xent 0.0341(0.0348) | Loss 1.1246(1.1246) | Error 0.0114(0.0112) Steps 584(585.87) | Grad Norm 0.3202(0.3160) | Total Time 10.00(10.00)\n",
      "Iter 0393 | Time 44.4656(43.6368) | Bit/dim 1.1045(1.1072) | Xent 0.0295(0.0346) | Loss 1.1192(1.1245) | Error 0.0095(0.0112) Steps 590(586.00) | Grad Norm 0.2949(0.3154) | Total Time 10.00(10.00)\n",
      "Iter 0394 | Time 44.9705(43.6768) | Bit/dim 1.1032(1.1070) | Xent 0.0365(0.0347) | Loss 1.1214(1.1244) | Error 0.0110(0.0111) Steps 590(586.12) | Grad Norm 0.2858(0.3145) | Total Time 10.00(10.00)\n",
      "Iter 0395 | Time 43.3260(43.6663) | Bit/dim 1.1050(1.1070) | Xent 0.0338(0.0347) | Loss 1.1219(1.1243) | Error 0.0114(0.0112) Steps 584(586.05) | Grad Norm 0.1416(0.3093) | Total Time 10.00(10.00)\n",
      "Iter 0396 | Time 44.2859(43.6849) | Bit/dim 1.1042(1.1069) | Xent 0.0374(0.0348) | Loss 1.1229(1.1243) | Error 0.0115(0.0112) Steps 584(585.99) | Grad Norm 0.2003(0.3061) | Total Time 10.00(10.00)\n",
      "Iter 0397 | Time 43.3150(43.6738) | Bit/dim 1.1074(1.1069) | Xent 0.0360(0.0348) | Loss 1.1254(1.1243) | Error 0.0111(0.0112) Steps 584(585.93) | Grad Norm 0.3614(0.3077) | Total Time 10.00(10.00)\n",
      "Iter 0398 | Time 42.5604(43.6404) | Bit/dim 1.1065(1.1069) | Xent 0.0290(0.0346) | Loss 1.1210(1.1242) | Error 0.0104(0.0111) Steps 584(585.87) | Grad Norm 0.2197(0.3051) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0057 | Time 11.9498, Epoch Time 330.8243(339.8882), Bit/dim 1.0924, Xent 0.0518, Loss 1.1183, Error 0.2126\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0399 | Time 43.7692(43.6442) | Bit/dim 1.1038(1.1068) | Xent 0.0352(0.0346) | Loss 1.1213(1.1241) | Error 0.0118(0.0112) Steps 596(586.18) | Grad Norm 0.1588(0.3007) | Total Time 10.00(10.00)\n",
      "Iter 0400 | Time 43.9325(43.6529) | Bit/dim 1.1110(1.1069) | Xent 0.0306(0.0345) | Loss 1.1263(1.1242) | Error 0.0098(0.0111) Steps 590(586.29) | Grad Norm 0.2329(0.2987) | Total Time 10.00(10.00)\n",
      "Iter 0401 | Time 44.0933(43.6661) | Bit/dim 1.1033(1.1068) | Xent 0.0297(0.0344) | Loss 1.1181(1.1240) | Error 0.0094(0.0111) Steps 590(586.40) | Grad Norm 0.3619(0.3006) | Total Time 10.00(10.00)\n",
      "Iter 0402 | Time 43.6008(43.6641) | Bit/dim 1.1035(1.1067) | Xent 0.0315(0.0343) | Loss 1.1192(1.1239) | Error 0.0104(0.0110) Steps 584(586.33) | Grad Norm 0.2019(0.2976) | Total Time 10.00(10.00)\n",
      "Iter 0403 | Time 42.8134(43.6386) | Bit/dim 1.1024(1.1066) | Xent 0.0338(0.0343) | Loss 1.1193(1.1237) | Error 0.0118(0.0111) Steps 578(586.08) | Grad Norm 0.3743(0.2999) | Total Time 10.00(10.00)\n",
      "Iter 0404 | Time 44.0085(43.6497) | Bit/dim 1.1054(1.1065) | Xent 0.0382(0.0344) | Loss 1.1244(1.1237) | Error 0.0112(0.0111) Steps 584(586.02) | Grad Norm 0.5125(0.3063) | Total Time 10.00(10.00)\n",
      "Iter 0405 | Time 44.2614(43.6681) | Bit/dim 1.1089(1.1066) | Xent 0.0336(0.0344) | Loss 1.1257(1.1238) | Error 0.0106(0.0111) Steps 578(585.78) | Grad Norm 0.4477(0.3105) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0058 | Time 11.9392, Epoch Time 331.1642(339.6265), Bit/dim 1.0913, Xent 0.0483, Loss 1.1155, Error 0.2116\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0406 | Time 43.8111(43.6723) | Bit/dim 1.1062(1.1066) | Xent 0.0339(0.0343) | Loss 1.1231(1.1238) | Error 0.0109(0.0111) Steps 578(585.55) | Grad Norm 0.1268(0.3050) | Total Time 10.00(10.00)\n",
      "Iter 0407 | Time 43.7602(43.6750) | Bit/dim 1.1062(1.1066) | Xent 0.0331(0.0343) | Loss 1.1228(1.1237) | Error 0.0101(0.0110) Steps 584(585.50) | Grad Norm 0.1626(0.3007) | Total Time 10.00(10.00)\n",
      "Iter 0408 | Time 43.0461(43.6561) | Bit/dim 1.1041(1.1065) | Xent 0.0339(0.0343) | Loss 1.1210(1.1237) | Error 0.0120(0.0111) Steps 578(585.27) | Grad Norm 0.1719(0.2969) | Total Time 10.00(10.00)\n",
      "Iter 0409 | Time 43.1187(43.6400) | Bit/dim 1.1073(1.1065) | Xent 0.0319(0.0342) | Loss 1.1233(1.1237) | Error 0.0104(0.0110) Steps 584(585.24) | Grad Norm 0.1986(0.2939) | Total Time 10.00(10.00)\n",
      "Iter 0410 | Time 44.2212(43.6574) | Bit/dim 1.1011(1.1064) | Xent 0.0320(0.0342) | Loss 1.1171(1.1235) | Error 0.0086(0.0110) Steps 584(585.20) | Grad Norm 0.2694(0.2932) | Total Time 10.00(10.00)\n",
      "Iter 0411 | Time 43.5516(43.6543) | Bit/dim 1.1049(1.1063) | Xent 0.0316(0.0341) | Loss 1.1207(1.1234) | Error 0.0105(0.0109) Steps 590(585.34) | Grad Norm 0.2387(0.2915) | Total Time 10.00(10.00)\n",
      "Iter 0412 | Time 43.9501(43.6631) | Bit/dim 1.1041(1.1063) | Xent 0.0332(0.0341) | Loss 1.1207(1.1233) | Error 0.0119(0.0110) Steps 584(585.30) | Grad Norm 0.3287(0.2927) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0059 | Time 12.0228, Epoch Time 330.1638(339.3426), Bit/dim 1.0915, Xent 0.0500, Loss 1.1165, Error 0.2126\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0413 | Time 44.1572(43.6780) | Bit/dim 1.1079(1.1063) | Xent 0.0332(0.0340) | Loss 1.1245(1.1233) | Error 0.0116(0.0110) Steps 590(585.44) | Grad Norm 0.5932(0.3017) | Total Time 10.00(10.00)\n",
      "Iter 0414 | Time 45.1034(43.7207) | Bit/dim 1.1068(1.1063) | Xent 0.0319(0.0340) | Loss 1.1228(1.1233) | Error 0.0100(0.0110) Steps 584(585.40) | Grad Norm 0.4344(0.3057) | Total Time 10.00(10.00)\n",
      "Iter 0415 | Time 43.4324(43.7121) | Bit/dim 1.1067(1.1063) | Xent 0.0356(0.0340) | Loss 1.1245(1.1234) | Error 0.0115(0.0110) Steps 584(585.36) | Grad Norm 0.2436(0.3038) | Total Time 10.00(10.00)\n",
      "Iter 0416 | Time 43.7700(43.7138) | Bit/dim 1.1039(1.1063) | Xent 0.0291(0.0339) | Loss 1.1185(1.1232) | Error 0.0102(0.0110) Steps 590(585.50) | Grad Norm 0.5269(0.3105) | Total Time 10.00(10.00)\n",
      "Iter 0417 | Time 43.5289(43.7083) | Bit/dim 1.1040(1.1062) | Xent 0.0381(0.0340) | Loss 1.1231(1.1232) | Error 0.0116(0.0110) Steps 584(585.45) | Grad Norm 0.2182(0.3077) | Total Time 10.00(10.00)\n",
      "Iter 0418 | Time 42.2793(43.6654) | Bit/dim 1.1030(1.1061) | Xent 0.0304(0.0339) | Loss 1.1182(1.1231) | Error 0.0112(0.0110) Steps 584(585.41) | Grad Norm 0.6383(0.3176) | Total Time 10.00(10.00)\n",
      "Iter 0419 | Time 44.4205(43.6880) | Bit/dim 1.1082(1.1062) | Xent 0.0359(0.0339) | Loss 1.1261(1.1231) | Error 0.0112(0.0110) Steps 584(585.37) | Grad Norm 0.2529(0.3157) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0060 | Time 12.1102, Epoch Time 331.6164(339.1108), Bit/dim 1.0914, Xent 0.0521, Loss 1.1174, Error 0.2139\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0420 | Time 42.0182(43.6379) | Bit/dim 1.0985(1.1059) | Xent 0.0317(0.0339) | Loss 1.1144(1.1229) | Error 0.0100(0.0110) Steps 584(585.33) | Grad Norm 0.7186(0.3278) | Total Time 10.00(10.00)\n",
      "Iter 0421 | Time 44.2343(43.6558) | Bit/dim 1.1094(1.1060) | Xent 0.0344(0.0339) | Loss 1.1266(1.1230) | Error 0.0118(0.0110) Steps 578(585.11) | Grad Norm 0.4759(0.3322) | Total Time 10.00(10.00)\n",
      "Iter 0422 | Time 42.4612(43.6200) | Bit/dim 1.1043(1.1060) | Xent 0.0372(0.0340) | Loss 1.1228(1.1230) | Error 0.0115(0.0110) Steps 584(585.07) | Grad Norm 0.5937(0.3401) | Total Time 10.00(10.00)\n",
      "Iter 0423 | Time 42.5906(43.5891) | Bit/dim 1.1008(1.1058) | Xent 0.0336(0.0340) | Loss 1.1176(1.1228) | Error 0.0105(0.0110) Steps 584(585.04) | Grad Norm 0.8305(0.3548) | Total Time 10.00(10.00)\n",
      "Iter 0424 | Time 43.7957(43.5953) | Bit/dim 1.1077(1.1059) | Xent 0.0318(0.0339) | Loss 1.1236(1.1228) | Error 0.0111(0.0110) Steps 590(585.19) | Grad Norm 0.2626(0.3520) | Total Time 10.00(10.00)\n",
      "Iter 0425 | Time 44.1962(43.6133) | Bit/dim 1.1035(1.1058) | Xent 0.0402(0.0341) | Loss 1.1236(1.1229) | Error 0.0130(0.0111) Steps 590(585.33) | Grad Norm 0.4719(0.3556) | Total Time 10.00(10.00)\n",
      "Iter 0426 | Time 43.8453(43.6203) | Bit/dim 1.1054(1.1058) | Xent 0.0352(0.0341) | Loss 1.1230(1.1229) | Error 0.0110(0.0111) Steps 584(585.29) | Grad Norm 0.4749(0.3592) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0061 | Time 11.8721, Epoch Time 327.6601(338.7673), Bit/dim 1.0916, Xent 0.0505, Loss 1.1168, Error 0.2133\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0427 | Time 43.0192(43.6023) | Bit/dim 1.1049(1.1058) | Xent 0.0339(0.0341) | Loss 1.1219(1.1228) | Error 0.0114(0.0111) Steps 584(585.25) | Grad Norm 0.2603(0.3562) | Total Time 10.00(10.00)\n",
      "Iter 0428 | Time 42.3981(43.5661) | Bit/dim 1.1054(1.1058) | Xent 0.0353(0.0342) | Loss 1.1230(1.1228) | Error 0.0106(0.0110) Steps 590(585.40) | Grad Norm 0.6300(0.3644) | Total Time 10.00(10.00)\n",
      "Iter 0429 | Time 43.9255(43.5769) | Bit/dim 1.1002(1.1056) | Xent 0.0329(0.0341) | Loss 1.1167(1.1227) | Error 0.0100(0.0110) Steps 584(585.36) | Grad Norm 0.2327(0.3605) | Total Time 10.00(10.00)\n",
      "Iter 0430 | Time 44.3956(43.6015) | Bit/dim 1.1069(1.1056) | Xent 0.0334(0.0341) | Loss 1.1236(1.1227) | Error 0.0108(0.0110) Steps 596(585.67) | Grad Norm 1.1070(0.3829) | Total Time 10.00(10.00)\n",
      "Iter 0431 | Time 43.4821(43.5979) | Bit/dim 1.1071(1.1057) | Xent 0.0402(0.0343) | Loss 1.1272(1.1228) | Error 0.0119(0.0110) Steps 584(585.62) | Grad Norm 0.8054(0.3956) | Total Time 10.00(10.00)\n",
      "Iter 0432 | Time 43.4991(43.5949) | Bit/dim 1.1025(1.1056) | Xent 0.0309(0.0342) | Loss 1.1180(1.1227) | Error 0.0110(0.0110) Steps 584(585.58) | Grad Norm 0.3214(0.3933) | Total Time 10.00(10.00)\n",
      "Iter 0433 | Time 44.4774(43.6214) | Bit/dim 1.1059(1.1056) | Xent 0.0371(0.0343) | Loss 1.1245(1.1227) | Error 0.0118(0.0111) Steps 584(585.53) | Grad Norm 0.5622(0.3984) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0062 | Time 11.8612, Epoch Time 329.9177(338.5018), Bit/dim 1.0907, Xent 0.0535, Loss 1.1175, Error 0.2138\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0434 | Time 43.7991(43.6267) | Bit/dim 1.1050(1.1056) | Xent 0.0354(0.0343) | Loss 1.1227(1.1227) | Error 0.0118(0.0111) Steps 584(585.48) | Grad Norm 0.1906(0.3922) | Total Time 10.00(10.00)\n",
      "Iter 0435 | Time 42.8916(43.6047) | Bit/dim 1.1003(1.1054) | Xent 0.0339(0.0343) | Loss 1.1172(1.1226) | Error 0.0104(0.0111) Steps 584(585.44) | Grad Norm 0.5519(0.3970) | Total Time 10.00(10.00)\n",
      "Iter 0436 | Time 42.6125(43.5749) | Bit/dim 1.1034(1.1054) | Xent 0.0287(0.0341) | Loss 1.1177(1.1224) | Error 0.0089(0.0110) Steps 584(585.39) | Grad Norm 0.3929(0.3968) | Total Time 10.00(10.00)\n",
      "Iter 0437 | Time 43.4804(43.5721) | Bit/dim 1.1112(1.1055) | Xent 0.0368(0.0342) | Loss 1.1296(1.1226) | Error 0.0121(0.0110) Steps 590(585.53) | Grad Norm 0.3336(0.3949) | Total Time 10.00(10.00)\n",
      "Iter 0438 | Time 44.3075(43.5942) | Bit/dim 1.1027(1.1054) | Xent 0.0292(0.0341) | Loss 1.1173(1.1225) | Error 0.0074(0.0109) Steps 584(585.49) | Grad Norm 0.7949(0.4069) | Total Time 10.00(10.00)\n",
      "Iter 0439 | Time 43.5670(43.5933) | Bit/dim 1.1067(1.1055) | Xent 0.0326(0.0340) | Loss 1.1230(1.1225) | Error 0.0116(0.0109) Steps 584(585.44) | Grad Norm 0.4247(0.4075) | Total Time 10.00(10.00)\n",
      "Iter 0440 | Time 44.1481(43.6100) | Bit/dim 1.1019(1.1054) | Xent 0.0342(0.0340) | Loss 1.1190(1.1224) | Error 0.0101(0.0109) Steps 590(585.58) | Grad Norm 0.5663(0.4122) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0063 | Time 11.9920, Epoch Time 329.4693(338.2309), Bit/dim 1.0916, Xent 0.0547, Loss 1.1189, Error 0.2132\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0441 | Time 43.2581(43.5994) | Bit/dim 1.1031(1.1053) | Xent 0.0347(0.0340) | Loss 1.1205(1.1223) | Error 0.0115(0.0109) Steps 584(585.53) | Grad Norm 0.7263(0.4217) | Total Time 10.00(10.00)\n",
      "Iter 0442 | Time 43.2942(43.5903) | Bit/dim 1.1006(1.1052) | Xent 0.0254(0.0338) | Loss 1.1133(1.1221) | Error 0.0086(0.0109) Steps 584(585.49) | Grad Norm 0.1991(0.4150) | Total Time 10.00(10.00)\n",
      "Iter 0443 | Time 44.5653(43.6195) | Bit/dim 1.1064(1.1052) | Xent 0.0358(0.0338) | Loss 1.1243(1.1221) | Error 0.0129(0.0109) Steps 578(585.26) | Grad Norm 0.9288(0.4304) | Total Time 10.00(10.00)\n",
      "Iter 0444 | Time 45.2788(43.6693) | Bit/dim 1.1041(1.1052) | Xent 0.0392(0.0340) | Loss 1.1237(1.1222) | Error 0.0121(0.0110) Steps 584(585.22) | Grad Norm 0.7462(0.4399) | Total Time 10.00(10.00)\n",
      "Iter 0445 | Time 43.7948(43.6731) | Bit/dim 1.1084(1.1053) | Xent 0.0309(0.0339) | Loss 1.1238(1.1222) | Error 0.0089(0.0109) Steps 584(585.19) | Grad Norm 0.2951(0.4355) | Total Time 10.00(10.00)\n",
      "Iter 0446 | Time 43.9271(43.6807) | Bit/dim 1.1034(1.1052) | Xent 0.0355(0.0340) | Loss 1.1211(1.1222) | Error 0.0120(0.0109) Steps 590(585.33) | Grad Norm 1.0810(0.4549) | Total Time 10.00(10.00)\n",
      "Iter 0447 | Time 44.3069(43.6995) | Bit/dim 1.1070(1.1053) | Xent 0.0325(0.0339) | Loss 1.1233(1.1222) | Error 0.0111(0.0109) Steps 584(585.29) | Grad Norm 0.9482(0.4697) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0064 | Time 11.8523, Epoch Time 333.0744(338.0762), Bit/dim 1.0904, Xent 0.0507, Loss 1.1158, Error 0.2119\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0448 | Time 43.0747(43.6807) | Bit/dim 1.1027(1.1052) | Xent 0.0335(0.0339) | Loss 1.1195(1.1221) | Error 0.0096(0.0109) Steps 584(585.25) | Grad Norm 0.2265(0.4624) | Total Time 10.00(10.00)\n",
      "Iter 0449 | Time 42.6186(43.6489) | Bit/dim 1.1060(1.1052) | Xent 0.0381(0.0340) | Loss 1.1251(1.1222) | Error 0.0110(0.0109) Steps 584(585.21) | Grad Norm 1.0230(0.4792) | Total Time 10.00(10.00)\n",
      "Iter 0450 | Time 43.0652(43.6314) | Bit/dim 1.1061(1.1052) | Xent 0.0265(0.0338) | Loss 1.1194(1.1221) | Error 0.0082(0.0108) Steps 584(585.18) | Grad Norm 0.4587(0.4786) | Total Time 10.00(10.00)\n",
      "Iter 0451 | Time 44.5041(43.6575) | Bit/dim 1.1040(1.1052) | Xent 0.0333(0.0338) | Loss 1.1206(1.1221) | Error 0.0120(0.0109) Steps 584(585.14) | Grad Norm 0.6425(0.4835) | Total Time 10.00(10.00)\n",
      "Iter 0452 | Time 43.7433(43.6601) | Bit/dim 1.1065(1.1052) | Xent 0.0432(0.0341) | Loss 1.1281(1.1223) | Error 0.0142(0.0110) Steps 584(585.11) | Grad Norm 0.4723(0.4832) | Total Time 10.00(10.00)\n",
      "Iter 0453 | Time 43.5661(43.6573) | Bit/dim 1.1028(1.1052) | Xent 0.0324(0.0340) | Loss 1.1190(1.1222) | Error 0.0109(0.0110) Steps 578(584.90) | Grad Norm 0.7632(0.4916) | Total Time 10.00(10.00)\n",
      "Iter 0454 | Time 43.4384(43.6507) | Bit/dim 1.1031(1.1051) | Xent 0.0318(0.0340) | Loss 1.1190(1.1221) | Error 0.0106(0.0109) Steps 578(584.69) | Grad Norm 1.2075(0.5131) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0065 | Time 11.9767, Epoch Time 328.7072(337.7951), Bit/dim 1.0904, Xent 0.0542, Loss 1.1175, Error 0.2141\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0455 | Time 44.0739(43.6634) | Bit/dim 1.1071(1.1052) | Xent 0.0365(0.0340) | Loss 1.1254(1.1222) | Error 0.0115(0.0110) Steps 584(584.67) | Grad Norm 0.4633(0.5116) | Total Time 10.00(10.00)\n",
      "Iter 0456 | Time 42.7124(43.6349) | Bit/dim 1.0995(1.1050) | Xent 0.0337(0.0340) | Loss 1.1164(1.1220) | Error 0.0114(0.0110) Steps 584(584.65) | Grad Norm 0.6334(0.5152) | Total Time 10.00(10.00)\n",
      "Iter 0457 | Time 43.1673(43.6209) | Bit/dim 1.1084(1.1051) | Xent 0.0363(0.0341) | Loss 1.1265(1.1221) | Error 0.0114(0.0110) Steps 584(584.63) | Grad Norm 0.6520(0.5193) | Total Time 10.00(10.00)\n",
      "Iter 0458 | Time 42.6865(43.5928) | Bit/dim 1.0956(1.1048) | Xent 0.0293(0.0339) | Loss 1.1102(1.1218) | Error 0.0095(0.0109) Steps 584(584.61) | Grad Norm 0.4677(0.5178) | Total Time 10.00(10.00)\n",
      "Iter 0459 | Time 43.6630(43.5949) | Bit/dim 1.1069(1.1049) | Xent 0.0320(0.0339) | Loss 1.1230(1.1218) | Error 0.0111(0.0109) Steps 584(584.59) | Grad Norm 1.3842(0.5438) | Total Time 10.00(10.00)\n",
      "Iter 0460 | Time 42.3975(43.5590) | Bit/dim 1.1042(1.1049) | Xent 0.0349(0.0339) | Loss 1.1216(1.1218) | Error 0.0114(0.0110) Steps 590(584.75) | Grad Norm 1.0515(0.5590) | Total Time 10.00(10.00)\n",
      "Iter 0461 | Time 44.1049(43.5754) | Bit/dim 1.1067(1.1049) | Xent 0.0314(0.0338) | Loss 1.1224(1.1218) | Error 0.0109(0.0110) Steps 590(584.91) | Grad Norm 0.3344(0.5523) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0066 | Time 11.9807, Epoch Time 327.6207(337.4899), Bit/dim 1.0901, Xent 0.0522, Loss 1.1162, Error 0.2116\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0462 | Time 44.2423(43.5954) | Bit/dim 1.1060(1.1049) | Xent 0.0361(0.0339) | Loss 1.1241(1.1219) | Error 0.0114(0.0110) Steps 590(585.06) | Grad Norm 0.7456(0.5581) | Total Time 10.00(10.00)\n",
      "Iter 0463 | Time 42.3195(43.5571) | Bit/dim 1.1045(1.1049) | Xent 0.0370(0.0340) | Loss 1.1230(1.1219) | Error 0.0114(0.0110) Steps 590(585.21) | Grad Norm 0.3299(0.5512) | Total Time 10.00(10.00)\n",
      "Iter 0464 | Time 42.3677(43.5214) | Bit/dim 1.1027(1.1049) | Xent 0.0328(0.0340) | Loss 1.1191(1.1218) | Error 0.0101(0.0110) Steps 590(585.36) | Grad Norm 0.2462(0.5421) | Total Time 10.00(10.00)\n",
      "Iter 0465 | Time 43.6013(43.5238) | Bit/dim 1.1059(1.1049) | Xent 0.0302(0.0339) | Loss 1.1209(1.1218) | Error 0.0102(0.0109) Steps 590(585.49) | Grad Norm 0.2826(0.5343) | Total Time 10.00(10.00)\n",
      "Iter 0466 | Time 42.8216(43.5028) | Bit/dim 1.1050(1.1049) | Xent 0.0343(0.0339) | Loss 1.1222(1.1218) | Error 0.0118(0.0110) Steps 584(585.45) | Grad Norm 0.3864(0.5298) | Total Time 10.00(10.00)\n",
      "Iter 0467 | Time 42.0813(43.4601) | Bit/dim 1.1031(1.1048) | Xent 0.0325(0.0338) | Loss 1.1194(1.1218) | Error 0.0119(0.0110) Steps 584(585.41) | Grad Norm 0.5661(0.5309) | Total Time 10.00(10.00)\n",
      "Iter 0468 | Time 44.0899(43.4790) | Bit/dim 1.1002(1.1047) | Xent 0.0323(0.0338) | Loss 1.1163(1.1216) | Error 0.0096(0.0109) Steps 584(585.36) | Grad Norm 0.4124(0.5274) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0067 | Time 11.9524, Epoch Time 326.2821(337.1536), Bit/dim 1.0899, Xent 0.0463, Loss 1.1131, Error 0.2115\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0469 | Time 43.1362(43.4687) | Bit/dim 1.1057(1.1047) | Xent 0.0317(0.0337) | Loss 1.1215(1.1216) | Error 0.0100(0.0109) Steps 590(585.50) | Grad Norm 0.4272(0.5244) | Total Time 10.00(10.00)\n",
      "Iter 0470 | Time 42.7958(43.4485) | Bit/dim 1.0997(1.1046) | Xent 0.0376(0.0338) | Loss 1.1185(1.1215) | Error 0.0115(0.0109) Steps 578(585.28) | Grad Norm 0.6335(0.5276) | Total Time 10.00(10.00)\n",
      "Iter 0471 | Time 44.5874(43.4827) | Bit/dim 1.1050(1.1046) | Xent 0.0331(0.0338) | Loss 1.1215(1.1215) | Error 0.0112(0.0109) Steps 590(585.42) | Grad Norm 0.3983(0.5238) | Total Time 10.00(10.00)\n",
      "Iter 0472 | Time 42.6331(43.4572) | Bit/dim 1.1048(1.1046) | Xent 0.0305(0.0337) | Loss 1.1201(1.1215) | Error 0.0118(0.0110) Steps 584(585.38) | Grad Norm 1.1668(0.5431) | Total Time 10.00(10.00)\n",
      "Iter 0473 | Time 44.8168(43.4980) | Bit/dim 1.1036(1.1046) | Xent 0.0363(0.0338) | Loss 1.1217(1.1215) | Error 0.0119(0.0110) Steps 584(585.34) | Grad Norm 0.3484(0.5372) | Total Time 10.00(10.00)\n",
      "Iter 0474 | Time 42.7608(43.4759) | Bit/dim 1.1060(1.1046) | Xent 0.0396(0.0340) | Loss 1.1259(1.1216) | Error 0.0139(0.0111) Steps 584(585.30) | Grad Norm 1.3981(0.5630) | Total Time 10.00(10.00)\n",
      "Iter 0475 | Time 43.7540(43.4842) | Bit/dim 1.1007(1.1045) | Xent 0.0312(0.0339) | Loss 1.1163(1.1214) | Error 0.0099(0.0110) Steps 584(585.26) | Grad Norm 1.6168(0.5947) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0068 | Time 12.0081, Epoch Time 329.6142(336.9274), Bit/dim 1.0902, Xent 0.0548, Loss 1.1176, Error 0.2133\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0476 | Time 42.6853(43.4603) | Bit/dim 1.1002(1.1044) | Xent 0.0363(0.0340) | Loss 1.1183(1.1213) | Error 0.0114(0.0111) Steps 584(585.22) | Grad Norm 0.2373(0.5839) | Total Time 10.00(10.00)\n",
      "Iter 0477 | Time 44.1882(43.4821) | Bit/dim 1.1067(1.1044) | Xent 0.0377(0.0341) | Loss 1.1255(1.1215) | Error 0.0109(0.0110) Steps 590(585.36) | Grad Norm 2.0698(0.6285) | Total Time 10.00(10.00)\n",
      "Iter 0478 | Time 43.3719(43.4788) | Bit/dim 1.1034(1.1044) | Xent 0.0335(0.0341) | Loss 1.1202(1.1214) | Error 0.0106(0.0110) Steps 584(585.32) | Grad Norm 1.9379(0.6678) | Total Time 10.00(10.00)\n",
      "Iter 0479 | Time 42.8278(43.4593) | Bit/dim 1.1051(1.1044) | Xent 0.0341(0.0341) | Loss 1.1221(1.1215) | Error 0.0112(0.0110) Steps 578(585.10) | Grad Norm 0.3262(0.6575) | Total Time 10.00(10.00)\n",
      "Iter 0480 | Time 43.1399(43.4497) | Bit/dim 1.1013(1.1043) | Xent 0.0335(0.0340) | Loss 1.1180(1.1213) | Error 0.0119(0.0111) Steps 590(585.25) | Grad Norm 2.4789(0.7122) | Total Time 10.00(10.00)\n",
      "Iter 0481 | Time 43.4364(43.4493) | Bit/dim 1.1041(1.1043) | Xent 0.0351(0.0341) | Loss 1.1216(1.1214) | Error 0.0114(0.0111) Steps 578(585.03) | Grad Norm 2.2798(0.7592) | Total Time 10.00(10.00)\n",
      "Iter 0482 | Time 42.6943(43.4266) | Bit/dim 1.1073(1.1044) | Xent 0.0315(0.0340) | Loss 1.1230(1.1214) | Error 0.0090(0.0110) Steps 584(585.00) | Grad Norm 0.3928(0.7482) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0069 | Time 12.1255, Epoch Time 327.1845(336.6352), Bit/dim 1.0898, Xent 0.0516, Loss 1.1156, Error 0.2139\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0483 | Time 42.5930(43.4016) | Bit/dim 1.1020(1.1043) | Xent 0.0352(0.0340) | Loss 1.1196(1.1214) | Error 0.0104(0.0110) Steps 584(584.97) | Grad Norm 2.6834(0.8063) | Total Time 10.00(10.00)\n",
      "Iter 0484 | Time 41.8095(43.3539) | Bit/dim 1.1011(1.1042) | Xent 0.0338(0.0340) | Loss 1.1181(1.1213) | Error 0.0116(0.0110) Steps 584(584.94) | Grad Norm 2.1778(0.8474) | Total Time 10.00(10.00)\n",
      "Iter 0485 | Time 42.5192(43.3288) | Bit/dim 1.1023(1.1042) | Xent 0.0285(0.0339) | Loss 1.1166(1.1211) | Error 0.0096(0.0110) Steps 584(584.91) | Grad Norm 0.6966(0.8429) | Total Time 10.00(10.00)\n",
      "Iter 0486 | Time 44.2404(43.3562) | Bit/dim 1.1049(1.1042) | Xent 0.0303(0.0338) | Loss 1.1201(1.1211) | Error 0.0096(0.0109) Steps 584(584.89) | Grad Norm 2.8322(0.9026) | Total Time 10.00(10.00)\n",
      "Iter 0487 | Time 42.0329(43.3165) | Bit/dim 1.1108(1.1044) | Xent 0.0375(0.0339) | Loss 1.1295(1.1213) | Error 0.0115(0.0109) Steps 584(584.86) | Grad Norm 2.3010(0.9445) | Total Time 10.00(10.00)\n",
      "Iter 0488 | Time 43.7811(43.3304) | Bit/dim 1.0998(1.1043) | Xent 0.0371(0.0340) | Loss 1.1183(1.1212) | Error 0.0122(0.0110) Steps 584(584.83) | Grad Norm 0.2748(0.9244) | Total Time 10.00(10.00)\n",
      "Iter 0489 | Time 43.6267(43.3393) | Bit/dim 1.1064(1.1043) | Xent 0.0370(0.0341) | Loss 1.1249(1.1214) | Error 0.0126(0.0110) Steps 590(584.99) | Grad Norm 2.2951(0.9656) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0070 | Time 11.9746, Epoch Time 325.4770(336.3004), Bit/dim 1.0902, Xent 0.0508, Loss 1.1156, Error 0.2133\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0490 | Time 42.3606(43.3099) | Bit/dim 1.1034(1.1043) | Xent 0.0373(0.0341) | Loss 1.1220(1.1214) | Error 0.0129(0.0111) Steps 572(584.60) | Grad Norm 1.8808(0.9930) | Total Time 10.00(10.00)\n",
      "Iter 0491 | Time 42.5096(43.2859) | Bit/dim 1.1069(1.1044) | Xent 0.0311(0.0341) | Loss 1.1225(1.1214) | Error 0.0100(0.0111) Steps 584(584.58) | Grad Norm 0.9204(0.9908) | Total Time 10.00(10.00)\n",
      "Iter 0492 | Time 43.2197(43.2839) | Bit/dim 1.1066(1.1044) | Xent 0.0378(0.0342) | Loss 1.1255(1.1215) | Error 0.0124(0.0111) Steps 590(584.74) | Grad Norm 3.4514(1.0647) | Total Time 10.00(10.00)\n",
      "Iter 0493 | Time 44.6512(43.3250) | Bit/dim 1.1056(1.1045) | Xent 0.0305(0.0341) | Loss 1.1209(1.1215) | Error 0.0114(0.0111) Steps 578(584.54) | Grad Norm 2.8388(1.1179) | Total Time 10.00(10.00)\n",
      "Iter 0494 | Time 42.7181(43.3068) | Bit/dim 1.1043(1.1045) | Xent 0.0279(0.0339) | Loss 1.1182(1.1214) | Error 0.0086(0.0110) Steps 584(584.52) | Grad Norm 0.8102(1.1086) | Total Time 10.00(10.00)\n",
      "Iter 0495 | Time 43.1772(43.3029) | Bit/dim 1.0992(1.1043) | Xent 0.0288(0.0337) | Loss 1.1136(1.1212) | Error 0.0091(0.0110) Steps 590(584.69) | Grad Norm 4.1170(1.1989) | Total Time 10.00(10.00)\n",
      "Iter 0496 | Time 43.1028(43.2969) | Bit/dim 1.1061(1.1044) | Xent 0.0369(0.0338) | Loss 1.1245(1.1213) | Error 0.0125(0.0110) Steps 572(584.31) | Grad Norm 3.4551(1.2666) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0071 | Time 12.1992, Epoch Time 327.1006(336.0244), Bit/dim 1.0896, Xent 0.0520, Loss 1.1156, Error 0.2143\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0497 | Time 43.4990(43.3029) | Bit/dim 1.1031(1.1043) | Xent 0.0314(0.0337) | Loss 1.1188(1.1212) | Error 0.0114(0.0110) Steps 590(584.48) | Grad Norm 1.0953(1.2614) | Total Time 10.00(10.00)\n",
      "Iter 0498 | Time 43.6351(43.3129) | Bit/dim 1.1058(1.1044) | Xent 0.0367(0.0338) | Loss 1.1241(1.1213) | Error 0.0124(0.0111) Steps 584(584.46) | Grad Norm 5.0593(1.3754) | Total Time 10.00(10.00)\n",
      "Iter 0499 | Time 41.4974(43.2584) | Bit/dim 1.1105(1.1046) | Xent 0.0299(0.0337) | Loss 1.1254(1.1214) | Error 0.0104(0.0111) Steps 572(584.09) | Grad Norm 3.9723(1.4533) | Total Time 10.00(10.00)\n",
      "Iter 0500 | Time 43.5371(43.2668) | Bit/dim 1.1055(1.1046) | Xent 0.0344(0.0337) | Loss 1.1227(1.1215) | Error 0.0106(0.0110) Steps 578(583.91) | Grad Norm 2.0017(1.4697) | Total Time 10.00(10.00)\n",
      "Iter 0501 | Time 45.6543(43.3384) | Bit/dim 1.1059(1.1046) | Xent 0.0354(0.0338) | Loss 1.1236(1.1215) | Error 0.0122(0.0111) Steps 608(584.63) | Grad Norm 7.2099(1.6419) | Total Time 10.00(10.00)\n",
      "Iter 0502 | Time 42.4658(43.3122) | Bit/dim 1.1114(1.1048) | Xent 0.0341(0.0338) | Loss 1.1285(1.1217) | Error 0.0105(0.0111) Steps 578(584.43) | Grad Norm 4.8308(1.7376) | Total Time 10.00(10.00)\n",
      "Iter 0503 | Time 42.2095(43.2792) | Bit/dim 1.1084(1.1049) | Xent 0.0340(0.0338) | Loss 1.1254(1.1218) | Error 0.0109(0.0111) Steps 578(584.24) | Grad Norm 3.6926(1.7963) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0072 | Time 12.4278, Epoch Time 327.7328(335.7757), Bit/dim 1.0975, Xent 0.0608, Loss 1.1279, Error 0.2154\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0504 | Time 45.3711(43.3419) | Bit/dim 1.1132(1.1052) | Xent 0.0381(0.0339) | Loss 1.1323(1.1222) | Error 0.0120(0.0111) Steps 596(584.59) | Grad Norm 8.4471(1.9958) | Total Time 10.00(10.00)\n",
      "Iter 0505 | Time 42.5816(43.3191) | Bit/dim 1.1086(1.1053) | Xent 0.0304(0.0338) | Loss 1.1238(1.1222) | Error 0.0111(0.0111) Steps 578(584.39) | Grad Norm 4.0446(2.0573) | Total Time 10.00(10.00)\n",
      "Iter 0506 | Time 41.2910(43.2583) | Bit/dim 1.1071(1.1053) | Xent 0.0302(0.0337) | Loss 1.1222(1.1222) | Error 0.0085(0.0110) Steps 578(584.20) | Grad Norm 4.1494(2.1200) | Total Time 10.00(10.00)\n",
      "Iter 0507 | Time 45.0729(43.3127) | Bit/dim 1.1051(1.1053) | Xent 0.0392(0.0339) | Loss 1.1248(1.1223) | Error 0.0124(0.0110) Steps 596(584.56) | Grad Norm 6.4932(2.2512) | Total Time 10.00(10.00)\n",
      "Iter 0508 | Time 42.7084(43.2946) | Bit/dim 1.1075(1.1054) | Xent 0.0387(0.0340) | Loss 1.1269(1.1224) | Error 0.0124(0.0111) Steps 584(584.54) | Grad Norm 1.9211(2.2413) | Total Time 10.00(10.00)\n",
      "Iter 0509 | Time 42.8028(43.2798) | Bit/dim 1.1031(1.1053) | Xent 0.0319(0.0340) | Loss 1.1191(1.1223) | Error 0.0106(0.0111) Steps 578(584.34) | Grad Norm 3.0166(2.2646) | Total Time 10.00(10.00)\n",
      "Iter 0510 | Time 45.5829(43.3489) | Bit/dim 1.1030(1.1053) | Xent 0.0305(0.0339) | Loss 1.1182(1.1222) | Error 0.0101(0.0110) Steps 590(584.51) | Grad Norm 3.0826(2.2891) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0073 | Time 12.1568, Epoch Time 330.3103(335.6117), Bit/dim 1.0895, Xent 0.0490, Loss 1.1140, Error 0.2124\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0511 | Time 44.2846(43.3770) | Bit/dim 1.0986(1.1051) | Xent 0.0356(0.0339) | Loss 1.1164(1.1220) | Error 0.0105(0.0110) Steps 590(584.68) | Grad Norm 0.9655(2.2494) | Total Time 10.00(10.00)\n",
      "Iter 0512 | Time 43.5037(43.3808) | Bit/dim 1.1033(1.1050) | Xent 0.0270(0.0337) | Loss 1.1168(1.1219) | Error 0.0082(0.0109) Steps 578(584.48) | Grad Norm 3.0978(2.2749) | Total Time 10.00(10.00)\n",
      "Iter 0513 | Time 42.0872(43.3420) | Bit/dim 1.1012(1.1049) | Xent 0.0330(0.0337) | Loss 1.1177(1.1217) | Error 0.0108(0.0109) Steps 584(584.46) | Grad Norm 0.4804(2.2210) | Total Time 10.00(10.00)\n",
      "Iter 0514 | Time 43.4259(43.3445) | Bit/dim 1.1049(1.1049) | Xent 0.0376(0.0338) | Loss 1.1237(1.1218) | Error 0.0118(0.0110) Steps 590(584.63) | Grad Norm 3.3918(2.2561) | Total Time 10.00(10.00)\n",
      "Iter 0515 | Time 42.6901(43.3249) | Bit/dim 1.1087(1.1050) | Xent 0.0330(0.0338) | Loss 1.1252(1.1219) | Error 0.0115(0.0110) Steps 584(584.61) | Grad Norm 2.9863(2.2780) | Total Time 10.00(10.00)\n",
      "Iter 0516 | Time 42.3740(43.2963) | Bit/dim 1.1072(1.1051) | Xent 0.0329(0.0337) | Loss 1.1237(1.1220) | Error 0.0106(0.0110) Steps 584(584.59) | Grad Norm 1.4959(2.2546) | Total Time 10.00(10.00)\n",
      "Iter 0517 | Time 44.5749(43.3347) | Bit/dim 1.1058(1.1051) | Xent 0.0360(0.0338) | Loss 1.1238(1.1220) | Error 0.0125(0.0110) Steps 590(584.75) | Grad Norm 4.4673(2.3210) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0074 | Time 11.7492, Epoch Time 327.0701(335.3555), Bit/dim 1.0906, Xent 0.0492, Loss 1.1152, Error 0.2123\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0518 | Time 42.1244(43.2984) | Bit/dim 1.1042(1.1051) | Xent 0.0358(0.0339) | Loss 1.1221(1.1220) | Error 0.0130(0.0111) Steps 584(584.73) | Grad Norm 2.4142(2.3238) | Total Time 10.00(10.00)\n",
      "Iter 0519 | Time 42.5364(43.2755) | Bit/dim 1.1050(1.1051) | Xent 0.0317(0.0338) | Loss 1.1209(1.1220) | Error 0.0106(0.0111) Steps 578(584.53) | Grad Norm 2.3415(2.3243) | Total Time 10.00(10.00)\n",
      "Iter 0520 | Time 45.1701(43.3324) | Bit/dim 1.1080(1.1052) | Xent 0.0413(0.0340) | Loss 1.1287(1.1222) | Error 0.0128(0.0111) Steps 596(584.87) | Grad Norm 4.3870(2.3862) | Total Time 10.00(10.00)\n",
      "Iter 0521 | Time 43.4029(43.3345) | Bit/dim 1.1014(1.1050) | Xent 0.0313(0.0340) | Loss 1.1170(1.1220) | Error 0.0084(0.0110) Steps 578(584.67) | Grad Norm 1.9138(2.3720) | Total Time 10.00(10.00)\n",
      "Iter 0522 | Time 41.5825(43.2819) | Bit/dim 1.1034(1.1050) | Xent 0.0310(0.0339) | Loss 1.1189(1.1219) | Error 0.0099(0.0110) Steps 584(584.65) | Grad Norm 2.0883(2.3635) | Total Time 10.00(10.00)\n",
      "Iter 0523 | Time 43.0501(43.2750) | Bit/dim 1.1055(1.1050) | Xent 0.0315(0.0338) | Loss 1.1212(1.1219) | Error 0.0096(0.0110) Steps 590(584.81) | Grad Norm 3.6139(2.4010) | Total Time 10.00(10.00)\n",
      "Iter 0524 | Time 42.1983(43.2427) | Bit/dim 1.0993(1.1048) | Xent 0.0338(0.0338) | Loss 1.1162(1.1217) | Error 0.0110(0.0110) Steps 584(584.78) | Grad Norm 1.6143(2.3774) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0075 | Time 11.9557, Epoch Time 324.7016(335.0358), Bit/dim 1.0894, Xent 0.0528, Loss 1.1158, Error 0.2137\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0525 | Time 42.7772(43.2287) | Bit/dim 1.1085(1.1049) | Xent 0.0319(0.0337) | Loss 1.1245(1.1218) | Error 0.0100(0.0109) Steps 584(584.76) | Grad Norm 1.5561(2.3528) | Total Time 10.00(10.00)\n",
      "Iter 0526 | Time 44.8699(43.2779) | Bit/dim 1.0998(1.1048) | Xent 0.0368(0.0338) | Loss 1.1182(1.1217) | Error 0.0125(0.0110) Steps 590(584.92) | Grad Norm 2.5394(2.3584) | Total Time 10.00(10.00)\n",
      "Iter 0527 | Time 43.9705(43.2987) | Bit/dim 1.0964(1.1045) | Xent 0.0344(0.0338) | Loss 1.1136(1.1215) | Error 0.0111(0.0110) Steps 584(584.89) | Grad Norm 0.5834(2.3051) | Total Time 10.00(10.00)\n",
      "Iter 0528 | Time 41.8565(43.2554) | Bit/dim 1.1045(1.1045) | Xent 0.0279(0.0337) | Loss 1.1184(1.1214) | Error 0.0086(0.0109) Steps 572(584.50) | Grad Norm 1.9477(2.2944) | Total Time 10.00(10.00)\n",
      "Iter 0529 | Time 44.1937(43.2836) | Bit/dim 1.1046(1.1045) | Xent 0.0302(0.0336) | Loss 1.1198(1.1213) | Error 0.0094(0.0109) Steps 590(584.67) | Grad Norm 2.4150(2.2980) | Total Time 10.00(10.00)\n",
      "Iter 0530 | Time 44.5059(43.3203) | Bit/dim 1.1061(1.1046) | Xent 0.0332(0.0336) | Loss 1.1227(1.1214) | Error 0.0114(0.0109) Steps 584(584.65) | Grad Norm 0.7604(2.2519) | Total Time 10.00(10.00)\n",
      "Iter 0531 | Time 44.2112(43.3470) | Bit/dim 1.0997(1.1044) | Xent 0.0308(0.0335) | Loss 1.1151(1.1212) | Error 0.0108(0.0109) Steps 578(584.45) | Grad Norm 1.1518(2.2189) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0076 | Time 12.0861, Epoch Time 331.1847(334.9203), Bit/dim 1.0883, Xent 0.0530, Loss 1.1147, Error 0.2134\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0532 | Time 43.5918(43.3543) | Bit/dim 1.1033(1.1044) | Xent 0.0322(0.0334) | Loss 1.1194(1.1211) | Error 0.0100(0.0108) Steps 584(584.44) | Grad Norm 1.6330(2.2013) | Total Time 10.00(10.00)\n",
      "Iter 0533 | Time 44.2453(43.3811) | Bit/dim 1.1016(1.1043) | Xent 0.0275(0.0333) | Loss 1.1154(1.1210) | Error 0.0080(0.0108) Steps 584(584.42) | Grad Norm 0.6069(2.1535) | Total Time 10.00(10.00)\n",
      "Iter 0534 | Time 42.7414(43.3619) | Bit/dim 1.1026(1.1043) | Xent 0.0357(0.0333) | Loss 1.1205(1.1209) | Error 0.0114(0.0108) Steps 584(584.41) | Grad Norm 1.0676(2.1209) | Total Time 10.00(10.00)\n",
      "Iter 0535 | Time 43.6480(43.3705) | Bit/dim 1.1001(1.1042) | Xent 0.0335(0.0333) | Loss 1.1169(1.1208) | Error 0.0100(0.0108) Steps 590(584.58) | Grad Norm 1.6483(2.1067) | Total Time 10.00(10.00)\n",
      "Iter 0536 | Time 43.1930(43.3651) | Bit/dim 1.1032(1.1041) | Xent 0.0271(0.0332) | Loss 1.1168(1.1207) | Error 0.0089(0.0107) Steps 584(584.56) | Grad Norm 0.8518(2.0691) | Total Time 10.00(10.00)\n",
      "Iter 0537 | Time 45.1182(43.4177) | Bit/dim 1.1046(1.1041) | Xent 0.0393(0.0333) | Loss 1.1242(1.1208) | Error 0.0119(0.0107) Steps 590(584.72) | Grad Norm 0.6808(2.0274) | Total Time 10.00(10.00)\n",
      "Iter 0538 | Time 42.9924(43.4050) | Bit/dim 1.1029(1.1041) | Xent 0.0289(0.0332) | Loss 1.1173(1.1207) | Error 0.0091(0.0107) Steps 584(584.70) | Grad Norm 1.6939(2.0174) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0077 | Time 11.9067, Epoch Time 330.3132(334.7821), Bit/dim 1.0895, Xent 0.0471, Loss 1.1130, Error 0.2119\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0539 | Time 43.7238(43.4145) | Bit/dim 1.1063(1.1042) | Xent 0.0244(0.0329) | Loss 1.1185(1.1206) | Error 0.0089(0.0106) Steps 590(584.86) | Grad Norm 1.3740(1.9981) | Total Time 10.00(10.00)\n",
      "Iter 0540 | Time 41.8612(43.3679) | Bit/dim 1.0996(1.1040) | Xent 0.0332(0.0329) | Loss 1.1162(1.1205) | Error 0.0106(0.0106) Steps 590(585.01) | Grad Norm 0.1810(1.9436) | Total Time 10.00(10.00)\n",
      "Iter 0541 | Time 43.0317(43.3578) | Bit/dim 1.1012(1.1039) | Xent 0.0295(0.0328) | Loss 1.1159(1.1204) | Error 0.0092(0.0106) Steps 584(584.98) | Grad Norm 1.6149(1.9337) | Total Time 10.00(10.00)\n",
      "Iter 0542 | Time 43.6433(43.3664) | Bit/dim 1.1035(1.1039) | Xent 0.0335(0.0329) | Loss 1.1203(1.1204) | Error 0.0109(0.0106) Steps 584(584.95) | Grad Norm 1.7854(1.9293) | Total Time 10.00(10.00)\n",
      "Iter 0543 | Time 43.3669(43.3664) | Bit/dim 1.0983(1.1038) | Xent 0.0374(0.0330) | Loss 1.1170(1.1203) | Error 0.0111(0.0106) Steps 590(585.11) | Grad Norm 0.2931(1.8802) | Total Time 10.00(10.00)\n",
      "Iter 0544 | Time 44.6292(43.4043) | Bit/dim 1.1013(1.1037) | Xent 0.0378(0.0331) | Loss 1.1202(1.1203) | Error 0.0128(0.0107) Steps 590(585.25) | Grad Norm 2.6071(1.9020) | Total Time 10.00(10.00)\n",
      "Iter 0545 | Time 42.0685(43.3642) | Bit/dim 1.1050(1.1037) | Xent 0.0320(0.0331) | Loss 1.1211(1.1203) | Error 0.0106(0.0107) Steps 572(584.86) | Grad Norm 2.7929(1.9287) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0078 | Time 12.1103, Epoch Time 327.0040(334.5488), Bit/dim 1.0876, Xent 0.0502, Loss 1.1127, Error 0.2136\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0546 | Time 42.8672(43.3493) | Bit/dim 1.0992(1.1036) | Xent 0.0333(0.0331) | Loss 1.1158(1.1201) | Error 0.0096(0.0106) Steps 584(584.83) | Grad Norm 0.2298(1.8778) | Total Time 10.00(10.00)\n",
      "Iter 0547 | Time 42.6724(43.3290) | Bit/dim 1.1000(1.1035) | Xent 0.0295(0.0330) | Loss 1.1147(1.1200) | Error 0.0089(0.0106) Steps 590(584.98) | Grad Norm 3.3977(1.9234) | Total Time 10.00(10.00)\n",
      "Iter 0548 | Time 43.9626(43.3480) | Bit/dim 1.1042(1.1035) | Xent 0.0334(0.0330) | Loss 1.1209(1.1200) | Error 0.0114(0.0106) Steps 572(584.60) | Grad Norm 3.6447(1.9750) | Total Time 10.00(10.00)\n",
      "Iter 0549 | Time 42.7605(43.3304) | Bit/dim 1.1048(1.1035) | Xent 0.0341(0.0331) | Loss 1.1219(1.1201) | Error 0.0108(0.0106) Steps 596(584.94) | Grad Norm 0.2564(1.9235) | Total Time 10.00(10.00)\n",
      "Iter 0550 | Time 44.9714(43.3796) | Bit/dim 1.1059(1.1036) | Xent 0.0373(0.0332) | Loss 1.1246(1.1202) | Error 0.0116(0.0107) Steps 596(585.27) | Grad Norm 4.2118(1.9921) | Total Time 10.00(10.00)\n",
      "Iter 0551 | Time 42.2280(43.3451) | Bit/dim 1.1038(1.1036) | Xent 0.0331(0.0332) | Loss 1.1203(1.1202) | Error 0.0109(0.0107) Steps 578(585.05) | Grad Norm 4.3226(2.0620) | Total Time 10.00(10.00)\n",
      "Iter 0552 | Time 42.3869(43.3163) | Bit/dim 1.1040(1.1036) | Xent 0.0249(0.0329) | Loss 1.1165(1.1201) | Error 0.0072(0.0106) Steps 584(585.02) | Grad Norm 1.0009(2.0302) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0079 | Time 12.6052, Epoch Time 327.1461(334.3267), Bit/dim 1.0926, Xent 0.0555, Loss 1.1204, Error 0.2141\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0553 | Time 43.9925(43.3366) | Bit/dim 1.1056(1.1037) | Xent 0.0429(0.0332) | Loss 1.1271(1.1203) | Error 0.0152(0.0107) Steps 596(585.35) | Grad Norm 7.3656(2.1902) | Total Time 10.00(10.00)\n",
      "Iter 0554 | Time 41.3550(43.2772) | Bit/dim 1.1137(1.1040) | Xent 0.0268(0.0330) | Loss 1.1271(1.1205) | Error 0.0082(0.0106) Steps 578(585.13) | Grad Norm 6.1589(2.3093) | Total Time 10.00(10.00)\n",
      "Iter 0555 | Time 41.0781(43.2112) | Bit/dim 1.1067(1.1041) | Xent 0.0325(0.0330) | Loss 1.1229(1.1206) | Error 0.0100(0.0106) Steps 584(585.09) | Grad Norm 4.0713(2.3622) | Total Time 10.00(10.00)\n",
      "Iter 0556 | Time 45.8378(43.2900) | Bit/dim 1.1197(1.1045) | Xent 0.0444(0.0334) | Loss 1.1418(1.1212) | Error 0.0156(0.0108) Steps 614(585.96) | Grad Norm 12.5578(2.6680) | Total Time 10.00(10.00)\n",
      "Iter 0557 | Time 43.3006(43.2903) | Bit/dim 1.1219(1.1051) | Xent 0.0318(0.0333) | Loss 1.1378(1.1217) | Error 0.0090(0.0107) Steps 572(585.54) | Grad Norm 6.5501(2.7845) | Total Time 10.00(10.00)\n",
      "Iter 0558 | Time 44.3863(43.3232) | Bit/dim 1.1271(1.1057) | Xent 0.0268(0.0331) | Loss 1.1405(1.1223) | Error 0.0088(0.0106) Steps 572(585.14) | Grad Norm 6.5582(2.8977) | Total Time 10.00(10.00)\n",
      "Iter 0559 | Time 44.6546(43.3631) | Bit/dim 1.1057(1.1057) | Xent 0.0353(0.0332) | Loss 1.1234(1.1223) | Error 0.0110(0.0107) Steps 590(585.28) | Grad Norm 4.9312(2.9587) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0080 | Time 12.3967, Epoch Time 330.0655(334.1988), Bit/dim 1.0950, Xent 0.0573, Loss 1.1236, Error 0.2152\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0560 | Time 44.4123(43.3946) | Bit/dim 1.1085(1.1058) | Xent 0.0380(0.0333) | Loss 1.1275(1.1225) | Error 0.0131(0.0107) Steps 590(585.42) | Grad Norm 7.0781(3.0823) | Total Time 10.00(10.00)\n",
      "Iter 0561 | Time 44.3082(43.4220) | Bit/dim 1.1237(1.1063) | Xent 0.0304(0.0332) | Loss 1.1389(1.1230) | Error 0.0096(0.0107) Steps 578(585.20) | Grad Norm 6.0711(3.1720) | Total Time 10.00(10.00)\n",
      "Iter 0562 | Time 42.9535(43.4080) | Bit/dim 1.1191(1.1067) | Xent 0.0277(0.0331) | Loss 1.1329(1.1233) | Error 0.0091(0.0106) Steps 578(584.99) | Grad Norm 5.7068(3.2480) | Total Time 10.00(10.00)\n",
      "Iter 0563 | Time 43.9771(43.4250) | Bit/dim 1.1030(1.1066) | Xent 0.0349(0.0331) | Loss 1.1205(1.1232) | Error 0.0102(0.0106) Steps 590(585.14) | Grad Norm 5.2863(3.3092) | Total Time 10.00(10.00)\n",
      "Iter 0564 | Time 43.9655(43.4413) | Bit/dim 1.1081(1.1067) | Xent 0.0347(0.0332) | Loss 1.1254(1.1232) | Error 0.0110(0.0106) Steps 590(585.28) | Grad Norm 5.3599(3.3707) | Total Time 10.00(10.00)\n",
      "Iter 0565 | Time 43.0835(43.4305) | Bit/dim 1.1114(1.1068) | Xent 0.0305(0.0331) | Loss 1.1267(1.1233) | Error 0.0089(0.0106) Steps 578(585.06) | Grad Norm 5.1188(3.4231) | Total Time 10.00(10.00)\n",
      "Iter 0566 | Time 43.1986(43.4236) | Bit/dim 1.1129(1.1070) | Xent 0.0279(0.0329) | Loss 1.1269(1.1235) | Error 0.0089(0.0105) Steps 578(584.85) | Grad Norm 4.7452(3.4628) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0081 | Time 12.1185, Epoch Time 330.9290(334.1007), Bit/dim 1.0927, Xent 0.0574, Loss 1.1214, Error 0.2150\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0567 | Time 44.3428(43.4511) | Bit/dim 1.1068(1.1070) | Xent 0.0378(0.0331) | Loss 1.1257(1.1235) | Error 0.0129(0.0106) Steps 590(585.01) | Grad Norm 5.9556(3.5376) | Total Time 10.00(10.00)\n",
      "Iter 0568 | Time 43.1575(43.4423) | Bit/dim 1.1022(1.1068) | Xent 0.0310(0.0330) | Loss 1.1178(1.1233) | Error 0.0095(0.0106) Steps 590(585.16) | Grad Norm 1.9472(3.4899) | Total Time 10.00(10.00)\n",
      "Iter 0569 | Time 41.2967(43.3780) | Bit/dim 1.1085(1.1069) | Xent 0.0301(0.0329) | Loss 1.1235(1.1234) | Error 0.0096(0.0106) Steps 572(584.76) | Grad Norm 3.9588(3.5039) | Total Time 10.00(10.00)\n",
      "Iter 0570 | Time 42.6346(43.3557) | Bit/dim 1.1087(1.1069) | Xent 0.0312(0.0329) | Loss 1.1243(1.1234) | Error 0.0102(0.0105) Steps 578(584.56) | Grad Norm 2.4587(3.4726) | Total Time 10.00(10.00)\n",
      "Iter 0571 | Time 45.3309(43.4149) | Bit/dim 1.1041(1.1069) | Xent 0.0338(0.0329) | Loss 1.1210(1.1233) | Error 0.0109(0.0106) Steps 590(584.72) | Grad Norm 6.1195(3.5520) | Total Time 10.00(10.00)\n",
      "Iter 0572 | Time 44.9075(43.4597) | Bit/dim 1.1004(1.1067) | Xent 0.0405(0.0331) | Loss 1.1207(1.1232) | Error 0.0124(0.0106) Steps 596(585.06) | Grad Norm 1.7619(3.4983) | Total Time 10.00(10.00)\n",
      "Iter 0573 | Time 42.2441(43.4232) | Bit/dim 1.1105(1.1068) | Xent 0.0331(0.0331) | Loss 1.1271(1.1233) | Error 0.0115(0.0106) Steps 572(584.67) | Grad Norm 3.2879(3.4920) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0082 | Time 12.0782, Epoch Time 328.7641(333.9406), Bit/dim 1.0880, Xent 0.0534, Loss 1.1146, Error 0.2137\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0574 | Time 43.6459(43.4299) | Bit/dim 1.1012(1.1066) | Xent 0.0357(0.0332) | Loss 1.1190(1.1232) | Error 0.0109(0.0106) Steps 578(584.47) | Grad Norm 1.8984(3.4442) | Total Time 10.00(10.00)\n",
      "Iter 0575 | Time 42.7115(43.4084) | Bit/dim 1.1021(1.1065) | Xent 0.0356(0.0333) | Loss 1.1199(1.1231) | Error 0.0118(0.0107) Steps 590(584.63) | Grad Norm 2.5742(3.4181) | Total Time 10.00(10.00)\n",
      "Iter 0576 | Time 42.9557(43.3948) | Bit/dim 1.1017(1.1063) | Xent 0.0357(0.0334) | Loss 1.1195(1.1230) | Error 0.0109(0.0107) Steps 572(584.25) | Grad Norm 2.6192(3.3941) | Total Time 10.00(10.00)\n",
      "Iter 0577 | Time 43.9177(43.4105) | Bit/dim 1.1059(1.1063) | Xent 0.0344(0.0334) | Loss 1.1231(1.1230) | Error 0.0104(0.0107) Steps 578(584.07) | Grad Norm 1.8422(3.3475) | Total Time 10.00(10.00)\n",
      "Iter 0578 | Time 43.5261(43.4139) | Bit/dim 1.1011(1.1062) | Xent 0.0349(0.0334) | Loss 1.1185(1.1229) | Error 0.0101(0.0107) Steps 584(584.07) | Grad Norm 4.2765(3.3754) | Total Time 10.00(10.00)\n",
      "Iter 0579 | Time 44.8169(43.4560) | Bit/dim 1.1019(1.1060) | Xent 0.0262(0.0332) | Loss 1.1150(1.1226) | Error 0.0086(0.0106) Steps 584(584.06) | Grad Norm 1.3569(3.3148) | Total Time 10.00(10.00)\n",
      "Iter 0580 | Time 43.3826(43.4538) | Bit/dim 1.1060(1.1060) | Xent 0.0270(0.0330) | Loss 1.1195(1.1225) | Error 0.0090(0.0105) Steps 578(583.88) | Grad Norm 2.5220(3.2911) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0083 | Time 11.9425, Epoch Time 329.6718(333.8126), Bit/dim 1.0881, Xent 0.0529, Loss 1.1146, Error 0.2133\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0581 | Time 43.5220(43.4559) | Bit/dim 1.1013(1.1059) | Xent 0.0290(0.0329) | Loss 1.1158(1.1223) | Error 0.0096(0.0105) Steps 590(584.06) | Grad Norm 2.1279(3.2562) | Total Time 10.00(10.00)\n",
      "Iter 0582 | Time 42.9096(43.4395) | Bit/dim 1.0978(1.1056) | Xent 0.0341(0.0329) | Loss 1.1149(1.1221) | Error 0.0105(0.0105) Steps 590(584.24) | Grad Norm 1.2594(3.1963) | Total Time 10.00(10.00)\n",
      "Iter 0583 | Time 42.9412(43.4245) | Bit/dim 1.1001(1.1055) | Xent 0.0256(0.0327) | Loss 1.1129(1.1218) | Error 0.0082(0.0104) Steps 572(583.88) | Grad Norm 2.4870(3.1750) | Total Time 10.00(10.00)\n",
      "Iter 0584 | Time 43.5472(43.4282) | Bit/dim 1.1002(1.1053) | Xent 0.0314(0.0327) | Loss 1.1159(1.1217) | Error 0.0091(0.0104) Steps 590(584.06) | Grad Norm 0.2849(3.0883) | Total Time 10.00(10.00)\n",
      "Iter 0585 | Time 43.6418(43.4346) | Bit/dim 1.1056(1.1053) | Xent 0.0360(0.0328) | Loss 1.1236(1.1217) | Error 0.0104(0.0104) Steps 590(584.24) | Grad Norm 2.5516(3.0722) | Total Time 10.00(10.00)\n",
      "Iter 0586 | Time 43.4466(43.4350) | Bit/dim 1.1033(1.1053) | Xent 0.0372(0.0329) | Loss 1.1220(1.1217) | Error 0.0128(0.0105) Steps 566(583.69) | Grad Norm 2.5538(3.0566) | Total Time 10.00(10.00)\n",
      "Iter 0587 | Time 43.6946(43.4428) | Bit/dim 1.1074(1.1053) | Xent 0.0352(0.0330) | Loss 1.1250(1.1218) | Error 0.0112(0.0105) Steps 584(583.70) | Grad Norm 0.4767(2.9792) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0084 | Time 11.9655, Epoch Time 328.4181(333.6507), Bit/dim 1.0884, Xent 0.0552, Loss 1.1160, Error 0.2139\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0588 | Time 43.7318(43.4514) | Bit/dim 1.1032(1.1053) | Xent 0.0370(0.0331) | Loss 1.1217(1.1218) | Error 0.0100(0.0105) Steps 590(583.89) | Grad Norm 3.1812(2.9853) | Total Time 10.00(10.00)\n",
      "Iter 0589 | Time 41.6845(43.3984) | Bit/dim 1.1009(1.1051) | Xent 0.0261(0.0329) | Loss 1.1140(1.1216) | Error 0.0090(0.0104) Steps 578(583.71) | Grad Norm 2.5578(2.9725) | Total Time 10.00(10.00)\n",
      "Iter 0590 | Time 44.1215(43.4201) | Bit/dim 1.0977(1.1049) | Xent 0.0294(0.0328) | Loss 1.1124(1.1213) | Error 0.0100(0.0104) Steps 572(583.36) | Grad Norm 0.9902(2.9130) | Total Time 10.00(10.00)\n",
      "Iter 0591 | Time 44.6753(43.4578) | Bit/dim 1.1006(1.1048) | Xent 0.0359(0.0329) | Loss 1.1185(1.1212) | Error 0.0115(0.0105) Steps 590(583.56) | Grad Norm 3.6876(2.9362) | Total Time 10.00(10.00)\n",
      "Iter 0592 | Time 41.0647(43.3860) | Bit/dim 1.1060(1.1048) | Xent 0.0330(0.0329) | Loss 1.1225(1.1213) | Error 0.0109(0.0105) Steps 572(583.21) | Grad Norm 2.7611(2.9310) | Total Time 10.00(10.00)\n",
      "Iter 0593 | Time 44.6886(43.4251) | Bit/dim 1.1031(1.1048) | Xent 0.0309(0.0328) | Loss 1.1186(1.1212) | Error 0.0105(0.0105) Steps 590(583.42) | Grad Norm 0.9641(2.8720) | Total Time 10.00(10.00)\n",
      "Iter 0594 | Time 44.2595(43.4501) | Bit/dim 1.1020(1.1047) | Xent 0.0331(0.0328) | Loss 1.1186(1.1211) | Error 0.0106(0.0105) Steps 590(583.61) | Grad Norm 3.7140(2.8972) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0085 | Time 11.8830, Epoch Time 329.5400(333.5274), Bit/dim 1.0889, Xent 0.0498, Loss 1.1138, Error 0.2119\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0595 | Time 43.5743(43.4538) | Bit/dim 1.1015(1.1046) | Xent 0.0293(0.0327) | Loss 1.1162(1.1210) | Error 0.0096(0.0105) Steps 578(583.45) | Grad Norm 2.6875(2.8909) | Total Time 10.00(10.00)\n",
      "Iter 0596 | Time 42.9862(43.4398) | Bit/dim 1.0982(1.1044) | Xent 0.0364(0.0328) | Loss 1.1164(1.1208) | Error 0.0115(0.0105) Steps 578(583.28) | Grad Norm 1.3936(2.8460) | Total Time 10.00(10.00)\n",
      "Iter 0597 | Time 43.2561(43.4343) | Bit/dim 1.1060(1.1045) | Xent 0.0379(0.0330) | Loss 1.1250(1.1210) | Error 0.0122(0.0105) Steps 590(583.48) | Grad Norm 4.6828(2.9011) | Total Time 10.00(10.00)\n",
      "Iter 0598 | Time 42.0257(43.3920) | Bit/dim 1.1057(1.1045) | Xent 0.0300(0.0329) | Loss 1.1207(1.1209) | Error 0.0095(0.0105) Steps 578(583.32) | Grad Norm 3.5484(2.9206) | Total Time 10.00(10.00)\n",
      "Iter 0599 | Time 44.0387(43.4114) | Bit/dim 1.0994(1.1043) | Xent 0.0285(0.0328) | Loss 1.1137(1.1207) | Error 0.0091(0.0105) Steps 578(583.16) | Grad Norm 1.5695(2.8800) | Total Time 10.00(10.00)\n",
      "Iter 0600 | Time 44.6642(43.4490) | Bit/dim 1.1059(1.1044) | Xent 0.0333(0.0328) | Loss 1.1225(1.1208) | Error 0.0111(0.0105) Steps 584(583.19) | Grad Norm 5.9420(2.9719) | Total Time 10.00(10.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p train_cnf_conditional.py --data mnist --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 8000 --test_batch_size 8000 --save experiments/cnf_cond_bs8K_ilr_0_01_tol_1e_m5_wy_0_5_dev_cont_lr_0_001 --resume experiments/cnf_cond_bs8K_ilr_0_01_tol_1e_m5_wy_0_5_dev/best_nll_checkpt.pth --conditional True --lr 0.001 --warmup_iters 113 --atol 1e-5  --rtol 1e-5 --weight_y 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
