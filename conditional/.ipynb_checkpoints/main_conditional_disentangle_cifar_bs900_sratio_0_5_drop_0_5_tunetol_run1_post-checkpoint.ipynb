{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0,1,2,3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_cifar.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "import glob\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--load_dir\", type=str, default=None)\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "if args.controlled_tol:\n",
      "    import lib.odenvp_conditional_tol as odenvp\n",
      "else:\n",
      "    import lib.odenvp_conditional as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted\n",
      "\n",
      "def compute_marginal_bits_per_dim(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    logpz_sup = []\n",
      "    \n",
      "    for indx in range(model.module.y_class):\n",
      "        y_i = torch.full_like(y, indx).to(y)\n",
      "        y_onehot = thops.onehot(y_i, num_classes=model.module.y_class).to(x)\n",
      "        # prior\n",
      "        mean, logs = model.module._prior(y_onehot)\n",
      "        logpz_sup.append(modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1) - torch.log(torch.tensor(model.module.y_class).to(z)))  # logp(z)_sup\n",
      "        \n",
      "    logpz_sup = torch.cat(logpz_sup,dim=1)\n",
      "    logpz_sup = torch.logsumexp(logpz_sup, 1)\n",
      "    \n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    \n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "    \n",
      "    if args.load_dir is not None:\n",
      "        filelist = glob.glob(os.path.join(args.load_dir,\"*epoch_*_checkpt.pth\"))\n",
      "        all_indx = []\n",
      "        for infile in sorted(filelist):\n",
      "            all_indx.append(int(infile.split('_')[-2]))\n",
      "            \n",
      "        indxmax = max(all_indx)\n",
      "        for i in range(1,indxmax+1):\n",
      "            model = create_model(args, data_shape, regularization_fns)\n",
      "            infile = os.path.join(args.load_dir,\"epoch_%i_checkpt.pth\"%i)\n",
      "            print(infile)\n",
      "            checkpt = torch.load(infile, map_location=lambda storage, loc: storage)\n",
      "            model.load_state_dict(checkpt[\"state_dict\"])\n",
      "            \n",
      "            if torch.cuda.is_available():\n",
      "                model = torch.nn.DataParallel(model).cuda()\n",
      "                \n",
      "            model.eval()\n",
      "            \n",
      "            with torch.no_grad():\n",
      "                logger.info(\"validating...\")\n",
      "                losses = []\n",
      "                for (x, y) in test_loader:\n",
      "                    if args.data == \"colormnist\":\n",
      "                        y = y[0]\n",
      "                        \n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    nll = compute_marginal_bits_per_dim(x, y, model)\n",
      "                    losses.append(nll.cpu().numpy())\n",
      "                    \n",
      "                loss = np.mean(losses)\n",
      "                writer.add_scalars('nll_marginal', {'validation': loss}, i)\n",
      "    \n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            loss.backward()\n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=0.0001, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, condition_ratio=0.5, conditional=True, controlled_tol=True, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, imagesize=None, l1int=None, l2int=None, layer_type='concat', load_dir=None, log_freq=10, lr=0.0001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume='../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_tunetol_run1/epoch_250_checkpt.pth', rtol=0.0001, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_tunetol_run1', seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000.0, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1414198\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc.py:286: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "Iter 13760 | Time 21.1302(21.6762) | Bit/dim 3.5467(3.5683) | Xent 0.1249(0.1728) | Loss 3.6091(3.6547) | Error 0.0422(0.0606) Steps 850(863.37) | Grad Norm 1.6674(2.8978) | Total Time 14.00(14.00)\n",
      "Iter 13770 | Time 21.4099(21.5025) | Bit/dim 3.5979(3.5659) | Xent 0.1260(0.1621) | Loss 3.6609(3.6470) | Error 0.0456(0.0573) Steps 838(861.33) | Grad Norm 1.4585(2.5320) | Total Time 14.00(14.00)\n",
      "Iter 13780 | Time 21.2358(21.4319) | Bit/dim 3.5430(3.5609) | Xent 0.1402(0.1518) | Loss 3.6131(3.6368) | Error 0.0522(0.0535) Steps 862(862.01) | Grad Norm 1.4343(2.2294) | Total Time 14.00(14.00)\n",
      "Iter 13790 | Time 20.4720(21.2986) | Bit/dim 3.5659(3.5602) | Xent 0.1155(0.1448) | Loss 3.6236(3.6326) | Error 0.0311(0.0510) Steps 862(864.19) | Grad Norm 1.4484(2.0145) | Total Time 14.00(14.00)\n",
      "Iter 13800 | Time 21.0719(21.3128) | Bit/dim 3.5498(3.5580) | Xent 0.1297(0.1426) | Loss 3.6147(3.6293) | Error 0.0478(0.0495) Steps 844(864.55) | Grad Norm 1.2658(1.8311) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0251 | Time 111.5097, Epoch Time 1315.5787(1198.3869), Bit/dim 3.5603(best: inf), Xent 0.9277, Loss 4.0241, Error 0.2190(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13810 | Time 21.0802(21.3246) | Bit/dim 3.5665(3.5560) | Xent 0.1186(0.1364) | Loss 3.6258(3.6242) | Error 0.0411(0.0477) Steps 868(864.85) | Grad Norm 1.5838(1.7116) | Total Time 14.00(14.00)\n",
      "Iter 13820 | Time 21.4819(21.3817) | Bit/dim 3.5451(3.5533) | Xent 0.1195(0.1318) | Loss 3.6049(3.6192) | Error 0.0489(0.0469) Steps 874(865.27) | Grad Norm 2.0025(1.6809) | Total Time 14.00(14.00)\n",
      "Iter 13830 | Time 21.5704(21.3658) | Bit/dim 3.5731(3.5525) | Xent 0.1217(0.1279) | Loss 3.6340(3.6165) | Error 0.0500(0.0454) Steps 892(869.16) | Grad Norm 1.6016(1.5834) | Total Time 14.00(14.00)\n",
      "Iter 13840 | Time 20.9218(21.3903) | Bit/dim 3.5081(3.5508) | Xent 0.1313(0.1271) | Loss 3.5738(3.6143) | Error 0.0433(0.0448) Steps 850(870.22) | Grad Norm 1.4117(1.5079) | Total Time 14.00(14.00)\n",
      "Iter 13850 | Time 21.8750(21.4389) | Bit/dim 3.5550(3.5488) | Xent 0.1285(0.1257) | Loss 3.6192(3.6117) | Error 0.0400(0.0437) Steps 904(873.07) | Grad Norm 1.4178(1.4606) | Total Time 14.00(14.00)\n",
      "Iter 13860 | Time 21.4578(21.4228) | Bit/dim 3.5579(3.5514) | Xent 0.1347(0.1230) | Loss 3.6252(3.6129) | Error 0.0511(0.0432) Steps 886(873.50) | Grad Norm 1.5271(1.4314) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0252 | Time 106.0257, Epoch Time 1304.1823(1201.5607), Bit/dim 3.5571(best: 3.5603), Xent 0.9359, Loss 4.0250, Error 0.2198(best: 0.2190)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13870 | Time 21.8409(21.4665) | Bit/dim 3.5363(3.5503) | Xent 0.0892(0.1206) | Loss 3.5809(3.6106) | Error 0.0322(0.0421) Steps 850(874.38) | Grad Norm 1.1938(1.4287) | Total Time 14.00(14.00)\n",
      "Iter 13880 | Time 21.3957(21.4782) | Bit/dim 3.5296(3.5473) | Xent 0.1247(0.1201) | Loss 3.5919(3.6074) | Error 0.0433(0.0417) Steps 868(875.78) | Grad Norm 1.7297(1.4328) | Total Time 14.00(14.00)\n",
      "Iter 13890 | Time 21.5848(21.4758) | Bit/dim 3.5606(3.5475) | Xent 0.1189(0.1185) | Loss 3.6200(3.6067) | Error 0.0433(0.0414) Steps 898(877.28) | Grad Norm 1.5199(1.4373) | Total Time 14.00(14.00)\n",
      "Iter 13900 | Time 22.1027(21.5121) | Bit/dim 3.5439(3.5498) | Xent 0.1190(0.1176) | Loss 3.6034(3.6086) | Error 0.0422(0.0410) Steps 910(877.59) | Grad Norm 1.2019(1.3993) | Total Time 14.00(14.00)\n",
      "Iter 13910 | Time 21.0362(21.4142) | Bit/dim 3.5741(3.5511) | Xent 0.0989(0.1186) | Loss 3.6236(3.6103) | Error 0.0356(0.0409) Steps 898(876.76) | Grad Norm 1.4423(1.4091) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0253 | Time 104.5560, Epoch Time 1305.3639(1204.6748), Bit/dim 3.5567(best: 3.5571), Xent 0.9360, Loss 4.0247, Error 0.2174(best: 0.2190)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13920 | Time 20.9516(21.4004) | Bit/dim 3.5549(3.5512) | Xent 0.1028(0.1175) | Loss 3.6063(3.6099) | Error 0.0389(0.0404) Steps 868(874.57) | Grad Norm 1.2022(1.4141) | Total Time 14.00(14.00)\n",
      "Iter 13930 | Time 21.2720(21.3613) | Bit/dim 3.5319(3.5510) | Xent 0.1058(0.1171) | Loss 3.5848(3.6095) | Error 0.0333(0.0404) Steps 898(875.09) | Grad Norm 1.3646(1.4086) | Total Time 14.00(14.00)\n",
      "Iter 13940 | Time 21.7276(21.5231) | Bit/dim 3.5617(3.5501) | Xent 0.1017(0.1153) | Loss 3.6126(3.6077) | Error 0.0378(0.0394) Steps 880(877.83) | Grad Norm 1.1615(1.3614) | Total Time 14.00(14.00)\n",
      "Iter 13950 | Time 21.6799(21.5703) | Bit/dim 3.5219(3.5494) | Xent 0.1048(0.1142) | Loss 3.5744(3.6065) | Error 0.0311(0.0388) Steps 904(881.90) | Grad Norm 1.1778(1.3392) | Total Time 14.00(14.00)\n",
      "Iter 13960 | Time 21.8194(21.5234) | Bit/dim 3.5265(3.5487) | Xent 0.1029(0.1141) | Loss 3.5780(3.6058) | Error 0.0344(0.0391) Steps 904(882.00) | Grad Norm 1.2098(1.3417) | Total Time 14.00(14.00)\n",
      "Iter 13970 | Time 22.1004(21.5716) | Bit/dim 3.5557(3.5469) | Xent 0.1167(0.1169) | Loss 3.6140(3.6054) | Error 0.0378(0.0403) Steps 886(881.24) | Grad Norm 1.4959(1.3498) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0254 | Time 106.3690, Epoch Time 1311.0380(1207.8657), Bit/dim 3.5553(best: 3.5567), Xent 0.9507, Loss 4.0307, Error 0.2245(best: 0.2174)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13980 | Time 21.5911(21.5482) | Bit/dim 3.5829(3.5478) | Xent 0.1105(0.1174) | Loss 3.6382(3.6065) | Error 0.0433(0.0404) Steps 886(884.22) | Grad Norm 1.3308(1.3567) | Total Time 14.00(14.00)\n",
      "Iter 13990 | Time 20.9007(21.5196) | Bit/dim 3.5515(3.5479) | Xent 0.0770(0.1152) | Loss 3.5900(3.6055) | Error 0.0256(0.0399) Steps 880(886.37) | Grad Norm 0.9802(1.3329) | Total Time 14.00(14.00)\n",
      "Iter 14000 | Time 21.2427(21.4496) | Bit/dim 3.5711(3.5480) | Xent 0.1189(0.1176) | Loss 3.6305(3.6068) | Error 0.0389(0.0403) Steps 898(886.64) | Grad Norm 1.4888(1.3516) | Total Time 14.00(14.00)\n",
      "Iter 14010 | Time 21.4106(21.5111) | Bit/dim 3.5396(3.5461) | Xent 0.1204(0.1167) | Loss 3.5998(3.6044) | Error 0.0467(0.0404) Steps 898(886.85) | Grad Norm 1.6743(1.3580) | Total Time 14.00(14.00)\n",
      "Iter 14020 | Time 21.4800(21.5053) | Bit/dim 3.5783(3.5484) | Xent 0.1128(0.1165) | Loss 3.6347(3.6066) | Error 0.0400(0.0410) Steps 892(885.75) | Grad Norm 1.4462(1.3864) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0255 | Time 105.0269, Epoch Time 1304.2650(1210.7577), Bit/dim 3.5578(best: 3.5553), Xent 0.9576, Loss 4.0366, Error 0.2173(best: 0.2174)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14030 | Time 22.1484(21.5108) | Bit/dim 3.5626(3.5469) | Xent 0.1064(0.1139) | Loss 3.6158(3.6039) | Error 0.0378(0.0404) Steps 880(884.69) | Grad Norm 1.3174(1.4265) | Total Time 14.00(14.00)\n",
      "Iter 14040 | Time 21.1598(21.4691) | Bit/dim 3.5719(3.5465) | Xent 0.1059(0.1143) | Loss 3.6249(3.6037) | Error 0.0400(0.0401) Steps 880(885.34) | Grad Norm 1.4239(1.4091) | Total Time 14.00(14.00)\n",
      "Iter 14050 | Time 21.6984(21.5007) | Bit/dim 3.5638(3.5460) | Xent 0.1061(0.1152) | Loss 3.6169(3.6035) | Error 0.0400(0.0405) Steps 874(884.58) | Grad Norm 1.4797(1.4560) | Total Time 14.00(14.00)\n",
      "Iter 14060 | Time 21.5885(21.5083) | Bit/dim 3.5320(3.5445) | Xent 0.1249(0.1153) | Loss 3.5944(3.6021) | Error 0.0444(0.0399) Steps 910(885.63) | Grad Norm 1.4138(1.4049) | Total Time 14.00(14.00)\n",
      "Iter 14070 | Time 21.3997(21.4549) | Bit/dim 3.5091(3.5445) | Xent 0.1310(0.1151) | Loss 3.5746(3.6021) | Error 0.0400(0.0396) Steps 898(884.79) | Grad Norm 1.4424(1.4206) | Total Time 14.00(14.00)\n",
      "Iter 14080 | Time 22.1388(21.4398) | Bit/dim 3.5599(3.5476) | Xent 0.1251(0.1136) | Loss 3.6224(3.6044) | Error 0.0411(0.0391) Steps 892(884.20) | Grad Norm 1.4968(1.3979) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0256 | Time 106.2283, Epoch Time 1305.3737(1213.5962), Bit/dim 3.5554(best: 3.5553), Xent 0.9614, Loss 4.0361, Error 0.2234(best: 0.2173)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14090 | Time 22.1853(21.5017) | Bit/dim 3.5564(3.5468) | Xent 0.1238(0.1131) | Loss 3.6183(3.6033) | Error 0.0400(0.0387) Steps 892(884.45) | Grad Norm 1.9054(1.4184) | Total Time 14.00(14.00)\n",
      "Iter 14100 | Time 21.2851(21.4357) | Bit/dim 3.5356(3.5469) | Xent 0.0865(0.1132) | Loss 3.5789(3.6035) | Error 0.0289(0.0386) Steps 910(886.53) | Grad Norm 1.1660(1.4073) | Total Time 14.00(14.00)\n",
      "Iter 14110 | Time 21.4087(21.4515) | Bit/dim 3.5398(3.5468) | Xent 0.1124(0.1127) | Loss 3.5960(3.6032) | Error 0.0400(0.0389) Steps 886(886.36) | Grad Norm 1.8670(1.4480) | Total Time 14.00(14.00)\n",
      "Iter 14120 | Time 21.2096(21.4670) | Bit/dim 3.5486(3.5475) | Xent 0.1029(0.1138) | Loss 3.6001(3.6044) | Error 0.0356(0.0390) Steps 904(888.60) | Grad Norm 1.3442(1.4597) | Total Time 14.00(14.00)\n",
      "Iter 14130 | Time 21.6843(21.4708) | Bit/dim 3.5407(3.5456) | Xent 0.0995(0.1153) | Loss 3.5904(3.6032) | Error 0.0333(0.0395) Steps 880(888.49) | Grad Norm 1.1552(1.4248) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0257 | Time 106.6407, Epoch Time 1308.2731(1216.4365), Bit/dim 3.5551(best: 3.5553), Xent 0.9631, Loss 4.0366, Error 0.2205(best: 0.2173)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14140 | Time 21.5599(21.5090) | Bit/dim 3.5250(3.5485) | Xent 0.1351(0.1144) | Loss 3.5926(3.6057) | Error 0.0444(0.0395) Steps 880(888.34) | Grad Norm 2.0440(1.3982) | Total Time 14.00(14.00)\n",
      "Iter 14150 | Time 21.4353(21.4975) | Bit/dim 3.5508(3.5487) | Xent 0.1033(0.1134) | Loss 3.6025(3.6054) | Error 0.0289(0.0388) Steps 868(887.94) | Grad Norm 1.3141(1.3887) | Total Time 14.00(14.00)\n",
      "Iter 14160 | Time 22.0933(21.5450) | Bit/dim 3.5549(3.5511) | Xent 0.1168(0.1115) | Loss 3.6133(3.6069) | Error 0.0400(0.0381) Steps 898(888.80) | Grad Norm 1.8065(1.4018) | Total Time 14.00(14.00)\n",
      "Iter 14170 | Time 21.6369(21.5926) | Bit/dim 3.5115(3.5465) | Xent 0.1021(0.1110) | Loss 3.5626(3.6020) | Error 0.0344(0.0376) Steps 874(890.35) | Grad Norm 1.1073(1.3858) | Total Time 14.00(14.00)\n",
      "Iter 14180 | Time 20.9520(21.5458) | Bit/dim 3.5475(3.5436) | Xent 0.0997(0.1110) | Loss 3.5973(3.5991) | Error 0.0311(0.0377) Steps 880(889.04) | Grad Norm 1.8782(1.3704) | Total Time 14.00(14.00)\n",
      "Iter 14190 | Time 21.3923(21.5383) | Bit/dim 3.5472(3.5456) | Xent 0.0994(0.1104) | Loss 3.5969(3.6008) | Error 0.0300(0.0373) Steps 886(891.03) | Grad Norm 1.1633(1.3390) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0258 | Time 106.7458, Epoch Time 1311.1227(1219.2771), Bit/dim 3.5541(best: 3.5551), Xent 0.9662, Loss 4.0372, Error 0.2210(best: 0.2173)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14200 | Time 20.8518(21.4939) | Bit/dim 3.5705(3.5500) | Xent 0.1241(0.1112) | Loss 3.6326(3.6057) | Error 0.0411(0.0378) Steps 892(890.77) | Grad Norm 1.5942(1.3580) | Total Time 14.00(14.00)\n",
      "Iter 14210 | Time 20.6929(21.4227) | Bit/dim 3.5502(3.5480) | Xent 0.0970(0.1112) | Loss 3.5987(3.6036) | Error 0.0400(0.0387) Steps 880(887.34) | Grad Norm 1.2833(1.3850) | Total Time 14.00(14.00)\n",
      "Iter 14220 | Time 21.3297(21.4396) | Bit/dim 3.5325(3.5474) | Xent 0.1017(0.1106) | Loss 3.5834(3.6027) | Error 0.0322(0.0385) Steps 892(888.01) | Grad Norm 1.4431(1.4693) | Total Time 14.00(14.00)\n",
      "Iter 14230 | Time 21.8059(21.5061) | Bit/dim 3.5321(3.5456) | Xent 0.0879(0.1102) | Loss 3.5760(3.6007) | Error 0.0244(0.0380) Steps 892(887.68) | Grad Norm 1.5032(1.4877) | Total Time 14.00(14.00)\n",
      "Iter 14240 | Time 21.1184(21.4999) | Bit/dim 3.5626(3.5436) | Xent 0.0889(0.1098) | Loss 3.6070(3.5985) | Error 0.0300(0.0380) Steps 892(888.92) | Grad Norm 0.9991(1.4732) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0259 | Time 107.4550, Epoch Time 1307.3290(1221.9186), Bit/dim 3.5555(best: 3.5541), Xent 0.9736, Loss 4.0423, Error 0.2220(best: 0.2173)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14250 | Time 21.5976(21.5230) | Bit/dim 3.5475(3.5458) | Xent 0.1160(0.1076) | Loss 3.6055(3.5996) | Error 0.0422(0.0367) Steps 892(889.88) | Grad Norm 1.2117(1.4254) | Total Time 14.00(14.00)\n",
      "Iter 14260 | Time 21.0095(21.4289) | Bit/dim 3.5710(3.5488) | Xent 0.1188(0.1083) | Loss 3.6304(3.6029) | Error 0.0367(0.0368) Steps 892(889.09) | Grad Norm 1.5218(1.4062) | Total Time 14.00(14.00)\n",
      "Iter 14270 | Time 21.6604(21.4056) | Bit/dim 3.5222(3.5450) | Xent 0.1193(0.1096) | Loss 3.5818(3.5998) | Error 0.0456(0.0378) Steps 868(889.97) | Grad Norm 1.5325(1.4161) | Total Time 14.00(14.00)\n",
      "Iter 14280 | Time 20.7249(21.3854) | Bit/dim 3.5475(3.5433) | Xent 0.0927(0.1102) | Loss 3.5938(3.5985) | Error 0.0322(0.0381) Steps 904(890.73) | Grad Norm 1.6662(1.5237) | Total Time 14.00(14.00)\n",
      "Iter 14290 | Time 21.7607(21.4309) | Bit/dim 3.5614(3.5444) | Xent 0.1225(0.1105) | Loss 3.6227(3.5997) | Error 0.0378(0.0384) Steps 886(891.04) | Grad Norm 1.3247(1.5050) | Total Time 14.00(14.00)\n",
      "Iter 14300 | Time 22.4768(21.4719) | Bit/dim 3.5732(3.5461) | Xent 0.0903(0.1079) | Loss 3.6183(3.6001) | Error 0.0322(0.0378) Steps 898(888.29) | Grad Norm 1.1899(1.4327) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0260 | Time 107.7780, Epoch Time 1303.6382(1224.3702), Bit/dim 3.5558(best: 3.5541), Xent 0.9891, Loss 4.0504, Error 0.2239(best: 0.2173)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14310 | Time 21.5947(21.5088) | Bit/dim 3.5806(3.5464) | Xent 0.0807(0.1075) | Loss 3.6210(3.6001) | Error 0.0256(0.0371) Steps 892(889.71) | Grad Norm 1.4792(1.3987) | Total Time 14.00(14.00)\n",
      "Iter 14320 | Time 21.6017(21.4582) | Bit/dim 3.5490(3.5472) | Xent 0.1185(0.1075) | Loss 3.6082(3.6009) | Error 0.0411(0.0370) Steps 922(892.39) | Grad Norm 1.4292(1.3990) | Total Time 14.00(14.00)\n",
      "Iter 14330 | Time 21.9471(21.4214) | Bit/dim 3.5146(3.5465) | Xent 0.1156(0.1077) | Loss 3.5725(3.6003) | Error 0.0433(0.0370) Steps 904(891.70) | Grad Norm 1.2807(1.3657) | Total Time 14.00(14.00)\n",
      "Iter 14340 | Time 21.7489(21.4190) | Bit/dim 3.5423(3.5478) | Xent 0.1180(0.1088) | Loss 3.6013(3.6022) | Error 0.0356(0.0376) Steps 898(892.79) | Grad Norm 1.3557(1.3759) | Total Time 14.00(14.00)\n",
      "Iter 14350 | Time 22.1609(21.4921) | Bit/dim 3.5471(3.5444) | Xent 0.1061(0.1091) | Loss 3.6002(3.5989) | Error 0.0356(0.0373) Steps 904(893.44) | Grad Norm 1.6073(1.3947) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0261 | Time 105.8770, Epoch Time 1306.4526(1226.8327), Bit/dim 3.5558(best: 3.5541), Xent 1.0036, Loss 4.0576, Error 0.2253(best: 0.2173)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14360 | Time 21.3651(21.5493) | Bit/dim 3.5483(3.5436) | Xent 0.1032(0.1087) | Loss 3.5999(3.5980) | Error 0.0356(0.0375) Steps 910(893.28) | Grad Norm 1.8770(1.4614) | Total Time 14.00(14.00)\n",
      "Iter 14370 | Time 20.8045(21.4757) | Bit/dim 3.5119(3.5441) | Xent 0.0908(0.1075) | Loss 3.5573(3.5978) | Error 0.0344(0.0372) Steps 880(894.31) | Grad Norm 1.2038(1.4672) | Total Time 14.00(14.00)\n",
      "Iter 14380 | Time 21.4254(21.4540) | Bit/dim 3.5553(3.5441) | Xent 0.1086(0.1075) | Loss 3.6096(3.5978) | Error 0.0422(0.0373) Steps 886(894.20) | Grad Norm 1.3205(1.4587) | Total Time 14.00(14.00)\n",
      "Iter 14390 | Time 21.2616(21.4542) | Bit/dim 3.5435(3.5467) | Xent 0.0931(0.1070) | Loss 3.5901(3.6002) | Error 0.0367(0.0371) Steps 910(891.40) | Grad Norm 1.1718(1.4340) | Total Time 14.00(14.00)\n",
      "Iter 14400 | Time 21.4959(21.4492) | Bit/dim 3.5494(3.5473) | Xent 0.1258(0.1074) | Loss 3.6123(3.6010) | Error 0.0489(0.0372) Steps 892(892.01) | Grad Norm 2.2323(1.4621) | Total Time 14.00(14.00)\n",
      "Iter 14410 | Time 21.2447(21.4646) | Bit/dim 3.5235(3.5456) | Xent 0.1049(0.1096) | Loss 3.5760(3.6005) | Error 0.0444(0.0386) Steps 904(892.40) | Grad Norm 1.3447(1.4720) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0262 | Time 105.4250, Epoch Time 1303.7344(1229.1397), Bit/dim 3.5549(best: 3.5541), Xent 0.9933, Loss 4.0515, Error 0.2234(best: 0.2173)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14420 | Time 20.8154(21.4488) | Bit/dim 3.5483(3.5462) | Xent 0.1032(0.1101) | Loss 3.6000(3.6012) | Error 0.0356(0.0388) Steps 880(891.02) | Grad Norm 1.5088(1.4867) | Total Time 14.00(14.00)\n",
      "Iter 14430 | Time 21.3362(21.4272) | Bit/dim 3.5676(3.5477) | Xent 0.1109(0.1089) | Loss 3.6231(3.6022) | Error 0.0467(0.0385) Steps 880(890.22) | Grad Norm 1.3954(1.4751) | Total Time 14.00(14.00)\n",
      "Iter 14440 | Time 21.9913(21.4745) | Bit/dim 3.5416(3.5452) | Xent 0.1275(0.1090) | Loss 3.6054(3.5997) | Error 0.0433(0.0383) Steps 898(890.40) | Grad Norm 1.6464(1.4750) | Total Time 14.00(14.00)\n",
      "Iter 14450 | Time 22.1458(21.4859) | Bit/dim 3.5105(3.5459) | Xent 0.1203(0.1075) | Loss 3.5706(3.5996) | Error 0.0433(0.0375) Steps 886(889.46) | Grad Norm 1.3861(1.4525) | Total Time 14.00(14.00)\n",
      "Iter 14460 | Time 21.7329(21.4821) | Bit/dim 3.5506(3.5456) | Xent 0.0845(0.1053) | Loss 3.5929(3.5982) | Error 0.0267(0.0367) Steps 874(890.13) | Grad Norm 1.2409(1.3958) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0263 | Time 107.4551, Epoch Time 1307.4057(1231.4877), Bit/dim 3.5552(best: 3.5541), Xent 0.9879, Loss 4.0492, Error 0.2211(best: 0.2173)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14470 | Time 22.5833(21.5859) | Bit/dim 3.4987(3.5465) | Xent 0.0867(0.1038) | Loss 3.5420(3.5984) | Error 0.0322(0.0359) Steps 904(891.71) | Grad Norm 1.3002(1.3706) | Total Time 14.00(14.00)\n",
      "Iter 14480 | Time 21.3100(21.5258) | Bit/dim 3.5582(3.5472) | Xent 0.0913(0.1030) | Loss 3.6038(3.5987) | Error 0.0367(0.0364) Steps 886(892.32) | Grad Norm 1.3155(1.3563) | Total Time 14.00(14.00)\n",
      "Iter 14490 | Time 21.4370(21.4637) | Bit/dim 3.5442(3.5459) | Xent 0.0834(0.1033) | Loss 3.5859(3.5975) | Error 0.0344(0.0361) Steps 886(888.62) | Grad Norm 1.5303(1.3561) | Total Time 14.00(14.00)\n",
      "Iter 14500 | Time 20.8194(21.4374) | Bit/dim 3.5431(3.5462) | Xent 0.1176(0.1026) | Loss 3.6019(3.5975) | Error 0.0489(0.0360) Steps 886(888.13) | Grad Norm 1.9301(1.3651) | Total Time 14.00(14.00)\n",
      "Iter 14510 | Time 21.6782(21.3899) | Bit/dim 3.5103(3.5419) | Xent 0.1049(0.1042) | Loss 3.5627(3.5940) | Error 0.0333(0.0360) Steps 880(886.48) | Grad Norm 1.2849(1.3883) | Total Time 14.00(14.00)\n",
      "Iter 14520 | Time 22.3716(21.3876) | Bit/dim 3.5562(3.5441) | Xent 0.1209(0.1049) | Loss 3.6167(3.5966) | Error 0.0378(0.0359) Steps 886(886.21) | Grad Norm 1.5086(1.3949) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0264 | Time 106.1220, Epoch Time 1302.9126(1233.6305), Bit/dim 3.5550(best: 3.5541), Xent 0.9953, Loss 4.0527, Error 0.2222(best: 0.2173)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14530 | Time 21.2335(21.3419) | Bit/dim 3.5178(3.5432) | Xent 0.0873(0.1033) | Loss 3.5614(3.5949) | Error 0.0311(0.0354) Steps 886(883.79) | Grad Norm 1.0932(1.3958) | Total Time 14.00(14.00)\n",
      "Iter 14540 | Time 22.2801(21.3498) | Bit/dim 3.5311(3.5455) | Xent 0.1026(0.1030) | Loss 3.5824(3.5970) | Error 0.0289(0.0350) Steps 898(885.02) | Grad Norm 1.9156(1.3925) | Total Time 14.00(14.00)\n",
      "Iter 14550 | Time 22.3166(21.4708) | Bit/dim 3.5476(3.5454) | Xent 0.1408(0.1041) | Loss 3.6180(3.5975) | Error 0.0433(0.0352) Steps 898(882.78) | Grad Norm 1.3357(1.4221) | Total Time 14.00(14.00)\n",
      "Iter 14560 | Time 21.3823(21.4268) | Bit/dim 3.5626(3.5440) | Xent 0.1189(0.1046) | Loss 3.6220(3.5963) | Error 0.0422(0.0355) Steps 892(884.15) | Grad Norm 1.4915(1.4233) | Total Time 14.00(14.00)\n",
      "Iter 14570 | Time 22.7660(21.5222) | Bit/dim 3.5396(3.5431) | Xent 0.1066(0.1047) | Loss 3.5929(3.5955) | Error 0.0344(0.0355) Steps 880(885.41) | Grad Norm 1.2308(1.4221) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0265 | Time 106.2766, Epoch Time 1307.0849(1235.8341), Bit/dim 3.5538(best: 3.5541), Xent 1.0036, Loss 4.0556, Error 0.2271(best: 0.2173)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14580 | Time 21.0172(21.4857) | Bit/dim 3.5475(3.5442) | Xent 0.0922(0.1037) | Loss 3.5936(3.5961) | Error 0.0233(0.0350) Steps 886(885.47) | Grad Norm 1.3076(1.4237) | Total Time 14.00(14.00)\n",
      "Iter 14590 | Time 21.1247(21.4643) | Bit/dim 3.5005(3.5402) | Xent 0.1142(0.1045) | Loss 3.5576(3.5924) | Error 0.0444(0.0357) Steps 886(886.03) | Grad Norm 1.4342(1.4332) | Total Time 14.00(14.00)\n",
      "Iter 14600 | Time 22.2890(21.4674) | Bit/dim 3.5368(3.5426) | Xent 0.0898(0.1038) | Loss 3.5817(3.5945) | Error 0.0322(0.0362) Steps 892(884.59) | Grad Norm 1.4509(1.4187) | Total Time 14.00(14.00)\n",
      "Iter 14610 | Time 21.5649(21.4082) | Bit/dim 3.5436(3.5413) | Xent 0.1124(0.1023) | Loss 3.5998(3.5925) | Error 0.0389(0.0358) Steps 880(883.81) | Grad Norm 1.7248(1.4427) | Total Time 14.00(14.00)\n",
      "Iter 14620 | Time 21.7589(21.3725) | Bit/dim 3.5726(3.5443) | Xent 0.1162(0.1041) | Loss 3.6307(3.5963) | Error 0.0344(0.0364) Steps 880(883.64) | Grad Norm 1.1467(1.4696) | Total Time 14.00(14.00)\n",
      "Iter 14630 | Time 21.6388(21.3910) | Bit/dim 3.5726(3.5471) | Xent 0.1096(0.1032) | Loss 3.6274(3.5987) | Error 0.0444(0.0366) Steps 904(885.23) | Grad Norm 1.8934(1.4671) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0266 | Time 105.2521, Epoch Time 1298.8118(1237.7234), Bit/dim 3.5532(best: 3.5538), Xent 0.9918, Loss 4.0491, Error 0.2226(best: 0.2173)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14640 | Time 21.1164(21.3059) | Bit/dim 3.5223(3.5443) | Xent 0.0973(0.1068) | Loss 3.5709(3.5977) | Error 0.0378(0.0374) Steps 892(886.95) | Grad Norm 1.7420(1.4835) | Total Time 14.00(14.00)\n",
      "Iter 14650 | Time 21.3310(21.2675) | Bit/dim 3.5595(3.5451) | Xent 0.1191(0.1054) | Loss 3.6191(3.5978) | Error 0.0367(0.0366) Steps 886(886.86) | Grad Norm 1.5619(1.4738) | Total Time 14.00(14.00)\n",
      "Iter 14660 | Time 21.1270(21.3105) | Bit/dim 3.5061(3.5454) | Xent 0.0971(0.1062) | Loss 3.5547(3.5985) | Error 0.0356(0.0367) Steps 862(887.61) | Grad Norm 1.2014(1.5242) | Total Time 14.00(14.00)\n",
      "Iter 14670 | Time 21.7009(21.4271) | Bit/dim 3.5559(3.5470) | Xent 0.1020(0.1064) | Loss 3.6069(3.6002) | Error 0.0311(0.0371) Steps 862(888.96) | Grad Norm 1.4478(1.5487) | Total Time 14.00(14.00)\n",
      "Iter 14680 | Time 21.3899(21.4161) | Bit/dim 3.5402(3.5463) | Xent 0.1149(0.1046) | Loss 3.5976(3.5986) | Error 0.0467(0.0365) Steps 880(888.38) | Grad Norm 1.9859(1.5829) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0267 | Time 105.8809, Epoch Time 1300.5788(1239.6091), Bit/dim 3.5531(best: 3.5532), Xent 1.0185, Loss 4.0623, Error 0.2228(best: 0.2173)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14690 | Time 21.1265(21.3735) | Bit/dim 3.5234(3.5455) | Xent 0.0999(0.1041) | Loss 3.5734(3.5976) | Error 0.0356(0.0364) Steps 886(886.88) | Grad Norm 1.4061(1.6085) | Total Time 14.00(14.00)\n",
      "Iter 14700 | Time 21.7532(21.3712) | Bit/dim 3.5224(3.5448) | Xent 0.0890(0.1015) | Loss 3.5669(3.5955) | Error 0.0256(0.0354) Steps 880(886.84) | Grad Norm 1.0811(1.5709) | Total Time 14.00(14.00)\n",
      "Iter 14710 | Time 21.1798(21.3539) | Bit/dim 3.5375(3.5443) | Xent 0.0866(0.1015) | Loss 3.5808(3.5951) | Error 0.0289(0.0353) Steps 868(885.21) | Grad Norm 1.7811(1.5531) | Total Time 14.00(14.00)\n",
      "Iter 14720 | Time 20.4998(21.3366) | Bit/dim 3.5342(3.5417) | Xent 0.1079(0.1005) | Loss 3.5881(3.5920) | Error 0.0356(0.0348) Steps 880(885.60) | Grad Norm 1.9671(1.5502) | Total Time 14.00(14.00)\n",
      "Iter 14730 | Time 21.4298(21.3384) | Bit/dim 3.5609(3.5418) | Xent 0.1073(0.1001) | Loss 3.6145(3.5919) | Error 0.0356(0.0347) Steps 880(883.74) | Grad Norm 1.4354(1.5489) | Total Time 14.00(14.00)\n",
      "Iter 14740 | Time 21.3043(21.3948) | Bit/dim 3.5701(3.5459) | Xent 0.1110(0.1016) | Loss 3.6256(3.5967) | Error 0.0356(0.0354) Steps 886(881.73) | Grad Norm 2.1475(1.5663) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0268 | Time 105.3526, Epoch Time 1297.7216(1241.3525), Bit/dim 3.5541(best: 3.5531), Xent 1.0194, Loss 4.0638, Error 0.2258(best: 0.2173)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14750 | Time 21.6544(21.4566) | Bit/dim 3.5499(3.5453) | Xent 0.0975(0.1035) | Loss 3.5987(3.5970) | Error 0.0278(0.0359) Steps 862(882.05) | Grad Norm 1.6046(1.5724) | Total Time 14.00(14.00)\n",
      "Iter 14760 | Time 22.1307(21.5381) | Bit/dim 3.5384(3.5464) | Xent 0.1001(0.1032) | Loss 3.5884(3.5980) | Error 0.0378(0.0358) Steps 880(882.71) | Grad Norm 1.4303(1.5303) | Total Time 14.00(14.00)\n",
      "Iter 14770 | Time 21.5299(21.4047) | Bit/dim 3.5352(3.5454) | Xent 0.1160(0.1009) | Loss 3.5932(3.5959) | Error 0.0400(0.0348) Steps 898(884.37) | Grad Norm 1.7953(1.4899) | Total Time 14.00(14.00)\n",
      "Iter 14780 | Time 21.5621(21.5065) | Bit/dim 3.5197(3.5441) | Xent 0.1295(0.1016) | Loss 3.5845(3.5949) | Error 0.0456(0.0350) Steps 868(884.18) | Grad Norm 1.8227(1.4915) | Total Time 14.00(14.00)\n",
      "Iter 14790 | Time 21.9627(21.4617) | Bit/dim 3.5165(3.5414) | Xent 0.1050(0.1010) | Loss 3.5690(3.5919) | Error 0.0378(0.0354) Steps 856(881.83) | Grad Norm 1.2865(1.4836) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0269 | Time 106.7988, Epoch Time 1308.7127(1243.3733), Bit/dim 3.5528(best: 3.5531), Xent 1.0215, Loss 4.0635, Error 0.2239(best: 0.2173)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14800 | Time 22.3012(21.5044) | Bit/dim 3.5101(3.5415) | Xent 0.0966(0.0990) | Loss 3.5584(3.5910) | Error 0.0367(0.0345) Steps 892(883.93) | Grad Norm 1.3775(1.4561) | Total Time 14.00(14.00)\n",
      "Iter 14810 | Time 21.0009(21.4763) | Bit/dim 3.5836(3.5407) | Xent 0.1068(0.1002) | Loss 3.6370(3.5908) | Error 0.0378(0.0353) Steps 862(883.89) | Grad Norm 1.3013(1.4483) | Total Time 14.00(14.00)\n",
      "Iter 14820 | Time 21.4576(21.4289) | Bit/dim 3.5761(3.5441) | Xent 0.1212(0.1013) | Loss 3.6367(3.5948) | Error 0.0467(0.0353) Steps 886(882.06) | Grad Norm 1.4464(1.4550) | Total Time 14.00(14.00)\n",
      "Iter 14830 | Time 20.8188(21.4027) | Bit/dim 3.5738(3.5443) | Xent 0.1199(0.1026) | Loss 3.6338(3.5956) | Error 0.0422(0.0360) Steps 868(883.32) | Grad Norm 1.5438(1.4864) | Total Time 14.00(14.00)\n",
      "Iter 14840 | Time 21.2319(21.3748) | Bit/dim 3.5194(3.5440) | Xent 0.0999(0.1022) | Loss 3.5693(3.5951) | Error 0.0367(0.0357) Steps 898(886.79) | Grad Norm 1.3002(1.4696) | Total Time 14.00(14.00)\n",
      "Iter 14850 | Time 21.4647(21.3597) | Bit/dim 3.5469(3.5429) | Xent 0.0966(0.1017) | Loss 3.5952(3.5937) | Error 0.0344(0.0354) Steps 886(888.52) | Grad Norm 1.2177(1.5004) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0270 | Time 105.5437, Epoch Time 1299.3136(1245.0515), Bit/dim 3.5530(best: 3.5528), Xent 1.0372, Loss 4.0716, Error 0.2247(best: 0.2173)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14860 | Time 21.4562(21.3648) | Bit/dim 3.5500(3.5435) | Xent 0.0975(0.0997) | Loss 3.5987(3.5934) | Error 0.0322(0.0349) Steps 868(888.57) | Grad Norm 0.9936(1.4846) | Total Time 14.00(14.00)\n",
      "Iter 14870 | Time 21.3453(21.4117) | Bit/dim 3.5554(3.5438) | Xent 0.0917(0.1005) | Loss 3.6013(3.5941) | Error 0.0311(0.0348) Steps 892(889.43) | Grad Norm 1.4091(1.4850) | Total Time 14.00(14.00)\n",
      "Iter 14880 | Time 22.1595(21.4024) | Bit/dim 3.5636(3.5432) | Xent 0.0962(0.1003) | Loss 3.6117(3.5933) | Error 0.0322(0.0348) Steps 892(891.74) | Grad Norm 1.3555(1.4716) | Total Time 14.00(14.00)\n",
      "Iter 14890 | Time 20.0798(21.3166) | Bit/dim 3.5354(3.5426) | Xent 0.1050(0.0993) | Loss 3.5879(3.5922) | Error 0.0400(0.0345) Steps 892(890.63) | Grad Norm 1.6018(1.4351) | Total Time 14.00(14.00)\n",
      "Iter 14900 | Time 21.6345(21.3134) | Bit/dim 3.5640(3.5427) | Xent 0.0938(0.1005) | Loss 3.6109(3.5929) | Error 0.0311(0.0349) Steps 886(887.93) | Grad Norm 1.7599(1.4903) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0271 | Time 105.5001, Epoch Time 1297.9577(1246.6387), Bit/dim 3.5530(best: 3.5528), Xent 1.0497, Loss 4.0778, Error 0.2260(best: 0.2173)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14910 | Time 22.3152(21.3416) | Bit/dim 3.5521(3.5439) | Xent 0.0864(0.0994) | Loss 3.5953(3.5936) | Error 0.0311(0.0350) Steps 874(885.99) | Grad Norm 1.4352(1.5250) | Total Time 14.00(14.00)\n",
      "Iter 14920 | Time 21.3583(21.4099) | Bit/dim 3.5111(3.5432) | Xent 0.0992(0.1001) | Loss 3.5607(3.5932) | Error 0.0311(0.0349) Steps 886(886.11) | Grad Norm 1.2853(1.5103) | Total Time 14.00(14.00)\n",
      "Iter 14930 | Time 21.1306(21.3526) | Bit/dim 3.5405(3.5431) | Xent 0.0894(0.1007) | Loss 3.5852(3.5935) | Error 0.0267(0.0347) Steps 892(886.20) | Grad Norm 1.2348(1.5700) | Total Time 14.00(14.00)\n",
      "Iter 14940 | Time 20.8903(21.3185) | Bit/dim 3.5221(3.5421) | Xent 0.1158(0.1018) | Loss 3.5800(3.5930) | Error 0.0389(0.0352) Steps 868(883.29) | Grad Norm 1.3669(1.5294) | Total Time 14.00(14.00)\n",
      "Iter 14950 | Time 21.2666(21.2875) | Bit/dim 3.5388(3.5447) | Xent 0.1038(0.1025) | Loss 3.5907(3.5960) | Error 0.0356(0.0357) Steps 874(883.99) | Grad Norm 1.2477(1.4804) | Total Time 14.00(14.00)\n",
      "Iter 14960 | Time 22.1283(21.2609) | Bit/dim 3.5083(3.5432) | Xent 0.1025(0.1016) | Loss 3.5596(3.5940) | Error 0.0378(0.0356) Steps 874(885.53) | Grad Norm 1.8906(1.5174) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0272 | Time 104.6909, Epoch Time 1294.7457(1248.0819), Bit/dim 3.5530(best: 3.5528), Xent 1.0390, Loss 4.0725, Error 0.2245(best: 0.2173)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14970 | Time 21.3615(21.2371) | Bit/dim 3.5197(3.5421) | Xent 0.1189(0.1005) | Loss 3.5791(3.5923) | Error 0.0400(0.0351) Steps 892(884.87) | Grad Norm 1.6526(1.4652) | Total Time 14.00(14.00)\n",
      "Iter 14980 | Time 21.6798(21.2835) | Bit/dim 3.5163(3.5417) | Xent 0.0969(0.1006) | Loss 3.5648(3.5920) | Error 0.0389(0.0354) Steps 898(883.22) | Grad Norm 1.6956(1.4805) | Total Time 14.00(14.00)\n",
      "Iter 14990 | Time 21.0109(21.2185) | Bit/dim 3.5624(3.5432) | Xent 0.1143(0.1024) | Loss 3.6196(3.5945) | Error 0.0422(0.0356) Steps 880(885.18) | Grad Norm 1.3141(1.5517) | Total Time 14.00(14.00)\n",
      "Iter 15000 | Time 20.9145(21.1960) | Bit/dim 3.5667(3.5436) | Xent 0.0891(0.1019) | Loss 3.6112(3.5946) | Error 0.0300(0.0356) Steps 874(885.96) | Grad Norm 1.7868(1.5853) | Total Time 14.00(14.00)\n",
      "Iter 15010 | Time 21.5052(21.2170) | Bit/dim 3.5568(3.5438) | Xent 0.1026(0.1038) | Loss 3.6081(3.5957) | Error 0.0422(0.0361) Steps 880(885.66) | Grad Norm 1.2985(1.6763) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0273 | Time 104.5479, Epoch Time 1289.2774(1249.3177), Bit/dim 3.5522(best: 3.5528), Xent 1.0359, Loss 4.0701, Error 0.2254(best: 0.2173)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15020 | Time 20.9565(21.1758) | Bit/dim 3.5373(3.5445) | Xent 0.1133(0.1026) | Loss 3.5940(3.5958) | Error 0.0356(0.0357) Steps 898(884.45) | Grad Norm 1.4564(1.6337) | Total Time 14.00(14.00)\n",
      "Iter 15030 | Time 21.4140(21.1977) | Bit/dim 3.5089(3.5414) | Xent 0.0959(0.1000) | Loss 3.5569(3.5914) | Error 0.0367(0.0345) Steps 880(885.64) | Grad Norm 1.5191(1.6224) | Total Time 14.00(14.00)\n",
      "Iter 15040 | Time 21.0233(21.2337) | Bit/dim 3.5302(3.5429) | Xent 0.1242(0.1004) | Loss 3.5922(3.5931) | Error 0.0456(0.0346) Steps 838(884.87) | Grad Norm 2.3044(1.6451) | Total Time 14.00(14.00)\n",
      "Iter 15050 | Time 21.4576(21.2354) | Bit/dim 3.5710(3.5411) | Xent 0.1212(0.1013) | Loss 3.6316(3.5918) | Error 0.0378(0.0348) Steps 874(883.68) | Grad Norm 1.8490(1.6113) | Total Time 14.00(14.00)\n",
      "Iter 15060 | Time 20.7305(21.2520) | Bit/dim 3.5645(3.5430) | Xent 0.0988(0.1002) | Loss 3.6139(3.5931) | Error 0.0356(0.0349) Steps 892(883.30) | Grad Norm 1.3613(1.5379) | Total Time 14.00(14.00)\n",
      "Iter 15070 | Time 21.1052(21.2461) | Bit/dim 3.5473(3.5429) | Xent 0.0997(0.1003) | Loss 3.5971(3.5931) | Error 0.0322(0.0348) Steps 874(883.23) | Grad Norm 1.1977(1.5156) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0274 | Time 104.8400, Epoch Time 1292.4520(1250.6118), Bit/dim 3.5518(best: 3.5522), Xent 1.0193, Loss 4.0615, Error 0.2250(best: 0.2173)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15080 | Time 20.6609(21.2219) | Bit/dim 3.5367(3.5398) | Xent 0.0971(0.1007) | Loss 3.5853(3.5901) | Error 0.0322(0.0352) Steps 904(882.94) | Grad Norm 1.5193(1.5121) | Total Time 14.00(14.00)\n",
      "Iter 15090 | Time 21.5311(21.1988) | Bit/dim 3.5675(3.5423) | Xent 0.1089(0.1003) | Loss 3.6219(3.5925) | Error 0.0344(0.0349) Steps 898(883.93) | Grad Norm 1.2385(1.5203) | Total Time 14.00(14.00)\n",
      "Iter 15100 | Time 20.6140(21.2422) | Bit/dim 3.5647(3.5434) | Xent 0.0827(0.0986) | Loss 3.6060(3.5927) | Error 0.0256(0.0344) Steps 892(885.21) | Grad Norm 1.1683(1.5141) | Total Time 14.00(14.00)\n",
      "Iter 15110 | Time 21.1654(21.2606) | Bit/dim 3.5649(3.5429) | Xent 0.0935(0.0988) | Loss 3.6117(3.5923) | Error 0.0322(0.0342) Steps 898(885.29) | Grad Norm 1.5176(1.5306) | Total Time 14.00(14.00)\n",
      "Iter 15120 | Time 20.4569(21.2207) | Bit/dim 3.5275(3.5437) | Xent 0.0896(0.0976) | Loss 3.5723(3.5925) | Error 0.0289(0.0337) Steps 886(884.94) | Grad Norm 1.4308(1.5320) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0275 | Time 105.2984, Epoch Time 1292.2829(1251.8619), Bit/dim 3.5541(best: 3.5518), Xent 1.0376, Loss 4.0729, Error 0.2215(best: 0.2173)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15130 | Time 20.9193(21.2341) | Bit/dim 3.5656(3.5450) | Xent 0.0871(0.0977) | Loss 3.6092(3.5939) | Error 0.0311(0.0338) Steps 898(883.93) | Grad Norm 1.4198(1.5031) | Total Time 14.00(14.00)\n",
      "Iter 15140 | Time 20.9754(21.2202) | Bit/dim 3.5747(3.5460) | Xent 0.1113(0.0980) | Loss 3.6303(3.5950) | Error 0.0367(0.0337) Steps 874(882.14) | Grad Norm 1.8204(1.5157) | Total Time 14.00(14.00)\n",
      "Iter 15150 | Time 21.1298(21.2406) | Bit/dim 3.5560(3.5471) | Xent 0.0820(0.0979) | Loss 3.5970(3.5961) | Error 0.0289(0.0334) Steps 892(881.05) | Grad Norm 1.5168(1.4740) | Total Time 14.00(14.00)\n",
      "Iter 15160 | Time 21.2331(21.2908) | Bit/dim 3.5330(3.5444) | Xent 0.0928(0.0962) | Loss 3.5794(3.5925) | Error 0.0344(0.0330) Steps 868(881.63) | Grad Norm 1.3612(1.4542) | Total Time 14.00(14.00)\n",
      "Iter 15170 | Time 21.5662(21.3356) | Bit/dim 3.4986(3.5410) | Xent 0.1245(0.0991) | Loss 3.5608(3.5906) | Error 0.0411(0.0335) Steps 862(879.03) | Grad Norm 1.7566(1.4393) | Total Time 14.00(14.00)\n",
      "Iter 15180 | Time 21.1082(21.3753) | Bit/dim 3.5184(3.5407) | Xent 0.0895(0.0989) | Loss 3.5631(3.5901) | Error 0.0300(0.0339) Steps 862(880.71) | Grad Norm 1.3009(1.4443) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0276 | Time 104.7015, Epoch Time 1298.1510(1253.2506), Bit/dim 3.5548(best: 3.5518), Xent 1.0660, Loss 4.0877, Error 0.2246(best: 0.2173)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15190 | Time 21.4909(21.3318) | Bit/dim 3.5563(3.5401) | Xent 0.1048(0.0991) | Loss 3.6087(3.5896) | Error 0.0322(0.0339) Steps 892(881.28) | Grad Norm 2.3166(1.5547) | Total Time 14.00(14.00)\n",
      "Iter 15200 | Time 21.9809(21.3667) | Bit/dim 3.5600(3.5414) | Xent 0.0746(0.0992) | Loss 3.5973(3.5910) | Error 0.0289(0.0337) Steps 874(882.60) | Grad Norm 1.4328(1.5745) | Total Time 14.00(14.00)\n",
      "Iter 15210 | Time 22.4238(21.3882) | Bit/dim 3.5254(3.5394) | Xent 0.0877(0.0994) | Loss 3.5693(3.5891) | Error 0.0311(0.0341) Steps 880(882.94) | Grad Norm 1.3540(1.5737) | Total Time 14.00(14.00)\n",
      "Iter 15220 | Time 21.2192(21.4183) | Bit/dim 3.5542(3.5418) | Xent 0.0861(0.0990) | Loss 3.5973(3.5913) | Error 0.0267(0.0347) Steps 886(883.85) | Grad Norm 1.4758(1.5405) | Total Time 14.00(14.00)\n",
      "Iter 15230 | Time 21.7565(21.3518) | Bit/dim 3.5505(3.5423) | Xent 0.1071(0.0981) | Loss 3.6040(3.5914) | Error 0.0389(0.0346) Steps 886(883.89) | Grad Norm 1.8937(1.5723) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0277 | Time 104.3017, Epoch Time 1296.2083(1254.5393), Bit/dim 3.5511(best: 3.5518), Xent 1.0546, Loss 4.0784, Error 0.2248(best: 0.2173)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15240 | Time 21.0608(21.2501) | Bit/dim 3.5540(3.5440) | Xent 0.1030(0.0973) | Loss 3.6056(3.5926) | Error 0.0356(0.0344) Steps 910(883.67) | Grad Norm 1.9354(1.6144) | Total Time 14.00(14.00)\n",
      "Iter 15250 | Time 22.1041(21.2501) | Bit/dim 3.5602(3.5460) | Xent 0.1074(0.0979) | Loss 3.6139(3.5950) | Error 0.0422(0.0350) Steps 892(884.02) | Grad Norm 1.6708(1.6052) | Total Time 14.00(14.00)\n",
      "Iter 15260 | Time 21.4561(21.2826) | Bit/dim 3.5534(3.5430) | Xent 0.1252(0.0982) | Loss 3.6161(3.5921) | Error 0.0378(0.0349) Steps 874(887.13) | Grad Norm 2.1597(1.7335) | Total Time 14.00(14.00)\n",
      "Iter 15270 | Time 22.0644(21.3560) | Bit/dim 3.5397(3.5459) | Xent 0.0847(0.0979) | Loss 3.5821(3.5948) | Error 0.0289(0.0348) Steps 886(886.71) | Grad Norm 1.7959(1.8491) | Total Time 14.00(14.00)\n",
      "Iter 15280 | Time 21.6163(21.3050) | Bit/dim 3.5433(3.5440) | Xent 0.0914(0.0967) | Loss 3.5890(3.5924) | Error 0.0289(0.0337) Steps 892(887.16) | Grad Norm 1.3121(1.8434) | Total Time 14.00(14.00)\n",
      "Iter 15290 | Time 21.0258(21.2542) | Bit/dim 3.5247(3.5417) | Xent 0.0840(0.0977) | Loss 3.5668(3.5905) | Error 0.0311(0.0350) Steps 892(888.19) | Grad Norm 1.4719(1.8083) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0278 | Time 105.2190, Epoch Time 1292.8165(1255.6876), Bit/dim 3.5526(best: 3.5511), Xent 1.0651, Loss 4.0851, Error 0.2274(best: 0.2173)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15300 | Time 20.8321(21.2552) | Bit/dim 3.5797(3.5434) | Xent 0.0870(0.0981) | Loss 3.6232(3.5924) | Error 0.0278(0.0346) Steps 892(888.16) | Grad Norm 1.7556(1.7389) | Total Time 14.00(14.00)\n",
      "Iter 15310 | Time 21.5559(21.2727) | Bit/dim 3.5493(3.5432) | Xent 0.0934(0.0964) | Loss 3.5960(3.5914) | Error 0.0244(0.0340) Steps 898(889.66) | Grad Norm 1.5602(1.6664) | Total Time 14.00(14.00)\n",
      "Iter 15320 | Time 21.7000(21.3589) | Bit/dim 3.5356(3.5421) | Xent 0.0916(0.0979) | Loss 3.5814(3.5910) | Error 0.0356(0.0343) Steps 874(889.46) | Grad Norm 1.4056(1.6896) | Total Time 14.00(14.00)\n",
      "Iter 15330 | Time 20.8735(21.3480) | Bit/dim 3.5540(3.5415) | Xent 0.1182(0.0985) | Loss 3.6131(3.5907) | Error 0.0400(0.0346) Steps 868(887.94) | Grad Norm 2.0951(1.6868) | Total Time 14.00(14.00)\n",
      "Iter 15340 | Time 21.4258(21.4053) | Bit/dim 3.5249(3.5414) | Xent 0.0925(0.0969) | Loss 3.5712(3.5898) | Error 0.0333(0.0341) Steps 886(887.79) | Grad Norm 1.5625(1.6531) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0279 | Time 107.0544, Epoch Time 1303.4419(1257.1202), Bit/dim 3.5517(best: 3.5511), Xent 1.0442, Loss 4.0738, Error 0.2231(best: 0.2173)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15350 | Time 21.8051(21.4384) | Bit/dim 3.5518(3.5408) | Xent 0.0971(0.0981) | Loss 3.6004(3.5898) | Error 0.0400(0.0343) Steps 874(885.76) | Grad Norm 1.7876(1.6527) | Total Time 14.00(14.00)\n",
      "Iter 15360 | Time 21.1866(21.4794) | Bit/dim 3.5716(3.5419) | Xent 0.0634(0.0966) | Loss 3.6033(3.5902) | Error 0.0211(0.0337) Steps 862(884.81) | Grad Norm 1.0551(1.6261) | Total Time 14.00(14.00)\n",
      "Iter 15370 | Time 21.4268(21.4834) | Bit/dim 3.5674(3.5421) | Xent 0.0842(0.0948) | Loss 3.6095(3.5895) | Error 0.0333(0.0336) Steps 868(884.47) | Grad Norm 1.2481(1.5827) | Total Time 14.00(14.00)\n",
      "Iter 15380 | Time 21.7146(21.4938) | Bit/dim 3.5448(3.5413) | Xent 0.1102(0.0952) | Loss 3.5999(3.5889) | Error 0.0356(0.0337) Steps 898(884.05) | Grad Norm 1.6129(1.5916) | Total Time 14.00(14.00)\n",
      "Iter 15390 | Time 21.2164(21.4841) | Bit/dim 3.5706(3.5432) | Xent 0.0753(0.0951) | Loss 3.6083(3.5908) | Error 0.0200(0.0333) Steps 874(882.64) | Grad Norm 1.4467(1.6360) | Total Time 14.00(14.00)\n",
      "Iter 15400 | Time 21.0036(21.4820) | Bit/dim 3.5190(3.5424) | Xent 0.1023(0.0949) | Loss 3.5702(3.5899) | Error 0.0356(0.0334) Steps 892(883.87) | Grad Norm 2.9441(1.7283) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0280 | Time 106.2238, Epoch Time 1308.7069(1258.6678), Bit/dim 3.5519(best: 3.5511), Xent 1.0740, Loss 4.0889, Error 0.2284(best: 0.2173)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15410 | Time 21.3561(21.3990) | Bit/dim 3.5494(3.5417) | Xent 0.1002(0.0961) | Loss 3.5996(3.5898) | Error 0.0356(0.0340) Steps 874(883.69) | Grad Norm 1.5553(1.6868) | Total Time 14.00(14.00)\n",
      "Iter 15420 | Time 21.1683(21.3661) | Bit/dim 3.5574(3.5458) | Xent 0.0993(0.0958) | Loss 3.6070(3.5937) | Error 0.0333(0.0341) Steps 868(881.95) | Grad Norm 1.6802(1.6788) | Total Time 14.00(14.00)\n",
      "Iter 15430 | Time 21.0999(21.3814) | Bit/dim 3.5406(3.5458) | Xent 0.1002(0.0952) | Loss 3.5908(3.5934) | Error 0.0389(0.0338) Steps 874(883.43) | Grad Norm 1.6082(1.6424) | Total Time 14.00(14.00)\n",
      "Iter 15440 | Time 20.7509(21.3520) | Bit/dim 3.5058(3.5425) | Xent 0.0879(0.0969) | Loss 3.5498(3.5910) | Error 0.0311(0.0339) Steps 874(885.17) | Grad Norm 1.6162(1.6171) | Total Time 14.00(14.00)\n",
      "Iter 15450 | Time 21.6807(21.3410) | Bit/dim 3.5434(3.5391) | Xent 0.0843(0.0965) | Loss 3.5855(3.5874) | Error 0.0300(0.0336) Steps 880(884.77) | Grad Norm 1.6568(1.6348) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0281 | Time 106.7018, Epoch Time 1297.5053(1259.8330), Bit/dim 3.5516(best: 3.5511), Xent 1.0722, Loss 4.0877, Error 0.2283(best: 0.2173)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15460 | Time 21.2394(21.3912) | Bit/dim 3.5631(3.5418) | Xent 0.1112(0.0952) | Loss 3.6187(3.5894) | Error 0.0333(0.0327) Steps 892(882.76) | Grad Norm 1.3888(1.5948) | Total Time 14.00(14.00)\n",
      "Iter 15470 | Time 21.6142(21.3465) | Bit/dim 3.5290(3.5401) | Xent 0.0869(0.0955) | Loss 3.5724(3.5878) | Error 0.0333(0.0332) Steps 868(882.77) | Grad Norm 1.8011(1.6110) | Total Time 14.00(14.00)\n",
      "Iter 15480 | Time 21.1939(21.3520) | Bit/dim 3.5830(3.5423) | Xent 0.0877(0.0949) | Loss 3.6269(3.5898) | Error 0.0311(0.0329) Steps 874(881.70) | Grad Norm 1.5479(1.5835) | Total Time 14.00(14.00)\n",
      "Iter 15490 | Time 21.4931(21.3758) | Bit/dim 3.5323(3.5418) | Xent 0.0851(0.0930) | Loss 3.5748(3.5883) | Error 0.0289(0.0323) Steps 904(883.18) | Grad Norm 1.0735(1.5501) | Total Time 14.00(14.00)\n",
      "Iter 15500 | Time 20.7439(21.3024) | Bit/dim 3.5590(3.5433) | Xent 0.1127(0.0949) | Loss 3.6153(3.5908) | Error 0.0411(0.0332) Steps 880(884.94) | Grad Norm 2.0457(1.6592) | Total Time 14.00(14.00)\n",
      "Iter 15510 | Time 21.1879(21.2803) | Bit/dim 3.5152(3.5405) | Xent 0.0827(0.0958) | Loss 3.5566(3.5884) | Error 0.0311(0.0335) Steps 874(885.52) | Grad Norm 1.4030(1.7574) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0282 | Time 106.0597, Epoch Time 1296.2544(1260.9256), Bit/dim 3.5517(best: 3.5511), Xent 1.0943, Loss 4.0989, Error 0.2249(best: 0.2173)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15520 | Time 20.9888(21.2901) | Bit/dim 3.5433(3.5407) | Xent 0.1130(0.0973) | Loss 3.5998(3.5893) | Error 0.0389(0.0336) Steps 874(884.62) | Grad Norm 1.7135(1.7059) | Total Time 14.00(14.00)\n",
      "Iter 15530 | Time 21.6774(21.2499) | Bit/dim 3.5311(3.5391) | Xent 0.1148(0.0960) | Loss 3.5885(3.5871) | Error 0.0422(0.0333) Steps 922(885.72) | Grad Norm 2.0380(1.7043) | Total Time 14.00(14.00)\n",
      "Iter 15540 | Time 20.8683(21.2909) | Bit/dim 3.5426(3.5379) | Xent 0.0879(0.0965) | Loss 3.5865(3.5862) | Error 0.0233(0.0337) Steps 868(887.05) | Grad Norm 1.4735(1.6902) | Total Time 14.00(14.00)\n",
      "Iter 15550 | Time 21.5233(21.2893) | Bit/dim 3.5191(3.5380) | Xent 0.0807(0.0976) | Loss 3.5595(3.5868) | Error 0.0267(0.0342) Steps 898(883.98) | Grad Norm 1.1020(1.6898) | Total Time 14.00(14.00)\n",
      "Iter 15560 | Time 21.7328(21.3815) | Bit/dim 3.5618(3.5412) | Xent 0.0995(0.0988) | Loss 3.6115(3.5906) | Error 0.0278(0.0344) Steps 880(883.14) | Grad Norm 1.9248(1.7148) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0283 | Time 105.6612, Epoch Time 1298.8808(1262.0643), Bit/dim 3.5513(best: 3.5511), Xent 1.0910, Loss 4.0967, Error 0.2278(best: 0.2173)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15570 | Time 21.3751(21.4205) | Bit/dim 3.5612(3.5412) | Xent 0.0796(0.0981) | Loss 3.6010(3.5902) | Error 0.0256(0.0343) Steps 880(882.56) | Grad Norm 1.9852(1.7118) | Total Time 14.00(14.00)\n",
      "Iter 15580 | Time 21.1012(21.3881) | Bit/dim 3.5484(3.5415) | Xent 0.1179(0.1009) | Loss 3.6073(3.5920) | Error 0.0378(0.0351) Steps 880(883.78) | Grad Norm 1.5974(1.7321) | Total Time 14.00(14.00)\n",
      "Iter 15590 | Time 21.5318(21.4598) | Bit/dim 3.5374(3.5422) | Xent 0.0977(0.0982) | Loss 3.5863(3.5912) | Error 0.0367(0.0343) Steps 892(886.38) | Grad Norm 1.3303(1.6369) | Total Time 14.00(14.00)\n",
      "Iter 15600 | Time 20.9415(21.4574) | Bit/dim 3.5381(3.5432) | Xent 0.0907(0.0967) | Loss 3.5834(3.5915) | Error 0.0367(0.0339) Steps 856(886.65) | Grad Norm 1.9405(1.6420) | Total Time 14.00(14.00)\n",
      "Iter 15610 | Time 21.0961(21.3263) | Bit/dim 3.5448(3.5428) | Xent 0.0900(0.0965) | Loss 3.5898(3.5911) | Error 0.0300(0.0335) Steps 886(886.15) | Grad Norm 2.2626(1.7039) | Total Time 14.00(14.00)\n",
      "Iter 15620 | Time 21.2149(21.2961) | Bit/dim 3.5231(3.5419) | Xent 0.0935(0.0968) | Loss 3.5699(3.5903) | Error 0.0333(0.0335) Steps 886(887.39) | Grad Norm 1.1834(1.7549) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0284 | Time 106.1181, Epoch Time 1299.5053(1263.1875), Bit/dim 3.5508(best: 3.5511), Xent 1.0854, Loss 4.0935, Error 0.2252(best: 0.2173)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15630 | Time 21.9600(21.3620) | Bit/dim 3.5719(3.5408) | Xent 0.0943(0.0962) | Loss 3.6190(3.5889) | Error 0.0344(0.0341) Steps 886(887.96) | Grad Norm 1.7357(1.7052) | Total Time 14.00(14.00)\n",
      "Iter 15640 | Time 21.6464(21.3633) | Bit/dim 3.5445(3.5421) | Xent 0.1083(0.0950) | Loss 3.5987(3.5896) | Error 0.0356(0.0332) Steps 898(889.27) | Grad Norm 1.8390(1.7245) | Total Time 14.00(14.00)\n",
      "Iter 15650 | Time 20.6106(21.2881) | Bit/dim 3.5400(3.5411) | Xent 0.0962(0.0964) | Loss 3.5880(3.5894) | Error 0.0311(0.0335) Steps 892(888.78) | Grad Norm 1.9314(1.7118) | Total Time 14.00(14.00)\n",
      "Iter 15660 | Time 21.2864(21.3006) | Bit/dim 3.5216(3.5397) | Xent 0.1034(0.0958) | Loss 3.5733(3.5876) | Error 0.0400(0.0329) Steps 880(888.39) | Grad Norm 2.0580(1.7032) | Total Time 14.00(14.00)\n",
      "Iter 15670 | Time 20.7806(21.1920) | Bit/dim 3.5586(3.5425) | Xent 0.0921(0.0963) | Loss 3.6046(3.5907) | Error 0.0267(0.0331) Steps 886(885.90) | Grad Norm 1.5760(1.7520) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0285 | Time 104.7444, Epoch Time 1292.3169(1264.0614), Bit/dim 3.5512(best: 3.5508), Xent 1.1134, Loss 4.1079, Error 0.2297(best: 0.2173)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15680 | Time 21.3056(21.2580) | Bit/dim 3.5605(3.5418) | Xent 0.1309(0.0996) | Loss 3.6260(3.5916) | Error 0.0456(0.0348) Steps 910(886.15) | Grad Norm 3.0399(1.9365) | Total Time 14.00(14.00)\n",
      "Iter 15690 | Time 21.4540(21.2363) | Bit/dim 3.5564(3.5400) | Xent 0.0887(0.0995) | Loss 3.6007(3.5897) | Error 0.0256(0.0350) Steps 904(887.60) | Grad Norm 2.1755(1.9858) | Total Time 14.00(14.00)\n",
      "Iter 15700 | Time 21.7128(21.3157) | Bit/dim 3.5650(3.5422) | Xent 0.0889(0.1005) | Loss 3.6095(3.5925) | Error 0.0311(0.0350) Steps 892(886.44) | Grad Norm 1.4934(1.9369) | Total Time 14.00(14.00)\n",
      "Iter 15710 | Time 21.8300(21.3955) | Bit/dim 3.5539(3.5436) | Xent 0.1017(0.0976) | Loss 3.6048(3.5924) | Error 0.0322(0.0338) Steps 892(885.71) | Grad Norm 1.9752(1.8303) | Total Time 14.00(14.00)\n",
      "Iter 15720 | Time 21.5980(21.4064) | Bit/dim 3.5300(3.5418) | Xent 0.0914(0.0973) | Loss 3.5757(3.5905) | Error 0.0300(0.0346) Steps 892(885.60) | Grad Norm 1.8078(1.8541) | Total Time 14.00(14.00)\n",
      "Iter 15730 | Time 21.5852(21.3927) | Bit/dim 3.5809(3.5420) | Xent 0.0909(0.0982) | Loss 3.6263(3.5911) | Error 0.0333(0.0350) Steps 868(885.83) | Grad Norm 2.1183(1.8578) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0286 | Time 104.3209, Epoch Time 1302.2804(1265.2079), Bit/dim 3.5509(best: 3.5508), Xent 1.0861, Loss 4.0940, Error 0.2250(best: 0.2173)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15740 | Time 21.7267(21.4587) | Bit/dim 3.5207(3.5416) | Xent 0.0889(0.0966) | Loss 3.5651(3.5899) | Error 0.0333(0.0344) Steps 904(886.11) | Grad Norm 1.6856(1.8378) | Total Time 14.00(14.00)\n",
      "Iter 15750 | Time 21.8419(21.5038) | Bit/dim 3.5480(3.5412) | Xent 0.0627(0.0957) | Loss 3.5793(3.5890) | Error 0.0167(0.0333) Steps 898(884.28) | Grad Norm 1.4976(1.8214) | Total Time 14.00(14.00)\n",
      "Iter 15760 | Time 22.0008(21.5140) | Bit/dim 3.5545(3.5389) | Xent 0.0756(0.0957) | Loss 3.5923(3.5867) | Error 0.0267(0.0333) Steps 898(883.72) | Grad Norm 1.4288(1.7727) | Total Time 14.00(14.00)\n",
      "Iter 15770 | Time 21.5145(21.4989) | Bit/dim 3.4997(3.5388) | Xent 0.1005(0.0963) | Loss 3.5500(3.5870) | Error 0.0333(0.0337) Steps 892(884.47) | Grad Norm 1.4146(1.7153) | Total Time 14.00(14.00)\n",
      "Iter 15780 | Time 22.0264(21.4945) | Bit/dim 3.5292(3.5398) | Xent 0.0713(0.0959) | Loss 3.5648(3.5878) | Error 0.0244(0.0336) Steps 898(882.56) | Grad Norm 1.6585(1.7559) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0287 | Time 106.2367, Epoch Time 1310.6428(1266.5710), Bit/dim 3.5506(best: 3.5508), Xent 1.0953, Loss 4.0982, Error 0.2256(best: 0.2173)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15790 | Time 21.2732(21.4731) | Bit/dim 3.5476(3.5415) | Xent 0.0880(0.0979) | Loss 3.5916(3.5905) | Error 0.0300(0.0343) Steps 898(883.84) | Grad Norm 1.5206(1.8038) | Total Time 14.00(14.00)\n",
      "Iter 15800 | Time 21.3078(21.5609) | Bit/dim 3.5727(3.5428) | Xent 0.0803(0.0953) | Loss 3.6128(3.5904) | Error 0.0278(0.0334) Steps 880(884.91) | Grad Norm 1.5089(1.7558) | Total Time 14.00(14.00)\n",
      "Iter 15810 | Time 21.4910(21.5235) | Bit/dim 3.5699(3.5444) | Xent 0.1056(0.0961) | Loss 3.6227(3.5924) | Error 0.0322(0.0336) Steps 886(884.91) | Grad Norm 2.2951(1.7515) | Total Time 14.00(14.00)\n",
      "Iter 15820 | Time 21.0773(21.4820) | Bit/dim 3.5114(3.5419) | Xent 0.1272(0.0974) | Loss 3.5750(3.5906) | Error 0.0411(0.0337) Steps 880(884.64) | Grad Norm 2.1297(1.7421) | Total Time 14.00(14.00)\n",
      "Iter 15830 | Time 21.6980(21.4004) | Bit/dim 3.5343(3.5412) | Xent 0.0892(0.0966) | Loss 3.5789(3.5895) | Error 0.0344(0.0339) Steps 856(884.50) | Grad Norm 1.5706(1.6969) | Total Time 14.00(14.00)\n",
      "Iter 15840 | Time 21.4422(21.4303) | Bit/dim 3.5363(3.5395) | Xent 0.1152(0.0952) | Loss 3.5939(3.5871) | Error 0.0367(0.0333) Steps 886(882.29) | Grad Norm 2.0347(1.6953) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0288 | Time 104.7484, Epoch Time 1302.5182(1267.6494), Bit/dim 3.5517(best: 3.5506), Xent 1.0880, Loss 4.0957, Error 0.2246(best: 0.2173)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15850 | Time 21.0366(21.3749) | Bit/dim 3.5688(3.5415) | Xent 0.0837(0.0931) | Loss 3.6106(3.5880) | Error 0.0344(0.0327) Steps 892(882.68) | Grad Norm 1.6582(1.7073) | Total Time 14.00(14.00)\n",
      "Iter 15860 | Time 21.4235(21.4455) | Bit/dim 3.5628(3.5435) | Xent 0.0893(0.0926) | Loss 3.6074(3.5898) | Error 0.0233(0.0324) Steps 886(882.97) | Grad Norm 1.3726(1.6796) | Total Time 14.00(14.00)\n",
      "Iter 15870 | Time 22.0677(21.4382) | Bit/dim 3.5824(3.5460) | Xent 0.0979(0.0933) | Loss 3.6313(3.5927) | Error 0.0322(0.0321) Steps 874(882.73) | Grad Norm 1.9452(1.7118) | Total Time 14.00(14.00)\n",
      "Iter 15880 | Time 21.6195(21.4473) | Bit/dim 3.5368(3.5424) | Xent 0.0952(0.0935) | Loss 3.5844(3.5891) | Error 0.0344(0.0328) Steps 892(882.61) | Grad Norm 1.8170(1.6653) | Total Time 14.00(14.00)\n",
      "Iter 15890 | Time 21.2946(21.4775) | Bit/dim 3.5426(3.5395) | Xent 0.1206(0.0951) | Loss 3.6029(3.5870) | Error 0.0444(0.0333) Steps 892(884.15) | Grad Norm 1.8679(1.6516) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0289 | Time 105.7144, Epoch Time 1305.0601(1268.7717), Bit/dim 3.5514(best: 3.5506), Xent 1.1258, Loss 4.1143, Error 0.2281(best: 0.2173)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15900 | Time 21.8351(21.5183) | Bit/dim 3.5421(3.5387) | Xent 0.1010(0.0937) | Loss 3.5926(3.5855) | Error 0.0344(0.0330) Steps 898(885.31) | Grad Norm 1.4276(1.6164) | Total Time 14.00(14.00)\n",
      "Iter 15910 | Time 20.8297(21.3681) | Bit/dim 3.5540(3.5397) | Xent 0.1056(0.0941) | Loss 3.6067(3.5867) | Error 0.0300(0.0325) Steps 892(883.63) | Grad Norm 1.6071(1.6254) | Total Time 14.00(14.00)\n",
      "Iter 15920 | Time 21.1818(21.3038) | Bit/dim 3.5317(3.5410) | Xent 0.1122(0.0933) | Loss 3.5878(3.5877) | Error 0.0367(0.0319) Steps 886(883.85) | Grad Norm 1.6639(1.6992) | Total Time 14.00(14.00)\n",
      "Iter 15930 | Time 20.9030(21.2817) | Bit/dim 3.5395(3.5385) | Xent 0.0949(0.0945) | Loss 3.5869(3.5857) | Error 0.0333(0.0323) Steps 886(885.34) | Grad Norm 1.7793(1.6697) | Total Time 14.00(14.00)\n",
      "Iter 15940 | Time 21.3070(21.3159) | Bit/dim 3.5197(3.5396) | Xent 0.0941(0.0948) | Loss 3.5667(3.5870) | Error 0.0344(0.0326) Steps 868(884.37) | Grad Norm 3.0722(1.7185) | Total Time 14.00(14.00)\n",
      "Iter 15950 | Time 21.4414(21.3257) | Bit/dim 3.5247(3.5395) | Xent 0.1094(0.0953) | Loss 3.5794(3.5872) | Error 0.0422(0.0331) Steps 886(885.47) | Grad Norm 3.2784(1.7859) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0290 | Time 106.6047, Epoch Time 1295.2490(1269.5661), Bit/dim 3.5507(best: 3.5506), Xent 1.1015, Loss 4.1015, Error 0.2284(best: 0.2173)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15960 | Time 21.1461(21.3137) | Bit/dim 3.5201(3.5392) | Xent 0.1129(0.0968) | Loss 3.5766(3.5876) | Error 0.0456(0.0342) Steps 892(886.86) | Grad Norm 2.3058(1.9249) | Total Time 14.00(14.00)\n",
      "Iter 15970 | Time 21.6325(21.3142) | Bit/dim 3.5411(3.5437) | Xent 0.1013(0.0980) | Loss 3.5918(3.5926) | Error 0.0367(0.0347) Steps 904(887.57) | Grad Norm 2.8362(1.9742) | Total Time 14.00(14.00)\n",
      "Iter 15980 | Time 21.9669(21.3993) | Bit/dim 3.5208(3.5420) | Xent 0.0618(0.0962) | Loss 3.5517(3.5901) | Error 0.0144(0.0338) Steps 928(889.04) | Grad Norm 1.2277(1.9758) | Total Time 14.00(14.00)\n",
      "Iter 15990 | Time 21.2121(21.4427) | Bit/dim 3.5427(3.5406) | Xent 0.0914(0.0943) | Loss 3.5884(3.5878) | Error 0.0322(0.0334) Steps 886(890.04) | Grad Norm 1.4372(1.9059) | Total Time 14.00(14.00)\n",
      "Iter 16000 | Time 21.4604(21.5290) | Bit/dim 3.5665(3.5405) | Xent 0.0862(0.0937) | Loss 3.6096(3.5874) | Error 0.0322(0.0331) Steps 874(888.58) | Grad Norm 2.0986(1.9020) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0291 | Time 106.1783, Epoch Time 1306.2468(1270.6665), Bit/dim 3.5509(best: 3.5506), Xent 1.1210, Loss 4.1114, Error 0.2283(best: 0.2173)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16010 | Time 21.4831(21.4757) | Bit/dim 3.5167(3.5388) | Xent 0.0887(0.0951) | Loss 3.5610(3.5864) | Error 0.0333(0.0335) Steps 904(889.43) | Grad Norm 1.4809(1.8368) | Total Time 14.00(14.00)\n",
      "Iter 16020 | Time 21.2828(21.5242) | Bit/dim 3.5702(3.5397) | Xent 0.0912(0.0933) | Loss 3.6158(3.5864) | Error 0.0300(0.0326) Steps 910(891.90) | Grad Norm 1.5068(1.8447) | Total Time 14.00(14.00)\n",
      "Iter 16030 | Time 21.2717(21.5381) | Bit/dim 3.5455(3.5398) | Xent 0.0827(0.0932) | Loss 3.5868(3.5864) | Error 0.0278(0.0323) Steps 898(888.26) | Grad Norm 1.4751(1.8259) | Total Time 14.00(14.00)\n",
      "Iter 16040 | Time 21.7252(21.5364) | Bit/dim 3.5324(3.5381) | Xent 0.0748(0.0916) | Loss 3.5698(3.5839) | Error 0.0256(0.0315) Steps 880(886.20) | Grad Norm 1.5802(1.8937) | Total Time 14.00(14.00)\n",
      "Iter 16050 | Time 21.0807(21.4701) | Bit/dim 3.5512(3.5393) | Xent 0.0916(0.0932) | Loss 3.5970(3.5859) | Error 0.0267(0.0323) Steps 886(885.67) | Grad Norm 1.6185(1.8492) | Total Time 14.00(14.00)\n",
      "Iter 16060 | Time 21.5425(21.4477) | Bit/dim 3.5689(3.5410) | Xent 0.0917(0.0923) | Loss 3.6148(3.5871) | Error 0.0344(0.0324) Steps 874(885.64) | Grad Norm 1.4007(1.8102) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0292 | Time 107.5915, Epoch Time 1308.6478(1271.8059), Bit/dim 3.5516(best: 3.5506), Xent 1.1254, Loss 4.1143, Error 0.2279(best: 0.2173)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16070 | Time 21.5467(21.4408) | Bit/dim 3.5223(3.5399) | Xent 0.0812(0.0940) | Loss 3.5629(3.5869) | Error 0.0278(0.0333) Steps 886(886.50) | Grad Norm 1.8979(1.8680) | Total Time 14.00(14.00)\n",
      "Iter 16080 | Time 22.2844(21.5284) | Bit/dim 3.5017(3.5385) | Xent 0.1317(0.0932) | Loss 3.5676(3.5851) | Error 0.0456(0.0330) Steps 886(886.56) | Grad Norm 1.7646(1.8106) | Total Time 14.00(14.00)\n",
      "Iter 16090 | Time 20.5826(21.5000) | Bit/dim 3.5413(3.5386) | Xent 0.0922(0.0926) | Loss 3.5874(3.5849) | Error 0.0289(0.0327) Steps 868(886.12) | Grad Norm 1.2255(1.7302) | Total Time 14.00(14.00)\n",
      "Iter 16100 | Time 21.6649(21.5302) | Bit/dim 3.5301(3.5408) | Xent 0.1001(0.0937) | Loss 3.5801(3.5877) | Error 0.0378(0.0329) Steps 898(884.60) | Grad Norm 1.4199(1.7352) | Total Time 14.00(14.00)\n",
      "Iter 16110 | Time 21.3438(21.4926) | Bit/dim 3.5475(3.5425) | Xent 0.1009(0.0935) | Loss 3.5979(3.5892) | Error 0.0378(0.0328) Steps 886(886.26) | Grad Norm 2.6038(1.8262) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0293 | Time 105.5676, Epoch Time 1307.0873(1272.8644), Bit/dim 3.5500(best: 3.5506), Xent 1.1115, Loss 4.1057, Error 0.2293(best: 0.2173)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16120 | Time 21.9748(21.4640) | Bit/dim 3.5187(3.5437) | Xent 0.0855(0.0930) | Loss 3.5615(3.5901) | Error 0.0322(0.0325) Steps 868(884.76) | Grad Norm 2.2174(1.9281) | Total Time 14.00(14.00)\n",
      "Iter 16130 | Time 21.2592(21.5493) | Bit/dim 3.5299(3.5404) | Xent 0.0953(0.0921) | Loss 3.5775(3.5865) | Error 0.0367(0.0317) Steps 880(885.57) | Grad Norm 1.4823(1.8297) | Total Time 14.00(14.00)\n",
      "Iter 16140 | Time 20.5686(21.4459) | Bit/dim 3.5250(3.5427) | Xent 0.0966(0.0910) | Loss 3.5733(3.5882) | Error 0.0322(0.0310) Steps 880(885.28) | Grad Norm 1.7043(1.7793) | Total Time 14.00(14.00)\n",
      "Iter 16150 | Time 20.6423(21.4175) | Bit/dim 3.5377(3.5411) | Xent 0.0989(0.0907) | Loss 3.5871(3.5865) | Error 0.0367(0.0313) Steps 874(883.33) | Grad Norm 1.5492(1.7881) | Total Time 14.00(14.00)\n",
      "Iter 16160 | Time 21.8896(21.4095) | Bit/dim 3.5182(3.5410) | Xent 0.0937(0.0907) | Loss 3.5650(3.5864) | Error 0.0311(0.0316) Steps 886(884.13) | Grad Norm 1.4078(1.7577) | Total Time 14.00(14.00)\n",
      "Iter 16170 | Time 21.3600(21.4587) | Bit/dim 3.5400(3.5386) | Xent 0.0931(0.0891) | Loss 3.5866(3.5831) | Error 0.0289(0.0311) Steps 898(886.41) | Grad Norm 1.8293(1.6612) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0294 | Time 105.4639, Epoch Time 1304.7898(1273.8221), Bit/dim 3.5514(best: 3.5500), Xent 1.1538, Loss 4.1283, Error 0.2303(best: 0.2173)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16180 | Time 21.6911(21.4342) | Bit/dim 3.5656(3.5423) | Xent 0.1022(0.0898) | Loss 3.6167(3.5871) | Error 0.0400(0.0320) Steps 904(887.24) | Grad Norm 1.4416(1.6745) | Total Time 14.00(14.00)\n",
      "Iter 16190 | Time 21.7329(21.5215) | Bit/dim 3.5412(3.5420) | Xent 0.0795(0.0916) | Loss 3.5809(3.5878) | Error 0.0244(0.0324) Steps 898(885.45) | Grad Norm 2.4403(1.7414) | Total Time 14.00(14.00)\n",
      "Iter 16200 | Time 21.8874(21.5251) | Bit/dim 3.5764(3.5410) | Xent 0.1098(0.0921) | Loss 3.6313(3.5871) | Error 0.0311(0.0326) Steps 886(885.99) | Grad Norm 1.3684(1.7960) | Total Time 14.00(14.00)\n",
      "Iter 16210 | Time 20.9152(21.4773) | Bit/dim 3.5146(3.5396) | Xent 0.0757(0.0913) | Loss 3.5524(3.5853) | Error 0.0278(0.0325) Steps 904(888.43) | Grad Norm 1.8391(1.8202) | Total Time 14.00(14.00)\n",
      "Iter 16220 | Time 21.1758(21.4882) | Bit/dim 3.5177(3.5394) | Xent 0.1293(0.0947) | Loss 3.5824(3.5868) | Error 0.0433(0.0336) Steps 892(886.70) | Grad Norm 3.1840(1.8883) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0295 | Time 104.7625, Epoch Time 1305.8268(1274.7823), Bit/dim 3.5495(best: 3.5500), Xent 1.1044, Loss 4.1017, Error 0.2282(best: 0.2173)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16230 | Time 21.3320(21.4382) | Bit/dim 3.5431(3.5379) | Xent 0.0808(0.0956) | Loss 3.5835(3.5857) | Error 0.0267(0.0341) Steps 892(887.03) | Grad Norm 1.3342(1.9004) | Total Time 14.00(14.00)\n",
      "Iter 16240 | Time 21.4662(21.4981) | Bit/dim 3.5654(3.5387) | Xent 0.1034(0.0957) | Loss 3.6171(3.5865) | Error 0.0378(0.0338) Steps 928(887.34) | Grad Norm 3.2318(1.9732) | Total Time 14.00(14.00)\n",
      "Iter 16250 | Time 21.4533(21.4725) | Bit/dim 3.5544(3.5399) | Xent 0.0912(0.0922) | Loss 3.6000(3.5860) | Error 0.0311(0.0320) Steps 886(885.64) | Grad Norm 2.0925(1.9879) | Total Time 14.00(14.00)\n",
      "Iter 16260 | Time 21.1593(21.3897) | Bit/dim 3.5405(3.5374) | Xent 0.0985(0.0921) | Loss 3.5897(3.5834) | Error 0.0367(0.0322) Steps 880(885.24) | Grad Norm 2.1414(1.9220) | Total Time 14.00(14.00)\n",
      "Iter 16270 | Time 21.1986(21.3783) | Bit/dim 3.5342(3.5397) | Xent 0.0855(0.0929) | Loss 3.5770(3.5861) | Error 0.0267(0.0325) Steps 862(883.98) | Grad Norm 1.7351(1.8628) | Total Time 14.00(14.00)\n",
      "Iter 16280 | Time 22.1752(21.4168) | Bit/dim 3.5223(3.5390) | Xent 0.0971(0.0929) | Loss 3.5709(3.5854) | Error 0.0356(0.0322) Steps 886(884.64) | Grad Norm 1.7137(1.7536) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0296 | Time 105.1652, Epoch Time 1301.4334(1275.5818), Bit/dim 3.5486(best: 3.5495), Xent 1.1218, Loss 4.1094, Error 0.2286(best: 0.2173)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16290 | Time 21.5565(21.4034) | Bit/dim 3.5428(3.5344) | Xent 0.0801(0.0914) | Loss 3.5828(3.5801) | Error 0.0278(0.0318) Steps 904(884.10) | Grad Norm 1.6628(1.7363) | Total Time 14.00(14.00)\n",
      "Iter 16300 | Time 21.3282(21.2851) | Bit/dim 3.5466(3.5392) | Xent 0.0852(0.0901) | Loss 3.5892(3.5842) | Error 0.0322(0.0320) Steps 868(883.50) | Grad Norm 1.5102(1.7950) | Total Time 14.00(14.00)\n",
      "Iter 16310 | Time 21.1102(21.3252) | Bit/dim 3.5412(3.5392) | Xent 0.0820(0.0894) | Loss 3.5822(3.5839) | Error 0.0333(0.0319) Steps 898(887.86) | Grad Norm 1.7919(1.8127) | Total Time 14.00(14.00)\n",
      "Iter 16320 | Time 21.4609(21.3802) | Bit/dim 3.5414(3.5382) | Xent 0.0911(0.0895) | Loss 3.5869(3.5830) | Error 0.0267(0.0317) Steps 850(886.41) | Grad Norm 2.0792(1.8246) | Total Time 14.00(14.00)\n",
      "Iter 16330 | Time 21.8054(21.4184) | Bit/dim 3.5426(3.5391) | Xent 0.0811(0.0899) | Loss 3.5831(3.5840) | Error 0.0244(0.0313) Steps 886(885.89) | Grad Norm 1.3567(1.7440) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0297 | Time 104.9741, Epoch Time 1298.2102(1276.2606), Bit/dim 3.5492(best: 3.5486), Xent 1.1063, Loss 4.1024, Error 0.2275(best: 0.2173)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16340 | Time 22.0993(21.4318) | Bit/dim 3.5092(3.5379) | Xent 0.0816(0.0898) | Loss 3.5500(3.5828) | Error 0.0267(0.0313) Steps 898(887.28) | Grad Norm 1.3209(1.7046) | Total Time 14.00(14.00)\n",
      "Iter 16350 | Time 20.9389(21.5030) | Bit/dim 3.5254(3.5387) | Xent 0.1041(0.0902) | Loss 3.5775(3.5838) | Error 0.0389(0.0316) Steps 874(886.27) | Grad Norm 3.1616(1.7405) | Total Time 14.00(14.00)\n",
      "Iter 16360 | Time 20.9282(21.5316) | Bit/dim 3.5589(3.5408) | Xent 0.0952(0.0901) | Loss 3.6065(3.5859) | Error 0.0367(0.0321) Steps 868(886.03) | Grad Norm 1.6339(1.8530) | Total Time 14.00(14.00)\n",
      "Iter 16370 | Time 21.3416(21.4545) | Bit/dim 3.5331(3.5404) | Xent 0.0842(0.0911) | Loss 3.5752(3.5860) | Error 0.0289(0.0326) Steps 874(884.67) | Grad Norm 2.1199(2.0581) | Total Time 14.00(14.00)\n",
      "Iter 16380 | Time 21.7083(21.4369) | Bit/dim 3.5235(3.5389) | Xent 0.0846(0.0926) | Loss 3.5658(3.5852) | Error 0.0300(0.0332) Steps 910(886.84) | Grad Norm 1.5654(2.0205) | Total Time 14.00(14.00)\n",
      "Iter 16390 | Time 21.3633(21.3847) | Bit/dim 3.5252(3.5398) | Xent 0.0927(0.0929) | Loss 3.5715(3.5863) | Error 0.0333(0.0329) Steps 892(886.27) | Grad Norm 1.7020(1.9512) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0298 | Time 104.6766, Epoch Time 1303.6410(1277.0821), Bit/dim 3.5495(best: 3.5486), Xent 1.1223, Loss 4.1106, Error 0.2268(best: 0.2173)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16400 | Time 21.5605(21.4127) | Bit/dim 3.5406(3.5395) | Xent 0.0795(0.0910) | Loss 3.5804(3.5850) | Error 0.0311(0.0322) Steps 880(886.74) | Grad Norm 1.5089(1.8684) | Total Time 14.00(14.00)\n",
      "Iter 16410 | Time 21.4637(21.3948) | Bit/dim 3.5929(3.5414) | Xent 0.0905(0.0907) | Loss 3.6382(3.5867) | Error 0.0411(0.0320) Steps 880(886.65) | Grad Norm 2.2491(1.8792) | Total Time 14.00(14.00)\n",
      "Iter 16420 | Time 21.5579(21.4389) | Bit/dim 3.5665(3.5400) | Xent 0.0972(0.0911) | Loss 3.6151(3.5856) | Error 0.0344(0.0327) Steps 880(885.25) | Grad Norm 2.1609(1.9530) | Total Time 14.00(14.00)\n",
      "Iter 16430 | Time 20.9905(21.4482) | Bit/dim 3.5485(3.5392) | Xent 0.0960(0.0919) | Loss 3.5964(3.5851) | Error 0.0400(0.0329) Steps 892(887.57) | Grad Norm 2.3218(2.0211) | Total Time 14.00(14.00)\n",
      "Iter 16440 | Time 21.2506(21.4254) | Bit/dim 3.5336(3.5378) | Xent 0.0906(0.0920) | Loss 3.5789(3.5838) | Error 0.0256(0.0326) Steps 892(887.51) | Grad Norm 1.3301(1.9649) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0299 | Time 105.0247, Epoch Time 1305.0111(1277.9199), Bit/dim 3.5483(best: 3.5486), Xent 1.1439, Loss 4.1202, Error 0.2286(best: 0.2173)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16450 | Time 21.3145(21.4688) | Bit/dim 3.5388(3.5360) | Xent 0.0739(0.0897) | Loss 3.5758(3.5808) | Error 0.0167(0.0312) Steps 886(888.98) | Grad Norm 1.1550(1.8986) | Total Time 14.00(14.00)\n",
      "Iter 16460 | Time 21.3401(21.4456) | Bit/dim 3.4863(3.5341) | Xent 0.0888(0.0890) | Loss 3.5307(3.5785) | Error 0.0311(0.0312) Steps 886(890.89) | Grad Norm 1.6203(1.8205) | Total Time 14.00(14.00)\n",
      "Iter 16470 | Time 22.0179(21.4185) | Bit/dim 3.5184(3.5336) | Xent 0.0900(0.0895) | Loss 3.5634(3.5784) | Error 0.0256(0.0309) Steps 898(889.57) | Grad Norm 1.7808(1.7707) | Total Time 14.00(14.00)\n",
      "Iter 16480 | Time 21.2506(21.2892) | Bit/dim 3.5269(3.5366) | Xent 0.0886(0.0892) | Loss 3.5712(3.5812) | Error 0.0289(0.0306) Steps 904(888.87) | Grad Norm 1.5493(1.6992) | Total Time 14.00(14.00)\n",
      "Iter 16490 | Time 20.6714(21.2786) | Bit/dim 3.5533(3.5413) | Xent 0.0798(0.0891) | Loss 3.5932(3.5858) | Error 0.0300(0.0306) Steps 892(889.13) | Grad Norm 1.6498(1.6559) | Total Time 14.00(14.00)\n",
      "Iter 16500 | Time 22.1745(21.3594) | Bit/dim 3.5544(3.5409) | Xent 0.0652(0.0881) | Loss 3.5870(3.5850) | Error 0.0244(0.0305) Steps 910(890.00) | Grad Norm 1.0862(1.6255) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0300 | Time 104.8717, Epoch Time 1295.8919(1278.4591), Bit/dim 3.5483(best: 3.5483), Xent 1.1484, Loss 4.1225, Error 0.2296(best: 0.2173)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16510 | Time 21.9631(21.3847) | Bit/dim 3.5328(3.5405) | Xent 0.0838(0.0875) | Loss 3.5747(3.5842) | Error 0.0267(0.0303) Steps 898(892.29) | Grad Norm 1.7707(1.6280) | Total Time 14.00(14.00)\n",
      "Iter 16520 | Time 21.9390(21.4738) | Bit/dim 3.5197(3.5406) | Xent 0.1042(0.0892) | Loss 3.5718(3.5852) | Error 0.0311(0.0311) Steps 898(894.96) | Grad Norm 1.7855(1.7382) | Total Time 14.00(14.00)\n",
      "Iter 16530 | Time 21.0501(21.5631) | Bit/dim 3.5627(3.5365) | Xent 0.0879(0.0888) | Loss 3.6067(3.5809) | Error 0.0344(0.0315) Steps 880(892.90) | Grad Norm 2.2101(1.8223) | Total Time 14.00(14.00)\n",
      "Iter 16540 | Time 21.2122(21.6076) | Bit/dim 3.5472(3.5372) | Xent 0.1113(0.0907) | Loss 3.6028(3.5826) | Error 0.0322(0.0322) Steps 886(894.34) | Grad Norm 1.3943(1.8070) | Total Time 14.00(14.00)\n",
      "Iter 16550 | Time 21.4790(21.6121) | Bit/dim 3.5249(3.5383) | Xent 0.0863(0.0910) | Loss 3.5681(3.5838) | Error 0.0211(0.0319) Steps 892(891.25) | Grad Norm 1.6837(1.8122) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0301 | Time 105.6587, Epoch Time 1316.7683(1279.6084), Bit/dim 3.5490(best: 3.5483), Xent 1.1467, Loss 4.1224, Error 0.2301(best: 0.2173)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16560 | Time 21.2637(21.5833) | Bit/dim 3.5186(3.5369) | Xent 0.0946(0.0916) | Loss 3.5659(3.5827) | Error 0.0300(0.0320) Steps 892(892.95) | Grad Norm 1.6394(1.7742) | Total Time 14.00(14.00)\n",
      "Iter 16570 | Time 21.8119(21.6124) | Bit/dim 3.5528(3.5369) | Xent 0.0764(0.0913) | Loss 3.5910(3.5825) | Error 0.0256(0.0314) Steps 868(891.82) | Grad Norm 1.7940(1.8083) | Total Time 14.00(14.00)\n",
      "Iter 16580 | Time 21.2212(21.6727) | Bit/dim 3.5019(3.5369) | Xent 0.0849(0.0917) | Loss 3.5444(3.5828) | Error 0.0289(0.0317) Steps 904(892.43) | Grad Norm 2.2141(1.8855) | Total Time 14.00(14.00)\n",
      "Iter 16590 | Time 22.0314(21.6448) | Bit/dim 3.5367(3.5386) | Xent 0.0720(0.0923) | Loss 3.5727(3.5848) | Error 0.0244(0.0318) Steps 922(892.34) | Grad Norm 1.7791(1.8532) | Total Time 14.00(14.00)\n",
      "Iter 16600 | Time 21.9445(21.6062) | Bit/dim 3.5237(3.5385) | Xent 0.0905(0.0933) | Loss 3.5690(3.5852) | Error 0.0256(0.0323) Steps 904(891.68) | Grad Norm 2.0602(1.8302) | Total Time 14.00(14.00)\n",
      "Iter 16610 | Time 22.1712(21.6720) | Bit/dim 3.5301(3.5410) | Xent 0.0832(0.0923) | Loss 3.5717(3.5872) | Error 0.0267(0.0318) Steps 904(892.75) | Grad Norm 1.3520(1.7694) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0302 | Time 106.6379, Epoch Time 1316.3049(1280.7093), Bit/dim 3.5481(best: 3.5483), Xent 1.1403, Loss 4.1182, Error 0.2285(best: 0.2173)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16620 | Time 22.2345(21.7372) | Bit/dim 3.5406(3.5414) | Xent 0.1103(0.0903) | Loss 3.5957(3.5865) | Error 0.0400(0.0313) Steps 904(893.67) | Grad Norm 1.8124(1.7564) | Total Time 14.00(14.00)\n",
      "Iter 16630 | Time 21.4367(21.7604) | Bit/dim 3.5389(3.5389) | Xent 0.0764(0.0893) | Loss 3.5771(3.5836) | Error 0.0256(0.0310) Steps 910(893.59) | Grad Norm 1.4899(1.8112) | Total Time 14.00(14.00)\n",
      "Iter 16640 | Time 22.1507(21.7837) | Bit/dim 3.5311(3.5385) | Xent 0.0882(0.0892) | Loss 3.5752(3.5831) | Error 0.0344(0.0313) Steps 886(895.53) | Grad Norm 1.5255(1.8368) | Total Time 14.00(14.00)\n",
      "Iter 16650 | Time 21.9169(21.8133) | Bit/dim 3.5388(3.5390) | Xent 0.0758(0.0887) | Loss 3.5767(3.5833) | Error 0.0278(0.0315) Steps 898(893.91) | Grad Norm 1.2321(1.7964) | Total Time 14.00(14.00)\n",
      "Iter 16660 | Time 21.3415(21.7545) | Bit/dim 3.5472(3.5392) | Xent 0.0898(0.0886) | Loss 3.5921(3.5835) | Error 0.0333(0.0316) Steps 898(893.72) | Grad Norm 2.7337(1.8551) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0303 | Time 105.6957, Epoch Time 1323.4519(1281.9915), Bit/dim 3.5495(best: 3.5481), Xent 1.1567, Loss 4.1278, Error 0.2273(best: 0.2173)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16670 | Time 22.0702(21.7356) | Bit/dim 3.5552(3.5376) | Xent 0.1062(0.0894) | Loss 3.6083(3.5823) | Error 0.0356(0.0316) Steps 880(891.96) | Grad Norm 1.7813(1.9049) | Total Time 14.00(14.00)\n",
      "Iter 16680 | Time 21.3251(21.7160) | Bit/dim 3.5464(3.5406) | Xent 0.0918(0.0878) | Loss 3.5923(3.5845) | Error 0.0344(0.0308) Steps 886(892.86) | Grad Norm 2.0949(1.8854) | Total Time 14.00(14.00)\n",
      "Iter 16690 | Time 21.7871(21.7170) | Bit/dim 3.5143(3.5412) | Xent 0.0841(0.0871) | Loss 3.5564(3.5848) | Error 0.0278(0.0309) Steps 892(893.30) | Grad Norm 1.8804(1.8155) | Total Time 14.00(14.00)\n",
      "Iter 16700 | Time 21.4242(21.7360) | Bit/dim 3.5166(3.5423) | Xent 0.0899(0.0866) | Loss 3.5615(3.5856) | Error 0.0289(0.0303) Steps 892(894.08) | Grad Norm 1.7073(1.7732) | Total Time 14.00(14.00)\n",
      "Iter 16710 | Time 21.9212(21.8194) | Bit/dim 3.5498(3.5381) | Xent 0.0994(0.0877) | Loss 3.5996(3.5820) | Error 0.0289(0.0303) Steps 898(897.57) | Grad Norm 1.3967(1.7535) | Total Time 14.00(14.00)\n",
      "Iter 16720 | Time 21.6860(21.7694) | Bit/dim 3.5358(3.5370) | Xent 0.0994(0.0887) | Loss 3.5855(3.5813) | Error 0.0344(0.0307) Steps 916(898.12) | Grad Norm 1.7952(1.7601) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0304 | Time 106.3206, Epoch Time 1322.7897(1283.2155), Bit/dim 3.5479(best: 3.5481), Xent 1.1364, Loss 4.1161, Error 0.2274(best: 0.2173)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16730 | Time 21.9690(21.7567) | Bit/dim 3.5146(3.5349) | Xent 0.0890(0.0892) | Loss 3.5591(3.5795) | Error 0.0356(0.0307) Steps 916(897.74) | Grad Norm 2.2343(1.8826) | Total Time 14.00(14.00)\n",
      "Iter 16740 | Time 22.0663(21.7304) | Bit/dim 3.5107(3.5352) | Xent 0.1018(0.0883) | Loss 3.5616(3.5794) | Error 0.0422(0.0310) Steps 904(898.18) | Grad Norm 2.5989(1.9971) | Total Time 14.00(14.00)\n",
      "Iter 16750 | Time 22.5383(21.7447) | Bit/dim 3.5365(3.5360) | Xent 0.0974(0.0897) | Loss 3.5853(3.5808) | Error 0.0367(0.0317) Steps 916(896.04) | Grad Norm 1.6669(1.9848) | Total Time 14.00(14.00)\n",
      "Iter 16760 | Time 21.4643(21.7589) | Bit/dim 3.5729(3.5410) | Xent 0.0856(0.0895) | Loss 3.6157(3.5857) | Error 0.0233(0.0315) Steps 868(897.88) | Grad Norm 2.1896(1.9618) | Total Time 14.00(14.00)\n",
      "Iter 16770 | Time 22.6415(21.7786) | Bit/dim 3.5381(3.5394) | Xent 0.0708(0.0874) | Loss 3.5735(3.5831) | Error 0.0267(0.0307) Steps 916(899.00) | Grad Norm 1.2563(1.8661) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0305 | Time 104.8842, Epoch Time 1319.4218(1284.3017), Bit/dim 3.5480(best: 3.5479), Xent 1.1527, Loss 4.1243, Error 0.2273(best: 0.2173)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16780 | Time 21.9273(21.7559) | Bit/dim 3.5709(3.5399) | Xent 0.1043(0.0887) | Loss 3.6231(3.5843) | Error 0.0400(0.0310) Steps 904(898.57) | Grad Norm 1.8256(1.8509) | Total Time 14.00(14.00)\n",
      "Iter 16790 | Time 21.9930(21.7909) | Bit/dim 3.5394(3.5394) | Xent 0.0904(0.0883) | Loss 3.5846(3.5836) | Error 0.0344(0.0311) Steps 886(897.52) | Grad Norm 2.5838(1.8967) | Total Time 14.00(14.00)\n",
      "Iter 16800 | Time 21.9582(21.7325) | Bit/dim 3.5680(3.5404) | Xent 0.0761(0.0867) | Loss 3.6061(3.5838) | Error 0.0289(0.0307) Steps 898(893.78) | Grad Norm 1.3026(1.8527) | Total Time 14.00(14.00)\n",
      "Iter 16810 | Time 22.3924(21.8498) | Bit/dim 3.5125(3.5384) | Xent 0.0909(0.0875) | Loss 3.5579(3.5822) | Error 0.0267(0.0303) Steps 856(893.91) | Grad Norm 2.4790(1.8482) | Total Time 14.00(14.00)\n",
      "Iter 16820 | Time 21.1434(21.8064) | Bit/dim 3.5427(3.5373) | Xent 0.1015(0.0885) | Loss 3.5934(3.5816) | Error 0.0367(0.0305) Steps 874(892.80) | Grad Norm 1.9008(1.8695) | Total Time 14.00(14.00)\n",
      "Iter 16830 | Time 22.0270(21.8197) | Bit/dim 3.5526(3.5387) | Xent 0.0830(0.0888) | Loss 3.5941(3.5831) | Error 0.0289(0.0309) Steps 892(894.44) | Grad Norm 1.6682(1.8535) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0306 | Time 106.0096, Epoch Time 1325.7730(1285.5458), Bit/dim 3.5492(best: 3.5479), Xent 1.1655, Loss 4.1319, Error 0.2315(best: 0.2173)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16840 | Time 21.7565(21.7725) | Bit/dim 3.4810(3.5370) | Xent 0.0868(0.0883) | Loss 3.5244(3.5812) | Error 0.0311(0.0303) Steps 892(893.81) | Grad Norm 1.7933(1.8546) | Total Time 14.00(14.00)\n",
      "Iter 16850 | Time 21.5504(21.7513) | Bit/dim 3.5372(3.5364) | Xent 0.0911(0.0869) | Loss 3.5827(3.5799) | Error 0.0289(0.0300) Steps 892(893.81) | Grad Norm 1.5506(1.8548) | Total Time 14.00(14.00)\n",
      "Iter 16860 | Time 22.2817(21.7696) | Bit/dim 3.5595(3.5385) | Xent 0.1011(0.0883) | Loss 3.6100(3.5826) | Error 0.0278(0.0303) Steps 880(893.93) | Grad Norm 2.8013(1.9092) | Total Time 14.00(14.00)\n",
      "Iter 16870 | Time 21.6417(21.7874) | Bit/dim 3.5781(3.5395) | Xent 0.0863(0.0865) | Loss 3.6213(3.5827) | Error 0.0333(0.0298) Steps 898(894.67) | Grad Norm 2.0259(1.8719) | Total Time 14.00(14.00)\n",
      "Iter 16880 | Time 21.2328(21.8261) | Bit/dim 3.5134(3.5371) | Xent 0.1066(0.0888) | Loss 3.5667(3.5816) | Error 0.0367(0.0309) Steps 880(896.88) | Grad Norm 1.6179(1.8653) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0307 | Time 105.7219, Epoch Time 1323.0715(1286.6716), Bit/dim 3.5472(best: 3.5479), Xent 1.1415, Loss 4.1179, Error 0.2285(best: 0.2173)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16890 | Time 21.8765(21.7974) | Bit/dim 3.5433(3.5400) | Xent 0.0866(0.0886) | Loss 3.5866(3.5843) | Error 0.0278(0.0305) Steps 910(898.62) | Grad Norm 1.5855(1.8159) | Total Time 14.00(14.00)\n",
      "Iter 16900 | Time 22.1021(21.8440) | Bit/dim 3.5408(3.5431) | Xent 0.0923(0.0861) | Loss 3.5870(3.5861) | Error 0.0333(0.0295) Steps 886(899.28) | Grad Norm 2.7482(1.8210) | Total Time 14.00(14.00)\n",
      "Iter 16910 | Time 21.1373(21.7995) | Bit/dim 3.5457(3.5407) | Xent 0.0674(0.0855) | Loss 3.5794(3.5834) | Error 0.0233(0.0293) Steps 892(899.70) | Grad Norm 1.7630(1.8434) | Total Time 14.00(14.00)\n",
      "Iter 16920 | Time 21.3638(21.8391) | Bit/dim 3.4816(3.5383) | Xent 0.1049(0.0862) | Loss 3.5341(3.5814) | Error 0.0389(0.0296) Steps 898(900.22) | Grad Norm 2.8073(1.8433) | Total Time 14.00(14.00)\n",
      "Iter 16930 | Time 21.5562(21.8735) | Bit/dim 3.5033(3.5373) | Xent 0.0850(0.0858) | Loss 3.5458(3.5802) | Error 0.0256(0.0296) Steps 892(898.52) | Grad Norm 2.4643(1.8494) | Total Time 14.00(14.00)\n",
      "Iter 16940 | Time 22.0171(21.8556) | Bit/dim 3.5289(3.5360) | Xent 0.0794(0.0860) | Loss 3.5686(3.5790) | Error 0.0311(0.0297) Steps 898(896.76) | Grad Norm 1.1852(1.8095) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0308 | Time 103.8585, Epoch Time 1325.0834(1287.8239), Bit/dim 3.5502(best: 3.5472), Xent 1.1912, Loss 4.1458, Error 0.2319(best: 0.2173)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16950 | Time 22.4667(21.8922) | Bit/dim 3.5289(3.5360) | Xent 0.0788(0.0854) | Loss 3.5683(3.5787) | Error 0.0278(0.0299) Steps 898(897.19) | Grad Norm 1.1763(1.7660) | Total Time 14.00(14.00)\n",
      "Iter 16960 | Time 22.6447(21.9367) | Bit/dim 3.5484(3.5362) | Xent 0.0995(0.0860) | Loss 3.5982(3.5792) | Error 0.0389(0.0302) Steps 892(896.53) | Grad Norm 1.5024(1.7841) | Total Time 14.00(14.00)\n",
      "Iter 16970 | Time 22.0954(21.9404) | Bit/dim 3.5141(3.5380) | Xent 0.0786(0.0858) | Loss 3.5534(3.5809) | Error 0.0300(0.0300) Steps 880(896.49) | Grad Norm 1.6618(1.7869) | Total Time 14.00(14.00)\n",
      "Iter 16980 | Time 21.7908(21.9057) | Bit/dim 3.4961(3.5363) | Xent 0.1003(0.0863) | Loss 3.5462(3.5795) | Error 0.0322(0.0301) Steps 928(899.37) | Grad Norm 2.3930(1.8535) | Total Time 14.00(14.00)\n",
      "Iter 16990 | Time 22.9223(21.9350) | Bit/dim 3.5231(3.5367) | Xent 0.0824(0.0867) | Loss 3.5643(3.5801) | Error 0.0333(0.0303) Steps 910(900.11) | Grad Norm 2.5604(1.8903) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0309 | Time 104.5199, Epoch Time 1331.8921(1289.1460), Bit/dim 3.5496(best: 3.5472), Xent 1.1667, Loss 4.1329, Error 0.2282(best: 0.2173)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17000 | Time 22.0166(21.9355) | Bit/dim 3.5386(3.5389) | Xent 0.0931(0.0882) | Loss 3.5852(3.5830) | Error 0.0267(0.0305) Steps 892(898.32) | Grad Norm 1.5656(1.8924) | Total Time 14.00(14.00)\n",
      "Iter 17010 | Time 21.4198(21.8774) | Bit/dim 3.5359(3.5382) | Xent 0.0923(0.0861) | Loss 3.5821(3.5813) | Error 0.0322(0.0297) Steps 898(899.04) | Grad Norm 2.0145(1.8390) | Total Time 14.00(14.00)\n",
      "Iter 17020 | Time 22.5582(21.8462) | Bit/dim 3.5692(3.5365) | Xent 0.0723(0.0855) | Loss 3.6053(3.5793) | Error 0.0200(0.0291) Steps 880(897.53) | Grad Norm 1.3381(1.7946) | Total Time 14.00(14.00)\n",
      "Iter 17030 | Time 21.8085(21.8230) | Bit/dim 3.5146(3.5369) | Xent 0.0790(0.0854) | Loss 3.5540(3.5796) | Error 0.0267(0.0298) Steps 910(899.04) | Grad Norm 1.5774(1.7260) | Total Time 14.00(14.00)\n",
      "Iter 17040 | Time 21.5525(21.8257) | Bit/dim 3.5528(3.5370) | Xent 0.0711(0.0856) | Loss 3.5883(3.5798) | Error 0.0256(0.0300) Steps 898(900.01) | Grad Norm 1.2784(1.7671) | Total Time 14.00(14.00)\n",
      "Iter 17050 | Time 21.5910(21.8285) | Bit/dim 3.5470(3.5382) | Xent 0.0994(0.0856) | Loss 3.5967(3.5811) | Error 0.0367(0.0301) Steps 904(901.93) | Grad Norm 1.5107(1.7232) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0310 | Time 104.4957, Epoch Time 1321.5534(1290.1182), Bit/dim 3.5462(best: 3.5472), Xent 1.1635, Loss 4.1279, Error 0.2299(best: 0.2173)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17060 | Time 21.7584(21.8411) | Bit/dim 3.5203(3.5390) | Xent 0.0962(0.0848) | Loss 3.5684(3.5814) | Error 0.0378(0.0298) Steps 916(902.96) | Grad Norm 3.0466(1.7930) | Total Time 14.00(14.00)\n",
      "Iter 17070 | Time 21.7588(21.8799) | Bit/dim 3.5639(3.5399) | Xent 0.0823(0.0871) | Loss 3.6050(3.5835) | Error 0.0311(0.0306) Steps 934(904.30) | Grad Norm 2.0007(1.9977) | Total Time 14.00(14.00)\n",
      "Iter 17080 | Time 21.6304(21.8679) | Bit/dim 3.5217(3.5394) | Xent 0.0929(0.0882) | Loss 3.5682(3.5835) | Error 0.0322(0.0310) Steps 910(904.11) | Grad Norm 2.2601(2.1360) | Total Time 14.00(14.00)\n",
      "Iter 17090 | Time 22.2865(21.9594) | Bit/dim 3.5594(3.5384) | Xent 0.0850(0.0884) | Loss 3.6019(3.5826) | Error 0.0267(0.0308) Steps 892(903.85) | Grad Norm 2.3282(2.0585) | Total Time 14.00(14.00)\n",
      "Iter 17100 | Time 22.3366(21.9608) | Bit/dim 3.5094(3.5367) | Xent 0.0929(0.0899) | Loss 3.5558(3.5816) | Error 0.0344(0.0313) Steps 916(905.41) | Grad Norm 1.9093(2.0650) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0311 | Time 104.7896, Epoch Time 1331.9913(1291.3744), Bit/dim 3.5484(best: 3.5462), Xent 1.1792, Loss 4.1380, Error 0.2282(best: 0.2173)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17110 | Time 22.2664(21.9482) | Bit/dim 3.5189(3.5362) | Xent 0.0837(0.0895) | Loss 3.5607(3.5809) | Error 0.0200(0.0313) Steps 922(907.22) | Grad Norm 1.6275(2.0050) | Total Time 14.00(14.00)\n",
      "Iter 17120 | Time 22.4989(21.9721) | Bit/dim 3.5388(3.5367) | Xent 0.0873(0.0877) | Loss 3.5825(3.5805) | Error 0.0322(0.0303) Steps 904(905.46) | Grad Norm 1.8044(1.9043) | Total Time 14.00(14.00)\n",
      "Iter 17130 | Time 22.0776(22.0623) | Bit/dim 3.5346(3.5373) | Xent 0.0924(0.0873) | Loss 3.5808(3.5809) | Error 0.0322(0.0306) Steps 868(907.07) | Grad Norm 1.4883(1.9038) | Total Time 14.00(14.00)\n",
      "Iter 17140 | Time 21.4006(22.0370) | Bit/dim 3.5272(3.5355) | Xent 0.0930(0.0881) | Loss 3.5737(3.5795) | Error 0.0389(0.0315) Steps 916(907.05) | Grad Norm 2.8119(2.0130) | Total Time 14.00(14.00)\n",
      "Iter 17150 | Time 21.4338(21.9963) | Bit/dim 3.5489(3.5379) | Xent 0.0750(0.0880) | Loss 3.5864(3.5818) | Error 0.0267(0.0314) Steps 880(901.88) | Grad Norm 1.4196(2.0133) | Total Time 14.00(14.00)\n",
      "Iter 17160 | Time 22.2473(22.0229) | Bit/dim 3.5505(3.5391) | Xent 0.0986(0.0908) | Loss 3.5998(3.5845) | Error 0.0411(0.0330) Steps 898(900.68) | Grad Norm 2.2465(2.0640) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0312 | Time 104.5551, Epoch Time 1336.3767(1292.7245), Bit/dim 3.5477(best: 3.5462), Xent 1.1887, Loss 4.1421, Error 0.2298(best: 0.2173)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17170 | Time 21.8880(21.9975) | Bit/dim 3.5361(3.5371) | Xent 0.0668(0.0888) | Loss 3.5695(3.5815) | Error 0.0200(0.0318) Steps 892(898.25) | Grad Norm 1.5164(1.9955) | Total Time 14.00(14.00)\n",
      "Iter 17180 | Time 22.0538(21.9446) | Bit/dim 3.5084(3.5354) | Xent 0.0980(0.0887) | Loss 3.5574(3.5797) | Error 0.0278(0.0321) Steps 898(899.07) | Grad Norm 1.8308(1.9516) | Total Time 14.00(14.00)\n",
      "Iter 17190 | Time 21.4725(21.8902) | Bit/dim 3.5698(3.5391) | Xent 0.1180(0.0888) | Loss 3.6288(3.5835) | Error 0.0422(0.0321) Steps 910(900.70) | Grad Norm 1.9562(1.9130) | Total Time 14.00(14.00)\n",
      "Iter 17200 | Time 21.7810(21.8221) | Bit/dim 3.5630(3.5390) | Xent 0.0822(0.0866) | Loss 3.6041(3.5823) | Error 0.0278(0.0312) Steps 910(901.72) | Grad Norm 1.4090(1.9537) | Total Time 14.00(14.00)\n",
      "Iter 17210 | Time 21.7538(21.7814) | Bit/dim 3.5413(3.5380) | Xent 0.0932(0.0896) | Loss 3.5879(3.5828) | Error 0.0367(0.0323) Steps 904(902.25) | Grad Norm 1.9249(2.0939) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0313 | Time 105.3158, Epoch Time 1320.4915(1293.5575), Bit/dim 3.5476(best: 3.5462), Xent 1.1696, Loss 4.1324, Error 0.2291(best: 0.2173)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17220 | Time 22.1332(21.8154) | Bit/dim 3.5123(3.5374) | Xent 0.0817(0.0909) | Loss 3.5531(3.5828) | Error 0.0267(0.0328) Steps 892(902.79) | Grad Norm 1.6457(2.0715) | Total Time 14.00(14.00)\n",
      "Iter 17230 | Time 21.9484(21.8663) | Bit/dim 3.5403(3.5366) | Xent 0.0826(0.0888) | Loss 3.5816(3.5810) | Error 0.0311(0.0322) Steps 892(901.53) | Grad Norm 1.7600(1.9976) | Total Time 14.00(14.00)\n",
      "Iter 17240 | Time 22.2800(21.8731) | Bit/dim 3.5555(3.5382) | Xent 0.0701(0.0868) | Loss 3.5906(3.5816) | Error 0.0256(0.0308) Steps 898(900.90) | Grad Norm 1.8213(1.9590) | Total Time 14.00(14.00)\n",
      "Iter 17250 | Time 22.1760(21.9099) | Bit/dim 3.5305(3.5390) | Xent 0.0966(0.0859) | Loss 3.5788(3.5819) | Error 0.0356(0.0307) Steps 880(901.72) | Grad Norm 2.2176(1.9754) | Total Time 14.00(14.00)\n",
      "Iter 17260 | Time 21.9773(21.9274) | Bit/dim 3.5311(3.5356) | Xent 0.1016(0.0870) | Loss 3.5820(3.5791) | Error 0.0378(0.0302) Steps 874(899.00) | Grad Norm 2.0713(2.0405) | Total Time 14.00(14.00)\n",
      "Iter 17270 | Time 22.2456(21.9135) | Bit/dim 3.5454(3.5379) | Xent 0.0705(0.0881) | Loss 3.5807(3.5819) | Error 0.0178(0.0309) Steps 910(899.72) | Grad Norm 1.5437(2.0169) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0314 | Time 106.5505, Epoch Time 1333.7672(1294.7638), Bit/dim 3.5473(best: 3.5462), Xent 1.1569, Loss 4.1257, Error 0.2302(best: 0.2173)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17280 | Time 21.1342(21.9281) | Bit/dim 3.5693(3.5391) | Xent 0.0920(0.0872) | Loss 3.6152(3.5827) | Error 0.0356(0.0308) Steps 880(899.27) | Grad Norm 1.8943(1.9847) | Total Time 14.00(14.00)\n",
      "Iter 17290 | Time 22.2117(21.8649) | Bit/dim 3.5555(3.5398) | Xent 0.0739(0.0863) | Loss 3.5925(3.5829) | Error 0.0256(0.0303) Steps 886(900.03) | Grad Norm 1.6092(1.9841) | Total Time 14.00(14.00)\n",
      "Iter 17300 | Time 22.0230(21.9567) | Bit/dim 3.5546(3.5366) | Xent 0.0865(0.0875) | Loss 3.5979(3.5803) | Error 0.0378(0.0310) Steps 934(899.81) | Grad Norm 2.1824(2.0227) | Total Time 14.00(14.00)\n",
      "Iter 17310 | Time 22.3336(21.9428) | Bit/dim 3.5337(3.5361) | Xent 0.0826(0.0865) | Loss 3.5751(3.5793) | Error 0.0322(0.0307) Steps 910(899.97) | Grad Norm 3.8153(2.1858) | Total Time 14.00(14.00)\n",
      "Iter 17320 | Time 22.1801(21.9038) | Bit/dim 3.5385(3.5386) | Xent 0.0793(0.0856) | Loss 3.5781(3.5814) | Error 0.0244(0.0302) Steps 928(901.24) | Grad Norm 1.6766(2.2297) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0315 | Time 106.0652, Epoch Time 1329.8634(1295.8168), Bit/dim 3.5474(best: 3.5462), Xent 1.1866, Loss 4.1407, Error 0.2297(best: 0.2173)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17330 | Time 21.6343(21.8841) | Bit/dim 3.5393(3.5371) | Xent 0.0865(0.0841) | Loss 3.5826(3.5791) | Error 0.0289(0.0296) Steps 922(901.80) | Grad Norm 1.4202(2.1507) | Total Time 14.00(14.00)\n",
      "Iter 17340 | Time 21.6856(21.8071) | Bit/dim 3.5069(3.5353) | Xent 0.0624(0.0832) | Loss 3.5381(3.5769) | Error 0.0244(0.0296) Steps 916(901.54) | Grad Norm 1.5183(2.0739) | Total Time 14.00(14.00)\n",
      "Iter 17350 | Time 21.5900(21.8028) | Bit/dim 3.5264(3.5385) | Xent 0.0915(0.0828) | Loss 3.5721(3.5799) | Error 0.0344(0.0292) Steps 934(902.09) | Grad Norm 1.5788(1.9932) | Total Time 14.00(14.00)\n",
      "Iter 17360 | Time 22.2033(21.7493) | Bit/dim 3.5304(3.5368) | Xent 0.0696(0.0829) | Loss 3.5651(3.5783) | Error 0.0278(0.0294) Steps 892(903.05) | Grad Norm 1.4643(1.9818) | Total Time 14.00(14.00)\n",
      "Iter 17370 | Time 22.2576(21.7786) | Bit/dim 3.5531(3.5389) | Xent 0.1092(0.0843) | Loss 3.6077(3.5811) | Error 0.0367(0.0298) Steps 910(904.10) | Grad Norm 1.8727(1.9630) | Total Time 14.00(14.00)\n",
      "Iter 17380 | Time 21.8624(21.8859) | Bit/dim 3.5025(3.5376) | Xent 0.0795(0.0840) | Loss 3.5423(3.5796) | Error 0.0278(0.0295) Steps 868(903.51) | Grad Norm 1.8035(1.9369) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0316 | Time 106.5318, Epoch Time 1324.9434(1296.6906), Bit/dim 3.5476(best: 3.5462), Xent 1.1948, Loss 4.1450, Error 0.2293(best: 0.2173)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17390 | Time 22.6327(21.9093) | Bit/dim 3.5465(3.5385) | Xent 0.0704(0.0824) | Loss 3.5817(3.5797) | Error 0.0233(0.0284) Steps 916(903.68) | Grad Norm 1.5255(1.9191) | Total Time 14.00(14.00)\n",
      "Iter 17400 | Time 22.2631(21.8687) | Bit/dim 3.5551(3.5373) | Xent 0.0962(0.0849) | Loss 3.6032(3.5797) | Error 0.0389(0.0292) Steps 886(902.41) | Grad Norm 2.6832(1.9545) | Total Time 14.00(14.00)\n",
      "Iter 17410 | Time 21.3305(21.7807) | Bit/dim 3.5433(3.5360) | Xent 0.0745(0.0840) | Loss 3.5805(3.5780) | Error 0.0300(0.0293) Steps 892(902.32) | Grad Norm 1.9225(1.8878) | Total Time 14.00(14.00)\n",
      "Iter 17420 | Time 21.5985(21.7940) | Bit/dim 3.5034(3.5362) | Xent 0.0724(0.0841) | Loss 3.5396(3.5782) | Error 0.0267(0.0294) Steps 928(901.24) | Grad Norm 1.5177(1.8875) | Total Time 14.00(14.00)\n",
      "Iter 17430 | Time 22.0647(21.8444) | Bit/dim 3.5422(3.5350) | Xent 0.0696(0.0849) | Loss 3.5770(3.5775) | Error 0.0211(0.0297) Steps 934(903.38) | Grad Norm 1.6405(1.8229) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0317 | Time 105.9029, Epoch Time 1327.0369(1297.6009), Bit/dim 3.5475(best: 3.5462), Xent 1.2114, Loss 4.1532, Error 0.2315(best: 0.2173)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17440 | Time 21.8693(21.8896) | Bit/dim 3.5503(3.5378) | Xent 0.0933(0.0868) | Loss 3.5969(3.5812) | Error 0.0367(0.0308) Steps 892(901.73) | Grad Norm 2.0603(1.9519) | Total Time 14.00(14.00)\n",
      "Iter 17450 | Time 21.2179(21.8016) | Bit/dim 3.5510(3.5382) | Xent 0.0895(0.0870) | Loss 3.5958(3.5817) | Error 0.0333(0.0310) Steps 910(901.52) | Grad Norm 1.9728(1.9994) | Total Time 14.00(14.00)\n",
      "Iter 17460 | Time 21.6211(21.8203) | Bit/dim 3.5713(3.5369) | Xent 0.0844(0.0879) | Loss 3.6135(3.5809) | Error 0.0233(0.0314) Steps 904(900.80) | Grad Norm 1.9834(2.0394) | Total Time 14.00(14.00)\n",
      "Iter 17470 | Time 22.5074(21.9023) | Bit/dim 3.4971(3.5366) | Xent 0.0870(0.0887) | Loss 3.5406(3.5809) | Error 0.0333(0.0317) Steps 916(902.83) | Grad Norm 3.3471(2.0646) | Total Time 14.00(14.00)\n",
      "Iter 17480 | Time 21.9861(21.8820) | Bit/dim 3.5166(3.5377) | Xent 0.0820(0.0877) | Loss 3.5576(3.5815) | Error 0.0300(0.0311) Steps 916(901.11) | Grad Norm 2.1473(2.0551) | Total Time 14.00(14.00)\n",
      "Iter 17490 | Time 21.9706(21.8894) | Bit/dim 3.5584(3.5372) | Xent 0.0780(0.0870) | Loss 3.5974(3.5807) | Error 0.0300(0.0305) Steps 898(903.17) | Grad Norm 2.8261(2.1413) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0318 | Time 106.3573, Epoch Time 1327.2078(1298.4892), Bit/dim 3.5476(best: 3.5462), Xent 1.2011, Loss 4.1482, Error 0.2302(best: 0.2173)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17500 | Time 21.9979(21.8787) | Bit/dim 3.5303(3.5362) | Xent 0.0669(0.0856) | Loss 3.5638(3.5791) | Error 0.0222(0.0303) Steps 898(903.50) | Grad Norm 1.5802(2.0794) | Total Time 14.00(14.00)\n",
      "Iter 17510 | Time 21.2988(21.8839) | Bit/dim 3.5504(3.5372) | Xent 0.0660(0.0839) | Loss 3.5834(3.5792) | Error 0.0289(0.0294) Steps 898(902.69) | Grad Norm 1.9120(2.0393) | Total Time 14.00(14.00)\n",
      "Iter 17520 | Time 22.3493(21.9205) | Bit/dim 3.5289(3.5361) | Xent 0.0847(0.0844) | Loss 3.5713(3.5783) | Error 0.0289(0.0299) Steps 892(901.99) | Grad Norm 1.3268(1.9304) | Total Time 14.00(14.00)\n",
      "Iter 17530 | Time 21.5504(21.9214) | Bit/dim 3.5160(3.5349) | Xent 0.0889(0.0851) | Loss 3.5604(3.5774) | Error 0.0278(0.0295) Steps 898(900.67) | Grad Norm 2.2844(1.9456) | Total Time 14.00(14.00)\n",
      "Iter 17540 | Time 22.3474(21.9932) | Bit/dim 3.5361(3.5361) | Xent 0.0998(0.0867) | Loss 3.5860(3.5794) | Error 0.0367(0.0306) Steps 910(901.62) | Grad Norm 2.3509(1.9786) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0319 | Time 105.5662, Epoch Time 1334.0667(1299.5565), Bit/dim 3.5454(best: 3.5462), Xent 1.1745, Loss 4.1326, Error 0.2324(best: 0.2173)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17550 | Time 21.6810(22.0018) | Bit/dim 3.5366(3.5352) | Xent 0.0952(0.0864) | Loss 3.5842(3.5785) | Error 0.0333(0.0309) Steps 910(901.58) | Grad Norm 1.8531(1.9721) | Total Time 14.00(14.00)\n",
      "Iter 17560 | Time 21.8285(21.9713) | Bit/dim 3.5066(3.5359) | Xent 0.0978(0.0852) | Loss 3.5554(3.5785) | Error 0.0278(0.0303) Steps 892(903.65) | Grad Norm 1.8470(1.9038) | Total Time 14.00(14.00)\n",
      "Iter 17570 | Time 21.6062(21.9583) | Bit/dim 3.5018(3.5368) | Xent 0.0723(0.0836) | Loss 3.5379(3.5786) | Error 0.0189(0.0296) Steps 922(905.50) | Grad Norm 1.7070(1.8673) | Total Time 14.00(14.00)\n",
      "Iter 17580 | Time 22.6805(22.0134) | Bit/dim 3.5385(3.5353) | Xent 0.1117(0.0848) | Loss 3.5943(3.5777) | Error 0.0489(0.0302) Steps 904(905.74) | Grad Norm 2.0790(1.8143) | Total Time 14.00(14.00)\n",
      "Iter 17590 | Time 22.2429(22.0185) | Bit/dim 3.5398(3.5371) | Xent 0.0839(0.0854) | Loss 3.5818(3.5798) | Error 0.0278(0.0304) Steps 910(907.20) | Grad Norm 1.8494(1.7740) | Total Time 14.00(14.00)\n",
      "Iter 17600 | Time 21.6314(22.0195) | Bit/dim 3.5131(3.5349) | Xent 0.0852(0.0852) | Loss 3.5557(3.5775) | Error 0.0344(0.0301) Steps 898(904.45) | Grad Norm 1.8962(1.7489) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0320 | Time 106.0796, Epoch Time 1334.9896(1300.6195), Bit/dim 3.5468(best: 3.5454), Xent 1.2440, Loss 4.1688, Error 0.2353(best: 0.2173)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17610 | Time 21.8102(21.9865) | Bit/dim 3.5434(3.5355) | Xent 0.0843(0.0847) | Loss 3.5855(3.5779) | Error 0.0289(0.0301) Steps 904(903.93) | Grad Norm 2.0757(1.8106) | Total Time 14.00(14.00)\n",
      "Iter 17620 | Time 21.6961(21.9420) | Bit/dim 3.5995(3.5365) | Xent 0.0717(0.0858) | Loss 3.6354(3.5794) | Error 0.0267(0.0304) Steps 904(903.50) | Grad Norm 2.0331(1.9514) | Total Time 14.00(14.00)\n",
      "Iter 17630 | Time 21.9371(21.9311) | Bit/dim 3.4997(3.5347) | Xent 0.0711(0.0844) | Loss 3.5353(3.5769) | Error 0.0211(0.0298) Steps 922(907.18) | Grad Norm 1.5007(1.9138) | Total Time 14.00(14.00)\n",
      "Iter 17640 | Time 21.5277(21.9518) | Bit/dim 3.5279(3.5363) | Xent 0.0887(0.0822) | Loss 3.5722(3.5774) | Error 0.0300(0.0286) Steps 898(908.07) | Grad Norm 2.2612(1.9252) | Total Time 14.00(14.00)\n",
      "Iter 17650 | Time 22.6217(21.9302) | Bit/dim 3.5234(3.5358) | Xent 0.0893(0.0842) | Loss 3.5680(3.5779) | Error 0.0322(0.0296) Steps 898(906.82) | Grad Norm 2.3281(1.9540) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0321 | Time 106.5077, Epoch Time 1330.8739(1301.5271), Bit/dim 3.5456(best: 3.5454), Xent 1.1819, Loss 4.1365, Error 0.2293(best: 0.2173)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17660 | Time 21.8837(21.9465) | Bit/dim 3.5310(3.5333) | Xent 0.0846(0.0868) | Loss 3.5733(3.5768) | Error 0.0289(0.0301) Steps 904(907.40) | Grad Norm 2.1869(1.9982) | Total Time 14.00(14.00)\n",
      "Iter 17670 | Time 21.9514(21.9295) | Bit/dim 3.5573(3.5369) | Xent 0.0866(0.0846) | Loss 3.6006(3.5792) | Error 0.0289(0.0303) Steps 904(907.13) | Grad Norm 1.5502(1.9257) | Total Time 14.00(14.00)\n",
      "Iter 17680 | Time 21.7984(21.9211) | Bit/dim 3.5546(3.5355) | Xent 0.0581(0.0824) | Loss 3.5836(3.5767) | Error 0.0178(0.0293) Steps 910(907.93) | Grad Norm 1.7802(1.8917) | Total Time 14.00(14.00)\n",
      "Iter 17690 | Time 21.5987(21.8984) | Bit/dim 3.5524(3.5374) | Xent 0.0713(0.0819) | Loss 3.5880(3.5783) | Error 0.0256(0.0291) Steps 904(908.12) | Grad Norm 1.4109(1.8535) | Total Time 14.00(14.00)\n",
      "Iter 17700 | Time 21.6596(21.9628) | Bit/dim 3.5311(3.5361) | Xent 0.0692(0.0821) | Loss 3.5657(3.5771) | Error 0.0222(0.0292) Steps 904(908.38) | Grad Norm 1.4495(1.8413) | Total Time 14.00(14.00)\n",
      "Iter 17710 | Time 21.6330(22.0058) | Bit/dim 3.5112(3.5356) | Xent 0.0890(0.0824) | Loss 3.5558(3.5768) | Error 0.0289(0.0291) Steps 898(906.49) | Grad Norm 1.6377(1.8702) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0322 | Time 104.7919, Epoch Time 1332.6278(1302.4601), Bit/dim 3.5460(best: 3.5454), Xent 1.2155, Loss 4.1537, Error 0.2320(best: 0.2173)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17720 | Time 22.4497(22.0502) | Bit/dim 3.5700(3.5385) | Xent 0.0791(0.0823) | Loss 3.6096(3.5796) | Error 0.0222(0.0289) Steps 874(904.46) | Grad Norm 1.4392(1.9184) | Total Time 14.00(14.00)\n",
      "Iter 17730 | Time 22.5289(22.0284) | Bit/dim 3.5458(3.5352) | Xent 0.0898(0.0828) | Loss 3.5907(3.5766) | Error 0.0311(0.0286) Steps 916(904.63) | Grad Norm 1.7123(1.9139) | Total Time 14.00(14.00)\n",
      "Iter 17740 | Time 21.6852(22.0020) | Bit/dim 3.5361(3.5353) | Xent 0.1168(0.0850) | Loss 3.5945(3.5778) | Error 0.0433(0.0296) Steps 892(904.90) | Grad Norm 1.4162(1.8654) | Total Time 14.00(14.00)\n",
      "Iter 17750 | Time 21.9247(21.9740) | Bit/dim 3.5462(3.5318) | Xent 0.0965(0.0842) | Loss 3.5945(3.5739) | Error 0.0367(0.0291) Steps 916(907.03) | Grad Norm 1.9266(1.7972) | Total Time 14.00(14.00)\n",
      "Iter 17760 | Time 22.4936(21.9342) | Bit/dim 3.5543(3.5347) | Xent 0.0818(0.0837) | Loss 3.5952(3.5765) | Error 0.0300(0.0290) Steps 916(906.52) | Grad Norm 1.8039(1.8113) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0323 | Time 106.8333, Epoch Time 1334.7383(1303.4285), Bit/dim 3.5472(best: 3.5454), Xent 1.2371, Loss 4.1658, Error 0.2302(best: 0.2173)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17770 | Time 22.7977(22.0011) | Bit/dim 3.5329(3.5364) | Xent 0.0814(0.0826) | Loss 3.5736(3.5777) | Error 0.0267(0.0285) Steps 910(906.22) | Grad Norm 1.6729(1.8932) | Total Time 14.00(14.00)\n",
      "Iter 17780 | Time 22.2502(22.0857) | Bit/dim 3.5230(3.5381) | Xent 0.0873(0.0823) | Loss 3.5667(3.5792) | Error 0.0378(0.0289) Steps 928(909.46) | Grad Norm 1.5971(1.8730) | Total Time 14.00(14.00)\n",
      "Iter 17790 | Time 22.2317(22.1056) | Bit/dim 3.5387(3.5363) | Xent 0.0743(0.0833) | Loss 3.5759(3.5780) | Error 0.0256(0.0294) Steps 916(912.53) | Grad Norm 2.1664(1.8796) | Total Time 14.00(14.00)\n",
      "Iter 17800 | Time 22.4083(22.1399) | Bit/dim 3.5161(3.5390) | Xent 0.0827(0.0835) | Loss 3.5575(3.5807) | Error 0.0289(0.0293) Steps 934(912.12) | Grad Norm 1.9704(1.9588) | Total Time 14.00(14.00)\n",
      "Iter 17810 | Time 22.2570(22.1001) | Bit/dim 3.5177(3.5367) | Xent 0.0771(0.0842) | Loss 3.5562(3.5788) | Error 0.0289(0.0296) Steps 910(912.04) | Grad Norm 2.1665(2.0122) | Total Time 14.00(14.00)\n",
      "Iter 17820 | Time 22.4286(22.1381) | Bit/dim 3.5344(3.5355) | Xent 0.1056(0.0858) | Loss 3.5872(3.5784) | Error 0.0267(0.0302) Steps 898(909.00) | Grad Norm 3.0962(2.1016) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0324 | Time 106.3137, Epoch Time 1345.5948(1304.6935), Bit/dim 3.5461(best: 3.5454), Xent 1.2282, Loss 4.1602, Error 0.2354(best: 0.2173)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17830 | Time 22.5037(22.1181) | Bit/dim 3.5295(3.5350) | Xent 0.1015(0.0865) | Loss 3.5802(3.5783) | Error 0.0322(0.0301) Steps 922(909.78) | Grad Norm 2.2456(2.1313) | Total Time 14.00(14.00)\n",
      "Iter 17840 | Time 22.1285(22.1016) | Bit/dim 3.5453(3.5331) | Xent 0.1106(0.0878) | Loss 3.6005(3.5770) | Error 0.0378(0.0303) Steps 898(909.16) | Grad Norm 2.3462(2.1803) | Total Time 14.00(14.00)\n",
      "Iter 17850 | Time 21.3981(22.0290) | Bit/dim 3.5311(3.5356) | Xent 0.0739(0.0876) | Loss 3.5680(3.5794) | Error 0.0222(0.0298) Steps 904(910.21) | Grad Norm 1.9297(2.1604) | Total Time 14.00(14.00)\n",
      "Iter 17860 | Time 22.2737(22.0143) | Bit/dim 3.5663(3.5360) | Xent 0.0843(0.0863) | Loss 3.6084(3.5791) | Error 0.0322(0.0298) Steps 916(909.97) | Grad Norm 1.4345(2.0417) | Total Time 14.00(14.00)\n",
      "Iter 17870 | Time 22.4551(22.0679) | Bit/dim 3.5388(3.5384) | Xent 0.0666(0.0854) | Loss 3.5721(3.5811) | Error 0.0244(0.0295) Steps 904(910.10) | Grad Norm 1.4886(1.9948) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0325 | Time 106.1820, Epoch Time 1337.7852(1305.6862), Bit/dim 3.5454(best: 3.5454), Xent 1.2050, Loss 4.1478, Error 0.2270(best: 0.2173)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17880 | Time 21.5974(22.1214) | Bit/dim 3.5114(3.5361) | Xent 0.1061(0.0846) | Loss 3.5645(3.5784) | Error 0.0322(0.0288) Steps 886(908.41) | Grad Norm 2.4427(1.9297) | Total Time 14.00(14.00)\n",
      "Iter 17890 | Time 22.1934(22.1484) | Bit/dim 3.5768(3.5365) | Xent 0.0594(0.0841) | Loss 3.6065(3.5785) | Error 0.0256(0.0292) Steps 916(908.61) | Grad Norm 1.8519(1.9214) | Total Time 14.00(14.00)\n",
      "Iter 17900 | Time 22.3673(22.1609) | Bit/dim 3.5044(3.5348) | Xent 0.0765(0.0862) | Loss 3.5426(3.5779) | Error 0.0233(0.0301) Steps 898(908.11) | Grad Norm 2.2287(1.9920) | Total Time 14.00(14.00)\n",
      "Iter 17910 | Time 22.2450(22.1520) | Bit/dim 3.5314(3.5350) | Xent 0.0850(0.0849) | Loss 3.5739(3.5775) | Error 0.0289(0.0304) Steps 910(908.17) | Grad Norm 1.7089(1.9472) | Total Time 14.00(14.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_cifar.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_tunetol_run1 --resume ../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_tunetol_run1/epoch_250_checkpt.pth --seed 1 --conditional True --controlled_tol True --train_mode semisup --lr 0.0001 --warmup_iters 1000 --atol 1e-4  --rtol 1e-4 --log_freq 10 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
