{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0,1,2,3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_rl_2cond.py\n",
      "from __future__ import print_function\n",
      "\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import torch.utils.data as data\n",
      "from torch.utils.data import Dataset\n",
      "\n",
      "from PIL import Image\n",
      "import os.path\n",
      "import errno\n",
      "import codecs\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time, count_nfe_gate\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "GATES = [\"cnn1\", \"cnn2\", \"rnn\"]\n",
      "\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"colormnist\", \"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--gate', type=str, default='cnn1', choices=GATES)\n",
      "parser.add_argument('--scale', type=float, default=1.0)\n",
      "parser.add_argument('--scale_fac', type=float, default=1.0)\n",
      "parser.add_argument('--scale_std', type=float, default=1.0)\n",
      "parser.add_argument('--eta', default=0.1, type=float,\n",
      "                        help='tuning parameter that allows us to trade-off the competing goals of' \n",
      "                                'minimizing the prediction loss and maximizing the gate rewards ')\n",
      "parser.add_argument('--rl-weight', default=0.01, type=float,\n",
      "                        help='rl weight')\n",
      "\n",
      "parser.add_argument('--gamma', default=0.99, type=float,\n",
      "                        help='discount factor, default: (0.99)')\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=500)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "parser.add_argument(\"--annealing_std\", type=eval, default=False, choices=[True, False])\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "parser.add_argument(\"--cond_nn\", choices=[\"linear\", \"mlp\"], type=str, default=\"linear\")\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "import lib.odenvp_conditional_rl_2cond as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "class ColorMNIST(data.Dataset):\n",
      "    \"\"\"`MNIST <http://yann.lecun.com/exdb/mnist/>`_ Dataset.\n",
      "\n",
      "    Args:\n",
      "        root (string): Root directory of dataset where ``processed/training.pt``\n",
      "            and  ``processed/test.pt`` exist.\n",
      "        train (bool, optional): If True, creates dataset from ``training.pt``,\n",
      "            otherwise from ``test.pt``.\n",
      "        download (bool, optional): If true, downloads the dataset from the internet and\n",
      "            puts it in root directory. If dataset is already downloaded, it is not\n",
      "            downloaded again.\n",
      "        transform (callable, optional): A function/transform that  takes in an PIL image\n",
      "            and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
      "        target_transform (callable, optional): A function/transform that takes in the\n",
      "            target and transforms it.\n",
      "    \"\"\"\n",
      "    urls = [\n",
      "        'http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz',\n",
      "    ]\n",
      "    raw_folder = 'raw'\n",
      "    processed_folder = 'processed'\n",
      "    training_file = 'training.pt'\n",
      "    test_file = 'test.pt'\n",
      "\n",
      "    def __init__(self, root, train=True, transform=None, target_transform=None, download=False):\n",
      "        self.root = os.path.expanduser(root)\n",
      "        self.transform = transform\n",
      "        self.target_transform = target_transform\n",
      "        self.train = train  # training set or test set\n",
      "\n",
      "        if download:\n",
      "            self.download()\n",
      "\n",
      "        if not self._check_exists():\n",
      "            raise RuntimeError('Dataset not found.' +\n",
      "                               ' You can use download=True to download it')\n",
      "\n",
      "        if self.train:\n",
      "            self.train_data, self.train_labels = torch.load(\n",
      "                os.path.join(self.root, self.processed_folder, self.training_file))\n",
      "            \n",
      "            self.train_data = np.tile(self.train_data[:, :, :, np.newaxis], 3)\n",
      "        else:\n",
      "            self.test_data, self.test_labels = torch.load(\n",
      "                os.path.join(self.root, self.processed_folder, self.test_file))\n",
      "            \n",
      "            self.test_data = np.tile(self.test_data[:, :, :, np.newaxis], 3)\n",
      "        \n",
      "        self.pallette = [[31, 119, 180],\n",
      "                         [255, 127, 14],\n",
      "                         [44, 160, 44],\n",
      "                         [214, 39, 40],\n",
      "                         [148, 103, 189],\n",
      "                         [140, 86, 75],\n",
      "                         [227, 119, 194],\n",
      "                         [127, 127, 127],\n",
      "                         [188, 189, 34],\n",
      "                         [23, 190, 207]]\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        \"\"\"\n",
      "        Args:\n",
      "            index (int): Index\n",
      "\n",
      "        Returns:\n",
      "            tuple: (image, target) where target is index of the target class.\n",
      "        \"\"\"\n",
      "        if self.train:\n",
      "            img, target = self.train_data[index], self.train_labels[index]\n",
      "        else:\n",
      "            img, target = self.test_data[index], self.test_labels[index]\n",
      "        \n",
      "        # doing this so that it is consistent with all other datasets\n",
      "        # to return a PIL Image\n",
      "        y_color_digit = np.random.randint(0, 10)\n",
      "        c_digit = self.pallette[y_color_digit]\n",
      "        \n",
      "        img[:, :, 0] = img[:, :, 0] / 255 * c_digit[0]\n",
      "        img[:, :, 1] = img[:, :, 1] / 255 * c_digit[1]\n",
      "        img[:, :, 2] = img[:, :, 2] / 255 * c_digit[2]\n",
      "        \n",
      "        img = Image.fromarray(img)\n",
      "\n",
      "        if self.transform is not None:\n",
      "            img = self.transform(img)\n",
      "\n",
      "        if self.target_transform is not None:\n",
      "            target = self.target_transform(target)\n",
      "\n",
      "        return img, [target,torch.from_numpy(np.array(y_color_digit))]\n",
      "\n",
      "    def __len__(self):\n",
      "        if self.train:\n",
      "            return len(self.train_data)\n",
      "        else:\n",
      "            return len(self.test_data)\n",
      "\n",
      "    def _check_exists(self):\n",
      "        return os.path.exists(os.path.join(self.root, self.processed_folder, self.training_file)) and \\\n",
      "            os.path.exists(os.path.join(self.root, self.processed_folder, self.test_file))\n",
      "\n",
      "    def download(self):\n",
      "        \"\"\"Download the MNIST data if it doesn't exist in processed_folder already.\"\"\"\n",
      "        from six.moves import urllib\n",
      "        import gzip\n",
      "\n",
      "        if self._check_exists():\n",
      "            return\n",
      "\n",
      "        # download files\n",
      "        try:\n",
      "            os.makedirs(os.path.join(self.root, self.raw_folder))\n",
      "            os.makedirs(os.path.join(self.root, self.processed_folder))\n",
      "        except OSError as e:\n",
      "            if e.errno == errno.EEXIST:\n",
      "                pass\n",
      "            else:\n",
      "                raise\n",
      "\n",
      "        for url in self.urls:\n",
      "            print('Downloading ' + url)\n",
      "            data = urllib.request.urlopen(url)\n",
      "            filename = url.rpartition('/')[2]\n",
      "            file_path = os.path.join(self.root, self.raw_folder, filename)\n",
      "            with open(file_path, 'wb') as f:\n",
      "                f.write(data.read())\n",
      "            with open(file_path.replace('.gz', ''), 'wb') as out_f, \\\n",
      "                    gzip.GzipFile(file_path) as zip_f:\n",
      "                out_f.write(zip_f.read())\n",
      "            os.unlink(file_path)\n",
      "\n",
      "        # process and save as torch files\n",
      "        print('Processing...')\n",
      "\n",
      "        training_set = (\n",
      "            read_image_file(os.path.join(self.root, self.raw_folder, 'train-images-idx3-ubyte')),\n",
      "            read_label_file(os.path.join(self.root, self.raw_folder, 'train-labels-idx1-ubyte'))\n",
      "        )\n",
      "        test_set = (\n",
      "            read_image_file(os.path.join(self.root, self.raw_folder, 't10k-images-idx3-ubyte')),\n",
      "            read_label_file(os.path.join(self.root, self.raw_folder, 't10k-labels-idx1-ubyte'))\n",
      "        )\n",
      "        with open(os.path.join(self.root, self.processed_folder, self.training_file), 'wb') as f:\n",
      "            torch.save(training_set, f)\n",
      "        with open(os.path.join(self.root, self.processed_folder, self.test_file), 'wb') as f:\n",
      "            torch.save(test_set, f)\n",
      "\n",
      "        print('Done!')\n",
      "\n",
      "    def __repr__(self):\n",
      "        fmt_str = 'Dataset ' + self.__class__.__name__ + '\\n'\n",
      "        fmt_str += '    Number of datapoints: {}\\n'.format(self.__len__())\n",
      "        tmp = 'train' if self.train is True else 'test'\n",
      "        fmt_str += '    Split: {}\\n'.format(tmp)\n",
      "        fmt_str += '    Root Location: {}\\n'.format(self.root)\n",
      "        tmp = '    Transforms (if any): '\n",
      "        fmt_str += '{0}{1}\\n'.format(tmp, self.transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
      "        tmp = '    Target Transforms (if any): '\n",
      "        fmt_str += '{0}{1}'.format(tmp, self.target_transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
      "        return fmt_str\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "        \n",
      "def update_scale_std(model, epoch):\n",
      "    epoch_frac = 1.0 - float(epoch - 1) / max(args.num_epochs + 1, 1)\n",
      "    scale_std = args.scale_std * epoch_frac\n",
      "    model.set_scale_std(scale_std)\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    if args.data == \"colormnist\":\n",
      "        im_dim = 3\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = ColorMNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = ColorMNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, y_color, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "    y_onehot_color = thops.onehot(y_color, num_classes=model.module.y_color).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "    mean_color, logs_color = model.module._prior_color(y_onehot_color)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_color_sup = modules.GaussianDiag.logp(mean_color, logs_color, z[:, dim_sup:(2*dim_sup)]).view(-1,1)  # logp(z)_color_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, (2*dim_sup):]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_color_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "        zcolorsup = model.module.dropout_color(z[:, dim_sup:(2*dim_sup)])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "        zcolorsup = z[:, dim_sup:(2*dim_sup)]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "    \n",
      "    y_logits_color = model.module.project_color(zcolorsup)\n",
      "    loss_xent_color = model.module.loss_class(y_logits_color, y_color.to(x.get_device()))\n",
      "    y_color_predicted = np.argmax(y_logits_color.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, loss_xent_color, y_predicted, y_color_predicted, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol, \"scale\": args.scale, \"scale_fac\": args.scale_fac, \"scale_std\": args.scale_std, \"gate\": args.gate},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,\n",
      "            cond_nn=args.cond_nn)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    xent_color_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    error_color_meter = utils.RunningAverageMeter(0.97)\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        xent_color_meter.set(checkpt['xent_train_color'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        error_color_meter.set(checkpt['error_train_color'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        \n",
      "        fixed_y_color = torch.from_numpy(np.arange(model.module.y_color)).repeat(model.module.y_color).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot_color = thops.onehot(fixed_y_color, num_classes=model.module.y_color)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            mean_color, logs_color = model.module._prior_color(fixed_y_onehot_color)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_color_sup = modules.GaussianDiag.sample(mean_color, logs_color)\n",
      "            dim_unsup = np.prod(data_shape) - np.prod(fixed_z_sup.shape[1:]) - np.prod(fixed_z_color_sup.shape[1:])\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_color_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    best_error_score_color = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        if args.annealing_std:\n",
      "            update_scale_std(model.module, epoch)\n",
      "            \n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y_all) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            \n",
      "            y = y_all[0]\n",
      "            y_color = y_all[1]\n",
      "            \n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, loss_xent_color, y_predicted, y_color_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, y_color, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * 0.5 * (loss_xent + loss_xent_color)\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  0.5 * (loss_xent + loss_xent_color)\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy()) \n",
      "                error_score_color = 1. - np.mean(y_color_predicted.astype(int) == y_color.numpy())\n",
      "                \n",
      "            else:\n",
      "                loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, loss_xent_color, error_score, error_score_color = loss, 0., 0., 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            # re-weight the gate rewards\n",
      "            normalized_eta = args.eta / len(logp_actions)\n",
      "            \n",
      "            # collect cumulative future rewards\n",
      "            R = - loss\n",
      "            cum_rewards = []\n",
      "            for r in nfe[::-1]:\n",
      "                R = -normalized_eta * r.view(-1,1) + args.gamma * R\n",
      "                cum_rewards.insert(0,R)\n",
      "            \n",
      "            # apply REINFORCE\n",
      "            rl_loss = 0\n",
      "            for lpa, r in zip(logp_actions, cum_rewards):\n",
      "                rl_loss = rl_loss - lpa.view(-1,1) * args.rl_weight * r\n",
      "                \n",
      "            loss = loss + rl_loss.mean()\n",
      "            \n",
      "            loss.backward()\n",
      "            \n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "                xent_color_meter.update(loss_xent_color.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "                xent_color_meter.update(loss_xent_color)\n",
      "            error_meter.update(error_score)\n",
      "            error_color_meter.update(error_score_color)\n",
      "            steps_meter.update(count_nfe_gate(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            for idx in range(len(model.module.transforms)):\n",
      "                for layer in model.module.transforms[idx].chain:\n",
      "                    if hasattr(layer, 'atol'):\n",
      "                        layer.odefunc.after_odeint()\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('xent_color', {'train_iter': xent_color_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('error_color', {'train_iter': error_color_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'train': atol[tol_indx].mean()}, itr)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'train': rtol[tol_indx].mean()}, itr)\n",
      "                    \n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Xent Color {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) | Error Color {:.4f}({:.4f}) |\"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, xent_color_meter.val, xent_color_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, error_color_meter.val, error_color_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent_color', {'train_epoch': xent_color_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('error_color', {'train_epoch': error_color_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses_xent_color = []; losses = []\n",
      "                total_correct = 0\n",
      "                total_correct_color = 0\n",
      "                \n",
      "                for (x, y_all) in test_loader:\n",
      "                    y = y_all[0]\n",
      "                    y_color = y_all[1]\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, loss_xent_color, y_predicted, y_color_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, y_color, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * 0.5 * (loss_xent + loss_xent_color)\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  0.5 * (loss_xent + loss_xent_color)\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                        total_correct_color += np.sum(y_color_predicted.astype(int) == y_color.numpy())\n",
      "                    else:\n",
      "                        loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent, loss_xent_color = loss, 0., 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                        losses_xent_color.append(loss_xent_color.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                        losses_xent_color.append(loss_xent_color)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss_xent_color = np.mean(losses_xent_color); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                error_score_color =  1. - total_correct_color / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('xent_color', {'validation': loss_xent_color}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                writer.add_scalars('error_color', {'validation': error_score_color}, epoch)\n",
      "                \n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Xent Color {:.4f}. Loss {:.4f}, Error {:.4f}(best: {:.4f}), Error Color {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss_xent_color, loss, error_score, best_error_score, error_score_color, best_error_score_color)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"error_color\": error_score_color,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"xent_color\": loss_xent_color,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"best_error_score_color\": best_error_score_color,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"error_train_color\": error_color_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"xent_train_color\": xent_color_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"error_color\": error_score_color,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"xent_color\": loss_xent_color,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"best_error_score_color\": best_error_score_color,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"error_train_color\": error_color_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"xent_train_color\": xent_color_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"error_color\": error_score_color,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"xent_color\": loss_xent_color,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"best_error_score_color\": best_error_score_color,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"error_train_color\": error_color_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"xent_train_color\": xent_color_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"error_color\": error_score_color,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"xent_color\": loss_xent_color,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"best_error_score_color\": best_error_score_color,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"error_train_color\": error_color_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"xent_train_color\": xent_color_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "                    if error_score_color < best_error_score_color:\n",
      "                        best_error_score_color = error_score_color\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"error_color\": error_score_color,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"xent_color\": loss_xent_color,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"best_error_score_color\": best_error_score_color,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"error_train_color\": error_color_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"xent_train_color\": xent_color_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_color_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples, atol, rtol, logp_actions, nfe = model(fixed_z, reverse=True)\n",
      "            generated_samples = generated_samples.view(-1, *data_shape)\n",
      "            for tol_indx in range(len(atol)):\n",
      "                writer.add_scalars('atol_gen_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                writer.add_scalars('rtol_gen_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, annealing_std=False, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, cond_nn='linear', condition_ratio=0.08333, conditional=True, controlled_tol=False, conv=True, data='colormnist', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, eta=0.1, gamma=0.99, gate='cnn1', imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=500, parallel=False, rademacher=True, residual=False, resume=None, rl_weight=0.01, rtol=1e-05, save='../experiments_published/cnf_conditional_disentangle_colormnist_bs900_sratio_1_12th_drop_0_5_rl_stdscale_6_2cond_linear_run1', scale=1.0, scale_fac=1.0, scale_std=6.0, seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5)\n",
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=390, bias=True)\n",
      "  (project_ycond_color): LinearZeros(in_features=10, out_features=390, bias=True)\n",
      "  (project_class): LinearZeros(in_features=195, out_features=10, bias=True)\n",
      "  (project_color): LinearZeros(in_features=195, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      "  (dropout_color): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 922808\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc_rl.py:291: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Iter 0010 | Time 6.6176(21.3916) | Bit/dim 25.4388(27.3524) | Xent 2.2975(2.3020) | Xent Color 2.3024(2.3025) | Loss 48.7002(51.9254) | Error 0.8944(0.8977) | Error Color 0.9067(0.8906) |Steps 296(291.73) | Grad Norm 261.1515(275.7335) | Total Time 0.00(0.00)\n",
      "Iter 0020 | Time 7.2178(17.6586) | Bit/dim 20.1170(26.0937) | Xent 2.2856(2.2991) | Xent Color 2.3016(2.3023) | Loss 39.1447(49.7187) | Error 0.8789(0.8953) | Error Color 0.9111(0.8925) |Steps 308(293.64) | Grad Norm 216.3112(265.5856) | Total Time 0.00(0.00)\n",
      "Iter 0030 | Time 7.4612(14.9239) | Bit/dim 13.8801(23.5779) | Xent 2.2602(2.2924) | Xent Color 2.2979(2.3016) | Loss 27.8193(45.2212) | Error 0.7811(0.8825) | Error Color 0.8800(0.8915) |Steps 320(299.42) | Grad Norm 160.5007(244.3623) | Total Time 0.00(0.00)\n",
      "Iter 0040 | Time 7.9620(13.0723) | Bit/dim 8.9888(20.2391) | Xent 2.2418(2.2814) | Xent Color 2.2963(2.3006) | Loss 18.7138(39.2251) | Error 0.6511(0.8411) | Error Color 0.8689(0.8874) |Steps 356(311.36) | Grad Norm 93.9610(212.5063) | Total Time 0.00(0.00)\n",
      "Iter 0050 | Time 9.1123(11.9489) | Bit/dim 6.6674(16.8833) | Xent 2.2132(2.2664) | Xent Color 2.2931(2.2989) | Loss 14.8774(33.2175) | Error 0.4133(0.7462) | Error Color 0.8922(0.8840) |Steps 374(328.66) | Grad Norm 33.9602(172.0995) | Total Time 0.00(0.00)\n",
      "Iter 0060 | Time 9.0910(11.2094) | Bit/dim 6.0962(14.1116) | Xent 2.1912(2.2488) | Xent Color 2.2925(2.2965) | Loss 13.7464(28.2355) | Error 0.4478(0.6637) | Error Color 0.9111(0.8859) |Steps 374(343.91) | Grad Norm 24.6352(132.9421) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0001 | Time 56.5580, Epoch Time 627.7620(627.7620), Bit/dim 5.7499(best: inf), Xent 2.1632, Xent Color 2.2869. Loss 6.8624, Error 0.3175(best: inf), Error Color 0.8995(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0070 | Time 8.6010(10.6443) | Bit/dim 4.2661(11.7848) | Xent 2.1889(2.2312) | Xent Color 2.2901(2.2944) | Loss 10.5601(24.5025) | Error 0.4467(0.6021) | Error Color 0.8756(0.8878) |Steps 404(354.05) | Grad Norm 16.5622(103.7796) | Total Time 0.00(0.00)\n",
      "Iter 0080 | Time 8.8243(10.1762) | Bit/dim 4.0130(9.7779) | Xent 2.1681(2.2160) | Xent Color 2.2794(2.2920) | Loss 10.0000(20.7722) | Error 0.4389(0.5537) | Error Color 0.8733(0.8856) |Steps 386(361.85) | Grad Norm 12.1895(79.9902) | Total Time 0.00(0.00)\n",
      "Iter 0090 | Time 9.1000(9.8490) | Bit/dim 3.7873(8.2305) | Xent 2.1402(2.2001) | Xent Color 2.2861(2.2903) | Loss 9.6396(17.8933) | Error 0.4100(0.5220) | Error Color 0.8878(0.8872) |Steps 386(365.74) | Grad Norm 9.8642(62.0065) | Total Time 0.00(0.00)\n",
      "Iter 0100 | Time 9.0267(9.5795) | Bit/dim 3.5488(7.0252) | Xent 2.1246(2.1835) | Xent Color 2.2735(2.2875) | Loss 9.3253(15.6521) | Error 0.4544(0.5055) | Error Color 0.8333(0.8802) |Steps 398(370.02) | Grad Norm 7.6886(47.7228) | Total Time 0.00(0.00)\n",
      "Iter 0110 | Time 8.9113(9.3927) | Bit/dim 3.2690(6.0716) | Xent 2.1125(2.1668) | Xent Color 2.2734(2.2845) | Loss 8.6745(13.8705) | Error 0.4378(0.4923) | Error Color 0.8344(0.8714) |Steps 380(371.27) | Grad Norm 6.7934(37.1105) | Total Time 0.00(0.00)\n",
      "Iter 0120 | Time 8.6186(9.2453) | Bit/dim 3.0748(5.3015) | Xent 2.1182(2.1520) | Xent Color 2.2696(2.2812) | Loss 8.1910(12.4440) | Error 0.4789(0.4804) | Error Color 0.8356(0.8662) |Steps 380(372.78) | Grad Norm 6.7280(29.2058) | Total Time 0.00(0.00)\n",
      "Iter 0130 | Time 8.7758(9.1488) | Bit/dim 2.7841(4.6639) | Xent 2.1161(2.1402) | Xent Color 2.2642(2.2767) | Loss 7.7505(11.2639) | Error 0.5000(0.4769) | Error Color 0.8122(0.8564) |Steps 392(375.71) | Grad Norm 5.6780(23.1649) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0002 | Time 54.1926, Epoch Time 663.7363(628.8412), Bit/dim 2.7075(best: 5.7499), Xent 2.0999, Xent Color 2.2587. Loss 3.7971, Error 0.3739(best: 0.3175), Error Color 0.8112(best: 0.8995)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0140 | Time 8.8562(9.1376) | Bit/dim 2.1672(4.0482) | Xent 2.1364(2.1389) | Xent Color 2.2597(2.2728) | Loss 6.6443(10.5801) | Error 0.5133(0.4882) | Error Color 0.8256(0.8488) |Steps 374(380.53) | Grad Norm 4.2692(18.5195) | Total Time 0.00(0.00)\n",
      "Iter 0150 | Time 9.2163(9.1807) | Bit/dim 2.0684(3.5385) | Xent 2.1494(2.1402) | Xent Color 2.2577(2.2688) | Loss 6.5022(9.5385) | Error 0.5389(0.5010) | Error Color 0.8533(0.8445) |Steps 392(384.09) | Grad Norm 3.8508(14.6565) | Total Time 0.00(0.00)\n",
      "Iter 0160 | Time 9.2510(9.2799) | Bit/dim 1.9890(3.1365) | Xent 2.1587(2.1437) | Xent Color 2.2537(2.2644) | Loss 6.4606(8.7322) | Error 0.5822(0.5185) | Error Color 0.8400(0.8407) |Steps 398(389.60) | Grad Norm 3.2012(11.7530) | Total Time 0.00(0.00)\n",
      "Iter 0170 | Time 9.4491(9.3148) | Bit/dim 1.8891(2.8192) | Xent 2.1556(2.1474) | Xent Color 2.2557(2.2602) | Loss 6.3191(8.0932) | Error 0.5911(0.5366) | Error Color 0.8500(0.8410) |Steps 416(391.62) | Grad Norm 2.8390(9.5063) | Total Time 0.00(0.00)\n",
      "Iter 0180 | Time 9.5765(9.3557) | Bit/dim 1.8130(2.5654) | Xent 2.1497(2.1503) | Xent Color 2.2448(2.2549) | Loss 6.0933(7.5938) | Error 0.5767(0.5507) | Error Color 0.8344(0.8389) |Steps 386(394.45) | Grad Norm 2.6490(7.7164) | Total Time 0.00(0.00)\n",
      "Iter 0190 | Time 9.1824(9.3472) | Bit/dim 1.7719(2.3631) | Xent 2.1585(2.1523) | Xent Color 2.2352(2.2506) | Loss 5.9975(7.1860) | Error 0.5944(0.5627) | Error Color 0.8422(0.8377) |Steps 404(394.99) | Grad Norm 2.0435(6.2880) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0003 | Time 56.0594, Epoch Time 697.9911(630.9157), Bit/dim 1.7360(best: 2.7075), Xent 2.1410, Xent Color 2.2252. Loss 2.8275, Error 0.5021(best: 0.3175), Error Color 0.8240(best: 0.8112)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0200 | Time 9.4659(9.3690) | Bit/dim 1.4801(2.1874) | Xent 2.1717(2.1535) | Xent Color 2.2437(2.2463) | Loss 5.5457(7.4013) | Error 0.6144(0.5731) | Error Color 0.8556(0.8379) |Steps 410(398.59) | Grad Norm 1.8734(5.1836) | Total Time 0.00(0.00)\n",
      "Iter 0210 | Time 9.2178(9.3925) | Bit/dim 1.4476(1.9972) | Xent 2.1643(2.1578) | Xent Color 2.2322(2.2441) | Loss 5.3915(6.9003) | Error 0.6167(0.5863) | Error Color 0.8344(0.8401) |Steps 374(398.25) | Grad Norm 1.4206(4.2606) | Total Time 0.00(0.00)\n",
      "Iter 0220 | Time 9.6398(9.3870) | Bit/dim 1.4310(1.8493) | Xent 2.1579(2.1574) | Xent Color 2.2257(2.2425) | Loss 5.4172(6.5118) | Error 0.6289(0.5938) | Error Color 0.8433(0.8400) |Steps 380(398.56) | Grad Norm 1.1316(3.5697) | Total Time 0.00(0.00)\n",
      "Iter 0230 | Time 9.1748(9.3687) | Bit/dim 1.3997(1.7335) | Xent 2.1475(2.1535) | Xent Color 2.2397(2.2414) | Loss 5.2827(6.2117) | Error 0.6411(0.5978) | Error Color 0.8378(0.8421) |Steps 404(400.16) | Grad Norm 1.2029(2.9963) | Total Time 0.00(0.00)\n",
      "Iter 0240 | Time 9.5076(9.3664) | Bit/dim 1.4090(1.6459) | Xent 2.1220(2.1445) | Xent Color 2.2248(2.2393) | Loss 5.3203(5.9880) | Error 0.6056(0.5933) | Error Color 0.8633(0.8432) |Steps 404(400.63) | Grad Norm 0.9439(2.6310) | Total Time 0.00(0.00)\n",
      "Iter 0250 | Time 9.5885(9.3868) | Bit/dim 1.3768(1.5777) | Xent 2.0850(2.1347) | Xent Color 2.2280(2.2375) | Loss 5.3159(5.8066) | Error 0.5333(0.5890) | Error Color 0.8389(0.8415) |Steps 416(400.64) | Grad Norm 3.4091(2.6955) | Total Time 0.00(0.00)\n",
      "Iter 0260 | Time 9.3676(9.3713) | Bit/dim 1.3844(1.5260) | Xent 2.0814(2.1216) | Xent Color 2.2378(2.2364) | Loss 5.3146(5.6633) | Error 0.5733(0.5836) | Error Color 0.8544(0.8423) |Steps 392(400.39) | Grad Norm 1.9067(2.7291) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0004 | Time 54.9401, Epoch Time 696.0295(632.8691), Bit/dim 1.3711(best: 1.7360), Xent 2.0464, Xent Color 2.2321. Loss 2.4407, Error 0.4356(best: 0.3175), Error Color 0.8386(best: 0.8112)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0270 | Time 9.1811(9.2770) | Bit/dim 1.1429(1.4484) | Xent 2.1071(2.1151) | Xent Color 2.2417(2.2370) | Loss 4.8423(5.9603) | Error 0.6033(0.5854) | Error Color 0.8467(0.8434) |Steps 386(397.44) | Grad Norm 1.2777(2.4494) | Total Time 0.00(0.00)\n",
      "Iter 0280 | Time 8.7775(9.1711) | Bit/dim 1.1516(1.3689) | Xent 2.0733(2.1091) | Xent Color 2.2356(2.2380) | Loss 4.8087(5.6719) | Error 0.5911(0.5892) | Error Color 0.8444(0.8438) |Steps 380(396.01) | Grad Norm 1.0777(2.2271) | Total Time 0.00(0.00)\n",
      "Iter 0290 | Time 8.9602(9.1143) | Bit/dim 1.1231(1.3079) | Xent 2.0444(2.0961) | Xent Color 2.2504(2.2406) | Loss 4.8021(5.4461) | Error 0.5689(0.5861) | Error Color 0.8422(0.8465) |Steps 398(393.35) | Grad Norm 2.3945(2.1593) | Total Time 0.00(0.00)\n",
      "Iter 0300 | Time 8.8963(9.0713) | Bit/dim 1.1313(1.2619) | Xent 1.9963(2.0790) | Xent Color 2.2415(2.2425) | Loss 4.7268(5.2843) | Error 0.5244(0.5802) | Error Color 0.8367(0.8471) |Steps 386(392.98) | Grad Norm 4.6495(2.3924) | Total Time 0.00(0.00)\n",
      "Iter 0310 | Time 8.9176(9.0611) | Bit/dim 1.1226(1.2283) | Xent 1.9724(2.0536) | Xent Color 2.2636(2.2450) | Loss 4.7732(5.1594) | Error 0.5300(0.5695) | Error Color 0.8700(0.8479) |Steps 386(393.59) | Grad Norm 3.6590(2.6788) | Total Time 0.00(0.00)\n",
      "Iter 0320 | Time 8.8947(9.0648) | Bit/dim 1.1436(1.2040) | Xent 1.9040(2.0194) | Xent Color 2.2555(2.2472) | Loss 4.7464(5.0559) | Error 0.5256(0.5544) | Error Color 0.8522(0.8492) |Steps 386(393.93) | Grad Norm 1.2876(3.0283) | Total Time 0.00(0.00)\n",
      "Iter 0330 | Time 9.0994(9.0887) | Bit/dim 1.1362(1.1868) | Xent 1.8368(1.9753) | Xent Color 2.2598(2.2504) | Loss 4.6881(4.9728) | Error 0.4989(0.5372) | Error Color 0.8489(0.8508) |Steps 386(393.87) | Grad Norm 7.5973(3.4298) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0005 | Time 54.2549, Epoch Time 670.7139(634.0045), Bit/dim 1.1263(best: 1.3711), Xent 1.7735, Xent Color 2.2478. Loss 2.1316, Error 0.3654(best: 0.3175), Error Color 0.8429(best: 0.8112)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0340 | Time 9.9744(9.1022) | Bit/dim 0.9357(1.1220) | Xent 1.9424(1.9712) | Xent Color 2.2768(2.2560) | Loss 4.3868(5.2472) | Error 0.5800(0.5486) | Error Color 0.8456(0.8530) |Steps 374(394.48) | Grad Norm 4.1234(3.6019) | Total Time 0.00(0.00)\n",
      "Iter 0350 | Time 8.8951(9.0628) | Bit/dim 0.9313(1.0741) | Xent 1.8147(1.9416) | Xent Color 2.2609(2.2585) | Loss 4.3291(5.0188) | Error 0.5244(0.5457) | Error Color 0.8500(0.8537) |Steps 374(393.82) | Grad Norm 1.7459(3.4249) | Total Time 0.00(0.00)\n",
      "Iter 0360 | Time 9.1203(9.0551) | Bit/dim 0.9586(1.0409) | Xent 1.6145(1.8824) | Xent Color 2.2517(2.2603) | Loss 4.2800(4.8440) | Error 0.4656(0.5309) | Error Color 0.8322(0.8534) |Steps 392(394.77) | Grad Norm 1.7925(3.4026) | Total Time 0.00(0.00)\n",
      "Iter 0370 | Time 9.2700(9.0533) | Bit/dim 0.9708(1.0214) | Xent 1.5191(1.7994) | Xent Color 2.2575(2.2619) | Loss 4.2936(4.7051) | Error 0.4278(0.5105) | Error Color 0.8567(0.8544) |Steps 386(395.35) | Grad Norm 3.7772(3.5141) | Total Time 0.00(0.00)\n",
      "Iter 0380 | Time 9.3507(9.0858) | Bit/dim 0.9647(1.0073) | Xent 1.4525(1.7180) | Xent Color 2.2652(2.2637) | Loss 4.2897(4.6043) | Error 0.4167(0.4899) | Error Color 0.8689(0.8563) |Steps 410(396.67) | Grad Norm 4.4787(3.8575) | Total Time 0.00(0.00)\n",
      "Iter 0390 | Time 8.7667(9.0795) | Bit/dim 0.9709(0.9959) | Xent 1.4169(1.6438) | Xent Color 2.2732(2.2657) | Loss 4.1628(4.5188) | Error 0.4167(0.4714) | Error Color 0.8500(0.8573) |Steps 386(394.93) | Grad Norm 6.8664(4.0107) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0006 | Time 54.9358, Epoch Time 676.3192(635.2739), Bit/dim 0.9535(best: 1.1263), Xent 1.2864, Xent Color 2.2602. Loss 1.8401, Error 0.3184(best: 0.3175), Error Color 0.8539(best: 0.8112)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0400 | Time 8.9716(9.0805) | Bit/dim 0.7931(0.9690) | Xent 1.8246(1.6254) | Xent Color 2.2937(2.2664) | Loss 4.1076(4.9187) | Error 0.6011(0.4761) | Error Color 0.8744(0.8587) |Steps 398(395.83) | Grad Norm 9.6080(4.5175) | Total Time 0.00(0.00)\n",
      "Iter 0410 | Time 9.1158(9.0802) | Bit/dim 0.8120(0.9264) | Xent 1.7444(1.6552) | Xent Color 2.2825(2.2701) | Loss 4.1759(4.7085) | Error 0.5811(0.4981) | Error Color 0.8867(0.8623) |Steps 404(395.58) | Grad Norm 15.0123(5.4817) | Total Time 0.00(0.00)\n",
      "Iter 0420 | Time 8.9538(9.0683) | Bit/dim 0.8134(0.8957) | Xent 1.6495(1.6627) | Xent Color 2.2645(2.2723) | Loss 4.0780(4.5453) | Error 0.5444(0.5123) | Error Color 0.8522(0.8654) |Steps 404(396.14) | Grad Norm 8.5995(6.3304) | Total Time 0.00(0.00)\n",
      "Iter 0430 | Time 9.4975(9.0835) | Bit/dim 0.8002(0.8731) | Xent 1.6347(1.6541) | Xent Color 2.2709(2.2733) | Loss 4.1212(4.4187) | Error 0.5144(0.5165) | Error Color 0.8644(0.8658) |Steps 404(396.51) | Grad Norm 3.7715(5.5363) | Total Time 0.00(0.00)\n",
      "Iter 0440 | Time 9.3021(9.0987) | Bit/dim 0.8216(0.8549) | Xent 1.5950(1.6488) | Xent Color 2.2727(2.2744) | Loss 4.1660(4.3326) | Error 0.5267(0.5229) | Error Color 0.8867(0.8676) |Steps 404(398.62) | Grad Norm 5.0351(5.5467) | Total Time 0.00(0.00)\n",
      "Iter 0450 | Time 8.9609(9.1033) | Bit/dim 0.7980(0.8420) | Xent 1.6316(1.6376) | Xent Color 2.2999(2.2762) | Loss 4.0454(4.2590) | Error 0.5411(0.5250) | Error Color 0.8833(0.8682) |Steps 392(396.46) | Grad Norm 4.5863(6.1189) | Total Time 0.00(0.00)\n",
      "Iter 0460 | Time 9.2452(9.1147) | Bit/dim 0.7994(0.8299) | Xent 1.5036(1.6256) | Xent Color 2.2713(2.2755) | Loss 4.0275(4.2045) | Error 0.4856(0.5236) | Error Color 0.8744(0.8684) |Steps 410(397.76) | Grad Norm 4.5462(5.8971) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0007 | Time 56.4599, Epoch Time 680.1326(636.6197), Bit/dim 0.7916(best: 0.9535), Xent 1.4995, Xent Color 2.2773. Loss 1.7358, Error 0.4609(best: 0.3175), Error Color 0.8722(best: 0.8112)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0470 | Time 9.0150(9.1183) | Bit/dim 0.7129(0.8013) | Xent 2.0501(1.7071) | Xent Color 2.2881(2.2797) | Loss 4.1039(4.6048) | Error 0.7144(0.5611) | Error Color 0.8767(0.8727) |Steps 392(399.57) | Grad Norm 16.3803(6.4910) | Total Time 0.00(0.00)\n",
      "Iter 0480 | Time 9.2765(9.1670) | Bit/dim 0.6924(0.7750) | Xent 1.9813(1.7820) | Xent Color 2.2969(2.2835) | Loss 3.9521(4.4566) | Error 0.6811(0.5950) | Error Color 0.8833(0.8764) |Steps 374(399.05) | Grad Norm 2.9323(7.2081) | Total Time 0.00(0.00)\n",
      "Iter 0490 | Time 8.8523(9.1570) | Bit/dim 0.6918(0.7544) | Xent 2.0001(1.8383) | Xent Color 2.3020(2.2853) | Loss 4.0599(4.3480) | Error 0.6989(0.6212) | Error Color 0.8956(0.8785) |Steps 392(399.79) | Grad Norm 7.1136(6.8421) | Total Time 0.00(0.00)\n",
      "Iter 0500 | Time 9.0810(9.1286) | Bit/dim 0.6970(0.7396) | Xent 1.9645(1.8723) | Xent Color 2.2928(2.2862) | Loss 4.0248(4.2616) | Error 0.6867(0.6363) | Error Color 0.8833(0.8804) |Steps 392(397.04) | Grad Norm 4.3965(6.3962) | Total Time 0.00(0.00)\n",
      "Iter 0510 | Time 9.1897(9.1447) | Bit/dim 0.7055(0.7285) | Xent 1.9789(1.9017) | Xent Color 2.2922(2.2871) | Loss 4.0497(4.2037) | Error 0.6756(0.6512) | Error Color 0.8867(0.8811) |Steps 404(397.12) | Grad Norm 6.7257(6.5316) | Total Time 0.00(0.00)\n",
      "Iter 0520 | Time 9.2220(9.1368) | Bit/dim 0.7091(0.7192) | Xent 1.9037(1.9210) | Xent Color 2.2846(2.2882) | Loss 4.0303(4.1599) | Error 0.6544(0.6607) | Error Color 0.8844(0.8828) |Steps 398(395.96) | Grad Norm 2.3757(6.4603) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0008 | Time 56.0209, Epoch Time 681.5216(637.9667), Bit/dim 0.6873(best: 0.7916), Xent 1.9199, Xent Color 2.2886. Loss 1.7394, Error 0.6582(best: 0.3175), Error Color 0.8888(best: 0.8112)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0530 | Time 9.2175(9.1045) | Bit/dim 0.6498(0.7100) | Xent 2.2064(1.9490) | Xent Color 2.2950(2.2891) | Loss 4.0559(4.6485) | Error 0.8022(0.6747) | Error Color 0.8822(0.8834) |Steps 386(394.69) | Grad Norm 8.1272(6.3924) | Total Time 0.00(0.00)\n",
      "Iter 0540 | Time 9.0724(9.1149) | Bit/dim 0.6337(0.6921) | Xent 2.2031(2.0195) | Xent Color 2.3018(2.2915) | Loss 4.0419(4.4891) | Error 0.8122(0.7073) | Error Color 0.8833(0.8862) |Steps 398(394.72) | Grad Norm 2.8805(5.7648) | Total Time 0.00(0.00)\n",
      "Iter 0550 | Time 9.1848(9.1498) | Bit/dim 0.6470(0.6804) | Xent 2.1728(2.0745) | Xent Color 2.2899(2.2938) | Loss 4.0554(4.3772) | Error 0.7833(0.7340) | Error Color 0.8833(0.8879) |Steps 398(393.47) | Grad Norm 5.2740(7.5058) | Total Time 0.00(0.00)\n",
      "Iter 0560 | Time 8.4744(9.1517) | Bit/dim 0.6534(0.6756) | Xent 2.2434(2.1159) | Xent Color 2.2964(2.2953) | Loss 3.9507(4.2984) | Error 0.8111(0.7549) | Error Color 0.8833(0.8884) |Steps 398(395.38) | Grad Norm 11.1343(10.2345) | Total Time 0.00(0.00)\n",
      "Iter 0570 | Time 9.5725(9.1961) | Bit/dim 0.6491(0.6683) | Xent 2.2281(2.1465) | Xent Color 2.3069(2.2974) | Loss 4.0772(4.2375) | Error 0.8078(0.7693) | Error Color 0.9067(0.8903) |Steps 410(397.71) | Grad Norm 5.9900(9.9858) | Total Time 0.00(0.00)\n",
      "Iter 0580 | Time 9.3209(9.2436) | Bit/dim 0.6434(0.6614) | Xent 2.1984(2.1644) | Xent Color 2.2909(2.2967) | Loss 4.0075(4.1916) | Error 0.8044(0.7798) | Error Color 0.8911(0.8902) |Steps 398(401.74) | Grad Norm 3.9767(8.8537) | Total Time 0.00(0.00)\n",
      "Iter 0590 | Time 9.7698(9.2733) | Bit/dim 0.6411(0.6560) | Xent 2.1848(2.1746) | Xent Color 2.3039(2.2968) | Loss 4.1095(4.1585) | Error 0.7911(0.7850) | Error Color 0.9022(0.8916) |Steps 398(401.72) | Grad Norm 7.5594(8.0792) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0009 | Time 58.2260, Epoch Time 692.7902(639.6114), Bit/dim 0.6373(best: 0.6873), Xent 2.1656, Xent Color 2.2947. Loss 1.7524, Error 0.7914(best: 0.3175), Error Color 0.8862(best: 0.8112)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0600 | Time 9.2694(9.2883) | Bit/dim 0.6106(0.6474) | Xent 2.2768(2.1962) | Xent Color 2.2942(2.2978) | Loss 4.0302(4.6308) | Error 0.8422(0.7969) | Error Color 0.8889(0.8920) |Steps 422(402.71) | Grad Norm 3.3671(7.4480) | Total Time 0.00(0.00)\n",
      "Iter 0610 | Time 9.7190(9.2724) | Bit/dim 0.6155(0.6384) | Xent 2.3025(2.2235) | Xent Color 2.2962(2.2981) | Loss 4.0896(4.4791) | Error 0.8644(0.8126) | Error Color 0.9022(0.8923) |Steps 380(402.70) | Grad Norm 3.5587(6.5839) | Total Time 0.00(0.00)\n",
      "Iter 0620 | Time 9.3329(9.3290) | Bit/dim 0.6058(0.6309) | Xent 2.3020(2.2428) | Xent Color 2.3087(2.2992) | Loss 4.0876(4.3762) | Error 0.8600(0.8234) | Error Color 0.9033(0.8930) |Steps 428(408.61) | Grad Norm 5.7131(5.8176) | Total Time 0.00(0.00)\n",
      "Iter 0630 | Time 9.2411(9.3554) | Bit/dim 0.6144(0.6257) | Xent 2.3125(2.2573) | Xent Color 2.2992(2.3002) | Loss 4.0641(4.3004) | Error 0.8611(0.8330) | Error Color 0.8967(0.8939) |Steps 422(409.99) | Grad Norm 6.5202(5.7201) | Total Time 0.00(0.00)\n",
      "Iter 0640 | Time 9.7114(9.3787) | Bit/dim 0.6076(0.6214) | Xent 2.2517(2.2622) | Xent Color 2.3025(2.3009) | Loss 4.0383(4.2472) | Error 0.8278(0.8367) | Error Color 0.8933(0.8942) |Steps 422(413.74) | Grad Norm 2.6303(5.0809) | Total Time 0.00(0.00)\n",
      "Iter 0650 | Time 9.2964(9.3510) | Bit/dim 0.6060(0.6183) | Xent 2.2963(2.2661) | Xent Color 2.3068(2.3014) | Loss 4.0714(4.1968) | Error 0.8800(0.8399) | Error Color 0.8856(0.8945) |Steps 428(411.57) | Grad Norm 2.7087(4.6109) | Total Time 0.00(0.00)\n",
      "Iter 0660 | Time 9.5612(9.3461) | Bit/dim 0.6132(0.6169) | Xent 2.2821(2.2690) | Xent Color 2.3105(2.3023) | Loss 4.1257(4.1659) | Error 0.8478(0.8420) | Error Color 0.9078(0.8961) |Steps 398(411.97) | Grad Norm 12.9617(5.7703) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0010 | Time 56.6586, Epoch Time 696.9375(641.3312), Bit/dim 0.6356(best: 0.6373), Xent 2.2508, Xent Color 2.2997. Loss 1.7733, Error 0.8434(best: 0.3175), Error Color 0.8957(best: 0.8112)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0670 | Time 9.3096(9.3670) | Bit/dim 0.7525(0.6352) | Xent 2.3143(2.2864) | Xent Color 2.3036(2.3044) | Loss 4.2896(4.5830) | Error 0.8644(0.8509) | Error Color 0.9144(0.8966) |Steps 404(412.70) | Grad Norm 23.8177(10.7749) | Total Time 0.00(0.00)\n",
      "Iter 0680 | Time 9.7984(9.4669) | Bit/dim 0.6468(0.6387) | Xent 2.3246(2.2997) | Xent Color 2.3103(2.3070) | Loss 4.1750(4.4824) | Error 0.8867(0.8605) | Error Color 0.9000(0.8975) |Steps 440(417.63) | Grad Norm 7.9372(10.9915) | Total Time 0.00(0.00)\n",
      "Iter 0690 | Time 9.5517(9.5326) | Bit/dim 0.6242(0.6357) | Xent 2.3388(2.3089) | Xent Color 2.3035(2.3067) | Loss 4.0890(4.4023) | Error 0.9000(0.8693) | Error Color 0.8922(0.8978) |Steps 416(423.43) | Grad Norm 4.1092(9.7950) | Total Time 0.00(0.00)\n",
      "Iter 0700 | Time 9.2621(9.5464) | Bit/dim 0.6116(0.6302) | Xent 2.3424(2.3153) | Xent Color 2.3057(2.3062) | Loss 4.1460(4.3381) | Error 0.8989(0.8748) | Error Color 0.8856(0.8966) |Steps 410(421.44) | Grad Norm 2.6553(8.2146) | Total Time 0.00(0.00)\n",
      "Iter 0710 | Time 9.4917(9.5761) | Bit/dim 0.6028(0.6237) | Xent 2.3226(2.3176) | Xent Color 2.3052(2.3066) | Loss 4.0708(4.2754) | Error 0.9022(0.8791) | Error Color 0.9022(0.8965) |Steps 434(419.60) | Grad Norm 2.8110(6.6253) | Total Time 0.00(0.00)\n",
      "Iter 0720 | Time 9.7675(9.5681) | Bit/dim 0.5960(0.6170) | Xent 2.3240(2.3196) | Xent Color 2.3044(2.3067) | Loss 4.1458(4.2345) | Error 0.8733(0.8794) | Error Color 0.8911(0.8979) |Steps 422(421.16) | Grad Norm 2.3131(5.3857) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0011 | Time 59.3190, Epoch Time 717.0023(643.6013), Bit/dim 0.5926(best: 0.6356), Xent 2.2981, Xent Color 2.3043. Loss 1.7432, Error 0.8803(best: 0.3175), Error Color 0.9002(best: 0.8112)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0730 | Time 9.3742(9.5373) | Bit/dim 0.5873(0.6106) | Xent 2.3329(2.3198) | Xent Color 2.3070(2.3066) | Loss 4.0924(4.6912) | Error 0.8800(0.8816) | Error Color 0.9144(0.8986) |Steps 434(417.62) | Grad Norm 1.0746(4.2922) | Total Time 0.00(0.00)\n",
      "Iter 0740 | Time 9.6286(9.5327) | Bit/dim 0.5821(0.6040) | Xent 2.3358(2.3215) | Xent Color 2.3046(2.3065) | Loss 4.0868(4.5308) | Error 0.9122(0.8838) | Error Color 0.8711(0.8977) |Steps 440(417.53) | Grad Norm 0.9387(3.5973) | Total Time 0.00(0.00)\n",
      "Iter 0750 | Time 9.3932(9.4760) | Bit/dim 0.5820(0.5989) | Xent 2.3303(2.3223) | Xent Color 2.3027(2.3063) | Loss 4.0267(4.4049) | Error 0.8967(0.8851) | Error Color 0.9044(0.8987) |Steps 440(418.00) | Grad Norm 2.9516(3.3910) | Total Time 0.00(0.00)\n",
      "Iter 0760 | Time 9.6831(9.4641) | Bit/dim 0.5830(0.5946) | Xent 2.3233(2.3227) | Xent Color 2.3048(2.3065) | Loss 4.1151(4.3132) | Error 0.8956(0.8886) | Error Color 0.8922(0.8986) |Steps 428(413.78) | Grad Norm 1.0415(3.1523) | Total Time 0.00(0.00)\n",
      "Iter 0770 | Time 9.6345(9.4565) | Bit/dim 0.5954(0.5915) | Xent 2.3161(2.3216) | Xent Color 2.3020(2.3063) | Loss 4.1546(4.2386) | Error 0.8678(0.8881) | Error Color 0.8989(0.8982) |Steps 428(415.24) | Grad Norm 15.1503(3.5479) | Total Time 0.00(0.00)\n",
      "Iter 0780 | Time 9.4636(9.4510) | Bit/dim 0.5960(0.5949) | Xent 2.3207(2.3207) | Xent Color 2.3097(2.3062) | Loss 3.9623(4.1970) | Error 0.8978(0.8879) | Error Color 0.8989(0.8986) |Steps 428(415.06) | Grad Norm 11.3298(6.7450) | Total Time 0.00(0.00)\n",
      "Iter 0790 | Time 9.8395(9.5119) | Bit/dim 0.5851(0.5927) | Xent 2.3115(2.3192) | Xent Color 2.3098(2.3065) | Loss 4.0549(4.1576) | Error 0.8944(0.8883) | Error Color 0.8956(0.8999) |Steps 386(412.17) | Grad Norm 7.1990(6.9636) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0012 | Time 58.2594, Epoch Time 704.9567(645.4420), Bit/dim 0.5789(best: 0.5926), Xent 2.3040, Xent Color 2.3047. Loss 1.7311, Error 0.8889(best: 0.3175), Error Color 0.9022(best: 0.8112)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0800 | Time 9.6191(9.5469) | Bit/dim 0.5736(0.5888) | Xent 2.3170(2.3187) | Xent Color 2.3037(2.3064) | Loss 4.0526(4.6065) | Error 0.8833(0.8892) | Error Color 0.8900(0.9012) |Steps 416(409.11) | Grad Norm 2.2552(6.2195) | Total Time 0.00(0.00)\n",
      "Iter 0810 | Time 8.8791(9.4851) | Bit/dim 0.5734(0.5847) | Xent 2.3282(2.3186) | Xent Color 2.3017(2.3062) | Loss 3.8731(4.4533) | Error 0.8856(0.8884) | Error Color 0.8933(0.9009) |Steps 398(411.08) | Grad Norm 1.1907(5.3039) | Total Time 0.00(0.00)\n",
      "Iter 0820 | Time 8.9762(9.4576) | Bit/dim 0.5676(0.5808) | Xent 2.3206(2.3183) | Xent Color 2.3008(2.3059) | Loss 4.0225(4.3419) | Error 0.8833(0.8891) | Error Color 0.8778(0.8998) |Steps 434(411.75) | Grad Norm 0.7898(4.3903) | Total Time 0.00(0.00)\n",
      "Iter 0830 | Time 9.9098(9.4904) | Bit/dim 0.5665(0.5770) | Xent 2.3205(2.3184) | Xent Color 2.3066(2.3059) | Loss 4.0201(4.2643) | Error 0.8944(0.8914) | Error Color 0.9211(0.9010) |Steps 380(412.78) | Grad Norm 1.4444(3.7564) | Total Time 0.00(0.00)\n",
      "Iter 0840 | Time 9.1484(9.4903) | Bit/dim 0.5970(0.5757) | Xent 2.3165(2.3180) | Xent Color 2.3046(2.3061) | Loss 4.0392(4.2090) | Error 0.8967(0.8931) | Error Color 0.8800(0.8999) |Steps 404(412.03) | Grad Norm 18.3205(5.2758) | Total Time 0.00(0.00)\n",
      "Iter 0850 | Time 9.1821(9.4624) | Bit/dim 0.5640(0.5782) | Xent 2.3235(2.3181) | Xent Color 2.3007(2.3061) | Loss 3.9307(4.1536) | Error 0.9156(0.8939) | Error Color 0.8789(0.9004) |Steps 386(410.89) | Grad Norm 4.3005(6.9418) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0013 | Time 57.0945, Epoch Time 705.3677(647.2398), Bit/dim 0.5615(best: 0.5789), Xent 2.3064, Xent Color 2.3032. Loss 1.7139, Error 0.8915(best: 0.3175), Error Color 0.9003(best: 0.8112)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0860 | Time 9.0622(9.4785) | Bit/dim 0.5546(0.5747) | Xent 2.3145(2.3175) | Xent Color 2.3033(2.3058) | Loss 3.9185(4.6714) | Error 0.8989(0.8950) | Error Color 0.9144(0.9010) |Steps 404(410.88) | Grad Norm 2.9513(6.6797) | Total Time 0.00(0.00)\n",
      "Iter 0870 | Time 9.5761(9.5016) | Bit/dim 0.5380(0.5671) | Xent 2.3047(2.3168) | Xent Color 2.3052(2.3055) | Loss 3.8762(4.4806) | Error 0.8822(0.8958) | Error Color 0.8911(0.9001) |Steps 374(408.59) | Grad Norm 1.6407(5.7651) | Total Time 0.00(0.00)\n",
      "Iter 0880 | Time 9.6848(9.4645) | Bit/dim 0.5151(0.5563) | Xent 2.3175(2.3159) | Xent Color 2.3078(2.3056) | Loss 4.0093(4.3448) | Error 0.9044(0.8956) | Error Color 0.9111(0.9008) |Steps 392(405.80) | Grad Norm 1.7250(4.9373) | Total Time 0.00(0.00)\n",
      "Iter 0890 | Time 9.9288(9.4588) | Bit/dim 0.6471(0.5724) | Xent 2.3149(2.3157) | Xent Color 2.3070(2.3057) | Loss 4.1829(4.2874) | Error 0.8922(0.8949) | Error Color 0.8944(0.9008) |Steps 428(406.44) | Grad Norm 33.8363(12.6875) | Total Time 0.00(0.00)\n",
      "Iter 0900 | Time 9.2379(9.4115) | Bit/dim 0.5385(0.5674) | Xent 2.3118(2.3146) | Xent Color 2.3033(2.3055) | Loss 3.9838(4.1950) | Error 0.8778(0.8939) | Error Color 0.8922(0.9003) |Steps 416(401.96) | Grad Norm 9.1785(12.7371) | Total Time 0.00(0.00)\n",
      "Iter 0910 | Time 9.6098(9.3900) | Bit/dim 0.5007(0.5529) | Xent 2.3111(2.3139) | Xent Color 2.3089(2.3055) | Loss 3.8712(4.1161) | Error 0.8989(0.8947) | Error Color 0.9078(0.9004) |Steps 446(404.30) | Grad Norm 5.2454(11.0081) | Total Time 0.00(0.00)\n",
      "Iter 0920 | Time 9.3503(9.3205) | Bit/dim 0.4741(0.5350) | Xent 2.3027(2.3139) | Xent Color 2.3059(2.3052) | Loss 3.8628(4.0441) | Error 0.8956(0.8952) | Error Color 0.9000(0.8997) |Steps 374(404.16) | Grad Norm 3.3708(8.9579) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0014 | Time 57.0882, Epoch Time 695.4139(648.6850), Bit/dim 0.4640(best: 0.5615), Xent 2.3064, Xent Color 2.3036. Loss 1.6165, Error 0.8933(best: 0.3175), Error Color 0.8994(best: 0.8112)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0930 | Time 9.0371(9.2526) | Bit/dim 0.4592(0.5163) | Xent 2.3137(2.3132) | Xent Color 2.3018(2.3051) | Loss 3.8963(4.4816) | Error 0.9111(0.8960) | Error Color 0.8900(0.9003) |Steps 422(403.89) | Grad Norm 5.1251(7.4259) | Total Time 0.00(0.00)\n",
      "Iter 0940 | Time 9.4668(9.2414) | Bit/dim 0.4523(0.4999) | Xent 2.3068(2.3124) | Xent Color 2.3052(2.3053) | Loss 3.8470(4.2992) | Error 0.8889(0.8953) | Error Color 0.8933(0.9009) |Steps 422(401.90) | Grad Norm 9.9808(7.7237) | Total Time 0.00(0.00)\n",
      "Iter 0950 | Time 9.0971(9.2611) | Bit/dim 0.4366(0.4842) | Xent 2.3128(2.3123) | Xent Color 2.3086(2.3053) | Loss 3.7616(4.1643) | Error 0.9167(0.8970) | Error Color 0.8978(0.8998) |Steps 422(402.82) | Grad Norm 4.2643(6.8373) | Total Time 0.00(0.00)\n",
      "Iter 0960 | Time 9.2975(9.2494) | Bit/dim 0.4798(0.4731) | Xent 2.3070(2.3119) | Xent Color 2.3053(2.3056) | Loss 3.8165(4.0684) | Error 0.9011(0.8968) | Error Color 0.9000(0.9002) |Steps 392(402.76) | Grad Norm 16.1181(6.9274) | Total Time 0.00(0.00)\n",
      "Iter 0970 | Time 9.4673(9.3011) | Bit/dim 0.6192(0.5003) | Xent 2.3050(2.3118) | Xent Color 2.3068(2.3054) | Loss 4.0717(4.0542) | Error 0.8822(0.8969) | Error Color 0.9056(0.8994) |Steps 428(404.97) | Grad Norm 16.4048(11.3346) | Total Time 0.00(0.00)\n",
      "Iter 0980 | Time 9.3154(9.3922) | Bit/dim 0.4849(0.4997) | Xent 2.3098(2.3112) | Xent Color 2.3023(2.3051) | Loss 3.9050(4.0128) | Error 0.8956(0.8970) | Error Color 0.8900(0.8987) |Steps 392(407.59) | Grad Norm 4.6794(9.9568) | Total Time 0.00(0.00)\n",
      "Iter 0990 | Time 9.3012(9.3723) | Bit/dim 0.4475(0.4903) | Xent 2.3064(2.3108) | Xent Color 2.3044(2.3052) | Loss 3.7514(3.9569) | Error 0.8967(0.8959) | Error Color 0.8978(0.9001) |Steps 410(405.02) | Grad Norm 2.6855(8.1355) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0015 | Time 53.5790, Epoch Time 692.3653(649.9954), Bit/dim 0.4467(best: 0.4640), Xent 2.3050, Xent Color 2.3045. Loss 1.5991, Error 0.8978(best: 0.3175), Error Color 0.9016(best: 0.8112)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1000 | Time 9.3066(9.3226) | Bit/dim 0.4271(0.4760) | Xent 2.3122(2.3104) | Xent Color 2.3060(2.3056) | Loss 3.7711(4.3006) | Error 0.9067(0.8962) | Error Color 0.9044(0.9009) |Steps 392(399.01) | Grad Norm 1.2019(6.6380) | Total Time 0.00(0.00)\n",
      "Iter 1010 | Time 9.2389(9.2599) | Bit/dim 0.4030(0.4591) | Xent 2.3077(2.3093) | Xent Color 2.3080(2.3054) | Loss 3.8194(4.1645) | Error 0.8767(0.8952) | Error Color 0.9033(0.8997) |Steps 410(397.44) | Grad Norm 4.5560(5.6792) | Total Time 0.00(0.00)\n",
      "Iter 1020 | Time 9.3494(9.2335) | Bit/dim 0.3801(0.4480) | Xent 2.3142(2.3100) | Xent Color 2.3058(2.3054) | Loss 3.8023(4.0635) | Error 0.8911(0.8959) | Error Color 0.8889(0.8993) |Steps 440(399.95) | Grad Norm 5.3753(8.2856) | Total Time 0.00(0.00)\n",
      "Iter 1030 | Time 10.0529(9.2581) | Bit/dim 0.3739(0.4359) | Xent 2.3098(2.3099) | Xent Color 2.3064(2.3054) | Loss 3.7901(3.9845) | Error 0.8778(0.8952) | Error Color 0.8967(0.8994) |Steps 428(401.72) | Grad Norm 7.2739(10.2715) | Total Time 0.00(0.00)\n",
      "Iter 1040 | Time 9.4256(9.2359) | Bit/dim 0.3545(0.4168) | Xent 2.3076(2.3090) | Xent Color 2.3020(2.3055) | Loss 3.7079(3.9015) | Error 0.9078(0.8953) | Error Color 0.8900(0.8991) |Steps 416(403.85) | Grad Norm 5.1806(9.5912) | Total Time 0.00(0.00)\n",
      "Iter 1050 | Time 9.7844(9.2832) | Bit/dim 0.3306(0.3963) | Xent 2.3104(2.3083) | Xent Color 2.3034(2.3051) | Loss 3.6211(3.8258) | Error 0.8822(0.8941) | Error Color 0.8978(0.8969) |Steps 380(402.07) | Grad Norm 4.9844(8.3920) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0016 | Time 57.3415, Epoch Time 689.5480(651.1820), Bit/dim 0.3220(best: 0.4467), Xent 2.3043, Xent Color 2.3051. Loss 1.4743, Error 0.8944(best: 0.3175), Error Color 0.9015(best: 0.8112)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1060 | Time 9.6928(9.3190) | Bit/dim 0.3188(0.3772) | Xent 2.3059(2.3084) | Xent Color 2.3107(2.3054) | Loss 3.5970(4.2756) | Error 0.8900(0.8937) | Error Color 0.9144(0.8979) |Steps 410(399.17) | Grad Norm 6.8562(7.9489) | Total Time 0.00(0.00)\n",
      "Iter 1070 | Time 10.1691(9.3107) | Bit/dim 0.3067(0.3592) | Xent 2.3073(2.3081) | Xent Color 2.3041(2.3052) | Loss 3.5604(4.0897) | Error 0.9044(0.8939) | Error Color 0.9000(0.8984) |Steps 380(399.90) | Grad Norm 9.8206(7.2426) | Total Time 0.00(0.00)\n",
      "Iter 1080 | Time 8.9480(9.2815) | Bit/dim 0.4210(0.3675) | Xent 2.3092(2.3084) | Xent Color 2.3062(2.3051) | Loss 3.7112(3.9733) | Error 0.8978(0.8946) | Error Color 0.8744(0.8979) |Steps 380(399.08) | Grad Norm 24.5316(11.9277) | Total Time 0.00(0.00)\n",
      "Iter 1090 | Time 8.2409(9.1913) | Bit/dim 0.3498(0.3711) | Xent 2.3074(2.3074) | Xent Color 2.3067(2.3052) | Loss 3.4694(3.8808) | Error 0.9111(0.8940) | Error Color 0.9056(0.8979) |Steps 356(396.40) | Grad Norm 9.4180(13.3596) | Total Time 0.00(0.00)\n",
      "Iter 1100 | Time 9.3167(9.1671) | Bit/dim 0.3131(0.3594) | Xent 2.3051(2.3078) | Xent Color 2.3033(2.3052) | Loss 3.5015(3.7809) | Error 0.8833(0.8944) | Error Color 0.8844(0.8986) |Steps 380(393.07) | Grad Norm 3.9510(11.4273) | Total Time 0.00(0.00)\n",
      "Iter 1110 | Time 9.4034(9.1960) | Bit/dim 0.2842(0.3431) | Xent 2.3026(2.3076) | Xent Color 2.3053(2.3055) | Loss 3.4777(3.6932) | Error 0.8789(0.8944) | Error Color 0.9078(0.9003) |Steps 380(391.48) | Grad Norm 2.0904(9.3705) | Total Time 0.00(0.00)\n",
      "Iter 1120 | Time 8.7661(9.1503) | Bit/dim 0.2625(0.3244) | Xent 2.3124(2.3077) | Xent Color 2.3043(2.3054) | Loss 3.3955(3.6201) | Error 0.8956(0.8958) | Error Color 0.8756(0.9005) |Steps 404(391.45) | Grad Norm 2.2593(7.6119) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0017 | Time 56.8171, Epoch Time 684.2465(652.1739), Bit/dim 0.2538(best: 0.3220), Xent 2.3034, Xent Color 2.3034. Loss 1.4055, Error 0.8919(best: 0.3175), Error Color 0.9003(best: 0.8112)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1130 | Time 8.6541(9.1011) | Bit/dim 0.2378(0.3042) | Xent 2.3069(2.3074) | Xent Color 2.3069(2.3052) | Loss 3.3589(4.0038) | Error 0.8867(0.8955) | Error Color 0.9200(0.9006) |Steps 398(392.01) | Grad Norm 2.7059(6.2430) | Total Time 0.00(0.00)\n",
      "Iter 1140 | Time 9.1786(9.1570) | Bit/dim 0.2372(0.2862) | Xent 2.3037(2.3069) | Xent Color 2.3057(2.3051) | Loss 3.3559(3.8307) | Error 0.8867(0.8955) | Error Color 0.8889(0.8995) |Steps 410(392.20) | Grad Norm 16.8414(7.2631) | Total Time 0.00(0.00)\n",
      "Iter 1150 | Time 8.7698(9.1209) | Bit/dim 0.2367(0.2732) | Xent 2.2990(2.3066) | Xent Color 2.3053(2.3048) | Loss 3.3322(3.7093) | Error 0.8922(0.8951) | Error Color 0.8967(0.8992) |Steps 380(391.44) | Grad Norm 15.7574(9.5082) | Total Time 0.00(0.00)\n",
      "Iter 1160 | Time 9.3542(9.0933) | Bit/dim 0.2654(0.2718) | Xent 2.3092(2.3064) | Xent Color 2.3042(2.3053) | Loss 3.4143(3.6309) | Error 0.9011(0.8958) | Error Color 0.8889(0.9002) |Steps 374(390.91) | Grad Norm 10.8359(12.4475) | Total Time 0.00(0.00)\n",
      "Iter 1170 | Time 8.8402(9.0749) | Bit/dim 0.2168(0.2584) | Xent 2.3047(2.3067) | Xent Color 2.3038(2.3051) | Loss 3.3034(3.5520) | Error 0.8789(0.8955) | Error Color 0.8922(0.9003) |Steps 410(389.25) | Grad Norm 6.8042(11.4216) | Total Time 0.00(0.00)\n",
      "Iter 1180 | Time 9.0636(9.0158) | Bit/dim 0.1951(0.2435) | Xent 2.3029(2.3068) | Xent Color 2.3033(2.3049) | Loss 3.2946(3.4787) | Error 0.8867(0.8958) | Error Color 0.9022(0.9001) |Steps 386(391.01) | Grad Norm 6.2492(10.0292) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0018 | Time 54.1532, Epoch Time 673.7752(652.8220), Bit/dim 0.2158(best: 0.2538), Xent 2.3016, Xent Color 2.3043. Loss 1.3673, Error 0.8912(best: 0.3175), Error Color 0.9022(best: 0.8112)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1190 | Time 9.2014(9.0556) | Bit/dim 0.2058(0.2326) | Xent 2.3080(2.3068) | Xent Color 2.3031(2.3052) | Loss 3.3078(3.9252) | Error 0.8911(0.8964) | Error Color 0.8911(0.8994) |Steps 386(392.99) | Grad Norm 13.1123(9.9464) | Total Time 0.00(0.00)\n",
      "Iter 1200 | Time 8.6335(9.0340) | Bit/dim 0.1866(0.2219) | Xent 2.3131(2.3074) | Xent Color 2.3005(2.3048) | Loss 3.2078(3.7504) | Error 0.9100(0.8968) | Error Color 0.9000(0.8987) |Steps 374(391.21) | Grad Norm 8.2280(9.4173) | Total Time 0.00(0.00)\n",
      "Iter 1210 | Time 9.7220(9.0704) | Bit/dim 0.1694(0.2097) | Xent 2.3057(2.3070) | Xent Color 2.3018(2.3048) | Loss 3.1847(3.6145) | Error 0.8878(0.8958) | Error Color 0.9056(0.9000) |Steps 416(390.29) | Grad Norm 3.4065(8.2512) | Total Time 0.00(0.00)\n",
      "Iter 1220 | Time 9.5103(9.0729) | Bit/dim 0.1660(0.1987) | Xent 2.3055(2.3066) | Xent Color 2.3067(2.3047) | Loss 3.2841(3.5111) | Error 0.8933(0.8956) | Error Color 0.8978(0.8990) |Steps 440(389.43) | Grad Norm 3.7295(7.0519) | Total Time 0.00(0.00)\n",
      "Iter 1230 | Time 8.6772(9.0566) | Bit/dim 0.1646(0.1891) | Xent 2.3045(2.3065) | Xent Color 2.3066(2.3044) | Loss 3.2236(3.4326) | Error 0.8856(0.8956) | Error Color 0.8933(0.8978) |Steps 392(389.11) | Grad Norm 5.0147(6.1825) | Total Time 0.00(0.00)\n",
      "Iter 1240 | Time 9.4811(9.0545) | Bit/dim 0.1530(0.1813) | Xent 2.3020(2.3064) | Xent Color 2.3045(2.3042) | Loss 3.2394(3.3812) | Error 0.8867(0.8943) | Error Color 0.8889(0.8964) |Steps 398(390.63) | Grad Norm 4.0272(6.0000) | Total Time 0.00(0.00)\n",
      "Iter 1250 | Time 9.2857(9.1099) | Bit/dim 0.2213(0.1780) | Xent 2.3145(2.3069) | Xent Color 2.3019(2.3043) | Loss 3.3309(3.3432) | Error 0.9033(0.8944) | Error Color 0.8844(0.8977) |Steps 422(394.64) | Grad Norm 25.6296(7.3173) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0019 | Time 54.7418, Epoch Time 677.7171(653.5688), Bit/dim 0.1933(best: 0.2158), Xent 2.3037, Xent Color 2.3044. Loss 1.3453, Error 0.8891(best: 0.3175), Error Color 0.9011(best: 0.8112)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1260 | Time 8.7194(9.0843) | Bit/dim 0.3455(0.1999) | Xent 2.3036(2.3063) | Xent Color 2.3029(2.3041) | Loss 3.5618(3.8023) | Error 0.8933(0.8940) | Error Color 0.8956(0.8979) |Steps 392(395.54) | Grad Norm 17.9581(11.1075) | Total Time 0.00(0.00)\n",
      "Iter 1270 | Time 8.7957(9.0288) | Bit/dim 0.2067(0.2076) | Xent 2.3005(2.3054) | Xent Color 2.3087(2.3046) | Loss 3.2745(3.6677) | Error 0.9056(0.8940) | Error Color 0.9089(0.8986) |Steps 392(389.42) | Grad Norm 9.0713(11.4635) | Total Time 0.00(0.00)\n",
      "Iter 1280 | Time 8.9966(9.0180) | Bit/dim 0.1791(0.2019) | Xent 2.3104(2.3062) | Xent Color 2.3025(2.3048) | Loss 3.2383(3.5466) | Error 0.8989(0.8959) | Error Color 0.8933(0.8974) |Steps 392(388.24) | Grad Norm 5.9553(10.4445) | Total Time 0.00(0.00)\n",
      "Iter 1290 | Time 8.9516(9.0187) | Bit/dim 0.1594(0.1929) | Xent 2.3038(2.3060) | Xent Color 2.3067(2.3049) | Loss 3.1843(3.4475) | Error 0.8800(0.8946) | Error Color 0.9044(0.8976) |Steps 392(388.30) | Grad Norm 4.2596(9.0662) | Total Time 0.00(0.00)\n",
      "Iter 1300 | Time 9.2104(8.9876) | Bit/dim 0.1505(0.1829) | Xent 2.3057(2.3057) | Xent Color 2.3044(2.3050) | Loss 3.1702(3.3728) | Error 0.8922(0.8960) | Error Color 0.9022(0.8999) |Steps 392(388.61) | Grad Norm 2.3309(7.5536) | Total Time 0.00(0.00)\n",
      "Iter 1310 | Time 9.2805(9.0152) | Bit/dim 0.1434(0.1735) | Xent 2.3079(2.3062) | Xent Color 2.3055(2.3048) | Loss 3.2863(3.3190) | Error 0.9056(0.8967) | Error Color 0.8844(0.8997) |Steps 434(387.95) | Grad Norm 1.4751(6.3782) | Total Time 0.00(0.00)\n",
      "Iter 1320 | Time 8.2346(9.0021) | Bit/dim 0.1392(0.1653) | Xent 2.3064(2.3060) | Xent Color 2.3011(2.3043) | Loss 3.1316(3.2786) | Error 0.9056(0.8962) | Error Color 0.8844(0.8980) |Steps 374(387.41) | Grad Norm 4.3183(5.3287) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0020 | Time 55.1130, Epoch Time 669.3259(654.0415), Bit/dim 0.1425(best: 0.1933), Xent 2.3018, Xent Color 2.3035. Loss 1.2938, Error 0.8892(best: 0.3175), Error Color 0.9023(best: 0.8112)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1330 | Time 8.7966(9.0791) | Bit/dim 0.1374(0.1596) | Xent 2.3089(2.3063) | Xent Color 2.3047(2.3044) | Loss 3.0559(3.6280) | Error 0.8967(0.8967) | Error Color 0.8956(0.8976) |Steps 356(386.28) | Grad Norm 4.7102(5.9092) | Total Time 0.00(0.00)\n",
      "Iter 1340 | Time 8.8607(9.1134) | Bit/dim 0.1353(0.1536) | Xent 2.3045(2.3058) | Xent Color 2.3042(2.3046) | Loss 3.0911(3.4918) | Error 0.8933(0.8947) | Error Color 0.9056(0.8985) |Steps 380(383.96) | Grad Norm 6.1759(5.7529) | Total Time 0.00(0.00)\n",
      "Iter 1350 | Time 9.6097(9.1554) | Bit/dim 0.1351(0.1498) | Xent 2.3044(2.3062) | Xent Color 2.3042(2.3047) | Loss 3.1190(3.3984) | Error 0.9033(0.8950) | Error Color 0.8933(0.9002) |Steps 410(385.95) | Grad Norm 6.2348(6.3728) | Total Time 0.00(0.00)\n",
      "Iter 1360 | Time 8.6758(9.1292) | Bit/dim 0.1461(0.1524) | Xent 2.3027(2.3061) | Xent Color 2.3029(2.3047) | Loss 3.1295(3.3385) | Error 0.8844(0.8949) | Error Color 0.9078(0.9011) |Steps 368(385.05) | Grad Norm 8.8740(8.4425) | Total Time 0.00(0.00)\n",
      "Iter 1370 | Time 8.8958(9.1120) | Bit/dim 0.1656(0.1539) | Xent 2.3075(2.3062) | Xent Color 2.3071(2.3049) | Loss 3.1972(3.2955) | Error 0.9000(0.8955) | Error Color 0.9100(0.9011) |Steps 380(382.62) | Grad Norm 11.7732(9.3048) | Total Time 0.00(0.00)\n",
      "Iter 1380 | Time 9.2307(9.1082) | Bit/dim 0.2203(0.1656) | Xent 2.3039(2.3060) | Xent Color 2.3044(2.3047) | Loss 3.2750(3.2782) | Error 0.8733(0.8940) | Error Color 0.8978(0.9008) |Steps 374(382.82) | Grad Norm 23.1081(11.9368) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0021 | Time 53.6201, Epoch Time 681.5936(654.8681), Bit/dim 0.1665(best: 0.1425), Xent 2.3054, Xent Color 2.3046. Loss 1.3190, Error 0.8996(best: 0.3175), Error Color 0.9003(best: 0.8112)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1390 | Time 8.9628(9.1021) | Bit/dim 0.1947(0.1731) | Xent 2.3045(2.3062) | Xent Color 2.3018(2.3046) | Loss 3.2587(3.7284) | Error 0.9011(0.8950) | Error Color 0.9111(0.9010) |Steps 410(384.28) | Grad Norm 13.6234(13.5547) | Total Time 0.00(0.00)\n",
      "Iter 1400 | Time 9.4272(9.1474) | Bit/dim 0.1543(0.1681) | Xent 2.3038(2.3057) | Xent Color 2.3051(2.3044) | Loss 3.2267(3.5848) | Error 0.8944(0.8959) | Error Color 0.8978(0.9000) |Steps 416(388.68) | Grad Norm 6.7400(12.0155) | Total Time 0.00(0.00)\n",
      "Iter 1410 | Time 9.0620(9.1740) | Bit/dim 0.1354(0.1609) | Xent 2.3070(2.3054) | Xent Color 2.3034(2.3042) | Loss 3.1188(3.4636) | Error 0.9022(0.8956) | Error Color 0.9100(0.9001) |Steps 380(388.12) | Grad Norm 2.8478(10.2374) | Total Time 0.00(0.00)\n",
      "Iter 1420 | Time 9.2272(9.1217) | Bit/dim 0.1263(0.1531) | Xent 2.3027(2.3057) | Xent Color 2.3085(2.3045) | Loss 3.1352(3.3697) | Error 0.9000(0.8959) | Error Color 0.8978(0.8998) |Steps 398(385.46) | Grad Norm 2.6722(8.5062) | Total Time 0.00(0.00)\n",
      "Iter 1430 | Time 8.7469(9.0893) | Bit/dim 0.1271(0.1462) | Xent 2.3039(2.3054) | Xent Color 2.3035(2.3045) | Loss 3.1214(3.2996) | Error 0.8722(0.8957) | Error Color 0.9000(0.9009) |Steps 380(382.77) | Grad Norm 2.1391(6.8365) | Total Time 0.00(0.00)\n",
      "Iter 1440 | Time 9.4464(9.1044) | Bit/dim 0.1208(0.1398) | Xent 2.2982(2.3052) | Xent Color 2.3014(2.3046) | Loss 3.0500(3.2472) | Error 0.8711(0.8948) | Error Color 0.8878(0.9016) |Steps 380(383.98) | Grad Norm 3.6246(5.7535) | Total Time 0.00(0.00)\n",
      "Iter 1450 | Time 8.8556(9.1051) | Bit/dim 0.1220(0.1343) | Xent 2.3088(2.3059) | Xent Color 2.3021(2.3044) | Loss 3.0723(3.2068) | Error 0.9033(0.8952) | Error Color 0.8889(0.9001) |Steps 410(384.94) | Grad Norm 5.0757(5.0368) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0022 | Time 55.8064, Epoch Time 680.8366(655.6471), Bit/dim 0.1179(best: 0.1425), Xent 2.3029, Xent Color 2.3039. Loss 1.2696, Error 0.8910(best: 0.3175), Error Color 0.9044(best: 0.8112)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1460 | Time 9.0341(9.1147) | Bit/dim 0.1255(0.1299) | Xent 2.3039(2.3059) | Xent Color 2.3054(2.3044) | Loss 3.0973(3.5906) | Error 0.8878(0.8954) | Error Color 0.9100(0.9007) |Steps 410(385.49) | Grad Norm 13.3057(4.8877) | Total Time 0.00(0.00)\n",
      "Iter 1470 | Time 8.8547(9.1540) | Bit/dim 0.3849(0.1559) | Xent 2.3061(2.3059) | Xent Color 2.3038(2.3043) | Loss 3.6406(3.5242) | Error 0.9044(0.8944) | Error Color 0.8967(0.9002) |Steps 392(389.84) | Grad Norm 20.7630(10.7634) | Total Time 0.00(0.00)\n",
      "Iter 1480 | Time 8.9358(9.0943) | Bit/dim 0.3164(0.2036) | Xent 2.3008(2.3062) | Xent Color 2.3039(2.3044) | Loss 3.4795(3.5245) | Error 0.8711(0.8941) | Error Color 0.8956(0.8996) |Steps 392(389.85) | Grad Norm 10.7841(10.8269) | Total Time 0.00(0.00)\n",
      "Iter 1490 | Time 9.4363(9.0373) | Bit/dim 0.2687(0.2243) | Xent 2.3034(2.3061) | Xent Color 2.3022(2.3044) | Loss 3.3975(3.4925) | Error 0.9033(0.8940) | Error Color 0.9000(0.8996) |Steps 422(385.71) | Grad Norm 6.0107(9.6828) | Total Time 0.00(0.00)\n",
      "Iter 1500 | Time 8.6809(8.9628) | Bit/dim 0.2322(0.2301) | Xent 2.3042(2.3058) | Xent Color 2.3043(2.3043) | Loss 3.2649(3.4522) | Error 0.8856(0.8939) | Error Color 0.8778(0.8974) |Steps 374(383.03) | Grad Norm 2.1175(8.1412) | Total Time 0.00(0.00)\n",
      "Iter 1510 | Time 9.0155(8.9351) | Bit/dim 0.2091(0.2272) | Xent 2.3050(2.3054) | Xent Color 2.3014(2.3042) | Loss 3.2586(3.4048) | Error 0.9056(0.8938) | Error Color 0.8844(0.8978) |Steps 380(381.12) | Grad Norm 3.3409(6.8882) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0023 | Time 54.0675, Epoch Time 665.7778(655.9511), Bit/dim 0.1930(best: 0.1179), Xent 2.3029, Xent Color 2.3031. Loss 1.3445, Error 0.8927(best: 0.3175), Error Color 0.8967(best: 0.8112)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1520 | Time 9.1678(8.8638) | Bit/dim 0.1911(0.2197) | Xent 2.3139(2.3058) | Xent Color 2.3010(2.3041) | Loss 3.2037(3.8473) | Error 0.8989(0.8943) | Error Color 0.9022(0.8994) |Steps 392(380.55) | Grad Norm 5.2652(6.0765) | Total Time 0.00(0.00)\n",
      "Iter 1530 | Time 8.6147(8.8753) | Bit/dim 0.1916(0.2114) | Xent 2.3074(2.3062) | Xent Color 2.3024(2.3040) | Loss 3.1529(3.6787) | Error 0.9044(0.8951) | Error Color 0.8889(0.9000) |Steps 368(380.51) | Grad Norm 10.5322(6.2442) | Total Time 0.00(0.00)\n",
      "Iter 1540 | Time 9.5168(8.9574) | Bit/dim 0.1835(0.2038) | Xent 2.3070(2.3059) | Xent Color 2.3026(2.3039) | Loss 3.2498(3.5629) | Error 0.8956(0.8946) | Error Color 0.9011(0.8997) |Steps 380(381.87) | Grad Norm 11.5891(6.9579) | Total Time 0.00(0.00)\n",
      "Iter 1550 | Time 8.8047(8.9459) | Bit/dim 0.1894(0.2018) | Xent 2.3019(2.3056) | Xent Color 2.3023(2.3039) | Loss 3.1974(3.4824) | Error 0.8867(0.8944) | Error Color 0.9044(0.8998) |Steps 380(382.83) | Grad Norm 13.2701(9.1772) | Total Time 0.00(0.00)\n",
      "Iter 1560 | Time 9.0700(8.9483) | Bit/dim 0.1710(0.1953) | Xent 2.3056(2.3054) | Xent Color 2.2996(2.3040) | Loss 3.1967(3.4082) | Error 0.9011(0.8951) | Error Color 0.8811(0.8992) |Steps 350(381.73) | Grad Norm 6.4271(9.3305) | Total Time 0.00(0.00)\n",
      "Iter 1570 | Time 8.8637(8.9271) | Bit/dim 0.1633(0.1908) | Xent 2.3037(2.3055) | Xent Color 2.3052(2.3043) | Loss 3.1675(3.3501) | Error 0.9078(0.8954) | Error Color 0.9033(0.8996) |Steps 410(382.45) | Grad Norm 6.8262(9.9876) | Total Time 0.00(0.00)\n",
      "Iter 1580 | Time 8.9118(8.8906) | Bit/dim 0.1585(0.1834) | Xent 2.3074(2.3053) | Xent Color 2.3044(2.3044) | Loss 3.1584(3.2952) | Error 0.9056(0.8961) | Error Color 0.8989(0.8992) |Steps 380(381.05) | Grad Norm 6.2495(9.5359) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0024 | Time 53.4483, Epoch Time 666.0977(656.2555), Bit/dim 0.1507(best: 0.1179), Xent 2.3028, Xent Color 2.3032. Loss 1.3022, Error 0.8883(best: 0.3175), Error Color 0.8999(best: 0.8112)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1590 | Time 9.1156(8.8845) | Bit/dim 0.1482(0.1753) | Xent 2.3069(2.3056) | Xent Color 2.3033(2.3044) | Loss 3.1526(3.6929) | Error 0.9011(0.8962) | Error Color 0.8800(0.9004) |Steps 410(382.95) | Grad Norm 2.6406(8.3906) | Total Time 0.00(0.00)\n",
      "Iter 1600 | Time 8.9826(8.9402) | Bit/dim 0.1426(0.1675) | Xent 2.3051(2.3057) | Xent Color 2.3019(2.3045) | Loss 3.1332(3.5505) | Error 0.8933(0.8963) | Error Color 0.9022(0.9013) |Steps 368(383.74) | Grad Norm 6.0852(7.4532) | Total Time 0.00(0.00)\n",
      "Iter 1610 | Time 8.5900(8.9574) | Bit/dim 0.1395(0.1606) | Xent 2.3039(2.3057) | Xent Color 2.3041(2.3044) | Loss 3.0624(3.4427) | Error 0.8944(0.8958) | Error Color 0.8933(0.9003) |Steps 386(383.48) | Grad Norm 1.9749(6.7517) | Total Time 0.00(0.00)\n",
      "Iter 1620 | Time 8.2138(8.9405) | Bit/dim 0.1404(0.1550) | Xent 2.3072(2.3057) | Xent Color 2.3059(2.3042) | Loss 3.0379(3.3581) | Error 0.9022(0.8957) | Error Color 0.9022(0.8985) |Steps 356(382.37) | Grad Norm 9.8782(7.0036) | Total Time 0.00(0.00)\n",
      "Iter 1630 | Time 8.9828(8.9665) | Bit/dim 0.2198(0.1568) | Xent 2.3008(2.3054) | Xent Color 2.3061(2.3043) | Loss 3.2389(3.3208) | Error 0.8911(0.8944) | Error Color 0.9056(0.8984) |Steps 374(385.47) | Grad Norm 34.0603(9.5343) | Total Time 0.00(0.00)\n",
      "Iter 1640 | Time 8.8776(9.0083) | Bit/dim 0.2039(0.1787) | Xent 2.3065(2.3057) | Xent Color 2.3064(2.3044) | Loss 3.1925(3.3133) | Error 0.9011(0.8949) | Error Color 0.8989(0.8986) |Steps 410(385.24) | Grad Norm 9.5112(12.1819) | Total Time 0.00(0.00)\n",
      "Iter 1650 | Time 9.1679(9.0701) | Bit/dim 0.1649(0.1821) | Xent 2.3068(2.3057) | Xent Color 2.3045(2.3043) | Loss 3.2643(3.2927) | Error 0.8989(0.8947) | Error Color 0.9089(0.8992) |Steps 422(387.83) | Grad Norm 7.0527(12.4344) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0025 | Time 59.8878, Epoch Time 679.9172(656.9653), Bit/dim 0.1725(best: 0.1179), Xent 2.3027, Xent Color 2.3033. Loss 1.3240, Error 0.8911(best: 0.3175), Error Color 0.9021(best: 0.8112)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1660 | Time 8.6938(9.0495) | Bit/dim 0.1485(0.1753) | Xent 2.3035(2.3057) | Xent Color 2.3061(2.3042) | Loss 3.1552(3.7233) | Error 0.8878(0.8937) | Error Color 0.9067(0.8993) |Steps 374(389.47) | Grad Norm 5.1494(11.2270) | Total Time 0.00(0.00)\n",
      "Iter 1670 | Time 9.2485(9.0867) | Bit/dim 0.1336(0.1662) | Xent 2.3053(2.3054) | Xent Color 2.3038(2.3041) | Loss 3.1503(3.5846) | Error 0.9033(0.8924) | Error Color 0.8956(0.8988) |Steps 386(388.97) | Grad Norm 4.0926(9.6292) | Total Time 0.00(0.00)\n",
      "Iter 1680 | Time 9.3977(9.1132) | Bit/dim 0.1294(0.1573) | Xent 2.3121(2.3055) | Xent Color 2.3023(2.3039) | Loss 3.1680(3.4799) | Error 0.8989(0.8937) | Error Color 0.8844(0.8979) |Steps 380(390.88) | Grad Norm 3.6815(8.0058) | Total Time 0.00(0.00)\n",
      "Iter 1690 | Time 9.2299(9.1298) | Bit/dim 0.1210(0.1491) | Xent 2.3063(2.3056) | Xent Color 2.3048(2.3043) | Loss 3.1979(3.4028) | Error 0.9167(0.8933) | Error Color 0.9089(0.8998) |Steps 404(391.68) | Grad Norm 3.0415(6.5938) | Total Time 0.00(0.00)\n",
      "Iter 1700 | Time 8.6695(9.1295) | Bit/dim 0.1214(0.1424) | Xent 2.3046(2.3057) | Xent Color 2.3034(2.3042) | Loss 3.0329(3.3353) | Error 0.9044(0.8936) | Error Color 0.9100(0.9007) |Steps 356(389.04) | Grad Norm 1.2875(5.5421) | Total Time 0.00(0.00)\n",
      "Iter 1710 | Time 8.6403(9.1498) | Bit/dim 0.1203(0.1364) | Xent 2.3025(2.3053) | Xent Color 2.3038(2.3042) | Loss 3.0451(3.2835) | Error 0.8933(0.8929) | Error Color 0.8989(0.9017) |Steps 386(391.22) | Grad Norm 8.6138(5.1630) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0026 | Time 56.2548, Epoch Time 684.0495(657.7778), Bit/dim 0.1310(best: 0.1179), Xent 2.3038, Xent Color 2.3037. Loss 1.2829, Error 0.8971(best: 0.3175), Error Color 0.8980(best: 0.8112)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1720 | Time 9.0546(9.1642) | Bit/dim 0.1221(0.1350) | Xent 2.3073(2.3054) | Xent Color 2.3059(2.3044) | Loss 3.0911(3.7172) | Error 0.8944(0.8927) | Error Color 0.9089(0.9014) |Steps 368(392.96) | Grad Norm 8.8528(7.0106) | Total Time 0.00(0.00)\n",
      "Iter 1730 | Time 9.3156(9.2161) | Bit/dim 0.2217(0.1428) | Xent 2.3039(2.3055) | Xent Color 2.3019(2.3041) | Loss 3.4106(3.5903) | Error 0.8967(0.8932) | Error Color 0.8844(0.9007) |Steps 410(392.48) | Grad Norm 16.9330(9.7688) | Total Time 0.00(0.00)\n",
      "Iter 1740 | Time 9.0614(9.2172) | Bit/dim 0.1471(0.1504) | Xent 2.3027(2.3054) | Xent Color 2.2994(2.3039) | Loss 3.1088(3.4880) | Error 0.8944(0.8930) | Error Color 0.8822(0.8985) |Steps 374(391.12) | Grad Norm 12.9214(11.4084) | Total Time 0.00(0.00)\n",
      "Iter 1750 | Time 8.8634(9.1929) | Bit/dim 0.1347(0.1474) | Xent 2.2990(2.3047) | Xent Color 2.3015(2.3039) | Loss 3.0839(3.3844) | Error 0.8856(0.8931) | Error Color 0.8900(0.8993) |Steps 386(387.49) | Grad Norm 7.5462(10.5899) | Total Time 0.00(0.00)\n",
      "Iter 1760 | Time 8.8424(9.2270) | Bit/dim 0.1218(0.1418) | Xent 2.3061(2.3048) | Xent Color 2.3058(2.3041) | Loss 3.0725(3.3082) | Error 0.8811(0.8926) | Error Color 0.9022(0.9001) |Steps 374(386.62) | Grad Norm 6.5612(9.3676) | Total Time 0.00(0.00)\n",
      "Iter 1770 | Time 9.5389(9.2337) | Bit/dim 0.1191(0.1365) | Xent 2.3048(2.3048) | Xent Color 2.3062(2.3041) | Loss 3.1829(3.2619) | Error 0.8911(0.8920) | Error Color 0.8944(0.8995) |Steps 434(389.92) | Grad Norm 4.3761(8.0525) | Total Time 0.00(0.00)\n",
      "Iter 1780 | Time 9.0076(9.2341) | Bit/dim 0.1120(0.1311) | Xent 2.3046(2.3052) | Xent Color 2.3074(2.3040) | Loss 3.0714(3.2205) | Error 0.8833(0.8922) | Error Color 0.8844(0.8992) |Steps 374(390.61) | Grad Norm 5.0005(6.9000) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0027 | Time 59.7770, Epoch Time 693.2521(658.8421), Bit/dim 0.1141(best: 0.1179), Xent 2.3030, Xent Color 2.3036. Loss 1.2658, Error 0.8885(best: 0.3175), Error Color 0.8997(best: 0.8112)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1790 | Time 9.4417(9.2404) | Bit/dim 0.1150(0.1264) | Xent 2.3067(2.3055) | Xent Color 2.2999(2.3035) | Loss 3.1551(3.6236) | Error 0.9111(0.8935) | Error Color 0.9011(0.8979) |Steps 398(394.59) | Grad Norm 6.5941(6.4039) | Total Time 0.00(0.00)\n",
      "Iter 1800 | Time 9.5498(9.2844) | Bit/dim 0.1138(0.1228) | Xent 2.3039(2.3055) | Xent Color 2.3043(2.3036) | Loss 3.1172(3.4978) | Error 0.8933(0.8942) | Error Color 0.8978(0.8980) |Steps 392(397.97) | Grad Norm 7.6519(6.1949) | Total Time 0.00(0.00)\n",
      "Iter 1810 | Time 9.7351(9.2943) | Bit/dim 0.1481(0.1226) | Xent 2.3066(2.3059) | Xent Color 2.3039(2.3040) | Loss 3.2697(3.4109) | Error 0.8889(0.8948) | Error Color 0.9000(0.9002) |Steps 422(401.04) | Grad Norm 17.4971(7.6471) | Total Time 0.00(0.00)\n",
      "Iter 1820 | Time 9.5638(9.3303) | Bit/dim 0.1630(0.1493) | Xent 2.3083(2.3055) | Xent Color 2.3051(2.3040) | Loss 3.3156(3.4008) | Error 0.9122(0.8949) | Error Color 0.8944(0.8996) |Steps 422(402.84) | Grad Norm 9.8515(10.9266) | Total Time 0.00(0.00)\n",
      "Iter 1830 | Time 9.5962(9.3962) | Bit/dim 0.1849(0.1700) | Xent 2.3014(2.3054) | Xent Color 2.3040(2.3044) | Loss 3.2072(3.3857) | Error 0.8956(0.8942) | Error Color 0.9022(0.9010) |Steps 404(402.79) | Grad Norm 11.1957(12.2282) | Total Time 0.00(0.00)\n",
      "Iter 1840 | Time 9.5281(9.4277) | Bit/dim 0.1383(0.1659) | Xent 2.3022(2.3052) | Xent Color 2.3056(2.3042) | Loss 3.2006(3.3440) | Error 0.8889(0.8940) | Error Color 0.9222(0.9017) |Steps 422(403.53) | Grad Norm 4.4624(10.9557) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0028 | Time 61.7059, Epoch Time 706.5394(660.2730), Bit/dim 0.1265(best: 0.1141), Xent 2.3035, Xent Color 2.3033. Loss 1.2782, Error 0.8926(best: 0.3175), Error Color 0.8990(best: 0.8112)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1850 | Time 9.9712(9.4671) | Bit/dim 0.1233(0.1571) | Xent 2.3031(2.3052) | Xent Color 2.3024(2.3037) | Loss 3.2508(3.8821) | Error 0.8756(0.8938) | Error Color 0.9089(0.9020) |Steps 392(405.61) | Grad Norm 2.2571(9.1524) | Total Time 0.00(0.00)\n",
      "Iter 1860 | Time 9.4164(9.4707) | Bit/dim 0.1175(0.1472) | Xent 2.3015(2.3051) | Xent Color 2.3058(2.3040) | Loss 3.2235(3.7043) | Error 0.8911(0.8927) | Error Color 0.9044(0.9019) |Steps 404(406.34) | Grad Norm 2.7856(7.5377) | Total Time 0.00(0.00)\n",
      "Iter 1870 | Time 9.3073(9.4540) | Bit/dim 0.1108(0.1381) | Xent 2.3034(2.3054) | Xent Color 2.3027(2.3040) | Loss 3.2107(3.5736) | Error 0.8889(0.8926) | Error Color 0.8833(0.9015) |Steps 380(406.24) | Grad Norm 2.0190(6.1611) | Total Time 0.00(0.00)\n",
      "Iter 1880 | Time 9.7988(9.4127) | Bit/dim 0.1056(0.1302) | Xent 2.3027(2.3049) | Xent Color 2.3033(2.3039) | Loss 3.1878(3.4664) | Error 0.8767(0.8914) | Error Color 0.9033(0.9010) |Steps 428(405.89) | Grad Norm 1.9057(5.0251) | Total Time 0.00(0.00)\n",
      "Iter 1890 | Time 9.3688(9.4036) | Bit/dim 0.1060(0.1238) | Xent 2.3060(2.3054) | Xent Color 2.3045(2.3040) | Loss 3.1393(3.3952) | Error 0.8889(0.8919) | Error Color 0.9078(0.9014) |Steps 392(404.65) | Grad Norm 1.4083(4.1827) | Total Time 0.00(0.00)\n",
      "Iter 1900 | Time 10.5659(9.4941) | Bit/dim 0.2311(0.1425) | Xent 2.3042(2.3048) | Xent Color 2.3046(2.3041) | Loss 3.4593(3.3859) | Error 0.8844(0.8919) | Error Color 0.9122(0.9022) |Steps 440(408.31) | Grad Norm 21.9708(8.2156) | Total Time 0.00(0.00)\n",
      "Iter 1910 | Time 9.8965(9.6543) | Bit/dim 0.1791(0.1771) | Xent 2.3114(2.3048) | Xent Color 2.3038(2.3042) | Loss 3.3633(3.4370) | Error 0.9067(0.8918) | Error Color 0.8911(0.9019) |Steps 446(417.64) | Grad Norm 7.0290(10.9224) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0029 | Time 61.3757, Epoch Time 716.2888(661.9535), Bit/dim 0.1950(best: 0.1141), Xent 2.3025, Xent Color 2.3034. Loss 1.3465, Error 0.8887(best: 0.3175), Error Color 0.8993(best: 0.8112)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1920 | Time 9.9368(9.7394) | Bit/dim 0.1648(0.1796) | Xent 2.3086(2.3052) | Xent Color 2.3067(2.3042) | Loss 3.4463(4.0104) | Error 0.9056(0.8934) | Error Color 0.9022(0.9016) |Steps 446(421.61) | Grad Norm 5.5028(10.4137) | Total Time 0.00(0.00)\n",
      "Iter 1930 | Time 8.7731(9.7176) | Bit/dim 0.1385(0.1714) | Xent 2.3115(2.3055) | Xent Color 2.3043(2.3039) | Loss 3.2225(3.8244) | Error 0.9033(0.8940) | Error Color 0.8911(0.8995) |Steps 398(419.01) | Grad Norm 5.0148(9.0596) | Total Time 0.00(0.00)\n",
      "Iter 1940 | Time 9.3872(9.6534) | Bit/dim 0.1237(0.1602) | Xent 2.3083(2.3055) | Xent Color 2.3041(2.3039) | Loss 3.2136(3.6698) | Error 0.9089(0.8941) | Error Color 0.8978(0.8993) |Steps 422(417.72) | Grad Norm 3.2563(7.6338) | Total Time 0.00(0.00)\n",
      "Iter 1950 | Time 8.8017(9.5808) | Bit/dim 0.1164(0.1492) | Xent 2.3091(2.3053) | Xent Color 2.3045(2.3038) | Loss 3.1726(3.5473) | Error 0.8911(0.8931) | Error Color 0.9111(0.9000) |Steps 410(416.04) | Grad Norm 2.3674(6.3170) | Total Time 0.00(0.00)\n",
      "Iter 1960 | Time 9.8709(9.5917) | Bit/dim 0.1093(0.1394) | Xent 2.3016(2.3057) | Xent Color 2.3050(2.3040) | Loss 3.1422(3.4502) | Error 0.8956(0.8950) | Error Color 0.9122(0.9000) |Steps 428(415.65) | Grad Norm 0.6807(5.0824) | Total Time 0.00(0.00)\n",
      "Iter 1970 | Time 9.6110(9.5612) | Bit/dim 0.1047(0.1313) | Xent 2.3052(2.3055) | Xent Color 2.3037(2.3040) | Loss 3.1678(3.3759) | Error 0.8822(0.8949) | Error Color 0.9122(0.9007) |Steps 416(411.89) | Grad Norm 1.6211(4.1234) | Total Time 0.00(0.00)\n",
      "Iter 1980 | Time 8.9959(9.5203) | Bit/dim 0.1042(0.1245) | Xent 2.2955(2.3048) | Xent Color 2.3013(2.3039) | Loss 3.1137(3.3228) | Error 0.8756(0.8925) | Error Color 0.9078(0.9004) |Steps 410(410.54) | Grad Norm 1.7010(3.4499) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0030 | Time 59.9877, Epoch Time 714.2923(663.5236), Bit/dim 0.1045(best: 0.1141), Xent 2.3033, Xent Color 2.3029. Loss 1.2560, Error 0.8890(best: 0.3175), Error Color 0.8941(best: 0.8112)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1990 | Time 9.5728(9.4996) | Bit/dim 0.1039(0.1189) | Xent 2.3121(2.3052) | Xent Color 2.3068(2.3039) | Loss 3.1589(3.7042) | Error 0.9011(0.8938) | Error Color 0.9222(0.9015) |Steps 392(410.49) | Grad Norm 1.1509(2.9027) | Total Time 0.00(0.00)\n",
      "Iter 2000 | Time 9.2189(9.4876) | Bit/dim 0.1357(0.1196) | Xent 2.2997(2.3048) | Xent Color 2.3039(2.3040) | Loss 3.2439(3.5688) | Error 0.8911(0.8931) | Error Color 0.9133(0.9019) |Steps 410(410.88) | Grad Norm 15.2555(4.7696) | Total Time 0.00(0.00)\n",
      "Iter 2010 | Time 9.8597(9.5099) | Bit/dim 0.2309(0.1485) | Xent 2.3033(2.3045) | Xent Color 2.3059(2.3038) | Loss 3.4365(3.5218) | Error 0.8867(0.8930) | Error Color 0.9200(0.9017) |Steps 398(411.25) | Grad Norm 4.7120(7.2051) | Total Time 0.00(0.00)\n",
      "Iter 2020 | Time 8.8871(9.4266) | Bit/dim 0.2165(0.1678) | Xent 2.3042(2.3043) | Xent Color 2.3046(2.3038) | Loss 3.3397(3.4748) | Error 0.8911(0.8921) | Error Color 0.9100(0.9010) |Steps 386(408.13) | Grad Norm 3.0802(6.5566) | Total Time 0.00(0.00)\n",
      "Iter 2030 | Time 9.0815(9.3534) | Bit/dim 0.1900(0.1760) | Xent 2.3023(2.3045) | Xent Color 2.3019(2.3038) | Loss 3.2831(3.4263) | Error 0.8722(0.8917) | Error Color 0.8833(0.9000) |Steps 392(405.58) | Grad Norm 2.3700(5.5571) | Total Time 0.00(0.00)\n",
      "Iter 2040 | Time 8.9925(9.3018) | Bit/dim 0.1714(0.1768) | Xent 2.3062(2.3043) | Xent Color 2.3022(2.3039) | Loss 3.2060(3.3734) | Error 0.8989(0.8927) | Error Color 0.9122(0.9007) |Steps 398(400.03) | Grad Norm 1.6475(4.6153) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0031 | Time 56.6835, Epoch Time 694.8626(664.4638), Bit/dim 0.1599(best: 0.1045), Xent 2.3026, Xent Color 2.3033. Loss 1.3114, Error 0.8912(best: 0.3175), Error Color 0.9029(best: 0.8112)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2050 | Time 9.0711(9.2895) | Bit/dim 0.1568(0.1730) | Xent 2.3128(2.3051) | Xent Color 2.3016(2.3040) | Loss 3.1638(3.8318) | Error 0.9144(0.8940) | Error Color 0.9022(0.9012) |Steps 404(401.30) | Grad Norm 1.1255(3.8029) | Total Time 0.00(0.00)\n",
      "Iter 2060 | Time 8.5334(9.3369) | Bit/dim 0.1809(0.1699) | Xent 2.3097(2.3052) | Xent Color 2.3027(2.3040) | Loss 3.1720(3.6602) | Error 0.8967(0.8956) | Error Color 0.8989(0.9025) |Steps 374(397.43) | Grad Norm 16.1815(4.9138) | Total Time 0.00(0.00)\n",
      "Iter 2070 | Time 9.2027(9.3347) | Bit/dim 0.1705(0.1729) | Xent 2.3079(2.3053) | Xent Color 2.3062(2.3039) | Loss 3.2966(3.5471) | Error 0.9022(0.8957) | Error Color 0.9100(0.9013) |Steps 392(392.87) | Grad Norm 15.6527(8.2645) | Total Time 0.00(0.00)\n",
      "Iter 2080 | Time 9.4737(9.2928) | Bit/dim 0.2936(0.1887) | Xent 2.3020(2.3048) | Xent Color 2.3034(2.3039) | Loss 3.4351(3.4913) | Error 0.8856(0.8941) | Error Color 0.8756(0.9015) |Steps 410(393.61) | Grad Norm 12.8552(10.7900) | Total Time 0.00(0.00)\n",
      "Iter 2090 | Time 9.3525(9.2705) | Bit/dim 0.1738(0.1962) | Xent 2.3057(2.3044) | Xent Color 2.3041(2.3038) | Loss 3.0955(3.4250) | Error 0.9100(0.8935) | Error Color 0.9033(0.9005) |Steps 350(389.47) | Grad Norm 6.4622(10.1377) | Total Time 0.00(0.00)\n",
      "Iter 2100 | Time 9.3443(9.2163) | Bit/dim 0.1432(0.1865) | Xent 2.3046(2.3050) | Xent Color 2.3031(2.3036) | Loss 3.1109(3.3463) | Error 0.8700(0.8937) | Error Color 0.9044(0.9000) |Steps 386(387.18) | Grad Norm 2.5336(8.8366) | Total Time 0.00(0.00)\n",
      "Iter 2110 | Time 8.9101(9.1886) | Bit/dim 0.1318(0.1738) | Xent 2.3086(2.3051) | Xent Color 2.3042(2.3035) | Loss 3.0916(3.2844) | Error 0.8956(0.8942) | Error Color 0.8967(0.9002) |Steps 392(388.05) | Grad Norm 4.5229(7.4464) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0032 | Time 54.5936, Epoch Time 686.5145(665.1253), Bit/dim 0.1270(best: 0.1045), Xent 2.3027, Xent Color 2.3028. Loss 1.2784, Error 0.8919(best: 0.3175), Error Color 0.8975(best: 0.8112)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2120 | Time 9.0240(9.2033) | Bit/dim 0.1222(0.1609) | Xent 2.2986(2.3048) | Xent Color 2.3044(2.3036) | Loss 3.0114(3.6389) | Error 0.8867(0.8946) | Error Color 0.8933(0.9009) |Steps 356(384.15) | Grad Norm 2.8345(6.1406) | Total Time 0.00(0.00)\n",
      "Iter 2130 | Time 9.5721(9.2384) | Bit/dim 0.1145(0.1493) | Xent 2.3083(2.3044) | Xent Color 2.3031(2.3036) | Loss 3.0685(3.4971) | Error 0.8978(0.8938) | Error Color 0.9011(0.9009) |Steps 416(385.16) | Grad Norm 1.0785(4.9804) | Total Time 0.00(0.00)\n",
      "Iter 2140 | Time 8.6469(9.2711) | Bit/dim 0.1332(0.1403) | Xent 2.3055(2.3042) | Xent Color 2.3045(2.3039) | Loss 3.1071(3.3927) | Error 0.8956(0.8934) | Error Color 0.8889(0.8999) |Steps 386(386.23) | Grad Norm 12.8198(4.7855) | Total Time 0.00(0.00)\n",
      "Iter 2150 | Time 9.2047(9.2765) | Bit/dim 0.1602(0.1396) | Xent 2.3089(2.3050) | Xent Color 2.3047(2.3038) | Loss 3.2030(3.3258) | Error 0.8944(0.8953) | Error Color 0.9044(0.9000) |Steps 386(387.54) | Grad Norm 16.0420(7.1346) | Total Time 0.00(0.00)\n",
      "Iter 2160 | Time 9.5328(9.3226) | Bit/dim 0.2548(0.1512) | Xent 2.3062(2.3049) | Xent Color 2.3037(2.3038) | Loss 3.3322(3.2970) | Error 0.9033(0.8949) | Error Color 0.8678(0.8996) |Steps 410(386.39) | Grad Norm 13.0707(9.4991) | Total Time 0.00(0.00)\n",
      "Iter 2170 | Time 9.6983(9.3156) | Bit/dim 0.1688(0.1623) | Xent 2.3051(2.3052) | Xent Color 2.3059(2.3037) | Loss 3.1875(3.2758) | Error 0.8911(0.8952) | Error Color 0.8978(0.8996) |Steps 386(387.31) | Grad Norm 10.0944(9.1617) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0033 | Time 55.5704, Epoch Time 692.5879(665.9492), Bit/dim 0.1367(best: 0.1045), Xent 2.3024, Xent Color 2.3027. Loss 1.2880, Error 0.8923(best: 0.3175), Error Color 0.8991(best: 0.8112)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2180 | Time 9.4106(9.2653) | Bit/dim 0.1296(0.1580) | Xent 2.3040(2.3053) | Xent Color 2.3028(2.3039) | Loss 3.1080(3.7026) | Error 0.9044(0.8954) | Error Color 0.9033(0.9007) |Steps 368(384.99) | Grad Norm 3.4271(8.2407) | Total Time 0.00(0.00)\n",
      "Iter 2190 | Time 8.6897(9.1524) | Bit/dim 0.1217(0.1495) | Xent 2.3012(2.3053) | Xent Color 2.3021(2.3037) | Loss 3.1186(3.5298) | Error 0.8867(0.8945) | Error Color 0.8889(0.8999) |Steps 398(382.25) | Grad Norm 4.0870(7.1535) | Total Time 0.00(0.00)\n",
      "Iter 2200 | Time 9.0357(9.1969) | Bit/dim 0.1077(0.1398) | Xent 2.3058(2.3053) | Xent Color 2.3027(2.3038) | Loss 3.0554(3.4109) | Error 0.8967(0.8948) | Error Color 0.8989(0.8993) |Steps 344(381.49) | Grad Norm 4.0552(6.2110) | Total Time 0.00(0.00)\n",
      "Iter 2210 | Time 8.9552(9.1559) | Bit/dim 0.1031(0.1308) | Xent 2.3052(2.3050) | Xent Color 2.3031(2.3038) | Loss 3.0396(3.3208) | Error 0.9078(0.8939) | Error Color 0.9022(0.8996) |Steps 356(381.83) | Grad Norm 2.4697(5.5405) | Total Time 0.00(0.00)\n",
      "Iter 2220 | Time 8.9024(9.2251) | Bit/dim 0.1016(0.1236) | Xent 2.3067(2.3050) | Xent Color 2.3040(2.3040) | Loss 3.0531(3.2539) | Error 0.9011(0.8941) | Error Color 0.8967(0.9008) |Steps 344(382.70) | Grad Norm 5.5242(5.1726) | Total Time 0.00(0.00)\n",
      "Iter 2230 | Time 9.3175(9.2647) | Bit/dim 0.1453(0.1232) | Xent 2.3117(2.3050) | Xent Color 2.3018(2.3037) | Loss 3.1501(3.2088) | Error 0.8933(0.8946) | Error Color 0.8911(0.8988) |Steps 422(386.00) | Grad Norm 20.9881(7.0542) | Total Time 0.00(0.00)\n",
      "Iter 2240 | Time 8.9470(9.2625) | Bit/dim 0.1530(0.1369) | Xent 2.3031(2.3045) | Xent Color 2.3058(2.3038) | Loss 3.1428(3.2081) | Error 0.8944(0.8944) | Error Color 0.9100(0.9000) |Steps 386(386.20) | Grad Norm 9.1150(9.7564) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0034 | Time 56.2676, Epoch Time 689.1005(666.6437), Bit/dim 0.1760(best: 0.1045), Xent 2.3041, Xent Color 2.3024. Loss 1.3276, Error 0.8992(best: 0.3175), Error Color 0.8914(best: 0.8112)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2250 | Time 8.8530(9.2464) | Bit/dim 0.1296(0.1399) | Xent 2.3055(2.3049) | Xent Color 2.3042(2.3036) | Loss 3.1604(3.6658) | Error 0.8944(0.8946) | Error Color 0.8911(0.8987) |Steps 380(388.07) | Grad Norm 6.1975(10.1147) | Total Time 0.00(0.00)\n",
      "Iter 2260 | Time 8.9290(9.2308) | Bit/dim 0.1176(0.1355) | Xent 2.3071(2.3052) | Xent Color 2.3048(2.3037) | Loss 3.0457(3.5191) | Error 0.9011(0.8941) | Error Color 0.9089(0.9000) |Steps 350(386.63) | Grad Norm 5.4351(8.9717) | Total Time 0.00(0.00)\n",
      "Iter 2270 | Time 9.4196(9.2010) | Bit/dim 0.1013(0.1281) | Xent 2.3049(2.3051) | Xent Color 2.3033(2.3037) | Loss 3.0881(3.4041) | Error 0.8978(0.8946) | Error Color 0.9078(0.9006) |Steps 386(386.28) | Grad Norm 2.7708(7.5840) | Total Time 0.00(0.00)\n",
      "Iter 2280 | Time 9.2223(9.2202) | Bit/dim 0.0977(0.1208) | Xent 2.3073(2.3053) | Xent Color 2.3031(2.3037) | Loss 3.0894(3.3158) | Error 0.9078(0.8951) | Error Color 0.9000(0.9013) |Steps 398(384.27) | Grad Norm 2.4240(6.3302) | Total Time 0.00(0.00)\n",
      "Iter 2290 | Time 9.1395(9.2022) | Bit/dim 0.0996(0.1144) | Xent 2.3081(2.3051) | Xent Color 2.3030(2.3036) | Loss 3.0870(3.2531) | Error 0.9056(0.8940) | Error Color 0.8933(0.8994) |Steps 386(384.96) | Grad Norm 6.3101(5.4816) | Total Time 0.00(0.00)\n",
      "Iter 2300 | Time 9.1049(9.2062) | Bit/dim 0.1251(0.1128) | Xent 2.3067(2.3048) | Xent Color 2.3035(2.3034) | Loss 3.1251(3.2120) | Error 0.8956(0.8927) | Error Color 0.8967(0.8976) |Steps 362(384.04) | Grad Norm 17.1738(6.4586) | Total Time 0.00(0.00)\n",
      "Iter 2310 | Time 9.4911(9.2210) | Bit/dim 0.1412(0.1270) | Xent 2.3000(2.3049) | Xent Color 2.3052(2.3035) | Loss 3.0718(3.2096) | Error 0.8811(0.8938) | Error Color 0.9256(0.9002) |Steps 362(382.91) | Grad Norm 7.3820(8.9925) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0035 | Time 53.6252, Epoch Time 681.7777(667.0978), Bit/dim 0.1739(best: 0.1045), Xent 2.3019, Xent Color 2.3030. Loss 1.3251, Error 0.8901(best: 0.3175), Error Color 0.9045(best: 0.8112)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2320 | Time 8.6928(9.1884) | Bit/dim 0.1221(0.1288) | Xent 2.3075(2.3045) | Xent Color 2.3037(2.3036) | Loss 3.1802(3.5579) | Error 0.8756(0.8928) | Error Color 0.8956(0.9008) |Steps 362(379.80) | Grad Norm 8.9476(9.0950) | Total Time 0.00(0.00)\n",
      "Iter 2330 | Time 9.2626(9.1626) | Bit/dim 0.1053(0.1238) | Xent 2.3030(2.3042) | Xent Color 2.3029(2.3035) | Loss 3.0147(3.4244) | Error 0.9022(0.8924) | Error Color 0.9056(0.9010) |Steps 404(379.01) | Grad Norm 5.4743(7.9543) | Total Time 0.00(0.00)\n",
      "Iter 2340 | Time 8.9670(9.1163) | Bit/dim 0.0999(0.1174) | Xent 2.3098(2.3046) | Xent Color 2.3064(2.3033) | Loss 3.0498(3.3241) | Error 0.9078(0.8945) | Error Color 0.9133(0.9000) |Steps 380(378.82) | Grad Norm 5.7525(6.9767) | Total Time 0.00(0.00)\n",
      "Iter 2350 | Time 9.1414(9.1366) | Bit/dim 0.0895(0.1111) | Xent 2.3090(2.3047) | Xent Color 2.3037(2.3034) | Loss 3.0019(3.2508) | Error 0.8822(0.8935) | Error Color 0.8878(0.8987) |Steps 374(380.57) | Grad Norm 3.1498(6.0527) | Total Time 0.00(0.00)\n",
      "Iter 2360 | Time 9.0506(9.1514) | Bit/dim 0.0860(0.1050) | Xent 2.3016(2.3047) | Xent Color 2.3023(2.3033) | Loss 3.0861(3.1929) | Error 0.8922(0.8928) | Error Color 0.9078(0.9002) |Steps 416(382.87) | Grad Norm 3.8263(5.4352) | Total Time 0.00(0.00)\n",
      "Iter 2370 | Time 10.5148(9.1890) | Bit/dim 0.0864(0.1013) | Xent 2.3078(2.3047) | Xent Color 2.3035(2.3033) | Loss 2.9937(3.1522) | Error 0.8956(0.8922) | Error Color 0.8956(0.9002) |Steps 374(384.14) | Grad Norm 5.1488(5.7728) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0036 | Time 55.3269, Epoch Time 681.8472(667.5402), Bit/dim 0.0839(best: 0.1045), Xent 2.3032, Xent Color 2.3030. Loss 1.2354, Error 0.8914(best: 0.3175), Error Color 0.8982(best: 0.8112)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2380 | Time 9.0918(9.1602) | Bit/dim 0.0869(0.0980) | Xent 2.3024(2.3045) | Xent Color 2.3043(2.3035) | Loss 3.0146(3.5888) | Error 0.8889(0.8918) | Error Color 0.8900(0.9007) |Steps 380(384.78) | Grad Norm 9.5020(5.9751) | Total Time 0.00(0.00)\n",
      "Iter 2390 | Time 9.2840(9.1539) | Bit/dim 0.0804(0.0946) | Xent 2.3057(2.3043) | Xent Color 2.3043(2.3036) | Loss 2.9900(3.4431) | Error 0.8933(0.8915) | Error Color 0.8967(0.9011) |Steps 362(385.45) | Grad Norm 2.7065(5.9456) | Total Time 0.00(0.00)\n",
      "Iter 2400 | Time 9.5004(9.1892) | Bit/dim 0.3952(0.1234) | Xent 2.3000(2.3041) | Xent Color 2.3035(2.3036) | Loss 3.6539(3.3949) | Error 0.8822(0.8916) | Error Color 0.9133(0.9012) |Steps 410(387.47) | Grad Norm 11.5304(9.2515) | Total Time 0.00(0.00)\n",
      "Iter 2410 | Time 9.3581(9.3802) | Bit/dim 0.2297(0.1665) | Xent 2.3013(2.3041) | Xent Color 2.3021(2.3036) | Loss 3.4399(3.4177) | Error 0.8778(0.8916) | Error Color 0.8900(0.9011) |Steps 410(400.03) | Grad Norm 5.1058(9.1176) | Total Time 0.00(0.00)\n",
      "Iter 2420 | Time 9.3745(9.4348) | Bit/dim 0.1855(0.1793) | Xent 2.3074(2.3043) | Xent Color 2.3054(2.3037) | Loss 3.3938(3.4152) | Error 0.8900(0.8925) | Error Color 0.8967(0.9009) |Steps 410(402.65) | Grad Norm 2.0177(8.1181) | Total Time 0.00(0.00)\n",
      "Iter 2430 | Time 8.5518(9.4060) | Bit/dim 0.1635(0.1772) | Xent 2.3018(2.3041) | Xent Color 2.3030(2.3038) | Loss 3.2686(3.3886) | Error 0.8900(0.8935) | Error Color 0.9011(0.9026) |Steps 386(401.76) | Grad Norm 7.9403(7.4482) | Total Time 0.00(0.00)\n",
      "Iter 2440 | Time 9.2388(9.3797) | Bit/dim 0.1424(0.1698) | Xent 2.3110(2.3047) | Xent Color 2.3045(2.3038) | Loss 3.1861(3.3410) | Error 0.9133(0.8960) | Error Color 0.9078(0.9014) |Steps 392(399.68) | Grad Norm 10.8750(7.6540) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0037 | Time 56.3168, Epoch Time 699.4733(668.4982), Bit/dim 0.1284(best: 0.0839), Xent 2.3034, Xent Color 2.3025. Loss 1.2798, Error 0.8908(best: 0.3175), Error Color 0.8941(best: 0.8112)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2450 | Time 8.9829(9.3591) | Bit/dim 0.1393(0.1611) | Xent 2.3081(2.3048) | Xent Color 2.3062(2.3037) | Loss 3.2051(3.7576) | Error 0.8900(0.8954) | Error Color 0.9100(0.9004) |Steps 398(398.50) | Grad Norm 10.3041(8.4075) | Total Time 0.00(0.00)\n",
      "Iter 2460 | Time 9.5737(9.3834) | Bit/dim 0.1583(0.1580) | Xent 2.3057(2.3048) | Xent Color 2.3044(2.3039) | Loss 3.1789(3.6270) | Error 0.8878(0.8943) | Error Color 0.9144(0.9011) |Steps 386(400.30) | Grad Norm 22.6984(9.9793) | Total Time 0.00(0.00)\n",
      "Iter 2470 | Time 9.7846(9.4110) | Bit/dim 0.1198(0.1519) | Xent 2.3076(2.3046) | Xent Color 2.3030(2.3035) | Loss 3.2644(3.5195) | Error 0.9089(0.8943) | Error Color 0.9100(0.9000) |Steps 416(401.73) | Grad Norm 8.1051(10.1270) | Total Time 0.00(0.00)\n",
      "Iter 2480 | Time 9.5728(9.4770) | Bit/dim 0.1067(0.1417) | Xent 2.3077(2.3047) | Xent Color 2.3030(2.3033) | Loss 3.1511(3.4287) | Error 0.8967(0.8948) | Error Color 0.8889(0.8988) |Steps 386(399.06) | Grad Norm 3.3327(9.0531) | Total Time 0.00(0.00)\n",
      "Iter 2490 | Time 10.3774(9.5492) | Bit/dim 0.0981(0.1313) | Xent 2.3017(2.3049) | Xent Color 2.3052(2.3035) | Loss 3.1859(3.3588) | Error 0.8911(0.8955) | Error Color 0.9111(0.8994) |Steps 446(402.70) | Grad Norm 5.6267(7.8586) | Total Time 0.00(0.00)\n",
      "Iter 2500 | Time 9.9486(9.5976) | Bit/dim 0.0956(0.1225) | Xent 2.3074(2.3049) | Xent Color 2.3023(2.3035) | Loss 3.1148(3.3037) | Error 0.9022(0.8938) | Error Color 0.8767(0.8992) |Steps 434(405.62) | Grad Norm 4.1011(7.0078) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0038 | Time 61.0002, Epoch Time 716.2816(669.9317), Bit/dim 0.1660(best: 0.0839), Xent 2.3022, Xent Color 2.3030. Loss 1.3173, Error 0.8868(best: 0.3175), Error Color 0.9000(best: 0.8112)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2510 | Time 9.3951(9.6241) | Bit/dim 0.1532(0.1251) | Xent 2.3008(2.3049) | Xent Color 2.3049(2.3035) | Loss 3.1635(3.8775) | Error 0.8833(0.8947) | Error Color 0.9222(0.9001) |Steps 380(406.91) | Grad Norm 17.8249(8.2133) | Total Time 0.00(0.00)\n",
      "Iter 2520 | Time 9.2590(9.5872) | Bit/dim 0.1206(0.1287) | Xent 2.3073(2.3049) | Xent Color 2.3035(2.3035) | Loss 3.1922(3.7031) | Error 0.9089(0.8950) | Error Color 0.8967(0.8999) |Steps 392(406.93) | Grad Norm 8.3805(9.2461) | Total Time 0.00(0.00)\n",
      "Iter 2530 | Time 9.0275(9.5477) | Bit/dim 0.1253(0.1287) | Xent 2.3044(2.3046) | Xent Color 2.3043(2.3037) | Loss 3.1156(3.5545) | Error 0.9033(0.8944) | Error Color 0.9078(0.9011) |Steps 392(403.87) | Grad Norm 8.6245(9.8545) | Total Time 0.00(0.00)\n",
      "Iter 2540 | Time 9.5012(9.5626) | Bit/dim 0.0991(0.1230) | Xent 2.3028(2.3042) | Xent Color 2.3038(2.3037) | Loss 3.0827(3.4496) | Error 0.8933(0.8922) | Error Color 0.8900(0.9015) |Steps 422(405.83) | Grad Norm 2.6771(9.2244) | Total Time 0.00(0.00)\n",
      "Iter 2550 | Time 9.4708(9.6244) | Bit/dim 0.0953(0.1164) | Xent 2.3040(2.3042) | Xent Color 2.3017(2.3036) | Loss 3.1776(3.3752) | Error 0.8878(0.8927) | Error Color 0.9033(0.9016) |Steps 398(406.13) | Grad Norm 5.5215(8.1904) | Total Time 0.00(0.00)\n",
      "Iter 2560 | Time 9.9981(9.6636) | Bit/dim 0.0913(0.1100) | Xent 2.3059(2.3043) | Xent Color 2.3037(2.3035) | Loss 3.1025(3.3122) | Error 0.8978(0.8950) | Error Color 0.9133(0.9023) |Steps 404(406.66) | Grad Norm 2.4089(6.8924) | Total Time 0.00(0.00)\n",
      "Iter 2570 | Time 9.4899(9.6661) | Bit/dim 0.0876(0.1044) | Xent 2.3027(2.3043) | Xent Color 2.3029(2.3034) | Loss 3.1686(3.2632) | Error 0.8989(0.8942) | Error Color 0.9000(0.9021) |Steps 428(408.24) | Grad Norm 3.6482(5.7955) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0039 | Time 59.1615, Epoch Time 719.2315(671.4107), Bit/dim 0.0876(best: 0.0839), Xent 2.3025, Xent Color 2.3033. Loss 1.2391, Error 0.8915(best: 0.3175), Error Color 0.8997(best: 0.8112)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2580 | Time 9.9266(9.7283) | Bit/dim 0.0829(0.0996) | Xent 2.2986(2.3038) | Xent Color 2.3011(2.3033) | Loss 3.0986(3.7025) | Error 0.8911(0.8945) | Error Color 0.8844(0.8989) |Steps 428(407.92) | Grad Norm 5.5746(5.0400) | Total Time 0.00(0.00)\n",
      "Iter 2590 | Time 9.5946(9.7487) | Bit/dim 0.0929(0.0985) | Xent 2.3039(2.3037) | Xent Color 2.3026(2.3031) | Loss 3.1329(3.5617) | Error 0.8867(0.8935) | Error Color 0.8956(0.8981) |Steps 392(407.97) | Grad Norm 7.2543(5.9183) | Total Time 0.00(0.00)\n",
      "Iter 2600 | Time 9.7267(9.7360) | Bit/dim 0.0833(0.0956) | Xent 2.3030(2.3035) | Xent Color 2.3043(2.3033) | Loss 3.1017(3.4479) | Error 0.8956(0.8938) | Error Color 0.9056(0.8993) |Steps 404(409.45) | Grad Norm 3.9100(6.0822) | Total Time 0.00(0.00)\n",
      "Iter 2610 | Time 9.8822(9.7489) | Bit/dim 0.0894(0.0937) | Xent 2.3063(2.3038) | Xent Color 2.3008(2.3031) | Loss 3.0873(3.3570) | Error 0.8978(0.8939) | Error Color 0.8900(0.8989) |Steps 422(409.26) | Grad Norm 7.4280(6.1014) | Total Time 0.00(0.00)\n",
      "Iter 2620 | Time 9.9848(9.7763) | Bit/dim 0.1437(0.0962) | Xent 2.3041(2.3041) | Xent Color 2.3033(2.3032) | Loss 3.2137(3.3006) | Error 0.8800(0.8949) | Error Color 0.9078(0.8996) |Steps 422(411.53) | Grad Norm 16.3223(7.4760) | Total Time 0.00(0.00)\n",
      "Iter 2630 | Time 9.2649(9.7125) | Bit/dim 0.2434(0.1304) | Xent 2.3070(2.3047) | Xent Color 2.3032(2.3033) | Loss 3.4064(3.3262) | Error 0.8956(0.8960) | Error Color 0.8956(0.8993) |Steps 386(409.99) | Grad Norm 2.9537(8.0439) | Total Time 0.00(0.00)\n",
      "Iter 2640 | Time 9.5204(9.7227) | Bit/dim 0.1581(0.1412) | Xent 2.3080(2.3043) | Xent Color 2.3048(2.3031) | Loss 3.3119(3.3205) | Error 0.8989(0.8931) | Error Color 0.9022(0.8993) |Steps 398(409.35) | Grad Norm 11.3308(8.5030) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0040 | Time 59.5477, Epoch Time 725.3722(673.0296), Bit/dim 0.1297(best: 0.0839), Xent 2.3021, Xent Color 2.3032. Loss 1.2810, Error 0.8922(best: 0.3175), Error Color 0.9052(best: 0.8112)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2650 | Time 9.2292(9.6964) | Bit/dim 0.1135(0.1360) | Xent 2.3056(2.3045) | Xent Color 2.3056(2.3034) | Loss 3.0521(3.7232) | Error 0.8933(0.8936) | Error Color 0.9133(0.9010) |Steps 404(407.95) | Grad Norm 8.6389(9.0094) | Total Time 0.00(0.00)\n",
      "Iter 2660 | Time 9.5903(9.6742) | Bit/dim 0.1123(0.1351) | Xent 2.3033(2.3041) | Xent Color 2.3008(2.3032) | Loss 3.2219(3.5928) | Error 0.8844(0.8922) | Error Color 0.8856(0.8993) |Steps 422(405.49) | Grad Norm 8.8680(10.6139) | Total Time 0.00(0.00)\n",
      "Iter 2670 | Time 9.5068(9.6014) | Bit/dim 0.1058(0.1363) | Xent 2.3051(2.3044) | Xent Color 2.3033(2.3033) | Loss 3.1882(3.4894) | Error 0.8956(0.8927) | Error Color 0.8978(0.9000) |Steps 422(404.01) | Grad Norm 5.1991(11.3079) | Total Time 0.00(0.00)\n",
      "Iter 2680 | Time 9.0359(9.6131) | Bit/dim 0.0926(0.1281) | Xent 2.3008(2.3045) | Xent Color 2.3036(2.3033) | Loss 2.9457(3.3953) | Error 0.8889(0.8934) | Error Color 0.9089(0.9002) |Steps 386(403.59) | Grad Norm 5.9339(10.3832) | Total Time 0.00(0.00)\n",
      "Iter 2690 | Time 9.5246(9.5651) | Bit/dim 0.0931(0.1191) | Xent 2.3038(2.3045) | Xent Color 2.3035(2.3034) | Loss 3.0159(3.3116) | Error 0.8778(0.8938) | Error Color 0.9011(0.9010) |Steps 404(401.16) | Grad Norm 7.4715(9.3854) | Total Time 0.00(0.00)\n",
      "Iter 2700 | Time 9.5886(9.5401) | Bit/dim 0.0826(0.1101) | Xent 2.3081(2.3050) | Xent Color 2.3029(2.3035) | Loss 3.0738(3.2419) | Error 0.9000(0.8936) | Error Color 0.9067(0.9000) |Steps 380(396.36) | Grad Norm 2.2637(7.8289) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0041 | Time 58.2222, Epoch Time 710.4033(674.1508), Bit/dim 0.0809(best: 0.0839), Xent 2.3018, Xent Color 2.3030. Loss 1.2320, Error 0.8922(best: 0.3175), Error Color 0.9013(best: 0.8112)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2710 | Time 9.1174(9.5384) | Bit/dim 0.0868(0.1026) | Xent 2.3041(2.3047) | Xent Color 2.3028(2.3034) | Loss 3.0417(3.6667) | Error 0.8944(0.8938) | Error Color 0.9100(0.8999) |Steps 410(397.66) | Grad Norm 6.4401(6.6022) | Total Time 0.00(0.00)\n",
      "Iter 2720 | Time 9.8112(9.5620) | Bit/dim 0.0864(0.0974) | Xent 2.3092(2.3048) | Xent Color 2.3016(2.3032) | Loss 2.9808(3.5064) | Error 0.9000(0.8942) | Error Color 0.8822(0.8990) |Steps 392(397.65) | Grad Norm 10.4813(6.4302) | Total Time 0.00(0.00)\n",
      "Iter 2730 | Time 9.9972(9.5694) | Bit/dim 0.1779(0.1024) | Xent 2.3111(2.3044) | Xent Color 2.3029(2.3032) | Loss 3.2248(3.4003) | Error 0.8878(0.8929) | Error Color 0.8978(0.8991) |Steps 398(396.56) | Grad Norm 36.7492(8.6691) | Total Time 0.00(0.00)\n",
      "Iter 2740 | Time 9.4987(9.6290) | Bit/dim 0.2511(0.1356) | Xent 2.3029(2.3040) | Xent Color 2.3026(2.3032) | Loss 3.3854(3.3982) | Error 0.8900(0.8921) | Error Color 0.8889(0.8998) |Steps 410(402.13) | Grad Norm 16.4606(10.9282) | Total Time 0.00(0.00)\n",
      "Iter 2750 | Time 9.9134(9.5217) | Bit/dim 0.1547(0.1435) | Xent 2.2981(2.3041) | Xent Color 2.3053(2.3033) | Loss 3.1298(3.3422) | Error 0.8789(0.8920) | Error Color 0.9078(0.9002) |Steps 416(399.36) | Grad Norm 15.2385(11.0033) | Total Time 0.00(0.00)\n",
      "Iter 2760 | Time 9.3484(9.4487) | Bit/dim 0.1097(0.1368) | Xent 2.3061(2.3041) | Xent Color 2.3032(2.3031) | Loss 3.1492(3.2796) | Error 0.8856(0.8925) | Error Color 0.8956(0.8994) |Steps 386(395.91) | Grad Norm 6.2958(9.9113) | Total Time 0.00(0.00)\n",
      "Iter 2770 | Time 10.0844(9.4480) | Bit/dim 0.0892(0.1262) | Xent 2.3034(2.3041) | Xent Color 2.3022(2.3031) | Loss 3.1394(3.2331) | Error 0.8789(0.8925) | Error Color 0.8911(0.9000) |Steps 440(397.91) | Grad Norm 2.8661(8.5280) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0042 | Time 58.2138, Epoch Time 706.0752(675.1085), Bit/dim 0.0880(best: 0.0809), Xent 2.3020, Xent Color 2.3029. Loss 1.2392, Error 0.8913(best: 0.3175), Error Color 0.9019(best: 0.8112)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2780 | Time 9.9414(9.5231) | Bit/dim 0.0822(0.1158) | Xent 2.3091(2.3043) | Xent Color 2.3028(2.3032) | Loss 3.1089(3.6323) | Error 0.9089(0.8938) | Error Color 0.8944(0.9007) |Steps 416(399.42) | Grad Norm 2.5495(7.1554) | Total Time 0.00(0.00)\n",
      "Iter 2790 | Time 9.1036(9.5312) | Bit/dim 0.0800(0.1064) | Xent 2.3050(2.3042) | Xent Color 2.3033(2.3035) | Loss 3.0554(3.4872) | Error 0.9089(0.8937) | Error Color 0.8956(0.9008) |Steps 392(397.80) | Grad Norm 1.1982(5.8333) | Total Time 0.00(0.00)\n",
      "Iter 2800 | Time 10.0028(9.6174) | Bit/dim 0.0786(0.0987) | Xent 2.3006(2.3039) | Xent Color 2.3048(2.3034) | Loss 3.1462(3.3812) | Error 0.8911(0.8934) | Error Color 0.9044(0.9007) |Steps 440(402.23) | Grad Norm 1.6853(4.7090) | Total Time 0.00(0.00)\n",
      "Iter 2810 | Time 9.7187(9.6369) | Bit/dim 0.0756(0.0927) | Xent 2.3025(2.3037) | Xent Color 2.3034(2.3036) | Loss 3.1063(3.3040) | Error 0.8811(0.8924) | Error Color 0.8944(0.9007) |Steps 416(403.04) | Grad Norm 4.8497(3.9836) | Total Time 0.00(0.00)\n",
      "Iter 2820 | Time 10.0935(9.6668) | Bit/dim 0.0891(0.0892) | Xent 2.3053(2.3036) | Xent Color 2.3032(2.3036) | Loss 3.0607(3.2439) | Error 0.8922(0.8914) | Error Color 0.8922(0.9001) |Steps 362(401.51) | Grad Norm 12.6403(4.6047) | Total Time 0.00(0.00)\n",
      "Iter 2830 | Time 9.7193(9.6945) | Bit/dim 0.1230(0.0949) | Xent 2.3009(2.3034) | Xent Color 2.3034(2.3034) | Loss 3.1636(3.2165) | Error 0.8956(0.8926) | Error Color 0.8889(0.8999) |Steps 386(403.50) | Grad Norm 21.7946(7.2054) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0043 | Time 57.9044, Epoch Time 721.1199(676.4889), Bit/dim 0.0866(best: 0.0809), Xent 2.3030, Xent Color 2.3030. Loss 1.2381, Error 0.8917(best: 0.3175), Error Color 0.8971(best: 0.8112)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2840 | Time 9.3739(9.6266) | Bit/dim 0.0839(0.0942) | Xent 2.3053(2.3037) | Xent Color 2.3033(2.3034) | Loss 3.0593(3.6952) | Error 0.8889(0.8931) | Error Color 0.9067(0.9001) |Steps 428(401.73) | Grad Norm 7.2888(7.6753) | Total Time 0.00(0.00)\n",
      "Iter 2850 | Time 9.6226(9.6110) | Bit/dim 0.0757(0.0904) | Xent 2.3042(2.3038) | Xent Color 2.3036(2.3034) | Loss 3.0402(3.5283) | Error 0.8967(0.8938) | Error Color 0.9156(0.9009) |Steps 386(399.53) | Grad Norm 6.4189(7.3397) | Total Time 0.00(0.00)\n",
      "Iter 2860 | Time 9.5666(9.5720) | Bit/dim 0.0735(0.0860) | Xent 2.3047(2.3037) | Xent Color 2.3049(2.3034) | Loss 3.0153(3.3976) | Error 0.9044(0.8935) | Error Color 0.9200(0.9017) |Steps 392(399.35) | Grad Norm 2.4248(6.3337) | Total Time 0.00(0.00)\n",
      "Iter 2870 | Time 9.4073(9.6344) | Bit/dim 0.0694(0.0821) | Xent 2.3031(2.3039) | Xent Color 2.3023(2.3032) | Loss 3.0617(3.3099) | Error 0.8800(0.8927) | Error Color 0.9011(0.9003) |Steps 392(401.25) | Grad Norm 2.2750(5.3629) | Total Time 0.00(0.00)\n",
      "Iter 2880 | Time 10.0101(9.6805) | Bit/dim 0.0699(0.0787) | Xent 2.3068(2.3038) | Xent Color 2.3041(2.3033) | Loss 3.0382(3.2432) | Error 0.8933(0.8920) | Error Color 0.9067(0.9003) |Steps 392(399.61) | Grad Norm 3.8866(4.5327) | Total Time 0.00(0.00)\n",
      "Iter 2890 | Time 10.3366(9.7480) | Bit/dim 0.0830(0.0775) | Xent 2.3011(2.3039) | Xent Color 2.3034(2.3034) | Loss 3.0928(3.1994) | Error 0.8856(0.8924) | Error Color 0.9022(0.9012) |Steps 440(403.21) | Grad Norm 10.3502(5.2522) | Total Time 0.00(0.00)\n",
      "Iter 2900 | Time 9.1860(9.7575) | Bit/dim 0.1775(0.0804) | Xent 2.3034(2.3039) | Xent Color 2.3031(2.3034) | Loss 3.2609(3.1683) | Error 0.8967(0.8937) | Error Color 0.8956(0.9016) |Steps 392(403.90) | Grad Norm 16.8684(6.6077) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0044 | Time 57.9247, Epoch Time 723.2063(677.8904), Bit/dim 0.3273(best: 0.0809), Xent 2.3024, Xent Color 2.3028. Loss 1.4786, Error 0.8930(best: 0.3175), Error Color 0.9009(best: 0.8112)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2910 | Time 10.7308(9.8334) | Bit/dim 0.3962(0.1489) | Xent 2.3071(2.3040) | Xent Color 2.3023(2.3033) | Loss 3.8053(3.7747) | Error 0.8989(0.8929) | Error Color 0.9189(0.9016) |Steps 464(409.61) | Grad Norm 23.6567(10.3563) | Total Time 0.00(0.00)\n",
      "Iter 2920 | Time 10.2150(9.9418) | Bit/dim 0.2597(0.1926) | Xent 2.3009(2.3039) | Xent Color 2.3041(2.3035) | Loss 3.4047(3.7314) | Error 0.8944(0.8924) | Error Color 0.9033(0.9026) |Steps 452(420.52) | Grad Norm 7.0647(10.4774) | Total Time 0.00(0.00)\n",
      "Iter 2930 | Time 9.4288(9.9008) | Bit/dim 0.1947(0.1990) | Xent 2.3033(2.3040) | Xent Color 2.3023(2.3034) | Loss 3.2656(3.6273) | Error 0.8811(0.8925) | Error Color 0.9100(0.9022) |Steps 404(418.46) | Grad Norm 4.6946(9.0282) | Total Time 0.00(0.00)\n",
      "Iter 2940 | Time 9.0950(9.7404) | Bit/dim 0.1669(0.1932) | Xent 2.3087(2.3039) | Xent Color 2.3019(2.3032) | Loss 3.1546(3.5093) | Error 0.9100(0.8924) | Error Color 0.8833(0.9000) |Steps 404(412.08) | Grad Norm 5.4638(7.6899) | Total Time 0.00(0.00)\n",
      "Iter 2950 | Time 9.6841(9.6212) | Bit/dim 0.1427(0.1828) | Xent 2.3077(2.3039) | Xent Color 2.3027(2.3032) | Loss 3.0766(3.4007) | Error 0.8967(0.8924) | Error Color 0.8922(0.9005) |Steps 392(407.46) | Grad Norm 2.4394(6.3686) | Total Time 0.00(0.00)\n",
      "Iter 2960 | Time 9.0899(9.5134) | Bit/dim 0.1399(0.1717) | Xent 2.3031(2.3038) | Xent Color 2.3032(2.3032) | Loss 3.0417(3.3130) | Error 0.9000(0.8917) | Error Color 0.9067(0.9020) |Steps 374(398.33) | Grad Norm 12.3800(6.2169) | Total Time 0.00(0.00)\n",
      "Iter 2970 | Time 8.7768(9.3922) | Bit/dim 0.1240(0.1634) | Xent 2.3042(2.3040) | Xent Color 2.3021(2.3033) | Loss 3.0174(3.2489) | Error 0.9056(0.8934) | Error Color 0.8878(0.9016) |Steps 386(392.97) | Grad Norm 5.7319(7.3708) | Total Time 0.00(0.00)\n",
      "validating...\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_rl_2cond.py --data colormnist --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/cnf_conditional_disentangle_colormnist_bs900_sratio_1_12th_drop_0_5_rl_stdscale_6_2cond_linear_run1 --seed 1 --lr 0.001 --conditional True --controlled_tol False --train_mode semisup --log_freq 10 --weight_y 0.5 --condition_ratio 0.083333 --dropout_rate 0.5 --scale_fac 1.0 --scale_std 6.0 --cond_nn linear --y_color 10 --y_class 10\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
