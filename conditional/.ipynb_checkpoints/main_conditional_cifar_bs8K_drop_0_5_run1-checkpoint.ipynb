{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_drop.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "if args.controlled_tol:\n",
      "    import lib.odenvp_conditional_tol as odenvp\n",
      "else:\n",
      "    import lib.odenvp_conditional as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz = modules.GaussianDiag.logp(mean, logs, z).view(-1,1)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        z = model.module.dropout(z)\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(z)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol},\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z = modules.GaussianDiag.sample(mean, logs)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            loss.backward()\n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=0.0001, autoencode=False, batch_norm=False, batch_size=8000, batch_size_schedule='', begin_epoch=1, conditional=True, controlled_tol=True, conv=True, data='mnist', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=1, lr=0.0001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume='../experiments_published/cnf_conditional_8K_drop_0_5_run1/epoch_365_checkpt.pth', rtol=0.0001, save='../experiments_published/cnf_conditional_8K_drop_0_5_run1', seed=0, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=5000, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=113.0, weight_decay=0.0, weight_y=0.5)\n",
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=1568, bias=True)\n",
      "  (project_class): LinearZeros(in_features=784, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 828890\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc.py:286: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "Iter 2556 | Time 68.8081(31.0555) | Bit/dim 1.1132(1.1225) | Xent 0.0440(0.0438) | Loss 1.1352(1.1444) | Error 0.0128(0.0136) Steps 416(424.06) | Grad Norm 1.7023(3.3306) | Total Time 10.00(10.00)\n",
      "Iter 2557 | Time 30.2874(31.0324) | Bit/dim 1.1125(1.1222) | Xent 0.0408(0.0437) | Loss 1.1329(1.1441) | Error 0.0124(0.0135) Steps 410(423.64) | Grad Norm 1.2656(3.2686) | Total Time 10.00(10.00)\n",
      "Iter 2558 | Time 28.8953(30.9683) | Bit/dim 1.1110(1.1219) | Xent 0.0443(0.0437) | Loss 1.1331(1.1437) | Error 0.0119(0.0135) Steps 416(423.41) | Grad Norm 0.5868(3.1881) | Total Time 10.00(10.00)\n",
      "Iter 2559 | Time 29.5932(30.9271) | Bit/dim 1.1113(1.1216) | Xent 0.0480(0.0438) | Loss 1.1353(1.1435) | Error 0.0152(0.0135) Steps 416(423.19) | Grad Norm 0.3971(3.1044) | Total Time 10.00(10.00)\n",
      "Iter 2560 | Time 28.7874(30.8629) | Bit/dim 1.1131(1.1213) | Xent 0.0374(0.0436) | Loss 1.1318(1.1431) | Error 0.0105(0.0134) Steps 416(422.97) | Grad Norm 1.5551(3.0579) | Total Time 10.00(10.00)\n",
      "Iter 2561 | Time 28.6999(30.7980) | Bit/dim 1.1147(1.1211) | Xent 0.0426(0.0436) | Loss 1.1360(1.1429) | Error 0.0132(0.0134) Steps 416(422.76) | Grad Norm 1.5038(3.0113) | Total Time 10.00(10.00)\n",
      "Iter 2562 | Time 28.7823(30.7375) | Bit/dim 1.1127(1.1209) | Xent 0.0400(0.0435) | Loss 1.1327(1.1426) | Error 0.0126(0.0134) Steps 416(422.56) | Grad Norm 1.1782(2.9563) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0366 | Time 27.3815, Epoch Time 283.8549(239.9955), Bit/dim 1.1066(best: inf), Xent 0.0299, Loss 1.1216, Error 0.0091(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2563 | Time 30.9558(30.7441) | Bit/dim 1.1097(1.1205) | Xent 0.0415(0.0434) | Loss 1.1305(1.1422) | Error 0.0136(0.0134) Steps 416(422.36) | Grad Norm 0.5010(2.8827) | Total Time 10.00(10.00)\n",
      "Iter 2564 | Time 28.6102(30.6800) | Bit/dim 1.1116(1.1203) | Xent 0.0453(0.0435) | Loss 1.1342(1.1420) | Error 0.0151(0.0135) Steps 422(422.35) | Grad Norm 0.6401(2.8154) | Total Time 10.00(10.00)\n",
      "Iter 2565 | Time 30.6876(30.6803) | Bit/dim 1.1120(1.1200) | Xent 0.0359(0.0433) | Loss 1.1299(1.1416) | Error 0.0108(0.0134) Steps 416(422.16) | Grad Norm 0.9401(2.7591) | Total Time 10.00(10.00)\n",
      "Iter 2566 | Time 29.0333(30.6309) | Bit/dim 1.1073(1.1196) | Xent 0.0422(0.0432) | Loss 1.1284(1.1412) | Error 0.0146(0.0134) Steps 410(421.80) | Grad Norm 1.2783(2.7147) | Total Time 10.00(10.00)\n",
      "Iter 2567 | Time 28.9633(30.5808) | Bit/dim 1.1142(1.1195) | Xent 0.0387(0.0431) | Loss 1.1336(1.1410) | Error 0.0112(0.0134) Steps 416(421.62) | Grad Norm 1.0302(2.6642) | Total Time 10.00(10.00)\n",
      "Iter 2568 | Time 29.8445(30.5587) | Bit/dim 1.1118(1.1192) | Xent 0.0359(0.0429) | Loss 1.1298(1.1407) | Error 0.0108(0.0133) Steps 416(421.46) | Grad Norm 0.2529(2.5918) | Total Time 10.00(10.00)\n",
      "Iter 2569 | Time 30.1360(30.5461) | Bit/dim 1.1174(1.1192) | Xent 0.0352(0.0427) | Loss 1.1350(1.1405) | Error 0.0116(0.0132) Steps 422(421.47) | Grad Norm 0.2723(2.5222) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0367 | Time 16.6017, Epoch Time 237.3894(239.9173), Bit/dim 1.1064(best: 1.1066), Xent 0.0263, Loss 1.1196, Error 0.0092(best: 0.0091)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2570 | Time 29.3971(30.5116) | Bit/dim 1.1104(1.1189) | Xent 0.0407(0.0426) | Loss 1.1308(1.1402) | Error 0.0139(0.0133) Steps 416(421.31) | Grad Norm 0.8643(2.4725) | Total Time 10.00(10.00)\n",
      "Iter 2571 | Time 29.8647(30.4922) | Bit/dim 1.1125(1.1187) | Xent 0.0386(0.0425) | Loss 1.1318(1.1400) | Error 0.0114(0.0132) Steps 416(421.15) | Grad Norm 0.9030(2.4254) | Total Time 10.00(10.00)\n",
      "Iter 2572 | Time 28.5228(30.4331) | Bit/dim 1.1117(1.1185) | Xent 0.0388(0.0424) | Loss 1.1311(1.1397) | Error 0.0106(0.0131) Steps 428(421.35) | Grad Norm 0.9577(2.3814) | Total Time 10.00(10.00)\n",
      "Iter 2573 | Time 28.9408(30.3883) | Bit/dim 1.1145(1.1184) | Xent 0.0383(0.0422) | Loss 1.1336(1.1395) | Error 0.0111(0.0131) Steps 416(421.19) | Grad Norm 0.3647(2.3209) | Total Time 10.00(10.00)\n",
      "Iter 2574 | Time 28.8097(30.3410) | Bit/dim 1.1154(1.1183) | Xent 0.0441(0.0423) | Loss 1.1375(1.1395) | Error 0.0131(0.0131) Steps 416(421.04) | Grad Norm 0.2443(2.2586) | Total Time 10.00(10.00)\n",
      "Iter 2575 | Time 30.0090(30.3310) | Bit/dim 1.1143(1.1182) | Xent 0.0394(0.0422) | Loss 1.1340(1.1393) | Error 0.0134(0.0131) Steps 416(420.89) | Grad Norm 0.8602(2.2166) | Total Time 10.00(10.00)\n",
      "Iter 2576 | Time 28.9968(30.2910) | Bit/dim 1.1092(1.1179) | Xent 0.0422(0.0422) | Loss 1.1303(1.1390) | Error 0.0129(0.0131) Steps 416(420.74) | Grad Norm 0.7960(2.1740) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0368 | Time 16.4301, Epoch Time 233.6521(239.7294), Bit/dim 1.1056(best: 1.1064), Xent 0.0287, Loss 1.1200, Error 0.0102(best: 0.0091)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2577 | Time 29.3672(30.2633) | Bit/dim 1.1098(1.1177) | Xent 0.0393(0.0421) | Loss 1.1295(1.1387) | Error 0.0110(0.0130) Steps 410(420.42) | Grad Norm 0.6572(2.1285) | Total Time 10.00(10.00)\n",
      "Iter 2578 | Time 28.5347(30.2114) | Bit/dim 1.1130(1.1175) | Xent 0.0388(0.0420) | Loss 1.1324(1.1385) | Error 0.0112(0.0130) Steps 416(420.28) | Grad Norm 0.1775(2.0700) | Total Time 10.00(10.00)\n",
      "Iter 2579 | Time 29.9827(30.2046) | Bit/dim 1.1086(1.1173) | Xent 0.0419(0.0420) | Loss 1.1296(1.1383) | Error 0.0124(0.0129) Steps 416(420.16) | Grad Norm 0.2353(2.0149) | Total Time 10.00(10.00)\n",
      "Iter 2580 | Time 28.6951(30.1593) | Bit/dim 1.1113(1.1171) | Xent 0.0375(0.0419) | Loss 1.1300(1.1380) | Error 0.0102(0.0129) Steps 416(420.03) | Grad Norm 0.8447(1.9798) | Total Time 10.00(10.00)\n",
      "Iter 2581 | Time 29.7193(30.1461) | Bit/dim 1.1077(1.1168) | Xent 0.0442(0.0420) | Loss 1.1298(1.1378) | Error 0.0148(0.0129) Steps 416(419.91) | Grad Norm 0.4840(1.9350) | Total Time 10.00(10.00)\n",
      "Iter 2582 | Time 30.5638(30.1586) | Bit/dim 1.1129(1.1167) | Xent 0.0388(0.0419) | Loss 1.1323(1.1376) | Error 0.0130(0.0129) Steps 422(419.97) | Grad Norm 0.4317(1.8899) | Total Time 10.00(10.00)\n",
      "Iter 2583 | Time 28.6541(30.1135) | Bit/dim 1.1172(1.1167) | Xent 0.0384(0.0418) | Loss 1.1364(1.1376) | Error 0.0111(0.0129) Steps 416(419.85) | Grad Norm 0.2733(1.8414) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0369 | Time 16.9311, Epoch Time 234.8032(239.5816), Bit/dim 1.1058(best: 1.1056), Xent 0.0270, Loss 1.1193, Error 0.0094(best: 0.0091)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2584 | Time 30.4305(30.1230) | Bit/dim 1.1159(1.1167) | Xent 0.0417(0.0418) | Loss 1.1367(1.1376) | Error 0.0135(0.0129) Steps 416(419.74) | Grad Norm 0.2307(1.7930) | Total Time 10.00(10.00)\n",
      "Iter 2585 | Time 30.2244(30.1260) | Bit/dim 1.1105(1.1165) | Xent 0.0416(0.0418) | Loss 1.1313(1.1374) | Error 0.0122(0.0129) Steps 416(419.63) | Grad Norm 0.3858(1.7508) | Total Time 10.00(10.00)\n",
      "Iter 2586 | Time 29.6291(30.1111) | Bit/dim 1.1165(1.1165) | Xent 0.0411(0.0417) | Loss 1.1371(1.1374) | Error 0.0140(0.0129) Steps 416(419.52) | Grad Norm 0.5659(1.7153) | Total Time 10.00(10.00)\n",
      "Iter 2587 | Time 29.0105(30.0781) | Bit/dim 1.1088(1.1163) | Xent 0.0450(0.0418) | Loss 1.1313(1.1372) | Error 0.0144(0.0129) Steps 422(419.59) | Grad Norm 0.6491(1.6833) | Total Time 10.00(10.00)\n",
      "Iter 2588 | Time 29.4452(30.0591) | Bit/dim 1.1121(1.1161) | Xent 0.0451(0.0419) | Loss 1.1346(1.1371) | Error 0.0128(0.0129) Steps 416(419.48) | Grad Norm 0.2931(1.6416) | Total Time 10.00(10.00)\n",
      "Iter 2589 | Time 30.1140(30.0608) | Bit/dim 1.1096(1.1159) | Xent 0.0410(0.0419) | Loss 1.1301(1.1369) | Error 0.0132(0.0129) Steps 416(419.38) | Grad Norm 0.2351(1.5994) | Total Time 10.00(10.00)\n",
      "Iter 2590 | Time 29.1145(30.0324) | Bit/dim 1.1105(1.1158) | Xent 0.0416(0.0419) | Loss 1.1313(1.1367) | Error 0.0129(0.0129) Steps 416(419.28) | Grad Norm 0.5128(1.5668) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0370 | Time 16.6880, Epoch Time 237.3955(239.5160), Bit/dim 1.1064(best: 1.1056), Xent 0.0293, Loss 1.1210, Error 0.0098(best: 0.0091)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2591 | Time 30.3256(30.0412) | Bit/dim 1.1127(1.1157) | Xent 0.0390(0.0418) | Loss 1.1322(1.1366) | Error 0.0134(0.0130) Steps 416(419.18) | Grad Norm 0.4091(1.5321) | Total Time 10.00(10.00)\n",
      "Iter 2592 | Time 28.9553(30.0086) | Bit/dim 1.1132(1.1156) | Xent 0.0452(0.0419) | Loss 1.1358(1.1366) | Error 0.0135(0.0130) Steps 416(419.08) | Grad Norm 0.5589(1.5029) | Total Time 10.00(10.00)\n",
      "Iter 2593 | Time 28.8084(29.9726) | Bit/dim 1.1127(1.1155) | Xent 0.0417(0.0419) | Loss 1.1336(1.1365) | Error 0.0129(0.0130) Steps 416(418.99) | Grad Norm 0.3248(1.4675) | Total Time 10.00(10.00)\n",
      "Iter 2594 | Time 29.2086(29.9497) | Bit/dim 1.1109(1.1154) | Xent 0.0377(0.0418) | Loss 1.1297(1.1363) | Error 0.0131(0.0130) Steps 422(419.08) | Grad Norm 0.2407(1.4307) | Total Time 10.00(10.00)\n",
      "Iter 2595 | Time 28.8498(29.9167) | Bit/dim 1.1068(1.1151) | Xent 0.0399(0.0417) | Loss 1.1267(1.1360) | Error 0.0111(0.0129) Steps 416(418.99) | Grad Norm 0.2385(1.3950) | Total Time 10.00(10.00)\n",
      "Iter 2596 | Time 28.4733(29.8734) | Bit/dim 1.1151(1.1151) | Xent 0.0452(0.0418) | Loss 1.1377(1.1360) | Error 0.0146(0.0130) Steps 416(418.90) | Grad Norm 0.5995(1.3711) | Total Time 10.00(10.00)\n",
      "Iter 2597 | Time 29.6311(29.8661) | Bit/dim 1.1134(1.1151) | Xent 0.0355(0.0416) | Loss 1.1312(1.1359) | Error 0.0090(0.0128) Steps 416(418.81) | Grad Norm 0.2154(1.3364) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0371 | Time 16.7752, Epoch Time 233.6692(239.3406), Bit/dim 1.1057(best: 1.1056), Xent 0.0271, Loss 1.1193, Error 0.0095(best: 0.0091)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2598 | Time 28.5442(29.8264) | Bit/dim 1.1161(1.1151) | Xent 0.0333(0.0414) | Loss 1.1328(1.1358) | Error 0.0115(0.0128) Steps 416(418.73) | Grad Norm 0.1321(1.3003) | Total Time 10.00(10.00)\n",
      "Iter 2599 | Time 29.2400(29.8089) | Bit/dim 1.1187(1.1152) | Xent 0.0542(0.0418) | Loss 1.1458(1.1361) | Error 0.0151(0.0129) Steps 416(418.65) | Grad Norm 0.2765(1.2696) | Total Time 10.00(10.00)\n",
      "Iter 2600 | Time 28.3782(29.7659) | Bit/dim 1.1123(1.1151) | Xent 0.0377(0.0416) | Loss 1.1311(1.1359) | Error 0.0124(0.0129) Steps 416(418.57) | Grad Norm 0.3822(1.2430) | Total Time 10.00(10.00)\n",
      "Iter 2601 | Time 28.9542(29.7416) | Bit/dim 1.1080(1.1149) | Xent 0.0426(0.0417) | Loss 1.1293(1.1357) | Error 0.0119(0.0128) Steps 416(418.49) | Grad Norm 0.2529(1.2133) | Total Time 10.00(10.00)\n",
      "Iter 2602 | Time 28.4982(29.7043) | Bit/dim 1.1091(1.1147) | Xent 0.0424(0.0417) | Loss 1.1303(1.1356) | Error 0.0122(0.0128) Steps 416(418.42) | Grad Norm 0.2852(1.1854) | Total Time 10.00(10.00)\n",
      "Iter 2603 | Time 28.4213(29.6658) | Bit/dim 1.1095(1.1146) | Xent 0.0447(0.0418) | Loss 1.1318(1.1355) | Error 0.0132(0.0128) Steps 422(418.52) | Grad Norm 0.2672(1.1579) | Total Time 10.00(10.00)\n",
      "Iter 2604 | Time 29.1987(29.6518) | Bit/dim 1.1092(1.1144) | Xent 0.0387(0.0417) | Loss 1.1286(1.1353) | Error 0.0111(0.0128) Steps 416(418.45) | Grad Norm 0.2411(1.1304) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0372 | Time 16.7858, Epoch Time 230.7543(239.0830), Bit/dim 1.1059(best: 1.1056), Xent 0.0296, Loss 1.1207, Error 0.0105(best: 0.0091)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2605 | Time 28.4293(29.6151) | Bit/dim 1.1150(1.1144) | Xent 0.0368(0.0415) | Loss 1.1334(1.1352) | Error 0.0115(0.0127) Steps 416(418.37) | Grad Norm 0.2827(1.1049) | Total Time 10.00(10.00)\n",
      "Iter 2606 | Time 30.0239(29.6274) | Bit/dim 1.1121(1.1144) | Xent 0.0410(0.0415) | Loss 1.1326(1.1351) | Error 0.0122(0.0127) Steps 422(418.48) | Grad Norm 0.2239(1.0785) | Total Time 10.00(10.00)\n",
      "Iter 2607 | Time 29.9548(29.6372) | Bit/dim 1.1061(1.1141) | Xent 0.0350(0.0413) | Loss 1.1236(1.1348) | Error 0.0115(0.0127) Steps 416(418.41) | Grad Norm 0.3193(1.0557) | Total Time 10.00(10.00)\n",
      "Iter 2608 | Time 28.5115(29.6034) | Bit/dim 1.1109(1.1140) | Xent 0.0405(0.0413) | Loss 1.1311(1.1347) | Error 0.0121(0.0127) Steps 416(418.34) | Grad Norm 0.1732(1.0293) | Total Time 10.00(10.00)\n",
      "Iter 2609 | Time 28.6415(29.5746) | Bit/dim 1.1094(1.1139) | Xent 0.0483(0.0415) | Loss 1.1336(1.1346) | Error 0.0138(0.0127) Steps 416(418.27) | Grad Norm 0.2298(1.0053) | Total Time 10.00(10.00)\n",
      "Iter 2610 | Time 28.8172(29.5518) | Bit/dim 1.1136(1.1139) | Xent 0.0452(0.0416) | Loss 1.1362(1.1347) | Error 0.0132(0.0127) Steps 422(418.38) | Grad Norm 0.2031(0.9812) | Total Time 10.00(10.00)\n",
      "Iter 2611 | Time 29.8760(29.5616) | Bit/dim 1.1144(1.1139) | Xent 0.0393(0.0416) | Loss 1.1340(1.1347) | Error 0.0139(0.0128) Steps 422(418.49) | Grad Norm 0.2149(0.9582) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0373 | Time 16.3949, Epoch Time 233.1591(238.9053), Bit/dim 1.1062(best: 1.1056), Xent 0.0277, Loss 1.1200, Error 0.0093(best: 0.0091)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2612 | Time 28.1347(29.5188) | Bit/dim 1.1123(1.1138) | Xent 0.0340(0.0413) | Loss 1.1293(1.1345) | Error 0.0105(0.0127) Steps 422(418.59) | Grad Norm 0.2081(0.9357) | Total Time 10.00(10.00)\n",
      "Iter 2613 | Time 29.8363(29.5283) | Bit/dim 1.1103(1.1137) | Xent 0.0451(0.0414) | Loss 1.1329(1.1345) | Error 0.0134(0.0127) Steps 416(418.51) | Grad Norm 0.1881(0.9133) | Total Time 10.00(10.00)\n",
      "Iter 2614 | Time 28.8506(29.5080) | Bit/dim 1.1137(1.1137) | Xent 0.0402(0.0414) | Loss 1.1338(1.1344) | Error 0.0120(0.0127) Steps 416(418.44) | Grad Norm 0.4770(0.9002) | Total Time 10.00(10.00)\n",
      "Iter 2615 | Time 28.9140(29.4901) | Bit/dim 1.1089(1.1136) | Xent 0.0397(0.0414) | Loss 1.1288(1.1343) | Error 0.0122(0.0127) Steps 422(418.55) | Grad Norm 0.2104(0.8795) | Total Time 10.00(10.00)\n",
      "Iter 2616 | Time 28.7252(29.4672) | Bit/dim 1.1138(1.1136) | Xent 0.0432(0.0414) | Loss 1.1354(1.1343) | Error 0.0126(0.0127) Steps 416(418.47) | Grad Norm 0.3302(0.8630) | Total Time 10.00(10.00)\n",
      "Iter 2617 | Time 28.8147(29.4476) | Bit/dim 1.1068(1.1134) | Xent 0.0331(0.0412) | Loss 1.1233(1.1340) | Error 0.0099(0.0126) Steps 416(418.40) | Grad Norm 0.1992(0.8431) | Total Time 10.00(10.00)\n",
      "Iter 2618 | Time 28.2670(29.4122) | Bit/dim 1.1149(1.1134) | Xent 0.0347(0.0410) | Loss 1.1323(1.1339) | Error 0.0118(0.0126) Steps 416(418.32) | Grad Norm 0.3151(0.8273) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0374 | Time 16.5987, Epoch Time 230.4328(238.6511), Bit/dim 1.1060(best: 1.1056), Xent 0.0280, Loss 1.1200, Error 0.0094(best: 0.0091)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2619 | Time 28.5821(29.3873) | Bit/dim 1.1097(1.1133) | Xent 0.0376(0.0409) | Loss 1.1285(1.1338) | Error 0.0099(0.0125) Steps 416(418.25) | Grad Norm 0.1868(0.8081) | Total Time 10.00(10.00)\n",
      "Iter 2620 | Time 28.9884(29.3753) | Bit/dim 1.1127(1.1133) | Xent 0.0343(0.0407) | Loss 1.1299(1.1336) | Error 0.0105(0.0124) Steps 422(418.37) | Grad Norm 0.1759(0.7891) | Total Time 10.00(10.00)\n",
      "Iter 2621 | Time 28.5763(29.3514) | Bit/dim 1.1083(1.1132) | Xent 0.0411(0.0407) | Loss 1.1289(1.1335) | Error 0.0110(0.0124) Steps 422(418.48) | Grad Norm 0.2161(0.7719) | Total Time 10.00(10.00)\n",
      "Iter 2622 | Time 28.5896(29.3285) | Bit/dim 1.1125(1.1131) | Xent 0.0434(0.0408) | Loss 1.1342(1.1335) | Error 0.0139(0.0124) Steps 422(418.58) | Grad Norm 0.2994(0.7577) | Total Time 10.00(10.00)\n",
      "Iter 2623 | Time 28.7875(29.3123) | Bit/dim 1.1161(1.1132) | Xent 0.0426(0.0408) | Loss 1.1374(1.1336) | Error 0.0135(0.0125) Steps 416(418.50) | Grad Norm 0.2037(0.7411) | Total Time 10.00(10.00)\n",
      "Iter 2624 | Time 29.1731(29.3081) | Bit/dim 1.1140(1.1133) | Xent 0.0361(0.0407) | Loss 1.1321(1.1336) | Error 0.0110(0.0124) Steps 422(418.61) | Grad Norm 0.2082(0.7251) | Total Time 10.00(10.00)\n",
      "Iter 2625 | Time 28.9858(29.2984) | Bit/dim 1.1108(1.1132) | Xent 0.0382(0.0406) | Loss 1.1299(1.1335) | Error 0.0108(0.0124) Steps 416(418.53) | Grad Norm 0.2428(0.7106) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0375 | Time 16.5785, Epoch Time 230.6810(238.4120), Bit/dim 1.1063(best: 1.1056), Xent 0.0270, Loss 1.1197, Error 0.0095(best: 0.0091)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2626 | Time 29.5835(29.3070) | Bit/dim 1.1104(1.1131) | Xent 0.0441(0.0407) | Loss 1.1324(1.1334) | Error 0.0139(0.0124) Steps 422(418.63) | Grad Norm 0.4426(0.7026) | Total Time 10.00(10.00)\n",
      "Iter 2627 | Time 30.2558(29.3354) | Bit/dim 1.1170(1.1132) | Xent 0.0407(0.0407) | Loss 1.1373(1.1336) | Error 0.0125(0.0124) Steps 422(418.74) | Grad Norm 0.2118(0.6879) | Total Time 10.00(10.00)\n",
      "Iter 2628 | Time 28.4994(29.3104) | Bit/dim 1.1153(1.1133) | Xent 0.0429(0.0408) | Loss 1.1367(1.1337) | Error 0.0136(0.0124) Steps 416(418.65) | Grad Norm 0.4778(0.6816) | Total Time 10.00(10.00)\n",
      "Iter 2629 | Time 28.8820(29.2975) | Bit/dim 1.1087(1.1131) | Xent 0.0345(0.0406) | Loss 1.1259(1.1334) | Error 0.0110(0.0124) Steps 416(418.57) | Grad Norm 0.2067(0.6673) | Total Time 10.00(10.00)\n",
      "Iter 2630 | Time 30.0950(29.3214) | Bit/dim 1.1066(1.1129) | Xent 0.0409(0.0406) | Loss 1.1270(1.1332) | Error 0.0144(0.0125) Steps 416(418.50) | Grad Norm 0.2536(0.6549) | Total Time 10.00(10.00)\n",
      "Iter 2631 | Time 29.0962(29.3147) | Bit/dim 1.1132(1.1129) | Xent 0.0455(0.0407) | Loss 1.1359(1.1333) | Error 0.0134(0.0125) Steps 416(418.42) | Grad Norm 0.2528(0.6429) | Total Time 10.00(10.00)\n",
      "Iter 2632 | Time 29.4778(29.3196) | Bit/dim 1.1085(1.1128) | Xent 0.0402(0.0407) | Loss 1.1286(1.1332) | Error 0.0112(0.0125) Steps 416(418.35) | Grad Norm 0.2280(0.6304) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0376 | Time 16.8053, Epoch Time 235.3754(238.3209), Bit/dim 1.1052(best: 1.1056), Xent 0.0287, Loss 1.1195, Error 0.0105(best: 0.0091)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2633 | Time 28.6934(29.3008) | Bit/dim 1.1138(1.1128) | Xent 0.0386(0.0407) | Loss 1.1331(1.1332) | Error 0.0125(0.0125) Steps 416(418.28) | Grad Norm 0.2516(0.6190) | Total Time 10.00(10.00)\n",
      "Iter 2634 | Time 30.2214(29.3284) | Bit/dim 1.1107(1.1128) | Xent 0.0397(0.0406) | Loss 1.1305(1.1331) | Error 0.0118(0.0124) Steps 416(418.21) | Grad Norm 0.1977(0.6064) | Total Time 10.00(10.00)\n",
      "Iter 2635 | Time 28.9630(29.3174) | Bit/dim 1.1073(1.1126) | Xent 0.0400(0.0406) | Loss 1.1273(1.1329) | Error 0.0125(0.0124) Steps 416(418.14) | Grad Norm 0.3069(0.5974) | Total Time 10.00(10.00)\n",
      "Iter 2636 | Time 28.9695(29.3070) | Bit/dim 1.1129(1.1126) | Xent 0.0323(0.0404) | Loss 1.1291(1.1328) | Error 0.0115(0.0124) Steps 416(418.08) | Grad Norm 0.2628(0.5874) | Total Time 10.00(10.00)\n",
      "Iter 2637 | Time 28.5896(29.2855) | Bit/dim 1.1110(1.1126) | Xent 0.0424(0.0404) | Loss 1.1322(1.1328) | Error 0.0126(0.0124) Steps 416(418.02) | Grad Norm 0.2199(0.5764) | Total Time 10.00(10.00)\n",
      "Iter 2638 | Time 28.7881(29.2706) | Bit/dim 1.1124(1.1126) | Xent 0.0381(0.0404) | Loss 1.1314(1.1327) | Error 0.0135(0.0124) Steps 416(417.96) | Grad Norm 0.3375(0.5692) | Total Time 10.00(10.00)\n",
      "Iter 2639 | Time 28.9559(29.2611) | Bit/dim 1.1120(1.1126) | Xent 0.0466(0.0405) | Loss 1.1353(1.1328) | Error 0.0142(0.0125) Steps 416(417.90) | Grad Norm 0.2128(0.5585) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0377 | Time 16.4615, Epoch Time 232.1479(238.1357), Bit/dim 1.1058(best: 1.1052), Xent 0.0273, Loss 1.1194, Error 0.0095(best: 0.0091)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2640 | Time 29.6938(29.2741) | Bit/dim 1.1100(1.1125) | Xent 0.0356(0.0404) | Loss 1.1278(1.1327) | Error 0.0109(0.0125) Steps 422(418.02) | Grad Norm 0.3010(0.5508) | Total Time 10.00(10.00)\n",
      "Iter 2641 | Time 29.0139(29.2663) | Bit/dim 1.1113(1.1124) | Xent 0.0379(0.0403) | Loss 1.1303(1.1326) | Error 0.0119(0.0124) Steps 416(417.96) | Grad Norm 0.3009(0.5433) | Total Time 10.00(10.00)\n",
      "Iter 2642 | Time 28.9037(29.2554) | Bit/dim 1.1156(1.1125) | Xent 0.0359(0.0402) | Loss 1.1336(1.1326) | Error 0.0121(0.0124) Steps 422(418.08) | Grad Norm 0.3246(0.5367) | Total Time 10.00(10.00)\n",
      "Iter 2643 | Time 28.8700(29.2439) | Bit/dim 1.1132(1.1126) | Xent 0.0374(0.0401) | Loss 1.1318(1.1326) | Error 0.0111(0.0124) Steps 422(418.20) | Grad Norm 0.1959(0.5265) | Total Time 10.00(10.00)\n",
      "Iter 2644 | Time 28.7441(29.2289) | Bit/dim 1.1069(1.1124) | Xent 0.0383(0.0400) | Loss 1.1260(1.1324) | Error 0.0119(0.0124) Steps 416(418.13) | Grad Norm 0.2526(0.5183) | Total Time 10.00(10.00)\n",
      "Iter 2645 | Time 28.3831(29.2035) | Bit/dim 1.1095(1.1123) | Xent 0.0442(0.0402) | Loss 1.1316(1.1324) | Error 0.0124(0.0124) Steps 416(418.07) | Grad Norm 0.3961(0.5146) | Total Time 10.00(10.00)\n",
      "Iter 2646 | Time 28.7065(29.1886) | Bit/dim 1.1146(1.1124) | Xent 0.0471(0.0404) | Loss 1.1382(1.1326) | Error 0.0144(0.0124) Steps 416(418.01) | Grad Norm 0.3556(0.5098) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0378 | Time 16.3621, Epoch Time 231.3034(237.9308), Bit/dim 1.1059(best: 1.1052), Xent 0.0271, Loss 1.1194, Error 0.0092(best: 0.0091)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2647 | Time 28.5293(29.1688) | Bit/dim 1.1099(1.1123) | Xent 0.0403(0.0404) | Loss 1.1300(1.1325) | Error 0.0131(0.0125) Steps 416(417.95) | Grad Norm 0.2627(0.5024) | Total Time 10.00(10.00)\n",
      "Iter 2648 | Time 28.6978(29.1547) | Bit/dim 1.1097(1.1122) | Xent 0.0392(0.0403) | Loss 1.1293(1.1324) | Error 0.0130(0.0125) Steps 416(417.89) | Grad Norm 0.1625(0.4922) | Total Time 10.00(10.00)\n",
      "Iter 2649 | Time 28.9620(29.1489) | Bit/dim 1.1122(1.1122) | Xent 0.0370(0.0402) | Loss 1.1307(1.1323) | Error 0.0116(0.0124) Steps 416(417.83) | Grad Norm 0.3157(0.4869) | Total Time 10.00(10.00)\n",
      "Iter 2650 | Time 29.0731(29.1466) | Bit/dim 1.1128(1.1122) | Xent 0.0413(0.0403) | Loss 1.1334(1.1324) | Error 0.0140(0.0125) Steps 422(417.96) | Grad Norm 0.2529(0.4799) | Total Time 10.00(10.00)\n",
      "Iter 2651 | Time 28.6877(29.1329) | Bit/dim 1.1121(1.1122) | Xent 0.0406(0.0403) | Loss 1.1324(1.1324) | Error 0.0119(0.0125) Steps 416(417.90) | Grad Norm 0.2250(0.4723) | Total Time 10.00(10.00)\n",
      "Iter 2652 | Time 28.6095(29.1172) | Bit/dim 1.1170(1.1124) | Xent 0.0480(0.0405) | Loss 1.1409(1.1326) | Error 0.0141(0.0125) Steps 416(417.84) | Grad Norm 0.1928(0.4639) | Total Time 10.00(10.00)\n",
      "Iter 2653 | Time 28.7839(29.1072) | Bit/dim 1.1108(1.1123) | Xent 0.0420(0.0406) | Loss 1.1319(1.1326) | Error 0.0136(0.0126) Steps 416(417.79) | Grad Norm 0.2244(0.4567) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0379 | Time 16.7744, Epoch Time 230.5962(237.7107), Bit/dim 1.1063(best: 1.1052), Xent 0.0284, Loss 1.1205, Error 0.0091(best: 0.0091)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2654 | Time 30.0927(29.1367) | Bit/dim 1.1067(1.1122) | Xent 0.0472(0.0408) | Loss 1.1303(1.1325) | Error 0.0128(0.0126) Steps 422(417.91) | Grad Norm 0.2219(0.4497) | Total Time 10.00(10.00)\n",
      "Iter 2655 | Time 28.8189(29.1272) | Bit/dim 1.1082(1.1120) | Xent 0.0475(0.0410) | Loss 1.1319(1.1325) | Error 0.0152(0.0126) Steps 416(417.85) | Grad Norm 0.2145(0.4426) | Total Time 10.00(10.00)\n",
      "Iter 2656 | Time 28.6420(29.1126) | Bit/dim 1.1149(1.1121) | Xent 0.0388(0.0409) | Loss 1.1343(1.1326) | Error 0.0115(0.0126) Steps 416(417.80) | Grad Norm 0.4338(0.4423) | Total Time 10.00(10.00)\n",
      "Iter 2657 | Time 28.8867(29.1059) | Bit/dim 1.1124(1.1121) | Xent 0.0388(0.0408) | Loss 1.1318(1.1325) | Error 0.0126(0.0126) Steps 416(417.75) | Grad Norm 0.3994(0.4411) | Total Time 10.00(10.00)\n",
      "Iter 2658 | Time 28.9254(29.1004) | Bit/dim 1.1155(1.1122) | Xent 0.0397(0.0408) | Loss 1.1353(1.1326) | Error 0.0110(0.0126) Steps 416(417.69) | Grad Norm 0.4417(0.4411) | Total Time 10.00(10.00)\n",
      "Iter 2659 | Time 30.1643(29.1324) | Bit/dim 1.1132(1.1123) | Xent 0.0389(0.0407) | Loss 1.1326(1.1326) | Error 0.0118(0.0125) Steps 416(417.64) | Grad Norm 0.5195(0.4434) | Total Time 10.00(10.00)\n",
      "Iter 2660 | Time 28.9878(29.1280) | Bit/dim 1.1073(1.1121) | Xent 0.0436(0.0408) | Loss 1.1291(1.1325) | Error 0.0140(0.0126) Steps 416(417.59) | Grad Norm 0.4153(0.4426) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0380 | Time 16.6684, Epoch Time 233.9252(237.5972), Bit/dim 1.1060(best: 1.1052), Xent 0.0277, Loss 1.1199, Error 0.0093(best: 0.0091)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2661 | Time 31.6232(29.2029) | Bit/dim 1.1119(1.1121) | Xent 0.0386(0.0408) | Loss 1.1312(1.1325) | Error 0.0121(0.0126) Steps 422(417.73) | Grad Norm 0.3322(0.4393) | Total Time 10.00(10.00)\n",
      "Iter 2662 | Time 30.4422(29.2401) | Bit/dim 1.1105(1.1121) | Xent 0.0381(0.0407) | Loss 1.1295(1.1324) | Error 0.0119(0.0125) Steps 416(417.67) | Grad Norm 0.2370(0.4332) | Total Time 10.00(10.00)\n",
      "Iter 2663 | Time 27.9608(29.2017) | Bit/dim 1.1136(1.1121) | Xent 0.0413(0.0407) | Loss 1.1342(1.1324) | Error 0.0131(0.0126) Steps 422(417.80) | Grad Norm 0.6603(0.4400) | Total Time 10.00(10.00)\n",
      "Iter 2664 | Time 29.8360(29.2207) | Bit/dim 1.1104(1.1120) | Xent 0.0448(0.0408) | Loss 1.1328(1.1325) | Error 0.0140(0.0126) Steps 422(417.93) | Grad Norm 0.2728(0.4350) | Total Time 10.00(10.00)\n",
      "Iter 2665 | Time 28.8456(29.2095) | Bit/dim 1.1097(1.1120) | Xent 0.0366(0.0407) | Loss 1.1280(1.1323) | Error 0.0111(0.0126) Steps 422(418.05) | Grad Norm 0.1602(0.4268) | Total Time 10.00(10.00)\n",
      "Iter 2666 | Time 28.4030(29.1853) | Bit/dim 1.1138(1.1120) | Xent 0.0402(0.0407) | Loss 1.1339(1.1324) | Error 0.0119(0.0125) Steps 416(417.99) | Grad Norm 0.1784(0.4193) | Total Time 10.00(10.00)\n",
      "Iter 2667 | Time 28.9393(29.1779) | Bit/dim 1.1113(1.1120) | Xent 0.0401(0.0407) | Loss 1.1314(1.1323) | Error 0.0120(0.0125) Steps 416(417.93) | Grad Norm 0.2596(0.4145) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0381 | Time 16.3772, Epoch Time 234.9446(237.5176), Bit/dim 1.1057(best: 1.1052), Xent 0.0280, Loss 1.1197, Error 0.0096(best: 0.0091)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2668 | Time 29.0737(29.1748) | Bit/dim 1.1147(1.1121) | Xent 0.0378(0.0406) | Loss 1.1337(1.1324) | Error 0.0118(0.0125) Steps 416(417.87) | Grad Norm 0.3674(0.4131) | Total Time 10.00(10.00)\n",
      "Iter 2669 | Time 28.5815(29.1570) | Bit/dim 1.1115(1.1121) | Xent 0.0428(0.0406) | Loss 1.1329(1.1324) | Error 0.0136(0.0125) Steps 416(417.82) | Grad Norm 0.2225(0.4074) | Total Time 10.00(10.00)\n",
      "Iter 2670 | Time 29.0252(29.1530) | Bit/dim 1.1147(1.1122) | Xent 0.0404(0.0406) | Loss 1.1349(1.1325) | Error 0.0126(0.0125) Steps 416(417.76) | Grad Norm 0.2795(0.4035) | Total Time 10.00(10.00)\n",
      "Iter 2671 | Time 28.6893(29.1391) | Bit/dim 1.1138(1.1122) | Xent 0.0452(0.0408) | Loss 1.1364(1.1326) | Error 0.0139(0.0126) Steps 422(417.89) | Grad Norm 0.2265(0.3982) | Total Time 10.00(10.00)\n",
      "Iter 2672 | Time 28.6619(29.1248) | Bit/dim 1.1117(1.1122) | Xent 0.0430(0.0408) | Loss 1.1332(1.1326) | Error 0.0135(0.0126) Steps 416(417.83) | Grad Norm 0.1643(0.3912) | Total Time 10.00(10.00)\n",
      "Iter 2673 | Time 28.2292(29.0979) | Bit/dim 1.1073(1.1120) | Xent 0.0407(0.0408) | Loss 1.1276(1.1325) | Error 0.0139(0.0126) Steps 422(417.96) | Grad Norm 0.2361(0.3866) | Total Time 10.00(10.00)\n",
      "Iter 2674 | Time 29.8201(29.1196) | Bit/dim 1.1050(1.1118) | Xent 0.0359(0.0407) | Loss 1.1229(1.1322) | Error 0.0120(0.0126) Steps 416(417.90) | Grad Norm 0.5630(0.3919) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0382 | Time 16.5817, Epoch Time 231.1014(237.3251), Bit/dim 1.1050(best: 1.1052), Xent 0.0295, Loss 1.1197, Error 0.0103(best: 0.0091)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2675 | Time 28.9139(29.1134) | Bit/dim 1.1094(1.1118) | Xent 0.0432(0.0408) | Loss 1.1310(1.1321) | Error 0.0132(0.0126) Steps 416(417.84) | Grad Norm 0.1952(0.3860) | Total Time 10.00(10.00)\n",
      "Iter 2676 | Time 29.0243(29.1107) | Bit/dim 1.1127(1.1118) | Xent 0.0392(0.0407) | Loss 1.1323(1.1321) | Error 0.0122(0.0126) Steps 416(417.79) | Grad Norm 0.1761(0.3797) | Total Time 10.00(10.00)\n",
      "Iter 2677 | Time 29.0680(29.1095) | Bit/dim 1.1072(1.1116) | Xent 0.0382(0.0406) | Loss 1.1263(1.1320) | Error 0.0114(0.0126) Steps 422(417.91) | Grad Norm 0.3797(0.3797) | Total Time 10.00(10.00)\n",
      "Iter 2678 | Time 28.9897(29.1059) | Bit/dim 1.1113(1.1116) | Xent 0.0382(0.0406) | Loss 1.1304(1.1319) | Error 0.0119(0.0126) Steps 422(418.04) | Grad Norm 0.2979(0.3772) | Total Time 10.00(10.00)\n",
      "Iter 2679 | Time 30.4673(29.1467) | Bit/dim 1.1112(1.1116) | Xent 0.0412(0.0406) | Loss 1.1318(1.1319) | Error 0.0120(0.0126) Steps 416(417.97) | Grad Norm 0.2513(0.3734) | Total Time 10.00(10.00)\n",
      "Iter 2680 | Time 28.4553(29.1260) | Bit/dim 1.1082(1.1115) | Xent 0.0395(0.0406) | Loss 1.1280(1.1318) | Error 0.0122(0.0125) Steps 422(418.09) | Grad Norm 0.2222(0.3689) | Total Time 10.00(10.00)\n",
      "Iter 2681 | Time 28.7288(29.1140) | Bit/dim 1.1178(1.1117) | Xent 0.0394(0.0405) | Loss 1.1375(1.1320) | Error 0.0112(0.0125) Steps 422(418.21) | Grad Norm 0.3139(0.3672) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0383 | Time 16.5758, Epoch Time 232.6431(237.1846), Bit/dim 1.1056(best: 1.1050), Xent 0.0295, Loss 1.1204, Error 0.0104(best: 0.0091)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2682 | Time 29.2565(29.1183) | Bit/dim 1.1123(1.1117) | Xent 0.0354(0.0404) | Loss 1.1300(1.1319) | Error 0.0115(0.0125) Steps 416(418.15) | Grad Norm 0.4921(0.3710) | Total Time 10.00(10.00)\n",
      "Iter 2683 | Time 29.6820(29.1352) | Bit/dim 1.1057(1.1115) | Xent 0.0406(0.0404) | Loss 1.1260(1.1317) | Error 0.0130(0.0125) Steps 416(418.08) | Grad Norm 0.1722(0.3650) | Total Time 10.00(10.00)\n",
      "Iter 2684 | Time 30.9593(29.1900) | Bit/dim 1.1082(1.1114) | Xent 0.0399(0.0404) | Loss 1.1282(1.1316) | Error 0.0146(0.0126) Steps 416(418.02) | Grad Norm 0.2188(0.3606) | Total Time 10.00(10.00)\n",
      "Iter 2685 | Time 29.3113(29.1936) | Bit/dim 1.1107(1.1114) | Xent 0.0404(0.0404) | Loss 1.1308(1.1316) | Error 0.0131(0.0126) Steps 416(417.96) | Grad Norm 0.2391(0.3570) | Total Time 10.00(10.00)\n",
      "Iter 2686 | Time 30.1423(29.2221) | Bit/dim 1.1104(1.1114) | Xent 0.0442(0.0405) | Loss 1.1325(1.1316) | Error 0.0136(0.0126) Steps 416(417.90) | Grad Norm 0.4170(0.3588) | Total Time 10.00(10.00)\n",
      "Iter 2687 | Time 29.6128(29.2338) | Bit/dim 1.1128(1.1114) | Xent 0.0425(0.0405) | Loss 1.1340(1.1317) | Error 0.0125(0.0126) Steps 416(417.84) | Grad Norm 0.2503(0.3555) | Total Time 10.00(10.00)\n",
      "Iter 2688 | Time 29.0119(29.2271) | Bit/dim 1.1147(1.1115) | Xent 0.0385(0.0405) | Loss 1.1339(1.1318) | Error 0.0118(0.0126) Steps 422(417.97) | Grad Norm 0.2039(0.3510) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0384 | Time 16.7597, Epoch Time 237.2100(237.1854), Bit/dim 1.1054(best: 1.1050), Xent 0.0264, Loss 1.1186, Error 0.0085(best: 0.0091)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2689 | Time 30.0358(29.2514) | Bit/dim 1.1122(1.1116) | Xent 0.0451(0.0406) | Loss 1.1347(1.1319) | Error 0.0145(0.0126) Steps 416(417.91) | Grad Norm 0.3013(0.3495) | Total Time 10.00(10.00)\n",
      "Iter 2690 | Time 29.5547(29.2605) | Bit/dim 1.1074(1.1114) | Xent 0.0396(0.0406) | Loss 1.1272(1.1317) | Error 0.0122(0.0126) Steps 416(417.85) | Grad Norm 0.2946(0.3479) | Total Time 10.00(10.00)\n",
      "Iter 2691 | Time 29.7006(29.2737) | Bit/dim 1.1121(1.1114) | Xent 0.0434(0.0407) | Loss 1.1338(1.1318) | Error 0.0148(0.0127) Steps 416(417.80) | Grad Norm 0.4125(0.3498) | Total Time 10.00(10.00)\n",
      "Iter 2692 | Time 30.5342(29.3115) | Bit/dim 1.1146(1.1115) | Xent 0.0374(0.0406) | Loss 1.1333(1.1318) | Error 0.0112(0.0126) Steps 416(417.74) | Grad Norm 0.2195(0.3459) | Total Time 10.00(10.00)\n",
      "Iter 2693 | Time 29.0418(29.3034) | Bit/dim 1.1078(1.1114) | Xent 0.0347(0.0404) | Loss 1.1252(1.1316) | Error 0.0118(0.0126) Steps 422(417.87) | Grad Norm 0.2386(0.3427) | Total Time 10.00(10.00)\n",
      "Iter 2694 | Time 28.4421(29.2776) | Bit/dim 1.1066(1.1113) | Xent 0.0400(0.0404) | Loss 1.1266(1.1315) | Error 0.0120(0.0126) Steps 416(417.81) | Grad Norm 0.2206(0.3390) | Total Time 10.00(10.00)\n",
      "Iter 2695 | Time 28.9889(29.2689) | Bit/dim 1.1163(1.1114) | Xent 0.0416(0.0404) | Loss 1.1371(1.1316) | Error 0.0148(0.0127) Steps 416(417.76) | Grad Norm 0.6467(0.3482) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0385 | Time 16.7975, Epoch Time 235.3691(237.1309), Bit/dim 1.1052(best: 1.1050), Xent 0.0284, Loss 1.1194, Error 0.0101(best: 0.0085)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2696 | Time 29.2058(29.2670) | Bit/dim 1.1126(1.1115) | Xent 0.0466(0.0406) | Loss 1.1359(1.1318) | Error 0.0138(0.0127) Steps 416(417.71) | Grad Norm 0.3338(0.3478) | Total Time 10.00(10.00)\n",
      "Iter 2697 | Time 29.0032(29.2591) | Bit/dim 1.1132(1.1115) | Xent 0.0439(0.0407) | Loss 1.1352(1.1319) | Error 0.0135(0.0127) Steps 416(417.65) | Grad Norm 0.2497(0.3449) | Total Time 10.00(10.00)\n",
      "Iter 2698 | Time 28.8149(29.2458) | Bit/dim 1.1179(1.1117) | Xent 0.0394(0.0407) | Loss 1.1376(1.1320) | Error 0.0131(0.0127) Steps 416(417.61) | Grad Norm 0.5299(0.3504) | Total Time 10.00(10.00)\n",
      "Iter 2699 | Time 29.8241(29.2631) | Bit/dim 1.1101(1.1117) | Xent 0.0349(0.0405) | Loss 1.1275(1.1319) | Error 0.0115(0.0127) Steps 422(417.74) | Grad Norm 0.4032(0.3520) | Total Time 10.00(10.00)\n",
      "Iter 2700 | Time 29.0412(29.2565) | Bit/dim 1.1119(1.1117) | Xent 0.0424(0.0405) | Loss 1.1331(1.1319) | Error 0.0121(0.0127) Steps 416(417.68) | Grad Norm 0.4227(0.3541) | Total Time 10.00(10.00)\n",
      "Iter 2701 | Time 29.7315(29.2707) | Bit/dim 1.1101(1.1116) | Xent 0.0432(0.0406) | Loss 1.1317(1.1319) | Error 0.0134(0.0127) Steps 416(417.63) | Grad Norm 0.3516(0.3540) | Total Time 10.00(10.00)\n",
      "Iter 2702 | Time 29.2298(29.2695) | Bit/dim 1.1075(1.1115) | Xent 0.0385(0.0406) | Loss 1.1268(1.1318) | Error 0.0130(0.0127) Steps 416(417.59) | Grad Norm 0.4402(0.3566) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0386 | Time 16.3212, Epoch Time 233.4877(237.0216), Bit/dim 1.1053(best: 1.1050), Xent 0.0296, Loss 1.1200, Error 0.0091(best: 0.0085)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2703 | Time 28.7843(29.2549) | Bit/dim 1.1090(1.1114) | Xent 0.0423(0.0406) | Loss 1.1301(1.1317) | Error 0.0122(0.0127) Steps 422(417.72) | Grad Norm 0.1912(0.3517) | Total Time 10.00(10.00)\n",
      "Iter 2704 | Time 29.5896(29.2650) | Bit/dim 1.1127(1.1115) | Xent 0.0403(0.0406) | Loss 1.1329(1.1318) | Error 0.0132(0.0127) Steps 416(417.67) | Grad Norm 0.4913(0.3559) | Total Time 10.00(10.00)\n",
      "Iter 2705 | Time 28.8594(29.2528) | Bit/dim 1.1106(1.1114) | Xent 0.0373(0.0405) | Loss 1.1292(1.1317) | Error 0.0116(0.0127) Steps 422(417.80) | Grad Norm 0.2368(0.3523) | Total Time 10.00(10.00)\n",
      "Iter 2706 | Time 28.5882(29.2329) | Bit/dim 1.1097(1.1114) | Xent 0.0343(0.0403) | Loss 1.1268(1.1315) | Error 0.0108(0.0126) Steps 416(417.74) | Grad Norm 0.1946(0.3476) | Total Time 10.00(10.00)\n",
      "Iter 2707 | Time 28.8387(29.2211) | Bit/dim 1.1106(1.1114) | Xent 0.0400(0.0403) | Loss 1.1306(1.1315) | Error 0.0130(0.0126) Steps 416(417.69) | Grad Norm 0.4526(0.3507) | Total Time 10.00(10.00)\n",
      "Iter 2708 | Time 29.2433(29.2217) | Bit/dim 1.1072(1.1112) | Xent 0.0502(0.0406) | Loss 1.1323(1.1315) | Error 0.0160(0.0127) Steps 416(417.64) | Grad Norm 0.3321(0.3501) | Total Time 10.00(10.00)\n",
      "Iter 2709 | Time 30.1491(29.2495) | Bit/dim 1.1168(1.1114) | Xent 0.0437(0.0407) | Loss 1.1387(1.1318) | Error 0.0131(0.0127) Steps 416(417.59) | Grad Norm 0.2489(0.3471) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0387 | Time 16.5902, Epoch Time 233.0175(236.9015), Bit/dim 1.1055(best: 1.1050), Xent 0.0263, Loss 1.1187, Error 0.0090(best: 0.0085)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2710 | Time 28.4428(29.2253) | Bit/dim 1.1095(1.1113) | Xent 0.0405(0.0407) | Loss 1.1297(1.1317) | Error 0.0126(0.0127) Steps 416(417.54) | Grad Norm 0.2402(0.3439) | Total Time 10.00(10.00)\n",
      "Iter 2711 | Time 28.9867(29.2182) | Bit/dim 1.1091(1.1113) | Xent 0.0355(0.0405) | Loss 1.1268(1.1315) | Error 0.0112(0.0127) Steps 422(417.68) | Grad Norm 0.1937(0.3394) | Total Time 10.00(10.00)\n",
      "Iter 2712 | Time 28.5142(29.1971) | Bit/dim 1.1142(1.1114) | Xent 0.0464(0.0407) | Loss 1.1374(1.1317) | Error 0.0145(0.0127) Steps 416(417.63) | Grad Norm 0.2848(0.3378) | Total Time 10.00(10.00)\n",
      "Iter 2713 | Time 28.5469(29.1776) | Bit/dim 1.1148(1.1115) | Xent 0.0333(0.0405) | Loss 1.1314(1.1317) | Error 0.0095(0.0127) Steps 422(417.76) | Grad Norm 0.2208(0.3342) | Total Time 10.00(10.00)\n",
      "Iter 2714 | Time 28.2561(29.1499) | Bit/dim 1.1096(1.1114) | Xent 0.0474(0.0407) | Loss 1.1333(1.1318) | Error 0.0129(0.0127) Steps 416(417.70) | Grad Norm 0.3850(0.3358) | Total Time 10.00(10.00)\n",
      "Iter 2715 | Time 29.3795(29.1568) | Bit/dim 1.1112(1.1114) | Xent 0.0426(0.0408) | Loss 1.1325(1.1318) | Error 0.0114(0.0126) Steps 422(417.83) | Grad Norm 0.2298(0.3326) | Total Time 10.00(10.00)\n",
      "Iter 2716 | Time 28.9495(29.1506) | Bit/dim 1.1122(1.1114) | Xent 0.0371(0.0406) | Loss 1.1307(1.1317) | Error 0.0114(0.0126) Steps 422(417.96) | Grad Norm 0.1733(0.3278) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0388 | Time 16.9257, Epoch Time 230.4045(236.7066), Bit/dim 1.1054(best: 1.1050), Xent 0.0260, Loss 1.1184, Error 0.0090(best: 0.0085)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2717 | Time 29.9908(29.1758) | Bit/dim 1.1077(1.1113) | Xent 0.0422(0.0407) | Loss 1.1288(1.1317) | Error 0.0124(0.0126) Steps 422(418.08) | Grad Norm 0.2178(0.3245) | Total Time 10.00(10.00)\n",
      "Iter 2718 | Time 29.7357(29.1926) | Bit/dim 1.1108(1.1113) | Xent 0.0370(0.0406) | Loss 1.1293(1.1316) | Error 0.0114(0.0125) Steps 416(418.02) | Grad Norm 0.3593(0.3256) | Total Time 10.00(10.00)\n",
      "Iter 2719 | Time 29.6293(29.2057) | Bit/dim 1.1112(1.1113) | Xent 0.0372(0.0405) | Loss 1.1298(1.1315) | Error 0.0109(0.0125) Steps 416(417.96) | Grad Norm 0.5270(0.3316) | Total Time 10.00(10.00)\n",
      "Iter 2720 | Time 28.7287(29.1914) | Bit/dim 1.1107(1.1113) | Xent 0.0431(0.0406) | Loss 1.1323(1.1316) | Error 0.0119(0.0125) Steps 416(417.90) | Grad Norm 0.2885(0.3303) | Total Time 10.00(10.00)\n",
      "Iter 2721 | Time 28.6797(29.1760) | Bit/dim 1.1148(1.1114) | Xent 0.0399(0.0405) | Loss 1.1348(1.1317) | Error 0.0119(0.0125) Steps 416(417.84) | Grad Norm 0.2000(0.3264) | Total Time 10.00(10.00)\n",
      "Iter 2722 | Time 28.5378(29.1569) | Bit/dim 1.1072(1.1113) | Xent 0.0402(0.0405) | Loss 1.1273(1.1315) | Error 0.0121(0.0124) Steps 422(417.97) | Grad Norm 0.4245(0.3293) | Total Time 10.00(10.00)\n",
      "Iter 2723 | Time 29.1170(29.1557) | Bit/dim 1.1143(1.1114) | Xent 0.0399(0.0405) | Loss 1.1343(1.1316) | Error 0.0121(0.0124) Steps 416(417.91) | Grad Norm 0.1957(0.3253) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0389 | Time 16.5964, Epoch Time 233.4004(236.6074), Bit/dim 1.1047(best: 1.1050), Xent 0.0265, Loss 1.1179, Error 0.0087(best: 0.0085)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2724 | Time 28.2172(29.1275) | Bit/dim 1.1120(1.1114) | Xent 0.0464(0.0407) | Loss 1.1352(1.1317) | Error 0.0139(0.0125) Steps 416(417.85) | Grad Norm 0.3933(0.3274) | Total Time 10.00(10.00)\n",
      "Iter 2725 | Time 30.0527(29.1553) | Bit/dim 1.1117(1.1114) | Xent 0.0425(0.0407) | Loss 1.1330(1.1318) | Error 0.0148(0.0125) Steps 416(417.79) | Grad Norm 0.2728(0.3257) | Total Time 10.00(10.00)\n",
      "Iter 2726 | Time 28.7471(29.1430) | Bit/dim 1.1103(1.1114) | Xent 0.0416(0.0408) | Loss 1.1311(1.1317) | Error 0.0142(0.0126) Steps 416(417.74) | Grad Norm 0.2170(0.3225) | Total Time 10.00(10.00)\n",
      "Iter 2727 | Time 28.6961(29.1296) | Bit/dim 1.1138(1.1114) | Xent 0.0400(0.0407) | Loss 1.1338(1.1318) | Error 0.0119(0.0126) Steps 416(417.69) | Grad Norm 0.5360(0.3289) | Total Time 10.00(10.00)\n",
      "Iter 2728 | Time 30.1202(29.1593) | Bit/dim 1.1141(1.1115) | Xent 0.0421(0.0408) | Loss 1.1352(1.1319) | Error 0.0141(0.0126) Steps 416(417.64) | Grad Norm 0.1799(0.3244) | Total Time 10.00(10.00)\n",
      "Iter 2729 | Time 28.6553(29.1442) | Bit/dim 1.1055(1.1113) | Xent 0.0430(0.0408) | Loss 1.1270(1.1318) | Error 0.0134(0.0126) Steps 422(417.77) | Grad Norm 0.1931(0.3205) | Total Time 10.00(10.00)\n",
      "Iter 2730 | Time 28.4892(29.1246) | Bit/dim 1.1113(1.1113) | Xent 0.0392(0.0408) | Loss 1.1309(1.1317) | Error 0.0124(0.0126) Steps 416(417.72) | Grad Norm 0.3188(0.3204) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0390 | Time 16.4943, Epoch Time 231.8100(236.4635), Bit/dim 1.1056(best: 1.1047), Xent 0.0268, Loss 1.1190, Error 0.0101(best: 0.0085)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2731 | Time 28.6219(29.1095) | Bit/dim 1.1116(1.1113) | Xent 0.0371(0.0407) | Loss 1.1301(1.1317) | Error 0.0114(0.0126) Steps 416(417.66) | Grad Norm 0.2342(0.3178) | Total Time 10.00(10.00)\n",
      "Iter 2732 | Time 28.8434(29.1015) | Bit/dim 1.1088(1.1113) | Xent 0.0435(0.0408) | Loss 1.1305(1.1316) | Error 0.0148(0.0127) Steps 416(417.61) | Grad Norm 0.3987(0.3203) | Total Time 10.00(10.00)\n",
      "Iter 2733 | Time 28.7937(29.0923) | Bit/dim 1.1151(1.1114) | Xent 0.0411(0.0408) | Loss 1.1357(1.1318) | Error 0.0140(0.0127) Steps 422(417.75) | Grad Norm 0.3049(0.3198) | Total Time 10.00(10.00)\n",
      "Iter 2734 | Time 30.1596(29.1243) | Bit/dim 1.1096(1.1113) | Xent 0.0380(0.0407) | Loss 1.1285(1.1317) | Error 0.0130(0.0127) Steps 416(417.69) | Grad Norm 0.1643(0.3151) | Total Time 10.00(10.00)\n",
      "Iter 2735 | Time 28.9900(29.1203) | Bit/dim 1.1138(1.1114) | Xent 0.0432(0.0408) | Loss 1.1354(1.1318) | Error 0.0132(0.0127) Steps 416(417.64) | Grad Norm 0.4101(0.3180) | Total Time 10.00(10.00)\n",
      "Iter 2736 | Time 28.8210(29.1113) | Bit/dim 1.1070(1.1113) | Xent 0.0422(0.0408) | Loss 1.1281(1.1317) | Error 0.0134(0.0127) Steps 416(417.59) | Grad Norm 0.2686(0.3165) | Total Time 10.00(10.00)\n",
      "Iter 2737 | Time 29.4535(29.1216) | Bit/dim 1.1096(1.1112) | Xent 0.0345(0.0406) | Loss 1.1268(1.1315) | Error 0.0105(0.0127) Steps 416(417.55) | Grad Norm 0.3333(0.3170) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0391 | Time 16.5478, Epoch Time 232.8247(236.3543), Bit/dim 1.1048(best: 1.1047), Xent 0.0278, Loss 1.1187, Error 0.0093(best: 0.0085)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2738 | Time 28.7672(29.1109) | Bit/dim 1.1091(1.1111) | Xent 0.0398(0.0406) | Loss 1.1290(1.1314) | Error 0.0130(0.0127) Steps 416(417.50) | Grad Norm 0.2027(0.3136) | Total Time 10.00(10.00)\n",
      "Iter 2739 | Time 29.4330(29.1206) | Bit/dim 1.1165(1.1113) | Xent 0.0412(0.0406) | Loss 1.1371(1.1316) | Error 0.0124(0.0127) Steps 422(417.63) | Grad Norm 0.4018(0.3162) | Total Time 10.00(10.00)\n",
      "Iter 2740 | Time 29.2890(29.1256) | Bit/dim 1.1099(1.1113) | Xent 0.0492(0.0409) | Loss 1.1345(1.1317) | Error 0.0151(0.0128) Steps 422(417.76) | Grad Norm 0.4480(0.3202) | Total Time 10.00(10.00)\n",
      "Iter 2741 | Time 28.3048(29.1010) | Bit/dim 1.1086(1.1112) | Xent 0.0405(0.0409) | Loss 1.1288(1.1316) | Error 0.0125(0.0127) Steps 416(417.71) | Grad Norm 0.2058(0.3167) | Total Time 10.00(10.00)\n",
      "Iter 2742 | Time 29.5180(29.1135) | Bit/dim 1.1111(1.1112) | Xent 0.0385(0.0408) | Loss 1.1304(1.1316) | Error 0.0116(0.0127) Steps 416(417.66) | Grad Norm 0.2585(0.3150) | Total Time 10.00(10.00)\n",
      "Iter 2743 | Time 28.6692(29.1002) | Bit/dim 1.1113(1.1112) | Xent 0.0352(0.0406) | Loss 1.1288(1.1315) | Error 0.0106(0.0127) Steps 416(417.61) | Grad Norm 0.4651(0.3195) | Total Time 10.00(10.00)\n",
      "Iter 2744 | Time 28.6200(29.0858) | Bit/dim 1.1104(1.1112) | Xent 0.0364(0.0405) | Loss 1.1286(1.1314) | Error 0.0112(0.0126) Steps 416(417.56) | Grad Norm 0.3207(0.3195) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0392 | Time 16.6975, Epoch Time 231.4878(236.2083), Bit/dim 1.1050(best: 1.1047), Xent 0.0293, Loss 1.1197, Error 0.0100(best: 0.0085)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2745 | Time 29.0453(29.0846) | Bit/dim 1.1097(1.1111) | Xent 0.0393(0.0405) | Loss 1.1293(1.1313) | Error 0.0126(0.0126) Steps 422(417.70) | Grad Norm 0.1822(0.3154) | Total Time 10.00(10.00)\n",
      "Iter 2746 | Time 28.6315(29.0710) | Bit/dim 1.1129(1.1112) | Xent 0.0412(0.0405) | Loss 1.1334(1.1314) | Error 0.0119(0.0126) Steps 422(417.82) | Grad Norm 0.2113(0.3123) | Total Time 10.00(10.00)\n",
      "Iter 2747 | Time 28.3657(29.0498) | Bit/dim 1.1094(1.1111) | Xent 0.0436(0.0406) | Loss 1.1312(1.1314) | Error 0.0141(0.0126) Steps 422(417.95) | Grad Norm 0.4176(0.3155) | Total Time 10.00(10.00)\n",
      "Iter 2748 | Time 28.7211(29.0400) | Bit/dim 1.1078(1.1110) | Xent 0.0371(0.0405) | Loss 1.1264(1.1313) | Error 0.0101(0.0126) Steps 416(417.89) | Grad Norm 0.2147(0.3124) | Total Time 10.00(10.00)\n",
      "Iter 2749 | Time 29.2511(29.0463) | Bit/dim 1.1106(1.1110) | Xent 0.0376(0.0404) | Loss 1.1294(1.1312) | Error 0.0115(0.0125) Steps 422(418.01) | Grad Norm 0.2291(0.3099) | Total Time 10.00(10.00)\n",
      "Iter 2750 | Time 28.7547(29.0375) | Bit/dim 1.1126(1.1110) | Xent 0.0401(0.0404) | Loss 1.1327(1.1312) | Error 0.0134(0.0126) Steps 416(417.95) | Grad Norm 0.1889(0.3063) | Total Time 10.00(10.00)\n",
      "Iter 2751 | Time 28.8481(29.0319) | Bit/dim 1.1081(1.1110) | Xent 0.0390(0.0403) | Loss 1.1276(1.1311) | Error 0.0106(0.0125) Steps 416(417.90) | Grad Norm 0.2400(0.3043) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0393 | Time 16.6295, Epoch Time 230.5583(236.0388), Bit/dim 1.1050(best: 1.1047), Xent 0.0298, Loss 1.1199, Error 0.0105(best: 0.0085)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2752 | Time 28.6058(29.0191) | Bit/dim 1.1119(1.1110) | Xent 0.0368(0.0402) | Loss 1.1303(1.1311) | Error 0.0114(0.0125) Steps 416(417.84) | Grad Norm 0.2482(0.3026) | Total Time 10.00(10.00)\n",
      "Iter 2753 | Time 28.2677(28.9965) | Bit/dim 1.1061(1.1108) | Xent 0.0465(0.0404) | Loss 1.1293(1.1311) | Error 0.0145(0.0125) Steps 416(417.78) | Grad Norm 0.3396(0.3037) | Total Time 10.00(10.00)\n",
      "Iter 2754 | Time 29.9835(29.0261) | Bit/dim 1.1122(1.1109) | Xent 0.0384(0.0404) | Loss 1.1314(1.1311) | Error 0.0109(0.0125) Steps 416(417.73) | Grad Norm 0.2250(0.3014) | Total Time 10.00(10.00)\n",
      "Iter 2755 | Time 30.1271(29.0592) | Bit/dim 1.1105(1.1109) | Xent 0.0359(0.0402) | Loss 1.1285(1.1310) | Error 0.0120(0.0125) Steps 416(417.68) | Grad Norm 0.1859(0.2979) | Total Time 10.00(10.00)\n",
      "Iter 2756 | Time 30.0251(29.0882) | Bit/dim 1.1142(1.1110) | Xent 0.0440(0.0403) | Loss 1.1362(1.1311) | Error 0.0132(0.0125) Steps 416(417.63) | Grad Norm 0.2168(0.2955) | Total Time 10.00(10.00)\n",
      "Iter 2757 | Time 29.3599(29.0963) | Bit/dim 1.1118(1.1110) | Xent 0.0409(0.0404) | Loss 1.1322(1.1312) | Error 0.0139(0.0125) Steps 416(417.58) | Grad Norm 0.2680(0.2947) | Total Time 10.00(10.00)\n",
      "Iter 2758 | Time 28.5489(29.0799) | Bit/dim 1.1053(1.1108) | Xent 0.0423(0.0404) | Loss 1.1265(1.1310) | Error 0.0128(0.0125) Steps 422(417.71) | Grad Norm 0.3666(0.2968) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0394 | Time 16.6532, Epoch Time 234.0742(235.9799), Bit/dim 1.1053(best: 1.1047), Xent 0.0314, Loss 1.1210, Error 0.0103(best: 0.0085)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2759 | Time 29.6004(29.0955) | Bit/dim 1.1099(1.1108) | Xent 0.0429(0.0405) | Loss 1.1314(1.1310) | Error 0.0130(0.0125) Steps 416(417.66) | Grad Norm 0.3736(0.2991) | Total Time 10.00(10.00)\n",
      "Iter 2760 | Time 29.4019(29.1047) | Bit/dim 1.1085(1.1107) | Xent 0.0369(0.0404) | Loss 1.1269(1.1309) | Error 0.0115(0.0125) Steps 416(417.61) | Grad Norm 0.2383(0.2973) | Total Time 10.00(10.00)\n",
      "Iter 2761 | Time 30.2970(29.1405) | Bit/dim 1.1108(1.1107) | Xent 0.0412(0.0404) | Loss 1.1314(1.1309) | Error 0.0136(0.0125) Steps 416(417.56) | Grad Norm 0.1944(0.2942) | Total Time 10.00(10.00)\n",
      "Iter 2762 | Time 28.6282(29.1251) | Bit/dim 1.1142(1.1108) | Xent 0.0422(0.0405) | Loss 1.1353(1.1311) | Error 0.0138(0.0126) Steps 422(417.70) | Grad Norm 0.2662(0.2934) | Total Time 10.00(10.00)\n",
      "Iter 2763 | Time 28.4041(29.1035) | Bit/dim 1.1082(1.1108) | Xent 0.0448(0.0406) | Loss 1.1306(1.1311) | Error 0.0135(0.0126) Steps 416(417.64) | Grad Norm 0.5399(0.3008) | Total Time 10.00(10.00)\n",
      "Iter 2764 | Time 30.3897(29.1421) | Bit/dim 1.1139(1.1109) | Xent 0.0362(0.0405) | Loss 1.1320(1.1311) | Error 0.0121(0.0126) Steps 422(417.78) | Grad Norm 0.3164(0.3012) | Total Time 10.00(10.00)\n",
      "Iter 2765 | Time 29.7048(29.1589) | Bit/dim 1.1111(1.1109) | Xent 0.0400(0.0404) | Loss 1.1311(1.1311) | Error 0.0115(0.0126) Steps 416(417.72) | Grad Norm 0.2574(0.2999) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0395 | Time 16.8501, Epoch Time 235.7680(235.9735), Bit/dim 1.1048(best: 1.1047), Xent 0.0297, Loss 1.1197, Error 0.0100(best: 0.0085)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2766 | Time 29.0262(29.1550) | Bit/dim 1.1113(1.1109) | Xent 0.0449(0.0406) | Loss 1.1337(1.1312) | Error 0.0142(0.0126) Steps 416(417.67) | Grad Norm 0.2635(0.2988) | Total Time 10.00(10.00)\n",
      "Iter 2767 | Time 30.3771(29.1916) | Bit/dim 1.1113(1.1109) | Xent 0.0324(0.0403) | Loss 1.1275(1.1311) | Error 0.0098(0.0125) Steps 416(417.62) | Grad Norm 0.4057(0.3020) | Total Time 10.00(10.00)\n",
      "Iter 2768 | Time 30.4729(29.2301) | Bit/dim 1.1087(1.1108) | Xent 0.0383(0.0403) | Loss 1.1279(1.1310) | Error 0.0135(0.0126) Steps 416(417.57) | Grad Norm 0.4379(0.3061) | Total Time 10.00(10.00)\n",
      "Iter 2769 | Time 28.5555(29.2098) | Bit/dim 1.1115(1.1108) | Xent 0.0423(0.0403) | Loss 1.1327(1.1310) | Error 0.0129(0.0126) Steps 416(417.52) | Grad Norm 0.1752(0.3022) | Total Time 10.00(10.00)\n",
      "Iter 2770 | Time 28.8206(29.1981) | Bit/dim 1.1056(1.1107) | Xent 0.0440(0.0404) | Loss 1.1276(1.1309) | Error 0.0135(0.0126) Steps 422(417.66) | Grad Norm 0.5814(0.3106) | Total Time 10.00(10.00)\n",
      "Iter 2771 | Time 28.6308(29.1811) | Bit/dim 1.1132(1.1108) | Xent 0.0384(0.0404) | Loss 1.1324(1.1309) | Error 0.0121(0.0126) Steps 416(417.61) | Grad Norm 0.3082(0.3105) | Total Time 10.00(10.00)\n",
      "Iter 2772 | Time 28.7773(29.1690) | Bit/dim 1.1105(1.1107) | Xent 0.0351(0.0402) | Loss 1.1280(1.1309) | Error 0.0116(0.0126) Steps 422(417.74) | Grad Norm 0.5024(0.3162) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0396 | Time 16.7286, Epoch Time 233.9636(235.9132), Bit/dim 1.1054(best: 1.1047), Xent 0.0286, Loss 1.1198, Error 0.0094(best: 0.0085)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2773 | Time 30.2492(29.2014) | Bit/dim 1.1149(1.1109) | Xent 0.0391(0.0402) | Loss 1.1345(1.1310) | Error 0.0124(0.0125) Steps 416(417.69) | Grad Norm 0.1928(0.3125) | Total Time 10.00(10.00)\n",
      "Iter 2774 | Time 28.8297(29.1903) | Bit/dim 1.1142(1.1110) | Xent 0.0347(0.0400) | Loss 1.1316(1.1310) | Error 0.0109(0.0125) Steps 416(417.64) | Grad Norm 0.3580(0.3139) | Total Time 10.00(10.00)\n",
      "Iter 2775 | Time 30.2438(29.2219) | Bit/dim 1.1144(1.1111) | Xent 0.0480(0.0403) | Loss 1.1384(1.1312) | Error 0.0142(0.0125) Steps 422(417.77) | Grad Norm 0.6584(0.3242) | Total Time 10.00(10.00)\n",
      "Iter 2776 | Time 29.7109(29.2365) | Bit/dim 1.1063(1.1109) | Xent 0.0381(0.0402) | Loss 1.1253(1.1310) | Error 0.0120(0.0125) Steps 416(417.72) | Grad Norm 0.4046(0.3267) | Total Time 10.00(10.00)\n",
      "Iter 2777 | Time 29.5865(29.2470) | Bit/dim 1.1099(1.1109) | Xent 0.0409(0.0402) | Loss 1.1303(1.1310) | Error 0.0122(0.0125) Steps 416(417.66) | Grad Norm 0.3542(0.3275) | Total Time 10.00(10.00)\n",
      "Iter 2778 | Time 29.5948(29.2575) | Bit/dim 1.1084(1.1108) | Xent 0.0419(0.0403) | Loss 1.1294(1.1310) | Error 0.0131(0.0125) Steps 416(417.61) | Grad Norm 0.3576(0.3284) | Total Time 10.00(10.00)\n",
      "Iter 2779 | Time 29.5152(29.2652) | Bit/dim 1.1063(1.1107) | Xent 0.0423(0.0403) | Loss 1.1275(1.1309) | Error 0.0122(0.0125) Steps 416(417.57) | Grad Norm 0.4698(0.3326) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0397 | Time 16.8062, Epoch Time 237.0250(235.9466), Bit/dim 1.1044(best: 1.1047), Xent 0.0270, Loss 1.1179, Error 0.0098(best: 0.0085)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2780 | Time 28.5360(29.2433) | Bit/dim 1.1068(1.1106) | Xent 0.0387(0.0403) | Loss 1.1262(1.1307) | Error 0.0129(0.0125) Steps 416(417.52) | Grad Norm 0.2481(0.3301) | Total Time 10.00(10.00)\n",
      "Iter 2781 | Time 28.4126(29.2184) | Bit/dim 1.1084(1.1105) | Xent 0.0289(0.0399) | Loss 1.1229(1.1305) | Error 0.0095(0.0125) Steps 422(417.65) | Grad Norm 0.2872(0.3288) | Total Time 10.00(10.00)\n",
      "Iter 2782 | Time 29.3092(29.2211) | Bit/dim 1.1175(1.1107) | Xent 0.0429(0.0400) | Loss 1.1390(1.1307) | Error 0.0131(0.0125) Steps 416(417.60) | Grad Norm 0.4281(0.3318) | Total Time 10.00(10.00)\n",
      "Iter 2783 | Time 28.7730(29.2077) | Bit/dim 1.1086(1.1107) | Xent 0.0462(0.0402) | Loss 1.1317(1.1308) | Error 0.0128(0.0125) Steps 416(417.56) | Grad Norm 0.5982(0.3398) | Total Time 10.00(10.00)\n",
      "Iter 2784 | Time 30.2920(29.2402) | Bit/dim 1.1135(1.1107) | Xent 0.0439(0.0403) | Loss 1.1354(1.1309) | Error 0.0146(0.0125) Steps 416(417.51) | Grad Norm 0.4341(0.3426) | Total Time 10.00(10.00)\n",
      "Iter 2785 | Time 29.5669(29.2500) | Bit/dim 1.1094(1.1107) | Xent 0.0340(0.0401) | Loss 1.1264(1.1308) | Error 0.0111(0.0125) Steps 416(417.46) | Grad Norm 0.2382(0.3395) | Total Time 10.00(10.00)\n",
      "Iter 2786 | Time 29.5693(29.2596) | Bit/dim 1.1110(1.1107) | Xent 0.0368(0.0400) | Loss 1.1294(1.1307) | Error 0.0110(0.0125) Steps 422(417.60) | Grad Norm 0.2807(0.3377) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0398 | Time 16.8657, Epoch Time 233.5980(235.8761), Bit/dim 1.1049(best: 1.1044), Xent 0.0304, Loss 1.1201, Error 0.0102(best: 0.0085)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2787 | Time 29.7767(29.2751) | Bit/dim 1.1124(1.1108) | Xent 0.0365(0.0399) | Loss 1.1307(1.1307) | Error 0.0124(0.0125) Steps 416(417.55) | Grad Norm 0.5577(0.3443) | Total Time 10.00(10.00)\n",
      "Iter 2788 | Time 29.4622(29.2807) | Bit/dim 1.1159(1.1109) | Xent 0.0438(0.0400) | Loss 1.1377(1.1309) | Error 0.0128(0.0125) Steps 416(417.51) | Grad Norm 0.2892(0.3427) | Total Time 10.00(10.00)\n",
      "Iter 2789 | Time 30.1087(29.3056) | Bit/dim 1.1121(1.1110) | Xent 0.0365(0.0399) | Loss 1.1304(1.1309) | Error 0.0108(0.0124) Steps 416(417.46) | Grad Norm 0.2511(0.3399) | Total Time 10.00(10.00)\n",
      "Iter 2790 | Time 29.1290(29.3003) | Bit/dim 1.1098(1.1109) | Xent 0.0519(0.0403) | Loss 1.1357(1.1311) | Error 0.0165(0.0125) Steps 416(417.42) | Grad Norm 0.3426(0.3400) | Total Time 10.00(10.00)\n",
      "Iter 2791 | Time 29.4043(29.3034) | Bit/dim 1.1080(1.1108) | Xent 0.0470(0.0405) | Loss 1.1315(1.1311) | Error 0.0149(0.0126) Steps 416(417.37) | Grad Norm 0.3882(0.3414) | Total Time 10.00(10.00)\n",
      "Iter 2792 | Time 28.7757(29.2876) | Bit/dim 1.1105(1.1108) | Xent 0.0363(0.0404) | Loss 1.1286(1.1310) | Error 0.0111(0.0126) Steps 422(417.51) | Grad Norm 0.1771(0.3365) | Total Time 10.00(10.00)\n",
      "Iter 2793 | Time 29.0049(29.2791) | Bit/dim 1.1069(1.1107) | Xent 0.0371(0.0403) | Loss 1.1255(1.1308) | Error 0.0109(0.0125) Steps 416(417.47) | Grad Norm 0.2433(0.3337) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0399 | Time 16.4557, Epoch Time 234.8900(235.8465), Bit/dim 1.1052(best: 1.1044), Xent 0.0295, Loss 1.1199, Error 0.0106(best: 0.0085)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2794 | Time 28.7583(29.2634) | Bit/dim 1.1052(1.1105) | Xent 0.0429(0.0404) | Loss 1.1267(1.1307) | Error 0.0141(0.0126) Steps 416(417.42) | Grad Norm 0.2325(0.3307) | Total Time 10.00(10.00)\n",
      "Iter 2795 | Time 28.6263(29.2443) | Bit/dim 1.1136(1.1106) | Xent 0.0368(0.0402) | Loss 1.1319(1.1308) | Error 0.0120(0.0125) Steps 422(417.56) | Grad Norm 0.4585(0.3345) | Total Time 10.00(10.00)\n",
      "Iter 2796 | Time 30.4663(29.2810) | Bit/dim 1.1066(1.1105) | Xent 0.0377(0.0402) | Loss 1.1255(1.1306) | Error 0.0126(0.0125) Steps 416(417.51) | Grad Norm 0.2942(0.3333) | Total Time 10.00(10.00)\n",
      "Iter 2797 | Time 29.2129(29.2789) | Bit/dim 1.1100(1.1105) | Xent 0.0404(0.0402) | Loss 1.1302(1.1306) | Error 0.0131(0.0126) Steps 416(417.47) | Grad Norm 0.2202(0.3299) | Total Time 10.00(10.00)\n",
      "Iter 2798 | Time 29.0631(29.2725) | Bit/dim 1.1130(1.1106) | Xent 0.0349(0.0400) | Loss 1.1305(1.1306) | Error 0.0115(0.0125) Steps 416(417.42) | Grad Norm 0.2618(0.3279) | Total Time 10.00(10.00)\n",
      "Iter 2799 | Time 28.6283(29.2531) | Bit/dim 1.1127(1.1106) | Xent 0.0443(0.0401) | Loss 1.1348(1.1307) | Error 0.0131(0.0125) Steps 422(417.56) | Grad Norm 0.6108(0.3364) | Total Time 10.00(10.00)\n",
      "Iter 2800 | Time 28.9866(29.2452) | Bit/dim 1.1127(1.1107) | Xent 0.0436(0.0402) | Loss 1.1345(1.1308) | Error 0.0124(0.0125) Steps 416(417.51) | Grad Norm 0.3611(0.3371) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0400 | Time 16.5935, Epoch Time 233.1155(235.7646), Bit/dim 1.1045(best: 1.1044), Xent 0.0277, Loss 1.1183, Error 0.0092(best: 0.0085)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2801 | Time 28.7985(29.2318) | Bit/dim 1.1085(1.1106) | Xent 0.0398(0.0402) | Loss 1.1284(1.1307) | Error 0.0118(0.0125) Steps 416(417.47) | Grad Norm 0.2225(0.3337) | Total Time 10.00(10.00)\n",
      "Iter 2802 | Time 28.7857(29.2184) | Bit/dim 1.1043(1.1104) | Xent 0.0432(0.0403) | Loss 1.1259(1.1306) | Error 0.0130(0.0125) Steps 416(417.43) | Grad Norm 0.2205(0.3303) | Total Time 10.00(10.00)\n",
      "Iter 2803 | Time 28.5801(29.1992) | Bit/dim 1.1074(1.1103) | Xent 0.0400(0.0403) | Loss 1.1274(1.1305) | Error 0.0119(0.0125) Steps 416(417.38) | Grad Norm 0.4724(0.3345) | Total Time 10.00(10.00)\n",
      "Iter 2804 | Time 30.3238(29.2330) | Bit/dim 1.1118(1.1104) | Xent 0.0404(0.0403) | Loss 1.1320(1.1306) | Error 0.0120(0.0125) Steps 428(417.70) | Grad Norm 0.2659(0.3325) | Total Time 10.00(10.00)\n",
      "Iter 2805 | Time 29.7384(29.2481) | Bit/dim 1.1146(1.1105) | Xent 0.0446(0.0404) | Loss 1.1369(1.1307) | Error 0.0146(0.0126) Steps 422(417.83) | Grad Norm 0.2461(0.3299) | Total Time 10.00(10.00)\n",
      "Iter 2806 | Time 28.3362(29.2208) | Bit/dim 1.1129(1.1106) | Xent 0.0404(0.0404) | Loss 1.1331(1.1308) | Error 0.0116(0.0125) Steps 416(417.77) | Grad Norm 0.2574(0.3277) | Total Time 10.00(10.00)\n",
      "Iter 2807 | Time 28.8490(29.2096) | Bit/dim 1.1158(1.1107) | Xent 0.0385(0.0404) | Loss 1.1351(1.1309) | Error 0.0121(0.0125) Steps 416(417.72) | Grad Norm 0.2710(0.3260) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0401 | Time 16.5426, Epoch Time 232.6284(235.6705), Bit/dim 1.1037(best: 1.1044), Xent 0.0266, Loss 1.1170, Error 0.0085(best: 0.0085)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2808 | Time 30.3383(29.2435) | Bit/dim 1.1081(1.1107) | Xent 0.0387(0.0403) | Loss 1.1274(1.1308) | Error 0.0128(0.0125) Steps 416(417.67) | Grad Norm 0.2368(0.3233) | Total Time 10.00(10.00)\n",
      "Iter 2809 | Time 29.0074(29.2364) | Bit/dim 1.1145(1.1108) | Xent 0.0439(0.0404) | Loss 1.1365(1.1310) | Error 0.0125(0.0125) Steps 416(417.62) | Grad Norm 0.2749(0.3219) | Total Time 10.00(10.00)\n",
      "Iter 2810 | Time 29.9564(29.2580) | Bit/dim 1.1086(1.1107) | Xent 0.0358(0.0403) | Loss 1.1265(1.1309) | Error 0.0121(0.0125) Steps 416(417.57) | Grad Norm 0.4044(0.3244) | Total Time 10.00(10.00)\n",
      "Iter 2811 | Time 28.8975(29.2472) | Bit/dim 1.1077(1.1106) | Xent 0.0388(0.0403) | Loss 1.1271(1.1308) | Error 0.0114(0.0125) Steps 416(417.52) | Grad Norm 0.3088(0.3239) | Total Time 10.00(10.00)\n",
      "Iter 2812 | Time 28.3638(29.2207) | Bit/dim 1.1086(1.1106) | Xent 0.0373(0.0402) | Loss 1.1272(1.1306) | Error 0.0112(0.0124) Steps 416(417.48) | Grad Norm 0.1835(0.3197) | Total Time 10.00(10.00)\n",
      "Iter 2813 | Time 28.1292(29.1879) | Bit/dim 1.1109(1.1106) | Xent 0.0375(0.0401) | Loss 1.1297(1.1306) | Error 0.0121(0.0124) Steps 416(417.43) | Grad Norm 0.3564(0.3208) | Total Time 10.00(10.00)\n",
      "Iter 2814 | Time 28.8648(29.1782) | Bit/dim 1.1134(1.1107) | Xent 0.0416(0.0401) | Loss 1.1341(1.1307) | Error 0.0125(0.0124) Steps 416(417.39) | Grad Norm 0.2110(0.3175) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0402 | Time 16.6934, Epoch Time 232.5389(235.5766), Bit/dim 1.1047(best: 1.1037), Xent 0.0294, Loss 1.1195, Error 0.0098(best: 0.0085)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2815 | Time 28.6748(29.1631) | Bit/dim 1.1119(1.1107) | Xent 0.0444(0.0403) | Loss 1.1341(1.1308) | Error 0.0141(0.0125) Steps 416(417.35) | Grad Norm 0.3617(0.3188) | Total Time 10.00(10.00)\n",
      "Iter 2816 | Time 29.1821(29.1637) | Bit/dim 1.1081(1.1106) | Xent 0.0305(0.0400) | Loss 1.1234(1.1306) | Error 0.0084(0.0124) Steps 416(417.31) | Grad Norm 0.2678(0.3173) | Total Time 10.00(10.00)\n",
      "Iter 2817 | Time 29.2383(29.1659) | Bit/dim 1.1071(1.1105) | Xent 0.0377(0.0399) | Loss 1.1259(1.1305) | Error 0.0112(0.0123) Steps 416(417.27) | Grad Norm 0.6258(0.3265) | Total Time 10.00(10.00)\n",
      "Iter 2818 | Time 29.0671(29.1630) | Bit/dim 1.1082(1.1104) | Xent 0.0431(0.0400) | Loss 1.1298(1.1304) | Error 0.0126(0.0123) Steps 416(417.23) | Grad Norm 0.2222(0.3234) | Total Time 10.00(10.00)\n",
      "Iter 2819 | Time 29.0861(29.1607) | Bit/dim 1.1153(1.1106) | Xent 0.0371(0.0399) | Loss 1.1338(1.1305) | Error 0.0116(0.0123) Steps 416(417.19) | Grad Norm 0.2741(0.3219) | Total Time 10.00(10.00)\n",
      "Iter 2820 | Time 28.8251(29.1506) | Bit/dim 1.1102(1.1106) | Xent 0.0424(0.0400) | Loss 1.1314(1.1306) | Error 0.0134(0.0123) Steps 416(417.16) | Grad Norm 0.2144(0.3187) | Total Time 10.00(10.00)\n",
      "Iter 2821 | Time 29.0198(29.1467) | Bit/dim 1.1108(1.1106) | Xent 0.0435(0.0401) | Loss 1.1325(1.1306) | Error 0.0134(0.0124) Steps 416(417.12) | Grad Norm 0.2166(0.3156) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0403 | Time 16.3153, Epoch Time 232.0227(235.4700), Bit/dim 1.1047(best: 1.1037), Xent 0.0280, Loss 1.1187, Error 0.0096(best: 0.0085)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2822 | Time 30.4994(29.1873) | Bit/dim 1.1163(1.1108) | Xent 0.0350(0.0399) | Loss 1.1337(1.1307) | Error 0.0109(0.0123) Steps 416(417.09) | Grad Norm 0.4687(0.3202) | Total Time 10.00(10.00)\n",
      "Iter 2823 | Time 29.8266(29.2064) | Bit/dim 1.1105(1.1107) | Xent 0.0430(0.0400) | Loss 1.1320(1.1308) | Error 0.0126(0.0123) Steps 416(417.06) | Grad Norm 0.4360(0.3237) | Total Time 10.00(10.00)\n",
      "Iter 2824 | Time 28.6409(29.1895) | Bit/dim 1.1091(1.1107) | Xent 0.0355(0.0399) | Loss 1.1269(1.1306) | Error 0.0118(0.0123) Steps 422(417.21) | Grad Norm 0.2185(0.3205) | Total Time 10.00(10.00)\n",
      "Iter 2825 | Time 29.7923(29.2076) | Bit/dim 1.1090(1.1106) | Xent 0.0480(0.0401) | Loss 1.1330(1.1307) | Error 0.0134(0.0124) Steps 422(417.35) | Grad Norm 0.2597(0.3187) | Total Time 10.00(10.00)\n",
      "Iter 2826 | Time 30.0429(29.2326) | Bit/dim 1.1115(1.1107) | Xent 0.0458(0.0403) | Loss 1.1344(1.1308) | Error 0.0135(0.0124) Steps 416(417.31) | Grad Norm 0.4143(0.3216) | Total Time 10.00(10.00)\n",
      "Iter 2827 | Time 30.0778(29.2580) | Bit/dim 1.1072(1.1106) | Xent 0.0407(0.0403) | Loss 1.1276(1.1307) | Error 0.0128(0.0124) Steps 422(417.45) | Grad Norm 0.3118(0.3213) | Total Time 10.00(10.00)\n",
      "Iter 2828 | Time 28.8221(29.2449) | Bit/dim 1.1070(1.1105) | Xent 0.0325(0.0401) | Loss 1.1233(1.1305) | Error 0.0114(0.0124) Steps 416(417.41) | Grad Norm 0.3624(0.3225) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0404 | Time 16.6826, Epoch Time 236.7020(235.5069), Bit/dim 1.1046(best: 1.1037), Xent 0.0282, Loss 1.1187, Error 0.0091(best: 0.0085)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2829 | Time 29.5313(29.2535) | Bit/dim 1.1116(1.1105) | Xent 0.0405(0.0401) | Loss 1.1318(1.1305) | Error 0.0132(0.0124) Steps 416(417.36) | Grad Norm 0.3278(0.3227) | Total Time 10.00(10.00)\n",
      "Iter 2830 | Time 30.8450(29.3012) | Bit/dim 1.1161(1.1107) | Xent 0.0424(0.0402) | Loss 1.1373(1.1307) | Error 0.0119(0.0124) Steps 416(417.32) | Grad Norm 0.2835(0.3215) | Total Time 10.00(10.00)\n",
      "Iter 2831 | Time 28.6441(29.2815) | Bit/dim 1.1086(1.1106) | Xent 0.0441(0.0403) | Loss 1.1306(1.1307) | Error 0.0126(0.0124) Steps 422(417.46) | Grad Norm 0.3825(0.3233) | Total Time 10.00(10.00)\n",
      "Iter 2832 | Time 29.2056(29.2792) | Bit/dim 1.1088(1.1105) | Xent 0.0413(0.0403) | Loss 1.1295(1.1307) | Error 0.0140(0.0124) Steps 416(417.42) | Grad Norm 0.2461(0.3210) | Total Time 10.00(10.00)\n",
      "Iter 2833 | Time 28.4085(29.2531) | Bit/dim 1.1109(1.1106) | Xent 0.0410(0.0403) | Loss 1.1314(1.1307) | Error 0.0132(0.0125) Steps 416(417.38) | Grad Norm 0.2363(0.3185) | Total Time 10.00(10.00)\n",
      "Iter 2834 | Time 28.9709(29.2447) | Bit/dim 1.1112(1.1106) | Xent 0.0409(0.0404) | Loss 1.1316(1.1308) | Error 0.0125(0.0125) Steps 422(417.52) | Grad Norm 0.1784(0.3143) | Total Time 10.00(10.00)\n",
      "Iter 2835 | Time 28.8107(29.2316) | Bit/dim 1.1055(1.1104) | Xent 0.0393(0.0403) | Loss 1.1252(1.1306) | Error 0.0131(0.0125) Steps 422(417.65) | Grad Norm 0.2007(0.3109) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0405 | Time 16.7044, Epoch Time 233.6935(235.4525), Bit/dim 1.1044(best: 1.1037), Xent 0.0283, Loss 1.1186, Error 0.0093(best: 0.0085)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2836 | Time 28.7899(29.2184) | Bit/dim 1.1118(1.1105) | Xent 0.0406(0.0403) | Loss 1.1321(1.1306) | Error 0.0110(0.0124) Steps 416(417.60) | Grad Norm 0.3197(0.3111) | Total Time 10.00(10.00)\n",
      "Iter 2837 | Time 28.4640(29.1958) | Bit/dim 1.1131(1.1105) | Xent 0.0375(0.0402) | Loss 1.1318(1.1307) | Error 0.0104(0.0124) Steps 422(417.73) | Grad Norm 0.3522(0.3124) | Total Time 10.00(10.00)\n",
      "Iter 2838 | Time 29.2692(29.1980) | Bit/dim 1.1054(1.1104) | Xent 0.0395(0.0402) | Loss 1.1252(1.1305) | Error 0.0106(0.0123) Steps 422(417.86) | Grad Norm 0.1778(0.3083) | Total Time 10.00(10.00)\n",
      "Iter 2839 | Time 28.6879(29.1827) | Bit/dim 1.1120(1.1104) | Xent 0.0383(0.0402) | Loss 1.1311(1.1305) | Error 0.0131(0.0123) Steps 422(417.98) | Grad Norm 0.3794(0.3105) | Total Time 10.00(10.00)\n",
      "Iter 2840 | Time 28.4584(29.1609) | Bit/dim 1.1104(1.1104) | Xent 0.0413(0.0402) | Loss 1.1310(1.1305) | Error 0.0135(0.0124) Steps 416(417.93) | Grad Norm 0.2465(0.3085) | Total Time 10.00(10.00)\n",
      "Iter 2841 | Time 28.2506(29.1336) | Bit/dim 1.1086(1.1104) | Xent 0.0455(0.0404) | Loss 1.1313(1.1306) | Error 0.0126(0.0124) Steps 422(418.05) | Grad Norm 0.2866(0.3079) | Total Time 10.00(10.00)\n",
      "Iter 2842 | Time 28.7694(29.1227) | Bit/dim 1.1114(1.1104) | Xent 0.0411(0.0404) | Loss 1.1319(1.1306) | Error 0.0128(0.0124) Steps 416(417.99) | Grad Norm 0.3280(0.3085) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0406 | Time 16.6725, Epoch Time 230.0567(235.2906), Bit/dim 1.1042(best: 1.1037), Xent 0.0270, Loss 1.1177, Error 0.0096(best: 0.0085)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2843 | Time 28.6411(29.1082) | Bit/dim 1.1137(1.1105) | Xent 0.0410(0.0404) | Loss 1.1342(1.1307) | Error 0.0121(0.0124) Steps 416(417.93) | Grad Norm 0.2593(0.3070) | Total Time 10.00(10.00)\n",
      "Iter 2844 | Time 28.5933(29.0928) | Bit/dim 1.1141(1.1106) | Xent 0.0325(0.0402) | Loss 1.1303(1.1307) | Error 0.0109(0.0123) Steps 416(417.87) | Grad Norm 0.1944(0.3036) | Total Time 10.00(10.00)\n",
      "Iter 2845 | Time 28.8819(29.0865) | Bit/dim 1.1077(1.1105) | Xent 0.0454(0.0403) | Loss 1.1303(1.1307) | Error 0.0160(0.0125) Steps 422(417.99) | Grad Norm 0.2543(0.3022) | Total Time 10.00(10.00)\n",
      "Iter 2846 | Time 28.6955(29.0747) | Bit/dim 1.1097(1.1105) | Xent 0.0372(0.0402) | Loss 1.1283(1.1306) | Error 0.0120(0.0124) Steps 416(417.93) | Grad Norm 0.2351(0.3001) | Total Time 10.00(10.00)\n",
      "Iter 2847 | Time 28.4613(29.0563) | Bit/dim 1.1053(1.1103) | Xent 0.0437(0.0403) | Loss 1.1271(1.1305) | Error 0.0140(0.0125) Steps 416(417.87) | Grad Norm 0.2018(0.2972) | Total Time 10.00(10.00)\n",
      "Iter 2848 | Time 28.6906(29.0454) | Bit/dim 1.1109(1.1104) | Xent 0.0349(0.0402) | Loss 1.1283(1.1304) | Error 0.0119(0.0125) Steps 416(417.82) | Grad Norm 0.4501(0.3018) | Total Time 10.00(10.00)\n",
      "Iter 2849 | Time 29.1255(29.0478) | Bit/dim 1.1083(1.1103) | Xent 0.0386(0.0401) | Loss 1.1276(1.1304) | Error 0.0116(0.0124) Steps 416(417.76) | Grad Norm 0.2704(0.3008) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0407 | Time 16.4593, Epoch Time 230.0026(235.1320), Bit/dim 1.1046(best: 1.1037), Xent 0.0280, Loss 1.1186, Error 0.0106(best: 0.0085)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2850 | Time 28.8583(29.0421) | Bit/dim 1.1111(1.1103) | Xent 0.0525(0.0405) | Loss 1.1374(1.1306) | Error 0.0164(0.0126) Steps 416(417.71) | Grad Norm 0.2560(0.2995) | Total Time 10.00(10.00)\n",
      "Iter 2851 | Time 28.6956(29.0317) | Bit/dim 1.1086(1.1103) | Xent 0.0413(0.0405) | Loss 1.1292(1.1305) | Error 0.0132(0.0126) Steps 416(417.66) | Grad Norm 0.2774(0.2988) | Total Time 10.00(10.00)\n",
      "Iter 2852 | Time 28.6761(29.0210) | Bit/dim 1.1117(1.1103) | Xent 0.0401(0.0405) | Loss 1.1318(1.1306) | Error 0.0116(0.0126) Steps 416(417.61) | Grad Norm 0.2284(0.2967) | Total Time 10.00(10.00)\n",
      "Iter 2853 | Time 28.9713(29.0195) | Bit/dim 1.1084(1.1103) | Xent 0.0395(0.0405) | Loss 1.1282(1.1305) | Error 0.0134(0.0126) Steps 416(417.56) | Grad Norm 0.2879(0.2964) | Total Time 10.00(10.00)\n",
      "Iter 2854 | Time 28.8198(29.0135) | Bit/dim 1.1109(1.1103) | Xent 0.0432(0.0406) | Loss 1.1325(1.1306) | Error 0.0149(0.0126) Steps 416(417.51) | Grad Norm 0.3765(0.2989) | Total Time 10.00(10.00)\n",
      "Iter 2855 | Time 28.7993(29.0071) | Bit/dim 1.1096(1.1103) | Xent 0.0380(0.0405) | Loss 1.1286(1.1305) | Error 0.0120(0.0126) Steps 416(417.47) | Grad Norm 0.2327(0.2969) | Total Time 10.00(10.00)\n",
      "Iter 2856 | Time 28.3391(28.9871) | Bit/dim 1.1087(1.1102) | Xent 0.0297(0.0402) | Loss 1.1236(1.1303) | Error 0.0092(0.0125) Steps 416(417.43) | Grad Norm 0.2365(0.2951) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0408 | Time 16.5462, Epoch Time 229.9789(234.9774), Bit/dim 1.1044(best: 1.1037), Xent 0.0281, Loss 1.1185, Error 0.0103(best: 0.0085)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2857 | Time 28.8196(28.9820) | Bit/dim 1.1059(1.1101) | Xent 0.0407(0.0402) | Loss 1.1262(1.1302) | Error 0.0115(0.0125) Steps 416(417.38) | Grad Norm 0.2771(0.2945) | Total Time 10.00(10.00)\n",
      "Iter 2858 | Time 28.8038(28.9767) | Bit/dim 1.1106(1.1101) | Xent 0.0359(0.0400) | Loss 1.1285(1.1301) | Error 0.0114(0.0125) Steps 422(417.52) | Grad Norm 0.1801(0.2911) | Total Time 10.00(10.00)\n",
      "Iter 2859 | Time 28.3709(28.9585) | Bit/dim 1.1127(1.1102) | Xent 0.0406(0.0401) | Loss 1.1329(1.1302) | Error 0.0130(0.0125) Steps 416(417.48) | Grad Norm 0.2417(0.2896) | Total Time 10.00(10.00)\n",
      "Iter 2860 | Time 28.6922(28.9505) | Bit/dim 1.1129(1.1103) | Xent 0.0392(0.0400) | Loss 1.1326(1.1303) | Error 0.0126(0.0125) Steps 416(417.43) | Grad Norm 0.2254(0.2877) | Total Time 10.00(10.00)\n",
      "Iter 2861 | Time 28.7914(28.9458) | Bit/dim 1.1104(1.1103) | Xent 0.0405(0.0401) | Loss 1.1306(1.1303) | Error 0.0124(0.0125) Steps 416(417.39) | Grad Norm 0.2489(0.2865) | Total Time 10.00(10.00)\n",
      "Iter 2862 | Time 29.3885(28.9590) | Bit/dim 1.1088(1.1102) | Xent 0.0400(0.0400) | Loss 1.1288(1.1302) | Error 0.0124(0.0125) Steps 416(417.35) | Grad Norm 0.2296(0.2848) | Total Time 10.00(10.00)\n",
      "Iter 2863 | Time 28.8776(28.9566) | Bit/dim 1.1078(1.1101) | Xent 0.0415(0.0401) | Loss 1.1286(1.1302) | Error 0.0138(0.0125) Steps 416(417.31) | Grad Norm 0.2188(0.2828) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0409 | Time 16.3908, Epoch Time 230.6438(234.8474), Bit/dim 1.1043(best: 1.1037), Xent 0.0258, Loss 1.1172, Error 0.0091(best: 0.0085)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2864 | Time 29.1672(28.9629) | Bit/dim 1.1139(1.1103) | Xent 0.0406(0.0401) | Loss 1.1342(1.1303) | Error 0.0128(0.0125) Steps 416(417.27) | Grad Norm 0.3272(0.2842) | Total Time 10.00(10.00)\n",
      "Iter 2865 | Time 28.7722(28.9572) | Bit/dim 1.1051(1.1101) | Xent 0.0391(0.0401) | Loss 1.1247(1.1301) | Error 0.0125(0.0125) Steps 422(417.41) | Grad Norm 0.2100(0.2819) | Total Time 10.00(10.00)\n",
      "Iter 2866 | Time 29.9109(28.9858) | Bit/dim 1.1125(1.1102) | Xent 0.0392(0.0401) | Loss 1.1321(1.1302) | Error 0.0118(0.0125) Steps 416(417.37) | Grad Norm 0.2061(0.2797) | Total Time 10.00(10.00)\n",
      "Iter 2867 | Time 28.7275(28.9781) | Bit/dim 1.1134(1.1103) | Xent 0.0495(0.0403) | Loss 1.1381(1.1304) | Error 0.0152(0.0126) Steps 416(417.33) | Grad Norm 0.2581(0.2790) | Total Time 10.00(10.00)\n",
      "Iter 2868 | Time 28.6003(28.9667) | Bit/dim 1.1111(1.1103) | Xent 0.0365(0.0402) | Loss 1.1293(1.1304) | Error 0.0119(0.0126) Steps 416(417.29) | Grad Norm 0.1801(0.2760) | Total Time 10.00(10.00)\n",
      "Iter 2869 | Time 29.1951(28.9736) | Bit/dim 1.1074(1.1102) | Xent 0.0405(0.0402) | Loss 1.1276(1.1303) | Error 0.0130(0.0126) Steps 422(417.43) | Grad Norm 0.2173(0.2743) | Total Time 10.00(10.00)\n",
      "Iter 2870 | Time 29.0766(28.9767) | Bit/dim 1.1065(1.1101) | Xent 0.0401(0.0402) | Loss 1.1266(1.1302) | Error 0.0130(0.0126) Steps 416(417.38) | Grad Norm 0.3285(0.2759) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0410 | Time 16.5652, Epoch Time 232.5845(234.7795), Bit/dim 1.1037(best: 1.1037), Xent 0.0283, Loss 1.1179, Error 0.0101(best: 0.0085)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2871 | Time 28.9819(28.9768) | Bit/dim 1.1119(1.1101) | Xent 0.0394(0.0402) | Loss 1.1316(1.1302) | Error 0.0134(0.0126) Steps 416(417.34) | Grad Norm 0.2458(0.2750) | Total Time 10.00(10.00)\n",
      "Iter 2872 | Time 29.0164(28.9780) | Bit/dim 1.1108(1.1102) | Xent 0.0418(0.0402) | Loss 1.1318(1.1303) | Error 0.0136(0.0126) Steps 416(417.30) | Grad Norm 0.3446(0.2771) | Total Time 10.00(10.00)\n",
      "Iter 2873 | Time 28.8238(28.9734) | Bit/dim 1.1088(1.1101) | Xent 0.0396(0.0402) | Loss 1.1286(1.1302) | Error 0.0109(0.0126) Steps 416(417.26) | Grad Norm 0.2156(0.2752) | Total Time 10.00(10.00)\n",
      "Iter 2874 | Time 28.9951(28.9740) | Bit/dim 1.1072(1.1100) | Xent 0.0346(0.0401) | Loss 1.1245(1.1301) | Error 0.0120(0.0126) Steps 416(417.23) | Grad Norm 0.2459(0.2744) | Total Time 10.00(10.00)\n",
      "Iter 2875 | Time 28.6089(28.9631) | Bit/dim 1.1130(1.1101) | Xent 0.0422(0.0401) | Loss 1.1341(1.1302) | Error 0.0126(0.0126) Steps 416(417.19) | Grad Norm 0.3203(0.2757) | Total Time 10.00(10.00)\n",
      "Iter 2876 | Time 28.9829(28.9637) | Bit/dim 1.1102(1.1101) | Xent 0.0480(0.0404) | Loss 1.1342(1.1303) | Error 0.0148(0.0126) Steps 416(417.15) | Grad Norm 0.3392(0.2776) | Total Time 10.00(10.00)\n",
      "Iter 2877 | Time 29.4219(28.9774) | Bit/dim 1.1104(1.1101) | Xent 0.0361(0.0402) | Loss 1.1284(1.1303) | Error 0.0108(0.0126) Steps 422(417.30) | Grad Norm 0.4306(0.2822) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0411 | Time 16.2886, Epoch Time 231.4855(234.6807), Bit/dim 1.1043(best: 1.1037), Xent 0.0286, Loss 1.1186, Error 0.0106(best: 0.0085)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2878 | Time 29.0744(28.9803) | Bit/dim 1.1088(1.1101) | Xent 0.0462(0.0404) | Loss 1.1319(1.1303) | Error 0.0140(0.0126) Steps 416(417.26) | Grad Norm 0.2380(0.2809) | Total Time 10.00(10.00)\n",
      "Iter 2879 | Time 28.4542(28.9645) | Bit/dim 1.1111(1.1101) | Xent 0.0424(0.0405) | Loss 1.1323(1.1304) | Error 0.0142(0.0127) Steps 422(417.40) | Grad Norm 0.2814(0.2809) | Total Time 10.00(10.00)\n",
      "Iter 2880 | Time 28.5669(28.9526) | Bit/dim 1.1123(1.1102) | Xent 0.0350(0.0403) | Loss 1.1298(1.1303) | Error 0.0116(0.0126) Steps 422(417.54) | Grad Norm 0.2864(0.2811) | Total Time 10.00(10.00)\n",
      "Iter 2881 | Time 28.7143(28.9455) | Bit/dim 1.1083(1.1101) | Xent 0.0373(0.0402) | Loss 1.1269(1.1302) | Error 0.0122(0.0126) Steps 416(417.49) | Grad Norm 0.4943(0.2875) | Total Time 10.00(10.00)\n",
      "Iter 2882 | Time 28.5288(28.9330) | Bit/dim 1.1100(1.1101) | Xent 0.0350(0.0401) | Loss 1.1275(1.1302) | Error 0.0102(0.0126) Steps 416(417.45) | Grad Norm 0.1882(0.2845) | Total Time 10.00(10.00)\n",
      "Iter 2883 | Time 29.1510(28.9395) | Bit/dim 1.1100(1.1101) | Xent 0.0359(0.0399) | Loss 1.1279(1.1301) | Error 0.0119(0.0125) Steps 416(417.41) | Grad Norm 0.2114(0.2823) | Total Time 10.00(10.00)\n",
      "Iter 2884 | Time 29.6847(28.9619) | Bit/dim 1.1121(1.1102) | Xent 0.0411(0.0400) | Loss 1.1326(1.1302) | Error 0.0129(0.0125) Steps 416(417.36) | Grad Norm 0.3946(0.2857) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0412 | Time 16.6043, Epoch Time 231.2356(234.5773), Bit/dim 1.1041(best: 1.1037), Xent 0.0285, Loss 1.1184, Error 0.0097(best: 0.0085)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2885 | Time 28.7030(28.9541) | Bit/dim 1.1065(1.1101) | Xent 0.0385(0.0399) | Loss 1.1257(1.1300) | Error 0.0109(0.0125) Steps 416(417.32) | Grad Norm 0.4678(0.2911) | Total Time 10.00(10.00)\n",
      "Iter 2886 | Time 28.5330(28.9415) | Bit/dim 1.1099(1.1101) | Xent 0.0401(0.0399) | Loss 1.1299(1.1300) | Error 0.0119(0.0125) Steps 416(417.28) | Grad Norm 0.2794(0.2908) | Total Time 10.00(10.00)\n",
      "Iter 2887 | Time 28.8890(28.9399) | Bit/dim 1.1136(1.1102) | Xent 0.0402(0.0399) | Loss 1.1337(1.1301) | Error 0.0136(0.0125) Steps 422(417.42) | Grad Norm 0.2224(0.2887) | Total Time 10.00(10.00)\n",
      "Iter 2888 | Time 28.9429(28.9400) | Bit/dim 1.1119(1.1102) | Xent 0.0431(0.0400) | Loss 1.1334(1.1302) | Error 0.0131(0.0125) Steps 416(417.38) | Grad Norm 0.4973(0.2950) | Total Time 10.00(10.00)\n",
      "Iter 2889 | Time 28.5634(28.9287) | Bit/dim 1.1074(1.1101) | Xent 0.0447(0.0402) | Loss 1.1298(1.1302) | Error 0.0129(0.0125) Steps 416(417.34) | Grad Norm 0.5155(0.3016) | Total Time 10.00(10.00)\n",
      "Iter 2890 | Time 28.8734(28.9270) | Bit/dim 1.1093(1.1101) | Xent 0.0403(0.0402) | Loss 1.1294(1.1302) | Error 0.0125(0.0125) Steps 416(417.30) | Grad Norm 0.3040(0.3017) | Total Time 10.00(10.00)\n",
      "Iter 2891 | Time 29.9344(28.9572) | Bit/dim 1.1095(1.1101) | Xent 0.0420(0.0402) | Loss 1.1305(1.1302) | Error 0.0131(0.0126) Steps 422(417.44) | Grad Norm 0.2223(0.2993) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0413 | Time 16.5048, Epoch Time 231.4183(234.4826), Bit/dim 1.1038(best: 1.1037), Xent 0.0255, Loss 1.1166, Error 0.0087(best: 0.0085)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2892 | Time 28.9387(28.9567) | Bit/dim 1.1107(1.1101) | Xent 0.0376(0.0402) | Loss 1.1295(1.1302) | Error 0.0121(0.0125) Steps 416(417.40) | Grad Norm 0.3900(0.3020) | Total Time 10.00(10.00)\n",
      "Iter 2893 | Time 28.5356(28.9441) | Bit/dim 1.1101(1.1101) | Xent 0.0429(0.0402) | Loss 1.1315(1.1302) | Error 0.0134(0.0126) Steps 422(417.54) | Grad Norm 0.6090(0.3112) | Total Time 10.00(10.00)\n",
      "Iter 2894 | Time 28.6559(28.9354) | Bit/dim 1.1128(1.1102) | Xent 0.0351(0.0401) | Loss 1.1304(1.1302) | Error 0.0116(0.0125) Steps 416(417.49) | Grad Norm 0.4454(0.3153) | Total Time 10.00(10.00)\n",
      "Iter 2895 | Time 29.4516(28.9509) | Bit/dim 1.1065(1.1101) | Xent 0.0362(0.0400) | Loss 1.1246(1.1301) | Error 0.0108(0.0125) Steps 416(417.44) | Grad Norm 0.2736(0.3140) | Total Time 10.00(10.00)\n",
      "Iter 2896 | Time 30.0884(28.9850) | Bit/dim 1.1080(1.1100) | Xent 0.0503(0.0403) | Loss 1.1332(1.1302) | Error 0.0152(0.0126) Steps 416(417.40) | Grad Norm 0.5460(0.3210) | Total Time 10.00(10.00)\n",
      "Iter 2897 | Time 29.8991(29.0124) | Bit/dim 1.1094(1.1100) | Xent 0.0327(0.0400) | Loss 1.1257(1.1300) | Error 0.0104(0.0125) Steps 416(417.36) | Grad Norm 0.2918(0.3201) | Total Time 10.00(10.00)\n",
      "Iter 2898 | Time 28.8665(29.0081) | Bit/dim 1.1091(1.1100) | Xent 0.0424(0.0401) | Loss 1.1303(1.1300) | Error 0.0128(0.0125) Steps 422(417.50) | Grad Norm 0.2274(0.3173) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0414 | Time 16.7515, Epoch Time 233.7896(234.4618), Bit/dim 1.1034(best: 1.1037), Xent 0.0286, Loss 1.1177, Error 0.0094(best: 0.0085)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2899 | Time 28.3751(28.9891) | Bit/dim 1.1093(1.1100) | Xent 0.0380(0.0401) | Loss 1.1283(1.1300) | Error 0.0119(0.0125) Steps 422(417.63) | Grad Norm 0.3607(0.3186) | Total Time 10.00(10.00)\n",
      "Iter 2900 | Time 29.4194(29.0020) | Bit/dim 1.1095(1.1099) | Xent 0.0350(0.0399) | Loss 1.1269(1.1299) | Error 0.0102(0.0124) Steps 422(417.76) | Grad Norm 0.2466(0.3165) | Total Time 10.00(10.00)\n",
      "Iter 2901 | Time 28.8473(28.9973) | Bit/dim 1.1133(1.1100) | Xent 0.0403(0.0399) | Loss 1.1334(1.1300) | Error 0.0116(0.0124) Steps 416(417.71) | Grad Norm 0.5513(0.3235) | Total Time 10.00(10.00)\n",
      "Iter 2902 | Time 29.0714(28.9996) | Bit/dim 1.1127(1.1101) | Xent 0.0340(0.0397) | Loss 1.1297(1.1300) | Error 0.0110(0.0124) Steps 422(417.84) | Grad Norm 0.2915(0.3225) | Total Time 10.00(10.00)\n",
      "Iter 2903 | Time 28.6691(28.9897) | Bit/dim 1.1087(1.1101) | Xent 0.0391(0.0397) | Loss 1.1282(1.1299) | Error 0.0135(0.0124) Steps 416(417.79) | Grad Norm 0.2325(0.3198) | Total Time 10.00(10.00)\n",
      "Iter 2904 | Time 29.5100(29.0053) | Bit/dim 1.1098(1.1101) | Xent 0.0332(0.0395) | Loss 1.1264(1.1298) | Error 0.0111(0.0124) Steps 422(417.91) | Grad Norm 0.1803(0.3157) | Total Time 10.00(10.00)\n",
      "Iter 2905 | Time 28.8999(29.0021) | Bit/dim 1.1063(1.1100) | Xent 0.0370(0.0394) | Loss 1.1248(1.1297) | Error 0.0116(0.0123) Steps 416(417.85) | Grad Norm 0.4288(0.3190) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0415 | Time 16.4153, Epoch Time 231.6679(234.3780), Bit/dim 1.1040(best: 1.1034), Xent 0.0277, Loss 1.1178, Error 0.0088(best: 0.0085)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2906 | Time 29.3753(29.0133) | Bit/dim 1.1092(1.1099) | Xent 0.0363(0.0394) | Loss 1.1274(1.1296) | Error 0.0111(0.0123) Steps 416(417.80) | Grad Norm 0.4310(0.3224) | Total Time 10.00(10.00)\n",
      "Iter 2907 | Time 28.4581(28.9966) | Bit/dim 1.1114(1.1100) | Xent 0.0414(0.0394) | Loss 1.1321(1.1297) | Error 0.0140(0.0123) Steps 416(417.74) | Grad Norm 0.3066(0.3219) | Total Time 10.00(10.00)\n",
      "Iter 2908 | Time 29.0727(28.9989) | Bit/dim 1.1119(1.1100) | Xent 0.0357(0.0393) | Loss 1.1298(1.1297) | Error 0.0118(0.0123) Steps 416(417.69) | Grad Norm 0.4013(0.3243) | Total Time 10.00(10.00)\n",
      "Iter 2909 | Time 30.4403(29.0422) | Bit/dim 1.1075(1.1100) | Xent 0.0398(0.0393) | Loss 1.1274(1.1296) | Error 0.0119(0.0123) Steps 416(417.64) | Grad Norm 0.5027(0.3297) | Total Time 10.00(10.00)\n",
      "Iter 2910 | Time 28.5464(29.0273) | Bit/dim 1.1128(1.1100) | Xent 0.0432(0.0394) | Loss 1.1344(1.1298) | Error 0.0132(0.0123) Steps 416(417.59) | Grad Norm 0.3986(0.3317) | Total Time 10.00(10.00)\n",
      "Iter 2911 | Time 29.3381(29.0366) | Bit/dim 1.1080(1.1100) | Xent 0.0413(0.0395) | Loss 1.1287(1.1297) | Error 0.0121(0.0123) Steps 416(417.54) | Grad Norm 0.1968(0.3277) | Total Time 10.00(10.00)\n",
      "Iter 2912 | Time 28.9081(29.0328) | Bit/dim 1.1112(1.1100) | Xent 0.0424(0.0396) | Loss 1.1324(1.1298) | Error 0.0138(0.0124) Steps 416(417.50) | Grad Norm 0.3868(0.3295) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0416 | Time 16.2361, Epoch Time 232.7936(234.3304), Bit/dim 1.1036(best: 1.1034), Xent 0.0286, Loss 1.1179, Error 0.0095(best: 0.0085)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2913 | Time 29.5320(29.0477) | Bit/dim 1.1120(1.1101) | Xent 0.0430(0.0397) | Loss 1.1335(1.1299) | Error 0.0145(0.0124) Steps 416(417.45) | Grad Norm 0.5219(0.3352) | Total Time 10.00(10.00)\n",
      "Iter 2914 | Time 28.6282(29.0352) | Bit/dim 1.1179(1.1103) | Xent 0.0378(0.0396) | Loss 1.1369(1.1301) | Error 0.0119(0.0124) Steps 416(417.41) | Grad Norm 0.1579(0.3299) | Total Time 10.00(10.00)\n",
      "Iter 2915 | Time 29.3114(29.0434) | Bit/dim 1.1069(1.1102) | Xent 0.0418(0.0397) | Loss 1.1279(1.1301) | Error 0.0136(0.0125) Steps 416(417.37) | Grad Norm 0.2438(0.3273) | Total Time 10.00(10.00)\n",
      "Iter 2916 | Time 28.4634(29.0260) | Bit/dim 1.1043(1.1100) | Xent 0.0380(0.0396) | Loss 1.1233(1.1299) | Error 0.0115(0.0124) Steps 416(417.33) | Grad Norm 0.2412(0.3247) | Total Time 10.00(10.00)\n",
      "Iter 2917 | Time 28.8752(29.0215) | Bit/dim 1.1068(1.1099) | Xent 0.0382(0.0396) | Loss 1.1259(1.1297) | Error 0.0112(0.0124) Steps 416(417.29) | Grad Norm 0.4949(0.3298) | Total Time 10.00(10.00)\n",
      "Iter 2918 | Time 28.6406(29.0101) | Bit/dim 1.1077(1.1099) | Xent 0.0385(0.0396) | Loss 1.1270(1.1297) | Error 0.0126(0.0124) Steps 416(417.25) | Grad Norm 0.3501(0.3305) | Total Time 10.00(10.00)\n",
      "Iter 2919 | Time 28.1549(28.9844) | Bit/dim 1.1080(1.1098) | Xent 0.0377(0.0395) | Loss 1.1269(1.1296) | Error 0.0125(0.0124) Steps 416(417.21) | Grad Norm 0.2599(0.3283) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0417 | Time 16.6953, Epoch Time 230.6842(234.2210), Bit/dim 1.1033(best: 1.1034), Xent 0.0284, Loss 1.1175, Error 0.0102(best: 0.0085)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2920 | Time 28.3537(28.9655) | Bit/dim 1.1075(1.1097) | Xent 0.0359(0.0394) | Loss 1.1254(1.1295) | Error 0.0111(0.0124) Steps 416(417.17) | Grad Norm 0.2307(0.3254) | Total Time 10.00(10.00)\n",
      "Iter 2921 | Time 30.0059(28.9967) | Bit/dim 1.1114(1.1098) | Xent 0.0414(0.0395) | Loss 1.1321(1.1295) | Error 0.0120(0.0124) Steps 416(417.14) | Grad Norm 0.2828(0.3241) | Total Time 10.00(10.00)\n",
      "Iter 2922 | Time 28.2900(28.9755) | Bit/dim 1.1072(1.1097) | Xent 0.0397(0.0395) | Loss 1.1270(1.1295) | Error 0.0120(0.0123) Steps 416(417.10) | Grad Norm 0.4635(0.3283) | Total Time 10.00(10.00)\n",
      "Iter 2923 | Time 28.7975(28.9702) | Bit/dim 1.1095(1.1097) | Xent 0.0430(0.0396) | Loss 1.1310(1.1295) | Error 0.0141(0.0124) Steps 416(417.07) | Grad Norm 0.4515(0.3320) | Total Time 10.00(10.00)\n",
      "Iter 2924 | Time 29.8244(28.9958) | Bit/dim 1.1107(1.1097) | Xent 0.0394(0.0396) | Loss 1.1304(1.1295) | Error 0.0126(0.0124) Steps 416(417.04) | Grad Norm 0.3111(0.3314) | Total Time 10.00(10.00)\n",
      "Iter 2925 | Time 29.1941(29.0018) | Bit/dim 1.1059(1.1096) | Xent 0.0341(0.0394) | Loss 1.1229(1.1293) | Error 0.0111(0.0124) Steps 416(417.01) | Grad Norm 0.2965(0.3303) | Total Time 10.00(10.00)\n",
      "Iter 2926 | Time 28.4182(28.9843) | Bit/dim 1.1125(1.1097) | Xent 0.0388(0.0394) | Loss 1.1320(1.1294) | Error 0.0128(0.0124) Steps 416(416.98) | Grad Norm 0.3505(0.3309) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0418 | Time 16.6307, Epoch Time 231.8432(234.1497), Bit/dim 1.1039(best: 1.1033), Xent 0.0277, Loss 1.1178, Error 0.0102(best: 0.0085)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2927 | Time 28.3519(28.9653) | Bit/dim 1.1052(1.1096) | Xent 0.0433(0.0395) | Loss 1.1269(1.1293) | Error 0.0140(0.0124) Steps 416(416.95) | Grad Norm 0.2345(0.3280) | Total Time 10.00(10.00)\n",
      "Iter 2928 | Time 28.4843(28.9509) | Bit/dim 1.1119(1.1096) | Xent 0.0385(0.0395) | Loss 1.1311(1.1294) | Error 0.0128(0.0124) Steps 416(416.92) | Grad Norm 0.1946(0.3240) | Total Time 10.00(10.00)\n",
      "Iter 2929 | Time 29.6432(28.9716) | Bit/dim 1.1077(1.1096) | Xent 0.0444(0.0396) | Loss 1.1299(1.1294) | Error 0.0140(0.0125) Steps 422(417.07) | Grad Norm 0.4032(0.3264) | Total Time 10.00(10.00)\n",
      "Iter 2930 | Time 28.8720(28.9686) | Bit/dim 1.1087(1.1096) | Xent 0.0388(0.0396) | Loss 1.1282(1.1294) | Error 0.0129(0.0125) Steps 416(417.04) | Grad Norm 0.2158(0.3231) | Total Time 10.00(10.00)\n",
      "Iter 2931 | Time 28.8162(28.9641) | Bit/dim 1.1136(1.1097) | Xent 0.0376(0.0395) | Loss 1.1324(1.1295) | Error 0.0118(0.0125) Steps 416(417.01) | Grad Norm 0.2199(0.3200) | Total Time 10.00(10.00)\n",
      "Iter 2932 | Time 29.7485(28.9876) | Bit/dim 1.1068(1.1096) | Xent 0.0375(0.0395) | Loss 1.1255(1.1293) | Error 0.0111(0.0124) Steps 416(416.98) | Grad Norm 0.3300(0.3203) | Total Time 10.00(10.00)\n",
      "Iter 2933 | Time 29.0403(28.9892) | Bit/dim 1.1091(1.1096) | Xent 0.0431(0.0396) | Loss 1.1306(1.1294) | Error 0.0139(0.0125) Steps 416(416.95) | Grad Norm 0.2069(0.3169) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0419 | Time 16.8491, Epoch Time 232.2294(234.0921), Bit/dim 1.1036(best: 1.1033), Xent 0.0287, Loss 1.1179, Error 0.0095(best: 0.0085)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2934 | Time 29.4823(29.0040) | Bit/dim 1.1117(1.1096) | Xent 0.0388(0.0396) | Loss 1.1312(1.1294) | Error 0.0131(0.0125) Steps 416(416.92) | Grad Norm 0.1816(0.3128) | Total Time 10.00(10.00)\n",
      "Iter 2935 | Time 28.6625(28.9937) | Bit/dim 1.1155(1.1098) | Xent 0.0341(0.0394) | Loss 1.1326(1.1295) | Error 0.0115(0.0125) Steps 416(416.89) | Grad Norm 0.1754(0.3087) | Total Time 10.00(10.00)\n",
      "Iter 2936 | Time 29.3204(29.0035) | Bit/dim 1.1037(1.1096) | Xent 0.0340(0.0392) | Loss 1.1207(1.1293) | Error 0.0102(0.0124) Steps 416(416.87) | Grad Norm 0.1701(0.3046) | Total Time 10.00(10.00)\n",
      "Iter 2937 | Time 30.2518(29.0410) | Bit/dim 1.1080(1.1096) | Xent 0.0343(0.0391) | Loss 1.1252(1.1291) | Error 0.0119(0.0124) Steps 416(416.84) | Grad Norm 0.2052(0.3016) | Total Time 10.00(10.00)\n",
      "Iter 2938 | Time 28.6197(29.0283) | Bit/dim 1.1107(1.1096) | Xent 0.0414(0.0392) | Loss 1.1314(1.1292) | Error 0.0135(0.0124) Steps 416(416.82) | Grad Norm 0.3148(0.3020) | Total Time 10.00(10.00)\n",
      "Iter 2939 | Time 28.2609(29.0053) | Bit/dim 1.1108(1.1097) | Xent 0.0503(0.0395) | Loss 1.1359(1.1294) | Error 0.0164(0.0125) Steps 416(416.79) | Grad Norm 0.3739(0.3041) | Total Time 10.00(10.00)\n",
      "Iter 2940 | Time 28.9307(29.0031) | Bit/dim 1.1088(1.1096) | Xent 0.0459(0.0397) | Loss 1.1318(1.1295) | Error 0.0148(0.0126) Steps 416(416.77) | Grad Norm 0.2957(0.3039) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0420 | Time 16.4889, Epoch Time 232.4842(234.0439), Bit/dim 1.1040(best: 1.1033), Xent 0.0284, Loss 1.1182, Error 0.0102(best: 0.0085)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2941 | Time 28.8348(28.9980) | Bit/dim 1.1098(1.1096) | Xent 0.0386(0.0397) | Loss 1.1291(1.1295) | Error 0.0112(0.0126) Steps 416(416.74) | Grad Norm 0.2311(0.3017) | Total Time 10.00(10.00)\n",
      "Iter 2942 | Time 28.7846(28.9916) | Bit/dim 1.1129(1.1097) | Xent 0.0430(0.0398) | Loss 1.1344(1.1296) | Error 0.0129(0.0126) Steps 416(416.72) | Grad Norm 0.3813(0.3041) | Total Time 10.00(10.00)\n",
      "Iter 2943 | Time 29.3174(29.0014) | Bit/dim 1.1048(1.1096) | Xent 0.0437(0.0399) | Loss 1.1266(1.1295) | Error 0.0134(0.0126) Steps 416(416.70) | Grad Norm 0.2690(0.3030) | Total Time 10.00(10.00)\n",
      "Iter 2944 | Time 29.1860(29.0069) | Bit/dim 1.1104(1.1096) | Xent 0.0378(0.0398) | Loss 1.1293(1.1295) | Error 0.0114(0.0126) Steps 416(416.68) | Grad Norm 0.3570(0.3047) | Total Time 10.00(10.00)\n",
      "Iter 2945 | Time 28.8515(29.0023) | Bit/dim 1.1098(1.1096) | Xent 0.0373(0.0397) | Loss 1.1284(1.1295) | Error 0.0124(0.0126) Steps 422(416.84) | Grad Norm 0.2603(0.3033) | Total Time 10.00(10.00)\n",
      "Iter 2946 | Time 28.8931(28.9990) | Bit/dim 1.1094(1.1096) | Xent 0.0401(0.0397) | Loss 1.1294(1.1295) | Error 0.0116(0.0125) Steps 416(416.81) | Grad Norm 0.3567(0.3049) | Total Time 10.00(10.00)\n",
      "Iter 2947 | Time 29.8063(29.0232) | Bit/dim 1.1122(1.1097) | Xent 0.0336(0.0396) | Loss 1.1290(1.1295) | Error 0.0095(0.0124) Steps 416(416.79) | Grad Norm 0.4796(0.3102) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0421 | Time 16.4447, Epoch Time 232.5078(233.9978), Bit/dim 1.1038(best: 1.1033), Xent 0.0278, Loss 1.1177, Error 0.0102(best: 0.0085)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2948 | Time 29.0668(29.0245) | Bit/dim 1.1081(1.1096) | Xent 0.0446(0.0397) | Loss 1.1304(1.1295) | Error 0.0126(0.0124) Steps 422(416.95) | Grad Norm 0.1862(0.3064) | Total Time 10.00(10.00)\n",
      "Iter 2949 | Time 29.8169(29.0483) | Bit/dim 1.1063(1.1095) | Xent 0.0394(0.0397) | Loss 1.1260(1.1294) | Error 0.0122(0.0124) Steps 416(416.92) | Grad Norm 0.2628(0.3051) | Total Time 10.00(10.00)\n",
      "Iter 2950 | Time 30.0920(29.0796) | Bit/dim 1.1131(1.1096) | Xent 0.0375(0.0396) | Loss 1.1319(1.1295) | Error 0.0120(0.0124) Steps 416(416.89) | Grad Norm 0.2437(0.3033) | Total Time 10.00(10.00)\n",
      "Iter 2951 | Time 29.2788(29.0856) | Bit/dim 1.1124(1.1097) | Xent 0.0361(0.0395) | Loss 1.1305(1.1295) | Error 0.0105(0.0124) Steps 416(416.86) | Grad Norm 0.1980(0.3001) | Total Time 10.00(10.00)\n",
      "Iter 2952 | Time 28.6510(29.0725) | Bit/dim 1.1068(1.1096) | Xent 0.0419(0.0396) | Loss 1.1278(1.1294) | Error 0.0130(0.0124) Steps 416(416.84) | Grad Norm 0.1968(0.2970) | Total Time 10.00(10.00)\n",
      "Iter 2953 | Time 28.6593(29.0602) | Bit/dim 1.1127(1.1097) | Xent 0.0445(0.0397) | Loss 1.1349(1.1296) | Error 0.0150(0.0125) Steps 416(416.81) | Grad Norm 0.4397(0.3013) | Total Time 10.00(10.00)\n",
      "Iter 2954 | Time 29.5093(29.0736) | Bit/dim 1.1088(1.1097) | Xent 0.0336(0.0396) | Loss 1.1256(1.1295) | Error 0.0104(0.0124) Steps 422(416.97) | Grad Norm 0.3926(0.3041) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0422 | Time 16.4635, Epoch Time 233.9622(233.9967), Bit/dim 1.1035(best: 1.1033), Xent 0.0300, Loss 1.1185, Error 0.0104(best: 0.0085)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2955 | Time 28.9973(29.0713) | Bit/dim 1.1080(1.1097) | Xent 0.0445(0.0397) | Loss 1.1302(1.1295) | Error 0.0128(0.0124) Steps 416(416.94) | Grad Norm 0.3384(0.3051) | Total Time 10.00(10.00)\n",
      "Iter 2956 | Time 30.1737(29.1044) | Bit/dim 1.1118(1.1097) | Xent 0.0393(0.0397) | Loss 1.1314(1.1296) | Error 0.0138(0.0125) Steps 416(416.91) | Grad Norm 0.2348(0.3030) | Total Time 10.00(10.00)\n",
      "Iter 2957 | Time 29.9404(29.1295) | Bit/dim 1.1114(1.1098) | Xent 0.0378(0.0396) | Loss 1.1303(1.1296) | Error 0.0122(0.0124) Steps 422(417.06) | Grad Norm 0.1915(0.2996) | Total Time 10.00(10.00)\n",
      "Iter 2958 | Time 28.9118(29.1230) | Bit/dim 1.1088(1.1097) | Xent 0.0382(0.0396) | Loss 1.1279(1.1295) | Error 0.0120(0.0124) Steps 416(417.03) | Grad Norm 0.4107(0.3030) | Total Time 10.00(10.00)\n",
      "Iter 2959 | Time 28.6077(29.1075) | Bit/dim 1.1059(1.1096) | Xent 0.0390(0.0396) | Loss 1.1254(1.1294) | Error 0.0112(0.0124) Steps 416(417.00) | Grad Norm 0.4807(0.3083) | Total Time 10.00(10.00)\n",
      "Iter 2960 | Time 29.1854(29.1098) | Bit/dim 1.1104(1.1096) | Xent 0.0457(0.0398) | Loss 1.1332(1.1295) | Error 0.0159(0.0125) Steps 416(416.97) | Grad Norm 0.2135(0.3055) | Total Time 10.00(10.00)\n",
      "Iter 2961 | Time 28.6334(29.0955) | Bit/dim 1.1100(1.1097) | Xent 0.0384(0.0397) | Loss 1.1292(1.1295) | Error 0.0132(0.0125) Steps 416(416.94) | Grad Norm 0.1968(0.3022) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0423 | Time 16.6629, Epoch Time 233.5426(233.9831), Bit/dim 1.1040(best: 1.1033), Xent 0.0292, Loss 1.1186, Error 0.0094(best: 0.0085)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2962 | Time 29.1205(29.0963) | Bit/dim 1.1128(1.1098) | Xent 0.0380(0.0397) | Loss 1.1319(1.1296) | Error 0.0116(0.0125) Steps 416(416.91) | Grad Norm 0.6226(0.3118) | Total Time 10.00(10.00)\n",
      "Iter 2963 | Time 28.0040(29.0635) | Bit/dim 1.1100(1.1098) | Xent 0.0396(0.0397) | Loss 1.1298(1.1296) | Error 0.0121(0.0125) Steps 416(416.89) | Grad Norm 0.6677(0.3225) | Total Time 10.00(10.00)\n",
      "Iter 2964 | Time 28.6375(29.0507) | Bit/dim 1.1055(1.1096) | Xent 0.0387(0.0396) | Loss 1.1249(1.1295) | Error 0.0130(0.0125) Steps 416(416.86) | Grad Norm 0.1939(0.3186) | Total Time 10.00(10.00)\n",
      "Iter 2965 | Time 29.4702(29.0633) | Bit/dim 1.1070(1.1096) | Xent 0.0390(0.0396) | Loss 1.1265(1.1294) | Error 0.0129(0.0125) Steps 416(416.83) | Grad Norm 0.2722(0.3172) | Total Time 10.00(10.00)\n",
      "Iter 2966 | Time 28.9935(29.0612) | Bit/dim 1.1098(1.1096) | Xent 0.0494(0.0399) | Loss 1.1345(1.1295) | Error 0.0155(0.0126) Steps 416(416.81) | Grad Norm 0.8655(0.3337) | Total Time 10.00(10.00)\n",
      "Iter 2967 | Time 28.2921(29.0382) | Bit/dim 1.1091(1.1095) | Xent 0.0393(0.0399) | Loss 1.1288(1.1295) | Error 0.0111(0.0126) Steps 416(416.78) | Grad Norm 0.5468(0.3401) | Total Time 10.00(10.00)\n",
      "Iter 2968 | Time 28.8392(29.0322) | Bit/dim 1.1086(1.1095) | Xent 0.0369(0.0398) | Loss 1.1271(1.1294) | Error 0.0115(0.0125) Steps 416(416.76) | Grad Norm 0.2950(0.3387) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0424 | Time 16.4690, Epoch Time 230.2080(233.8698), Bit/dim 1.1033(best: 1.1033), Xent 0.0276, Loss 1.1171, Error 0.0097(best: 0.0085)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2969 | Time 28.7103(29.0225) | Bit/dim 1.1077(1.1095) | Xent 0.0446(0.0399) | Loss 1.1300(1.1294) | Error 0.0152(0.0126) Steps 422(416.92) | Grad Norm 0.5350(0.3446) | Total Time 10.00(10.00)\n",
      "Iter 2970 | Time 29.8104(29.0462) | Bit/dim 1.1136(1.1096) | Xent 0.0421(0.0400) | Loss 1.1346(1.1296) | Error 0.0126(0.0126) Steps 416(416.89) | Grad Norm 0.6851(0.3548) | Total Time 10.00(10.00)\n",
      "Iter 2971 | Time 29.7783(29.0681) | Bit/dim 1.1059(1.1095) | Xent 0.0384(0.0400) | Loss 1.1251(1.1295) | Error 0.0116(0.0126) Steps 422(417.04) | Grad Norm 0.1766(0.3495) | Total Time 10.00(10.00)\n",
      "Iter 2972 | Time 28.0719(29.0382) | Bit/dim 1.1071(1.1094) | Xent 0.0397(0.0400) | Loss 1.1270(1.1294) | Error 0.0112(0.0125) Steps 422(417.19) | Grad Norm 0.3366(0.3491) | Total Time 10.00(10.00)\n",
      "Iter 2973 | Time 28.3575(29.0178) | Bit/dim 1.1071(1.1093) | Xent 0.0418(0.0400) | Loss 1.1280(1.1293) | Error 0.0118(0.0125) Steps 416(417.16) | Grad Norm 0.6181(0.3572) | Total Time 10.00(10.00)\n",
      "Iter 2974 | Time 28.3663(28.9983) | Bit/dim 1.1112(1.1094) | Xent 0.0423(0.0401) | Loss 1.1323(1.1294) | Error 0.0116(0.0125) Steps 416(417.12) | Grad Norm 0.2042(0.3526) | Total Time 10.00(10.00)\n",
      "Iter 2975 | Time 29.8955(29.0252) | Bit/dim 1.1086(1.1094) | Xent 0.0372(0.0400) | Loss 1.1272(1.1294) | Error 0.0101(0.0124) Steps 416(417.09) | Grad Norm 0.3240(0.3517) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0425 | Time 16.5624, Epoch Time 232.1200(233.8173), Bit/dim 1.1037(best: 1.1033), Xent 0.0295, Loss 1.1184, Error 0.0099(best: 0.0085)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2976 | Time 29.2652(29.0324) | Bit/dim 1.1046(1.1092) | Xent 0.0422(0.0401) | Loss 1.1257(1.1293) | Error 0.0139(0.0125) Steps 416(417.06) | Grad Norm 0.4752(0.3554) | Total Time 10.00(10.00)\n",
      "Iter 2977 | Time 28.6981(29.0224) | Bit/dim 1.1096(1.1092) | Xent 0.0399(0.0401) | Loss 1.1296(1.1293) | Error 0.0125(0.0125) Steps 416(417.02) | Grad Norm 0.2259(0.3515) | Total Time 10.00(10.00)\n",
      "Iter 2978 | Time 28.7267(29.0135) | Bit/dim 1.1098(1.1093) | Xent 0.0444(0.0402) | Loss 1.1320(1.1293) | Error 0.0136(0.0125) Steps 416(416.99) | Grad Norm 0.5646(0.3579) | Total Time 10.00(10.00)\n",
      "Iter 2979 | Time 28.7342(29.0051) | Bit/dim 1.1125(1.1094) | Xent 0.0416(0.0402) | Loss 1.1332(1.1295) | Error 0.0126(0.0125) Steps 416(416.96) | Grad Norm 0.3398(0.3574) | Total Time 10.00(10.00)\n",
      "Iter 2980 | Time 29.5629(29.0218) | Bit/dim 1.1097(1.1094) | Xent 0.0423(0.0403) | Loss 1.1308(1.1295) | Error 0.0129(0.0125) Steps 416(416.93) | Grad Norm 0.2499(0.3542) | Total Time 10.00(10.00)\n",
      "Iter 2981 | Time 29.9300(29.0491) | Bit/dim 1.1107(1.1094) | Xent 0.0409(0.0403) | Loss 1.1312(1.1296) | Error 0.0134(0.0125) Steps 416(416.91) | Grad Norm 0.3743(0.3548) | Total Time 10.00(10.00)\n",
      "Iter 2982 | Time 29.0060(29.0478) | Bit/dim 1.1105(1.1094) | Xent 0.0450(0.0404) | Loss 1.1330(1.1297) | Error 0.0142(0.0126) Steps 422(417.06) | Grad Norm 0.2486(0.3516) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0426 | Time 16.5491, Epoch Time 233.2261(233.7996), Bit/dim 1.1034(best: 1.1033), Xent 0.0265, Loss 1.1167, Error 0.0091(best: 0.0085)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2983 | Time 30.0487(29.0778) | Bit/dim 1.1093(1.1094) | Xent 0.0380(0.0404) | Loss 1.1283(1.1296) | Error 0.0115(0.0126) Steps 416(417.03) | Grad Norm 0.4474(0.3545) | Total Time 10.00(10.00)\n",
      "Iter 2984 | Time 30.2885(29.1141) | Bit/dim 1.1099(1.1094) | Xent 0.0346(0.0402) | Loss 1.1272(1.1295) | Error 0.0110(0.0125) Steps 422(417.18) | Grad Norm 0.2460(0.3512) | Total Time 10.00(10.00)\n",
      "Iter 2985 | Time 29.5220(29.1264) | Bit/dim 1.1098(1.1095) | Xent 0.0410(0.0402) | Loss 1.1303(1.1296) | Error 0.0122(0.0125) Steps 416(417.14) | Grad Norm 0.3272(0.3505) | Total Time 10.00(10.00)\n",
      "Iter 2986 | Time 29.2198(29.1292) | Bit/dim 1.1099(1.1095) | Xent 0.0444(0.0403) | Loss 1.1321(1.1296) | Error 0.0129(0.0125) Steps 416(417.11) | Grad Norm 0.3566(0.3507) | Total Time 10.00(10.00)\n",
      "Iter 2987 | Time 30.1288(29.1592) | Bit/dim 1.1113(1.1095) | Xent 0.0431(0.0404) | Loss 1.1328(1.1297) | Error 0.0138(0.0126) Steps 416(417.07) | Grad Norm 0.1921(0.3459) | Total Time 10.00(10.00)\n",
      "Iter 2988 | Time 29.4720(29.1686) | Bit/dim 1.1100(1.1095) | Xent 0.0398(0.0404) | Loss 1.1299(1.1297) | Error 0.0125(0.0125) Steps 416(417.04) | Grad Norm 0.2421(0.3428) | Total Time 10.00(10.00)\n",
      "Iter 2989 | Time 28.7260(29.1553) | Bit/dim 1.1042(1.1094) | Xent 0.0339(0.0402) | Loss 1.1211(1.1295) | Error 0.0112(0.0125) Steps 428(417.37) | Grad Norm 0.2019(0.3386) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0427 | Time 16.6745, Epoch Time 236.5539(233.8822), Bit/dim 1.1039(best: 1.1033), Xent 0.0288, Loss 1.1184, Error 0.0100(best: 0.0085)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2990 | Time 29.8981(29.1776) | Bit/dim 1.1073(1.1093) | Xent 0.0408(0.0402) | Loss 1.1277(1.1294) | Error 0.0126(0.0125) Steps 416(417.33) | Grad Norm 0.2122(0.3348) | Total Time 10.00(10.00)\n",
      "Iter 2991 | Time 29.4920(29.1870) | Bit/dim 1.1116(1.1094) | Xent 0.0359(0.0401) | Loss 1.1295(1.1294) | Error 0.0116(0.0125) Steps 416(417.29) | Grad Norm 0.2035(0.3308) | Total Time 10.00(10.00)\n",
      "Iter 2992 | Time 28.4146(29.1638) | Bit/dim 1.1093(1.1094) | Xent 0.0436(0.0402) | Loss 1.1311(1.1295) | Error 0.0122(0.0125) Steps 416(417.25) | Grad Norm 0.2302(0.3278) | Total Time 10.00(10.00)\n",
      "Iter 2993 | Time 28.5842(29.1464) | Bit/dim 1.1114(1.1094) | Xent 0.0416(0.0402) | Loss 1.1322(1.1296) | Error 0.0141(0.0125) Steps 416(417.21) | Grad Norm 0.3199(0.3276) | Total Time 10.00(10.00)\n",
      "Iter 2994 | Time 30.5145(29.1875) | Bit/dim 1.1096(1.1094) | Xent 0.0394(0.0402) | Loss 1.1293(1.1296) | Error 0.0130(0.0125) Steps 422(417.36) | Grad Norm 0.1737(0.3230) | Total Time 10.00(10.00)\n",
      "Iter 2995 | Time 28.9522(29.1804) | Bit/dim 1.1028(1.1092) | Xent 0.0347(0.0401) | Loss 1.1201(1.1293) | Error 0.0111(0.0125) Steps 416(417.32) | Grad Norm 0.2639(0.3212) | Total Time 10.00(10.00)\n",
      "Iter 2996 | Time 28.1882(29.1507) | Bit/dim 1.1140(1.1094) | Xent 0.0395(0.0400) | Loss 1.1337(1.1294) | Error 0.0120(0.0125) Steps 416(417.28) | Grad Norm 0.1813(0.3170) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0428 | Time 16.7608, Epoch Time 233.2843(233.8643), Bit/dim 1.1043(best: 1.1033), Xent 0.0310, Loss 1.1198, Error 0.0103(best: 0.0085)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2997 | Time 30.3785(29.1875) | Bit/dim 1.1051(1.1093) | Xent 0.0392(0.0400) | Loss 1.1247(1.1293) | Error 0.0121(0.0125) Steps 416(417.24) | Grad Norm 0.1789(0.3129) | Total Time 10.00(10.00)\n",
      "Iter 2998 | Time 29.8571(29.2076) | Bit/dim 1.1095(1.1093) | Xent 0.0348(0.0399) | Loss 1.1269(1.1292) | Error 0.0112(0.0124) Steps 416(417.20) | Grad Norm 0.3514(0.3140) | Total Time 10.00(10.00)\n",
      "Iter 2999 | Time 29.7200(29.2230) | Bit/dim 1.1080(1.1092) | Xent 0.0483(0.0401) | Loss 1.1321(1.1293) | Error 0.0140(0.0125) Steps 422(417.35) | Grad Norm 0.3212(0.3142) | Total Time 10.00(10.00)\n",
      "Iter 3000 | Time 28.5336(29.2023) | Bit/dim 1.1093(1.1092) | Xent 0.0402(0.0401) | Loss 1.1295(1.1293) | Error 0.0129(0.0125) Steps 416(417.30) | Grad Norm 0.2616(0.3126) | Total Time 10.00(10.00)\n",
      "Iter 3001 | Time 28.6661(29.1862) | Bit/dim 1.1099(1.1093) | Xent 0.0348(0.0400) | Loss 1.1273(1.1292) | Error 0.0131(0.0125) Steps 416(417.27) | Grad Norm 0.4030(0.3154) | Total Time 10.00(10.00)\n",
      "Iter 3002 | Time 28.9099(29.1779) | Bit/dim 1.1088(1.1092) | Xent 0.0418(0.0400) | Loss 1.1297(1.1292) | Error 0.0129(0.0125) Steps 416(417.23) | Grad Norm 0.1972(0.3118) | Total Time 10.00(10.00)\n",
      "Iter 3003 | Time 30.2069(29.2088) | Bit/dim 1.1130(1.1093) | Xent 0.0340(0.0398) | Loss 1.1300(1.1293) | Error 0.0091(0.0124) Steps 422(417.37) | Grad Norm 0.3202(0.3121) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0429 | Time 16.6998, Epoch Time 235.8615(233.9242), Bit/dim 1.1038(best: 1.1033), Xent 0.0273, Loss 1.1175, Error 0.0101(best: 0.0085)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3004 | Time 28.8309(29.1974) | Bit/dim 1.1094(1.1094) | Xent 0.0433(0.0399) | Loss 1.1311(1.1293) | Error 0.0151(0.0125) Steps 416(417.33) | Grad Norm 0.2336(0.3097) | Total Time 10.00(10.00)\n",
      "Iter 3005 | Time 29.5919(29.2093) | Bit/dim 1.1107(1.1094) | Xent 0.0443(0.0401) | Loss 1.1328(1.1294) | Error 0.0140(0.0125) Steps 416(417.29) | Grad Norm 0.2129(0.3068) | Total Time 10.00(10.00)\n",
      "Iter 3006 | Time 28.6088(29.1913) | Bit/dim 1.1130(1.1095) | Xent 0.0485(0.0403) | Loss 1.1372(1.1297) | Error 0.0148(0.0126) Steps 416(417.25) | Grad Norm 0.5026(0.3127) | Total Time 10.00(10.00)\n",
      "Iter 3007 | Time 29.3397(29.1957) | Bit/dim 1.1082(1.1095) | Xent 0.0384(0.0403) | Loss 1.1274(1.1296) | Error 0.0121(0.0126) Steps 416(417.21) | Grad Norm 0.2659(0.3113) | Total Time 10.00(10.00)\n",
      "Iter 3008 | Time 28.3886(29.1715) | Bit/dim 1.1106(1.1095) | Xent 0.0425(0.0403) | Loss 1.1318(1.1297) | Error 0.0135(0.0126) Steps 422(417.36) | Grad Norm 0.2218(0.3086) | Total Time 10.00(10.00)\n",
      "Iter 3009 | Time 29.5826(29.1838) | Bit/dim 1.1066(1.1094) | Xent 0.0313(0.0401) | Loss 1.1222(1.1294) | Error 0.0106(0.0126) Steps 416(417.32) | Grad Norm 0.4261(0.3121) | Total Time 10.00(10.00)\n",
      "Iter 3010 | Time 29.5329(29.1943) | Bit/dim 1.1109(1.1095) | Xent 0.0363(0.0399) | Loss 1.1291(1.1294) | Error 0.0121(0.0126) Steps 416(417.28) | Grad Norm 0.2873(0.3114) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0430 | Time 16.5669, Epoch Time 232.8399(233.8917), Bit/dim 1.1023(best: 1.1033), Xent 0.0280, Loss 1.1163, Error 0.0096(best: 0.0085)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3011 | Time 28.8949(29.1853) | Bit/dim 1.1101(1.1095) | Xent 0.0398(0.0399) | Loss 1.1300(1.1294) | Error 0.0125(0.0126) Steps 416(417.24) | Grad Norm 0.3036(0.3111) | Total Time 10.00(10.00)\n",
      "Iter 3012 | Time 28.6788(29.1701) | Bit/dim 1.1105(1.1095) | Xent 0.0390(0.0399) | Loss 1.1300(1.1295) | Error 0.0124(0.0125) Steps 416(417.20) | Grad Norm 0.3819(0.3133) | Total Time 10.00(10.00)\n",
      "Iter 3013 | Time 28.6417(29.1543) | Bit/dim 1.1053(1.1094) | Xent 0.0443(0.0400) | Loss 1.1274(1.1294) | Error 0.0135(0.0126) Steps 416(417.17) | Grad Norm 0.3950(0.3157) | Total Time 10.00(10.00)\n",
      "Iter 3014 | Time 28.7448(29.1420) | Bit/dim 1.1120(1.1095) | Xent 0.0394(0.0400) | Loss 1.1317(1.1295) | Error 0.0119(0.0126) Steps 416(417.13) | Grad Norm 0.3817(0.3177) | Total Time 10.00(10.00)\n",
      "Iter 3015 | Time 29.6641(29.1577) | Bit/dim 1.1071(1.1094) | Xent 0.0438(0.0401) | Loss 1.1290(1.1295) | Error 0.0144(0.0126) Steps 422(417.28) | Grad Norm 0.2229(0.3148) | Total Time 10.00(10.00)\n",
      "Iter 3016 | Time 28.9710(29.1521) | Bit/dim 1.1083(1.1094) | Xent 0.0417(0.0402) | Loss 1.1291(1.1294) | Error 0.0136(0.0126) Steps 416(417.24) | Grad Norm 0.2678(0.3134) | Total Time 10.00(10.00)\n",
      "Iter 3017 | Time 28.9777(29.1468) | Bit/dim 1.1133(1.1095) | Xent 0.0391(0.0402) | Loss 1.1329(1.1295) | Error 0.0109(0.0126) Steps 416(417.20) | Grad Norm 0.2344(0.3111) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0431 | Time 16.6432, Epoch Time 231.6020(233.8230), Bit/dim 1.1032(best: 1.1023), Xent 0.0293, Loss 1.1178, Error 0.0096(best: 0.0085)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3018 | Time 29.3504(29.1529) | Bit/dim 1.1113(1.1095) | Xent 0.0415(0.0402) | Loss 1.1320(1.1296) | Error 0.0144(0.0126) Steps 416(417.17) | Grad Norm 0.2856(0.3103) | Total Time 10.00(10.00)\n",
      "Iter 3019 | Time 28.5296(29.1342) | Bit/dim 1.1041(1.1094) | Xent 0.0440(0.0403) | Loss 1.1261(1.1295) | Error 0.0135(0.0127) Steps 422(417.31) | Grad Norm 0.3058(0.3102) | Total Time 10.00(10.00)\n",
      "Iter 3020 | Time 28.6466(29.1196) | Bit/dim 1.1105(1.1094) | Xent 0.0357(0.0402) | Loss 1.1283(1.1295) | Error 0.0112(0.0126) Steps 422(417.45) | Grad Norm 0.2970(0.3098) | Total Time 10.00(10.00)\n",
      "Iter 3021 | Time 28.5542(29.1026) | Bit/dim 1.1106(1.1094) | Xent 0.0349(0.0400) | Loss 1.1280(1.1294) | Error 0.0121(0.0126) Steps 416(417.41) | Grad Norm 0.4787(0.3148) | Total Time 10.00(10.00)\n",
      "Iter 3022 | Time 28.3646(29.0805) | Bit/dim 1.1103(1.1095) | Xent 0.0382(0.0400) | Loss 1.1295(1.1294) | Error 0.0114(0.0126) Steps 416(417.37) | Grad Norm 0.4601(0.3192) | Total Time 10.00(10.00)\n",
      "Iter 3023 | Time 29.1604(29.0829) | Bit/dim 1.1066(1.1094) | Xent 0.0401(0.0400) | Loss 1.1267(1.1294) | Error 0.0124(0.0126) Steps 422(417.50) | Grad Norm 0.2694(0.3177) | Total Time 10.00(10.00)\n",
      "Iter 3024 | Time 30.1967(29.1163) | Bit/dim 1.1109(1.1094) | Xent 0.0347(0.0398) | Loss 1.1282(1.1293) | Error 0.0109(0.0125) Steps 416(417.46) | Grad Norm 0.4870(0.3228) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0432 | Time 16.6913, Epoch Time 231.7804(233.7617), Bit/dim 1.1023(best: 1.1023), Xent 0.0257, Loss 1.1151, Error 0.0088(best: 0.0085)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3025 | Time 30.4866(29.1574) | Bit/dim 1.1117(1.1095) | Xent 0.0385(0.0398) | Loss 1.1310(1.1294) | Error 0.0122(0.0125) Steps 428(417.78) | Grad Norm 0.2927(0.3219) | Total Time 10.00(10.00)\n",
      "Iter 3026 | Time 28.7183(29.1442) | Bit/dim 1.1075(1.1094) | Xent 0.0407(0.0398) | Loss 1.1279(1.1293) | Error 0.0125(0.0125) Steps 422(417.90) | Grad Norm 0.2781(0.3206) | Total Time 10.00(10.00)\n",
      "Iter 3027 | Time 28.5968(29.1278) | Bit/dim 1.1101(1.1094) | Xent 0.0413(0.0398) | Loss 1.1307(1.1294) | Error 0.0125(0.0125) Steps 416(417.84) | Grad Norm 0.2035(0.3171) | Total Time 10.00(10.00)\n",
      "Iter 3028 | Time 31.4731(29.1982) | Bit/dim 1.1073(1.1094) | Xent 0.0406(0.0399) | Loss 1.1275(1.1293) | Error 0.0130(0.0125) Steps 428(418.15) | Grad Norm 0.2088(0.3138) | Total Time 10.00(10.00)\n",
      "Iter 3029 | Time 29.7804(29.2156) | Bit/dim 1.1054(1.1093) | Xent 0.0376(0.0398) | Loss 1.1242(1.1292) | Error 0.0118(0.0125) Steps 422(418.27) | Grad Norm 0.2067(0.3106) | Total Time 10.00(10.00)\n",
      "Iter 3030 | Time 30.0511(29.2407) | Bit/dim 1.1124(1.1094) | Xent 0.0392(0.0398) | Loss 1.1320(1.1292) | Error 0.0129(0.0125) Steps 422(418.38) | Grad Norm 0.2565(0.3090) | Total Time 10.00(10.00)\n",
      "Iter 3031 | Time 30.5363(29.2796) | Bit/dim 1.1110(1.1094) | Xent 0.0421(0.0398) | Loss 1.1321(1.1293) | Error 0.0134(0.0125) Steps 416(418.31) | Grad Norm 0.3551(0.3104) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0433 | Time 16.6162, Epoch Time 238.5507(233.9054), Bit/dim 1.1040(best: 1.1023), Xent 0.0263, Loss 1.1171, Error 0.0094(best: 0.0085)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3032 | Time 30.3447(29.3115) | Bit/dim 1.1065(1.1093) | Xent 0.0367(0.0398) | Loss 1.1249(1.1292) | Error 0.0110(0.0125) Steps 422(418.42) | Grad Norm 0.3806(0.3125) | Total Time 10.00(10.00)\n",
      "Iter 3033 | Time 30.9722(29.3614) | Bit/dim 1.1119(1.1094) | Xent 0.0326(0.0395) | Loss 1.1282(1.1292) | Error 0.0101(0.0124) Steps 416(418.34) | Grad Norm 0.1774(0.3084) | Total Time 10.00(10.00)\n",
      "Iter 3034 | Time 29.7830(29.3740) | Bit/dim 1.1117(1.1095) | Xent 0.0441(0.0397) | Loss 1.1337(1.1293) | Error 0.0148(0.0125) Steps 416(418.27) | Grad Norm 0.4575(0.3129) | Total Time 10.00(10.00)\n",
      "Iter 3035 | Time 30.3759(29.4041) | Bit/dim 1.1092(1.1095) | Xent 0.0432(0.0398) | Loss 1.1309(1.1294) | Error 0.0139(0.0125) Steps 416(418.21) | Grad Norm 0.2893(0.3122) | Total Time 10.00(10.00)\n",
      "Iter 3036 | Time 29.1836(29.3974) | Bit/dim 1.1061(1.1094) | Xent 0.0374(0.0397) | Loss 1.1248(1.1292) | Error 0.0126(0.0125) Steps 422(418.32) | Grad Norm 0.3023(0.3119) | Total Time 10.00(10.00)\n",
      "Iter 3037 | Time 29.4358(29.3986) | Bit/dim 1.1092(1.1094) | Xent 0.0450(0.0399) | Loss 1.1317(1.1293) | Error 0.0151(0.0126) Steps 416(418.25) | Grad Norm 0.4441(0.3158) | Total Time 10.00(10.00)\n",
      "Iter 3038 | Time 29.2354(29.3937) | Bit/dim 1.1108(1.1094) | Xent 0.0407(0.0399) | Loss 1.1311(1.1293) | Error 0.0138(0.0126) Steps 422(418.36) | Grad Norm 0.2972(0.3153) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0434 | Time 16.6735, Epoch Time 238.4954(234.0431), Bit/dim 1.1033(best: 1.1023), Xent 0.0267, Loss 1.1167, Error 0.0092(best: 0.0085)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3039 | Time 30.1486(29.4163) | Bit/dim 1.1091(1.1094) | Xent 0.0349(0.0397) | Loss 1.1266(1.1293) | Error 0.0115(0.0126) Steps 416(418.29) | Grad Norm 0.2379(0.3130) | Total Time 10.00(10.00)\n",
      "Iter 3040 | Time 28.5629(29.3907) | Bit/dim 1.1092(1.1094) | Xent 0.0297(0.0394) | Loss 1.1241(1.1291) | Error 0.0092(0.0125) Steps 416(418.22) | Grad Norm 0.1985(0.3095) | Total Time 10.00(10.00)\n",
      "Iter 3041 | Time 30.4474(29.4224) | Bit/dim 1.1082(1.1093) | Xent 0.0386(0.0394) | Loss 1.1275(1.1291) | Error 0.0134(0.0125) Steps 422(418.34) | Grad Norm 0.2259(0.3070) | Total Time 10.00(10.00)\n",
      "Iter 3042 | Time 29.0762(29.4121) | Bit/dim 1.1091(1.1093) | Xent 0.0422(0.0395) | Loss 1.1302(1.1291) | Error 0.0140(0.0126) Steps 416(418.27) | Grad Norm 0.2862(0.3064) | Total Time 10.00(10.00)\n",
      "Iter 3043 | Time 28.8635(29.3956) | Bit/dim 1.1043(1.1092) | Xent 0.0416(0.0396) | Loss 1.1250(1.1290) | Error 0.0135(0.0126) Steps 422(418.38) | Grad Norm 0.2041(0.3033) | Total Time 10.00(10.00)\n",
      "Iter 3044 | Time 29.6806(29.4041) | Bit/dim 1.1111(1.1092) | Xent 0.0448(0.0397) | Loss 1.1335(1.1291) | Error 0.0139(0.0126) Steps 416(418.31) | Grad Norm 0.3170(0.3037) | Total Time 10.00(10.00)\n",
      "Iter 3045 | Time 28.2525(29.3696) | Bit/dim 1.1096(1.1093) | Xent 0.0383(0.0397) | Loss 1.1287(1.1291) | Error 0.0111(0.0126) Steps 416(418.24) | Grad Norm 0.2635(0.3025) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0435 | Time 16.4443, Epoch Time 233.9313(234.0397), Bit/dim 1.1035(best: 1.1023), Xent 0.0278, Loss 1.1174, Error 0.0087(best: 0.0085)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3046 | Time 28.6009(29.3465) | Bit/dim 1.1137(1.1094) | Xent 0.0443(0.0398) | Loss 1.1359(1.1293) | Error 0.0141(0.0126) Steps 416(418.17) | Grad Norm 0.2015(0.2995) | Total Time 10.00(10.00)\n",
      "Iter 3047 | Time 29.9525(29.3647) | Bit/dim 1.1044(1.1092) | Xent 0.0394(0.0398) | Loss 1.1240(1.1291) | Error 0.0114(0.0126) Steps 422(418.29) | Grad Norm 0.2248(0.2973) | Total Time 10.00(10.00)\n",
      "Iter 3048 | Time 30.0501(29.3853) | Bit/dim 1.1062(1.1091) | Xent 0.0404(0.0398) | Loss 1.1264(1.1291) | Error 0.0118(0.0126) Steps 416(418.22) | Grad Norm 0.4479(0.3018) | Total Time 10.00(10.00)\n",
      "Iter 3049 | Time 28.9109(29.3710) | Bit/dim 1.1120(1.1092) | Xent 0.0382(0.0398) | Loss 1.1311(1.1291) | Error 0.0118(0.0126) Steps 416(418.15) | Grad Norm 0.3964(0.3046) | Total Time 10.00(10.00)\n",
      "Iter 3050 | Time 29.7722(29.3831) | Bit/dim 1.1138(1.1094) | Xent 0.0430(0.0399) | Loss 1.1353(1.1293) | Error 0.0122(0.0125) Steps 416(418.09) | Grad Norm 0.1907(0.3012) | Total Time 10.00(10.00)\n",
      "Iter 3051 | Time 28.8854(29.3681) | Bit/dim 1.1052(1.1092) | Xent 0.0380(0.0398) | Loss 1.1242(1.1291) | Error 0.0128(0.0126) Steps 416(418.02) | Grad Norm 0.4577(0.3059) | Total Time 10.00(10.00)\n",
      "Iter 3052 | Time 30.0465(29.3885) | Bit/dim 1.1096(1.1093) | Xent 0.0370(0.0397) | Loss 1.1281(1.1291) | Error 0.0116(0.0125) Steps 422(418.14) | Grad Norm 0.3934(0.3085) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0436 | Time 16.8849, Epoch Time 235.5083(234.0838), Bit/dim 1.1028(best: 1.1023), Xent 0.0301, Loss 1.1178, Error 0.0103(best: 0.0085)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3053 | Time 29.2390(29.3840) | Bit/dim 1.1098(1.1093) | Xent 0.0378(0.0397) | Loss 1.1287(1.1291) | Error 0.0121(0.0125) Steps 422(418.26) | Grad Norm 0.2572(0.3070) | Total Time 10.00(10.00)\n",
      "Iter 3054 | Time 28.5581(29.3592) | Bit/dim 1.1087(1.1093) | Xent 0.0414(0.0397) | Loss 1.1294(1.1291) | Error 0.0121(0.0125) Steps 416(418.19) | Grad Norm 0.5272(0.3136) | Total Time 10.00(10.00)\n",
      "Iter 3055 | Time 28.4304(29.3314) | Bit/dim 1.1087(1.1092) | Xent 0.0377(0.0397) | Loss 1.1276(1.1291) | Error 0.0124(0.0125) Steps 416(418.12) | Grad Norm 0.1936(0.3100) | Total Time 10.00(10.00)\n",
      "Iter 3056 | Time 29.9758(29.3507) | Bit/dim 1.1062(1.1091) | Xent 0.0371(0.0396) | Loss 1.1248(1.1289) | Error 0.0116(0.0125) Steps 416(418.06) | Grad Norm 0.2085(0.3069) | Total Time 10.00(10.00)\n",
      "Iter 3057 | Time 29.7430(29.3625) | Bit/dim 1.1144(1.1093) | Xent 0.0360(0.0395) | Loss 1.1324(1.1290) | Error 0.0108(0.0124) Steps 422(418.18) | Grad Norm 0.2934(0.3065) | Total Time 10.00(10.00)\n",
      "Iter 3058 | Time 30.4132(29.3940) | Bit/dim 1.1042(1.1092) | Xent 0.0429(0.0396) | Loss 1.1257(1.1289) | Error 0.0128(0.0124) Steps 416(418.11) | Grad Norm 0.2796(0.3057) | Total Time 10.00(10.00)\n",
      "Iter 3059 | Time 29.0616(29.3840) | Bit/dim 1.1099(1.1092) | Xent 0.0473(0.0398) | Loss 1.1336(1.1291) | Error 0.0131(0.0125) Steps 416(418.05) | Grad Norm 0.3164(0.3060) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0437 | Time 16.6581, Epoch Time 234.4341(234.0943), Bit/dim 1.1027(best: 1.1023), Xent 0.0281, Loss 1.1168, Error 0.0093(best: 0.0085)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3060 | Time 29.5403(29.3887) | Bit/dim 1.1085(1.1092) | Xent 0.0425(0.0399) | Loss 1.1297(1.1291) | Error 0.0132(0.0125) Steps 422(418.17) | Grad Norm 0.3980(0.3088) | Total Time 10.00(10.00)\n",
      "Iter 3061 | Time 29.4703(29.3912) | Bit/dim 1.1072(1.1091) | Xent 0.0382(0.0398) | Loss 1.1263(1.1290) | Error 0.0132(0.0125) Steps 422(418.28) | Grad Norm 0.3789(0.3109) | Total Time 10.00(10.00)\n",
      "Iter 3062 | Time 28.4591(29.3632) | Bit/dim 1.1105(1.1091) | Xent 0.0400(0.0398) | Loss 1.1305(1.1291) | Error 0.0134(0.0125) Steps 416(418.22) | Grad Norm 0.4289(0.3144) | Total Time 10.00(10.00)\n",
      "Iter 3063 | Time 28.7877(29.3459) | Bit/dim 1.1103(1.1092) | Xent 0.0417(0.0399) | Loss 1.1311(1.1291) | Error 0.0134(0.0126) Steps 422(418.33) | Grad Norm 0.4545(0.3187) | Total Time 10.00(10.00)\n",
      "Iter 3064 | Time 31.0531(29.3971) | Bit/dim 1.1093(1.1092) | Xent 0.0383(0.0399) | Loss 1.1285(1.1291) | Error 0.0101(0.0125) Steps 422(418.44) | Grad Norm 0.3284(0.3189) | Total Time 10.00(10.00)\n",
      "Iter 3065 | Time 29.4401(29.3984) | Bit/dim 1.1094(1.1092) | Xent 0.0364(0.0398) | Loss 1.1276(1.1291) | Error 0.0115(0.0124) Steps 416(418.37) | Grad Norm 0.2456(0.3167) | Total Time 10.00(10.00)\n",
      "Iter 3066 | Time 29.0819(29.3889) | Bit/dim 1.1066(1.1091) | Xent 0.0377(0.0397) | Loss 1.1255(1.1290) | Error 0.0124(0.0124) Steps 416(418.29) | Grad Norm 0.2355(0.3143) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0438 | Time 16.5676, Epoch Time 234.7904(234.1152), Bit/dim 1.1030(best: 1.1023), Xent 0.0291, Loss 1.1176, Error 0.0100(best: 0.0085)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3067 | Time 28.1004(29.3503) | Bit/dim 1.1069(1.1090) | Xent 0.0375(0.0396) | Loss 1.1256(1.1289) | Error 0.0115(0.0124) Steps 416(418.23) | Grad Norm 0.2535(0.3125) | Total Time 10.00(10.00)\n",
      "Iter 3068 | Time 29.1580(29.3445) | Bit/dim 1.1081(1.1090) | Xent 0.0373(0.0396) | Loss 1.1267(1.1288) | Error 0.0114(0.0124) Steps 416(418.16) | Grad Norm 0.3696(0.3142) | Total Time 10.00(10.00)\n",
      "Iter 3069 | Time 30.2405(29.3714) | Bit/dim 1.1134(1.1091) | Xent 0.0413(0.0396) | Loss 1.1341(1.1289) | Error 0.0129(0.0124) Steps 422(418.27) | Grad Norm 0.4246(0.3175) | Total Time 10.00(10.00)\n",
      "Iter 3070 | Time 28.9089(29.3575) | Bit/dim 1.1122(1.1092) | Xent 0.0389(0.0396) | Loss 1.1316(1.1290) | Error 0.0132(0.0124) Steps 416(418.21) | Grad Norm 0.2339(0.3150) | Total Time 10.00(10.00)\n",
      "Iter 3071 | Time 29.5177(29.3623) | Bit/dim 1.1030(1.1090) | Xent 0.0439(0.0397) | Loss 1.1249(1.1289) | Error 0.0149(0.0125) Steps 416(418.14) | Grad Norm 0.3402(0.3158) | Total Time 10.00(10.00)\n",
      "Iter 3072 | Time 29.4497(29.3649) | Bit/dim 1.1114(1.1091) | Xent 0.0406(0.0397) | Loss 1.1317(1.1290) | Error 0.0132(0.0125) Steps 422(418.26) | Grad Norm 0.3075(0.3155) | Total Time 10.00(10.00)\n",
      "Iter 3073 | Time 30.8031(29.4081) | Bit/dim 1.1053(1.1090) | Xent 0.0383(0.0397) | Loss 1.1244(1.1289) | Error 0.0125(0.0125) Steps 422(418.37) | Grad Norm 0.3710(0.3172) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0439 | Time 16.7441, Epoch Time 235.3230(234.1514), Bit/dim 1.1026(best: 1.1023), Xent 0.0255, Loss 1.1153, Error 0.0090(best: 0.0085)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3074 | Time 29.8413(29.4211) | Bit/dim 1.1054(1.1089) | Xent 0.0419(0.0398) | Loss 1.1263(1.1288) | Error 0.0136(0.0126) Steps 416(418.30) | Grad Norm 0.3761(0.3189) | Total Time 10.00(10.00)\n",
      "Iter 3075 | Time 30.5202(29.4541) | Bit/dim 1.1099(1.1089) | Xent 0.0337(0.0396) | Loss 1.1267(1.1287) | Error 0.0122(0.0125) Steps 416(418.23) | Grad Norm 0.2167(0.3159) | Total Time 10.00(10.00)\n",
      "Iter 3076 | Time 29.4142(29.4529) | Bit/dim 1.1052(1.1088) | Xent 0.0332(0.0394) | Loss 1.1218(1.1285) | Error 0.0104(0.0125) Steps 416(418.16) | Grad Norm 0.2077(0.3126) | Total Time 10.00(10.00)\n",
      "Iter 3077 | Time 29.7432(29.4616) | Bit/dim 1.1093(1.1088) | Xent 0.0369(0.0393) | Loss 1.1277(1.1285) | Error 0.0118(0.0125) Steps 422(418.28) | Grad Norm 0.3123(0.3126) | Total Time 10.00(10.00)\n",
      "Iter 3078 | Time 30.7242(29.4995) | Bit/dim 1.1122(1.1089) | Xent 0.0487(0.0396) | Loss 1.1365(1.1287) | Error 0.0160(0.0126) Steps 422(418.39) | Grad Norm 0.2950(0.3121) | Total Time 10.00(10.00)\n",
      "Iter 3079 | Time 30.3986(29.5264) | Bit/dim 1.1104(1.1090) | Xent 0.0398(0.0396) | Loss 1.1303(1.1288) | Error 0.0126(0.0126) Steps 422(418.50) | Grad Norm 0.3826(0.3142) | Total Time 10.00(10.00)\n",
      "Iter 3080 | Time 29.7316(29.5326) | Bit/dim 1.1065(1.1089) | Xent 0.0389(0.0396) | Loss 1.1260(1.1287) | Error 0.0111(0.0125) Steps 422(418.60) | Grad Norm 0.5755(0.3220) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0440 | Time 16.5652, Epoch Time 239.6074(234.3151), Bit/dim 1.1024(best: 1.1023), Xent 0.0293, Loss 1.1171, Error 0.0102(best: 0.0085)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3081 | Time 30.7504(29.5691) | Bit/dim 1.1068(1.1088) | Xent 0.0365(0.0395) | Loss 1.1251(1.1286) | Error 0.0114(0.0125) Steps 428(418.88) | Grad Norm 0.1989(0.3183) | Total Time 10.00(10.00)\n",
      "Iter 3082 | Time 28.6885(29.5427) | Bit/dim 1.1120(1.1089) | Xent 0.0349(0.0394) | Loss 1.1295(1.1286) | Error 0.0100(0.0124) Steps 416(418.80) | Grad Norm 0.5949(0.3266) | Total Time 10.00(10.00)\n",
      "Iter 3083 | Time 29.0379(29.5276) | Bit/dim 1.1127(1.1090) | Xent 0.0386(0.0393) | Loss 1.1320(1.1287) | Error 0.0105(0.0124) Steps 416(418.71) | Grad Norm 0.5879(0.3345) | Total Time 10.00(10.00)\n",
      "Iter 3084 | Time 28.2380(29.4889) | Bit/dim 1.1067(1.1090) | Xent 0.0367(0.0392) | Loss 1.1250(1.1286) | Error 0.0130(0.0124) Steps 416(418.63) | Grad Norm 0.2559(0.3321) | Total Time 10.00(10.00)\n",
      "Iter 3085 | Time 30.1853(29.5098) | Bit/dim 1.1094(1.1090) | Xent 0.0424(0.0393) | Loss 1.1305(1.1287) | Error 0.0135(0.0124) Steps 422(418.73) | Grad Norm 0.2053(0.3283) | Total Time 10.00(10.00)\n",
      "Iter 3086 | Time 30.8118(29.5488) | Bit/dim 1.1088(1.1090) | Xent 0.0413(0.0394) | Loss 1.1294(1.1287) | Error 0.0126(0.0124) Steps 422(418.83) | Grad Norm 0.5556(0.3351) | Total Time 10.00(10.00)\n",
      "Iter 3087 | Time 28.8878(29.5290) | Bit/dim 1.1061(1.1089) | Xent 0.0411(0.0395) | Loss 1.1267(1.1286) | Error 0.0111(0.0124) Steps 416(418.75) | Grad Norm 0.7486(0.3475) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0441 | Time 16.5929, Epoch Time 235.9579(234.3644), Bit/dim 1.1031(best: 1.1023), Xent 0.0271, Loss 1.1166, Error 0.0091(best: 0.0085)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3088 | Time 28.4224(29.4958) | Bit/dim 1.1118(1.1090) | Xent 0.0409(0.0395) | Loss 1.1322(1.1287) | Error 0.0142(0.0124) Steps 416(418.66) | Grad Norm 0.4141(0.3495) | Total Time 10.00(10.00)\n",
      "Iter 3089 | Time 29.2057(29.4871) | Bit/dim 1.1081(1.1090) | Xent 0.0361(0.0394) | Loss 1.1262(1.1287) | Error 0.0121(0.0124) Steps 422(418.76) | Grad Norm 0.6337(0.3581) | Total Time 10.00(10.00)\n",
      "Iter 3090 | Time 28.9705(29.4716) | Bit/dim 1.1103(1.1090) | Xent 0.0345(0.0392) | Loss 1.1275(1.1286) | Error 0.0106(0.0124) Steps 416(418.68) | Grad Norm 0.7622(0.3702) | Total Time 10.00(10.00)\n",
      "Iter 3091 | Time 29.9072(29.4847) | Bit/dim 1.1093(1.1090) | Xent 0.0384(0.0392) | Loss 1.1285(1.1286) | Error 0.0098(0.0123) Steps 422(418.78) | Grad Norm 0.5005(0.3741) | Total Time 10.00(10.00)\n",
      "Iter 3092 | Time 30.2847(29.5087) | Bit/dim 1.1030(1.1088) | Xent 0.0355(0.0391) | Loss 1.1207(1.1284) | Error 0.0118(0.0123) Steps 428(419.06) | Grad Norm 0.2026(0.3690) | Total Time 10.00(10.00)\n",
      "Iter 3093 | Time 29.0389(29.4946) | Bit/dim 1.1052(1.1087) | Xent 0.0383(0.0391) | Loss 1.1243(1.1283) | Error 0.0125(0.0123) Steps 416(418.97) | Grad Norm 0.7803(0.3813) | Total Time 10.00(10.00)\n",
      "Iter 3094 | Time 28.8634(29.4756) | Bit/dim 1.1129(1.1088) | Xent 0.0435(0.0392) | Loss 1.1347(1.1284) | Error 0.0142(0.0123) Steps 416(418.88) | Grad Norm 0.5921(0.3876) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0442 | Time 16.5343, Epoch Time 233.7582(234.3462), Bit/dim 1.1021(best: 1.1023), Xent 0.0265, Loss 1.1154, Error 0.0092(best: 0.0085)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3095 | Time 28.6139(29.4498) | Bit/dim 1.1106(1.1089) | Xent 0.0410(0.0393) | Loss 1.1311(1.1285) | Error 0.0121(0.0123) Steps 416(418.79) | Grad Norm 0.4493(0.3895) | Total Time 10.00(10.00)\n",
      "Iter 3096 | Time 30.2279(29.4731) | Bit/dim 1.1064(1.1088) | Xent 0.0368(0.0392) | Loss 1.1248(1.1284) | Error 0.0101(0.0123) Steps 422(418.89) | Grad Norm 0.6407(0.3970) | Total Time 10.00(10.00)\n",
      "Iter 3097 | Time 28.7171(29.4504) | Bit/dim 1.1071(1.1088) | Xent 0.0396(0.0392) | Loss 1.1269(1.1284) | Error 0.0121(0.0123) Steps 422(418.98) | Grad Norm 0.8469(0.4105) | Total Time 10.00(10.00)\n",
      "Iter 3098 | Time 28.8581(29.4327) | Bit/dim 1.1078(1.1087) | Xent 0.0430(0.0393) | Loss 1.1293(1.1284) | Error 0.0124(0.0123) Steps 416(418.89) | Grad Norm 0.3101(0.4075) | Total Time 10.00(10.00)\n",
      "Iter 3099 | Time 29.7069(29.4409) | Bit/dim 1.1074(1.1087) | Xent 0.0375(0.0393) | Loss 1.1262(1.1283) | Error 0.0124(0.0123) Steps 416(418.80) | Grad Norm 0.4908(0.4100) | Total Time 10.00(10.00)\n",
      "Iter 3100 | Time 29.3156(29.4371) | Bit/dim 1.1126(1.1088) | Xent 0.0398(0.0393) | Loss 1.1325(1.1285) | Error 0.0129(0.0123) Steps 416(418.72) | Grad Norm 0.3306(0.4076) | Total Time 10.00(10.00)\n",
      "Iter 3101 | Time 28.7248(29.4158) | Bit/dim 1.1090(1.1088) | Xent 0.0435(0.0394) | Loss 1.1308(1.1285) | Error 0.0148(0.0124) Steps 422(418.82) | Grad Norm 0.5356(0.4114) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0443 | Time 16.6471, Epoch Time 233.1704(234.3109), Bit/dim 1.1023(best: 1.1021), Xent 0.0277, Loss 1.1161, Error 0.0102(best: 0.0085)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3102 | Time 29.3964(29.4152) | Bit/dim 1.1085(1.1088) | Xent 0.0435(0.0395) | Loss 1.1303(1.1286) | Error 0.0128(0.0124) Steps 416(418.73) | Grad Norm 0.2294(0.4060) | Total Time 10.00(10.00)\n",
      "Iter 3103 | Time 28.2153(29.3792) | Bit/dim 1.1068(1.1087) | Xent 0.0386(0.0395) | Loss 1.1261(1.1285) | Error 0.0121(0.0124) Steps 416(418.65) | Grad Norm 0.4764(0.4081) | Total Time 10.00(10.00)\n",
      "Iter 3104 | Time 29.9473(29.3962) | Bit/dim 1.1065(1.1087) | Xent 0.0359(0.0394) | Loss 1.1244(1.1284) | Error 0.0116(0.0123) Steps 428(418.93) | Grad Norm 0.4817(0.4103) | Total Time 10.00(10.00)\n",
      "Iter 3105 | Time 29.1489(29.3888) | Bit/dim 1.1101(1.1087) | Xent 0.0423(0.0395) | Loss 1.1313(1.1285) | Error 0.0130(0.0124) Steps 416(418.84) | Grad Norm 0.2678(0.4060) | Total Time 10.00(10.00)\n",
      "Iter 3106 | Time 28.7918(29.3709) | Bit/dim 1.1049(1.1086) | Xent 0.0442(0.0396) | Loss 1.1270(1.1284) | Error 0.0135(0.0124) Steps 422(418.94) | Grad Norm 0.2110(0.4002) | Total Time 10.00(10.00)\n",
      "Iter 3107 | Time 30.1419(29.3940) | Bit/dim 1.1102(1.1087) | Xent 0.0341(0.0395) | Loss 1.1272(1.1284) | Error 0.0100(0.0123) Steps 434(419.39) | Grad Norm 0.3364(0.3983) | Total Time 10.00(10.00)\n",
      "Iter 3108 | Time 30.3952(29.4241) | Bit/dim 1.1092(1.1087) | Xent 0.0418(0.0395) | Loss 1.1301(1.1284) | Error 0.0132(0.0124) Steps 422(419.47) | Grad Norm 0.3078(0.3956) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0444 | Time 16.4919, Epoch Time 235.2703(234.3397), Bit/dim 1.1030(best: 1.1021), Xent 0.0303, Loss 1.1182, Error 0.0102(best: 0.0085)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3109 | Time 28.7070(29.4026) | Bit/dim 1.1127(1.1088) | Xent 0.0409(0.0396) | Loss 1.1331(1.1286) | Error 0.0126(0.0124) Steps 422(419.54) | Grad Norm 0.1919(0.3894) | Total Time 10.00(10.00)\n",
      "Iter 3110 | Time 30.8892(29.4472) | Bit/dim 1.1039(1.1086) | Xent 0.0472(0.0398) | Loss 1.1275(1.1285) | Error 0.0135(0.0124) Steps 416(419.44) | Grad Norm 0.2632(0.3857) | Total Time 10.00(10.00)\n",
      "Iter 3111 | Time 30.1183(29.4673) | Bit/dim 1.1067(1.1086) | Xent 0.0386(0.0398) | Loss 1.1260(1.1285) | Error 0.0128(0.0124) Steps 416(419.34) | Grad Norm 0.3422(0.3844) | Total Time 10.00(10.00)\n",
      "Iter 3112 | Time 28.9553(29.4519) | Bit/dim 1.1072(1.1085) | Xent 0.0418(0.0398) | Loss 1.1281(1.1285) | Error 0.0119(0.0124) Steps 416(419.24) | Grad Norm 0.2352(0.3799) | Total Time 10.00(10.00)\n",
      "Iter 3113 | Time 29.3448(29.4487) | Bit/dim 1.1072(1.1085) | Xent 0.0383(0.0398) | Loss 1.1263(1.1284) | Error 0.0106(0.0123) Steps 422(419.32) | Grad Norm 0.2396(0.3757) | Total Time 10.00(10.00)\n",
      "Iter 3114 | Time 28.7636(29.4282) | Bit/dim 1.1108(1.1086) | Xent 0.0357(0.0397) | Loss 1.1286(1.1284) | Error 0.0121(0.0123) Steps 416(419.22) | Grad Norm 0.2853(0.3730) | Total Time 10.00(10.00)\n",
      "Iter 3115 | Time 29.9590(29.4441) | Bit/dim 1.1130(1.1087) | Xent 0.0380(0.0396) | Loss 1.1320(1.1285) | Error 0.0120(0.0123) Steps 416(419.12) | Grad Norm 0.2682(0.3698) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0445 | Time 16.3551, Epoch Time 235.4774(234.3738), Bit/dim 1.1029(best: 1.1021), Xent 0.0271, Loss 1.1165, Error 0.0096(best: 0.0085)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3116 | Time 28.6397(29.4200) | Bit/dim 1.1132(1.1088) | Xent 0.0428(0.0397) | Loss 1.1346(1.1287) | Error 0.0134(0.0124) Steps 416(419.03) | Grad Norm 0.2878(0.3674) | Total Time 10.00(10.00)\n",
      "Iter 3117 | Time 31.1292(29.4712) | Bit/dim 1.1082(1.1088) | Xent 0.0317(0.0395) | Loss 1.1240(1.1286) | Error 0.0110(0.0123) Steps 416(418.94) | Grad Norm 0.1731(0.3615) | Total Time 10.00(10.00)\n",
      "Iter 3118 | Time 30.2790(29.4955) | Bit/dim 1.1103(1.1089) | Xent 0.0412(0.0395) | Loss 1.1309(1.1286) | Error 0.0131(0.0123) Steps 422(419.03) | Grad Norm 0.2406(0.3579) | Total Time 10.00(10.00)\n",
      "Iter 3119 | Time 30.7634(29.5335) | Bit/dim 1.1089(1.1089) | Xent 0.0420(0.0396) | Loss 1.1299(1.1287) | Error 0.0128(0.0123) Steps 428(419.30) | Grad Norm 0.4150(0.3596) | Total Time 10.00(10.00)\n",
      "Iter 3120 | Time 31.3642(29.5884) | Bit/dim 1.1082(1.1088) | Xent 0.0388(0.0396) | Loss 1.1276(1.1286) | Error 0.0121(0.0123) Steps 422(419.38) | Grad Norm 0.2515(0.3564) | Total Time 10.00(10.00)\n",
      "Iter 3121 | Time 30.9426(29.6291) | Bit/dim 1.1085(1.1088) | Xent 0.0422(0.0396) | Loss 1.1296(1.1287) | Error 0.0105(0.0123) Steps 428(419.64) | Grad Norm 0.3437(0.3560) | Total Time 10.00(10.00)\n",
      "Iter 3122 | Time 30.9697(29.6693) | Bit/dim 1.1040(1.1087) | Xent 0.0477(0.0399) | Loss 1.1279(1.1286) | Error 0.0146(0.0124) Steps 428(419.89) | Grad Norm 0.4073(0.3575) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0446 | Time 16.7131, Epoch Time 243.2235(234.6393), Bit/dim 1.1029(best: 1.1021), Xent 0.0283, Loss 1.1170, Error 0.0101(best: 0.0085)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3123 | Time 32.1941(29.7450) | Bit/dim 1.1117(1.1088) | Xent 0.0343(0.0397) | Loss 1.1289(1.1286) | Error 0.0111(0.0123) Steps 422(419.95) | Grad Norm 0.6033(0.3649) | Total Time 10.00(10.00)\n",
      "Iter 3124 | Time 29.5032(29.7378) | Bit/dim 1.1031(1.1086) | Xent 0.0349(0.0396) | Loss 1.1205(1.1284) | Error 0.0100(0.0123) Steps 422(420.01) | Grad Norm 0.2845(0.3625) | Total Time 10.00(10.00)\n",
      "Iter 3125 | Time 31.7873(29.7992) | Bit/dim 1.1121(1.1087) | Xent 0.0396(0.0396) | Loss 1.1319(1.1285) | Error 0.0111(0.0122) Steps 422(420.07) | Grad Norm 0.4113(0.3640) | Total Time 10.00(10.00)\n",
      "Iter 3126 | Time 29.9756(29.8045) | Bit/dim 1.1057(1.1086) | Xent 0.0392(0.0396) | Loss 1.1252(1.1284) | Error 0.0116(0.0122) Steps 422(420.13) | Grad Norm 0.3363(0.3631) | Total Time 10.00(10.00)\n",
      "Iter 3127 | Time 30.5275(29.8262) | Bit/dim 1.1075(1.1086) | Xent 0.0455(0.0397) | Loss 1.1303(1.1285) | Error 0.0119(0.0122) Steps 416(420.01) | Grad Norm 0.5407(0.3684) | Total Time 10.00(10.00)\n",
      "Iter 3128 | Time 29.3999(29.8134) | Bit/dim 1.1086(1.1086) | Xent 0.0411(0.0398) | Loss 1.1291(1.1285) | Error 0.0121(0.0122) Steps 422(420.07) | Grad Norm 0.2894(0.3661) | Total Time 10.00(10.00)\n",
      "Iter 3129 | Time 30.8105(29.8433) | Bit/dim 1.1103(1.1086) | Xent 0.0410(0.0398) | Loss 1.1308(1.1286) | Error 0.0134(0.0122) Steps 422(420.12) | Grad Norm 0.2006(0.3611) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0447 | Time 16.2567, Epoch Time 243.0861(234.8927), Bit/dim 1.1027(best: 1.1021), Xent 0.0287, Loss 1.1170, Error 0.0091(best: 0.0085)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3130 | Time 28.9827(29.8175) | Bit/dim 1.1121(1.1087) | Xent 0.0365(0.0397) | Loss 1.1304(1.1286) | Error 0.0122(0.0122) Steps 416(420.00) | Grad Norm 0.4678(0.3643) | Total Time 10.00(10.00)\n",
      "Iter 3131 | Time 29.2256(29.7998) | Bit/dim 1.1100(1.1088) | Xent 0.0424(0.0398) | Loss 1.1312(1.1287) | Error 0.0128(0.0122) Steps 422(420.06) | Grad Norm 0.3017(0.3624) | Total Time 10.00(10.00)\n",
      "Iter 3132 | Time 28.3525(29.7564) | Bit/dim 1.1048(1.1087) | Xent 0.0337(0.0396) | Loss 1.1217(1.1285) | Error 0.0108(0.0122) Steps 416(419.94) | Grad Norm 0.2454(0.3589) | Total Time 10.00(10.00)\n",
      "Iter 3133 | Time 29.6814(29.7541) | Bit/dim 1.1063(1.1086) | Xent 0.0402(0.0396) | Loss 1.1263(1.1284) | Error 0.0138(0.0122) Steps 416(419.82) | Grad Norm 0.4390(0.3613) | Total Time 10.00(10.00)\n",
      "Iter 3134 | Time 31.2131(29.7979) | Bit/dim 1.1052(1.1085) | Xent 0.0381(0.0396) | Loss 1.1243(1.1283) | Error 0.0122(0.0122) Steps 428(420.07) | Grad Norm 0.2340(0.3575) | Total Time 10.00(10.00)\n",
      "Iter 3135 | Time 29.0578(29.7757) | Bit/dim 1.1073(1.1085) | Xent 0.0437(0.0397) | Loss 1.1291(1.1283) | Error 0.0139(0.0123) Steps 416(419.94) | Grad Norm 0.2375(0.3539) | Total Time 10.00(10.00)\n",
      "Iter 3136 | Time 29.2324(29.7594) | Bit/dim 1.1112(1.1085) | Xent 0.0385(0.0397) | Loss 1.1305(1.1284) | Error 0.0136(0.0123) Steps 422(420.01) | Grad Norm 0.3625(0.3542) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0448 | Time 16.3733, Epoch Time 234.6830(234.8864), Bit/dim 1.1025(best: 1.1021), Xent 0.0289, Loss 1.1169, Error 0.0093(best: 0.0085)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3137 | Time 28.7300(29.7285) | Bit/dim 1.1094(1.1086) | Xent 0.0383(0.0396) | Loss 1.1285(1.1284) | Error 0.0120(0.0123) Steps 416(419.89) | Grad Norm 0.2267(0.3503) | Total Time 10.00(10.00)\n",
      "Iter 3138 | Time 29.1253(29.7104) | Bit/dim 1.1043(1.1084) | Xent 0.0332(0.0394) | Loss 1.1209(1.1282) | Error 0.0096(0.0122) Steps 416(419.77) | Grad Norm 0.5257(0.3556) | Total Time 10.00(10.00)\n",
      "Iter 3139 | Time 30.5100(29.7344) | Bit/dim 1.1081(1.1084) | Xent 0.0389(0.0394) | Loss 1.1275(1.1281) | Error 0.0119(0.0122) Steps 422(419.84) | Grad Norm 0.2736(0.3531) | Total Time 10.00(10.00)\n",
      "Iter 3140 | Time 30.4765(29.7566) | Bit/dim 1.1116(1.1085) | Xent 0.0396(0.0394) | Loss 1.1314(1.1282) | Error 0.0138(0.0123) Steps 416(419.72) | Grad Norm 0.4170(0.3551) | Total Time 10.00(10.00)\n",
      "Iter 3141 | Time 28.7781(29.7273) | Bit/dim 1.1091(1.1085) | Xent 0.0441(0.0396) | Loss 1.1312(1.1283) | Error 0.0138(0.0123) Steps 416(419.61) | Grad Norm 0.2984(0.3534) | Total Time 10.00(10.00)\n",
      "Iter 3142 | Time 30.6628(29.7554) | Bit/dim 1.1095(1.1086) | Xent 0.0390(0.0396) | Loss 1.1290(1.1283) | Error 0.0131(0.0123) Steps 416(419.50) | Grad Norm 0.3029(0.3518) | Total Time 10.00(10.00)\n",
      "Iter 3143 | Time 31.6568(29.8124) | Bit/dim 1.1107(1.1086) | Xent 0.0368(0.0395) | Loss 1.1290(1.1284) | Error 0.0126(0.0124) Steps 422(419.58) | Grad Norm 0.2851(0.3498) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0449 | Time 16.7286, Epoch Time 239.3096(235.0191), Bit/dim 1.1027(best: 1.1021), Xent 0.0270, Loss 1.1163, Error 0.0096(best: 0.0085)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3144 | Time 30.8354(29.8431) | Bit/dim 1.1140(1.1088) | Xent 0.0366(0.0394) | Loss 1.1323(1.1285) | Error 0.0114(0.0123) Steps 428(419.83) | Grad Norm 0.2481(0.3468) | Total Time 10.00(10.00)\n",
      "Iter 3145 | Time 30.5516(29.8643) | Bit/dim 1.1107(1.1088) | Xent 0.0389(0.0394) | Loss 1.1302(1.1285) | Error 0.0121(0.0123) Steps 428(420.07) | Grad Norm 0.3700(0.3475) | Total Time 10.00(10.00)\n",
      "Iter 3146 | Time 31.8322(29.9234) | Bit/dim 1.1079(1.1088) | Xent 0.0374(0.0393) | Loss 1.1266(1.1285) | Error 0.0120(0.0123) Steps 434(420.49) | Grad Norm 0.2764(0.3454) | Total Time 10.00(10.00)\n",
      "Iter 3147 | Time 30.4336(29.9387) | Bit/dim 1.1027(1.1086) | Xent 0.0373(0.0393) | Loss 1.1213(1.1283) | Error 0.0115(0.0123) Steps 416(420.36) | Grad Norm 0.1808(0.3404) | Total Time 10.00(10.00)\n",
      "Iter 3148 | Time 31.4867(29.9851) | Bit/dim 1.1104(1.1087) | Xent 0.0379(0.0392) | Loss 1.1294(1.1283) | Error 0.0119(0.0123) Steps 428(420.59) | Grad Norm 0.3246(0.3399) | Total Time 10.00(10.00)\n",
      "Iter 3149 | Time 30.2396(29.9928) | Bit/dim 1.1036(1.1085) | Xent 0.0408(0.0393) | Loss 1.1240(1.1282) | Error 0.0130(0.0123) Steps 422(420.63) | Grad Norm 0.3369(0.3399) | Total Time 10.00(10.00)\n",
      "Iter 3150 | Time 31.2304(30.0299) | Bit/dim 1.1097(1.1086) | Xent 0.0387(0.0392) | Loss 1.1291(1.1282) | Error 0.0119(0.0123) Steps 428(420.85) | Grad Norm 0.5079(0.3449) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0450 | Time 16.5995, Epoch Time 245.9586(235.3473), Bit/dim 1.1024(best: 1.1021), Xent 0.0257, Loss 1.1153, Error 0.0091(best: 0.0085)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3151 | Time 31.2577(30.0667) | Bit/dim 1.1084(1.1086) | Xent 0.0389(0.0392) | Loss 1.1278(1.1282) | Error 0.0115(0.0123) Steps 416(420.70) | Grad Norm 0.2005(0.3406) | Total Time 10.00(10.00)\n",
      "Iter 3152 | Time 30.6730(30.0849) | Bit/dim 1.1088(1.1086) | Xent 0.0471(0.0395) | Loss 1.1323(1.1283) | Error 0.0156(0.0124) Steps 422(420.74) | Grad Norm 0.3749(0.3416) | Total Time 10.00(10.00)\n",
      "Iter 3153 | Time 30.4451(30.0957) | Bit/dim 1.1091(1.1086) | Xent 0.0386(0.0394) | Loss 1.1283(1.1283) | Error 0.0119(0.0123) Steps 416(420.60) | Grad Norm 0.2771(0.3397) | Total Time 10.00(10.00)\n",
      "Iter 3154 | Time 30.3106(30.1022) | Bit/dim 1.1075(1.1086) | Xent 0.0364(0.0393) | Loss 1.1257(1.1282) | Error 0.0130(0.0124) Steps 422(420.64) | Grad Norm 0.2459(0.3368) | Total Time 10.00(10.00)\n",
      "Iter 3155 | Time 29.4395(30.0823) | Bit/dim 1.1086(1.1086) | Xent 0.0423(0.0394) | Loss 1.1298(1.1283) | Error 0.0124(0.0124) Steps 422(420.68) | Grad Norm 0.2630(0.3346) | Total Time 10.00(10.00)\n",
      "Iter 3156 | Time 29.6634(30.0697) | Bit/dim 1.1104(1.1086) | Xent 0.0350(0.0393) | Loss 1.1279(1.1283) | Error 0.0114(0.0123) Steps 422(420.72) | Grad Norm 0.2357(0.3317) | Total Time 10.00(10.00)\n",
      "Iter 3157 | Time 31.4415(30.1109) | Bit/dim 1.1101(1.1087) | Xent 0.0455(0.0395) | Loss 1.1328(1.1284) | Error 0.0145(0.0124) Steps 422(420.76) | Grad Norm 0.2194(0.3283) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0451 | Time 16.7824, Epoch Time 242.4092(235.5592), Bit/dim 1.1028(best: 1.1021), Xent 0.0292, Loss 1.1173, Error 0.0101(best: 0.0085)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3158 | Time 30.4198(30.1201) | Bit/dim 1.1085(1.1087) | Xent 0.0419(0.0396) | Loss 1.1295(1.1284) | Error 0.0122(0.0124) Steps 428(420.98) | Grad Norm 0.3215(0.3281) | Total Time 10.00(10.00)\n",
      "Iter 3159 | Time 29.3363(30.0966) | Bit/dim 1.1058(1.1086) | Xent 0.0372(0.0395) | Loss 1.1244(1.1283) | Error 0.0120(0.0124) Steps 422(421.01) | Grad Norm 0.2794(0.3266) | Total Time 10.00(10.00)\n",
      "Iter 3160 | Time 29.5479(30.0802) | Bit/dim 1.1040(1.1084) | Xent 0.0466(0.0397) | Loss 1.1273(1.1283) | Error 0.0131(0.0124) Steps 422(421.04) | Grad Norm 0.1996(0.3228) | Total Time 10.00(10.00)\n",
      "Iter 3161 | Time 31.9364(30.1359) | Bit/dim 1.1104(1.1085) | Xent 0.0382(0.0397) | Loss 1.1295(1.1283) | Error 0.0109(0.0124) Steps 434(421.43) | Grad Norm 0.4657(0.3271) | Total Time 10.00(10.00)\n",
      "Iter 3162 | Time 31.2382(30.1689) | Bit/dim 1.1096(1.1085) | Xent 0.0366(0.0396) | Loss 1.1279(1.1283) | Error 0.0109(0.0123) Steps 428(421.63) | Grad Norm 0.2680(0.3253) | Total Time 10.00(10.00)\n",
      "Iter 3163 | Time 30.9873(30.1935) | Bit/dim 1.1088(1.1085) | Xent 0.0405(0.0396) | Loss 1.1291(1.1283) | Error 0.0128(0.0123) Steps 434(422.00) | Grad Norm 0.2877(0.3242) | Total Time 10.00(10.00)\n",
      "Iter 3164 | Time 30.1883(30.1933) | Bit/dim 1.1104(1.1086) | Xent 0.0360(0.0395) | Loss 1.1284(1.1283) | Error 0.0119(0.0123) Steps 428(422.18) | Grad Norm 0.3241(0.3242) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0452 | Time 16.6499, Epoch Time 242.6719(235.7725), Bit/dim 1.1019(best: 1.1021), Xent 0.0310, Loss 1.1174, Error 0.0096(best: 0.0085)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3165 | Time 31.4555(30.2312) | Bit/dim 1.1091(1.1086) | Xent 0.0424(0.0396) | Loss 1.1303(1.1284) | Error 0.0141(0.0124) Steps 434(422.53) | Grad Norm 0.2080(0.3207) | Total Time 10.00(10.00)\n",
      "Iter 3166 | Time 29.9299(30.2221) | Bit/dim 1.1086(1.1086) | Xent 0.0356(0.0395) | Loss 1.1264(1.1283) | Error 0.0118(0.0123) Steps 422(422.52) | Grad Norm 0.2945(0.3199) | Total Time 10.00(10.00)\n",
      "Iter 3167 | Time 29.5655(30.2024) | Bit/dim 1.1098(1.1086) | Xent 0.0398(0.0395) | Loss 1.1297(1.1284) | Error 0.0122(0.0123) Steps 428(422.68) | Grad Norm 0.2002(0.3163) | Total Time 10.00(10.00)\n",
      "Iter 3168 | Time 30.9565(30.2251) | Bit/dim 1.1110(1.1087) | Xent 0.0371(0.0394) | Loss 1.1296(1.1284) | Error 0.0109(0.0123) Steps 422(422.66) | Grad Norm 0.5798(0.3242) | Total Time 10.00(10.00)\n",
      "Iter 3169 | Time 31.3487(30.2588) | Bit/dim 1.1101(1.1087) | Xent 0.0452(0.0396) | Loss 1.1327(1.1285) | Error 0.0138(0.0123) Steps 428(422.82) | Grad Norm 0.2286(0.3214) | Total Time 10.00(10.00)\n",
      "Iter 3170 | Time 29.2963(30.2299) | Bit/dim 1.1076(1.1087) | Xent 0.0326(0.0394) | Loss 1.1239(1.1284) | Error 0.0095(0.0123) Steps 422(422.79) | Grad Norm 0.2607(0.3196) | Total Time 10.00(10.00)\n",
      "Iter 3171 | Time 30.4636(30.2369) | Bit/dim 1.1051(1.1086) | Xent 0.0371(0.0393) | Loss 1.1237(1.1282) | Error 0.0116(0.0122) Steps 428(422.95) | Grad Norm 0.3552(0.3206) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0453 | Time 16.5359, Epoch Time 242.0790(235.9617), Bit/dim 1.1024(best: 1.1019), Xent 0.0268, Loss 1.1158, Error 0.0088(best: 0.0085)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3172 | Time 31.8284(30.2847) | Bit/dim 1.1073(1.1086) | Xent 0.0366(0.0392) | Loss 1.1256(1.1282) | Error 0.0096(0.0122) Steps 416(422.74) | Grad Norm 0.2172(0.3175) | Total Time 10.00(10.00)\n",
      "Iter 3173 | Time 30.8890(30.3028) | Bit/dim 1.1101(1.1086) | Xent 0.0391(0.0392) | Loss 1.1297(1.1282) | Error 0.0129(0.0122) Steps 428(422.90) | Grad Norm 0.2523(0.3156) | Total Time 10.00(10.00)\n",
      "Iter 3174 | Time 29.6930(30.2845) | Bit/dim 1.1044(1.1085) | Xent 0.0352(0.0391) | Loss 1.1220(1.1280) | Error 0.0111(0.0122) Steps 422(422.87) | Grad Norm 0.2315(0.3130) | Total Time 10.00(10.00)\n",
      "Iter 3175 | Time 29.8435(30.2713) | Bit/dim 1.1083(1.1085) | Xent 0.0412(0.0392) | Loss 1.1289(1.1281) | Error 0.0142(0.0122) Steps 416(422.67) | Grad Norm 0.3453(0.3140) | Total Time 10.00(10.00)\n",
      "Iter 3176 | Time 29.1258(30.2369) | Bit/dim 1.1073(1.1084) | Xent 0.0378(0.0391) | Loss 1.1262(1.1280) | Error 0.0116(0.0122) Steps 422(422.65) | Grad Norm 0.5138(0.3200) | Total Time 10.00(10.00)\n",
      "Iter 3177 | Time 29.7366(30.2219) | Bit/dim 1.1101(1.1085) | Xent 0.0365(0.0390) | Loss 1.1283(1.1280) | Error 0.0120(0.0122) Steps 416(422.45) | Grad Norm 0.2598(0.3182) | Total Time 10.00(10.00)\n",
      "Iter 3178 | Time 30.4456(30.2286) | Bit/dim 1.1110(1.1086) | Xent 0.0427(0.0391) | Loss 1.1323(1.1281) | Error 0.0131(0.0122) Steps 422(422.43) | Grad Norm 0.2436(0.3160) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0454 | Time 16.7760, Epoch Time 240.6497(236.1024), Bit/dim 1.1025(best: 1.1019), Xent 0.0293, Loss 1.1172, Error 0.0101(best: 0.0085)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3179 | Time 29.9537(30.2204) | Bit/dim 1.1052(1.1085) | Xent 0.0320(0.0389) | Loss 1.1212(1.1279) | Error 0.0102(0.0122) Steps 422(422.42) | Grad Norm 0.1946(0.3123) | Total Time 10.00(10.00)\n",
      "Iter 3180 | Time 29.2309(30.1907) | Bit/dim 1.1114(1.1086) | Xent 0.0373(0.0389) | Loss 1.1300(1.1280) | Error 0.0119(0.0122) Steps 422(422.41) | Grad Norm 0.2875(0.3116) | Total Time 10.00(10.00)\n",
      "Iter 3181 | Time 28.8719(30.1511) | Bit/dim 1.1101(1.1086) | Xent 0.0438(0.0390) | Loss 1.1320(1.1281) | Error 0.0138(0.0122) Steps 428(422.58) | Grad Norm 0.2544(0.3099) | Total Time 10.00(10.00)\n",
      "Iter 3182 | Time 29.1518(30.1211) | Bit/dim 1.1092(1.1086) | Xent 0.0430(0.0391) | Loss 1.1308(1.1282) | Error 0.0136(0.0122) Steps 422(422.56) | Grad Norm 0.3164(0.3101) | Total Time 10.00(10.00)\n",
      "Iter 3183 | Time 30.1201(30.1211) | Bit/dim 1.1080(1.1086) | Xent 0.0369(0.0391) | Loss 1.1265(1.1281) | Error 0.0112(0.0122) Steps 434(422.90) | Grad Norm 0.3703(0.3119) | Total Time 10.00(10.00)\n",
      "Iter 3184 | Time 30.2007(30.1235) | Bit/dim 1.1059(1.1085) | Xent 0.0439(0.0392) | Loss 1.1278(1.1281) | Error 0.0124(0.0122) Steps 422(422.88) | Grad Norm 0.3982(0.3145) | Total Time 10.00(10.00)\n",
      "Iter 3185 | Time 29.4878(30.1044) | Bit/dim 1.1068(1.1085) | Xent 0.0415(0.0393) | Loss 1.1276(1.1281) | Error 0.0124(0.0122) Steps 422(422.85) | Grad Norm 0.3192(0.3146) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0455 | Time 16.5612, Epoch Time 235.8089(236.0936), Bit/dim 1.1021(best: 1.1019), Xent 0.0289, Loss 1.1166, Error 0.0102(best: 0.0085)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3186 | Time 30.2876(30.1099) | Bit/dim 1.1062(1.1084) | Xent 0.0400(0.0393) | Loss 1.1261(1.1281) | Error 0.0131(0.0122) Steps 428(423.00) | Grad Norm 0.2118(0.3115) | Total Time 10.00(10.00)\n",
      "Iter 3187 | Time 29.4403(30.0898) | Bit/dim 1.1082(1.1084) | Xent 0.0347(0.0392) | Loss 1.1255(1.1280) | Error 0.0090(0.0122) Steps 422(422.97) | Grad Norm 0.2388(0.3093) | Total Time 10.00(10.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_drop.py --data mnist --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 8000 --test_batch_size 5000 --save ../experiments_published/cnf_conditional_8K_drop_0_5_run1 --resume ../experiments_published/cnf_conditional_8K_drop_0_5_run1/epoch_365_checkpt.pth --seed 0 --conditional True --controlled_tol True --train_mode semisup --lr 0.0001 --warmup_iters 113 --atol 1e-4  --rtol 1e-4 --weight_y 0.5 --dropout_rate 0.5\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
