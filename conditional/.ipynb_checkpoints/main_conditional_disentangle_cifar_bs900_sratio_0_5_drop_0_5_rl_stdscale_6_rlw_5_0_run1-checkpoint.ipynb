{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0,1,2,3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_rl.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "GATES = [\"cnn1\", \"cnn2\", \"rnn\"]\n",
      "\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--gate', type=str, default='cnn1', choices=GATES)\n",
      "parser.add_argument('--scale', type=float, default=1.0)\n",
      "parser.add_argument('--scale_fac', type=float, default=1.0)\n",
      "parser.add_argument('--scale_std', type=float, default=1.0)\n",
      "parser.add_argument('--eta', default=0.1, type=float,\n",
      "                        help='tuning parameter that allows us to trade-off the competing goals of' \n",
      "                                'minimizing the prediction loss and maximizing the gate rewards ')\n",
      "parser.add_argument('--rl-weight', default=0.01, type=float,\n",
      "                        help='rl weight')\n",
      "\n",
      "parser.add_argument('--gamma', default=0.99, type=float,\n",
      "                        help='discount factor, default: (0.99)')\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "import lib.odenvp_conditional_rl as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol, \"scale\": args.scale, \"scale_fac\": args.scale_fac, \"scale_std\": args.scale_std, \"gate\": args.gate},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            # re-weight the gate rewards\n",
      "            normalized_eta = args.eta / len(logp_actions)\n",
      "            \n",
      "            # collect cumulative future rewards\n",
      "            R = - loss\n",
      "            cum_rewards = []\n",
      "            for r in nfe[::-1]:\n",
      "                R = -normalized_eta * r.view(-1,1) + args.gamma * R\n",
      "                cum_rewards.insert(0,R)\n",
      "            \n",
      "            # apply REINFORCE\n",
      "            rl_loss = 0\n",
      "            for lpa, r in zip(logp_actions, cum_rewards):\n",
      "                rl_loss = rl_loss - lpa.view(-1,1) * args.rl_weight * r\n",
      "                \n",
      "            loss = loss + rl_loss.mean()\n",
      "            \n",
      "            loss.backward()\n",
      "            \n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "            \n",
      "            for idx in range(len(model.module.transforms)):\n",
      "                for layer in model.module.transforms[idx].chain:\n",
      "                    if hasattr(layer, 'atol'):\n",
      "                        layer.odefunc.after_odeint()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'train': atol[tol_indx].mean()}, itr)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'train': rtol[tol_indx].mean()}, itr)\n",
      "                    \n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples, atol, rtol, logp_actions, nfe = model(fixed_z, reverse=True)\n",
      "            generated_samples = generated_samples.view(-1, *data_shape)\n",
      "            for tol_indx in range(len(atol)):\n",
      "                writer.add_scalars('atol_gen_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                writer.add_scalars('rtol_gen_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, condition_ratio=0.5, conditional=True, controlled_tol=False, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, eta=0.1, gamma=0.99, gate='cnn1', imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume='../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_6_run1/epoch_40_checkpt.pth', rl_weight=0.01, rtol=1e-05, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_6_run1', scale=1.0, scale_fac=1.0, scale_std=6.0, seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1450732\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc_rl.py:291: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Iter 2210 | Time 17.1438(17.3653) | Bit/dim 3.8270(3.8402) | Xent 1.0362(1.0228) | Loss 9.9634(10.5749) | Error 0.3500(0.3634) Steps 0(0.00) | Grad Norm 7.5267(9.6875) | Total Time 0.00(0.00)\n",
      "Iter 2220 | Time 18.4146(17.2962) | Bit/dim 3.8419(3.8384) | Xent 1.0098(1.0185) | Loss 9.9499(10.3752) | Error 0.3611(0.3636) Steps 0(0.00) | Grad Norm 8.7056(9.5784) | Total Time 0.00(0.00)\n",
      "Iter 2230 | Time 18.8209(17.2046) | Bit/dim 3.8137(3.8362) | Xent 0.9914(1.0136) | Loss 10.0715(10.2432) | Error 0.3667(0.3612) Steps 0(0.00) | Grad Norm 12.1230(9.4685) | Total Time 0.00(0.00)\n",
      "Iter 2240 | Time 17.5164(17.2308) | Bit/dim 3.8220(3.8347) | Xent 0.9656(1.0103) | Loss 9.8159(10.1461) | Error 0.3511(0.3615) Steps 0(0.00) | Grad Norm 7.2555(9.3721) | Total Time 0.00(0.00)\n",
      "Iter 2250 | Time 16.2931(17.2268) | Bit/dim 3.8444(3.8348) | Xent 1.0729(1.0058) | Loss 9.8551(10.0634) | Error 0.3822(0.3606) Steps 0(0.00) | Grad Norm 15.9099(10.2563) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0041 | Time 93.3879, Epoch Time 1076.5607(949.7084), Bit/dim 3.8363(best: inf), Xent 0.9813, Loss 4.3270, Error 0.3446(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2260 | Time 16.2000(17.2381) | Bit/dim 3.8123(3.8322) | Xent 0.9690(1.0079) | Loss 9.6983(10.6321) | Error 0.3489(0.3603) Steps 0(0.00) | Grad Norm 8.5690(10.2056) | Total Time 0.00(0.00)\n",
      "Iter 2270 | Time 15.9474(17.1828) | Bit/dim 3.8271(3.8291) | Xent 1.0140(1.0034) | Loss 9.8854(10.4297) | Error 0.3567(0.3588) Steps 0(0.00) | Grad Norm 10.2103(9.8760) | Total Time 0.00(0.00)\n",
      "Iter 2280 | Time 18.6151(17.2617) | Bit/dim 3.8154(3.8297) | Xent 0.9678(0.9966) | Loss 9.9194(10.2659) | Error 0.3400(0.3556) Steps 0(0.00) | Grad Norm 5.8280(9.8389) | Total Time 0.00(0.00)\n",
      "Iter 2290 | Time 16.8114(17.2290) | Bit/dim 3.8393(3.8294) | Xent 0.9686(0.9965) | Loss 9.9043(10.1445) | Error 0.3489(0.3558) Steps 0(0.00) | Grad Norm 13.9680(9.8385) | Total Time 0.00(0.00)\n",
      "Iter 2300 | Time 18.2880(17.1843) | Bit/dim 3.7978(3.8273) | Xent 0.9756(1.0003) | Loss 9.8774(10.0603) | Error 0.3378(0.3562) Steps 0(0.00) | Grad Norm 10.4708(9.8858) | Total Time 0.00(0.00)\n",
      "Iter 2310 | Time 18.4267(17.2025) | Bit/dim 3.8322(3.8265) | Xent 1.0211(1.0010) | Loss 10.0983(10.0108) | Error 0.3778(0.3577) Steps 0(0.00) | Grad Norm 9.8170(10.3385) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0042 | Time 88.4643, Epoch Time 1051.9546(952.7758), Bit/dim 3.8206(best: 3.8363), Xent 0.9777, Loss 4.3095, Error 0.3482(best: 0.3446)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2320 | Time 17.9439(17.2711) | Bit/dim 3.7863(3.8243) | Xent 1.0604(0.9997) | Loss 9.7725(10.5493) | Error 0.3689(0.3560) Steps 0(0.00) | Grad Norm 12.9502(10.4806) | Total Time 0.00(0.00)\n",
      "Iter 2330 | Time 16.8577(17.2593) | Bit/dim 3.8145(3.8258) | Xent 0.9664(0.9908) | Loss 9.8686(10.3558) | Error 0.3411(0.3540) Steps 0(0.00) | Grad Norm 7.2122(10.1963) | Total Time 0.00(0.00)\n",
      "Iter 2340 | Time 16.9847(17.1666) | Bit/dim 3.7857(3.8229) | Xent 1.0101(0.9875) | Loss 9.7211(10.1989) | Error 0.3622(0.3524) Steps 0(0.00) | Grad Norm 11.5887(10.1178) | Total Time 0.00(0.00)\n",
      "Iter 2350 | Time 16.9358(17.2815) | Bit/dim 3.7923(3.8202) | Xent 1.0215(1.0002) | Loss 9.6578(10.1005) | Error 0.3500(0.3562) Steps 0(0.00) | Grad Norm 11.7108(11.2767) | Total Time 0.00(0.00)\n",
      "Iter 2360 | Time 18.2778(17.2678) | Bit/dim 3.8288(3.8217) | Xent 0.9818(1.0079) | Loss 10.0190(10.0479) | Error 0.3544(0.3589) Steps 0(0.00) | Grad Norm 7.8739(10.9339) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0043 | Time 89.1187, Epoch Time 1055.7014(955.8636), Bit/dim 3.8182(best: 3.8206), Xent 0.9672, Loss 4.3018, Error 0.3421(best: 0.3446)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2370 | Time 16.8712(17.1248) | Bit/dim 3.8268(3.8203) | Xent 0.9702(1.0046) | Loss 9.8617(10.6601) | Error 0.3533(0.3592) Steps 0(0.00) | Grad Norm 12.6225(10.7123) | Total Time 0.00(0.00)\n",
      "Iter 2380 | Time 17.2192(17.2501) | Bit/dim 3.8076(3.8215) | Xent 0.9830(0.9941) | Loss 9.7558(10.4407) | Error 0.3600(0.3563) Steps 0(0.00) | Grad Norm 8.3062(10.1194) | Total Time 0.00(0.00)\n",
      "Iter 2390 | Time 17.4834(17.3254) | Bit/dim 3.8259(3.8201) | Xent 0.9510(0.9874) | Loss 9.7918(10.2953) | Error 0.3444(0.3535) Steps 0(0.00) | Grad Norm 15.9595(10.5982) | Total Time 0.00(0.00)\n",
      "Iter 2400 | Time 17.2733(17.2799) | Bit/dim 3.8096(3.8197) | Xent 0.9435(0.9844) | Loss 9.7765(10.1763) | Error 0.3300(0.3508) Steps 0(0.00) | Grad Norm 16.7527(10.5264) | Total Time 0.00(0.00)\n",
      "Iter 2410 | Time 16.5905(17.2675) | Bit/dim 3.7983(3.8145) | Xent 1.0000(0.9834) | Loss 9.8370(10.0652) | Error 0.3678(0.3515) Steps 0(0.00) | Grad Norm 9.9330(10.2360) | Total Time 0.00(0.00)\n",
      "Iter 2420 | Time 18.8817(17.3085) | Bit/dim 3.7906(3.8107) | Xent 0.9665(0.9772) | Loss 9.9621(9.9883) | Error 0.3311(0.3490) Steps 0(0.00) | Grad Norm 7.8311(10.2736) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0044 | Time 87.4812, Epoch Time 1058.3789(958.9390), Bit/dim 3.8062(best: 3.8182), Xent 0.9319, Loss 4.2721, Error 0.3319(best: 0.3421)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2430 | Time 17.1793(17.5672) | Bit/dim 3.8206(3.8084) | Xent 1.0079(0.9696) | Loss 9.7961(10.4683) | Error 0.3856(0.3468) Steps 0(0.00) | Grad Norm 12.1732(10.6500) | Total Time 0.00(0.00)\n",
      "Iter 2440 | Time 17.4591(17.5492) | Bit/dim 3.8013(3.8098) | Xent 0.9816(0.9728) | Loss 9.8973(10.2753) | Error 0.3433(0.3473) Steps 0(0.00) | Grad Norm 10.9319(10.7661) | Total Time 0.00(0.00)\n",
      "Iter 2450 | Time 15.6955(17.4657) | Bit/dim 3.8048(3.8095) | Xent 0.9685(0.9771) | Loss 9.6971(10.1515) | Error 0.3389(0.3482) Steps 0(0.00) | Grad Norm 14.6652(11.3657) | Total Time 0.00(0.00)\n",
      "Iter 2460 | Time 17.8281(17.3710) | Bit/dim 3.7951(3.8083) | Xent 1.0254(0.9807) | Loss 9.7220(10.0671) | Error 0.3589(0.3500) Steps 0(0.00) | Grad Norm 10.7173(10.8846) | Total Time 0.00(0.00)\n",
      "Iter 2470 | Time 16.3338(17.4032) | Bit/dim 3.7842(3.8089) | Xent 0.9680(0.9852) | Loss 9.3123(10.0010) | Error 0.3356(0.3513) Steps 0(0.00) | Grad Norm 9.0666(11.4786) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0045 | Time 90.5354, Epoch Time 1068.7656(962.2338), Bit/dim 3.8007(best: 3.8062), Xent 0.9774, Loss 4.2894, Error 0.3414(best: 0.3319)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2480 | Time 17.7646(17.4002) | Bit/dim 3.7954(3.8059) | Xent 0.9748(0.9803) | Loss 9.9494(10.5624) | Error 0.3400(0.3497) Steps 0(0.00) | Grad Norm 10.6895(11.3572) | Total Time 0.00(0.00)\n",
      "Iter 2490 | Time 17.8501(17.4193) | Bit/dim 3.8111(3.8063) | Xent 0.9136(0.9674) | Loss 9.8251(10.3444) | Error 0.3156(0.3452) Steps 0(0.00) | Grad Norm 4.2077(10.4656) | Total Time 0.00(0.00)\n",
      "Iter 2500 | Time 16.2415(17.2799) | Bit/dim 3.7606(3.8030) | Xent 1.0152(0.9645) | Loss 9.6392(10.1900) | Error 0.3556(0.3432) Steps 0(0.00) | Grad Norm 5.9060(9.9873) | Total Time 0.00(0.00)\n",
      "Iter 2510 | Time 16.3111(17.2726) | Bit/dim 3.8044(3.8029) | Xent 0.9877(0.9646) | Loss 9.4895(10.0784) | Error 0.3411(0.3419) Steps 0(0.00) | Grad Norm 10.9168(10.2406) | Total Time 0.00(0.00)\n",
      "Iter 2520 | Time 17.2706(17.3781) | Bit/dim 3.7560(3.8013) | Xent 1.0413(0.9739) | Loss 9.7448(10.0040) | Error 0.3789(0.3459) Steps 0(0.00) | Grad Norm 12.2387(11.5062) | Total Time 0.00(0.00)\n",
      "Iter 2530 | Time 16.1256(17.3399) | Bit/dim 3.8214(3.8043) | Xent 1.0413(0.9789) | Loss 9.8797(9.9506) | Error 0.3811(0.3489) Steps 0(0.00) | Grad Norm 23.4557(11.6075) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0046 | Time 90.5365, Epoch Time 1063.7849(965.2804), Bit/dim 3.8259(best: 3.8007), Xent 1.0216, Loss 4.3367, Error 0.3677(best: 0.3319)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2540 | Time 16.8600(17.2155) | Bit/dim 3.8205(3.8094) | Xent 0.9600(0.9787) | Loss 9.9081(10.4967) | Error 0.3322(0.3497) Steps 0(0.00) | Grad Norm 7.2128(11.6358) | Total Time 0.00(0.00)\n",
      "Iter 2550 | Time 17.6044(17.2342) | Bit/dim 3.7991(3.8069) | Xent 0.8953(0.9686) | Loss 9.7870(10.3045) | Error 0.3089(0.3459) Steps 0(0.00) | Grad Norm 7.1824(11.6035) | Total Time 0.00(0.00)\n",
      "Iter 2560 | Time 16.8507(17.1433) | Bit/dim 3.8106(3.8040) | Xent 0.9857(0.9665) | Loss 9.7223(10.1523) | Error 0.3489(0.3455) Steps 0(0.00) | Grad Norm 7.7426(10.5642) | Total Time 0.00(0.00)\n",
      "Iter 2570 | Time 16.2833(17.1913) | Bit/dim 3.7657(3.8042) | Xent 0.9621(0.9601) | Loss 9.5713(10.0371) | Error 0.3622(0.3433) Steps 0(0.00) | Grad Norm 7.2131(10.3015) | Total Time 0.00(0.00)\n",
      "Iter 2580 | Time 15.2178(17.2990) | Bit/dim 3.8240(3.8017) | Xent 1.0444(0.9616) | Loss 9.6860(9.9600) | Error 0.3756(0.3436) Steps 0(0.00) | Grad Norm 14.1761(10.1059) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0047 | Time 88.6806, Epoch Time 1052.6748(967.9022), Bit/dim 3.8027(best: 3.8007), Xent 0.9769, Loss 4.2912, Error 0.3504(best: 0.3319)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2590 | Time 19.5222(17.3542) | Bit/dim 3.7901(3.8015) | Xent 0.9746(0.9647) | Loss 9.8780(10.5311) | Error 0.3422(0.3449) Steps 0(0.00) | Grad Norm 19.5979(10.8718) | Total Time 0.00(0.00)\n",
      "Iter 2600 | Time 17.6076(17.2704) | Bit/dim 3.7988(3.8009) | Xent 0.9044(0.9554) | Loss 9.8205(10.3136) | Error 0.3244(0.3414) Steps 0(0.00) | Grad Norm 12.5757(10.6758) | Total Time 0.00(0.00)\n",
      "Iter 2610 | Time 19.1054(17.3833) | Bit/dim 3.7809(3.7954) | Xent 0.9197(0.9454) | Loss 9.6166(10.1520) | Error 0.3444(0.3386) Steps 0(0.00) | Grad Norm 5.7483(9.9130) | Total Time 0.00(0.00)\n",
      "Iter 2620 | Time 18.7720(17.4560) | Bit/dim 3.8474(3.7954) | Xent 0.9331(0.9466) | Loss 9.9563(10.0535) | Error 0.3333(0.3396) Steps 0(0.00) | Grad Norm 9.3152(9.7724) | Total Time 0.00(0.00)\n",
      "Iter 2630 | Time 17.2239(17.4498) | Bit/dim 3.8234(3.7932) | Xent 0.9183(0.9506) | Loss 9.8643(9.9601) | Error 0.3289(0.3419) Steps 0(0.00) | Grad Norm 13.9539(10.6091) | Total Time 0.00(0.00)\n",
      "Iter 2640 | Time 16.5032(17.3595) | Bit/dim 3.8519(3.7929) | Xent 0.9450(0.9442) | Loss 9.7792(9.8921) | Error 0.3533(0.3390) Steps 0(0.00) | Grad Norm 16.0782(10.9261) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0048 | Time 88.8145, Epoch Time 1064.3342(970.7951), Bit/dim 3.7897(best: 3.8007), Xent 0.9141, Loss 4.2468, Error 0.3224(best: 0.3319)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2650 | Time 16.5933(17.2055) | Bit/dim 3.8001(3.7919) | Xent 0.9643(0.9482) | Loss 9.5668(10.4220) | Error 0.3456(0.3386) Steps 0(0.00) | Grad Norm 13.4190(11.1583) | Total Time 0.00(0.00)\n",
      "Iter 2660 | Time 17.6960(17.2066) | Bit/dim 3.7772(3.7879) | Xent 0.8843(0.9387) | Loss 9.6673(10.2326) | Error 0.3178(0.3359) Steps 0(0.00) | Grad Norm 7.7446(10.0867) | Total Time 0.00(0.00)\n",
      "Iter 2670 | Time 17.6991(17.2146) | Bit/dim 3.7740(3.7869) | Xent 0.9119(0.9341) | Loss 9.5621(10.0944) | Error 0.3244(0.3333) Steps 0(0.00) | Grad Norm 8.3545(9.5707) | Total Time 0.00(0.00)\n",
      "Iter 2680 | Time 17.8407(17.2212) | Bit/dim 3.7646(3.7854) | Xent 0.8416(0.9289) | Loss 9.6518(9.9804) | Error 0.3033(0.3313) Steps 0(0.00) | Grad Norm 10.4307(10.0673) | Total Time 0.00(0.00)\n",
      "Iter 2690 | Time 15.8845(17.1406) | Bit/dim 3.8223(3.7851) | Xent 0.9593(0.9274) | Loss 9.8941(9.9099) | Error 0.3333(0.3307) Steps 0(0.00) | Grad Norm 11.2249(10.4308) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0049 | Time 87.9127, Epoch Time 1049.4511(973.1548), Bit/dim 3.7913(best: 3.7897), Xent 0.9066, Loss 4.2446, Error 0.3218(best: 0.3224)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2700 | Time 16.9030(17.1806) | Bit/dim 3.7744(3.7854) | Xent 0.9559(0.9256) | Loss 9.5069(10.4393) | Error 0.3356(0.3299) Steps 0(0.00) | Grad Norm 8.8668(10.6214) | Total Time 0.00(0.00)\n",
      "Iter 2710 | Time 16.9405(17.1767) | Bit/dim 3.8139(3.7847) | Xent 0.9220(0.9184) | Loss 9.7929(10.2365) | Error 0.3256(0.3269) Steps 0(0.00) | Grad Norm 14.5892(10.6374) | Total Time 0.00(0.00)\n",
      "Iter 2720 | Time 16.7622(17.3120) | Bit/dim 3.7647(3.7833) | Xent 0.8813(0.9182) | Loss 9.7531(10.1032) | Error 0.3100(0.3278) Steps 0(0.00) | Grad Norm 7.0923(10.6526) | Total Time 0.00(0.00)\n",
      "Iter 2730 | Time 15.9361(17.3982) | Bit/dim 3.7840(3.7815) | Xent 0.9730(0.9264) | Loss 9.5195(10.0076) | Error 0.3500(0.3307) Steps 0(0.00) | Grad Norm 13.2495(10.7067) | Total Time 0.00(0.00)\n",
      "Iter 2740 | Time 17.4720(17.4101) | Bit/dim 3.7886(3.7845) | Xent 0.9408(0.9326) | Loss 9.8324(9.9296) | Error 0.3411(0.3330) Steps 0(0.00) | Grad Norm 12.9733(11.1316) | Total Time 0.00(0.00)\n",
      "Iter 2750 | Time 18.2190(17.4665) | Bit/dim 3.7652(3.7854) | Xent 0.8946(0.9268) | Loss 9.6959(9.8701) | Error 0.3189(0.3308) Steps 0(0.00) | Grad Norm 6.4863(10.3361) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0050 | Time 88.2521, Epoch Time 1065.9383(975.9383), Bit/dim 3.7840(best: 3.7897), Xent 0.9106, Loss 4.2393, Error 0.3238(best: 0.3218)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2760 | Time 16.3854(17.4494) | Bit/dim 3.7839(3.7825) | Xent 0.9691(0.9195) | Loss 9.6593(10.3833) | Error 0.3589(0.3285) Steps 0(0.00) | Grad Norm 17.1316(10.1803) | Total Time 0.00(0.00)\n",
      "Iter 2770 | Time 15.8607(17.3261) | Bit/dim 3.7754(3.7824) | Xent 0.9052(0.9155) | Loss 9.5189(10.1795) | Error 0.3289(0.3289) Steps 0(0.00) | Grad Norm 5.9207(9.9866) | Total Time 0.00(0.00)\n",
      "Iter 2780 | Time 16.6145(17.2782) | Bit/dim 3.8186(3.7803) | Xent 0.9297(0.9118) | Loss 9.6911(10.0605) | Error 0.3322(0.3267) Steps 0(0.00) | Grad Norm 6.5940(9.5818) | Total Time 0.00(0.00)\n",
      "Iter 2890 | Time 17.2369(17.3177) | Bit/dim 3.7943(3.7708) | Xent 0.9267(0.8942) | Loss 9.4898(9.9985) | Error 0.3311(0.3183) Steps 0(0.00) | Grad Norm 8.9910(9.8601) | Total Time 0.00(0.00)\n",
      "Iter 2900 | Time 18.9679(17.2079) | Bit/dim 3.8079(3.7697) | Xent 0.8918(0.8944) | Loss 9.7124(9.8921) | Error 0.3022(0.3175) Steps 0(0.00) | Grad Norm 13.8391(10.1800) | Total Time 0.00(0.00)\n",
      "Iter 2910 | Time 18.2883(17.2530) | Bit/dim 3.7680(3.7692) | Xent 0.8406(0.8924) | Loss 9.7284(9.8409) | Error 0.3067(0.3169) Steps 0(0.00) | Grad Norm 7.8380(10.2234) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0053 | Time 91.0958, Epoch Time 1057.5856(982.8081), Bit/dim 3.7738(best: 3.7679), Xent 0.8619, Loss 4.2048, Error 0.3052(best: 0.3067)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2920 | Time 18.0224(17.2240) | Bit/dim 3.7537(3.7690) | Xent 0.8660(0.8950) | Loss 9.5278(10.4542) | Error 0.2922(0.3186) Steps 0(0.00) | Grad Norm 8.5439(10.2386) | Total Time 0.00(0.00)\n",
      "Iter 2930 | Time 15.8897(17.0991) | Bit/dim 3.7277(3.7683) | Xent 0.8291(0.8891) | Loss 9.2230(10.2148) | Error 0.3044(0.3162) Steps 0(0.00) | Grad Norm 5.8978(9.9162) | Total Time 0.00(0.00)\n",
      "Iter 2940 | Time 16.4635(17.2844) | Bit/dim 3.7390(3.7673) | Xent 0.8367(0.8792) | Loss 9.5919(10.0549) | Error 0.2889(0.3118) Steps 0(0.00) | Grad Norm 5.9289(9.1335) | Total Time 0.00(0.00)\n",
      "Iter 2950 | Time 16.4027(17.3347) | Bit/dim 3.7755(3.7667) | Xent 0.8648(0.8922) | Loss 9.5603(9.9397) | Error 0.3244(0.3169) Steps 0(0.00) | Grad Norm 8.2691(10.6427) | Total Time 0.00(0.00)\n",
      "Iter 2960 | Time 18.2515(17.3959) | Bit/dim 3.7664(3.7685) | Xent 0.8381(0.8910) | Loss 9.7817(9.8686) | Error 0.2989(0.3172) Steps 0(0.00) | Grad Norm 10.7535(10.5211) | Total Time 0.00(0.00)\n",
      "Iter 2970 | Time 17.4958(17.2586) | Bit/dim 3.7547(3.7667) | Xent 0.8753(0.8897) | Loss 9.6339(9.7955) | Error 0.3000(0.3164) Steps 0(0.00) | Grad Norm 10.0087(10.5166) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0054 | Time 90.3133, Epoch Time 1061.3986(985.1658), Bit/dim 3.7606(best: 3.7679), Xent 0.8537, Loss 4.1874, Error 0.3060(best: 0.3052)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2980 | Time 19.3063(17.5336) | Bit/dim 3.7641(3.7655) | Xent 0.9027(0.8854) | Loss 9.4582(10.3264) | Error 0.3133(0.3140) Steps 0(0.00) | Grad Norm 6.9206(10.0185) | Total Time 0.00(0.00)\n",
      "Iter 2990 | Time 16.5861(17.3639) | Bit/dim 3.7656(3.7663) | Xent 0.8805(0.8804) | Loss 9.7811(10.1450) | Error 0.3256(0.3135) Steps 0(0.00) | Grad Norm 10.9407(9.8584) | Total Time 0.00(0.00)\n",
      "Iter 3000 | Time 17.0846(17.2947) | Bit/dim 3.7479(3.7638) | Xent 0.8450(0.8772) | Loss 9.4837(10.0003) | Error 0.3000(0.3126) Steps 0(0.00) | Grad Norm 4.2315(9.4267) | Total Time 0.00(0.00)\n",
      "Iter 3010 | Time 17.0185(17.3280) | Bit/dim 3.7473(3.7608) | Xent 0.9744(0.8822) | Loss 9.6628(9.9009) | Error 0.3400(0.3149) Steps 0(0.00) | Grad Norm 18.9779(9.9445) | Total Time 0.00(0.00)\n",
      "Iter 3020 | Time 19.0821(17.2955) | Bit/dim 3.7324(3.7589) | Xent 0.8079(0.8821) | Loss 9.4333(9.8075) | Error 0.2911(0.3141) Steps 0(0.00) | Grad Norm 5.5082(9.8708) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0055 | Time 89.2087, Epoch Time 1063.9110(987.5281), Bit/dim 3.7588(best: 3.7606), Xent 0.8714, Loss 4.1945, Error 0.3060(best: 0.3052)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3030 | Time 16.5702(17.2649) | Bit/dim 3.7542(3.7594) | Xent 0.8473(0.8762) | Loss 9.4608(10.4267) | Error 0.2944(0.3114) Steps 0(0.00) | Grad Norm 8.1150(9.7937) | Total Time 0.00(0.00)\n",
      "Iter 3040 | Time 17.0526(17.2705) | Bit/dim 3.7565(3.7563) | Xent 0.8616(0.8726) | Loss 9.5601(10.1989) | Error 0.2978(0.3109) Steps 0(0.00) | Grad Norm 11.2110(9.3991) | Total Time 0.00(0.00)\n",
      "Iter 3050 | Time 17.1453(17.1846) | Bit/dim 3.7548(3.7568) | Xent 0.9072(0.8721) | Loss 9.6594(10.0451) | Error 0.3233(0.3119) Steps 0(0.00) | Grad Norm 14.0469(9.9626) | Total Time 0.00(0.00)\n",
      "Iter 3060 | Time 16.2432(17.1504) | Bit/dim 3.7583(3.7590) | Xent 0.8738(0.8715) | Loss 9.4830(9.9226) | Error 0.2933(0.3129) Steps 0(0.00) | Grad Norm 5.9678(10.1415) | Total Time 0.00(0.00)\n",
      "Iter 3070 | Time 16.3866(17.2182) | Bit/dim 3.7809(3.7584) | Xent 0.8488(0.8626) | Loss 9.6588(9.8579) | Error 0.3044(0.3094) Steps 0(0.00) | Grad Norm 8.7600(9.9937) | Total Time 0.00(0.00)\n",
      "Iter 3080 | Time 19.2655(17.3605) | Bit/dim 3.7342(3.7559) | Xent 0.8581(0.8637) | Loss 9.5329(9.7962) | Error 0.2989(0.3096) Steps 0(0.00) | Grad Norm 12.3652(9.8983) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0056 | Time 88.5126, Epoch Time 1054.8223(989.5470), Bit/dim 3.7548(best: 3.7588), Xent 0.8478, Loss 4.1787, Error 0.2999(best: 0.3052)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3090 | Time 16.6991(17.3397) | Bit/dim 3.7669(3.7580) | Xent 0.8547(0.8791) | Loss 9.5564(10.3271) | Error 0.2978(0.3131) Steps 0(0.00) | Grad Norm 11.2895(11.1903) | Total Time 0.00(0.00)\n",
      "Iter 3100 | Time 18.1929(17.4679) | Bit/dim 3.7514(3.7574) | Xent 0.8215(0.8746) | Loss 9.3623(10.1329) | Error 0.2900(0.3119) Steps 0(0.00) | Grad Norm 8.9472(10.7897) | Total Time 0.00(0.00)\n",
      "Iter 3110 | Time 19.8902(17.6031) | Bit/dim 3.7739(3.7589) | Xent 0.8534(0.8717) | Loss 9.8200(10.0222) | Error 0.3133(0.3112) Steps 0(0.00) | Grad Norm 9.8796(10.3924) | Total Time 0.00(0.00)\n",
      "Iter 3120 | Time 16.5104(17.5975) | Bit/dim 3.7457(3.7572) | Xent 0.9390(0.8676) | Loss 9.4075(9.8913) | Error 0.3433(0.3091) Steps 0(0.00) | Grad Norm 11.0816(9.7894) | Total Time 0.00(0.00)\n",
      "Iter 3130 | Time 17.4740(17.5192) | Bit/dim 3.7434(3.7561) | Xent 0.8964(0.8673) | Loss 9.6157(9.8148) | Error 0.3289(0.3088) Steps 0(0.00) | Grad Norm 12.0549(9.5344) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0057 | Time 89.8869, Epoch Time 1072.0802(992.0230), Bit/dim 3.7512(best: 3.7548), Xent 0.8297, Loss 4.1660, Error 0.2956(best: 0.2999)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3140 | Time 18.6605(17.4071) | Bit/dim 3.7838(3.7539) | Xent 0.8593(0.8604) | Loss 9.6254(10.3970) | Error 0.3033(0.3065) Steps 0(0.00) | Grad Norm 12.2211(9.9562) | Total Time 0.00(0.00)\n",
      "Iter 3150 | Time 18.0204(17.3276) | Bit/dim 3.7365(3.7531) | Xent 0.8592(0.8575) | Loss 9.7636(10.1738) | Error 0.3100(0.3059) Steps 0(0.00) | Grad Norm 14.4315(9.9092) | Total Time 0.00(0.00)\n",
      "Iter 3160 | Time 16.3437(17.2579) | Bit/dim 3.7799(3.7527) | Xent 0.9919(0.8572) | Loss 9.4884(9.9887) | Error 0.3533(0.3059) Steps 0(0.00) | Grad Norm 9.1381(9.5510) | Total Time 0.00(0.00)\n",
      "Iter 3170 | Time 16.8654(17.4111) | Bit/dim 3.7421(3.7515) | Xent 0.8978(0.8629) | Loss 9.5931(9.8948) | Error 0.3367(0.3073) Steps 0(0.00) | Grad Norm 9.7327(9.3505) | Total Time 0.00(0.00)\n",
      "Iter 3180 | Time 17.4423(17.3781) | Bit/dim 3.7631(3.7514) | Xent 0.8630(0.8561) | Loss 9.5491(9.8055) | Error 0.3233(0.3059) Steps 0(0.00) | Grad Norm 11.1338(9.3763) | Total Time 0.00(0.00)\n",
      "Iter 3190 | Time 20.3894(17.5420) | Bit/dim 3.7325(3.7525) | Xent 0.8349(0.8544) | Loss 9.6289(9.7501) | Error 0.2911(0.3048) Steps 0(0.00) | Grad Norm 8.7211(9.4517) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0058 | Time 92.3787, Epoch Time 1069.8026(994.3564), Bit/dim 3.7489(best: 3.7512), Xent 0.8413, Loss 4.1696, Error 0.2985(best: 0.2956)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3200 | Time 17.8044(17.5394) | Bit/dim 3.7441(3.7493) | Xent 0.8176(0.8468) | Loss 9.6807(10.2683) | Error 0.2856(0.3023) Steps 0(0.00) | Grad Norm 8.2093(8.8066) | Total Time 0.00(0.00)\n",
      "Iter 3210 | Time 16.1293(17.4481) | Bit/dim 3.7793(3.7493) | Xent 0.8665(0.8436) | Loss 9.5133(10.0704) | Error 0.3167(0.3006) Steps 0(0.00) | Grad Norm 7.3675(9.2644) | Total Time 0.00(0.00)\n",
      "Iter 3220 | Time 17.8325(17.3865) | Bit/dim 3.7420(3.7499) | Xent 0.8541(0.8367) | Loss 9.6110(9.9427) | Error 0.3156(0.2984) Steps 0(0.00) | Grad Norm 10.5341(9.2698) | Total Time 0.00(0.00)\n",
      "Iter 3230 | Time 17.0743(17.2569) | Bit/dim 3.7270(3.7487) | Xent 0.8026(0.8396) | Loss 9.4827(9.8532) | Error 0.3078(0.3001) Steps 0(0.00) | Grad Norm 5.5885(9.3150) | Total Time 0.00(0.00)\n",
      "Iter 3240 | Time 16.8404(17.1778) | Bit/dim 3.7595(3.7472) | Xent 0.8371(0.8364) | Loss 9.4611(9.7635) | Error 0.3044(0.2989) Steps 0(0.00) | Grad Norm 6.1795(8.5241) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0059 | Time 90.4260, Epoch Time 1056.5901(996.2234), Bit/dim 3.7499(best: 3.7489), Xent 0.9101, Loss 4.2049, Error 0.3183(best: 0.2956)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3250 | Time 17.9508(17.3686) | Bit/dim 3.7223(3.7471) | Xent 1.0623(0.8468) | Loss 9.7749(10.3471) | Error 0.3744(0.3011) Steps 0(0.00) | Grad Norm 19.4000(9.8893) | Total Time 0.00(0.00)\n",
      "Iter 3260 | Time 18.2129(17.5743) | Bit/dim 3.7282(3.7501) | Xent 0.8667(0.8516) | Loss 9.6682(10.1652) | Error 0.3156(0.3031) Steps 0(0.00) | Grad Norm 8.6125(10.3044) | Total Time 0.00(0.00)\n",
      "Iter 3270 | Time 17.4398(17.5555) | Bit/dim 3.7401(3.7482) | Xent 0.7986(0.8429) | Loss 9.4375(9.9982) | Error 0.2778(0.3011) Steps 0(0.00) | Grad Norm 6.6857(9.7078) | Total Time 0.00(0.00)\n",
      "Iter 3280 | Time 18.2833(17.5884) | Bit/dim 3.7358(3.7474) | Xent 0.8902(0.8408) | Loss 9.6576(9.8905) | Error 0.3167(0.3010) Steps 0(0.00) | Grad Norm 10.4610(8.9739) | Total Time 0.00(0.00)\n",
      "Iter 3290 | Time 17.9245(17.5591) | Bit/dim 3.7239(3.7471) | Xent 0.8214(0.8389) | Loss 9.6043(9.7969) | Error 0.2911(0.3007) Steps 0(0.00) | Grad Norm 7.0443(8.6774) | Total Time 0.00(0.00)\n",
      "Iter 3300 | Time 16.9507(17.6096) | Bit/dim 3.7445(3.7474) | Xent 0.8631(0.8389) | Loss 9.4867(9.7252) | Error 0.3167(0.2994) Steps 0(0.00) | Grad Norm 7.8893(8.9535) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0060 | Time 91.4016, Epoch Time 1083.9151(998.8541), Bit/dim 3.7451(best: 3.7489), Xent 0.8170, Loss 4.1536, Error 0.2869(best: 0.2956)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3310 | Time 17.9735(17.6857) | Bit/dim 3.7664(3.7461) | Xent 0.8250(0.8337) | Loss 9.5062(10.2334) | Error 0.3011(0.2979) Steps 0(0.00) | Grad Norm 5.4300(8.8835) | Total Time 0.00(0.00)\n",
      "Iter 3320 | Time 19.7600(17.6554) | Bit/dim 3.7483(3.7413) | Xent 0.8062(0.8283) | Loss 9.6087(10.0413) | Error 0.2711(0.2961) Steps 0(0.00) | Grad Norm 8.2007(8.6270) | Total Time 0.00(0.00)\n",
      "Iter 3330 | Time 17.9569(17.4645) | Bit/dim 3.7379(3.7425) | Xent 0.8594(0.8261) | Loss 9.4309(9.9016) | Error 0.3067(0.2955) Steps 0(0.00) | Grad Norm 13.9657(8.9601) | Total Time 0.00(0.00)\n",
      "Iter 3340 | Time 17.1569(17.5243) | Bit/dim 3.7450(3.7423) | Xent 0.8222(0.8255) | Loss 9.3883(9.8112) | Error 0.2867(0.2944) Steps 0(0.00) | Grad Norm 9.8523(8.7089) | Total Time 0.00(0.00)\n",
      "Iter 3350 | Time 15.5375(17.3878) | Bit/dim 3.7058(3.7373) | Xent 0.8091(0.8227) | Loss 9.3540(9.6938) | Error 0.2811(0.2938) Steps 0(0.00) | Grad Norm 11.6658(8.3851) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0061 | Time 90.4549, Epoch Time 1068.8937(1000.9553), Bit/dim 3.7387(best: 3.7451), Xent 0.8078, Loss 4.1426, Error 0.2838(best: 0.2869)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3360 | Time 17.7771(17.5530) | Bit/dim 3.7143(3.7369) | Xent 0.8866(0.8254) | Loss 9.5523(10.3188) | Error 0.3167(0.2947) Steps 0(0.00) | Grad Norm 12.2968(8.7504) | Total Time 0.00(0.00)\n",
      "Iter 3370 | Time 17.9457(17.5616) | Bit/dim 3.7145(3.7391) | Xent 0.7731(0.8181) | Loss 9.5055(10.1093) | Error 0.2800(0.2922) Steps 0(0.00) | Grad Norm 9.6633(8.9791) | Total Time 0.00(0.00)\n",
      "Iter 3380 | Time 17.2237(17.4786) | Bit/dim 3.7359(3.7421) | Xent 0.7904(0.8207) | Loss 9.2108(9.9553) | Error 0.2789(0.2939) Steps 0(0.00) | Grad Norm 8.0642(9.4764) | Total Time 0.00(0.00)\n",
      "Iter 3390 | Time 16.9947(17.3432) | Bit/dim 3.7419(3.7390) | Xent 0.8606(0.8159) | Loss 9.5155(9.8221) | Error 0.3178(0.2922) Steps 0(0.00) | Grad Norm 5.8463(9.6769) | Total Time 0.00(0.00)\n",
      "Iter 3400 | Time 16.4901(17.1498) | Bit/dim 3.7688(3.7361) | Xent 0.8771(0.8210) | Loss 9.6583(9.7520) | Error 0.3122(0.2931) Steps 0(0.00) | Grad Norm 15.7670(10.0117) | Total Time 0.00(0.00)\n",
      "Iter 3410 | Time 17.0330(17.1925) | Bit/dim 3.7171(3.7348) | Xent 0.8354(0.8286) | Loss 9.5093(9.6873) | Error 0.3033(0.2962) Steps 0(0.00) | Grad Norm 10.9908(10.4051) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0062 | Time 86.9073, Epoch Time 1051.4725(1002.4708), Bit/dim 3.7414(best: 3.7387), Xent 0.8334, Loss 4.1581, Error 0.2917(best: 0.2838)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3420 | Time 17.1970(17.3267) | Bit/dim 3.7160(3.7325) | Xent 0.8217(0.8183) | Loss 9.6353(10.2099) | Error 0.2833(0.2917) Steps 0(0.00) | Grad Norm 9.3690(9.8935) | Total Time 0.00(0.00)\n",
      "Iter 3430 | Time 17.5113(17.1274) | Bit/dim 3.7299(3.7342) | Xent 0.8625(0.8237) | Loss 9.3485(10.0153) | Error 0.2878(0.2923) Steps 0(0.00) | Grad Norm 11.6368(9.7269) | Total Time 0.00(0.00)\n",
      "Iter 3440 | Time 17.4529(17.0660) | Bit/dim 3.7503(3.7333) | Xent 0.7762(0.8194) | Loss 9.4893(9.8500) | Error 0.2700(0.2904) Steps 0(0.00) | Grad Norm 11.1614(9.7334) | Total Time 0.00(0.00)\n",
      "Iter 3450 | Time 19.4475(17.0685) | Bit/dim 3.7296(3.7371) | Xent 0.8313(0.8187) | Loss 9.6845(9.7746) | Error 0.2856(0.2908) Steps 0(0.00) | Grad Norm 10.1636(9.8601) | Total Time 0.00(0.00)\n",
      "Iter 3460 | Time 15.4661(16.9725) | Bit/dim 3.7407(3.7355) | Xent 0.7345(0.8160) | Loss 9.2623(9.6883) | Error 0.2622(0.2906) Steps 0(0.00) | Grad Norm 5.1397(9.4651) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0063 | Time 86.4017, Epoch Time 1039.1579(1003.5714), Bit/dim 3.7387(best: 3.7387), Xent 0.7937, Loss 4.1356, Error 0.2793(best: 0.2838)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3470 | Time 17.9891(17.0809) | Bit/dim 3.7181(3.7362) | Xent 0.8109(0.8049) | Loss 9.5934(10.2998) | Error 0.2900(0.2871) Steps 0(0.00) | Grad Norm 5.1792(8.8078) | Total Time 0.00(0.00)\n",
      "Iter 3480 | Time 15.4495(16.9145) | Bit/dim 3.7427(3.7341) | Xent 0.7989(0.8041) | Loss 9.3273(10.0846) | Error 0.2744(0.2869) Steps 0(0.00) | Grad Norm 9.9579(8.7985) | Total Time 0.00(0.00)\n",
      "Iter 3490 | Time 18.0465(16.9900) | Bit/dim 3.7328(3.7344) | Xent 0.8351(0.8028) | Loss 9.5417(9.9414) | Error 0.2978(0.2851) Steps 0(0.00) | Grad Norm 14.3359(9.5479) | Total Time 0.00(0.00)\n",
      "Iter 3500 | Time 15.5991(16.7957) | Bit/dim 3.7391(3.7322) | Xent 0.7641(0.8044) | Loss 9.3581(9.8108) | Error 0.2889(0.2860) Steps 0(0.00) | Grad Norm 6.6839(9.8538) | Total Time 0.00(0.00)\n",
      "Iter 3510 | Time 17.1076(16.8582) | Bit/dim 3.7638(3.7349) | Xent 0.9143(0.8139) | Loss 9.8595(9.7434) | Error 0.3078(0.2899) Steps 0(0.00) | Grad Norm 14.0809(10.7768) | Total Time 0.00(0.00)\n",
      "Iter 3520 | Time 16.1691(16.8345) | Bit/dim 3.7414(3.7337) | Xent 0.8038(0.8202) | Loss 9.5921(9.6675) | Error 0.2956(0.2920) Steps 0(0.00) | Grad Norm 13.6965(10.9383) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0064 | Time 88.2264, Epoch Time 1029.1542(1004.3389), Bit/dim 3.7392(best: 3.7387), Xent 0.8296, Loss 4.1540, Error 0.2886(best: 0.2793)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3530 | Time 16.4910(16.8500) | Bit/dim 3.7396(3.7345) | Xent 0.7279(0.8140) | Loss 9.5011(10.2128) | Error 0.2611(0.2908) Steps 0(0.00) | Grad Norm 6.1967(9.8984) | Total Time 0.00(0.00)\n",
      "Iter 3540 | Time 15.3060(16.8576) | Bit/dim 3.7194(3.7317) | Xent 0.8433(0.8075) | Loss 9.2726(10.0135) | Error 0.2989(0.2879) Steps 0(0.00) | Grad Norm 9.7182(9.6660) | Total Time 0.00(0.00)\n",
      "Iter 3550 | Time 17.5377(16.8832) | Bit/dim 3.7400(3.7301) | Xent 0.8188(0.8048) | Loss 9.4631(9.8787) | Error 0.3011(0.2868) Steps 0(0.00) | Grad Norm 8.6352(9.2499) | Total Time 0.00(0.00)\n",
      "Iter 3560 | Time 15.6916(16.8959) | Bit/dim 3.7496(3.7295) | Xent 0.8417(0.7978) | Loss 9.6049(9.7701) | Error 0.2944(0.2836) Steps 0(0.00) | Grad Norm 4.1174(8.4423) | Total Time 0.00(0.00)\n",
      "Iter 3570 | Time 17.1009(16.9446) | Bit/dim 3.7039(3.7248) | Xent 0.7834(0.7900) | Loss 9.4663(9.6874) | Error 0.2911(0.2813) Steps 0(0.00) | Grad Norm 6.9410(8.2095) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0065 | Time 88.0804, Epoch Time 1039.4153(1005.3912), Bit/dim 3.7218(best: 3.7387), Xent 0.8013, Loss 4.1224, Error 0.2807(best: 0.2793)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3580 | Time 16.9891(17.0003) | Bit/dim 3.6949(3.7211) | Xent 0.7486(0.7874) | Loss 9.4649(10.2855) | Error 0.2411(0.2791) Steps 0(0.00) | Grad Norm 11.6636(8.4140) | Total Time 0.00(0.00)\n",
      "Iter 3590 | Time 17.5157(17.0957) | Bit/dim 3.7271(3.7176) | Xent 0.7926(0.7844) | Loss 9.6745(10.0608) | Error 0.2800(0.2788) Steps 0(0.00) | Grad Norm 12.4816(8.3520) | Total Time 0.00(0.00)\n",
      "Iter 3600 | Time 15.7826(17.0821) | Bit/dim 3.7452(3.7199) | Xent 0.7115(0.7883) | Loss 9.4822(9.9052) | Error 0.2522(0.2808) Steps 0(0.00) | Grad Norm 7.7071(9.1134) | Total Time 0.00(0.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_rl.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_6_rlw_0_5_run1 --seed 1 --lr 0.001 --conditional True --controlled_tol False --train_mode semisup --log_freq 10 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5 --scale_fac 1.0 --scale_std 6.0 --rl-weight 0.5\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
