{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0,1,2,3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_rl.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "GATES = [\"cnn1\", \"cnn2\", \"rnn\"]\n",
      "\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--gate', type=str, default='cnn1', choices=GATES)\n",
      "parser.add_argument('--scale', type=float, default=1.0)\n",
      "parser.add_argument('--scale_fac', type=float, default=1.0)\n",
      "parser.add_argument('--scale_std', type=float, default=1.0)\n",
      "parser.add_argument('--eta', default=0.1, type=float,\n",
      "                        help='tuning parameter that allows us to trade-off the competing goals of' \n",
      "                                'minimizing the prediction loss and maximizing the gate rewards ')\n",
      "parser.add_argument('--rl-weight', default=0.01, type=float,\n",
      "                        help='rl weight')\n",
      "\n",
      "parser.add_argument('--gamma', default=0.99, type=float,\n",
      "                        help='discount factor, default: (0.99)')\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "import lib.odenvp_conditional_rl as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol, \"scale\": args.scale, \"scale_fac\": args.scale_fac, \"scale_std\": args.scale_std, \"gate\": args.gate},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            # re-weight the gate rewards\n",
      "            normalized_eta = args.eta / len(logp_actions)\n",
      "            \n",
      "            # collect cumulative future rewards\n",
      "            R = - loss\n",
      "            cum_rewards = []\n",
      "            for r in nfe[::-1]:\n",
      "                R = -normalized_eta * r.view(-1,1) + args.gamma * R\n",
      "                cum_rewards.insert(0,R)\n",
      "            \n",
      "            # apply REINFORCE\n",
      "            rl_loss = 0\n",
      "            for lpa, r in zip(logp_actions, cum_rewards):\n",
      "                rl_loss = rl_loss - lpa.view(-1,1) * args.rl_weight * r\n",
      "                \n",
      "            loss = loss + rl_loss.mean()\n",
      "            \n",
      "            loss.backward()\n",
      "            \n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "            \n",
      "            for idx in range(len(model.module.transforms)):\n",
      "                for layer in model.module.transforms[idx].chain:\n",
      "                    if hasattr(layer, 'atol'):\n",
      "                        layer.odefunc.after_odeint()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'train': atol[tol_indx].mean()}, itr)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'train': rtol[tol_indx].mean()}, itr)\n",
      "                    \n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples, atol, rtol, logp_actions, nfe = model(fixed_z, reverse=True)\n",
      "            generated_samples = generated_samples.view(-1, *data_shape)\n",
      "            for tol_indx in range(len(atol)):\n",
      "                writer.add_scalars('atol_gen_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                writer.add_scalars('rtol_gen_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, condition_ratio=0.5, conditional=True, controlled_tol=False, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, eta=0.1, gamma=0.99, gate='cnn2', imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume=None, rl_weight=0.01, rtol=1e-05, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdlearnscale_30_run1', scale=1.0, scale_fac=1.0, scale_std=30.0, seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1450886\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc_rl.py:291: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Iter 0010 | Time 11.1332(27.6209) | Bit/dim 8.6500(8.8188) | Xent 2.2786(2.2999) | Loss 19.4220(19.5839) | Error 0.7822(0.8626) Steps 0(0.00) | Grad Norm 20.3977(25.2896) | Total Time 0.00(0.00)\n",
      "Iter 0020 | Time 10.4795(23.3069) | Bit/dim 8.5935(8.7630) | Xent 2.2220(2.2871) | Loss 19.2350(19.4569) | Error 0.7444(0.8382) Steps 0(0.00) | Grad Norm 7.5722(21.9516) | Total Time 0.00(0.00)\n",
      "Iter 0030 | Time 10.8275(20.1830) | Bit/dim 8.3573(8.6678) | Xent 2.1903(2.2637) | Loss 19.0239(19.2680) | Error 0.7667(0.8165) Steps 0(0.00) | Grad Norm 8.7151(17.9962) | Total Time 0.00(0.00)\n",
      "Iter 0040 | Time 11.1135(17.8590) | Bit/dim 8.1429(8.5564) | Xent 2.1532(2.2355) | Loss 18.2622(19.0438) | Error 0.7567(0.7966) Steps 0(0.00) | Grad Norm 5.2691(14.9613) | Total Time 0.00(0.00)\n",
      "Iter 0050 | Time 11.1732(16.1712) | Bit/dim 7.9164(8.4243) | Xent 2.0927(2.2032) | Loss 17.5995(18.7708) | Error 0.7233(0.7770) Steps 0(0.00) | Grad Norm 5.4623(12.4722) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0001 | Time 69.1741, Epoch Time 732.0558(732.0558), Bit/dim 7.7768(best: inf), Xent 2.0748, Loss 8.8142, Error 0.7009(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0060 | Time 11.1817(14.9031) | Bit/dim 7.6587(8.2548) | Xent 2.0658(2.1709) | Loss 17.0133(18.8417) | Error 0.6956(0.7582) Steps 0(0.00) | Grad Norm 4.6128(10.4775) | Total Time 0.00(0.00)\n",
      "Iter 0070 | Time 11.2226(14.0133) | Bit/dim 7.3838(8.0591) | Xent 2.0803(2.1435) | Loss 16.8541(18.3523) | Error 0.7067(0.7423) Steps 0(0.00) | Grad Norm 3.8726(8.8653) | Total Time 0.00(0.00)\n",
      "Iter 0080 | Time 12.7556(13.3899) | Bit/dim 7.1911(7.8532) | Xent 2.0720(2.1235) | Loss 16.4482(17.8544) | Error 0.7244(0.7302) Steps 0(0.00) | Grad Norm 3.1766(7.5255) | Total Time 0.00(0.00)\n",
      "Iter 0090 | Time 11.9231(12.9376) | Bit/dim 7.0806(7.6622) | Xent 2.0684(2.1102) | Loss 16.2632(17.4118) | Error 0.7211(0.7218) Steps 0(0.00) | Grad Norm 3.5049(6.3076) | Total Time 0.00(0.00)\n",
      "Iter 0100 | Time 12.4285(12.6274) | Bit/dim 7.0468(7.5004) | Xent 2.0972(2.1033) | Loss 16.2819(17.0643) | Error 0.7189(0.7185) Steps 0(0.00) | Grad Norm 1.7084(5.3128) | Total Time 0.00(0.00)\n",
      "Iter 0110 | Time 11.2503(12.3643) | Bit/dim 6.9945(7.3694) | Xent 2.0374(2.0943) | Loss 16.1733(16.7903) | Error 0.6878(0.7137) Steps 0(0.00) | Grad Norm 1.8014(4.5076) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0002 | Time 69.9980, Epoch Time 725.6408(731.8634), Bit/dim 6.9835(best: 7.7768), Xent 2.0611, Loss 8.0141, Error 0.6903(best: 0.7009)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0120 | Time 11.0397(12.1531) | Bit/dim 6.9524(7.2624) | Xent 2.0354(2.0855) | Loss 15.7127(16.9837) | Error 0.6811(0.7108) Steps 0(0.00) | Grad Norm 3.3836(4.0243) | Total Time 0.00(0.00)\n",
      "Iter 0130 | Time 12.9161(12.1814) | Bit/dim 6.8993(7.1746) | Xent 2.0687(2.0755) | Loss 15.9481(16.6791) | Error 0.7156(0.7062) Steps 0(0.00) | Grad Norm 3.1392(3.8970) | Total Time 0.00(0.00)\n",
      "Iter 0140 | Time 11.4830(12.1645) | Bit/dim 6.8294(7.0957) | Xent 2.0315(2.0653) | Loss 15.5897(16.4238) | Error 0.6911(0.7027) Steps 0(0.00) | Grad Norm 4.6630(3.6406) | Total Time 0.00(0.00)\n",
      "Iter 0150 | Time 11.8500(12.1971) | Bit/dim 6.7745(7.0218) | Xent 2.0293(2.0557) | Loss 15.3237(16.1826) | Error 0.6767(0.6973) Steps 0(0.00) | Grad Norm 5.7838(4.4474) | Total Time 0.00(0.00)\n",
      "Iter 0160 | Time 13.2016(12.2969) | Bit/dim 6.6627(6.9418) | Xent 2.0258(2.0471) | Loss 15.4854(15.9921) | Error 0.7100(0.6962) Steps 0(0.00) | Grad Norm 17.2956(5.9922) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0003 | Time 73.6201, Epoch Time 765.7846(732.8810), Bit/dim 6.6174(best: 6.9835), Xent 2.1368, Loss 7.6858, Error 0.7767(best: 0.6903)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0170 | Time 12.7738(12.4537) | Bit/dim 6.5680(6.8539) | Xent 2.0749(2.0511) | Loss 15.4107(16.3120) | Error 0.7522(0.7032) Steps 0(0.00) | Grad Norm 55.8151(14.5069) | Total Time 0.00(0.00)\n",
      "Iter 0180 | Time 12.4674(12.6418) | Bit/dim 6.3476(6.7479) | Xent 2.0215(2.0402) | Loss 14.6318(15.9498) | Error 0.6833(0.6969) Steps 0(0.00) | Grad Norm 16.7429(16.9670) | Total Time 0.00(0.00)\n",
      "Iter 0190 | Time 12.3112(12.6929) | Bit/dim 6.1796(6.6173) | Xent 1.9759(2.0281) | Loss 14.3743(15.5963) | Error 0.6811(0.6922) Steps 0(0.00) | Grad Norm 40.7384(19.7372) | Total Time 0.00(0.00)\n",
      "Iter 0200 | Time 13.9369(12.8287) | Bit/dim 5.9946(6.4799) | Xent 2.1002(2.0439) | Loss 14.1017(15.2668) | Error 0.7344(0.7045) Steps 0(0.00) | Grad Norm 61.2639(35.5249) | Total Time 0.00(0.00)\n",
      "Iter 0210 | Time 15.5839(13.0808) | Bit/dim 5.8746(6.3351) | Xent 2.0003(2.0408) | Loss 14.0052(14.9148) | Error 0.6600(0.7043) Steps 0(0.00) | Grad Norm 39.9955(40.2018) | Total Time 0.00(0.00)\n",
      "Iter 0220 | Time 13.7384(13.2481) | Bit/dim 5.7633(6.1996) | Xent 1.9937(2.0364) | Loss 13.8122(14.6075) | Error 0.7011(0.7030) Steps 0(0.00) | Grad Norm 25.2791(39.2877) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0004 | Time 72.5557, Epoch Time 820.4382(735.5077), Bit/dim 5.7724(best: 6.6174), Xent 1.9829, Loss 6.7638, Error 0.6562(best: 0.6903)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0230 | Time 13.2233(13.3590) | Bit/dim 5.7277(6.0807) | Xent 1.9724(2.0232) | Loss 13.3872(14.7371) | Error 0.6856(0.6949) Steps 0(0.00) | Grad Norm 49.2186(36.3809) | Total Time 0.00(0.00)\n",
      "Iter 0240 | Time 13.9506(13.2815) | Bit/dim 5.6622(5.9808) | Xent 1.9626(2.0141) | Loss 13.5181(14.3768) | Error 0.6722(0.6928) Steps 0(0.00) | Grad Norm 66.7009(41.1764) | Total Time 0.00(0.00)\n",
      "Iter 0250 | Time 12.9451(13.1083) | Bit/dim 5.6654(5.8947) | Xent 2.0004(2.0053) | Loss 13.3842(14.1126) | Error 0.6767(0.6887) Steps 0(0.00) | Grad Norm 22.4030(39.1268) | Total Time 0.00(0.00)\n",
      "Iter 0260 | Time 12.7672(13.0593) | Bit/dim 5.6599(5.8322) | Xent 2.0004(2.0033) | Loss 13.3446(13.9431) | Error 0.6844(0.6920) Steps 0(0.00) | Grad Norm 56.0991(41.8950) | Total Time 0.00(0.00)\n",
      "Iter 0270 | Time 11.9587(13.0084) | Bit/dim 5.6343(5.7799) | Xent 1.9777(2.0041) | Loss 12.8838(13.7988) | Error 0.6700(0.6958) Steps 0(0.00) | Grad Norm 36.4782(43.8788) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0005 | Time 72.1500, Epoch Time 805.2647(737.6004), Bit/dim 5.5944(best: 5.7724), Xent 1.9579, Loss 6.5734, Error 0.6473(best: 0.6562)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0280 | Time 13.7678(12.9696) | Bit/dim 5.5993(5.7339) | Xent 1.9781(1.9961) | Loss 13.4431(14.1154) | Error 0.6789(0.6889) Steps 0(0.00) | Grad Norm 27.9552(37.6298) | Total Time 0.00(0.00)\n",
      "Iter 0290 | Time 13.2186(12.9525) | Bit/dim 5.6210(5.6939) | Xent 1.9226(1.9812) | Loss 13.3717(13.8769) | Error 0.6378(0.6789) Steps 0(0.00) | Grad Norm 42.2008(32.4647) | Total Time 0.00(0.00)\n",
      "Iter 0300 | Time 12.5044(12.8894) | Bit/dim 5.6020(5.6661) | Xent 1.9457(1.9968) | Loss 13.1928(13.7383) | Error 0.6756(0.6908) Steps 0(0.00) | Grad Norm 21.1772(43.2221) | Total Time 0.00(0.00)\n",
      "Iter 0310 | Time 12.4243(12.8462) | Bit/dim 5.5371(5.6337) | Xent 1.9880(1.9958) | Loss 12.8158(13.6026) | Error 0.6489(0.6919) Steps 0(0.00) | Grad Norm 13.8540(39.0700) | Total Time 0.00(0.00)\n",
      "Iter 0320 | Time 13.4794(12.9068) | Bit/dim 5.4472(5.6016) | Xent 1.9858(1.9900) | Loss 12.8978(13.4618) | Error 0.6789(0.6865) Steps 0(0.00) | Grad Norm 5.2123(31.4366) | Total Time 0.00(0.00)\n",
      "Iter 0330 | Time 12.2959(12.9508) | Bit/dim 5.4783(5.5660) | Xent 1.9326(1.9758) | Loss 13.0312(13.3345) | Error 0.6422(0.6795) Steps 0(0.00) | Grad Norm 7.2712(25.2325) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0006 | Time 72.3329, Epoch Time 800.6092(739.4907), Bit/dim 5.4652(best: 5.5944), Xent 1.9059, Loss 6.4181, Error 0.6337(best: 0.6473)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0340 | Time 12.8553(12.9673) | Bit/dim 5.4206(5.5383) | Xent 1.9806(1.9693) | Loss 13.0533(13.6787) | Error 0.6989(0.6797) Steps 0(0.00) | Grad Norm 73.4175(30.2755) | Total Time 0.00(0.00)\n",
      "Iter 0350 | Time 13.1185(13.0175) | Bit/dim 5.3831(5.5062) | Xent 1.8846(1.9577) | Loss 12.8283(13.4823) | Error 0.6600(0.6765) Steps 0(0.00) | Grad Norm 24.7555(30.7113) | Total Time 0.00(0.00)\n",
      "Iter 0360 | Time 13.3258(13.0514) | Bit/dim 5.3534(5.4718) | Xent 1.9110(1.9493) | Loss 12.6324(13.2936) | Error 0.6633(0.6735) Steps 0(0.00) | Grad Norm 26.7991(30.3500) | Total Time 0.00(0.00)\n",
      "Iter 0370 | Time 13.5028(13.2000) | Bit/dim 5.3288(5.4397) | Xent 1.9583(1.9413) | Loss 12.6937(13.1506) | Error 0.6856(0.6703) Steps 0(0.00) | Grad Norm 56.0934(30.8141) | Total Time 0.00(0.00)\n",
      "Iter 0380 | Time 13.6369(13.2567) | Bit/dim 5.3782(5.4078) | Xent 2.1138(1.9415) | Loss 12.7967(13.0376) | Error 0.7789(0.6726) Steps 0(0.00) | Grad Norm 67.9003(34.1566) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0007 | Time 75.9625, Epoch Time 822.3111(741.9753), Bit/dim 5.2528(best: 5.4652), Xent 1.9414, Loss 6.2236, Error 0.6672(best: 0.6337)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0390 | Time 13.4270(13.2240) | Bit/dim 5.2535(5.3670) | Xent 1.9846(1.9554) | Loss 12.7778(13.4109) | Error 0.6878(0.6799) Steps 0(0.00) | Grad Norm 19.4242(32.4589) | Total Time 0.00(0.00)\n",
      "Iter 0400 | Time 14.1778(13.2542) | Bit/dim 5.2634(5.3349) | Xent 1.9131(1.9470) | Loss 12.8179(13.1909) | Error 0.6500(0.6755) Steps 0(0.00) | Grad Norm 32.8155(28.9599) | Total Time 0.00(0.00)\n",
      "Iter 0410 | Time 13.9613(13.2655) | Bit/dim 5.1349(5.3014) | Xent 1.9050(1.9333) | Loss 12.1194(13.0064) | Error 0.6656(0.6721) Steps 0(0.00) | Grad Norm 16.1069(27.5820) | Total Time 0.00(0.00)\n",
      "Iter 0420 | Time 13.4820(13.3198) | Bit/dim 5.0968(5.2610) | Xent 1.9197(1.9248) | Loss 12.4495(12.8513) | Error 0.6633(0.6686) Steps 0(0.00) | Grad Norm 11.5280(23.3300) | Total Time 0.00(0.00)\n",
      "Iter 0430 | Time 13.4779(13.2930) | Bit/dim 5.1293(5.2336) | Xent 1.9294(1.9194) | Loss 12.4967(12.7235) | Error 0.6978(0.6669) Steps 0(0.00) | Grad Norm 47.3742(25.3881) | Total Time 0.00(0.00)\n",
      "Iter 0440 | Time 13.1673(13.3102) | Bit/dim 5.1182(5.2016) | Xent 1.9848(1.9252) | Loss 12.3457(12.6231) | Error 0.7056(0.6701) Steps 0(0.00) | Grad Norm 40.1432(28.6689) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0008 | Time 75.3827, Epoch Time 827.2973(744.5350), Bit/dim 5.0944(best: 5.2528), Xent 1.8774, Loss 6.0331, Error 0.6384(best: 0.6337)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0450 | Time 14.2441(13.3743) | Bit/dim 5.0674(5.1694) | Xent 1.8992(1.9119) | Loss 11.9670(12.9555) | Error 0.6722(0.6652) Steps 0(0.00) | Grad Norm 15.3582(25.9816) | Total Time 0.00(0.00)\n",
      "Iter 0460 | Time 14.0865(13.4557) | Bit/dim 5.0374(5.1365) | Xent 1.8486(1.9008) | Loss 12.2367(12.7637) | Error 0.6267(0.6610) Steps 0(0.00) | Grad Norm 27.1401(25.1809) | Total Time 0.00(0.00)\n",
      "Iter 0470 | Time 13.9774(13.5121) | Bit/dim 5.0040(5.1092) | Xent 1.8638(1.8984) | Loss 11.9103(12.6145) | Error 0.6644(0.6619) Steps 0(0.00) | Grad Norm 19.1420(25.5025) | Total Time 0.00(0.00)\n",
      "Iter 0480 | Time 12.7451(13.4696) | Bit/dim 4.9745(5.0808) | Xent 1.8425(1.8914) | Loss 12.1002(12.4702) | Error 0.6300(0.6610) Steps 0(0.00) | Grad Norm 23.1104(25.6984) | Total Time 0.00(0.00)\n",
      "Iter 0490 | Time 13.4471(13.5233) | Bit/dim 4.9870(5.0487) | Xent 1.9677(1.8902) | Loss 12.3171(12.3676) | Error 0.7000(0.6609) Steps 0(0.00) | Grad Norm 63.4189(26.3160) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0009 | Time 76.3167, Epoch Time 844.0878(747.5215), Bit/dim 4.9417(best: 5.0944), Xent 1.9275, Loss 5.9055, Error 0.6808(best: 0.6337)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0500 | Time 14.2997(13.6900) | Bit/dim 4.8756(5.0170) | Xent 1.8960(1.8995) | Loss 11.7410(12.7172) | Error 0.6733(0.6658) Steps 0(0.00) | Grad Norm 33.0900(28.5782) | Total Time 0.00(0.00)\n",
      "Iter 0510 | Time 14.1588(13.6873) | Bit/dim 4.8949(4.9919) | Xent 1.8465(1.8951) | Loss 12.0097(12.5205) | Error 0.6656(0.6651) Steps 0(0.00) | Grad Norm 32.1491(28.0400) | Total Time 0.00(0.00)\n",
      "Iter 0520 | Time 14.3968(13.7660) | Bit/dim 4.8285(4.9574) | Xent 1.8330(1.8840) | Loss 11.9282(12.3487) | Error 0.6456(0.6590) Steps 0(0.00) | Grad Norm 31.4361(24.8378) | Total Time 0.00(0.00)\n",
      "Iter 0530 | Time 13.2071(13.8717) | Bit/dim 4.8622(4.9312) | Xent 1.8214(1.8750) | Loss 11.5827(12.2197) | Error 0.6511(0.6562) Steps 0(0.00) | Grad Norm 16.0992(24.9052) | Total Time 0.00(0.00)\n",
      "Iter 0540 | Time 13.8191(14.0378) | Bit/dim 4.8660(4.9238) | Xent 1.8959(1.8771) | Loss 11.9699(12.1348) | Error 0.6678(0.6574) Steps 0(0.00) | Grad Norm 39.6320(29.6012) | Total Time 0.00(0.00)\n",
      "Iter 0550 | Time 14.6179(13.9557) | Bit/dim 4.7942(4.8995) | Xent 1.8602(1.8785) | Loss 11.7532(12.0444) | Error 0.6567(0.6593) Steps 0(0.00) | Grad Norm 13.0287(28.2930) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0010 | Time 77.5563, Epoch Time 867.1699(751.1110), Bit/dim 4.8009(best: 4.9417), Xent 1.7859, Loss 5.6938, Error 0.6130(best: 0.6337)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0560 | Time 13.4665(14.1116) | Bit/dim 4.8163(4.8688) | Xent 1.7971(1.8623) | Loss 11.5985(12.3868) | Error 0.6222(0.6540) Steps 0(0.00) | Grad Norm 17.6286(24.7099) | Total Time 0.00(0.00)\n",
      "Iter 0570 | Time 13.6281(14.0898) | Bit/dim 4.7877(4.8442) | Xent 1.7860(1.8389) | Loss 11.5470(12.1603) | Error 0.6100(0.6443) Steps 0(0.00) | Grad Norm 12.0174(23.5195) | Total Time 0.00(0.00)\n",
      "Iter 0580 | Time 14.7085(14.1435) | Bit/dim 4.7773(4.8181) | Xent 1.7722(1.8196) | Loss 11.5454(12.0190) | Error 0.6133(0.6383) Steps 0(0.00) | Grad Norm 29.4272(23.8622) | Total Time 0.00(0.00)\n",
      "Iter 0590 | Time 13.1174(14.0396) | Bit/dim 4.7745(4.8046) | Xent 2.1538(1.8364) | Loss 11.6368(11.9204) | Error 0.7300(0.6439) Steps 0(0.00) | Grad Norm 64.3676(29.5943) | Total Time 0.00(0.00)\n",
      "Iter 0600 | Time 14.6197(14.0805) | Bit/dim 4.7624(4.7884) | Xent 1.8862(1.8452) | Loss 11.5572(11.8347) | Error 0.6589(0.6477) Steps 0(0.00) | Grad Norm 35.3741(29.4078) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0011 | Time 79.9631, Epoch Time 878.7713(754.9408), Bit/dim 4.7018(best: 4.8009), Xent 1.7430, Loss 5.5733, Error 0.6111(best: 0.6130)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0610 | Time 13.6142(14.2618) | Bit/dim 4.7354(4.7662) | Xent 1.8366(1.8325) | Loss 11.2189(12.2598) | Error 0.6556(0.6435) Steps 0(0.00) | Grad Norm 19.0144(26.2445) | Total Time 0.00(0.00)\n",
      "Iter 0620 | Time 13.5215(14.3570) | Bit/dim 4.7147(4.7442) | Xent 1.7660(1.8118) | Loss 11.5122(12.0461) | Error 0.6233(0.6386) Steps 0(0.00) | Grad Norm 12.9397(22.9808) | Total Time 0.00(0.00)\n",
      "Iter 0630 | Time 14.7762(14.3963) | Bit/dim 4.6981(4.7270) | Xent 1.7216(1.7920) | Loss 11.2282(11.8598) | Error 0.6033(0.6320) Steps 0(0.00) | Grad Norm 29.2295(23.4474) | Total Time 0.00(0.00)\n",
      "Iter 0640 | Time 14.7559(14.5922) | Bit/dim 4.6672(4.7085) | Xent 1.7400(1.7695) | Loss 11.4711(11.7211) | Error 0.6178(0.6247) Steps 0(0.00) | Grad Norm 12.6784(21.1198) | Total Time 0.00(0.00)\n",
      "Iter 0650 | Time 13.6688(14.5748) | Bit/dim 4.6449(4.7028) | Xent 1.7857(1.7863) | Loss 11.2158(11.6565) | Error 0.6256(0.6306) Steps 0(0.00) | Grad Norm 21.2379(24.5620) | Total Time 0.00(0.00)\n",
      "Iter 0660 | Time 15.0921(14.7133) | Bit/dim 4.6234(4.6865) | Xent 1.7488(1.7898) | Loss 11.3646(11.5865) | Error 0.6256(0.6320) Steps 0(0.00) | Grad Norm 10.4713(22.2954) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0012 | Time 80.4685, Epoch Time 914.3265(759.7224), Bit/dim 4.6314(best: 4.7018), Xent 1.6919, Loss 5.4773, Error 0.5948(best: 0.6111)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0670 | Time 14.8202(14.7498) | Bit/dim 4.5942(4.6689) | Xent 1.7206(1.7763) | Loss 11.2874(11.9254) | Error 0.6078(0.6301) Steps 0(0.00) | Grad Norm 6.4359(20.3009) | Total Time 0.00(0.00)\n",
      "Iter 0680 | Time 14.7138(14.8369) | Bit/dim 4.6013(4.6525) | Xent 1.6761(1.7597) | Loss 11.2384(11.7457) | Error 0.6022(0.6238) Steps 0(0.00) | Grad Norm 19.6726(21.0277) | Total Time 0.00(0.00)\n",
      "Iter 0690 | Time 15.4956(14.8081) | Bit/dim 4.6123(4.6418) | Xent 1.6864(1.7586) | Loss 11.0253(11.6191) | Error 0.6067(0.6241) Steps 0(0.00) | Grad Norm 14.2222(23.9158) | Total Time 0.00(0.00)\n",
      "Iter 0700 | Time 16.0352(14.9392) | Bit/dim 4.5642(4.6258) | Xent 1.6341(1.7498) | Loss 11.0195(11.4988) | Error 0.5767(0.6226) Steps 0(0.00) | Grad Norm 4.3442(21.8361) | Total Time 0.00(0.00)\n",
      "Iter 0710 | Time 15.5361(14.9478) | Bit/dim 4.6656(4.6245) | Xent 1.9042(1.7664) | Loss 11.7063(11.4801) | Error 0.6689(0.6277) Steps 0(0.00) | Grad Norm 34.9596(25.9153) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0013 | Time 82.9634, Epoch Time 928.9795(764.8001), Bit/dim 4.5832(best: 4.6314), Xent 1.7326, Loss 5.4495, Error 0.6157(best: 0.5948)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0720 | Time 15.4827(15.1366) | Bit/dim 4.5802(4.6151) | Xent 1.7138(1.7682) | Loss 11.0877(11.9435) | Error 0.5944(0.6270) Steps 0(0.00) | Grad Norm 13.1153(23.8067) | Total Time 0.00(0.00)\n",
      "Iter 0730 | Time 14.8589(15.1048) | Bit/dim 4.5641(4.5979) | Xent 1.6937(1.7560) | Loss 10.7719(11.7120) | Error 0.6244(0.6244) Steps 0(0.00) | Grad Norm 20.5456(20.7548) | Total Time 0.00(0.00)\n",
      "Iter 0740 | Time 14.6475(15.1654) | Bit/dim 4.5046(4.5821) | Xent 1.7389(1.7387) | Loss 11.1980(11.5578) | Error 0.5978(0.6194) Steps 0(0.00) | Grad Norm 6.9932(19.0184) | Total Time 0.00(0.00)\n",
      "Iter 0750 | Time 14.3186(15.2239) | Bit/dim 4.5306(4.5700) | Xent 1.6933(1.7264) | Loss 10.9882(11.4197) | Error 0.6111(0.6149) Steps 0(0.00) | Grad Norm 26.1411(20.4111) | Total Time 0.00(0.00)\n",
      "Iter 0760 | Time 15.2462(15.3210) | Bit/dim 4.4979(4.5551) | Xent 1.7186(1.7232) | Loss 11.2280(11.3342) | Error 0.6133(0.6136) Steps 0(0.00) | Grad Norm 14.8185(20.4191) | Total Time 0.00(0.00)\n",
      "Iter 0770 | Time 16.0930(15.3385) | Bit/dim 4.4804(4.5393) | Xent 1.7903(1.7156) | Loss 11.1090(11.2393) | Error 0.6456(0.6113) Steps 0(0.00) | Grad Norm 23.4444(20.2739) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0014 | Time 81.9780, Epoch Time 945.5403(770.2223), Bit/dim 4.4970(best: 4.5832), Xent 1.6398, Loss 5.3169, Error 0.5857(best: 0.5948)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0780 | Time 14.7442(15.4292) | Bit/dim 4.5026(4.5311) | Xent 1.7912(1.7096) | Loss 10.8432(11.6390) | Error 0.6300(0.6105) Steps 0(0.00) | Grad Norm 35.1899(22.0056) | Total Time 0.00(0.00)\n",
      "Iter 0790 | Time 14.5775(15.3812) | Bit/dim 4.5140(4.5186) | Xent 1.6168(1.7026) | Loss 11.1413(11.4729) | Error 0.5833(0.6081) Steps 0(0.00) | Grad Norm 16.0493(20.7313) | Total Time 0.00(0.00)\n",
      "Iter 0800 | Time 14.5428(15.3423) | Bit/dim 4.4403(4.5033) | Xent 1.6294(1.6892) | Loss 10.6994(11.3198) | Error 0.5978(0.6045) Steps 0(0.00) | Grad Norm 7.8709(18.2600) | Total Time 0.00(0.00)\n",
      "Iter 0810 | Time 17.5002(15.4186) | Bit/dim 4.4570(4.4915) | Xent 1.6623(1.6829) | Loss 10.8787(11.2207) | Error 0.5967(0.6033) Steps 0(0.00) | Grad Norm 6.9242(16.9831) | Total Time 0.00(0.00)\n",
      "Iter 0820 | Time 14.7939(15.5361) | Bit/dim 4.4311(4.4797) | Xent 1.6253(1.6771) | Loss 10.7132(11.1272) | Error 0.5644(0.5997) Steps 0(0.00) | Grad Norm 13.9555(17.1468) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0015 | Time 81.8381, Epoch Time 957.7909(775.8494), Bit/dim 4.4639(best: 4.4970), Xent 1.6205, Loss 5.2741, Error 0.5693(best: 0.5857)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0830 | Time 16.3152(15.6616) | Bit/dim 4.4285(4.4725) | Xent 1.7587(1.6834) | Loss 10.9032(11.6217) | Error 0.6011(0.6012) Steps 0(0.00) | Grad Norm 33.7493(19.9121) | Total Time 0.00(0.00)\n",
      "Iter 0840 | Time 15.8828(15.6605) | Bit/dim 4.4140(4.4632) | Xent 1.6498(1.6757) | Loss 10.7480(11.4143) | Error 0.5878(0.5974) Steps 0(0.00) | Grad Norm 14.2279(18.5029) | Total Time 0.00(0.00)\n",
      "Iter 0850 | Time 17.3874(15.6944) | Bit/dim 4.4100(4.4509) | Xent 1.6199(1.6680) | Loss 10.7834(11.2610) | Error 0.5889(0.5934) Steps 0(0.00) | Grad Norm 12.6262(17.7956) | Total Time 0.00(0.00)\n",
      "Iter 0860 | Time 15.0280(15.6012) | Bit/dim 4.4081(4.4432) | Xent 1.7031(1.6691) | Loss 10.7675(11.1415) | Error 0.5956(0.5929) Steps 0(0.00) | Grad Norm 27.9440(19.8222) | Total Time 0.00(0.00)\n",
      "Iter 0870 | Time 16.1773(15.6791) | Bit/dim 4.3756(4.4301) | Xent 1.5796(1.6592) | Loss 10.7469(11.0824) | Error 0.5633(0.5910) Steps 0(0.00) | Grad Norm 7.1126(18.7225) | Total Time 0.00(0.00)\n",
      "Iter 0880 | Time 15.3682(15.6024) | Bit/dim 4.3707(4.4190) | Xent 1.6224(1.6511) | Loss 10.8026(11.0182) | Error 0.5878(0.5877) Steps 0(0.00) | Grad Norm 10.1679(17.7631) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0016 | Time 83.0253, Epoch Time 960.7034(781.3950), Bit/dim 4.3857(best: 4.4639), Xent 1.5367, Loss 5.1541, Error 0.5466(best: 0.5693)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0890 | Time 14.7013(15.5655) | Bit/dim 4.3623(4.4070) | Xent 1.5621(1.6366) | Loss 10.5634(11.3905) | Error 0.5700(0.5852) Steps 0(0.00) | Grad Norm 4.7765(16.7045) | Total Time 0.00(0.00)\n",
      "Iter 0900 | Time 14.8617(15.5455) | Bit/dim 4.3428(4.3952) | Xent 1.6145(1.6306) | Loss 10.4868(11.2138) | Error 0.5889(0.5832) Steps 0(0.00) | Grad Norm 13.9849(16.2733) | Total Time 0.00(0.00)\n",
      "Iter 0910 | Time 18.3836(15.7693) | Bit/dim 4.3323(4.3805) | Xent 1.5824(1.6189) | Loss 10.7787(11.0920) | Error 0.5756(0.5787) Steps 0(0.00) | Grad Norm 8.2433(15.6416) | Total Time 0.00(0.00)\n",
      "Iter 0920 | Time 16.1508(15.8130) | Bit/dim 4.3332(4.3721) | Xent 1.6541(1.6206) | Loss 10.4476(10.9713) | Error 0.6144(0.5803) Steps 0(0.00) | Grad Norm 13.5987(17.0806) | Total Time 0.00(0.00)\n",
      "Iter 0930 | Time 15.1065(15.7427) | Bit/dim 4.3669(4.3639) | Xent 1.6852(1.6169) | Loss 10.6453(10.8897) | Error 0.6244(0.5799) Steps 0(0.00) | Grad Norm 31.8009(17.8611) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0017 | Time 84.5410, Epoch Time 970.4925(787.0679), Bit/dim 4.3194(best: 4.3857), Xent 1.5354, Loss 5.0870, Error 0.5434(best: 0.5466)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0940 | Time 16.2570(15.7733) | Bit/dim 4.3131(4.3560) | Xent 1.6121(1.6188) | Loss 10.6924(11.3556) | Error 0.5844(0.5809) Steps 0(0.00) | Grad Norm 8.3784(17.5755) | Total Time 0.00(0.00)\n",
      "Iter 0950 | Time 15.7862(15.7897) | Bit/dim 4.2957(4.3482) | Xent 1.6145(1.6123) | Loss 10.7446(11.1599) | Error 0.5789(0.5779) Steps 0(0.00) | Grad Norm 21.2570(18.1913) | Total Time 0.00(0.00)\n",
      "Iter 0960 | Time 16.0426(15.7395) | Bit/dim 4.3275(4.3374) | Xent 1.5830(1.6051) | Loss 10.3172(10.9751) | Error 0.5878(0.5767) Steps 0(0.00) | Grad Norm 6.0090(15.8258) | Total Time 0.00(0.00)\n",
      "Iter 0970 | Time 16.0683(15.7981) | Bit/dim 4.2647(4.3251) | Xent 1.5705(1.5914) | Loss 10.4747(10.8569) | Error 0.5789(0.5721) Steps 0(0.00) | Grad Norm 13.8006(14.6356) | Total Time 0.00(0.00)\n",
      "Iter 0980 | Time 16.3915(15.8140) | Bit/dim 4.3310(4.3292) | Xent 1.6119(1.6089) | Loss 10.5960(10.8162) | Error 0.5722(0.5762) Steps 0(0.00) | Grad Norm 20.8353(18.4471) | Total Time 0.00(0.00)\n",
      "Iter 0990 | Time 15.3792(15.8712) | Bit/dim 4.3120(4.3257) | Xent 1.6496(1.6188) | Loss 10.6467(10.7727) | Error 0.5944(0.5808) Steps 0(0.00) | Grad Norm 27.4692(18.2428) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0018 | Time 85.2261, Epoch Time 974.3517(792.6864), Bit/dim 4.3075(best: 4.3194), Xent 1.6168, Loss 5.1159, Error 0.5892(best: 0.5434)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1000 | Time 16.4702(15.8477) | Bit/dim 4.2887(4.3167) | Xent 1.5487(1.6139) | Loss 10.6855(11.1951) | Error 0.5578(0.5787) Steps 0(0.00) | Grad Norm 18.4576(18.2997) | Total Time 0.00(0.00)\n",
      "Iter 1010 | Time 14.4927(15.8435) | Bit/dim 4.2580(4.3028) | Xent 1.5700(1.5965) | Loss 10.3522(10.9885) | Error 0.5700(0.5720) Steps 0(0.00) | Grad Norm 11.0492(15.9459) | Total Time 0.00(0.00)\n",
      "Iter 1020 | Time 15.1081(15.8564) | Bit/dim 4.2316(4.2859) | Xent 1.4672(1.5823) | Loss 10.4104(10.8434) | Error 0.5244(0.5679) Steps 0(0.00) | Grad Norm 7.7184(14.4385) | Total Time 0.00(0.00)\n",
      "Iter 1030 | Time 17.0668(15.8493) | Bit/dim 4.2349(4.2735) | Xent 1.4931(1.5658) | Loss 10.3457(10.7379) | Error 0.5456(0.5622) Steps 0(0.00) | Grad Norm 19.0307(14.0962) | Total Time 0.00(0.00)\n",
      "Iter 1040 | Time 14.3543(15.7713) | Bit/dim 4.2336(4.2574) | Xent 1.5068(1.5604) | Loss 10.2463(10.6340) | Error 0.5644(0.5607) Steps 0(0.00) | Grad Norm 13.3810(13.7629) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0019 | Time 84.0404, Epoch Time 968.4652(797.9598), Bit/dim 4.2155(best: 4.3075), Xent 1.4334, Loss 4.9322, Error 0.5147(best: 0.5434)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1050 | Time 15.5546(15.7900) | Bit/dim 4.1520(4.2459) | Xent 1.4497(1.5441) | Loss 10.1729(11.0542) | Error 0.5156(0.5546) Steps 0(0.00) | Grad Norm 7.3292(12.9277) | Total Time 0.00(0.00)\n",
      "Iter 1060 | Time 16.4259(15.8370) | Bit/dim 4.2432(4.2380) | Xent 1.4567(1.5316) | Loss 10.4643(10.8718) | Error 0.5000(0.5491) Steps 0(0.00) | Grad Norm 15.2148(14.1714) | Total Time 0.00(0.00)\n",
      "Iter 1070 | Time 15.0793(15.9226) | Bit/dim 4.1774(4.2292) | Xent 1.5739(1.5197) | Loss 10.3081(10.7279) | Error 0.5700(0.5472) Steps 0(0.00) | Grad Norm 8.4519(13.3393) | Total Time 0.00(0.00)\n",
      "Iter 1080 | Time 16.7174(16.0042) | Bit/dim 4.1877(4.2179) | Xent 1.5093(1.5268) | Loss 10.3840(10.6273) | Error 0.5456(0.5498) Steps 0(0.00) | Grad Norm 13.2843(14.0652) | Total Time 0.00(0.00)\n",
      "Iter 1090 | Time 15.8157(15.9717) | Bit/dim 4.1624(4.2108) | Xent 1.5262(1.5310) | Loss 10.3988(10.5411) | Error 0.5356(0.5516) Steps 0(0.00) | Grad Norm 23.4085(15.4282) | Total Time 0.00(0.00)\n",
      "Iter 1100 | Time 17.1213(16.0989) | Bit/dim 4.1783(4.2033) | Xent 1.4599(1.5280) | Loss 10.1489(10.4698) | Error 0.5244(0.5509) Steps 0(0.00) | Grad Norm 9.2275(15.3592) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0020 | Time 83.6247, Epoch Time 989.2337(803.6980), Bit/dim 4.1839(best: 4.2155), Xent 1.4176, Loss 4.8927, Error 0.5128(best: 0.5147)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1110 | Time 20.2053(16.3511) | Bit/dim 4.1965(4.1966) | Xent 1.4549(1.5121) | Loss 10.2501(10.8596) | Error 0.5222(0.5456) Steps 0(0.00) | Grad Norm 20.5764(14.4094) | Total Time 0.00(0.00)\n",
      "Iter 1120 | Time 16.3365(16.1984) | Bit/dim 4.1693(4.1880) | Xent 1.4116(1.4932) | Loss 10.1962(10.6766) | Error 0.5133(0.5391) Steps 0(0.00) | Grad Norm 12.7191(13.6753) | Total Time 0.00(0.00)\n",
      "Iter 1130 | Time 15.7612(16.0772) | Bit/dim 4.1461(4.1741) | Xent 1.4596(1.4780) | Loss 10.0407(10.5267) | Error 0.5333(0.5333) Steps 0(0.00) | Grad Norm 16.3403(13.0063) | Total Time 0.00(0.00)\n",
      "Iter 1140 | Time 15.7951(16.0430) | Bit/dim 4.1727(4.1682) | Xent 1.5258(1.4679) | Loss 10.1104(10.4291) | Error 0.5578(0.5298) Steps 0(0.00) | Grad Norm 22.7653(14.2285) | Total Time 0.00(0.00)\n",
      "Iter 1150 | Time 16.0199(15.9348) | Bit/dim 4.1194(4.1631) | Xent 1.4036(1.4700) | Loss 10.0844(10.3313) | Error 0.5067(0.5312) Steps 0(0.00) | Grad Norm 6.1686(13.9954) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0021 | Time 84.0072, Epoch Time 985.4963(809.1519), Bit/dim 4.1332(best: 4.1839), Xent 1.3472, Loss 4.8068, Error 0.4861(best: 0.5128)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1160 | Time 16.9226(16.1079) | Bit/dim 4.1497(4.1543) | Xent 1.3326(1.4566) | Loss 10.0222(10.7999) | Error 0.5089(0.5279) Steps 0(0.00) | Grad Norm 6.0476(12.5214) | Total Time 0.00(0.00)\n",
      "Iter 1170 | Time 16.8994(16.0020) | Bit/dim 4.1512(4.1491) | Xent 1.4389(1.4441) | Loss 10.2126(10.6139) | Error 0.5356(0.5235) Steps 0(0.00) | Grad Norm 13.9188(12.1533) | Total Time 0.00(0.00)\n",
      "Iter 1180 | Time 15.5053(15.9694) | Bit/dim 4.1053(4.1425) | Xent 1.3624(1.4350) | Loss 9.7024(10.4614) | Error 0.4833(0.5223) Steps 0(0.00) | Grad Norm 11.3425(12.9118) | Total Time 0.00(0.00)\n",
      "Iter 1190 | Time 16.0587(15.9745) | Bit/dim 4.1135(4.1346) | Xent 1.3759(1.4318) | Loss 9.9387(10.3448) | Error 0.4889(0.5190) Steps 0(0.00) | Grad Norm 5.7960(12.1680) | Total Time 0.00(0.00)\n",
      "Iter 1200 | Time 14.9408(16.0924) | Bit/dim 4.1585(4.1349) | Xent 1.4313(1.4388) | Loss 10.2101(10.3039) | Error 0.5322(0.5214) Steps 0(0.00) | Grad Norm 10.4194(13.6176) | Total Time 0.00(0.00)\n",
      "Iter 1210 | Time 15.8732(15.9437) | Bit/dim 4.1117(4.1283) | Xent 1.5006(1.4375) | Loss 10.1250(10.2301) | Error 0.5244(0.5216) Steps 0(0.00) | Grad Norm 13.2116(12.5018) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0022 | Time 83.9986, Epoch Time 980.1301(814.2813), Bit/dim 4.1129(best: 4.1332), Xent 1.3250, Loss 4.7754, Error 0.4777(best: 0.4861)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1220 | Time 15.1153(15.9580) | Bit/dim 4.0957(4.1207) | Xent 1.3989(1.4217) | Loss 10.0743(10.6193) | Error 0.5089(0.5164) Steps 0(0.00) | Grad Norm 7.4414(11.7298) | Total Time 0.00(0.00)\n",
      "Iter 1230 | Time 16.2405(15.9118) | Bit/dim 4.1054(4.1121) | Xent 1.3808(1.4117) | Loss 9.7739(10.4322) | Error 0.4900(0.5120) Steps 0(0.00) | Grad Norm 23.7419(12.3577) | Total Time 0.00(0.00)\n",
      "Iter 1240 | Time 16.4829(15.8883) | Bit/dim 4.0871(4.1105) | Xent 1.4376(1.4078) | Loss 9.9844(10.2874) | Error 0.5278(0.5106) Steps 0(0.00) | Grad Norm 16.2257(13.1111) | Total Time 0.00(0.00)\n",
      "Iter 1250 | Time 16.3977(16.0222) | Bit/dim 4.0979(4.1033) | Xent 1.4356(1.4052) | Loss 10.1686(10.2105) | Error 0.5311(0.5089) Steps 0(0.00) | Grad Norm 14.3613(12.6206) | Total Time 0.00(0.00)\n",
      "Iter 1260 | Time 14.7357(15.8245) | Bit/dim 4.0771(4.0984) | Xent 1.4414(1.4074) | Loss 9.8604(10.1320) | Error 0.5211(0.5089) Steps 0(0.00) | Grad Norm 6.7464(12.8512) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0023 | Time 83.8061, Epoch Time 976.1712(819.1380), Bit/dim 4.0778(best: 4.1129), Xent 1.3158, Loss 4.7357, Error 0.4785(best: 0.4777)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1270 | Time 17.3766(15.9274) | Bit/dim 4.0735(4.0954) | Xent 1.3233(1.3943) | Loss 10.0512(10.6273) | Error 0.4922(0.5063) Steps 0(0.00) | Grad Norm 7.3646(12.6422) | Total Time 0.00(0.00)\n",
      "Iter 1280 | Time 15.4645(16.0227) | Bit/dim 4.0078(4.0877) | Xent 1.3173(1.3837) | Loss 9.5002(10.4355) | Error 0.4733(0.5025) Steps 0(0.00) | Grad Norm 6.1382(12.1748) | Total Time 0.00(0.00)\n",
      "Iter 1290 | Time 14.2986(16.0062) | Bit/dim 4.0494(4.0811) | Xent 1.3318(1.3769) | Loss 9.8481(10.3060) | Error 0.5011(0.4997) Steps 0(0.00) | Grad Norm 10.0646(12.0366) | Total Time 0.00(0.00)\n",
      "Iter 1300 | Time 14.7521(15.9350) | Bit/dim 4.0399(4.0744) | Xent 1.3263(1.3642) | Loss 9.6990(10.1756) | Error 0.4878(0.4957) Steps 0(0.00) | Grad Norm 12.0581(12.3554) | Total Time 0.00(0.00)\n",
      "Iter 1310 | Time 16.8683(15.9624) | Bit/dim 4.0843(4.0731) | Xent 1.3302(1.3614) | Loss 10.1542(10.1073) | Error 0.4878(0.4944) Steps 0(0.00) | Grad Norm 6.6647(12.2244) | Total Time 0.00(0.00)\n",
      "Iter 1320 | Time 16.0638(15.9693) | Bit/dim 4.0512(4.0697) | Xent 1.4471(1.3598) | Loss 10.0824(10.0339) | Error 0.5411(0.4950) Steps 0(0.00) | Grad Norm 23.1623(11.6317) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0024 | Time 81.8880, Epoch Time 982.2791(824.0322), Bit/dim 4.1109(best: 4.0778), Xent 1.3363, Loss 4.7791, Error 0.4829(best: 0.4777)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1330 | Time 16.5341(15.9932) | Bit/dim 3.9985(4.0689) | Xent 1.2887(1.3619) | Loss 9.8153(10.4661) | Error 0.4900(0.4958) Steps 0(0.00) | Grad Norm 5.5871(12.5047) | Total Time 0.00(0.00)\n",
      "Iter 1340 | Time 14.6559(15.9471) | Bit/dim 4.0353(4.0645) | Xent 1.3549(1.3512) | Loss 9.5528(10.2902) | Error 0.4822(0.4917) Steps 0(0.00) | Grad Norm 12.6630(11.8631) | Total Time 0.00(0.00)\n",
      "Iter 1350 | Time 16.1348(15.8397) | Bit/dim 4.0369(4.0605) | Xent 1.3986(1.3489) | Loss 9.8199(10.1808) | Error 0.5089(0.4905) Steps 0(0.00) | Grad Norm 10.6388(12.2556) | Total Time 0.00(0.00)\n",
      "Iter 1360 | Time 15.1844(15.9881) | Bit/dim 4.0602(4.0549) | Xent 1.2529(1.3367) | Loss 9.8617(10.0909) | Error 0.4389(0.4852) Steps 0(0.00) | Grad Norm 9.2052(10.9627) | Total Time 0.00(0.00)\n",
      "Iter 1370 | Time 16.4915(15.9998) | Bit/dim 4.0881(4.0710) | Xent 1.4988(1.4033) | Loss 10.1365(10.1351) | Error 0.5278(0.5053) Steps 0(0.00) | Grad Norm 12.3390(15.3697) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0025 | Time 85.3547, Epoch Time 983.1767(828.8066), Bit/dim 4.0728(best: 4.0778), Xent 1.3825, Loss 4.7640, Error 0.4977(best: 0.4777)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1380 | Time 15.9360(16.0602) | Bit/dim 4.0575(4.0706) | Xent 1.3854(1.4146) | Loss 9.9614(10.6195) | Error 0.4911(0.5111) Steps 0(0.00) | Grad Norm 9.8864(13.9027) | Total Time 0.00(0.00)\n",
      "Iter 1390 | Time 16.6989(16.0286) | Bit/dim 4.0452(4.0642) | Xent 1.3582(1.4004) | Loss 10.0542(10.4213) | Error 0.4867(0.5062) Steps 0(0.00) | Grad Norm 9.3006(12.2310) | Total Time 0.00(0.00)\n",
      "Iter 1400 | Time 15.5823(15.9914) | Bit/dim 4.0319(4.0537) | Xent 1.2945(1.3780) | Loss 9.7954(10.2498) | Error 0.4778(0.4982) Steps 0(0.00) | Grad Norm 9.0657(11.2075) | Total Time 0.00(0.00)\n",
      "Iter 1410 | Time 16.0643(15.8302) | Bit/dim 3.9946(4.0435) | Xent 1.3397(1.3628) | Loss 9.6969(10.1131) | Error 0.4867(0.4922) Steps 0(0.00) | Grad Norm 7.0824(10.0051) | Total Time 0.00(0.00)\n",
      "Iter 1420 | Time 17.7482(15.9602) | Bit/dim 4.0207(4.0395) | Xent 1.2624(1.3471) | Loss 9.9015(10.0381) | Error 0.4511(0.4841) Steps 0(0.00) | Grad Norm 11.8201(10.2983) | Total Time 0.00(0.00)\n",
      "Iter 1430 | Time 15.3989(16.0272) | Bit/dim 3.9942(4.0301) | Xent 1.3391(1.3453) | Loss 9.8685(9.9753) | Error 0.4856(0.4844) Steps 0(0.00) | Grad Norm 9.6078(9.9844) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0026 | Time 84.0078, Epoch Time 981.0824(833.3748), Bit/dim 4.0073(best: 4.0728), Xent 1.2317, Loss 4.6231, Error 0.4425(best: 0.4777)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1440 | Time 16.5195(16.0760) | Bit/dim 3.9841(4.0222) | Xent 1.2730(1.3388) | Loss 9.7272(10.3430) | Error 0.4678(0.4826) Steps 0(0.00) | Grad Norm 8.2949(10.0312) | Total Time 0.00(0.00)\n",
      "Iter 1450 | Time 15.4831(16.0094) | Bit/dim 4.0479(4.0171) | Xent 1.3264(1.3263) | Loss 9.6459(10.1717) | Error 0.4744(0.4777) Steps 0(0.00) | Grad Norm 14.5393(10.0006) | Total Time 0.00(0.00)\n",
      "Iter 1460 | Time 16.2889(15.9979) | Bit/dim 3.9850(4.0124) | Xent 1.2635(1.3148) | Loss 9.5849(10.0538) | Error 0.4556(0.4745) Steps 0(0.00) | Grad Norm 4.0469(10.3101) | Total Time 0.00(0.00)\n",
      "Iter 1470 | Time 15.7574(16.0095) | Bit/dim 3.9872(4.0055) | Xent 1.3464(1.3048) | Loss 9.9708(9.9770) | Error 0.4756(0.4713) Steps 0(0.00) | Grad Norm 9.1276(10.0760) | Total Time 0.00(0.00)\n",
      "Iter 1480 | Time 14.2477(16.0155) | Bit/dim 4.0063(4.0051) | Xent 1.3572(1.3041) | Loss 9.4187(9.9105) | Error 0.4967(0.4717) Steps 0(0.00) | Grad Norm 20.7095(10.4729) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0027 | Time 83.7845, Epoch Time 980.3546(837.7842), Bit/dim 3.9920(best: 4.0073), Xent 1.2056, Loss 4.5948, Error 0.4308(best: 0.4425)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1490 | Time 15.4382(15.8700) | Bit/dim 4.0163(4.0021) | Xent 1.2979(1.2995) | Loss 9.8504(10.3915) | Error 0.4500(0.4712) Steps 0(0.00) | Grad Norm 9.3266(10.7028) | Total Time 0.00(0.00)\n",
      "Iter 1500 | Time 18.2174(16.0869) | Bit/dim 3.9591(4.0011) | Xent 1.4155(1.3073) | Loss 9.6940(10.2229) | Error 0.5211(0.4728) Steps 0(0.00) | Grad Norm 14.3926(12.1879) | Total Time 0.00(0.00)\n",
      "Iter 1510 | Time 15.9802(16.3284) | Bit/dim 3.9782(3.9959) | Xent 1.3892(1.3063) | Loss 9.7873(10.1009) | Error 0.4967(0.4733) Steps 0(0.00) | Grad Norm 8.7688(12.1818) | Total Time 0.00(0.00)\n",
      "Iter 1520 | Time 16.2815(16.2594) | Bit/dim 3.9955(3.9941) | Xent 1.2533(1.3011) | Loss 9.6926(9.9983) | Error 0.4422(0.4694) Steps 0(0.00) | Grad Norm 11.8939(11.8873) | Total Time 0.00(0.00)\n",
      "Iter 1530 | Time 16.0193(16.1702) | Bit/dim 3.9876(3.9921) | Xent 1.3043(1.2920) | Loss 9.8201(9.9152) | Error 0.4722(0.4678) Steps 0(0.00) | Grad Norm 14.6005(12.0840) | Total Time 0.00(0.00)\n",
      "Iter 1540 | Time 15.3783(15.9965) | Bit/dim 4.0251(3.9866) | Xent 1.3252(1.2891) | Loss 9.5887(9.8263) | Error 0.4744(0.4655) Steps 0(0.00) | Grad Norm 13.1812(11.7992) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0028 | Time 82.7592, Epoch Time 989.9217(842.3483), Bit/dim 3.9753(best: 3.9920), Xent 1.2228, Loss 4.5867, Error 0.4372(best: 0.4308)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1550 | Time 15.6203(15.9990) | Bit/dim 3.9740(3.9842) | Xent 1.2560(1.2739) | Loss 9.6603(10.2536) | Error 0.4522(0.4596) Steps 0(0.00) | Grad Norm 8.1674(11.9026) | Total Time 0.00(0.00)\n",
      "Iter 1560 | Time 15.3281(15.8827) | Bit/dim 3.9995(3.9818) | Xent 1.2675(1.2660) | Loss 9.5113(10.0696) | Error 0.4511(0.4571) Steps 0(0.00) | Grad Norm 12.5038(11.1991) | Total Time 0.00(0.00)\n",
      "Iter 1570 | Time 15.0546(15.8813) | Bit/dim 3.9151(3.9760) | Xent 1.2522(1.2651) | Loss 9.5748(9.9602) | Error 0.4211(0.4565) Steps 0(0.00) | Grad Norm 11.7599(10.4339) | Total Time 0.00(0.00)\n",
      "Iter 1580 | Time 15.0728(15.8097) | Bit/dim 3.9563(3.9715) | Xent 1.3074(1.2617) | Loss 9.5494(9.8770) | Error 0.4856(0.4554) Steps 0(0.00) | Grad Norm 10.3635(10.5317) | Total Time 0.00(0.00)\n",
      "Iter 1590 | Time 15.9980(15.8414) | Bit/dim 3.9710(3.9682) | Xent 1.3954(1.2643) | Loss 9.7417(9.7999) | Error 0.4889(0.4554) Steps 0(0.00) | Grad Norm 16.9585(11.2938) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0029 | Time 83.7225, Epoch Time 969.8924(846.1747), Bit/dim 3.9577(best: 3.9753), Xent 1.1952, Loss 4.5553, Error 0.4275(best: 0.4308)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1600 | Time 14.4563(15.7589) | Bit/dim 3.9731(3.9668) | Xent 1.2162(1.2620) | Loss 9.4767(10.2763) | Error 0.4456(0.4569) Steps 0(0.00) | Grad Norm 10.2142(11.0208) | Total Time 0.00(0.00)\n",
      "Iter 1610 | Time 15.4843(15.6550) | Bit/dim 3.9678(3.9632) | Xent 1.2382(1.2546) | Loss 9.3320(10.0677) | Error 0.4389(0.4529) Steps 0(0.00) | Grad Norm 11.9801(10.9879) | Total Time 0.00(0.00)\n",
      "Iter 1620 | Time 17.7551(15.7235) | Bit/dim 3.9279(3.9586) | Xent 1.2146(1.2487) | Loss 9.4886(9.9351) | Error 0.4444(0.4516) Steps 0(0.00) | Grad Norm 7.1996(10.8240) | Total Time 0.00(0.00)\n",
      "Iter 1630 | Time 17.4564(15.8040) | Bit/dim 3.9331(3.9548) | Xent 1.2794(1.2432) | Loss 9.5208(9.8326) | Error 0.4489(0.4492) Steps 0(0.00) | Grad Norm 14.1566(10.6707) | Total Time 0.00(0.00)\n",
      "Iter 1640 | Time 15.0068(15.7732) | Bit/dim 3.9555(3.9555) | Xent 1.3752(1.2526) | Loss 9.5737(9.7654) | Error 0.5067(0.4536) Steps 0(0.00) | Grad Norm 13.5330(11.6451) | Total Time 0.00(0.00)\n",
      "Iter 1650 | Time 16.5486(15.8343) | Bit/dim 3.9491(3.9522) | Xent 1.3280(1.2533) | Loss 9.6564(9.7125) | Error 0.4856(0.4526) Steps 0(0.00) | Grad Norm 11.1380(11.8173) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0030 | Time 83.2726, Epoch Time 969.2681(849.8675), Bit/dim 3.9538(best: 3.9577), Xent 1.1648, Loss 4.5362, Error 0.4205(best: 0.4275)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1660 | Time 14.8722(15.8293) | Bit/dim 3.9377(3.9498) | Xent 1.1469(1.2421) | Loss 9.4147(10.1098) | Error 0.4244(0.4495) Steps 0(0.00) | Grad Norm 5.1853(11.0460) | Total Time 0.00(0.00)\n",
      "Iter 1670 | Time 15.6129(15.7339) | Bit/dim 3.9347(3.9456) | Xent 1.2320(1.2317) | Loss 9.5299(9.9237) | Error 0.4489(0.4468) Steps 0(0.00) | Grad Norm 12.1624(10.5514) | Total Time 0.00(0.00)\n",
      "Iter 1680 | Time 15.2586(15.6384) | Bit/dim 3.9467(3.9432) | Xent 1.2406(1.2391) | Loss 9.4266(9.8301) | Error 0.4489(0.4488) Steps 0(0.00) | Grad Norm 6.7871(10.5866) | Total Time 0.00(0.00)\n",
      "Iter 1690 | Time 15.9072(15.6687) | Bit/dim 3.9091(3.9390) | Xent 1.2291(1.2315) | Loss 9.5345(9.7441) | Error 0.4511(0.4452) Steps 0(0.00) | Grad Norm 14.9296(10.2439) | Total Time 0.00(0.00)\n",
      "Iter 1700 | Time 14.0555(15.6931) | Bit/dim 3.9027(3.9380) | Xent 1.1683(1.2290) | Loss 9.2504(9.6717) | Error 0.3956(0.4427) Steps 0(0.00) | Grad Norm 11.5132(10.5970) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0031 | Time 82.5303, Epoch Time 966.3498(853.3619), Bit/dim 3.9266(best: 3.9538), Xent 1.1731, Loss 4.5131, Error 0.4226(best: 0.4205)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1710 | Time 16.1248(15.7655) | Bit/dim 3.9768(3.9371) | Xent 1.2885(1.2340) | Loss 9.7892(10.1554) | Error 0.4644(0.4453) Steps 0(0.00) | Grad Norm 15.5025(11.7379) | Total Time 0.00(0.00)\n",
      "Iter 1720 | Time 13.8326(15.7064) | Bit/dim 3.9321(3.9359) | Xent 1.1510(1.2286) | Loss 9.3695(9.9803) | Error 0.4156(0.4431) Steps 0(0.00) | Grad Norm 12.9726(11.4566) | Total Time 0.00(0.00)\n",
      "Iter 1730 | Time 16.1145(15.7278) | Bit/dim 3.9032(3.9315) | Xent 1.2057(1.2219) | Loss 9.5514(9.8535) | Error 0.4478(0.4410) Steps 0(0.00) | Grad Norm 6.3507(10.5496) | Total Time 0.00(0.00)\n",
      "Iter 1740 | Time 15.8525(15.7400) | Bit/dim 3.9253(3.9294) | Xent 1.2286(1.2139) | Loss 9.3746(9.7506) | Error 0.4444(0.4378) Steps 0(0.00) | Grad Norm 8.1069(10.0390) | Total Time 0.00(0.00)\n",
      "Iter 1750 | Time 15.0395(15.7203) | Bit/dim 3.9360(3.9275) | Xent 1.1509(1.2104) | Loss 9.4464(9.6764) | Error 0.4144(0.4372) Steps 0(0.00) | Grad Norm 16.7965(10.4275) | Total Time 0.00(0.00)\n",
      "Iter 1760 | Time 15.4725(15.7291) | Bit/dim 3.9004(3.9229) | Xent 1.1996(1.2057) | Loss 9.2086(9.6134) | Error 0.4267(0.4346) Steps 0(0.00) | Grad Norm 10.6639(10.5459) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0032 | Time 83.7553, Epoch Time 964.9602(856.7099), Bit/dim 3.9184(best: 3.9266), Xent 1.1695, Loss 4.5031, Error 0.4179(best: 0.4205)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1770 | Time 15.9633(15.7579) | Bit/dim 3.9041(3.9214) | Xent 1.1589(1.2009) | Loss 9.3220(10.0325) | Error 0.4089(0.4332) Steps 0(0.00) | Grad Norm 5.7974(10.5674) | Total Time 0.00(0.00)\n",
      "Iter 1780 | Time 14.9329(15.7277) | Bit/dim 3.9297(3.9202) | Xent 1.2346(1.1939) | Loss 9.5029(9.8768) | Error 0.4389(0.4302) Steps 0(0.00) | Grad Norm 9.4601(10.3811) | Total Time 0.00(0.00)\n",
      "Iter 1790 | Time 14.5074(15.6885) | Bit/dim 3.9357(3.9203) | Xent 1.2800(1.1933) | Loss 9.3735(9.7490) | Error 0.4533(0.4305) Steps 0(0.00) | Grad Norm 16.7383(10.6860) | Total Time 0.00(0.00)\n",
      "Iter 1800 | Time 15.0886(15.6475) | Bit/dim 3.9324(3.9240) | Xent 1.2480(1.2055) | Loss 9.4501(9.6868) | Error 0.4278(0.4340) Steps 0(0.00) | Grad Norm 15.1422(12.3174) | Total Time 0.00(0.00)\n",
      "Iter 1810 | Time 15.8936(15.6440) | Bit/dim 3.8919(3.9192) | Xent 1.2594(1.2099) | Loss 9.5141(9.5987) | Error 0.4378(0.4355) Steps 0(0.00) | Grad Norm 12.0026(12.3678) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0033 | Time 81.3971, Epoch Time 956.8587(859.7143), Bit/dim 3.9031(best: 3.9184), Xent 1.1200, Loss 4.4631, Error 0.4048(best: 0.4179)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1820 | Time 15.0365(15.5681) | Bit/dim 3.9226(3.9149) | Xent 1.2041(1.2074) | Loss 9.6037(10.0607) | Error 0.4389(0.4346) Steps 0(0.00) | Grad Norm 11.0754(11.5980) | Total Time 0.00(0.00)\n",
      "Iter 1830 | Time 13.9238(15.4704) | Bit/dim 3.9114(3.9135) | Xent 1.1885(1.1951) | Loss 9.3993(9.8884) | Error 0.4233(0.4325) Steps 0(0.00) | Grad Norm 13.7157(11.0247) | Total Time 0.00(0.00)\n",
      "Iter 1840 | Time 15.4692(15.4932) | Bit/dim 3.9146(3.9115) | Xent 1.1927(1.1911) | Loss 8.9576(9.7442) | Error 0.4233(0.4301) Steps 0(0.00) | Grad Norm 7.7975(11.1755) | Total Time 0.00(0.00)\n",
      "Iter 1850 | Time 17.5271(15.6312) | Bit/dim 3.8744(3.9077) | Xent 1.2477(1.1908) | Loss 9.5841(9.6588) | Error 0.4556(0.4296) Steps 0(0.00) | Grad Norm 10.5793(10.7397) | Total Time 0.00(0.00)\n",
      "Iter 1860 | Time 16.2858(15.6273) | Bit/dim 3.8757(3.9032) | Xent 1.1697(1.1917) | Loss 9.2453(9.5732) | Error 0.4378(0.4291) Steps 0(0.00) | Grad Norm 12.0631(10.2624) | Total Time 0.00(0.00)\n",
      "Iter 1870 | Time 15.7363(15.7558) | Bit/dim 3.9194(3.9018) | Xent 1.0749(1.1794) | Loss 9.2690(9.5253) | Error 0.3856(0.4234) Steps 0(0.00) | Grad Norm 4.5400(10.0089) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0034 | Time 81.3987, Epoch Time 964.5642(862.8598), Bit/dim 3.8877(best: 3.9031), Xent 1.1026, Loss 4.4390, Error 0.3935(best: 0.4048)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1880 | Time 16.0787(15.6231) | Bit/dim 3.8952(3.8972) | Xent 1.1900(1.1717) | Loss 9.3945(9.9122) | Error 0.4300(0.4206) Steps 0(0.00) | Grad Norm 11.2975(9.7703) | Total Time 0.00(0.00)\n",
      "Iter 1890 | Time 15.7984(15.6195) | Bit/dim 3.8836(3.8927) | Xent 1.0959(1.1622) | Loss 9.1499(9.7474) | Error 0.3922(0.4167) Steps 0(0.00) | Grad Norm 9.2537(9.5568) | Total Time 0.00(0.00)\n",
      "Iter 1900 | Time 14.3158(15.7142) | Bit/dim 3.8591(3.8918) | Xent 1.1854(1.1660) | Loss 9.0753(9.6476) | Error 0.4189(0.4171) Steps 0(0.00) | Grad Norm 6.9032(10.1275) | Total Time 0.00(0.00)\n",
      "Iter 1910 | Time 15.1111(15.7163) | Bit/dim 3.9022(3.8919) | Xent 1.1755(1.1647) | Loss 9.5460(9.5738) | Error 0.4211(0.4164) Steps 0(0.00) | Grad Norm 6.6464(9.9745) | Total Time 0.00(0.00)\n",
      "Iter 1920 | Time 16.0549(15.7720) | Bit/dim 3.8969(3.8930) | Xent 1.1564(1.1638) | Loss 9.3300(9.5174) | Error 0.3967(0.4160) Steps 0(0.00) | Grad Norm 7.6477(9.9068) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0035 | Time 81.6570, Epoch Time 966.7874(865.9777), Bit/dim 3.8885(best: 3.8877), Xent 1.1261, Loss 4.4516, Error 0.4037(best: 0.3935)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1930 | Time 16.5559(15.9653) | Bit/dim 3.9112(3.8943) | Xent 1.1676(1.1599) | Loss 9.4391(10.0016) | Error 0.4333(0.4142) Steps 0(0.00) | Grad Norm 10.6685(10.7816) | Total Time 0.00(0.00)\n",
      "Iter 1940 | Time 16.1372(15.9414) | Bit/dim 3.8801(3.8900) | Xent 1.1210(1.1528) | Loss 9.2246(9.8149) | Error 0.4211(0.4138) Steps 0(0.00) | Grad Norm 8.0992(10.6182) | Total Time 0.00(0.00)\n",
      "Iter 1950 | Time 15.3938(15.7171) | Bit/dim 3.8766(3.8892) | Xent 1.1307(1.1440) | Loss 9.3882(9.6763) | Error 0.4122(0.4117) Steps 0(0.00) | Grad Norm 9.8845(10.7898) | Total Time 0.00(0.00)\n",
      "Iter 1960 | Time 16.3705(15.5752) | Bit/dim 3.8425(3.8854) | Xent 1.1691(1.1517) | Loss 9.2967(9.5841) | Error 0.4356(0.4148) Steps 0(0.00) | Grad Norm 12.5090(11.2957) | Total Time 0.00(0.00)\n",
      "Iter 1970 | Time 16.6639(15.6666) | Bit/dim 3.8598(3.8807) | Xent 1.1470(1.1542) | Loss 9.2237(9.5045) | Error 0.4111(0.4160) Steps 0(0.00) | Grad Norm 4.7291(11.4607) | Total Time 0.00(0.00)\n",
      "Iter 1980 | Time 16.5852(15.7661) | Bit/dim 3.8676(3.8810) | Xent 1.1540(1.1510) | Loss 9.3865(9.4428) | Error 0.4056(0.4148) Steps 0(0.00) | Grad Norm 14.2334(11.1114) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0036 | Time 81.0294, Epoch Time 961.9811(868.8578), Bit/dim 3.8894(best: 3.8877), Xent 1.1403, Loss 4.4596, Error 0.4113(best: 0.3935)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1990 | Time 15.9528(15.7665) | Bit/dim 3.8644(3.8790) | Xent 1.1200(1.1452) | Loss 9.5066(9.8545) | Error 0.3989(0.4114) Steps 0(0.00) | Grad Norm 8.0978(11.8901) | Total Time 0.00(0.00)\n",
      "Iter 2000 | Time 16.8492(15.7966) | Bit/dim 3.8894(3.8805) | Xent 1.1590(1.1446) | Loss 9.4685(9.7303) | Error 0.4156(0.4106) Steps 0(0.00) | Grad Norm 10.8856(10.8987) | Total Time 0.00(0.00)\n",
      "Iter 2010 | Time 13.9802(15.7913) | Bit/dim 3.9390(3.8827) | Xent 1.1566(1.1462) | Loss 9.2864(9.6268) | Error 0.4167(0.4120) Steps 0(0.00) | Grad Norm 16.9864(11.8646) | Total Time 0.00(0.00)\n",
      "Iter 2020 | Time 15.5865(15.6780) | Bit/dim 3.8349(3.8819) | Xent 1.0741(1.1432) | Loss 9.1914(9.5351) | Error 0.3967(0.4120) Steps 0(0.00) | Grad Norm 12.3930(11.6375) | Total Time 0.00(0.00)\n",
      "Iter 2030 | Time 15.0081(15.5740) | Bit/dim 3.9027(3.8801) | Xent 1.1664(1.1431) | Loss 9.0905(9.4714) | Error 0.4111(0.4113) Steps 0(0.00) | Grad Norm 8.6288(10.7099) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0037 | Time 80.6678, Epoch Time 963.6683(871.7021), Bit/dim 3.8694(best: 3.8877), Xent 1.0619, Loss 4.4004, Error 0.3827(best: 0.3935)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2040 | Time 15.7799(15.8245) | Bit/dim 3.8890(3.8760) | Xent 1.0992(1.1391) | Loss 9.3977(9.9291) | Error 0.3789(0.4092) Steps 0(0.00) | Grad Norm 5.1728(10.5117) | Total Time 0.00(0.00)\n",
      "Iter 2050 | Time 15.2910(15.7929) | Bit/dim 3.8382(3.8706) | Xent 1.1100(1.1304) | Loss 9.3016(9.7443) | Error 0.4011(0.4067) Steps 0(0.00) | Grad Norm 6.4295(10.1389) | Total Time 0.00(0.00)\n",
      "Iter 2060 | Time 15.7823(15.8768) | Bit/dim 3.8625(3.8710) | Xent 1.0617(1.1176) | Loss 9.2063(9.6326) | Error 0.3733(0.4009) Steps 0(0.00) | Grad Norm 7.0772(9.4152) | Total Time 0.00(0.00)\n",
      "Iter 2070 | Time 15.3112(15.8620) | Bit/dim 3.8780(3.8674) | Xent 1.1175(1.1098) | Loss 9.2051(9.5208) | Error 0.4144(0.3982) Steps 0(0.00) | Grad Norm 16.3900(9.5131) | Total Time 0.00(0.00)\n",
      "Iter 2080 | Time 14.8029(15.9151) | Bit/dim 3.8854(3.8673) | Xent 1.2220(1.1149) | Loss 9.2605(9.4603) | Error 0.4378(0.3989) Steps 0(0.00) | Grad Norm 14.7862(10.0879) | Total Time 0.00(0.00)\n",
      "Iter 2090 | Time 16.3602(15.8885) | Bit/dim 3.8777(3.8665) | Xent 1.0950(1.1158) | Loss 9.4132(9.4304) | Error 0.3944(0.3992) Steps 0(0.00) | Grad Norm 6.3143(10.2694) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0038 | Time 83.4479, Epoch Time 978.9843(874.9206), Bit/dim 3.8637(best: 3.8694), Xent 1.0579, Loss 4.3927, Error 0.3805(best: 0.3827)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2100 | Time 16.5258(15.8200) | Bit/dim 3.8746(3.8646) | Xent 1.0472(1.1098) | Loss 9.2973(9.8478) | Error 0.3778(0.3981) Steps 0(0.00) | Grad Norm 7.0697(9.5963) | Total Time 0.00(0.00)\n",
      "Iter 2110 | Time 15.8277(15.8748) | Bit/dim 3.8480(3.8637) | Xent 1.0865(1.1109) | Loss 9.1921(9.6697) | Error 0.3867(0.3986) Steps 0(0.00) | Grad Norm 14.2856(10.3333) | Total Time 0.00(0.00)\n",
      "Iter 2120 | Time 15.5957(15.8192) | Bit/dim 3.8035(3.8625) | Xent 1.0840(1.1034) | Loss 9.2235(9.5592) | Error 0.3778(0.3942) Steps 0(0.00) | Grad Norm 8.6665(10.1347) | Total Time 0.00(0.00)\n",
      "Iter 2130 | Time 16.9382(15.7613) | Bit/dim 3.8137(3.8598) | Xent 1.1349(1.0919) | Loss 9.1672(9.4654) | Error 0.3933(0.3908) Steps 0(0.00) | Grad Norm 8.0245(9.7825) | Total Time 0.00(0.00)\n",
      "Iter 2140 | Time 15.4845(15.6376) | Bit/dim 3.8526(3.8572) | Xent 1.0727(1.0849) | Loss 9.0355(9.3884) | Error 0.3711(0.3877) Steps 0(0.00) | Grad Norm 5.6376(9.2351) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0039 | Time 82.4810, Epoch Time 960.7450(877.4953), Bit/dim 3.8699(best: 3.8637), Xent 1.0539, Loss 4.3969, Error 0.3766(best: 0.3805)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2150 | Time 17.3761(15.7748) | Bit/dim 3.8729(3.8586) | Xent 1.1535(1.1012) | Loss 9.3121(9.8866) | Error 0.4189(0.3948) Steps 0(0.00) | Grad Norm 11.2732(10.0769) | Total Time 0.00(0.00)\n",
      "Iter 2160 | Time 15.9786(15.9659) | Bit/dim 3.8727(3.8592) | Xent 1.0421(1.0981) | Loss 9.2303(9.7308) | Error 0.3900(0.3928) Steps 0(0.00) | Grad Norm 13.7316(10.5735) | Total Time 0.00(0.00)\n",
      "Iter 2170 | Time 16.6021(15.9760) | Bit/dim 3.8397(3.8592) | Xent 1.0666(1.0973) | Loss 9.2112(9.5934) | Error 0.3867(0.3936) Steps 0(0.00) | Grad Norm 6.7171(10.3242) | Total Time 0.00(0.00)\n",
      "Iter 2180 | Time 16.0308(15.9681) | Bit/dim 3.8343(3.8565) | Xent 1.1084(1.0935) | Loss 9.2543(9.4867) | Error 0.4178(0.3932) Steps 0(0.00) | Grad Norm 8.2411(9.9120) | Total Time 0.00(0.00)\n",
      "Iter 2190 | Time 16.1838(15.8437) | Bit/dim 3.8607(3.8556) | Xent 1.1175(1.0884) | Loss 9.3995(9.4324) | Error 0.4022(0.3918) Steps 0(0.00) | Grad Norm 8.7603(10.3924) | Total Time 0.00(0.00)\n",
      "Iter 2200 | Time 14.7561(15.9875) | Bit/dim 3.8529(3.8539) | Xent 1.0070(1.0805) | Loss 9.0500(9.3718) | Error 0.3422(0.3887) Steps 0(0.00) | Grad Norm 9.7753(10.4330) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0040 | Time 83.5054, Epoch Time 987.7211(880.8021), Bit/dim 3.8471(best: 3.8637), Xent 0.9976, Loss 4.3459, Error 0.3519(best: 0.3766)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2210 | Time 15.4444(15.9059) | Bit/dim 3.8502(3.8522) | Xent 1.1499(1.0712) | Loss 9.1837(9.7390) | Error 0.4144(0.3850) Steps 0(0.00) | Grad Norm 11.8070(10.0253) | Total Time 0.00(0.00)\n",
      "Iter 2220 | Time 14.7365(15.8172) | Bit/dim 3.8616(3.8490) | Xent 1.0311(1.0591) | Loss 8.9683(9.5657) | Error 0.3478(0.3806) Steps 0(0.00) | Grad Norm 12.1882(10.0460) | Total Time 0.00(0.00)\n",
      "Iter 2230 | Time 15.4087(15.8611) | Bit/dim 3.8497(3.8471) | Xent 1.1289(1.0636) | Loss 9.2799(9.4779) | Error 0.4033(0.3799) Steps 0(0.00) | Grad Norm 9.3712(10.3135) | Total Time 0.00(0.00)\n",
      "Iter 2240 | Time 14.4746(15.7266) | Bit/dim 3.8336(3.8451) | Xent 1.0601(1.0550) | Loss 8.8569(9.3625) | Error 0.3911(0.3775) Steps 0(0.00) | Grad Norm 8.9955(10.3006) | Total Time 0.00(0.00)\n",
      "Iter 2250 | Time 16.2770(15.6939) | Bit/dim 3.8634(3.8448) | Xent 1.0640(1.0480) | Loss 9.2689(9.3267) | Error 0.3689(0.3752) Steps 0(0.00) | Grad Norm 6.6367(9.7708) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0041 | Time 83.2385, Epoch Time 961.7680(883.2310), Bit/dim 3.8431(best: 3.8471), Xent 1.0331, Loss 4.3597, Error 0.3759(best: 0.3519)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2260 | Time 14.8038(15.5874) | Bit/dim 3.8393(3.8444) | Xent 1.0712(1.0547) | Loss 9.0846(9.7998) | Error 0.3967(0.3761) Steps 0(0.00) | Grad Norm 11.1630(9.5814) | Total Time 0.00(0.00)\n",
      "Iter 2270 | Time 15.9503(15.6081) | Bit/dim 3.8315(3.8410) | Xent 0.9474(1.0455) | Loss 8.9223(9.6178) | Error 0.3411(0.3740) Steps 0(0.00) | Grad Norm 7.8572(8.9725) | Total Time 0.00(0.00)\n",
      "Iter 2280 | Time 14.7159(15.6304) | Bit/dim 3.8587(3.8411) | Xent 1.0568(1.0537) | Loss 9.2593(9.5009) | Error 0.3756(0.3775) Steps 0(0.00) | Grad Norm 13.2366(10.3522) | Total Time 0.00(0.00)\n",
      "Iter 2290 | Time 15.4609(15.5252) | Bit/dim 3.8509(3.8380) | Xent 1.0257(1.0494) | Loss 9.0691(9.3946) | Error 0.3678(0.3760) Steps 0(0.00) | Grad Norm 5.5088(9.9142) | Total Time 0.00(0.00)\n",
      "Iter 2300 | Time 16.2156(15.6550) | Bit/dim 3.8031(3.8343) | Xent 1.0204(1.0456) | Loss 8.9478(9.3209) | Error 0.3800(0.3736) Steps 0(0.00) | Grad Norm 7.6452(9.4341) | Total Time 0.00(0.00)\n",
      "Iter 2310 | Time 15.9721(15.6732) | Bit/dim 3.8279(3.8303) | Xent 1.0356(1.0382) | Loss 9.2004(9.2713) | Error 0.3711(0.3704) Steps 0(0.00) | Grad Norm 6.8831(9.0946) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0042 | Time 80.8888, Epoch Time 957.4924(885.4589), Bit/dim 3.8351(best: 3.8431), Xent 1.0063, Loss 4.3382, Error 0.3614(best: 0.3519)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2320 | Time 15.9153(15.7503) | Bit/dim 3.8453(3.8283) | Xent 1.0016(1.0293) | Loss 9.1252(9.6648) | Error 0.3644(0.3681) Steps 0(0.00) | Grad Norm 9.3303(9.4775) | Total Time 0.00(0.00)\n",
      "Iter 2330 | Time 17.6517(15.7476) | Bit/dim 3.8154(3.8262) | Xent 1.0155(1.0205) | Loss 9.3351(9.5040) | Error 0.3800(0.3666) Steps 0(0.00) | Grad Norm 11.3916(9.4796) | Total Time 0.00(0.00)\n",
      "Iter 2340 | Time 14.5253(15.8162) | Bit/dim 3.8242(3.8301) | Xent 1.0828(1.0189) | Loss 9.2362(9.4182) | Error 0.3733(0.3656) Steps 0(0.00) | Grad Norm 10.5503(9.8366) | Total Time 0.00(0.00)\n",
      "Iter 2350 | Time 17.1166(16.0028) | Bit/dim 3.8531(3.8320) | Xent 1.0425(1.0215) | Loss 9.2867(9.3543) | Error 0.3589(0.3651) Steps 0(0.00) | Grad Norm 12.9825(10.0657) | Total Time 0.00(0.00)\n",
      "Iter 2360 | Time 16.1744(15.9100) | Bit/dim 3.8182(3.8300) | Xent 1.0537(1.0214) | Loss 9.0777(9.2979) | Error 0.3578(0.3646) Steps 0(0.00) | Grad Norm 14.8215(10.0747) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0043 | Time 82.9221, Epoch Time 978.5893(888.2528), Bit/dim 3.8254(best: 3.8351), Xent 0.9610, Loss 4.3059, Error 0.3344(best: 0.3519)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2370 | Time 15.6013(15.8487) | Bit/dim 3.8073(3.8300) | Xent 0.9764(1.0185) | Loss 9.1614(9.7745) | Error 0.3422(0.3632) Steps 0(0.00) | Grad Norm 9.5269(9.5567) | Total Time 0.00(0.00)\n",
      "Iter 2380 | Time 15.7788(15.6219) | Bit/dim 3.8252(3.8297) | Xent 0.9810(1.0084) | Loss 9.2427(9.5868) | Error 0.3589(0.3587) Steps 0(0.00) | Grad Norm 7.7453(9.4025) | Total Time 0.00(0.00)\n",
      "Iter 2390 | Time 14.7610(15.6496) | Bit/dim 3.8380(3.8276) | Xent 0.9567(1.0094) | Loss 9.0383(9.4690) | Error 0.3589(0.3589) Steps 0(0.00) | Grad Norm 9.9698(9.9116) | Total Time 0.00(0.00)\n",
      "Iter 2400 | Time 15.2256(15.6120) | Bit/dim 3.8031(3.8265) | Xent 0.9247(1.0021) | Loss 9.0175(9.3575) | Error 0.3333(0.3557) Steps 0(0.00) | Grad Norm 6.9591(9.4863) | Total Time 0.00(0.00)\n",
      "Iter 2410 | Time 15.5979(15.6318) | Bit/dim 3.8236(3.8243) | Xent 1.0305(1.0059) | Loss 9.0565(9.2661) | Error 0.3722(0.3580) Steps 0(0.00) | Grad Norm 11.0161(10.1431) | Total Time 0.00(0.00)\n",
      "Iter 2420 | Time 15.8705(15.6273) | Bit/dim 3.8142(3.8221) | Xent 1.0028(1.0105) | Loss 9.1647(9.2333) | Error 0.3456(0.3597) Steps 0(0.00) | Grad Norm 9.4467(10.4124) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0044 | Time 83.5880, Epoch Time 955.9558(890.2839), Bit/dim 3.8229(best: 3.8254), Xent 0.9653, Loss 4.3056, Error 0.3445(best: 0.3344)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2430 | Time 15.6774(15.5109) | Bit/dim 3.7979(3.8203) | Xent 0.9959(1.0052) | Loss 9.2085(9.6429) | Error 0.3667(0.3583) Steps 0(0.00) | Grad Norm 7.5822(10.0258) | Total Time 0.00(0.00)\n",
      "Iter 2440 | Time 15.7426(15.5549) | Bit/dim 3.7960(3.8186) | Xent 1.0095(0.9965) | Loss 9.0639(9.4904) | Error 0.3611(0.3562) Steps 0(0.00) | Grad Norm 12.5347(9.7259) | Total Time 0.00(0.00)\n",
      "Iter 2450 | Time 16.0353(15.4930) | Bit/dim 3.7726(3.8153) | Xent 1.0253(0.9986) | Loss 9.0440(9.3867) | Error 0.3633(0.3561) Steps 0(0.00) | Grad Norm 6.2571(9.9216) | Total Time 0.00(0.00)\n",
      "Iter 2460 | Time 15.2216(15.4491) | Bit/dim 3.8521(3.8181) | Xent 1.0185(0.9957) | Loss 9.2411(9.3117) | Error 0.3700(0.3557) Steps 0(0.00) | Grad Norm 12.6809(9.8862) | Total Time 0.00(0.00)\n",
      "Iter 2470 | Time 16.7808(15.5828) | Bit/dim 3.8134(3.8168) | Xent 0.9918(0.9926) | Loss 8.9745(9.2267) | Error 0.3633(0.3547) Steps 0(0.00) | Grad Norm 15.4978(9.3283) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0045 | Time 83.5224, Epoch Time 953.4756(892.1796), Bit/dim 3.8052(best: 3.8229), Xent 0.9558, Loss 4.2831, Error 0.3370(best: 0.3344)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2480 | Time 15.5300(15.5535) | Bit/dim 3.8279(3.8179) | Xent 1.0087(0.9847) | Loss 8.7575(9.7031) | Error 0.3711(0.3521) Steps 0(0.00) | Grad Norm 9.8213(8.9107) | Total Time 0.00(0.00)\n",
      "Iter 2490 | Time 15.0230(15.4716) | Bit/dim 3.8306(3.8183) | Xent 1.1130(0.9904) | Loss 9.2213(9.5378) | Error 0.3978(0.3544) Steps 0(0.00) | Grad Norm 18.1363(9.7598) | Total Time 0.00(0.00)\n",
      "Iter 2500 | Time 14.6963(15.4409) | Bit/dim 3.8246(3.8174) | Xent 0.9845(0.9880) | Loss 9.0680(9.4009) | Error 0.3378(0.3525) Steps 0(0.00) | Grad Norm 12.7615(9.8557) | Total Time 0.00(0.00)\n",
      "Iter 2510 | Time 15.7421(15.6029) | Bit/dim 3.8556(3.8190) | Xent 1.0931(0.9959) | Loss 9.4791(9.3350) | Error 0.3833(0.3543) Steps 0(0.00) | Grad Norm 21.8009(10.6676) | Total Time 0.00(0.00)\n",
      "Iter 2520 | Time 15.0426(15.4181) | Bit/dim 3.8202(3.8194) | Xent 1.0732(1.0088) | Loss 9.0865(9.2736) | Error 0.3933(0.3594) Steps 0(0.00) | Grad Norm 10.0067(11.1946) | Total Time 0.00(0.00)\n",
      "Iter 2530 | Time 17.4310(15.6093) | Bit/dim 3.7352(3.8165) | Xent 1.1227(1.0095) | Loss 9.0948(9.2159) | Error 0.3911(0.3593) Steps 0(0.00) | Grad Norm 14.8327(10.9970) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0046 | Time 83.5203, Epoch Time 955.2035(894.0704), Bit/dim 3.8264(best: 3.8052), Xent 1.0322, Loss 4.3424, Error 0.3623(best: 0.3344)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2540 | Time 16.8618(15.6011) | Bit/dim 3.8256(3.8166) | Xent 0.9919(1.0069) | Loss 8.8364(9.6342) | Error 0.3511(0.3588) Steps 0(0.00) | Grad Norm 7.2070(10.8199) | Total Time 0.00(0.00)\n",
      "Iter 2550 | Time 15.3626(15.5023) | Bit/dim 3.8488(3.8191) | Xent 0.9373(0.9992) | Loss 9.1834(9.4894) | Error 0.3300(0.3576) Steps 0(0.00) | Grad Norm 10.6971(10.7830) | Total Time 0.00(0.00)\n",
      "Iter 2560 | Time 17.1017(15.5036) | Bit/dim 3.8322(3.8147) | Xent 0.9792(0.9947) | Loss 9.1258(9.3570) | Error 0.3178(0.3539) Steps 0(0.00) | Grad Norm 7.8970(10.1964) | Total Time 0.00(0.00)\n",
      "Iter 2570 | Time 15.2278(15.5534) | Bit/dim 3.8517(3.8140) | Xent 0.9587(0.9861) | Loss 8.8950(9.2691) | Error 0.3378(0.3502) Steps 0(0.00) | Grad Norm 7.9038(9.6724) | Total Time 0.00(0.00)\n",
      "Iter 2580 | Time 16.3430(15.8150) | Bit/dim 3.8017(3.8096) | Xent 1.0404(0.9915) | Loss 9.2168(9.2280) | Error 0.3389(0.3509) Steps 0(0.00) | Grad Norm 14.0857(9.5412) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0047 | Time 82.2143, Epoch Time 968.4380(896.3014), Bit/dim 3.8089(best: 3.8052), Xent 1.0375, Loss 4.3276, Error 0.3632(best: 0.3344)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2590 | Time 16.5815(15.9205) | Bit/dim 3.7543(3.8043) | Xent 1.0628(0.9991) | Loss 9.0847(9.7283) | Error 0.3922(0.3532) Steps 0(0.00) | Grad Norm 9.6835(10.1316) | Total Time 0.00(0.00)\n",
      "Iter 2600 | Time 15.9263(15.8322) | Bit/dim 3.8092(3.8048) | Xent 0.9747(0.9945) | Loss 8.7762(9.5123) | Error 0.3356(0.3526) Steps 0(0.00) | Grad Norm 6.5133(9.7946) | Total Time 0.00(0.00)\n",
      "Iter 2610 | Time 15.8208(15.7740) | Bit/dim 3.7849(3.8041) | Xent 0.9240(0.9803) | Loss 9.1244(9.3742) | Error 0.3300(0.3482) Steps 0(0.00) | Grad Norm 6.5762(8.8215) | Total Time 0.00(0.00)\n",
      "Iter 2620 | Time 14.7530(15.6453) | Bit/dim 3.8417(3.8041) | Xent 0.9571(0.9770) | Loss 8.9836(9.2805) | Error 0.3400(0.3463) Steps 0(0.00) | Grad Norm 9.5123(9.3185) | Total Time 0.00(0.00)\n",
      "Iter 2630 | Time 14.8834(15.6569) | Bit/dim 3.8017(3.8045) | Xent 0.9130(0.9799) | Loss 8.9446(9.2009) | Error 0.3356(0.3481) Steps 0(0.00) | Grad Norm 10.7482(9.6079) | Total Time 0.00(0.00)\n",
      "Iter 2640 | Time 14.5080(15.6483) | Bit/dim 3.7497(3.7989) | Xent 0.9260(0.9745) | Loss 8.7163(9.1286) | Error 0.3456(0.3469) Steps 0(0.00) | Grad Norm 4.9968(9.1333) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0048 | Time 84.7751, Epoch Time 961.4199(898.2549), Bit/dim 3.7931(best: 3.8052), Xent 0.9240, Loss 4.2551, Error 0.3281(best: 0.3344)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2650 | Time 17.3668(15.5806) | Bit/dim 3.7715(3.7996) | Xent 0.9237(0.9630) | Loss 9.1705(9.5388) | Error 0.3322(0.3437) Steps 0(0.00) | Grad Norm 11.0223(9.1042) | Total Time 0.00(0.00)\n",
      "Iter 2660 | Time 16.8855(15.7543) | Bit/dim 3.7798(3.8004) | Xent 0.9814(0.9674) | Loss 9.0735(9.4115) | Error 0.3644(0.3458) Steps 0(0.00) | Grad Norm 9.0217(9.1541) | Total Time 0.00(0.00)\n",
      "Iter 2670 | Time 19.3698(15.7851) | Bit/dim 3.7915(3.7990) | Xent 0.9588(0.9611) | Loss 9.0144(9.3030) | Error 0.3344(0.3434) Steps 0(0.00) | Grad Norm 10.9651(9.7859) | Total Time 0.00(0.00)\n",
      "Iter 2680 | Time 16.7816(15.8177) | Bit/dim 3.8017(3.7958) | Xent 0.9442(0.9561) | Loss 9.0684(9.2222) | Error 0.3367(0.3411) Steps 0(0.00) | Grad Norm 8.5959(9.7541) | Total Time 0.00(0.00)\n",
      "Iter 2690 | Time 16.0408(15.8629) | Bit/dim 3.7723(3.7915) | Xent 0.9595(0.9588) | Loss 9.0621(9.1590) | Error 0.3622(0.3416) Steps 0(0.00) | Grad Norm 11.3525(9.2724) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0049 | Time 83.8756, Epoch Time 970.8119(900.4316), Bit/dim 3.7849(best: 3.7931), Xent 0.9093, Loss 4.2396, Error 0.3199(best: 0.3281)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2700 | Time 16.4465(15.8883) | Bit/dim 3.7921(3.7933) | Xent 0.9383(0.9606) | Loss 9.0304(9.6657) | Error 0.3167(0.3422) Steps 0(0.00) | Grad Norm 10.6612(9.7773) | Total Time 0.00(0.00)\n",
      "Iter 2710 | Time 16.6395(15.9142) | Bit/dim 3.7855(3.7887) | Xent 0.9194(0.9519) | Loss 9.0665(9.4738) | Error 0.3444(0.3411) Steps 0(0.00) | Grad Norm 9.3667(9.4664) | Total Time 0.00(0.00)\n",
      "Iter 2720 | Time 15.2311(15.7469) | Bit/dim 3.8269(3.7915) | Xent 0.9593(0.9470) | Loss 9.1209(9.3461) | Error 0.3278(0.3383) Steps 0(0.00) | Grad Norm 15.6804(9.3721) | Total Time 0.00(0.00)\n",
      "Iter 2730 | Time 15.7776(15.6490) | Bit/dim 3.7760(3.7913) | Xent 0.9644(0.9564) | Loss 8.9683(9.2568) | Error 0.3411(0.3417) Steps 0(0.00) | Grad Norm 7.3446(9.5719) | Total Time 0.00(0.00)\n",
      "Iter 2740 | Time 14.2374(15.7549) | Bit/dim 3.7834(3.7925) | Xent 0.9542(0.9547) | Loss 8.9631(9.2018) | Error 0.3333(0.3413) Steps 0(0.00) | Grad Norm 8.8543(9.2052) | Total Time 0.00(0.00)\n",
      "Iter 2750 | Time 15.7678(15.6890) | Bit/dim 3.8326(3.7890) | Xent 0.9949(0.9483) | Loss 8.8455(9.1174) | Error 0.3656(0.3390) Steps 0(0.00) | Grad Norm 16.3703(9.3657) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0050 | Time 84.1950, Epoch Time 966.5756(902.4160), Bit/dim 3.7939(best: 3.7849), Xent 0.9138, Loss 4.2509, Error 0.3229(best: 0.3199)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2760 | Time 16.7116(15.6199) | Bit/dim 3.7744(3.7901) | Xent 0.8751(0.9407) | Loss 8.9116(9.5242) | Error 0.3156(0.3355) Steps 0(0.00) | Grad Norm 7.8898(9.8615) | Total Time 0.00(0.00)\n",
      "Iter 2770 | Time 14.1191(15.6485) | Bit/dim 3.7755(3.7884) | Xent 0.9105(0.9426) | Loss 8.8998(9.3544) | Error 0.3322(0.3362) Steps 0(0.00) | Grad Norm 6.6705(9.5623) | Total Time 0.00(0.00)\n",
      "Iter 2780 | Time 16.8801(15.7561) | Bit/dim 3.7591(3.7872) | Xent 0.9492(0.9385) | Loss 8.8528(9.2562) | Error 0.3389(0.3339) Steps 0(0.00) | Grad Norm 7.8309(9.1934) | Total Time 0.00(0.00)\n",
      "Iter 2790 | Time 16.1247(15.8635) | Bit/dim 3.7590(3.7856) | Xent 0.9758(0.9350) | Loss 9.1124(9.1948) | Error 0.3311(0.3319) Steps 0(0.00) | Grad Norm 5.7561(8.5822) | Total Time 0.00(0.00)\n",
      "Iter 2800 | Time 15.6290(15.9131) | Bit/dim 3.7737(3.7807) | Xent 0.9300(0.9306) | Loss 8.8169(9.1301) | Error 0.3244(0.3300) Steps 0(0.00) | Grad Norm 6.5707(8.7551) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0051 | Time 82.4183, Epoch Time 974.5388(904.5796), Bit/dim 3.7811(best: 3.7849), Xent 0.8905, Loss 4.2264, Error 0.3154(best: 0.3199)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2810 | Time 15.4955(15.9763) | Bit/dim 3.7617(3.7827) | Xent 0.9429(0.9250) | Loss 9.0410(9.6127) | Error 0.3544(0.3284) Steps 0(0.00) | Grad Norm 14.2451(9.0442) | Total Time 0.00(0.00)\n",
      "Iter 2820 | Time 15.9188(15.7881) | Bit/dim 3.7555(3.7810) | Xent 0.9147(0.9227) | Loss 8.8551(9.4130) | Error 0.3400(0.3287) Steps 0(0.00) | Grad Norm 6.3037(9.3298) | Total Time 0.00(0.00)\n",
      "Iter 2830 | Time 15.8938(15.6033) | Bit/dim 3.7776(3.7830) | Xent 1.0001(0.9249) | Loss 8.9958(9.2842) | Error 0.3333(0.3298) Steps 0(0.00) | Grad Norm 9.8550(9.4505) | Total Time 0.00(0.00)\n",
      "Iter 2840 | Time 14.7184(15.5550) | Bit/dim 3.7836(3.7830) | Xent 0.8924(0.9252) | Loss 8.8934(9.1863) | Error 0.3189(0.3301) Steps 0(0.00) | Grad Norm 10.6439(9.6249) | Total Time 0.00(0.00)\n",
      "Iter 2850 | Time 14.3337(15.5034) | Bit/dim 3.8152(3.7844) | Xent 0.9300(0.9287) | Loss 8.9542(9.1219) | Error 0.3278(0.3317) Steps 0(0.00) | Grad Norm 12.2215(9.7139) | Total Time 0.00(0.00)\n",
      "Iter 2860 | Time 14.7916(15.4797) | Bit/dim 3.7554(3.7807) | Xent 1.0130(0.9324) | Loss 9.0311(9.0718) | Error 0.3700(0.3335) Steps 0(0.00) | Grad Norm 7.0797(9.6788) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0052 | Time 84.8088, Epoch Time 950.3198(905.9519), Bit/dim 3.7742(best: 3.7811), Xent 0.9155, Loss 4.2319, Error 0.3264(best: 0.3154)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2870 | Time 14.8584(15.5226) | Bit/dim 3.7514(3.7795) | Xent 0.9427(0.9205) | Loss 8.9620(9.5077) | Error 0.3233(0.3287) Steps 0(0.00) | Grad Norm 10.8980(9.5294) | Total Time 0.00(0.00)\n",
      "Iter 2880 | Time 15.7251(15.5779) | Bit/dim 3.8258(3.7818) | Xent 0.9364(0.9243) | Loss 9.1576(9.3653) | Error 0.3344(0.3311) Steps 0(0.00) | Grad Norm 9.2465(9.5992) | Total Time 0.00(0.00)\n",
      "Iter 2890 | Time 14.4588(15.6471) | Bit/dim 3.7459(3.7793) | Xent 0.8678(0.9242) | Loss 8.9347(9.2557) | Error 0.2911(0.3294) Steps 0(0.00) | Grad Norm 8.0974(9.2724) | Total Time 0.00(0.00)\n",
      "Iter 2900 | Time 14.9210(15.5703) | Bit/dim 3.7648(3.7778) | Xent 1.0081(0.9288) | Loss 8.9765(9.1732) | Error 0.3444(0.3303) Steps 0(0.00) | Grad Norm 10.2458(9.6907) | Total Time 0.00(0.00)\n",
      "Iter 2910 | Time 14.6866(15.5349) | Bit/dim 3.7255(3.7748) | Xent 0.9332(0.9229) | Loss 8.9421(9.0791) | Error 0.3322(0.3285) Steps 0(0.00) | Grad Norm 9.6616(9.2182) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0053 | Time 81.7527, Epoch Time 960.0580(907.5750), Bit/dim 3.7716(best: 3.7742), Xent 0.8778, Loss 4.2104, Error 0.3091(best: 0.3154)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2920 | Time 15.6140(15.5027) | Bit/dim 3.7832(3.7749) | Xent 0.8566(0.9179) | Loss 8.8937(9.5294) | Error 0.3067(0.3257) Steps 0(0.00) | Grad Norm 3.9418(8.9370) | Total Time 0.00(0.00)\n",
      "Iter 2930 | Time 14.4975(15.5635) | Bit/dim 3.7576(3.7739) | Xent 0.9261(0.9170) | Loss 8.8357(9.3799) | Error 0.3133(0.3251) Steps 0(0.00) | Grad Norm 16.2045(9.4328) | Total Time 0.00(0.00)\n",
      "Iter 2940 | Time 17.3198(15.4941) | Bit/dim 3.7631(3.7695) | Xent 0.8495(0.9128) | Loss 8.8838(9.2438) | Error 0.3067(0.3231) Steps 0(0.00) | Grad Norm 8.4015(9.1445) | Total Time 0.00(0.00)\n",
      "Iter 2950 | Time 17.8264(15.7407) | Bit/dim 3.8008(3.7693) | Xent 0.9411(0.9055) | Loss 8.7236(9.1389) | Error 0.3367(0.3211) Steps 0(0.00) | Grad Norm 10.7110(8.7566) | Total Time 0.00(0.00)\n",
      "Iter 2960 | Time 16.8965(15.6625) | Bit/dim 3.7615(3.7681) | Xent 0.9077(0.9064) | Loss 8.9491(9.0640) | Error 0.3189(0.3234) Steps 0(0.00) | Grad Norm 6.8639(8.3364) | Total Time 0.00(0.00)\n",
      "Iter 2970 | Time 17.3729(15.7991) | Bit/dim 3.7520(3.7672) | Xent 0.9183(0.9066) | Loss 8.9757(9.0090) | Error 0.3311(0.3256) Steps 0(0.00) | Grad Norm 7.8822(8.5426) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0054 | Time 82.8944, Epoch Time 965.2864(909.3064), Bit/dim 3.7687(best: 3.7716), Xent 0.8894, Loss 4.2134, Error 0.3085(best: 0.3091)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2980 | Time 15.1354(15.7987) | Bit/dim 3.7856(3.7664) | Xent 0.9176(0.9019) | Loss 8.9047(9.4350) | Error 0.3211(0.3244) Steps 0(0.00) | Grad Norm 4.9887(8.3767) | Total Time 0.00(0.00)\n",
      "Iter 2990 | Time 14.5960(15.5997) | Bit/dim 3.7830(3.7663) | Xent 0.9262(0.9001) | Loss 8.9098(9.2724) | Error 0.3244(0.3234) Steps 0(0.00) | Grad Norm 7.3963(8.0607) | Total Time 0.00(0.00)\n",
      "Iter 3000 | Time 14.5773(15.5168) | Bit/dim 3.7787(3.7640) | Xent 0.9212(0.8977) | Loss 8.7822(9.1464) | Error 0.3311(0.3228) Steps 0(0.00) | Grad Norm 10.4491(8.2956) | Total Time 0.00(0.00)\n",
      "Iter 3010 | Time 16.2120(15.4686) | Bit/dim 3.7819(3.7666) | Xent 0.8605(0.8988) | Loss 8.9771(9.0918) | Error 0.3022(0.3215) Steps 0(0.00) | Grad Norm 8.4863(9.2272) | Total Time 0.00(0.00)\n",
      "Iter 3020 | Time 15.6717(15.4157) | Bit/dim 3.7655(3.7664) | Xent 0.8663(0.8932) | Loss 8.9576(9.0336) | Error 0.3100(0.3199) Steps 0(0.00) | Grad Norm 6.6105(8.3999) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0055 | Time 83.7165, Epoch Time 947.3332(910.4472), Bit/dim 3.7643(best: 3.7687), Xent 0.8705, Loss 4.1995, Error 0.3055(best: 0.3085)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3030 | Time 15.4283(15.4384) | Bit/dim 3.7561(3.7640) | Xent 0.9276(0.8869) | Loss 9.0410(9.4956) | Error 0.3289(0.3183) Steps 0(0.00) | Grad Norm 9.9856(7.8357) | Total Time 0.00(0.00)\n",
      "Iter 3040 | Time 14.5856(15.3723) | Bit/dim 3.7659(3.7618) | Xent 0.9028(0.8845) | Loss 8.8930(9.3252) | Error 0.3211(0.3172) Steps 0(0.00) | Grad Norm 7.9183(8.2425) | Total Time 0.00(0.00)\n",
      "Iter 3050 | Time 15.7522(15.3802) | Bit/dim 3.7556(3.7596) | Xent 0.9141(0.8857) | Loss 8.8349(9.1942) | Error 0.3056(0.3168) Steps 0(0.00) | Grad Norm 7.1572(8.3990) | Total Time 0.00(0.00)\n",
      "Iter 3060 | Time 15.1146(15.4101) | Bit/dim 3.7762(3.7600) | Xent 0.9541(0.8852) | Loss 9.1619(9.1118) | Error 0.3456(0.3162) Steps 0(0.00) | Grad Norm 8.7257(8.6031) | Total Time 0.00(0.00)\n",
      "Iter 3070 | Time 14.9030(15.5517) | Bit/dim 3.7928(3.7599) | Xent 0.9007(0.8879) | Loss 8.8113(9.0467) | Error 0.3178(0.3176) Steps 0(0.00) | Grad Norm 10.5045(8.6821) | Total Time 0.00(0.00)\n",
      "Iter 3080 | Time 15.6248(15.6881) | Bit/dim 3.7626(3.7599) | Xent 0.8980(0.8971) | Loss 8.9083(9.0077) | Error 0.3067(0.3199) Steps 0(0.00) | Grad Norm 6.9376(8.6442) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0056 | Time 85.2369, Epoch Time 960.9832(911.9633), Bit/dim 3.7528(best: 3.7643), Xent 0.8660, Loss 4.1858, Error 0.3069(best: 0.3055)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3090 | Time 16.9650(15.7460) | Bit/dim 3.7535(3.7604) | Xent 0.8843(0.8934) | Loss 8.9904(9.4413) | Error 0.3289(0.3196) Steps 0(0.00) | Grad Norm 11.7617(8.7175) | Total Time 0.00(0.00)\n",
      "Iter 3100 | Time 14.0048(15.6687) | Bit/dim 3.7675(3.7624) | Xent 0.8921(0.8894) | Loss 8.7781(9.2837) | Error 0.3111(0.3183) Steps 0(0.00) | Grad Norm 11.6580(9.1344) | Total Time 0.00(0.00)\n",
      "Iter 3110 | Time 14.5931(15.6788) | Bit/dim 3.7304(3.7581) | Xent 0.8511(0.8857) | Loss 8.8097(9.1705) | Error 0.2956(0.3146) Steps 0(0.00) | Grad Norm 7.9425(9.0176) | Total Time 0.00(0.00)\n",
      "Iter 3120 | Time 14.7738(15.7901) | Bit/dim 3.7460(3.7591) | Xent 0.9106(0.8832) | Loss 8.8010(9.0848) | Error 0.3400(0.3139) Steps 0(0.00) | Grad Norm 8.3410(8.6556) | Total Time 0.00(0.00)\n",
      "Iter 3130 | Time 15.7806(15.7706) | Bit/dim 3.7604(3.7571) | Xent 0.9446(0.8846) | Loss 8.8949(9.0160) | Error 0.3356(0.3148) Steps 0(0.00) | Grad Norm 9.4660(8.8152) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0057 | Time 84.0513, Epoch Time 973.3263(913.8042), Bit/dim 3.7611(best: 3.7528), Xent 0.8658, Loss 4.1940, Error 0.3055(best: 0.3055)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3140 | Time 15.8445(15.7096) | Bit/dim 3.7574(3.7555) | Xent 0.8705(0.8842) | Loss 9.0010(9.4757) | Error 0.2989(0.3146) Steps 0(0.00) | Grad Norm 12.6986(8.8747) | Total Time 0.00(0.00)\n",
      "Iter 3150 | Time 15.5504(15.7079) | Bit/dim 3.7416(3.7529) | Xent 0.8626(0.8807) | Loss 8.7550(9.3067) | Error 0.2989(0.3129) Steps 0(0.00) | Grad Norm 6.7533(8.9205) | Total Time 0.00(0.00)\n",
      "Iter 3160 | Time 15.0331(15.6972) | Bit/dim 3.7569(3.7541) | Xent 0.9817(0.8830) | Loss 9.0060(9.1881) | Error 0.3433(0.3141) Steps 0(0.00) | Grad Norm 21.8225(9.4822) | Total Time 0.00(0.00)\n",
      "Iter 3170 | Time 14.8978(15.6852) | Bit/dim 3.7565(3.7533) | Xent 0.8774(0.8821) | Loss 8.8958(9.1069) | Error 0.3311(0.3136) Steps 0(0.00) | Grad Norm 10.7262(9.8188) | Total Time 0.00(0.00)\n",
      "Iter 3180 | Time 15.1425(15.6457) | Bit/dim 3.7378(3.7561) | Xent 0.8911(0.8853) | Loss 8.9073(9.0557) | Error 0.3289(0.3145) Steps 0(0.00) | Grad Norm 7.1222(9.4354) | Total Time 0.00(0.00)\n",
      "Iter 3190 | Time 15.3178(15.5667) | Bit/dim 3.7293(3.7562) | Xent 0.9521(0.8838) | Loss 8.9420(8.9893) | Error 0.3489(0.3151) Steps 0(0.00) | Grad Norm 9.2082(9.1917) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0058 | Time 84.1179, Epoch Time 955.4152(915.0525), Bit/dim 3.7564(best: 3.7528), Xent 0.8645, Loss 4.1887, Error 0.3052(best: 0.3055)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3200 | Time 16.0592(15.6854) | Bit/dim 3.7771(3.7548) | Xent 0.8035(0.8729) | Loss 8.6905(9.4004) | Error 0.2778(0.3120) Steps 0(0.00) | Grad Norm 6.2556(8.4674) | Total Time 0.00(0.00)\n",
      "Iter 3210 | Time 16.0836(15.5928) | Bit/dim 3.7747(3.7505) | Xent 0.9059(0.8677) | Loss 8.9329(9.2380) | Error 0.3244(0.3087) Steps 0(0.00) | Grad Norm 8.6783(8.6076) | Total Time 0.00(0.00)\n",
      "Iter 3220 | Time 16.8145(15.6568) | Bit/dim 3.7541(3.7471) | Xent 0.7876(0.8672) | Loss 8.7601(9.1297) | Error 0.2611(0.3094) Steps 0(0.00) | Grad Norm 4.4098(8.2013) | Total Time 0.00(0.00)\n",
      "Iter 3230 | Time 16.3504(15.8160) | Bit/dim 3.7693(3.7506) | Xent 0.8850(0.8684) | Loss 8.9992(9.0500) | Error 0.3133(0.3091) Steps 0(0.00) | Grad Norm 5.4081(8.5078) | Total Time 0.00(0.00)\n",
      "Iter 3240 | Time 16.4091(15.7709) | Bit/dim 3.7387(3.7505) | Xent 0.8779(0.8683) | Loss 8.8054(8.9863) | Error 0.3067(0.3084) Steps 0(0.00) | Grad Norm 4.9026(8.2012) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0059 | Time 84.8872, Epoch Time 969.6946(916.6917), Bit/dim 3.7473(best: 3.7528), Xent 0.8483, Loss 4.1714, Error 0.2986(best: 0.3052)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3250 | Time 13.7229(15.5278) | Bit/dim 3.7272(3.7507) | Xent 0.8927(0.8643) | Loss 8.6336(9.4556) | Error 0.3256(0.3071) Steps 0(0.00) | Grad Norm 7.7432(8.2690) | Total Time 0.00(0.00)\n",
      "Iter 3260 | Time 15.2664(15.4815) | Bit/dim 3.7052(3.7494) | Xent 0.8030(0.8519) | Loss 8.5330(9.2686) | Error 0.2722(0.3010) Steps 0(0.00) | Grad Norm 9.3053(7.6629) | Total Time 0.00(0.00)\n",
      "Iter 3270 | Time 18.1631(15.6396) | Bit/dim 3.7540(3.7483) | Xent 0.8349(0.8475) | Loss 8.8607(9.1566) | Error 0.2833(0.2993) Steps 0(0.00) | Grad Norm 6.1786(7.6957) | Total Time 0.00(0.00)\n",
      "Iter 3280 | Time 15.6613(15.7483) | Bit/dim 3.7088(3.7460) | Xent 0.9421(0.8478) | Loss 8.7754(9.0660) | Error 0.3344(0.3006) Steps 0(0.00) | Grad Norm 13.7733(7.8733) | Total Time 0.00(0.00)\n",
      "Iter 3290 | Time 14.5250(15.7956) | Bit/dim 3.7606(3.7466) | Xent 0.9102(0.8540) | Loss 8.8956(9.0090) | Error 0.3178(0.3040) Steps 0(0.00) | Grad Norm 6.4415(8.1070) | Total Time 0.00(0.00)\n",
      "Iter 3300 | Time 16.2066(15.8922) | Bit/dim 3.7467(3.7452) | Xent 0.8353(0.8559) | Loss 8.8584(8.9532) | Error 0.2900(0.3052) Steps 0(0.00) | Grad Norm 10.6124(7.9257) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0060 | Time 84.0278, Epoch Time 970.3665(918.3020), Bit/dim 3.7475(best: 3.7473), Xent 0.8438, Loss 4.1694, Error 0.2968(best: 0.2986)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3310 | Time 14.3799(15.7995) | Bit/dim 3.7577(3.7442) | Xent 0.8536(0.8479) | Loss 8.6511(9.3802) | Error 0.3100(0.3030) Steps 0(0.00) | Grad Norm 8.2370(7.5794) | Total Time 0.00(0.00)\n",
      "Iter 3320 | Time 14.2148(15.7286) | Bit/dim 3.7767(3.7443) | Xent 0.8929(0.8515) | Loss 8.8216(9.2358) | Error 0.3356(0.3039) Steps 0(0.00) | Grad Norm 7.0623(8.0537) | Total Time 0.00(0.00)\n",
      "Iter 3330 | Time 16.7291(15.7136) | Bit/dim 3.7608(3.7451) | Xent 0.9381(0.8576) | Loss 9.1650(9.1296) | Error 0.3444(0.3066) Steps 0(0.00) | Grad Norm 5.0166(8.1300) | Total Time 0.00(0.00)\n",
      "Iter 3340 | Time 16.0645(15.6840) | Bit/dim 3.7663(3.7451) | Xent 0.8717(0.8591) | Loss 8.9197(9.0455) | Error 0.3078(0.3066) Steps 0(0.00) | Grad Norm 9.6508(8.5215) | Total Time 0.00(0.00)\n",
      "Iter 3350 | Time 16.1798(15.6685) | Bit/dim 3.7459(3.7457) | Xent 0.8675(0.8697) | Loss 8.7875(8.9842) | Error 0.2989(0.3106) Steps 0(0.00) | Grad Norm 9.0006(9.3713) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0061 | Time 84.1212, Epoch Time 955.8492(919.4284), Bit/dim 3.7482(best: 3.7473), Xent 0.8503, Loss 4.1733, Error 0.2976(best: 0.2968)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3360 | Time 15.9352(15.5222) | Bit/dim 3.7594(3.7474) | Xent 0.8293(0.8669) | Loss 8.7939(9.4667) | Error 0.2933(0.3097) Steps 0(0.00) | Grad Norm 9.3408(9.4285) | Total Time 0.00(0.00)\n",
      "Iter 3370 | Time 16.0153(15.5717) | Bit/dim 3.7424(3.7459) | Xent 0.8674(0.8635) | Loss 8.7487(9.2928) | Error 0.3167(0.3073) Steps 0(0.00) | Grad Norm 9.2491(9.2299) | Total Time 0.00(0.00)\n",
      "Iter 3380 | Time 18.7701(15.5614) | Bit/dim 3.7224(3.7464) | Xent 0.8560(0.8584) | Loss 8.6032(9.1751) | Error 0.3000(0.3053) Steps 0(0.00) | Grad Norm 6.2843(8.5951) | Total Time 0.00(0.00)\n",
      "Iter 3390 | Time 15.1472(15.5278) | Bit/dim 3.7596(3.7437) | Xent 0.9417(0.8549) | Loss 9.0334(9.0722) | Error 0.3422(0.3032) Steps 0(0.00) | Grad Norm 13.4383(8.5540) | Total Time 0.00(0.00)\n",
      "Iter 3400 | Time 16.6695(15.4743) | Bit/dim 3.7320(3.7401) | Xent 0.7698(0.8523) | Loss 8.6340(8.9745) | Error 0.2833(0.3028) Steps 0(0.00) | Grad Norm 6.3429(8.4205) | Total Time 0.00(0.00)\n",
      "Iter 3410 | Time 15.2205(15.4758) | Bit/dim 3.7356(3.7385) | Xent 0.8322(0.8491) | Loss 8.8070(8.9178) | Error 0.2911(0.3022) Steps 0(0.00) | Grad Norm 8.1587(7.7547) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0062 | Time 84.5202, Epoch Time 952.7418(920.4278), Bit/dim 3.7381(best: 3.7473), Xent 0.8158, Loss 4.1460, Error 0.2850(best: 0.2968)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3420 | Time 15.2819(15.5735) | Bit/dim 3.7258(3.7358) | Xent 0.8904(0.8526) | Loss 8.9445(9.3716) | Error 0.3056(0.3032) Steps 0(0.00) | Grad Norm 13.3589(8.1282) | Total Time 0.00(0.00)\n",
      "Iter 3430 | Time 14.8501(15.5262) | Bit/dim 3.7278(3.7378) | Xent 0.8733(0.8540) | Loss 8.7822(9.2295) | Error 0.3067(0.3038) Steps 0(0.00) | Grad Norm 9.7534(8.5882) | Total Time 0.00(0.00)\n",
      "Iter 3440 | Time 14.5714(15.5693) | Bit/dim 3.7181(3.7374) | Xent 0.8273(0.8498) | Loss 8.6087(9.0985) | Error 0.2900(0.3024) Steps 0(0.00) | Grad Norm 8.6274(8.1971) | Total Time 0.00(0.00)\n",
      "Iter 3450 | Time 15.9135(15.4913) | Bit/dim 3.7478(3.7374) | Xent 0.8017(0.8427) | Loss 8.7386(8.9984) | Error 0.2922(0.3001) Steps 0(0.00) | Grad Norm 9.1612(7.9151) | Total Time 0.00(0.00)\n",
      "Iter 3460 | Time 15.6771(15.5260) | Bit/dim 3.7190(3.7376) | Xent 0.8542(0.8384) | Loss 8.8320(8.9425) | Error 0.3133(0.2990) Steps 0(0.00) | Grad Norm 9.9159(8.4211) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0063 | Time 85.8349, Epoch Time 964.8023(921.7590), Bit/dim 3.7346(best: 3.7381), Xent 0.8386, Loss 4.1539, Error 0.2946(best: 0.2850)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3470 | Time 15.6406(15.6414) | Bit/dim 3.7597(3.7377) | Xent 0.8282(0.8380) | Loss 8.9255(9.4549) | Error 0.2922(0.2989) Steps 0(0.00) | Grad Norm 7.7443(8.4958) | Total Time 0.00(0.00)\n",
      "Iter 3480 | Time 14.5900(15.7799) | Bit/dim 3.7635(3.7384) | Xent 0.8554(0.8320) | Loss 8.8066(9.2934) | Error 0.2989(0.2970) Steps 0(0.00) | Grad Norm 8.6130(8.5090) | Total Time 0.00(0.00)\n",
      "Iter 3490 | Time 15.7411(15.7259) | Bit/dim 3.7688(3.7383) | Xent 0.8136(0.8301) | Loss 8.8123(9.1434) | Error 0.2933(0.2969) Steps 0(0.00) | Grad Norm 6.5862(8.4767) | Total Time 0.00(0.00)\n",
      "Iter 3500 | Time 15.4948(15.8912) | Bit/dim 3.7255(3.7330) | Xent 0.7937(0.8300) | Loss 8.7410(9.0486) | Error 0.2767(0.2963) Steps 0(0.00) | Grad Norm 5.0997(8.5859) | Total Time 0.00(0.00)\n",
      "Iter 3510 | Time 17.2838(15.9395) | Bit/dim 3.7279(3.7345) | Xent 0.8561(0.8278) | Loss 8.5301(8.9810) | Error 0.2978(0.2960) Steps 0(0.00) | Grad Norm 7.1770(8.0782) | Total Time 0.00(0.00)\n",
      "Iter 3520 | Time 15.5568(15.8716) | Bit/dim 3.7750(3.7315) | Xent 0.8347(0.8252) | Loss 8.7874(8.9341) | Error 0.2922(0.2949) Steps 0(0.00) | Grad Norm 8.2379(8.0173) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0064 | Time 84.7799, Epoch Time 978.7037(923.4674), Bit/dim 3.7326(best: 3.7346), Xent 0.8182, Loss 4.1417, Error 0.2877(best: 0.2850)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3530 | Time 14.5822(15.8742) | Bit/dim 3.7885(3.7289) | Xent 0.7782(0.8220) | Loss 8.8705(9.3731) | Error 0.2778(0.2934) Steps 0(0.00) | Grad Norm 11.6458(8.4402) | Total Time 0.00(0.00)\n",
      "Iter 3540 | Time 15.5689(15.7773) | Bit/dim 3.7097(3.7282) | Xent 0.8306(0.8224) | Loss 8.5903(9.2166) | Error 0.3033(0.2929) Steps 0(0.00) | Grad Norm 7.9847(8.4845) | Total Time 0.00(0.00)\n",
      "Iter 3550 | Time 16.7352(15.8506) | Bit/dim 3.7060(3.7270) | Xent 0.7710(0.8203) | Loss 8.6838(9.0926) | Error 0.2711(0.2917) Steps 0(0.00) | Grad Norm 7.0619(8.4149) | Total Time 0.00(0.00)\n",
      "Iter 3560 | Time 16.4059(16.0773) | Bit/dim 3.7113(3.7287) | Xent 0.8459(0.8221) | Loss 8.8254(9.0133) | Error 0.3011(0.2941) Steps 0(0.00) | Grad Norm 11.1514(8.1837) | Total Time 0.00(0.00)\n",
      "Iter 3570 | Time 16.7538(16.0907) | Bit/dim 3.7758(3.7304) | Xent 0.9189(0.8337) | Loss 9.1028(8.9586) | Error 0.3200(0.2978) Steps 0(0.00) | Grad Norm 14.0363(8.5584) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0065 | Time 84.3257, Epoch Time 985.1000(925.3164), Bit/dim 3.7217(best: 3.7326), Xent 0.8460, Loss 4.1447, Error 0.2967(best: 0.2850)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3580 | Time 15.7383(16.0561) | Bit/dim 3.7345(3.7304) | Xent 0.7883(0.8313) | Loss 8.7046(9.4284) | Error 0.2811(0.2976) Steps 0(0.00) | Grad Norm 7.4997(8.3333) | Total Time 0.00(0.00)\n",
      "Iter 3590 | Time 15.9703(15.9140) | Bit/dim 3.7024(3.7266) | Xent 0.8210(0.8225) | Loss 8.7251(9.2375) | Error 0.2922(0.2948) Steps 0(0.00) | Grad Norm 10.3239(8.1318) | Total Time 0.00(0.00)\n",
      "Iter 3600 | Time 15.6594(15.9613) | Bit/dim 3.7006(3.7254) | Xent 0.7426(0.8151) | Loss 8.6746(9.1246) | Error 0.2644(0.2917) Steps 0(0.00) | Grad Norm 3.9694(7.7464) | Total Time 0.00(0.00)\n",
      "Iter 3610 | Time 16.5588(16.0721) | Bit/dim 3.7295(3.7282) | Xent 0.7876(0.8134) | Loss 8.7946(9.0500) | Error 0.2811(0.2908) Steps 0(0.00) | Grad Norm 6.1334(8.1189) | Total Time 0.00(0.00)\n",
      "Iter 3620 | Time 14.6097(16.0114) | Bit/dim 3.7336(3.7266) | Xent 0.7947(0.8123) | Loss 8.7971(8.9811) | Error 0.2811(0.2907) Steps 0(0.00) | Grad Norm 5.5205(7.6728) | Total Time 0.00(0.00)\n",
      "Iter 3630 | Time 15.1376(16.0277) | Bit/dim 3.7289(3.7247) | Xent 0.8640(0.8105) | Loss 8.7727(8.9121) | Error 0.3044(0.2900) Steps 0(0.00) | Grad Norm 11.0343(7.4790) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0066 | Time 85.2827, Epoch Time 982.4286(927.0297), Bit/dim 3.7207(best: 3.7217), Xent 0.8346, Loss 4.1380, Error 0.2933(best: 0.2850)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3640 | Time 15.3446(16.0920) | Bit/dim 3.7599(3.7251) | Xent 0.8379(0.8124) | Loss 8.8086(9.3298) | Error 0.3011(0.2895) Steps 0(0.00) | Grad Norm 11.4397(8.2450) | Total Time 0.00(0.00)\n",
      "Iter 3650 | Time 15.5866(16.1816) | Bit/dim 3.7483(3.7233) | Xent 0.7517(0.8106) | Loss 8.8951(9.1823) | Error 0.2633(0.2899) Steps 0(0.00) | Grad Norm 6.1176(8.3991) | Total Time 0.00(0.00)\n",
      "Iter 3660 | Time 15.9945(16.1540) | Bit/dim 3.7001(3.7229) | Xent 0.7373(0.8022) | Loss 8.7732(9.0522) | Error 0.2589(0.2859) Steps 0(0.00) | Grad Norm 5.9912(8.0957) | Total Time 0.00(0.00)\n",
      "Iter 3670 | Time 14.9613(16.1082) | Bit/dim 3.7163(3.7231) | Xent 0.8487(0.7999) | Loss 8.8442(8.9696) | Error 0.2944(0.2855) Steps 0(0.00) | Grad Norm 4.9177(8.0308) | Total Time 0.00(0.00)\n",
      "Iter 3680 | Time 15.3174(16.1643) | Bit/dim 3.7023(3.7226) | Xent 0.8199(0.8019) | Loss 8.7177(8.9241) | Error 0.3078(0.2865) Steps 0(0.00) | Grad Norm 9.1587(8.4781) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0067 | Time 87.5337, Epoch Time 996.9201(929.1264), Bit/dim 3.7250(best: 3.7207), Xent 0.8849, Loss 4.1674, Error 0.3099(best: 0.2850)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3690 | Time 16.0153(16.1361) | Bit/dim 3.7395(3.7263) | Xent 0.8251(0.8116) | Loss 8.8530(9.4376) | Error 0.3089(0.2908) Steps 0(0.00) | Grad Norm 8.7238(8.8304) | Total Time 0.00(0.00)\n",
      "Iter 3700 | Time 17.9751(16.2914) | Bit/dim 3.7277(3.7230) | Xent 0.8059(0.8089) | Loss 8.9226(9.2533) | Error 0.2867(0.2904) Steps 0(0.00) | Grad Norm 6.3529(8.7165) | Total Time 0.00(0.00)\n",
      "Iter 3710 | Time 15.7471(16.4380) | Bit/dim 3.7395(3.7263) | Xent 0.8664(0.8024) | Loss 8.5815(9.1201) | Error 0.3122(0.2876) Steps 0(0.00) | Grad Norm 9.0394(8.4483) | Total Time 0.00(0.00)\n",
      "Iter 3720 | Time 15.7924(16.4310) | Bit/dim 3.7365(3.7245) | Xent 0.7899(0.8085) | Loss 8.6789(9.0262) | Error 0.2833(0.2899) Steps 0(0.00) | Grad Norm 9.6057(8.7064) | Total Time 0.00(0.00)\n",
      "Iter 3730 | Time 17.8923(16.4237) | Bit/dim 3.6912(3.7243) | Xent 0.8117(0.8112) | Loss 8.7504(8.9676) | Error 0.2733(0.2893) Steps 0(0.00) | Grad Norm 10.1330(9.0166) | Total Time 0.00(0.00)\n",
      "Iter 3740 | Time 15.4952(16.2820) | Bit/dim 3.7392(3.7234) | Xent 0.8224(0.8123) | Loss 8.9854(8.9103) | Error 0.2922(0.2901) Steps 0(0.00) | Grad Norm 14.4997(8.8798) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0068 | Time 85.4687, Epoch Time 1005.8817(931.4291), Bit/dim 3.7226(best: 3.7207), Xent 0.8225, Loss 4.1339, Error 0.2884(best: 0.2850)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3750 | Time 16.3219(16.1618) | Bit/dim 3.7141(3.7236) | Xent 0.7744(0.8101) | Loss 8.7906(9.3269) | Error 0.2700(0.2903) Steps 0(0.00) | Grad Norm 5.3456(8.8811) | Total Time 0.00(0.00)\n",
      "Iter 3760 | Time 17.2811(16.2739) | Bit/dim 3.7304(3.7215) | Xent 0.7540(0.8021) | Loss 8.9383(9.1606) | Error 0.2556(0.2856) Steps 0(0.00) | Grad Norm 8.5697(8.6632) | Total Time 0.00(0.00)\n",
      "Iter 3770 | Time 18.5187(16.3357) | Bit/dim 3.7350(3.7189) | Xent 0.7393(0.8000) | Loss 8.8757(9.0394) | Error 0.2622(0.2850) Steps 0(0.00) | Grad Norm 5.9126(8.0869) | Total Time 0.00(0.00)\n",
      "Iter 3780 | Time 16.8974(16.2757) | Bit/dim 3.6918(3.7160) | Xent 0.7647(0.7936) | Loss 8.6018(8.9452) | Error 0.2811(0.2828) Steps 0(0.00) | Grad Norm 7.2534(8.4070) | Total Time 0.00(0.00)\n",
      "Iter 3790 | Time 18.2246(16.3193) | Bit/dim 3.7265(3.7152) | Xent 0.7741(0.7938) | Loss 8.8502(8.8874) | Error 0.2778(0.2831) Steps 0(0.00) | Grad Norm 6.0266(8.2939) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0069 | Time 86.8225, Epoch Time 998.7516(933.4488), Bit/dim 3.7166(best: 3.7207), Xent 0.8466, Loss 4.1399, Error 0.3011(best: 0.2850)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3800 | Time 16.4555(16.2451) | Bit/dim 3.7256(3.7172) | Xent 0.7755(0.7909) | Loss 8.7207(9.4022) | Error 0.2844(0.2815) Steps 0(0.00) | Grad Norm 9.6237(8.1941) | Total Time 0.00(0.00)\n",
      "Iter 3810 | Time 15.5322(16.1257) | Bit/dim 3.7561(3.7157) | Xent 0.7266(0.7856) | Loss 8.5183(9.1998) | Error 0.2600(0.2797) Steps 0(0.00) | Grad Norm 5.8229(7.8704) | Total Time 0.00(0.00)\n",
      "Iter 3820 | Time 16.1975(16.2186) | Bit/dim 3.6818(3.7174) | Xent 0.7843(0.7848) | Loss 8.6607(9.0920) | Error 0.2744(0.2778) Steps 0(0.00) | Grad Norm 7.7333(7.6905) | Total Time 0.00(0.00)\n",
      "Iter 3830 | Time 15.8021(16.3433) | Bit/dim 3.7322(3.7165) | Xent 0.7910(0.7865) | Loss 8.6167(8.9901) | Error 0.2778(0.2793) Steps 0(0.00) | Grad Norm 8.8761(8.1564) | Total Time 0.00(0.00)\n",
      "Iter 3840 | Time 17.2277(16.3579) | Bit/dim 3.6912(3.7155) | Xent 0.8329(0.7859) | Loss 8.8995(8.9205) | Error 0.2767(0.2781) Steps 0(0.00) | Grad Norm 11.1977(8.5502) | Total Time 0.00(0.00)\n",
      "Iter 3850 | Time 16.2644(16.4759) | Bit/dim 3.7379(3.7158) | Xent 0.8157(0.7931) | Loss 8.6018(8.8615) | Error 0.2922(0.2825) Steps 0(0.00) | Grad Norm 6.2351(8.5194) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0070 | Time 85.7121, Epoch Time 1005.7090(935.6166), Bit/dim 3.7192(best: 3.7166), Xent 0.8206, Loss 4.1295, Error 0.2902(best: 0.2850)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3860 | Time 16.4347(16.3118) | Bit/dim 3.7294(3.7171) | Xent 0.8152(0.7929) | Loss 8.6032(9.2872) | Error 0.2922(0.2841) Steps 0(0.00) | Grad Norm 15.3283(9.1053) | Total Time 0.00(0.00)\n",
      "Iter 3870 | Time 16.8064(16.3702) | Bit/dim 3.7131(3.7212) | Xent 0.8395(0.7974) | Loss 8.7698(9.1505) | Error 0.2922(0.2837) Steps 0(0.00) | Grad Norm 7.8552(9.2336) | Total Time 0.00(0.00)\n",
      "Iter 3880 | Time 15.8313(16.3321) | Bit/dim 3.7012(3.7183) | Xent 0.7285(0.7953) | Loss 8.6194(9.0383) | Error 0.2433(0.2838) Steps 0(0.00) | Grad Norm 9.6090(9.0315) | Total Time 0.00(0.00)\n",
      "Iter 3890 | Time 16.0883(16.3363) | Bit/dim 3.7360(3.7179) | Xent 0.8148(0.7965) | Loss 8.7219(8.9536) | Error 0.2689(0.2839) Steps 0(0.00) | Grad Norm 8.7104(9.0050) | Total Time 0.00(0.00)\n",
      "Iter 3900 | Time 15.5223(16.5764) | Bit/dim 3.7085(3.7165) | Xent 0.7240(0.7857) | Loss 8.6064(8.8968) | Error 0.2411(0.2804) Steps 0(0.00) | Grad Norm 4.8774(8.5486) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0071 | Time 87.9139, Epoch Time 1007.5827(937.7756), Bit/dim 3.7182(best: 3.7166), Xent 0.8228, Loss 4.1296, Error 0.2862(best: 0.2850)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3910 | Time 19.4456(16.4661) | Bit/dim 3.6959(3.7131) | Xent 0.7556(0.7766) | Loss 8.6197(9.3784) | Error 0.2778(0.2766) Steps 0(0.00) | Grad Norm 8.1078(8.3533) | Total Time 0.00(0.00)\n",
      "Iter 3920 | Time 15.7326(16.4252) | Bit/dim 3.7242(3.7117) | Xent 0.8570(0.7810) | Loss 8.7284(9.1834) | Error 0.2989(0.2771) Steps 0(0.00) | Grad Norm 11.1718(8.3149) | Total Time 0.00(0.00)\n",
      "Iter 3930 | Time 17.0044(16.5300) | Bit/dim 3.7463(3.7117) | Xent 0.6880(0.7838) | Loss 8.6876(9.0577) | Error 0.2478(0.2784) Steps 0(0.00) | Grad Norm 7.2756(8.4715) | Total Time 0.00(0.00)\n",
      "Iter 3940 | Time 15.9258(16.5361) | Bit/dim 3.7044(3.7120) | Xent 0.7883(0.7747) | Loss 8.6818(8.9545) | Error 0.2878(0.2747) Steps 0(0.00) | Grad Norm 9.5523(8.0951) | Total Time 0.00(0.00)\n",
      "Iter 3950 | Time 16.7805(16.4275) | Bit/dim 3.6978(3.7113) | Xent 0.7121(0.7721) | Loss 8.6889(8.8635) | Error 0.2444(0.2740) Steps 0(0.00) | Grad Norm 6.2570(7.9044) | Total Time 0.00(0.00)\n",
      "Iter 3960 | Time 16.0991(16.4647) | Bit/dim 3.7017(3.7103) | Xent 0.7548(0.7717) | Loss 8.6417(8.8250) | Error 0.2567(0.2739) Steps 0(0.00) | Grad Norm 8.1639(7.6666) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0072 | Time 86.0950, Epoch Time 1009.3652(939.9233), Bit/dim 3.7043(best: 3.7166), Xent 0.7857, Loss 4.0972, Error 0.2784(best: 0.2850)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3970 | Time 16.5005(16.4525) | Bit/dim 3.6934(3.7085) | Xent 0.7507(0.7707) | Loss 8.7426(9.2651) | Error 0.2678(0.2734) Steps 0(0.00) | Grad Norm 8.5758(7.5786) | Total Time 0.00(0.00)\n",
      "Iter 3980 | Time 16.2398(16.5585) | Bit/dim 3.7285(3.7071) | Xent 0.7340(0.7668) | Loss 8.3239(9.0971) | Error 0.2667(0.2728) Steps 0(0.00) | Grad Norm 5.3555(7.5169) | Total Time 0.00(0.00)\n",
      "Iter 3990 | Time 16.2554(16.5389) | Bit/dim 3.6780(3.7079) | Xent 0.7656(0.7671) | Loss 8.5436(8.9892) | Error 0.2822(0.2729) Steps 0(0.00) | Grad Norm 7.7727(7.5516) | Total Time 0.00(0.00)\n",
      "Iter 4000 | Time 17.7103(16.5844) | Bit/dim 3.6678(3.7048) | Xent 0.7330(0.7627) | Loss 8.7765(8.8970) | Error 0.2533(0.2722) Steps 0(0.00) | Grad Norm 6.8102(7.5147) | Total Time 0.00(0.00)\n",
      "Iter 4010 | Time 18.3888(16.5209) | Bit/dim 3.6882(3.7080) | Xent 0.7378(0.7667) | Loss 8.7843(8.8468) | Error 0.2589(0.2727) Steps 0(0.00) | Grad Norm 9.9296(7.7646) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0073 | Time 87.3452, Epoch Time 1012.8311(942.1105), Bit/dim 3.7132(best: 3.7043), Xent 0.8672, Loss 4.1468, Error 0.3111(best: 0.2784)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4020 | Time 16.3412(16.4293) | Bit/dim 3.7236(3.7110) | Xent 0.7792(0.7740) | Loss 8.8144(9.3500) | Error 0.2833(0.2754) Steps 0(0.00) | Grad Norm 12.1862(8.3550) | Total Time 0.00(0.00)\n",
      "Iter 4030 | Time 18.2980(16.5864) | Bit/dim 3.7201(3.7099) | Xent 0.7485(0.7766) | Loss 8.9412(9.1997) | Error 0.2800(0.2774) Steps 0(0.00) | Grad Norm 7.2593(8.5084) | Total Time 0.00(0.00)\n",
      "Iter 4040 | Time 16.3826(16.6415) | Bit/dim 3.7207(3.7093) | Xent 0.7349(0.7710) | Loss 8.7374(9.0584) | Error 0.2678(0.2758) Steps 0(0.00) | Grad Norm 6.3780(7.9004) | Total Time 0.00(0.00)\n",
      "Iter 4050 | Time 17.4066(16.7201) | Bit/dim 3.6614(3.7074) | Xent 0.7367(0.7642) | Loss 8.5502(8.9443) | Error 0.2578(0.2722) Steps 0(0.00) | Grad Norm 6.1037(7.4991) | Total Time 0.00(0.00)\n",
      "Iter 4060 | Time 16.6803(16.8496) | Bit/dim 3.7132(3.7064) | Xent 0.7524(0.7606) | Loss 8.7360(8.8870) | Error 0.2700(0.2723) Steps 0(0.00) | Grad Norm 7.4727(7.1895) | Total Time 0.00(0.00)\n",
      "Iter 4070 | Time 16.1896(16.8430) | Bit/dim 3.6943(3.7027) | Xent 0.7871(0.7588) | Loss 8.8483(8.8251) | Error 0.2856(0.2717) Steps 0(0.00) | Grad Norm 11.8919(7.3519) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0074 | Time 87.8679, Epoch Time 1035.6807(944.9176), Bit/dim 3.7037(best: 3.7043), Xent 0.8027, Loss 4.1051, Error 0.2796(best: 0.2784)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4080 | Time 17.0396(16.8703) | Bit/dim 3.7245(3.7029) | Xent 0.7723(0.7536) | Loss 8.5698(9.2650) | Error 0.2878(0.2691) Steps 0(0.00) | Grad Norm 8.1132(7.7965) | Total Time 0.00(0.00)\n",
      "Iter 4090 | Time 16.0861(16.7553) | Bit/dim 3.7325(3.7071) | Xent 0.8858(0.7795) | Loss 8.8784(9.1310) | Error 0.3111(0.2775) Steps 0(0.00) | Grad Norm 12.9963(8.7402) | Total Time 0.00(0.00)\n",
      "Iter 4100 | Time 15.8666(16.8317) | Bit/dim 3.6987(3.7060) | Xent 0.8063(0.7850) | Loss 8.5395(9.0176) | Error 0.2867(0.2798) Steps 0(0.00) | Grad Norm 7.6423(8.6508) | Total Time 0.00(0.00)\n",
      "Iter 4110 | Time 15.4915(16.8740) | Bit/dim 3.7014(3.7087) | Xent 0.7923(0.7764) | Loss 8.6701(8.9246) | Error 0.2900(0.2768) Steps 0(0.00) | Grad Norm 6.9979(8.5891) | Total Time 0.00(0.00)\n",
      "Iter 4120 | Time 16.1305(16.7059) | Bit/dim 3.7030(3.7070) | Xent 0.7153(0.7704) | Loss 8.4744(8.8432) | Error 0.2667(0.2743) Steps 0(0.00) | Grad Norm 8.7092(8.4871) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0075 | Time 87.4237, Epoch Time 1028.1916(947.4158), Bit/dim 3.7170(best: 3.7037), Xent 0.7815, Loss 4.1077, Error 0.2771(best: 0.2784)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4130 | Time 16.0694(16.7149) | Bit/dim 3.6699(3.7038) | Xent 0.7955(0.7687) | Loss 8.7738(9.3650) | Error 0.2967(0.2739) Steps 0(0.00) | Grad Norm 8.5257(8.4846) | Total Time 0.00(0.00)\n",
      "Iter 4140 | Time 16.8945(16.7134) | Bit/dim 3.7027(3.7059) | Xent 0.7723(0.7641) | Loss 8.7373(9.1595) | Error 0.2644(0.2716) Steps 0(0.00) | Grad Norm 9.2523(8.8690) | Total Time 0.00(0.00)\n",
      "Iter 4150 | Time 16.0950(16.6897) | Bit/dim 3.7376(3.7087) | Xent 0.7386(0.7553) | Loss 8.6954(9.0337) | Error 0.2611(0.2696) Steps 0(0.00) | Grad Norm 7.2735(8.3184) | Total Time 0.00(0.00)\n",
      "Iter 4160 | Time 16.1767(16.7508) | Bit/dim 3.7096(3.7064) | Xent 0.7686(0.7556) | Loss 8.7014(8.9446) | Error 0.2711(0.2690) Steps 0(0.00) | Grad Norm 11.0690(8.3186) | Total Time 0.00(0.00)\n",
      "Iter 4170 | Time 15.8953(16.6205) | Bit/dim 3.6840(3.7060) | Xent 0.7666(0.7585) | Loss 8.6863(8.8769) | Error 0.2633(0.2700) Steps 0(0.00) | Grad Norm 5.9456(8.4528) | Total Time 0.00(0.00)\n",
      "Iter 4180 | Time 16.6507(16.6468) | Bit/dim 3.7296(3.7030) | Xent 0.7710(0.7553) | Loss 8.5073(8.8063) | Error 0.2744(0.2688) Steps 0(0.00) | Grad Norm 3.6836(7.7881) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0076 | Time 88.1231, Epoch Time 1019.8617(949.5892), Bit/dim 3.7059(best: 3.7037), Xent 0.7620, Loss 4.0869, Error 0.2663(best: 0.2771)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4190 | Time 14.8712(16.6681) | Bit/dim 3.7316(3.7009) | Xent 0.6775(0.7450) | Loss 8.3710(9.2061) | Error 0.2367(0.2652) Steps 0(0.00) | Grad Norm 5.0950(7.1939) | Total Time 0.00(0.00)\n",
      "Iter 4200 | Time 16.8457(16.5840) | Bit/dim 3.7081(3.6987) | Xent 0.7632(0.7394) | Loss 8.7080(9.0527) | Error 0.2578(0.2626) Steps 0(0.00) | Grad Norm 6.2692(6.9234) | Total Time 0.00(0.00)\n",
      "Iter 4210 | Time 19.0402(16.8251) | Bit/dim 3.7019(3.6978) | Xent 0.6769(0.7342) | Loss 8.6961(8.9572) | Error 0.2411(0.2617) Steps 0(0.00) | Grad Norm 6.0320(6.9549) | Total Time 0.00(0.00)\n",
      "Iter 4220 | Time 17.0709(16.7219) | Bit/dim 3.7118(3.6973) | Xent 0.8142(0.7359) | Loss 8.6500(8.8609) | Error 0.3033(0.2626) Steps 0(0.00) | Grad Norm 11.2697(7.1943) | Total Time 0.00(0.00)\n",
      "Iter 4230 | Time 15.2757(16.6578) | Bit/dim 3.6994(3.6991) | Xent 0.7174(0.7348) | Loss 8.7169(8.8001) | Error 0.2789(0.2621) Steps 0(0.00) | Grad Norm 4.7742(7.3801) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0077 | Time 89.2252, Epoch Time 1031.4218(952.0442), Bit/dim 3.7074(best: 3.7037), Xent 0.7734, Loss 4.0941, Error 0.2715(best: 0.2663)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4240 | Time 17.9821(16.8761) | Bit/dim 3.7239(3.6999) | Xent 0.7259(0.7403) | Loss 8.6136(9.3098) | Error 0.2600(0.2639) Steps 0(0.00) | Grad Norm 6.5739(7.8929) | Total Time 0.00(0.00)\n",
      "Iter 4250 | Time 16.6701(16.8155) | Bit/dim 3.6644(3.6982) | Xent 0.7626(0.7405) | Loss 8.6299(9.1242) | Error 0.2833(0.2644) Steps 0(0.00) | Grad Norm 5.8707(7.6570) | Total Time 0.00(0.00)\n",
      "Iter 4260 | Time 17.2277(16.7546) | Bit/dim 3.7014(3.6979) | Xent 0.7152(0.7360) | Loss 8.6098(8.9772) | Error 0.2511(0.2630) Steps 0(0.00) | Grad Norm 5.6257(7.1841) | Total Time 0.00(0.00)\n",
      "Iter 4270 | Time 17.1929(16.9069) | Bit/dim 3.7416(3.6978) | Xent 0.7211(0.7386) | Loss 8.6946(8.8823) | Error 0.2556(0.2647) Steps 0(0.00) | Grad Norm 8.4525(7.2561) | Total Time 0.00(0.00)\n",
      "Iter 4280 | Time 16.6124(16.9275) | Bit/dim 3.7377(3.7002) | Xent 0.7726(0.7377) | Loss 8.8007(8.8295) | Error 0.2800(0.2643) Steps 0(0.00) | Grad Norm 7.6913(7.6880) | Total Time 0.00(0.00)\n",
      "Iter 4290 | Time 16.0836(16.9493) | Bit/dim 3.6775(3.6962) | Xent 0.6386(0.7342) | Loss 8.3982(8.7726) | Error 0.2378(0.2635) Steps 0(0.00) | Grad Norm 4.7838(7.2245) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0078 | Time 87.2807, Epoch Time 1037.4129(954.6052), Bit/dim 3.6958(best: 3.7037), Xent 0.7520, Loss 4.0719, Error 0.2664(best: 0.2663)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4300 | Time 16.7815(16.8674) | Bit/dim 3.7120(3.6946) | Xent 0.6940(0.7255) | Loss 8.2890(9.2073) | Error 0.2500(0.2595) Steps 0(0.00) | Grad Norm 6.3028(7.2009) | Total Time 0.00(0.00)\n",
      "Iter 4310 | Time 16.9970(16.9470) | Bit/dim 3.6921(3.6954) | Xent 0.8360(0.7274) | Loss 8.8287(9.0737) | Error 0.3156(0.2598) Steps 0(0.00) | Grad Norm 8.2418(7.4991) | Total Time 0.00(0.00)\n",
      "Iter 4320 | Time 16.5020(16.8665) | Bit/dim 3.7110(3.6958) | Xent 0.7111(0.7307) | Loss 8.5964(8.9380) | Error 0.2511(0.2607) Steps 0(0.00) | Grad Norm 10.1757(7.4368) | Total Time 0.00(0.00)\n",
      "Iter 4330 | Time 16.1051(16.8094) | Bit/dim 3.7350(3.6986) | Xent 0.7516(0.7347) | Loss 8.6517(8.8488) | Error 0.2500(0.2625) Steps 0(0.00) | Grad Norm 9.0414(7.7052) | Total Time 0.00(0.00)\n",
      "Iter 4340 | Time 18.1151(16.9273) | Bit/dim 3.6805(3.6955) | Xent 0.7121(0.7305) | Loss 8.5871(8.7913) | Error 0.2622(0.2614) Steps 0(0.00) | Grad Norm 4.8313(7.4633) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0079 | Time 88.3397, Epoch Time 1031.4727(956.9113), Bit/dim 3.7082(best: 3.6958), Xent 0.7794, Loss 4.0979, Error 0.2775(best: 0.2663)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4350 | Time 18.6250(16.8922) | Bit/dim 3.6819(3.6959) | Xent 0.6819(0.7379) | Loss 8.4289(9.3110) | Error 0.2322(0.2637) Steps 0(0.00) | Grad Norm 6.1646(8.1328) | Total Time 0.00(0.00)\n",
      "Iter 4360 | Time 16.7109(16.8977) | Bit/dim 3.6536(3.6943) | Xent 0.7263(0.7420) | Loss 8.7600(9.1454) | Error 0.2544(0.2650) Steps 0(0.00) | Grad Norm 9.2116(8.4455) | Total Time 0.00(0.00)\n",
      "Iter 4370 | Time 16.4625(16.8011) | Bit/dim 3.6937(3.6962) | Xent 0.6959(0.7428) | Loss 8.4084(9.0056) | Error 0.2456(0.2644) Steps 0(0.00) | Grad Norm 4.3056(8.3403) | Total Time 0.00(0.00)\n",
      "Iter 4380 | Time 15.8948(16.6366) | Bit/dim 3.6915(3.6967) | Xent 0.7304(0.7357) | Loss 8.3606(8.8889) | Error 0.2589(0.2612) Steps 0(0.00) | Grad Norm 7.5664(8.3288) | Total Time 0.00(0.00)\n",
      "Iter 4390 | Time 16.9221(16.6987) | Bit/dim 3.7241(3.6987) | Xent 0.7255(0.7404) | Loss 8.6627(8.8393) | Error 0.2589(0.2639) Steps 0(0.00) | Grad Norm 7.4135(8.4222) | Total Time 0.00(0.00)\n",
      "Iter 4400 | Time 15.7811(16.7397) | Bit/dim 3.7060(3.7013) | Xent 0.7501(0.7479) | Loss 8.6046(8.7967) | Error 0.2644(0.2672) Steps 0(0.00) | Grad Norm 9.9497(9.0612) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0080 | Time 88.9442, Epoch Time 1026.5795(959.0013), Bit/dim 3.6948(best: 3.6958), Xent 0.7944, Loss 4.0920, Error 0.2796(best: 0.2663)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4410 | Time 16.2804(16.8805) | Bit/dim 3.7024(3.7025) | Xent 0.7116(0.7383) | Loss 8.4952(9.2614) | Error 0.2656(0.2628) Steps 0(0.00) | Grad Norm 8.9185(8.8460) | Total Time 0.00(0.00)\n",
      "Iter 4420 | Time 18.1285(17.0157) | Bit/dim 3.6282(3.6956) | Xent 0.7360(0.7345) | Loss 8.6560(9.0823) | Error 0.2633(0.2618) Steps 0(0.00) | Grad Norm 10.0633(9.0522) | Total Time 0.00(0.00)\n",
      "Iter 4430 | Time 17.1694(17.1143) | Bit/dim 3.6763(3.6966) | Xent 0.7134(0.7363) | Loss 8.7847(8.9807) | Error 0.2478(0.2622) Steps 0(0.00) | Grad Norm 5.0494(8.7510) | Total Time 0.00(0.00)\n",
      "Iter 4440 | Time 17.4178(17.1258) | Bit/dim 3.6596(3.6970) | Xent 0.6859(0.7317) | Loss 8.6823(8.9003) | Error 0.2356(0.2602) Steps 0(0.00) | Grad Norm 7.0233(8.4887) | Total Time 0.00(0.00)\n",
      "Iter 4450 | Time 16.8938(17.0985) | Bit/dim 3.6971(3.6936) | Xent 0.7242(0.7313) | Loss 8.7620(8.8040) | Error 0.2611(0.2611) Steps 0(0.00) | Grad Norm 7.7802(8.4139) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0081 | Time 88.7203, Epoch Time 1058.1098(961.9746), Bit/dim 3.7013(best: 3.6948), Xent 0.8333, Loss 4.1180, Error 0.2884(best: 0.2663)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4460 | Time 19.9598(17.2907) | Bit/dim 3.6868(3.6901) | Xent 0.7424(0.7388) | Loss 8.9026(9.3017) | Error 0.2700(0.2636) Steps 0(0.00) | Grad Norm 6.7720(8.6162) | Total Time 0.00(0.00)\n",
      "Iter 4470 | Time 17.9227(17.3754) | Bit/dim 3.6520(3.6871) | Xent 0.6901(0.7340) | Loss 8.5784(9.1357) | Error 0.2433(0.2610) Steps 0(0.00) | Grad Norm 4.7856(8.1421) | Total Time 0.00(0.00)\n",
      "Iter 4480 | Time 18.3783(17.4046) | Bit/dim 3.6973(3.6878) | Xent 0.6960(0.7292) | Loss 8.6035(9.0119) | Error 0.2500(0.2588) Steps 0(0.00) | Grad Norm 6.2100(7.9898) | Total Time 0.00(0.00)\n",
      "Iter 4490 | Time 18.1122(17.2671) | Bit/dim 3.7183(3.6893) | Xent 0.7606(0.7203) | Loss 8.7671(8.9047) | Error 0.2778(0.2569) Steps 0(0.00) | Grad Norm 5.1214(7.2501) | Total Time 0.00(0.00)\n",
      "Iter 4500 | Time 17.7473(17.1770) | Bit/dim 3.6642(3.6877) | Xent 0.7547(0.7177) | Loss 8.7145(8.8127) | Error 0.2800(0.2558) Steps 0(0.00) | Grad Norm 10.1960(6.9897) | Total Time 0.00(0.00)\n",
      "Iter 4510 | Time 16.7768(17.1052) | Bit/dim 3.7013(3.6888) | Xent 0.7499(0.7160) | Loss 8.6351(8.7440) | Error 0.2667(0.2558) Steps 0(0.00) | Grad Norm 9.9417(7.0462) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0082 | Time 90.6707, Epoch Time 1056.1240(964.7990), Bit/dim 3.6797(best: 3.6948), Xent 0.7430, Loss 4.0512, Error 0.2600(best: 0.2663)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4520 | Time 16.3241(17.1586) | Bit/dim 3.7112(3.6900) | Xent 0.6785(0.7110) | Loss 8.5879(9.1873) | Error 0.2622(0.2554) Steps 0(0.00) | Grad Norm 10.5598(7.5478) | Total Time 0.00(0.00)\n",
      "Iter 4530 | Time 18.6438(17.2867) | Bit/dim 3.7036(3.6912) | Xent 0.6669(0.7063) | Loss 8.5251(9.0340) | Error 0.2356(0.2523) Steps 0(0.00) | Grad Norm 9.9340(7.5818) | Total Time 0.00(0.00)\n",
      "Iter 4540 | Time 15.9042(17.3468) | Bit/dim 3.7010(3.6893) | Xent 0.6572(0.7033) | Loss 8.4827(8.9213) | Error 0.2289(0.2513) Steps 0(0.00) | Grad Norm 5.3930(7.6229) | Total Time 0.00(0.00)\n",
      "Iter 4550 | Time 16.9591(17.4087) | Bit/dim 3.6970(3.6900) | Xent 0.8531(0.7137) | Loss 8.6781(8.8577) | Error 0.3000(0.2545) Steps 0(0.00) | Grad Norm 19.6956(8.2976) | Total Time 0.00(0.00)\n",
      "Iter 4560 | Time 16.4485(17.3692) | Bit/dim 3.6860(3.6895) | Xent 0.7012(0.7234) | Loss 8.2959(8.7865) | Error 0.2400(0.2572) Steps 0(0.00) | Grad Norm 5.8830(8.3233) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0083 | Time 89.1766, Epoch Time 1066.8439(967.8604), Bit/dim 3.6907(best: 3.6797), Xent 0.7656, Loss 4.0735, Error 0.2673(best: 0.2600)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4570 | Time 17.5879(17.4423) | Bit/dim 3.7077(3.6901) | Xent 0.6890(0.7278) | Loss 8.7066(9.3151) | Error 0.2456(0.2586) Steps 0(0.00) | Grad Norm 7.2239(8.2672) | Total Time 0.00(0.00)\n",
      "Iter 4580 | Time 16.2624(17.5045) | Bit/dim 3.6911(3.6879) | Xent 0.6852(0.7198) | Loss 8.2516(9.1189) | Error 0.2333(0.2552) Steps 0(0.00) | Grad Norm 7.5591(7.8444) | Total Time 0.00(0.00)\n",
      "Iter 4590 | Time 18.8206(17.5195) | Bit/dim 3.6743(3.6908) | Xent 0.7738(0.7179) | Loss 8.6991(8.9898) | Error 0.2800(0.2554) Steps 0(0.00) | Grad Norm 12.0315(8.1678) | Total Time 0.00(0.00)\n",
      "Iter 4600 | Time 17.7109(17.4520) | Bit/dim 3.6734(3.6879) | Xent 0.7470(0.7136) | Loss 8.5029(8.8761) | Error 0.2789(0.2531) Steps 0(0.00) | Grad Norm 6.2290(7.6995) | Total Time 0.00(0.00)\n",
      "Iter 4610 | Time 15.8758(17.3591) | Bit/dim 3.7037(3.6867) | Xent 0.7552(0.7147) | Loss 8.7487(8.8123) | Error 0.2656(0.2539) Steps 0(0.00) | Grad Norm 6.8748(7.4388) | Total Time 0.00(0.00)\n",
      "Iter 4620 | Time 17.1601(17.2443) | Bit/dim 3.6912(3.6853) | Xent 0.7889(0.7151) | Loss 8.7081(8.7495) | Error 0.2811(0.2559) Steps 0(0.00) | Grad Norm 9.0718(7.5333) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0084 | Time 88.9070, Epoch Time 1062.9536(970.7132), Bit/dim 3.6890(best: 3.6797), Xent 0.8198, Loss 4.0989, Error 0.2805(best: 0.2600)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4630 | Time 16.7916(17.3951) | Bit/dim 3.6641(3.6859) | Xent 0.7190(0.7225) | Loss 8.5122(9.2065) | Error 0.2389(0.2575) Steps 0(0.00) | Grad Norm 10.0337(8.3795) | Total Time 0.00(0.00)\n",
      "Iter 4640 | Time 16.9021(17.3291) | Bit/dim 3.6667(3.6868) | Xent 0.6977(0.7199) | Loss 8.5281(9.0500) | Error 0.2400(0.2563) Steps 0(0.00) | Grad Norm 6.2841(7.9656) | Total Time 0.00(0.00)\n",
      "Iter 4650 | Time 18.0808(17.3464) | Bit/dim 3.6945(3.6879) | Xent 0.7021(0.7085) | Loss 8.6771(8.9246) | Error 0.2556(0.2528) Steps 0(0.00) | Grad Norm 6.4411(7.7190) | Total Time 0.00(0.00)\n",
      "Iter 4660 | Time 18.3404(17.4151) | Bit/dim 3.6825(3.6854) | Xent 0.7444(0.7129) | Loss 8.9896(8.8532) | Error 0.2478(0.2533) Steps 0(0.00) | Grad Norm 8.6761(7.8483) | Total Time 0.00(0.00)\n",
      "Iter 4670 | Time 17.4477(17.3241) | Bit/dim 3.6639(3.6846) | Xent 0.6860(0.7092) | Loss 8.5747(8.7753) | Error 0.2567(0.2532) Steps 0(0.00) | Grad Norm 6.9423(7.9591) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0085 | Time 89.2229, Epoch Time 1064.1930(973.5176), Bit/dim 3.6871(best: 3.6797), Xent 0.7518, Loss 4.0630, Error 0.2603(best: 0.2600)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4680 | Time 16.6374(17.3526) | Bit/dim 3.6858(3.6839) | Xent 0.6755(0.7006) | Loss 8.6821(9.2911) | Error 0.2300(0.2503) Steps 0(0.00) | Grad Norm 5.1724(7.4391) | Total Time 0.00(0.00)\n",
      "Iter 4690 | Time 18.2532(17.5453) | Bit/dim 3.7035(3.6819) | Xent 0.7137(0.6964) | Loss 8.4470(9.0880) | Error 0.2633(0.2482) Steps 0(0.00) | Grad Norm 8.2576(7.3029) | Total Time 0.00(0.00)\n",
      "Iter 4700 | Time 16.9760(17.5188) | Bit/dim 3.6741(3.6849) | Xent 0.7467(0.6977) | Loss 8.5924(8.9556) | Error 0.2744(0.2482) Steps 0(0.00) | Grad Norm 7.0788(7.7613) | Total Time 0.00(0.00)\n",
      "Iter 4710 | Time 18.9800(17.6430) | Bit/dim 3.6531(3.6845) | Xent 0.7130(0.7007) | Loss 8.6976(8.8854) | Error 0.2456(0.2489) Steps 0(0.00) | Grad Norm 9.7014(7.6915) | Total Time 0.00(0.00)\n",
      "Iter 4720 | Time 18.9215(17.5554) | Bit/dim 3.6631(3.6835) | Xent 0.6907(0.6980) | Loss 8.5295(8.7925) | Error 0.2422(0.2496) Steps 0(0.00) | Grad Norm 5.0949(7.5006) | Total Time 0.00(0.00)\n",
      "Iter 4730 | Time 16.9979(17.6654) | Bit/dim 3.6554(3.6789) | Xent 0.7059(0.6989) | Loss 8.6849(8.7455) | Error 0.2311(0.2469) Steps 0(0.00) | Grad Norm 6.3299(7.3584) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0086 | Time 89.1055, Epoch Time 1083.1836(976.8076), Bit/dim 3.6821(best: 3.6797), Xent 0.7509, Loss 4.0576, Error 0.2649(best: 0.2600)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4740 | Time 17.5630(17.7054) | Bit/dim 3.6853(3.6775) | Xent 0.7072(0.6958) | Loss 8.8089(9.1854) | Error 0.2489(0.2461) Steps 0(0.00) | Grad Norm 7.0526(7.4244) | Total Time 0.00(0.00)\n",
      "Iter 4750 | Time 16.9566(17.7435) | Bit/dim 3.6701(3.6762) | Xent 0.7041(0.6930) | Loss 8.5542(9.0343) | Error 0.2378(0.2464) Steps 0(0.00) | Grad Norm 11.2844(7.8487) | Total Time 0.00(0.00)\n",
      "Iter 4760 | Time 16.8670(17.6160) | Bit/dim 3.6740(3.6770) | Xent 0.7499(0.6932) | Loss 8.3799(8.9087) | Error 0.2644(0.2470) Steps 0(0.00) | Grad Norm 8.0204(7.6161) | Total Time 0.00(0.00)\n",
      "Iter 4770 | Time 18.3621(17.6317) | Bit/dim 3.6726(3.6767) | Xent 0.7120(0.6928) | Loss 8.7262(8.8123) | Error 0.2467(0.2473) Steps 0(0.00) | Grad Norm 6.4223(7.4537) | Total Time 0.00(0.00)\n",
      "Iter 4780 | Time 15.9629(17.4662) | Bit/dim 3.7068(3.6765) | Xent 0.6518(0.6903) | Loss 8.3737(8.7409) | Error 0.2256(0.2469) Steps 0(0.00) | Grad Norm 8.0279(7.4636) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0087 | Time 88.3300, Epoch Time 1075.1657(979.7583), Bit/dim 3.6758(best: 3.6797), Xent 0.7731, Loss 4.0623, Error 0.2690(best: 0.2600)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4790 | Time 17.2929(17.5654) | Bit/dim 3.6858(3.6770) | Xent 0.6434(0.6905) | Loss 8.7372(9.2762) | Error 0.2344(0.2457) Steps 0(0.00) | Grad Norm 8.6078(7.3850) | Total Time 0.00(0.00)\n",
      "Iter 4800 | Time 17.5512(17.6046) | Bit/dim 3.6411(3.6757) | Xent 0.6978(0.6817) | Loss 8.4230(9.0779) | Error 0.2478(0.2429) Steps 0(0.00) | Grad Norm 6.1061(7.3201) | Total Time 0.00(0.00)\n",
      "Iter 4810 | Time 16.3190(17.6012) | Bit/dim 3.6687(3.6790) | Xent 0.7184(0.6911) | Loss 8.4843(8.9573) | Error 0.2689(0.2465) Steps 0(0.00) | Grad Norm 8.9863(7.6510) | Total Time 0.00(0.00)\n",
      "Iter 4820 | Time 17.2365(17.5707) | Bit/dim 3.6937(3.6820) | Xent 0.6919(0.6902) | Loss 8.3231(8.8624) | Error 0.2467(0.2458) Steps 0(0.00) | Grad Norm 6.6112(7.4797) | Total Time 0.00(0.00)\n",
      "Iter 4830 | Time 17.4525(17.5145) | Bit/dim 3.6711(3.6801) | Xent 0.6743(0.6890) | Loss 8.4406(8.7780) | Error 0.2356(0.2457) Steps 0(0.00) | Grad Norm 5.6902(7.4883) | Total Time 0.00(0.00)\n",
      "Iter 4840 | Time 17.0248(17.4512) | Bit/dim 3.6819(3.6787) | Xent 0.7040(0.6922) | Loss 8.5354(8.7139) | Error 0.2622(0.2478) Steps 0(0.00) | Grad Norm 7.2831(7.4496) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0088 | Time 89.0233, Epoch Time 1069.3672(982.4466), Bit/dim 3.6755(best: 3.6758), Xent 0.7417, Loss 4.0464, Error 0.2588(best: 0.2600)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4850 | Time 16.9046(17.5496) | Bit/dim 3.6640(3.6796) | Xent 0.6379(0.6903) | Loss 8.2536(9.1386) | Error 0.2200(0.2472) Steps 0(0.00) | Grad Norm 8.5650(7.4610) | Total Time 0.00(0.00)\n",
      "Iter 4860 | Time 17.4485(17.5920) | Bit/dim 3.6362(3.6781) | Xent 0.6353(0.6830) | Loss 8.4083(8.9855) | Error 0.2267(0.2447) Steps 0(0.00) | Grad Norm 4.3629(7.4723) | Total Time 0.00(0.00)\n",
      "Iter 4870 | Time 17.8046(17.5335) | Bit/dim 3.6917(3.6798) | Xent 0.6465(0.6834) | Loss 8.5604(8.8916) | Error 0.2367(0.2443) Steps 0(0.00) | Grad Norm 5.4411(7.4371) | Total Time 0.00(0.00)\n",
      "Iter 4880 | Time 17.5053(17.4835) | Bit/dim 3.6923(3.6768) | Xent 0.7315(0.6788) | Loss 8.5451(8.7920) | Error 0.2667(0.2443) Steps 0(0.00) | Grad Norm 5.5540(7.0025) | Total Time 0.00(0.00)\n",
      "Iter 4890 | Time 17.2250(17.4696) | Bit/dim 3.7095(3.6788) | Xent 0.6871(0.6837) | Loss 8.6499(8.7428) | Error 0.2511(0.2462) Steps 0(0.00) | Grad Norm 9.6596(7.5516) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0089 | Time 89.0314, Epoch Time 1071.5992(985.1211), Bit/dim 3.6937(best: 3.6755), Xent 0.7459, Loss 4.0667, Error 0.2604(best: 0.2588)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4900 | Time 16.7792(17.3708) | Bit/dim 3.6968(3.6784) | Xent 0.7056(0.6846) | Loss 8.6020(9.2491) | Error 0.2522(0.2453) Steps 0(0.00) | Grad Norm 8.4540(7.8266) | Total Time 0.00(0.00)\n",
      "Iter 4910 | Time 17.3616(17.4459) | Bit/dim 3.6635(3.6778) | Xent 0.6865(0.6866) | Loss 8.5073(9.0814) | Error 0.2500(0.2459) Steps 0(0.00) | Grad Norm 10.7556(8.0474) | Total Time 0.00(0.00)\n",
      "Iter 4920 | Time 17.1066(17.5235) | Bit/dim 3.6879(3.6771) | Xent 0.6721(0.6848) | Loss 8.7008(8.9554) | Error 0.2433(0.2448) Steps 0(0.00) | Grad Norm 6.0766(7.9044) | Total Time 0.00(0.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_rl.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdlearnscale_30_run1 --seed 1 --lr 0.001 --conditional True --controlled_tol False --train_mode semisup --log_freq 10 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5 --scale_fac 1.0 --gate cnn2 --scale_std 30.0\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
