{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0,1,2,3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_rl.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "GATES = [\"cnn1\", \"cnn2\", \"rnn\"]\n",
      "\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--gate', type=str, default='cnn1', choices=GATES)\n",
      "parser.add_argument('--scale', type=float, default=1.0)\n",
      "parser.add_argument('--scale_fac', type=float, default=1.0)\n",
      "parser.add_argument('--scale_std', type=float, default=1.0)\n",
      "parser.add_argument('--eta', default=0.1, type=float,\n",
      "                        help='tuning parameter that allows us to trade-off the competing goals of' \n",
      "                                'minimizing the prediction loss and maximizing the gate rewards ')\n",
      "parser.add_argument('--rl-weight', default=0.01, type=float,\n",
      "                        help='rl weight')\n",
      "\n",
      "parser.add_argument('--gamma', default=0.99, type=float,\n",
      "                        help='discount factor, default: (0.99)')\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "import lib.odenvp_conditional_rl as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol, \"scale\": args.scale, \"scale_fac\": args.scale_fac, \"scale_std\": args.scale_std, \"gate\": args.gate},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            # re-weight the gate rewards\n",
      "            normalized_eta = args.eta / len(logp_actions)\n",
      "            \n",
      "            # collect cumulative future rewards\n",
      "            R = - loss\n",
      "            cum_rewards = []\n",
      "            for r in nfe[::-1]:\n",
      "                R = -normalized_eta * r.view(-1,1) + args.gamma * R\n",
      "                cum_rewards.insert(0,R)\n",
      "            \n",
      "            # apply REINFORCE\n",
      "            rl_loss = 0\n",
      "            for lpa, r in zip(logp_actions, cum_rewards):\n",
      "                rl_loss = rl_loss - lpa.view(-1,1) * args.rl_weight * r\n",
      "                \n",
      "            loss = loss + rl_loss.mean()\n",
      "            \n",
      "            loss.backward()\n",
      "            \n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "            \n",
      "            for idx in range(len(model.module.transforms)):\n",
      "                for layer in model.module.transforms[idx].chain:\n",
      "                    if hasattr(layer, 'atol'):\n",
      "                        layer.odefunc.after_odeint()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'train': atol[tol_indx].mean()}, itr)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'train': rtol[tol_indx].mean()}, itr)\n",
      "                    \n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples, atol, rtol, logp_actions, nfe = model(fixed_z, reverse=True)\n",
      "            generated_samples = generated_samples.view(-1, *data_shape)\n",
      "            for tol_indx in range(len(atol)):\n",
      "                writer.add_scalars('atol_gen_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                writer.add_scalars('rtol_gen_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, condition_ratio=0.5, conditional=True, controlled_tol=False, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, eta=0.1, gamma=0.99, gate='cnn1', imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume='../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_3_run1/current_checkpt.pth', rl_weight=0.01, rtol=1e-05, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_3_run1', scale=1.0, scale_fac=1.0, scale_std=3.0, seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1450732\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc_rl.py:291: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Iter 7710 | Time 19.0462(19.8555) | Bit/dim 3.6335(3.6215) | Xent 0.4057(0.4338) | Loss 9.5298(10.3052) | Error 0.1400(0.1557) Steps 0(0.00) | Grad Norm 8.9328(7.5814) | Total Time 0.00(0.00)\n",
      "Iter 7720 | Time 17.8600(19.6019) | Bit/dim 3.6265(3.6225) | Xent 0.4522(0.4285) | Loss 9.5804(10.0778) | Error 0.1511(0.1538) Steps 0(0.00) | Grad Norm 7.7236(7.7895) | Total Time 0.00(0.00)\n",
      "Iter 7730 | Time 20.0291(19.5177) | Bit/dim 3.6154(3.6217) | Xent 0.5383(0.4356) | Loss 9.8276(9.9295) | Error 0.1844(0.1558) Steps 0(0.00) | Grad Norm 17.5623(8.3476) | Total Time 0.00(0.00)\n",
      "Iter 7740 | Time 20.6231(19.4802) | Bit/dim 3.6068(3.6226) | Xent 0.4412(0.4412) | Loss 9.5363(9.8328) | Error 0.1433(0.1573) Steps 0(0.00) | Grad Norm 8.1125(8.6094) | Total Time 0.00(0.00)\n",
      "Iter 7750 | Time 18.5715(19.2979) | Bit/dim 3.6260(3.6215) | Xent 0.4072(0.4353) | Loss 9.4547(9.7342) | Error 0.1489(0.1551) Steps 0(0.00) | Grad Norm 7.1533(8.2786) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0141 | Time 100.5764, Epoch Time 1193.4486(1149.8200), Bit/dim 3.6183(best: inf), Xent 0.6906, Loss 3.9636, Error 0.2237(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7760 | Time 19.0170(19.2460) | Bit/dim 3.5946(3.6179) | Xent 0.4095(0.4330) | Loss 9.3262(10.4448) | Error 0.1533(0.1555) Steps 0(0.00) | Grad Norm 7.8031(7.9494) | Total Time 0.00(0.00)\n",
      "Iter 7770 | Time 19.1371(19.2747) | Bit/dim 3.6233(3.6167) | Xent 0.3919(0.4288) | Loss 9.4652(10.1865) | Error 0.1400(0.1534) Steps 0(0.00) | Grad Norm 6.6265(7.6105) | Total Time 0.00(0.00)\n",
      "Iter 7780 | Time 19.8063(19.2200) | Bit/dim 3.6160(3.6196) | Xent 0.4802(0.4301) | Loss 9.5863(10.0077) | Error 0.1644(0.1533) Steps 0(0.00) | Grad Norm 8.8101(8.0904) | Total Time 0.00(0.00)\n",
      "Iter 7790 | Time 18.7366(19.2010) | Bit/dim 3.6413(3.6208) | Xent 0.3839(0.4257) | Loss 9.5875(9.8651) | Error 0.1500(0.1522) Steps 0(0.00) | Grad Norm 7.8038(8.1018) | Total Time 0.00(0.00)\n",
      "Iter 7800 | Time 19.7724(19.4088) | Bit/dim 3.5880(3.6205) | Xent 0.3933(0.4261) | Loss 9.4531(9.7703) | Error 0.1300(0.1514) Steps 0(0.00) | Grad Norm 4.1744(7.7548) | Total Time 0.00(0.00)\n",
      "Iter 7810 | Time 19.8132(19.4344) | Bit/dim 3.6228(3.6202) | Xent 0.4180(0.4274) | Loss 9.6101(9.6995) | Error 0.1578(0.1517) Steps 0(0.00) | Grad Norm 4.2147(7.5158) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0142 | Time 93.6711, Epoch Time 1176.1702(1150.6105), Bit/dim 3.6137(best: 3.6183), Xent 0.6791, Loss 3.9532, Error 0.2216(best: 0.2237)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7820 | Time 18.8869(19.2808) | Bit/dim 3.5801(3.6180) | Xent 0.4392(0.4226) | Loss 9.4268(10.3616) | Error 0.1644(0.1516) Steps 0(0.00) | Grad Norm 6.0053(7.2592) | Total Time 0.00(0.00)\n",
      "Iter 7830 | Time 19.4233(19.4940) | Bit/dim 3.6036(3.6203) | Xent 0.4631(0.4188) | Loss 9.5234(10.1323) | Error 0.1489(0.1494) Steps 0(0.00) | Grad Norm 7.3159(7.0925) | Total Time 0.00(0.00)\n",
      "Iter 7840 | Time 19.6450(19.4195) | Bit/dim 3.5840(3.6187) | Xent 0.4274(0.4181) | Loss 9.3657(9.9503) | Error 0.1544(0.1487) Steps 0(0.00) | Grad Norm 9.2138(7.1533) | Total Time 0.00(0.00)\n",
      "Iter 7850 | Time 19.2422(19.4200) | Bit/dim 3.5890(3.6164) | Xent 0.4208(0.4261) | Loss 9.3545(9.8178) | Error 0.1544(0.1529) Steps 0(0.00) | Grad Norm 5.8515(7.4379) | Total Time 0.00(0.00)\n",
      "Iter 7860 | Time 20.2771(19.3791) | Bit/dim 3.6316(3.6192) | Xent 0.4276(0.4292) | Loss 9.6792(9.7501) | Error 0.1522(0.1534) Steps 0(0.00) | Grad Norm 5.5386(7.4872) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0143 | Time 99.3776, Epoch Time 1186.5019(1151.6873), Bit/dim 3.6162(best: 3.6137), Xent 0.6712, Loss 3.9518, Error 0.2238(best: 0.2216)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7870 | Time 19.9476(19.5695) | Bit/dim 3.6212(3.6176) | Xent 0.4113(0.4327) | Loss 9.6196(10.5328) | Error 0.1433(0.1539) Steps 0(0.00) | Grad Norm 8.4637(7.7363) | Total Time 0.00(0.00)\n",
      "Iter 7880 | Time 20.5242(19.7664) | Bit/dim 3.6081(3.6201) | Xent 0.3783(0.4246) | Loss 9.3822(10.2652) | Error 0.1333(0.1518) Steps 0(0.00) | Grad Norm 8.9048(7.7082) | Total Time 0.00(0.00)\n",
      "Iter 7890 | Time 19.2327(19.7434) | Bit/dim 3.6400(3.6200) | Xent 0.3936(0.4253) | Loss 9.4770(10.0758) | Error 0.1422(0.1511) Steps 0(0.00) | Grad Norm 5.8651(7.9558) | Total Time 0.00(0.00)\n",
      "Iter 7900 | Time 19.9416(19.9033) | Bit/dim 3.6158(3.6212) | Xent 0.3879(0.4184) | Loss 9.4728(9.9342) | Error 0.1300(0.1479) Steps 0(0.00) | Grad Norm 7.5989(7.5824) | Total Time 0.00(0.00)\n",
      "Iter 7910 | Time 21.1068(19.9278) | Bit/dim 3.6097(3.6175) | Xent 0.3840(0.4162) | Loss 9.5403(9.7964) | Error 0.1522(0.1481) Steps 0(0.00) | Grad Norm 6.9812(7.3654) | Total Time 0.00(0.00)\n",
      "Iter 7920 | Time 19.2114(20.0287) | Bit/dim 3.5947(3.6140) | Xent 0.4034(0.4163) | Loss 9.5860(9.7115) | Error 0.1478(0.1493) Steps 0(0.00) | Grad Norm 6.5955(7.1995) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0144 | Time 97.8047, Epoch Time 1222.6973(1153.8176), Bit/dim 3.6189(best: 3.6137), Xent 0.6856, Loss 3.9618, Error 0.2245(best: 0.2216)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7930 | Time 22.4041(20.2792) | Bit/dim 3.6298(3.6122) | Xent 0.4390(0.4095) | Loss 9.5964(10.3506) | Error 0.1589(0.1461) Steps 0(0.00) | Grad Norm 7.6562(7.0525) | Total Time 0.00(0.00)\n",
      "Iter 7940 | Time 19.2987(20.1352) | Bit/dim 3.6160(3.6159) | Xent 0.4338(0.4155) | Loss 9.5876(10.1178) | Error 0.1589(0.1473) Steps 0(0.00) | Grad Norm 10.6490(7.7994) | Total Time 0.00(0.00)\n",
      "Iter 7950 | Time 19.1545(20.1141) | Bit/dim 3.6029(3.6160) | Xent 0.3491(0.4153) | Loss 9.2666(9.9525) | Error 0.1311(0.1483) Steps 0(0.00) | Grad Norm 6.6846(8.0117) | Total Time 0.00(0.00)\n",
      "Iter 7960 | Time 20.7358(20.1286) | Bit/dim 3.5999(3.6161) | Xent 0.3759(0.4100) | Loss 9.3659(9.8327) | Error 0.1367(0.1463) Steps 0(0.00) | Grad Norm 3.8679(7.7245) | Total Time 0.00(0.00)\n",
      "Iter 7970 | Time 19.3189(20.1955) | Bit/dim 3.5973(3.6164) | Xent 0.4963(0.4245) | Loss 9.1696(9.7541) | Error 0.1633(0.1518) Steps 0(0.00) | Grad Norm 5.7931(7.9542) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0145 | Time 97.6430, Epoch Time 1231.8617(1156.1589), Bit/dim 3.6298(best: 3.6137), Xent 0.6804, Loss 3.9701, Error 0.2265(best: 0.2216)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7980 | Time 19.6299(20.2385) | Bit/dim 3.6098(3.6161) | Xent 0.3950(0.4298) | Loss 9.5520(10.4726) | Error 0.1456(0.1543) Steps 0(0.00) | Grad Norm 6.0253(7.9251) | Total Time 0.00(0.00)\n",
      "Iter 7990 | Time 19.3351(20.3358) | Bit/dim 3.6208(3.6180) | Xent 0.4375(0.4270) | Loss 9.5628(10.2195) | Error 0.1556(0.1519) Steps 0(0.00) | Grad Norm 7.9260(7.5774) | Total Time 0.00(0.00)\n",
      "Iter 8000 | Time 19.7510(20.3170) | Bit/dim 3.5865(3.6147) | Xent 0.4426(0.4179) | Loss 9.4200(10.0194) | Error 0.1611(0.1490) Steps 0(0.00) | Grad Norm 6.8941(7.2606) | Total Time 0.00(0.00)\n",
      "Iter 8010 | Time 20.3319(20.2513) | Bit/dim 3.6071(3.6143) | Xent 0.3995(0.4085) | Loss 9.1309(9.8644) | Error 0.1400(0.1461) Steps 0(0.00) | Grad Norm 7.3997(7.0112) | Total Time 0.00(0.00)\n",
      "Iter 8020 | Time 18.8349(20.1117) | Bit/dim 3.5672(3.6131) | Xent 0.4256(0.4059) | Loss 9.3533(9.7511) | Error 0.1589(0.1454) Steps 0(0.00) | Grad Norm 7.1955(6.8713) | Total Time 0.00(0.00)\n",
      "Iter 8030 | Time 21.4562(20.1508) | Bit/dim 3.6353(3.6169) | Xent 0.4488(0.4097) | Loss 9.6406(9.6845) | Error 0.1589(0.1464) Steps 0(0.00) | Grad Norm 11.4776(7.4763) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0146 | Time 99.1753, Epoch Time 1227.4720(1158.2983), Bit/dim 3.6177(best: 3.6137), Xent 0.7137, Loss 3.9745, Error 0.2327(best: 0.2216)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8040 | Time 19.5697(20.1502) | Bit/dim 3.6344(3.6206) | Xent 0.4219(0.4085) | Loss 9.5623(10.3520) | Error 0.1489(0.1452) Steps 0(0.00) | Grad Norm 5.8149(7.6251) | Total Time 0.00(0.00)\n",
      "Iter 8050 | Time 20.2749(20.1080) | Bit/dim 3.6219(3.6179) | Xent 0.3663(0.4042) | Loss 9.5805(10.1191) | Error 0.1222(0.1433) Steps 0(0.00) | Grad Norm 6.9125(7.7142) | Total Time 0.00(0.00)\n",
      "Iter 8060 | Time 19.7997(20.0707) | Bit/dim 3.6317(3.6159) | Xent 0.4558(0.4070) | Loss 9.4421(9.9433) | Error 0.1644(0.1448) Steps 0(0.00) | Grad Norm 9.0433(7.4166) | Total Time 0.00(0.00)\n",
      "Iter 8070 | Time 19.4082(20.2099) | Bit/dim 3.5827(3.6164) | Xent 0.4112(0.4076) | Loss 9.3353(9.8218) | Error 0.1378(0.1439) Steps 0(0.00) | Grad Norm 6.3696(7.2970) | Total Time 0.00(0.00)\n",
      "Iter 8080 | Time 19.8837(20.2459) | Bit/dim 3.6393(3.6161) | Xent 0.4460(0.4070) | Loss 9.3882(9.7289) | Error 0.1689(0.1445) Steps 0(0.00) | Grad Norm 6.4783(7.1135) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0147 | Time 99.2420, Epoch Time 1227.7276(1160.3812), Bit/dim 3.6215(best: 3.6137), Xent 0.6838, Loss 3.9634, Error 0.2217(best: 0.2216)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8090 | Time 19.5698(20.2211) | Bit/dim 3.6135(3.6167) | Xent 0.4206(0.4122) | Loss 9.5307(10.4722) | Error 0.1467(0.1462) Steps 0(0.00) | Grad Norm 10.5386(7.8385) | Total Time 0.00(0.00)\n",
      "Iter 8100 | Time 21.9260(20.1874) | Bit/dim 3.6171(3.6171) | Xent 0.4145(0.4088) | Loss 9.6506(10.2063) | Error 0.1456(0.1462) Steps 0(0.00) | Grad Norm 6.7454(7.4421) | Total Time 0.00(0.00)\n",
      "Iter 8110 | Time 20.6875(20.3720) | Bit/dim 3.6079(3.6139) | Xent 0.3654(0.4028) | Loss 9.3625(10.0148) | Error 0.1278(0.1434) Steps 0(0.00) | Grad Norm 6.4360(7.2204) | Total Time 0.00(0.00)\n",
      "Iter 8120 | Time 21.0710(20.6003) | Bit/dim 3.6659(3.6157) | Xent 0.3727(0.4000) | Loss 9.6936(9.8754) | Error 0.1278(0.1426) Steps 0(0.00) | Grad Norm 5.2528(7.0562) | Total Time 0.00(0.00)\n",
      "Iter 8130 | Time 20.3259(20.5000) | Bit/dim 3.6405(3.6136) | Xent 0.3747(0.4000) | Loss 9.6180(9.7557) | Error 0.1289(0.1424) Steps 0(0.00) | Grad Norm 12.8749(7.4859) | Total Time 0.00(0.00)\n",
      "Iter 8140 | Time 18.6080(20.4107) | Bit/dim 3.6834(3.6143) | Xent 0.4191(0.4003) | Loss 9.5756(9.6739) | Error 0.1444(0.1421) Steps 0(0.00) | Grad Norm 11.0534(7.7498) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0148 | Time 99.1820, Epoch Time 1246.8654(1162.9757), Bit/dim 3.6180(best: 3.6137), Xent 0.6944, Loss 3.9652, Error 0.2205(best: 0.2216)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8150 | Time 20.0752(20.2453) | Bit/dim 3.6303(3.6148) | Xent 0.4028(0.4039) | Loss 9.3227(10.3512) | Error 0.1444(0.1426) Steps 0(0.00) | Grad Norm 9.9134(8.2791) | Total Time 0.00(0.00)\n",
      "Iter 8160 | Time 19.1180(20.1552) | Bit/dim 3.6087(3.6122) | Xent 0.3518(0.4014) | Loss 9.3878(10.1166) | Error 0.1156(0.1423) Steps 0(0.00) | Grad Norm 5.1546(7.8934) | Total Time 0.00(0.00)\n",
      "Iter 8170 | Time 19.4095(20.2808) | Bit/dim 3.6114(3.6127) | Xent 0.3851(0.3958) | Loss 9.3362(9.9471) | Error 0.1389(0.1404) Steps 0(0.00) | Grad Norm 6.9259(7.3343) | Total Time 0.00(0.00)\n",
      "Iter 8180 | Time 21.9922(20.3762) | Bit/dim 3.5932(3.6128) | Xent 0.3510(0.3941) | Loss 9.5972(9.8183) | Error 0.1222(0.1396) Steps 0(0.00) | Grad Norm 6.5703(7.1683) | Total Time 0.00(0.00)\n",
      "Iter 8190 | Time 20.0783(20.4122) | Bit/dim 3.6451(3.6122) | Xent 0.3765(0.3943) | Loss 9.5736(9.7285) | Error 0.1289(0.1403) Steps 0(0.00) | Grad Norm 6.6347(7.1125) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0149 | Time 98.3106, Epoch Time 1232.8763(1165.0727), Bit/dim 3.6225(best: 3.6137), Xent 0.6590, Loss 3.9520, Error 0.2197(best: 0.2205)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8200 | Time 20.7468(20.3321) | Bit/dim 3.6093(3.6135) | Xent 0.4074(0.3936) | Loss 9.3937(10.4041) | Error 0.1467(0.1408) Steps 0(0.00) | Grad Norm 10.0081(7.3736) | Total Time 0.00(0.00)\n",
      "Iter 8210 | Time 20.6312(20.3210) | Bit/dim 3.6424(3.6133) | Xent 0.3492(0.3871) | Loss 9.5348(10.1380) | Error 0.1400(0.1395) Steps 0(0.00) | Grad Norm 6.2952(7.2095) | Total Time 0.00(0.00)\n",
      "Iter 8220 | Time 19.3806(20.4324) | Bit/dim 3.5996(3.6124) | Xent 0.3802(0.3883) | Loss 9.5010(9.9673) | Error 0.1356(0.1392) Steps 0(0.00) | Grad Norm 7.3727(7.2214) | Total Time 0.00(0.00)\n",
      "Iter 8230 | Time 20.4273(20.4478) | Bit/dim 3.6052(3.6102) | Xent 0.3678(0.3997) | Loss 9.2095(9.8470) | Error 0.1311(0.1429) Steps 0(0.00) | Grad Norm 4.5730(7.2426) | Total Time 0.00(0.00)\n",
      "Iter 8240 | Time 20.7078(20.5506) | Bit/dim 3.6135(3.6136) | Xent 0.3912(0.4020) | Loss 9.6164(9.7574) | Error 0.1389(0.1437) Steps 0(0.00) | Grad Norm 9.0673(7.7970) | Total Time 0.00(0.00)\n",
      "Iter 8250 | Time 22.4163(20.5023) | Bit/dim 3.5999(3.6151) | Xent 0.3565(0.3970) | Loss 9.5106(9.6805) | Error 0.1256(0.1422) Steps 0(0.00) | Grad Norm 5.1792(7.6657) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0150 | Time 99.8028, Epoch Time 1243.8428(1167.4358), Bit/dim 3.6221(best: 3.6137), Xent 0.6925, Loss 3.9684, Error 0.2198(best: 0.2197)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8260 | Time 21.1777(20.5628) | Bit/dim 3.6219(3.6141) | Xent 0.3861(0.3886) | Loss 9.3988(10.3323) | Error 0.1367(0.1395) Steps 0(0.00) | Grad Norm 4.4121(7.3779) | Total Time 0.00(0.00)\n",
      "Iter 8270 | Time 21.5424(20.7379) | Bit/dim 3.6126(3.6154) | Xent 0.4545(0.3896) | Loss 9.5515(10.1072) | Error 0.1567(0.1398) Steps 0(0.00) | Grad Norm 12.4489(7.6699) | Total Time 0.00(0.00)\n",
      "Iter 8280 | Time 20.5824(20.7245) | Bit/dim 3.6547(3.6152) | Xent 0.3925(0.3974) | Loss 9.5274(9.9671) | Error 0.1267(0.1426) Steps 0(0.00) | Grad Norm 4.9761(7.8301) | Total Time 0.00(0.00)\n",
      "Iter 8290 | Time 19.4671(20.6397) | Bit/dim 3.6257(3.6149) | Xent 0.3782(0.3967) | Loss 9.1912(9.8290) | Error 0.1322(0.1424) Steps 0(0.00) | Grad Norm 8.5596(7.4768) | Total Time 0.00(0.00)\n",
      "Iter 8300 | Time 21.1971(20.8602) | Bit/dim 3.6284(3.6132) | Xent 0.4000(0.3945) | Loss 9.4429(9.7301) | Error 0.1378(0.1413) Steps 0(0.00) | Grad Norm 6.0290(7.4417) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0151 | Time 99.2990, Epoch Time 1267.1280(1170.4266), Bit/dim 3.6122(best: 3.6137), Xent 0.7229, Loss 3.9737, Error 0.2220(best: 0.2197)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8310 | Time 18.7774(20.7875) | Bit/dim 3.6328(3.6130) | Xent 0.3890(0.3988) | Loss 9.1880(10.5036) | Error 0.1367(0.1418) Steps 0(0.00) | Grad Norm 7.0258(7.5159) | Total Time 0.00(0.00)\n",
      "Iter 8320 | Time 20.0212(20.6409) | Bit/dim 3.5853(3.6114) | Xent 0.3755(0.3937) | Loss 9.3900(10.2195) | Error 0.1189(0.1395) Steps 0(0.00) | Grad Norm 7.9683(7.3343) | Total Time 0.00(0.00)\n",
      "Iter 8330 | Time 20.5586(20.7132) | Bit/dim 3.6251(3.6133) | Xent 0.3392(0.3869) | Loss 9.2975(10.0076) | Error 0.1211(0.1380) Steps 0(0.00) | Grad Norm 4.5717(7.0968) | Total Time 0.00(0.00)\n",
      "Iter 8340 | Time 19.5074(20.6127) | Bit/dim 3.5878(3.6107) | Xent 0.3882(0.3891) | Loss 9.4424(9.8635) | Error 0.1422(0.1395) Steps 0(0.00) | Grad Norm 6.4764(7.1330) | Total Time 0.00(0.00)\n",
      "Iter 8350 | Time 21.3412(20.5175) | Bit/dim 3.6220(3.6108) | Xent 0.3807(0.3872) | Loss 9.3908(9.7462) | Error 0.1244(0.1377) Steps 0(0.00) | Grad Norm 7.0157(6.9288) | Total Time 0.00(0.00)\n",
      "Iter 8360 | Time 20.7541(20.4913) | Bit/dim 3.6033(3.6122) | Xent 0.4517(0.3964) | Loss 9.5044(9.6788) | Error 0.1544(0.1417) Steps 0(0.00) | Grad Norm 12.0761(7.8694) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0152 | Time 100.1559, Epoch Time 1240.0563(1172.5155), Bit/dim 3.6288(best: 3.6122), Xent 0.6934, Loss 3.9755, Error 0.2204(best: 0.2197)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8370 | Time 19.6617(20.3952) | Bit/dim 3.6191(3.6156) | Xent 0.4420(0.3954) | Loss 9.4572(10.3048) | Error 0.1500(0.1406) Steps 0(0.00) | Grad Norm 14.3467(8.1607) | Total Time 0.00(0.00)\n",
      "Iter 8380 | Time 21.9849(20.4558) | Bit/dim 3.5781(3.6119) | Xent 0.4162(0.3939) | Loss 9.4114(10.0715) | Error 0.1433(0.1408) Steps 0(0.00) | Grad Norm 10.7578(8.3115) | Total Time 0.00(0.00)\n",
      "Iter 8390 | Time 20.0852(20.4951) | Bit/dim 3.6369(3.6150) | Xent 0.4202(0.3989) | Loss 9.3391(9.9158) | Error 0.1467(0.1417) Steps 0(0.00) | Grad Norm 8.0580(8.5504) | Total Time 0.00(0.00)\n",
      "Iter 8400 | Time 19.9448(20.3065) | Bit/dim 3.6475(3.6138) | Xent 0.3650(0.3970) | Loss 9.4954(9.7886) | Error 0.1200(0.1401) Steps 0(0.00) | Grad Norm 6.1046(8.3765) | Total Time 0.00(0.00)\n",
      "Iter 8410 | Time 20.7581(20.4400) | Bit/dim 3.6003(3.6131) | Xent 0.3392(0.3940) | Loss 9.4536(9.7083) | Error 0.1200(0.1389) Steps 0(0.00) | Grad Norm 5.8292(7.9873) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0153 | Time 101.2682, Epoch Time 1241.9640(1174.5989), Bit/dim 3.6176(best: 3.6122), Xent 0.6855, Loss 3.9603, Error 0.2198(best: 0.2197)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8420 | Time 21.0191(20.4480) | Bit/dim 3.5922(3.6134) | Xent 0.3395(0.3849) | Loss 9.2624(10.4503) | Error 0.1189(0.1360) Steps 0(0.00) | Grad Norm 5.3250(7.7130) | Total Time 0.00(0.00)\n",
      "Iter 8430 | Time 20.0804(20.5434) | Bit/dim 3.5722(3.6131) | Xent 0.4018(0.3868) | Loss 9.1056(10.1731) | Error 0.1478(0.1366) Steps 0(0.00) | Grad Norm 10.9759(8.2482) | Total Time 0.00(0.00)\n",
      "Iter 8440 | Time 21.5082(20.7593) | Bit/dim 3.5873(3.6134) | Xent 0.3821(0.3910) | Loss 9.4424(9.9999) | Error 0.1433(0.1384) Steps 0(0.00) | Grad Norm 5.8737(8.2769) | Total Time 0.00(0.00)\n",
      "Iter 8450 | Time 20.1158(20.7756) | Bit/dim 3.6141(3.6123) | Xent 0.4412(0.4035) | Loss 9.5390(9.8597) | Error 0.1633(0.1437) Steps 0(0.00) | Grad Norm 7.2526(8.6803) | Total Time 0.00(0.00)\n",
      "Iter 8460 | Time 21.2408(20.7343) | Bit/dim 3.6160(3.6125) | Xent 0.3594(0.4064) | Loss 9.5775(9.7685) | Error 0.1344(0.1448) Steps 0(0.00) | Grad Norm 6.1612(8.1979) | Total Time 0.00(0.00)\n",
      "Iter 8470 | Time 19.7285(20.6439) | Bit/dim 3.6018(3.6122) | Xent 0.4422(0.4097) | Loss 9.5492(9.6973) | Error 0.1589(0.1455) Steps 0(0.00) | Grad Norm 11.9819(8.3106) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0154 | Time 102.5773, Epoch Time 1263.6674(1177.2710), Bit/dim 3.6079(best: 3.6122), Xent 0.6739, Loss 3.9449, Error 0.2176(best: 0.2197)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8480 | Time 20.0843(20.5877) | Bit/dim 3.6125(3.6128) | Xent 0.3132(0.4001) | Loss 9.2031(10.3559) | Error 0.1222(0.1421) Steps 0(0.00) | Grad Norm 6.8389(7.9393) | Total Time 0.00(0.00)\n",
      "Iter 8490 | Time 21.1359(20.5377) | Bit/dim 3.6198(3.6146) | Xent 0.3392(0.3919) | Loss 9.5261(10.1175) | Error 0.1133(0.1388) Steps 0(0.00) | Grad Norm 6.3043(7.8673) | Total Time 0.00(0.00)\n",
      "Iter 8500 | Time 20.2520(20.4922) | Bit/dim 3.6018(3.6131) | Xent 0.3818(0.3922) | Loss 9.3369(9.9378) | Error 0.1356(0.1381) Steps 0(0.00) | Grad Norm 6.5027(7.8819) | Total Time 0.00(0.00)\n",
      "Iter 8510 | Time 20.0252(20.5508) | Bit/dim 3.5839(3.6106) | Xent 0.3655(0.3904) | Loss 9.3855(9.8102) | Error 0.1200(0.1374) Steps 0(0.00) | Grad Norm 7.2691(7.7040) | Total Time 0.00(0.00)\n",
      "Iter 8520 | Time 20.1957(20.5163) | Bit/dim 3.5916(3.6092) | Xent 0.3098(0.3865) | Loss 9.2182(9.6975) | Error 0.1167(0.1371) Steps 0(0.00) | Grad Norm 6.2212(7.6823) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0155 | Time 100.4730, Epoch Time 1247.8212(1179.3875), Bit/dim 3.6152(best: 3.6079), Xent 0.7193, Loss 3.9749, Error 0.2242(best: 0.2176)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8530 | Time 20.3904(20.5167) | Bit/dim 3.6196(3.6116) | Xent 0.4025(0.3844) | Loss 9.4227(10.4748) | Error 0.1400(0.1365) Steps 0(0.00) | Grad Norm 7.8711(7.5608) | Total Time 0.00(0.00)\n",
      "Iter 8540 | Time 22.9364(20.6038) | Bit/dim 3.6130(3.6098) | Xent 0.3668(0.3821) | Loss 9.4779(10.1979) | Error 0.1333(0.1357) Steps 0(0.00) | Grad Norm 6.4501(7.6304) | Total Time 0.00(0.00)\n",
      "Iter 8550 | Time 18.1806(20.4655) | Bit/dim 3.6046(3.6105) | Xent 0.4533(0.3913) | Loss 9.4695(10.0113) | Error 0.1500(0.1396) Steps 0(0.00) | Grad Norm 6.9705(7.8513) | Total Time 0.00(0.00)\n",
      "Iter 8560 | Time 20.0545(20.5508) | Bit/dim 3.6206(3.6123) | Xent 0.4353(0.3938) | Loss 9.3810(9.8603) | Error 0.1478(0.1409) Steps 0(0.00) | Grad Norm 6.5180(7.8611) | Total Time 0.00(0.00)\n",
      "Iter 8570 | Time 20.6217(20.5618) | Bit/dim 3.6381(3.6131) | Xent 0.3529(0.3907) | Loss 9.4695(9.7742) | Error 0.1322(0.1387) Steps 0(0.00) | Grad Norm 6.4022(7.6789) | Total Time 0.00(0.00)\n",
      "Iter 8580 | Time 20.8321(20.5894) | Bit/dim 3.5923(3.6121) | Xent 0.3864(0.3916) | Loss 9.4540(9.7021) | Error 0.1333(0.1391) Steps 0(0.00) | Grad Norm 10.3845(8.1116) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0156 | Time 102.3615, Epoch Time 1251.3316(1181.5458), Bit/dim 3.6211(best: 3.6079), Xent 0.7297, Loss 3.9859, Error 0.2327(best: 0.2176)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8590 | Time 21.6989(20.8407) | Bit/dim 3.6214(3.6147) | Xent 0.3541(0.3906) | Loss 9.4701(10.3647) | Error 0.1333(0.1392) Steps 0(0.00) | Grad Norm 10.3343(7.9186) | Total Time 0.00(0.00)\n",
      "Iter 8600 | Time 21.1477(20.7143) | Bit/dim 3.5934(3.6121) | Xent 0.3927(0.3847) | Loss 9.2662(10.1174) | Error 0.1278(0.1371) Steps 0(0.00) | Grad Norm 6.2631(7.3925) | Total Time 0.00(0.00)\n",
      "Iter 8610 | Time 20.5999(20.7143) | Bit/dim 3.6290(3.6114) | Xent 0.3102(0.3761) | Loss 9.5665(9.9601) | Error 0.1078(0.1345) Steps 0(0.00) | Grad Norm 7.4239(7.2289) | Total Time 0.00(0.00)\n",
      "Iter 8620 | Time 21.5808(20.6134) | Bit/dim 3.5929(3.6092) | Xent 0.4206(0.3762) | Loss 9.1942(9.8069) | Error 0.1500(0.1338) Steps 0(0.00) | Grad Norm 7.5873(6.8822) | Total Time 0.00(0.00)\n",
      "Iter 8630 | Time 21.9734(20.5936) | Bit/dim 3.5985(3.6092) | Xent 0.4239(0.3820) | Loss 9.4140(9.7155) | Error 0.1533(0.1354) Steps 0(0.00) | Grad Norm 8.6429(7.4173) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0157 | Time 101.0518, Epoch Time 1267.2836(1184.1179), Bit/dim 3.6152(best: 3.6079), Xent 0.6888, Loss 3.9596, Error 0.2170(best: 0.2176)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8640 | Time 18.7299(20.6884) | Bit/dim 3.6309(3.6080) | Xent 0.3649(0.3826) | Loss 9.4892(10.4671) | Error 0.1311(0.1355) Steps 0(0.00) | Grad Norm 6.2535(7.6116) | Total Time 0.00(0.00)\n",
      "Iter 8650 | Time 19.9089(20.6158) | Bit/dim 3.6100(3.6083) | Xent 0.4087(0.3861) | Loss 9.5834(10.1946) | Error 0.1511(0.1372) Steps 0(0.00) | Grad Norm 11.0761(7.9607) | Total Time 0.00(0.00)\n",
      "Iter 8660 | Time 20.3014(20.4924) | Bit/dim 3.6370(3.6095) | Xent 0.4917(0.3857) | Loss 9.3972(9.9734) | Error 0.1744(0.1371) Steps 0(0.00) | Grad Norm 13.6399(8.0116) | Total Time 0.00(0.00)\n",
      "Iter 8670 | Time 19.4491(20.6792) | Bit/dim 3.6117(3.6114) | Xent 0.3812(0.3839) | Loss 9.5611(9.8623) | Error 0.1367(0.1366) Steps 0(0.00) | Grad Norm 8.6687(8.2350) | Total Time 0.00(0.00)\n",
      "Iter 8680 | Time 18.7810(20.7153) | Bit/dim 3.6230(3.6118) | Xent 0.4845(0.3852) | Loss 9.5571(9.7635) | Error 0.1911(0.1373) Steps 0(0.00) | Grad Norm 10.3902(8.3374) | Total Time 0.00(0.00)\n",
      "Iter 8690 | Time 20.3634(20.7608) | Bit/dim 3.5981(3.6129) | Xent 0.3929(0.3878) | Loss 9.4121(9.6867) | Error 0.1333(0.1385) Steps 0(0.00) | Grad Norm 7.0659(8.5739) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0158 | Time 102.2497, Epoch Time 1251.8619(1186.1503), Bit/dim 3.6146(best: 3.6079), Xent 0.6986, Loss 3.9639, Error 0.2278(best: 0.2170)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8700 | Time 20.4572(20.7658) | Bit/dim 3.6194(3.6125) | Xent 0.3371(0.3809) | Loss 9.5660(10.3514) | Error 0.1167(0.1359) Steps 0(0.00) | Grad Norm 7.1725(8.2181) | Total Time 0.00(0.00)\n",
      "Iter 8710 | Time 19.6521(20.7599) | Bit/dim 3.6511(3.6125) | Xent 0.3625(0.3819) | Loss 9.4153(10.1101) | Error 0.1278(0.1349) Steps 0(0.00) | Grad Norm 7.5739(8.2900) | Total Time 0.00(0.00)\n",
      "Iter 8720 | Time 19.9994(20.8573) | Bit/dim 3.6054(3.6144) | Xent 0.3636(0.3747) | Loss 9.4795(9.9492) | Error 0.1344(0.1336) Steps 0(0.00) | Grad Norm 6.4730(7.8085) | Total Time 0.00(0.00)\n",
      "Iter 8730 | Time 20.2320(20.7636) | Bit/dim 3.5939(3.6123) | Xent 0.3760(0.3750) | Loss 9.3488(9.8277) | Error 0.1500(0.1344) Steps 0(0.00) | Grad Norm 6.2365(7.6336) | Total Time 0.00(0.00)\n",
      "Iter 8740 | Time 22.3992(20.8298) | Bit/dim 3.6177(3.6107) | Xent 0.3583(0.3764) | Loss 9.3455(9.7230) | Error 0.1322(0.1352) Steps 0(0.00) | Grad Norm 4.5116(7.1584) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0159 | Time 102.4985, Epoch Time 1266.8148(1188.5702), Bit/dim 3.6108(best: 3.6079), Xent 0.7229, Loss 3.9722, Error 0.2250(best: 0.2170)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8750 | Time 21.0201(20.9419) | Bit/dim 3.5788(3.6090) | Xent 0.3748(0.3772) | Loss 9.4870(10.4786) | Error 0.1333(0.1348) Steps 0(0.00) | Grad Norm 10.3923(7.1471) | Total Time 0.00(0.00)\n",
      "Iter 8760 | Time 19.9045(24.6892) | Bit/dim 3.5826(3.6083) | Xent 0.3083(0.3687) | Loss 9.4204(10.3788) | Error 0.1100(0.1313) Steps 0(0.00) | Grad Norm 4.9906(7.0040) | Total Time 0.00(0.00)\n",
      "Iter 8770 | Time 19.8409(23.6769) | Bit/dim 3.5901(3.6045) | Xent 0.4295(0.3683) | Loss 9.4757(10.1293) | Error 0.1489(0.1310) Steps 0(0.00) | Grad Norm 10.7260(6.9231) | Total Time 0.00(0.00)\n",
      "Iter 8780 | Time 20.8029(23.1005) | Bit/dim 3.5982(3.6065) | Xent 0.4573(0.3735) | Loss 9.6503(9.9729) | Error 0.1556(0.1329) Steps 0(0.00) | Grad Norm 9.2886(7.4537) | Total Time 0.00(0.00)\n",
      "Iter 8790 | Time 22.4605(22.6139) | Bit/dim 3.5940(3.6095) | Xent 0.3688(0.3753) | Loss 9.5480(9.8389) | Error 0.1389(0.1347) Steps 0(0.00) | Grad Norm 4.4838(7.3386) | Total Time 0.00(0.00)\n",
      "Iter 8800 | Time 19.8202(22.1703) | Bit/dim 3.6086(3.6101) | Xent 0.3611(0.3715) | Loss 9.4232(9.7345) | Error 0.1456(0.1332) Steps 0(0.00) | Grad Norm 5.2055(7.0880) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0160 | Time 103.0232, Epoch Time 1417.3688(1195.4342), Bit/dim 3.6118(best: 3.6079), Xent 0.6711, Loss 3.9473, Error 0.2129(best: 0.2170)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8810 | Time 19.6479(21.8382) | Bit/dim 3.6345(3.6095) | Xent 0.3715(0.3630) | Loss 9.4661(10.3729) | Error 0.1311(0.1296) Steps 0(0.00) | Grad Norm 5.4718(7.0397) | Total Time 0.00(0.00)\n",
      "Iter 8820 | Time 20.3957(21.4124) | Bit/dim 3.6291(3.6074) | Xent 0.3426(0.3651) | Loss 9.4671(10.1268) | Error 0.1178(0.1310) Steps 0(0.00) | Grad Norm 5.8876(7.6079) | Total Time 0.00(0.00)\n",
      "Iter 8830 | Time 21.1386(21.3409) | Bit/dim 3.6146(3.6101) | Xent 0.3520(0.3619) | Loss 9.3589(9.9466) | Error 0.1144(0.1288) Steps 0(0.00) | Grad Norm 7.3717(7.4849) | Total Time 0.00(0.00)\n",
      "Iter 8840 | Time 21.7376(21.5059) | Bit/dim 3.6148(3.6118) | Xent 0.3961(0.3635) | Loss 9.3321(9.8288) | Error 0.1433(0.1295) Steps 0(0.00) | Grad Norm 9.1452(7.3531) | Total Time 0.00(0.00)\n",
      "Iter 8850 | Time 20.0500(21.2880) | Bit/dim 3.5836(3.6075) | Xent 0.3863(0.3670) | Loss 9.4058(9.7016) | Error 0.1411(0.1307) Steps 0(0.00) | Grad Norm 8.5761(7.3263) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0161 | Time 102.3731, Epoch Time 1272.7296(1197.7530), Bit/dim 3.6182(best: 3.6079), Xent 0.7078, Loss 3.9721, Error 0.2280(best: 0.2129)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8860 | Time 19.8698(21.1783) | Bit/dim 3.5868(3.6075) | Xent 0.3983(0.3710) | Loss 9.4661(10.4766) | Error 0.1367(0.1329) Steps 0(0.00) | Grad Norm 6.7980(7.2685) | Total Time 0.00(0.00)\n",
      "Iter 8870 | Time 22.6251(21.1748) | Bit/dim 3.5876(3.6107) | Xent 0.3593(0.3768) | Loss 9.4497(10.2331) | Error 0.1389(0.1349) Steps 0(0.00) | Grad Norm 9.7949(8.1542) | Total Time 0.00(0.00)\n",
      "Iter 8880 | Time 20.6319(21.1328) | Bit/dim 3.6147(3.6141) | Xent 0.3977(0.3854) | Loss 9.2076(10.0494) | Error 0.1444(0.1382) Steps 0(0.00) | Grad Norm 9.0191(8.3393) | Total Time 0.00(0.00)\n",
      "Iter 8890 | Time 21.1794(21.1649) | Bit/dim 3.6234(3.6107) | Xent 0.3929(0.3828) | Loss 9.4409(9.8838) | Error 0.1333(0.1367) Steps 0(0.00) | Grad Norm 11.3986(8.2476) | Total Time 0.00(0.00)\n",
      "Iter 8900 | Time 21.6049(21.4156) | Bit/dim 3.6389(3.6089) | Xent 0.3888(0.3817) | Loss 9.5778(9.7856) | Error 0.1256(0.1356) Steps 0(0.00) | Grad Norm 7.5562(8.0967) | Total Time 0.00(0.00)\n",
      "Iter 8910 | Time 21.1081(21.2770) | Bit/dim 3.5907(3.6070) | Xent 0.3180(0.3752) | Loss 9.4036(9.6894) | Error 0.1111(0.1336) Steps 0(0.00) | Grad Norm 7.5331(7.6050) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0162 | Time 102.1865, Epoch Time 1290.5334(1200.5364), Bit/dim 3.6116(best: 3.6079), Xent 0.6987, Loss 3.9610, Error 0.2157(best: 0.2129)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8920 | Time 21.7854(21.2506) | Bit/dim 3.5795(3.6033) | Xent 0.3909(0.3632) | Loss 9.4821(10.3453) | Error 0.1422(0.1297) Steps 0(0.00) | Grad Norm 5.8668(7.3001) | Total Time 0.00(0.00)\n",
      "Iter 8930 | Time 20.6920(21.1242) | Bit/dim 3.5973(3.6037) | Xent 0.3605(0.3577) | Loss 9.2162(10.0848) | Error 0.1222(0.1278) Steps 0(0.00) | Grad Norm 8.2225(7.0229) | Total Time 0.00(0.00)\n",
      "Iter 8940 | Time 20.4672(21.0600) | Bit/dim 3.6172(3.6026) | Xent 0.3571(0.3610) | Loss 9.3584(9.8942) | Error 0.1256(0.1287) Steps 0(0.00) | Grad Norm 4.7204(7.2191) | Total Time 0.00(0.00)\n",
      "Iter 8950 | Time 19.5530(20.9531) | Bit/dim 3.6087(3.6062) | Xent 0.3694(0.3607) | Loss 9.5339(9.7848) | Error 0.1344(0.1292) Steps 0(0.00) | Grad Norm 10.3596(7.2788) | Total Time 0.00(0.00)\n",
      "Iter 8960 | Time 20.1669(21.0708) | Bit/dim 3.6161(3.6051) | Xent 0.3538(0.3597) | Loss 9.3123(9.6771) | Error 0.1178(0.1282) Steps 0(0.00) | Grad Norm 6.0467(7.2909) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0163 | Time 101.1942, Epoch Time 1275.4637(1202.7843), Bit/dim 3.6133(best: 3.6079), Xent 0.7174, Loss 3.9720, Error 0.2217(best: 0.2129)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8970 | Time 19.8608(21.0537) | Bit/dim 3.5935(3.6065) | Xent 0.3818(0.3559) | Loss 9.5590(10.4669) | Error 0.1267(0.1276) Steps 0(0.00) | Grad Norm 9.6589(7.3576) | Total Time 0.00(0.00)\n",
      "Iter 8980 | Time 20.7785(21.0798) | Bit/dim 3.6229(3.6063) | Xent 0.3348(0.3643) | Loss 9.1826(10.2092) | Error 0.1133(0.1304) Steps 0(0.00) | Grad Norm 7.9712(8.2286) | Total Time 0.00(0.00)\n",
      "Iter 8990 | Time 22.7663(21.0537) | Bit/dim 3.6047(3.6078) | Xent 0.3867(0.3630) | Loss 9.5368(10.0209) | Error 0.1433(0.1292) Steps 0(0.00) | Grad Norm 6.4556(7.9938) | Total Time 0.00(0.00)\n",
      "Iter 9000 | Time 20.4208(20.7669) | Bit/dim 3.6191(3.6059) | Xent 0.3235(0.3657) | Loss 9.2887(9.8623) | Error 0.1211(0.1300) Steps 0(0.00) | Grad Norm 6.1324(7.6306) | Total Time 0.00(0.00)\n",
      "Iter 9010 | Time 20.3784(20.7675) | Bit/dim 3.6299(3.6078) | Xent 0.3853(0.3612) | Loss 9.6452(9.7480) | Error 0.1378(0.1290) Steps 0(0.00) | Grad Norm 8.2938(7.1844) | Total Time 0.00(0.00)\n",
      "Iter 9020 | Time 21.1896(20.9332) | Bit/dim 3.5999(3.6056) | Xent 0.4198(0.3615) | Loss 9.5455(9.6573) | Error 0.1444(0.1286) Steps 0(0.00) | Grad Norm 9.1626(7.3701) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0164 | Time 101.7513, Epoch Time 1265.8864(1204.6773), Bit/dim 3.6111(best: 3.6079), Xent 0.7111, Loss 3.9667, Error 0.2175(best: 0.2129)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9030 | Time 21.1746(20.9174) | Bit/dim 3.6196(3.6076) | Xent 0.3730(0.3603) | Loss 9.5535(10.3412) | Error 0.1289(0.1280) Steps 0(0.00) | Grad Norm 8.8293(7.3659) | Total Time 0.00(0.00)\n",
      "Iter 9040 | Time 18.9546(20.9696) | Bit/dim 3.5904(3.6055) | Xent 0.2990(0.3563) | Loss 9.1120(10.0938) | Error 0.1067(0.1272) Steps 0(0.00) | Grad Norm 6.0543(7.2601) | Total Time 0.00(0.00)\n",
      "Iter 9050 | Time 19.4468(20.9094) | Bit/dim 3.6203(3.6061) | Xent 0.3666(0.3521) | Loss 9.3706(9.9245) | Error 0.1489(0.1264) Steps 0(0.00) | Grad Norm 5.7501(7.0642) | Total Time 0.00(0.00)\n",
      "Iter 9060 | Time 23.1587(21.0546) | Bit/dim 3.6371(3.6074) | Xent 0.3724(0.3472) | Loss 9.7485(9.8096) | Error 0.1278(0.1244) Steps 0(0.00) | Grad Norm 6.5783(6.6964) | Total Time 0.00(0.00)\n",
      "Iter 9070 | Time 21.9495(21.1154) | Bit/dim 3.5875(3.6038) | Xent 0.3675(0.3551) | Loss 9.5061(9.7321) | Error 0.1367(0.1266) Steps 0(0.00) | Grad Norm 8.3535(7.1227) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0165 | Time 102.2315, Epoch Time 1280.1461(1206.9414), Bit/dim 3.6132(best: 3.6079), Xent 0.6977, Loss 3.9621, Error 0.2197(best: 0.2129)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9080 | Time 19.8259(21.0605) | Bit/dim 3.5883(3.6024) | Xent 0.3150(0.3591) | Loss 9.4570(10.4794) | Error 0.1067(0.1276) Steps 0(0.00) | Grad Norm 6.8706(7.2242) | Total Time 0.00(0.00)\n",
      "Iter 9090 | Time 20.3638(20.9566) | Bit/dim 3.6124(3.6007) | Xent 0.3553(0.3587) | Loss 9.5713(10.2038) | Error 0.1344(0.1285) Steps 0(0.00) | Grad Norm 7.5104(7.5275) | Total Time 0.00(0.00)\n",
      "Iter 9100 | Time 19.3995(21.0057) | Bit/dim 3.6277(3.6025) | Xent 0.3269(0.3552) | Loss 9.4646(10.0117) | Error 0.1100(0.1265) Steps 0(0.00) | Grad Norm 9.0303(7.4536) | Total Time 0.00(0.00)\n",
      "Iter 9110 | Time 21.0127(21.2361) | Bit/dim 3.6187(3.6050) | Xent 0.3984(0.3551) | Loss 9.5143(9.8696) | Error 0.1322(0.1266) Steps 0(0.00) | Grad Norm 10.2130(7.5512) | Total Time 0.00(0.00)\n",
      "Iter 9120 | Time 21.0202(21.0914) | Bit/dim 3.6000(3.6070) | Xent 0.3719(0.3609) | Loss 9.5799(9.7746) | Error 0.1156(0.1275) Steps 0(0.00) | Grad Norm 8.6505(7.8793) | Total Time 0.00(0.00)\n",
      "Iter 9130 | Time 20.1567(21.0761) | Bit/dim 3.5791(3.6101) | Xent 0.3184(0.3574) | Loss 9.3825(9.6989) | Error 0.1267(0.1272) Steps 0(0.00) | Grad Norm 8.9095(7.5394) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0166 | Time 102.6376, Epoch Time 1278.8617(1209.0990), Bit/dim 3.6160(best: 3.6079), Xent 0.6950, Loss 3.9635, Error 0.2156(best: 0.2129)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9140 | Time 21.8306(21.1670) | Bit/dim 3.6065(3.6081) | Xent 0.2776(0.3532) | Loss 9.3547(10.3383) | Error 0.0956(0.1245) Steps 0(0.00) | Grad Norm 5.5771(7.3137) | Total Time 0.00(0.00)\n",
      "Iter 9150 | Time 23.6235(21.2861) | Bit/dim 3.5901(3.6069) | Xent 0.3591(0.3431) | Loss 9.4045(10.0964) | Error 0.1300(0.1208) Steps 0(0.00) | Grad Norm 5.1414(6.8659) | Total Time 0.00(0.00)\n",
      "Iter 9160 | Time 21.4641(21.3677) | Bit/dim 3.5495(3.6035) | Xent 0.3223(0.3421) | Loss 9.1870(9.9225) | Error 0.1133(0.1206) Steps 0(0.00) | Grad Norm 4.8836(6.6465) | Total Time 0.00(0.00)\n",
      "Iter 9170 | Time 20.6239(21.2709) | Bit/dim 3.5879(3.6041) | Xent 0.4078(0.3453) | Loss 9.6775(9.8255) | Error 0.1322(0.1216) Steps 0(0.00) | Grad Norm 9.9126(7.1133) | Total Time 0.00(0.00)\n",
      "Iter 9180 | Time 19.5482(21.1796) | Bit/dim 3.5993(3.6051) | Xent 0.3376(0.3535) | Loss 9.0726(9.7276) | Error 0.1256(0.1251) Steps 0(0.00) | Grad Norm 5.0321(7.2050) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0167 | Time 102.6098, Epoch Time 1290.4803(1211.5404), Bit/dim 3.6020(best: 3.6079), Xent 0.7188, Loss 3.9614, Error 0.2208(best: 0.2129)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9190 | Time 22.1547(21.1746) | Bit/dim 3.5860(3.6039) | Xent 0.2993(0.3447) | Loss 9.4704(10.4952) | Error 0.1033(0.1220) Steps 0(0.00) | Grad Norm 6.2965(6.9237) | Total Time 0.00(0.00)\n",
      "Iter 9200 | Time 22.1199(21.1833) | Bit/dim 3.5966(3.6051) | Xent 0.3216(0.3398) | Loss 9.2821(10.2157) | Error 0.1122(0.1215) Steps 0(0.00) | Grad Norm 5.6876(6.7375) | Total Time 0.00(0.00)\n",
      "Iter 9210 | Time 23.7573(21.3551) | Bit/dim 3.5862(3.6022) | Xent 0.3452(0.3404) | Loss 9.4786(10.0269) | Error 0.1278(0.1223) Steps 0(0.00) | Grad Norm 5.4322(6.8794) | Total Time 0.00(0.00)\n",
      "Iter 9220 | Time 22.0738(21.3932) | Bit/dim 3.6180(3.6025) | Xent 0.3425(0.3391) | Loss 9.5247(9.8790) | Error 0.1211(0.1219) Steps 0(0.00) | Grad Norm 5.6342(6.9514) | Total Time 0.00(0.00)\n",
      "Iter 9230 | Time 25.6011(21.7366) | Bit/dim 3.6317(3.6020) | Xent 0.3780(0.3487) | Loss 9.7114(9.7824) | Error 0.1378(0.1245) Steps 0(0.00) | Grad Norm 7.8385(7.4120) | Total Time 0.00(0.00)\n",
      "Iter 9240 | Time 21.0629(21.5192) | Bit/dim 3.6196(3.6042) | Xent 0.3582(0.3537) | Loss 9.2772(9.6873) | Error 0.1200(0.1254) Steps 0(0.00) | Grad Norm 8.4740(7.2641) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0168 | Time 104.3447, Epoch Time 1309.8827(1214.4907), Bit/dim 3.5990(best: 3.6020), Xent 0.6792, Loss 3.9386, Error 0.2152(best: 0.2129)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9250 | Time 21.1307(21.4386) | Bit/dim 3.6133(3.6059) | Xent 0.3123(0.3411) | Loss 9.4944(10.3820) | Error 0.1122(0.1215) Steps 0(0.00) | Grad Norm 4.1441(6.8449) | Total Time 0.00(0.00)\n",
      "Iter 9260 | Time 23.0813(21.4361) | Bit/dim 3.5611(3.6043) | Xent 0.3564(0.3427) | Loss 9.2659(10.1212) | Error 0.1256(0.1215) Steps 0(0.00) | Grad Norm 7.8874(7.0788) | Total Time 0.00(0.00)\n",
      "Iter 9270 | Time 20.6953(21.3219) | Bit/dim 3.5915(3.6043) | Xent 0.2688(0.3391) | Loss 9.4119(9.9480) | Error 0.0956(0.1205) Steps 0(0.00) | Grad Norm 5.6328(7.0844) | Total Time 0.00(0.00)\n",
      "Iter 9280 | Time 20.3341(21.2768) | Bit/dim 3.5970(3.6043) | Xent 0.3702(0.3496) | Loss 9.3216(9.8434) | Error 0.1244(0.1248) Steps 0(0.00) | Grad Norm 8.9100(7.8927) | Total Time 0.00(0.00)\n",
      "Iter 9290 | Time 19.8848(21.3393) | Bit/dim 3.6458(3.6052) | Xent 0.3447(0.3539) | Loss 9.5199(9.7521) | Error 0.1311(0.1271) Steps 0(0.00) | Grad Norm 6.1427(7.8976) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0169 | Time 103.8191, Epoch Time 1291.0429(1216.7873), Bit/dim 3.6099(best: 3.5990), Xent 0.7466, Loss 3.9832, Error 0.2289(best: 0.2129)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9300 | Time 20.0093(21.2041) | Bit/dim 3.6146(3.6065) | Xent 0.3587(0.3493) | Loss 9.3459(10.5315) | Error 0.1411(0.1254) Steps 0(0.00) | Grad Norm 10.4719(7.7415) | Total Time 0.00(0.00)\n",
      "Iter 9310 | Time 21.1410(21.2534) | Bit/dim 3.5963(3.6036) | Xent 0.3301(0.3428) | Loss 9.1566(10.2259) | Error 0.1178(0.1225) Steps 0(0.00) | Grad Norm 5.1566(7.4396) | Total Time 0.00(0.00)\n",
      "Iter 9320 | Time 21.9310(21.3428) | Bit/dim 3.5979(3.6036) | Xent 0.3709(0.3490) | Loss 9.4602(10.0338) | Error 0.1367(0.1245) Steps 0(0.00) | Grad Norm 9.7828(7.5539) | Total Time 0.00(0.00)\n",
      "Iter 9330 | Time 20.7121(21.2852) | Bit/dim 3.6001(3.6045) | Xent 0.3940(0.3609) | Loss 9.4896(9.8990) | Error 0.1422(0.1285) Steps 0(0.00) | Grad Norm 8.2223(8.1924) | Total Time 0.00(0.00)\n",
      "Iter 9340 | Time 20.0859(21.2660) | Bit/dim 3.6284(3.6079) | Xent 0.3539(0.3602) | Loss 9.3915(9.7813) | Error 0.1211(0.1279) Steps 0(0.00) | Grad Norm 5.1584(7.7616) | Total Time 0.00(0.00)\n",
      "Iter 9350 | Time 21.5380(21.3645) | Bit/dim 3.6007(3.6068) | Xent 0.3206(0.3544) | Loss 9.3608(9.6904) | Error 0.1133(0.1265) Steps 0(0.00) | Grad Norm 7.8498(7.3609) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0170 | Time 103.7934, Epoch Time 1295.0018(1219.1337), Bit/dim 3.6144(best: 3.5990), Xent 0.7187, Loss 3.9738, Error 0.2206(best: 0.2129)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9360 | Time 21.9909(21.5892) | Bit/dim 3.5822(3.6060) | Xent 0.3210(0.3558) | Loss 9.4164(10.3378) | Error 0.1111(0.1269) Steps 0(0.00) | Grad Norm 7.0112(7.7982) | Total Time 0.00(0.00)\n",
      "Iter 9370 | Time 21.2804(21.6272) | Bit/dim 3.5780(3.6063) | Xent 0.3755(0.3544) | Loss 9.3812(10.0934) | Error 0.1489(0.1277) Steps 0(0.00) | Grad Norm 11.8759(8.3358) | Total Time 0.00(0.00)\n",
      "Iter 9380 | Time 20.3049(21.5789) | Bit/dim 3.6040(3.6064) | Xent 0.3729(0.3500) | Loss 9.2814(9.9217) | Error 0.1333(0.1254) Steps 0(0.00) | Grad Norm 6.8918(8.1045) | Total Time 0.00(0.00)\n",
      "Iter 9390 | Time 20.4377(21.6084) | Bit/dim 3.6418(3.6089) | Xent 0.3392(0.3498) | Loss 9.5304(9.8049) | Error 0.1144(0.1245) Steps 0(0.00) | Grad Norm 8.6718(7.8294) | Total Time 0.00(0.00)\n",
      "Iter 9400 | Time 21.9566(21.5868) | Bit/dim 3.5977(3.6072) | Xent 0.3663(0.3509) | Loss 9.3636(9.7101) | Error 0.1278(0.1250) Steps 0(0.00) | Grad Norm 10.5559(7.9987) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0171 | Time 103.7597, Epoch Time 1313.5341(1221.9657), Bit/dim 3.6103(best: 3.5990), Xent 0.7117, Loss 3.9662, Error 0.2162(best: 0.2129)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9410 | Time 22.4665(21.5217) | Bit/dim 3.6012(3.6033) | Xent 0.2984(0.3447) | Loss 9.5120(10.4828) | Error 0.1067(0.1228) Steps 0(0.00) | Grad Norm 5.1572(7.5266) | Total Time 0.00(0.00)\n",
      "Iter 9420 | Time 21.6393(21.5200) | Bit/dim 3.6240(3.6048) | Xent 0.3089(0.3386) | Loss 9.5323(10.2083) | Error 0.1044(0.1201) Steps 0(0.00) | Grad Norm 5.7240(7.3368) | Total Time 0.00(0.00)\n",
      "Iter 9430 | Time 26.0738(21.5122) | Bit/dim 3.6107(3.6055) | Xent 0.4168(0.3444) | Loss 9.6735(10.0184) | Error 0.1422(0.1216) Steps 0(0.00) | Grad Norm 9.8363(7.7018) | Total Time 0.00(0.00)\n",
      "Iter 9440 | Time 24.6479(21.6139) | Bit/dim 3.6443(3.6048) | Xent 0.3189(0.3512) | Loss 9.4140(9.8794) | Error 0.1133(0.1246) Steps 0(0.00) | Grad Norm 9.3763(8.2882) | Total Time 0.00(0.00)\n",
      "Iter 9450 | Time 21.5069(21.6826) | Bit/dim 3.6524(3.6051) | Xent 0.3675(0.3560) | Loss 9.5719(9.7793) | Error 0.1222(0.1263) Steps 0(0.00) | Grad Norm 11.8043(8.6756) | Total Time 0.00(0.00)\n",
      "Iter 9460 | Time 21.8054(21.6424) | Bit/dim 3.5952(3.6039) | Xent 0.3571(0.3574) | Loss 9.3084(9.6973) | Error 0.1367(0.1272) Steps 0(0.00) | Grad Norm 8.8123(8.8506) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0172 | Time 104.8304, Epoch Time 1313.3318(1224.7067), Bit/dim 3.6173(best: 3.5990), Xent 0.7708, Loss 4.0027, Error 0.2271(best: 0.2129)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9470 | Time 20.9817(21.7896) | Bit/dim 3.5957(3.6047) | Xent 0.3065(0.3543) | Loss 9.3578(10.3722) | Error 0.1044(0.1263) Steps 0(0.00) | Grad Norm 8.5350(8.8695) | Total Time 0.00(0.00)\n",
      "Iter 9480 | Time 21.5741(21.7417) | Bit/dim 3.6227(3.6061) | Xent 0.3498(0.3488) | Loss 9.4487(10.1307) | Error 0.1444(0.1252) Steps 0(0.00) | Grad Norm 6.4905(8.3316) | Total Time 0.00(0.00)\n",
      "Iter 9490 | Time 20.5785(21.7300) | Bit/dim 3.6088(3.6031) | Xent 0.3100(0.3422) | Loss 9.2730(9.9362) | Error 0.1133(0.1226) Steps 0(0.00) | Grad Norm 5.4770(7.7538) | Total Time 0.00(0.00)\n",
      "Iter 9500 | Time 24.0590(21.6858) | Bit/dim 3.6115(3.6035) | Xent 0.2839(0.3407) | Loss 9.3698(9.8048) | Error 0.1011(0.1214) Steps 0(0.00) | Grad Norm 7.1085(7.6378) | Total Time 0.00(0.00)\n",
      "Iter 9510 | Time 21.1606(21.7240) | Bit/dim 3.6122(3.6016) | Xent 0.3951(0.3398) | Loss 9.5659(9.6923) | Error 0.1367(0.1213) Steps 0(0.00) | Grad Norm 10.3890(7.3512) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0173 | Time 104.7101, Epoch Time 1322.4071(1227.6377), Bit/dim 3.6061(best: 3.5990), Xent 0.7601, Loss 3.9862, Error 0.2269(best: 0.2129)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9520 | Time 20.3097(21.6756) | Bit/dim 3.6283(3.6032) | Xent 0.2993(0.3425) | Loss 9.5463(10.4986) | Error 0.1011(0.1230) Steps 0(0.00) | Grad Norm 7.2244(7.7536) | Total Time 0.00(0.00)\n",
      "Iter 9530 | Time 21.3985(21.9436) | Bit/dim 3.5621(3.6023) | Xent 0.3127(0.3465) | Loss 9.2325(10.2297) | Error 0.1122(0.1245) Steps 0(0.00) | Grad Norm 5.5441(7.6358) | Total Time 0.00(0.00)\n",
      "Iter 9540 | Time 20.0602(21.9736) | Bit/dim 3.5793(3.6012) | Xent 0.3946(0.3479) | Loss 9.0352(10.0164) | Error 0.1378(0.1244) Steps 0(0.00) | Grad Norm 12.7489(8.0012) | Total Time 0.00(0.00)\n",
      "Iter 9550 | Time 21.9103(21.8997) | Bit/dim 3.5809(3.6043) | Xent 0.3216(0.3481) | Loss 9.5084(9.8769) | Error 0.1044(0.1237) Steps 0(0.00) | Grad Norm 4.8764(7.8115) | Total Time 0.00(0.00)\n",
      "Iter 9560 | Time 20.2151(21.8569) | Bit/dim 3.5781(3.6027) | Xent 0.3239(0.3477) | Loss 9.2369(9.7482) | Error 0.1189(0.1232) Steps 0(0.00) | Grad Norm 5.9679(7.8672) | Total Time 0.00(0.00)\n",
      "Iter 9570 | Time 23.9566(21.8279) | Bit/dim 3.6022(3.6028) | Xent 0.3672(0.3385) | Loss 9.3954(9.6737) | Error 0.1389(0.1204) Steps 0(0.00) | Grad Norm 10.5459(7.2158) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0174 | Time 105.4169, Epoch Time 1327.9890(1230.6482), Bit/dim 3.6007(best: 3.5990), Xent 0.7272, Loss 3.9643, Error 0.2209(best: 0.2129)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9580 | Time 29.0666(22.1301) | Bit/dim 3.6171(3.6028) | Xent 0.2734(0.3340) | Loss 9.5875(10.3665) | Error 0.0867(0.1183) Steps 0(0.00) | Grad Norm 6.1658(7.1717) | Total Time 0.00(0.00)\n",
      "Iter 9590 | Time 23.6779(22.1728) | Bit/dim 3.6176(3.6039) | Xent 0.3612(0.3350) | Loss 9.5192(10.1279) | Error 0.1333(0.1194) Steps 0(0.00) | Grad Norm 9.9849(7.6149) | Total Time 0.00(0.00)\n",
      "Iter 9600 | Time 22.8962(22.0746) | Bit/dim 3.5918(3.6045) | Xent 0.2963(0.3382) | Loss 9.3049(9.9607) | Error 0.1011(0.1205) Steps 0(0.00) | Grad Norm 8.3963(7.9114) | Total Time 0.00(0.00)\n",
      "Iter 9610 | Time 22.2960(21.8073) | Bit/dim 3.5931(3.6035) | Xent 0.3281(0.3371) | Loss 9.5652(9.8254) | Error 0.1156(0.1193) Steps 0(0.00) | Grad Norm 8.0489(7.5181) | Total Time 0.00(0.00)\n",
      "Iter 9620 | Time 22.3557(21.8127) | Bit/dim 3.6148(3.5980) | Xent 0.3746(0.3323) | Loss 9.4832(9.7175) | Error 0.1211(0.1178) Steps 0(0.00) | Grad Norm 5.6662(7.0643) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0175 | Time 103.3665, Epoch Time 1327.9030(1233.5659), Bit/dim 3.6067(best: 3.5990), Xent 0.7425, Loss 3.9779, Error 0.2189(best: 0.2129)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9630 | Time 24.4350(21.8061) | Bit/dim 3.6058(3.6021) | Xent 0.3943(0.3359) | Loss 9.5528(10.5376) | Error 0.1444(0.1203) Steps 0(0.00) | Grad Norm 11.2694(7.5950) | Total Time 0.00(0.00)\n",
      "Iter 9640 | Time 20.4693(21.6987) | Bit/dim 3.5972(3.5995) | Xent 0.3549(0.3337) | Loss 9.3392(10.2325) | Error 0.1189(0.1199) Steps 0(0.00) | Grad Norm 10.8451(7.6949) | Total Time 0.00(0.00)\n",
      "Iter 9650 | Time 23.6722(21.9305) | Bit/dim 3.6312(3.6024) | Xent 0.3223(0.3309) | Loss 9.6274(10.0271) | Error 0.1089(0.1182) Steps 0(0.00) | Grad Norm 5.8960(7.6144) | Total Time 0.00(0.00)\n",
      "Iter 9660 | Time 21.2844(21.8438) | Bit/dim 3.5905(3.6014) | Xent 0.3834(0.3434) | Loss 9.3858(9.8805) | Error 0.1389(0.1220) Steps 0(0.00) | Grad Norm 8.8235(8.0941) | Total Time 0.00(0.00)\n",
      "Iter 9670 | Time 19.7890(21.6613) | Bit/dim 3.6109(3.6001) | Xent 0.3834(0.3453) | Loss 9.4490(9.7574) | Error 0.1378(0.1230) Steps 0(0.00) | Grad Norm 10.9037(8.3192) | Total Time 0.00(0.00)\n",
      "Iter 9680 | Time 21.7056(21.6445) | Bit/dim 3.6086(3.6047) | Xent 0.3360(0.3463) | Loss 9.5161(9.6640) | Error 0.1200(0.1234) Steps 0(0.00) | Grad Norm 8.3729(8.0108) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0176 | Time 103.2326, Epoch Time 1314.3553(1235.9896), Bit/dim 3.6004(best: 3.5990), Xent 0.7230, Loss 3.9619, Error 0.2222(best: 0.2129)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9690 | Time 22.5856(21.6626) | Bit/dim 3.6151(3.6040) | Xent 0.3429(0.3384) | Loss 9.7671(10.3434) | Error 0.1278(0.1205) Steps 0(0.00) | Grad Norm 5.1489(7.5590) | Total Time 0.00(0.00)\n",
      "Iter 9700 | Time 20.8113(21.6265) | Bit/dim 3.6002(3.6047) | Xent 0.2973(0.3289) | Loss 9.4666(10.1122) | Error 0.1022(0.1173) Steps 0(0.00) | Grad Norm 10.4826(7.4827) | Total Time 0.00(0.00)\n",
      "Iter 9710 | Time 21.1562(21.5638) | Bit/dim 3.6167(3.6047) | Xent 0.3073(0.3298) | Loss 9.2715(9.9264) | Error 0.1133(0.1180) Steps 0(0.00) | Grad Norm 7.7095(7.6464) | Total Time 0.00(0.00)\n",
      "Iter 9720 | Time 21.1837(21.6364) | Bit/dim 3.6064(3.6021) | Xent 0.3426(0.3301) | Loss 9.5170(9.7968) | Error 0.1233(0.1181) Steps 0(0.00) | Grad Norm 6.4323(7.4637) | Total Time 0.00(0.00)\n",
      "Iter 9730 | Time 22.5121(21.6694) | Bit/dim 3.6611(3.6013) | Xent 0.3409(0.3260) | Loss 9.3325(9.6909) | Error 0.1289(0.1168) Steps 0(0.00) | Grad Norm 7.1466(7.2385) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0177 | Time 105.7928, Epoch Time 1318.7132(1238.4713), Bit/dim 3.6047(best: 3.5990), Xent 0.7186, Loss 3.9641, Error 0.2136(best: 0.2129)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9740 | Time 20.4667(21.6162) | Bit/dim 3.6102(3.6006) | Xent 0.2774(0.3198) | Loss 9.4055(10.4688) | Error 0.1056(0.1147) Steps 0(0.00) | Grad Norm 5.6800(7.0462) | Total Time 0.00(0.00)\n",
      "Iter 9750 | Time 20.5716(21.6459) | Bit/dim 3.6134(3.6002) | Xent 0.2587(0.3156) | Loss 9.4031(10.1862) | Error 0.0989(0.1127) Steps 0(0.00) | Grad Norm 7.3077(7.0637) | Total Time 0.00(0.00)\n",
      "Iter 9760 | Time 21.7353(21.5907) | Bit/dim 3.6047(3.6002) | Xent 0.3498(0.3195) | Loss 9.4357(10.0038) | Error 0.1244(0.1131) Steps 0(0.00) | Grad Norm 7.2648(7.3578) | Total Time 0.00(0.00)\n",
      "Iter 9770 | Time 23.8480(21.7572) | Bit/dim 3.5800(3.6000) | Xent 0.3409(0.3201) | Loss 9.4571(9.8510) | Error 0.1156(0.1131) Steps 0(0.00) | Grad Norm 6.4211(7.6447) | Total Time 0.00(0.00)\n",
      "Iter 9780 | Time 21.9800(21.9114) | Bit/dim 3.6178(3.6047) | Xent 0.2806(0.3215) | Loss 9.3469(9.7549) | Error 0.0956(0.1136) Steps 0(0.00) | Grad Norm 5.4598(7.6607) | Total Time 0.00(0.00)\n",
      "Iter 9790 | Time 22.5237(21.9574) | Bit/dim 3.5529(3.6004) | Xent 0.3535(0.3230) | Loss 9.4866(9.6794) | Error 0.1311(0.1151) Steps 0(0.00) | Grad Norm 8.6912(7.5002) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0178 | Time 104.6112, Epoch Time 1323.2381(1241.0143), Bit/dim 3.6083(best: 3.5990), Xent 0.7234, Loss 3.9700, Error 0.2197(best: 0.2129)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9800 | Time 21.6854(21.7631) | Bit/dim 3.6084(3.5998) | Xent 0.2855(0.3186) | Loss 9.3725(10.3608) | Error 0.1011(0.1133) Steps 0(0.00) | Grad Norm 5.8707(7.5659) | Total Time 0.00(0.00)\n",
      "Iter 9810 | Time 23.5549(21.8048) | Bit/dim 3.6045(3.6026) | Xent 0.3438(0.3164) | Loss 9.4120(10.0971) | Error 0.1178(0.1118) Steps 0(0.00) | Grad Norm 5.8377(7.4118) | Total Time 0.00(0.00)\n",
      "Iter 9820 | Time 22.2669(21.8881) | Bit/dim 3.5768(3.6005) | Xent 0.3583(0.3171) | Loss 9.5549(9.9321) | Error 0.1289(0.1131) Steps 0(0.00) | Grad Norm 8.0495(7.3153) | Total Time 0.00(0.00)\n",
      "Iter 9830 | Time 21.1010(21.7523) | Bit/dim 3.5891(3.6009) | Xent 0.3262(0.3183) | Loss 9.3841(9.8037) | Error 0.1144(0.1140) Steps 0(0.00) | Grad Norm 7.7356(7.2104) | Total Time 0.00(0.00)\n",
      "Iter 9840 | Time 21.5197(21.8216) | Bit/dim 3.6368(3.6016) | Xent 0.3114(0.3183) | Loss 9.3532(9.7037) | Error 0.1133(0.1136) Steps 0(0.00) | Grad Norm 7.0649(6.9774) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0179 | Time 104.6288, Epoch Time 1313.3100(1243.1832), Bit/dim 3.6048(best: 3.5990), Xent 0.7130, Loss 3.9614, Error 0.2147(best: 0.2129)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9860 | Time 19.8786(21.5956) | Bit/dim 3.6117(3.6011) | Xent 0.3056(0.3110) | Loss 9.4115(10.2274) | Error 0.1022(0.1112) Steps 0(0.00) | Grad Norm 10.2813(6.6336) | Total Time 0.00(0.00)\n",
      "Iter 9870 | Time 21.6160(21.5057) | Bit/dim 3.6035(3.6038) | Xent 0.3463(0.3096) | Loss 9.4470(10.0194) | Error 0.1244(0.1105) Steps 0(0.00) | Grad Norm 9.5319(6.7140) | Total Time 0.00(0.00)\n",
      "Iter 9880 | Time 21.0377(21.5450) | Bit/dim 3.5803(3.6028) | Xent 0.3112(0.3169) | Loss 9.3199(9.8617) | Error 0.1000(0.1128) Steps 0(0.00) | Grad Norm 9.5572(7.0156) | Total Time 0.00(0.00)\n",
      "Iter 9890 | Time 21.7342(21.6558) | Bit/dim 3.5689(3.5998) | Xent 0.3585(0.3254) | Loss 9.5379(9.7756) | Error 0.1144(0.1156) Steps 0(0.00) | Grad Norm 8.2386(7.3713) | Total Time 0.00(0.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_rl.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_3_run1 --seed 1 --lr 0.001 --conditional True --controlled_tol False --train_mode semisup --log_freq 10 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5 --scale_fac 1.0 --scale_std 3.0\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
