{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "if args.controlled_tol:\n",
      "    import lib.odenvp_conditional_tol as odenvp\n",
      "else:\n",
      "    import lib.odenvp_conditional as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz = modules.GaussianDiag.logp(mean, logs, z).view(-1,1)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(z)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol},)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z = modules.GaussianDiag.sample(mean, logs)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            loss.backward()\n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=0.0001, autoencode=False, batch_norm=False, batch_size=8000, batch_size_schedule='', begin_epoch=1, conditional=True, controlled_tol=True, conv=True, data='mnist', dims='64,64,64', divergence_fn='approximate', dl2int=None, imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=1, lr=0.01, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume=None, rtol=0.0001, save='../experiments_published/cnf_conditional_8K_run1', seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=5000, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=113.0, weight_decay=0.0, weight_y=0.5)\n",
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=1568, bias=True)\n",
      "  (project_class): LinearZeros(in_features=784, out_features=10, bias=True)\n",
      ")\n",
      "Number of trainable parameters: 828890\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc.py:286: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "Iter 0001 | Time 59.8100(59.8100) | Bit/dim 19.2775(19.2775) | Xent 2.3026(2.3026) | Loss 20.4288(20.4288) | Error 0.9005(0.9005) Steps 296(296.00) | Grad Norm 144.3572(144.3572) | Total Time 10.00(10.00)\n",
      "Iter 0002 | Time 21.0110(58.6461) | Bit/dim 15.1293(19.1530) | Xent 2.2244(2.3002) | Loss 16.2415(20.3031) | Error 0.7071(0.8947) Steps 302(296.18) | Grad Norm 92.0987(142.7894) | Total Time 10.00(10.00)\n",
      "Iter 0003 | Time 25.3395(57.6469) | Bit/dim 12.4742(18.9526) | Xent 2.0714(2.2934) | Loss 13.5099(20.0993) | Error 0.4574(0.8816) Steps 386(298.87) | Grad Norm 38.0012(139.6458) | Total Time 10.00(10.00)\n",
      "Iter 0004 | Time 26.4499(56.7110) | Bit/dim 12.0523(18.7456) | Xent 1.9372(2.2827) | Loss 13.0209(19.8870) | Error 0.4157(0.8676) Steps 404(302.03) | Grad Norm 52.9839(137.0459) | Total Time 10.00(10.00)\n",
      "Iter 0005 | Time 27.7285(55.8415) | Bit/dim 11.4810(18.5277) | Xent 1.8038(2.2683) | Loss 12.3829(19.6619) | Error 0.3565(0.8523) Steps 410(305.27) | Grad Norm 68.6752(134.9948) | Total Time 10.00(10.00)\n",
      "Iter 0006 | Time 26.4893(54.9609) | Bit/dim 10.0534(18.2735) | Xent 1.6591(2.2500) | Loss 10.8829(19.3985) | Error 0.2879(0.8353) Steps 410(308.41) | Grad Norm 56.8691(132.6510) | Total Time 10.00(10.00)\n",
      "Iter 0007 | Time 24.9544(54.0607) | Bit/dim 8.6384(17.9844) | Xent 1.5239(2.2283) | Loss 9.4004(19.0985) | Error 0.2372(0.8174) Steps 368(310.20) | Grad Norm 35.9847(129.7511) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0001 | Time 23.2229, Epoch Time 248.1193(248.1193), Bit/dim 8.1386(best: inf), Xent 1.4069, Loss 8.8420, Error 0.2522(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0008 | Time 25.5514(53.2055) | Bit/dim 8.0655(17.6869) | Xent 1.4256(2.2042) | Loss 8.7783(18.7889) | Error 0.2570(0.8006) Steps 344(311.21) | Grad Norm 31.6719(126.8087) | Total Time 10.00(10.00)\n",
      "Iter 0009 | Time 23.1180(52.3028) | Bit/dim 7.5081(17.3815) | Xent 1.3569(2.1788) | Loss 8.1866(18.4709) | Error 0.2847(0.7851) Steps 338(312.01) | Grad Norm 35.1942(124.0602) | Total Time 10.00(10.00)\n",
      "Iter 0010 | Time 23.3916(51.4355) | Bit/dim 6.3757(17.0513) | Xent 1.2927(2.1522) | Loss 7.0220(18.1274) | Error 0.2562(0.7692) Steps 350(313.15) | Grad Norm 29.9125(121.2358) | Total Time 10.00(10.00)\n",
      "Iter 0011 | Time 24.6649(50.6324) | Bit/dim 5.0641(16.6917) | Xent 1.2790(2.1260) | Loss 5.7036(17.7547) | Error 0.2484(0.7536) Steps 368(314.80) | Grad Norm 19.2690(118.1768) | Total Time 10.00(10.00)\n",
      "Iter 0012 | Time 25.2253(49.8702) | Bit/dim 4.0893(16.3136) | Xent 1.3205(2.1018) | Loss 4.7495(17.3645) | Error 0.2488(0.7385) Steps 380(316.76) | Grad Norm 11.0774(114.9638) | Total Time 10.00(10.00)\n",
      "Iter 0013 | Time 25.5010(49.1391) | Bit/dim 3.5828(15.9317) | Xent 1.3927(2.0805) | Loss 4.2791(16.9720) | Error 0.2556(0.7240) Steps 386(318.83) | Grad Norm 9.6069(111.8031) | Total Time 10.00(10.00)\n",
      "Iter 0014 | Time 26.5748(48.4622) | Bit/dim 3.3729(15.5549) | Xent 1.5259(2.0639) | Loss 4.1358(16.5869) | Error 0.3343(0.7123) Steps 392(321.03) | Grad Norm 11.2283(108.7859) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0002 | Time 14.8782, Epoch Time 201.4183(246.7182), Bit/dim 3.2334(best: 8.1386), Xent 1.6843, Loss 4.0756, Error 0.4633(best: 0.2522)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0015 | Time 25.0911(47.7610) | Bit/dim 3.2389(15.1855) | Xent 1.6992(2.0530) | Loss 4.0885(16.2119) | Error 0.4769(0.7052) Steps 386(322.98) | Grad Norm 12.6459(105.9017) | Total Time 10.00(10.00)\n",
      "Iter 0016 | Time 26.7474(47.1306) | Bit/dim 3.0853(14.8225) | Xent 1.8700(2.0475) | Loss 4.0203(15.8462) | Error 0.6208(0.7027) Steps 404(325.41) | Grad Norm 12.5963(103.1025) | Total Time 10.00(10.00)\n",
      "Iter 0017 | Time 28.1911(46.5624) | Bit/dim 2.9163(14.4653) | Xent 1.9686(2.0451) | Loss 3.9006(15.4878) | Error 0.6876(0.7022) Steps 404(327.77) | Grad Norm 10.8376(100.3346) | Total Time 10.00(10.00)\n",
      "Iter 0018 | Time 28.9275(46.0334) | Bit/dim 2.8029(14.1154) | Xent 2.0214(2.0444) | Loss 3.8136(15.1376) | Error 0.6527(0.7008) Steps 416(330.41) | Grad Norm 8.5091(97.5798) | Total Time 10.00(10.00)\n",
      "Iter 0019 | Time 28.6860(45.5130) | Bit/dim 2.7826(13.7754) | Xent 2.0690(2.0451) | Loss 3.8171(14.7980) | Error 0.6162(0.6982) Steps 416(332.98) | Grad Norm 7.3815(94.8738) | Total Time 10.00(10.00)\n",
      "Iter 0020 | Time 29.0136(45.0180) | Bit/dim 2.8095(13.4464) | Xent 2.0946(2.0466) | Loss 3.8568(14.4697) | Error 0.6222(0.6959) Steps 422(335.65) | Grad Norm 7.0928(92.2404) | Total Time 10.00(10.00)\n",
      "Iter 0021 | Time 29.8698(44.5635) | Bit/dim 2.8226(13.1277) | Xent 2.1151(2.0487) | Loss 3.8802(14.1521) | Error 0.6705(0.6952) Steps 434(338.60) | Grad Norm 6.4278(89.6660) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0003 | Time 17.3084, Epoch Time 226.3451(246.1071), Bit/dim 2.8073(best: 3.2334), Xent 2.1247, Loss 3.8696, Error 0.6998(best: 0.2522)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0022 | Time 31.9205(44.1843) | Bit/dim 2.8162(12.8184) | Xent 2.1222(2.0509) | Loss 3.8773(13.8438) | Error 0.6989(0.6953) Steps 446(341.82) | Grad Norm 5.3314(87.1360) | Total Time 10.00(10.00)\n",
      "Iter 0023 | Time 31.9139(43.8161) | Bit/dim 2.8043(12.5179) | Xent 2.1366(2.0535) | Loss 3.8726(13.5447) | Error 0.7312(0.6964) Steps 452(345.13) | Grad Norm 5.1549(84.6766) | Total Time 10.00(10.00)\n",
      "Iter 0024 | Time 33.9319(43.5196) | Bit/dim 2.7760(12.2257) | Xent 2.1338(2.0559) | Loss 3.8429(13.2536) | Error 0.7318(0.6974) Steps 434(347.79) | Grad Norm 5.7934(82.3101) | Total Time 10.00(10.00)\n",
      "Iter 0025 | Time 31.4276(43.1569) | Bit/dim 2.7262(11.9407) | Xent 2.0831(2.0567) | Loss 3.7677(12.9690) | Error 0.6694(0.6966) Steps 428(350.20) | Grad Norm 5.8397(80.0160) | Total Time 10.00(10.00)\n",
      "Iter 0026 | Time 31.0570(42.7939) | Bit/dim 2.6362(11.6616) | Xent 2.0177(2.0555) | Loss 3.6451(12.6893) | Error 0.5847(0.6932) Steps 416(352.17) | Grad Norm 5.3263(77.7753) | Total Time 10.00(10.00)\n",
      "Iter 0027 | Time 28.8489(42.3755) | Bit/dim 2.5436(11.3880) | Xent 1.9154(2.0513) | Loss 3.5013(12.4137) | Error 0.5075(0.6877) Steps 392(353.37) | Grad Norm 4.8530(75.5876) | Total Time 10.00(10.00)\n",
      "Iter 0028 | Time 29.6424(41.9935) | Bit/dim 2.4500(11.1199) | Xent 1.7863(2.0434) | Loss 3.3431(12.1416) | Error 0.4374(0.6802) Steps 392(354.53) | Grad Norm 4.6587(73.4597) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0004 | Time 14.3793, Epoch Time 245.7699(246.0969), Bit/dim 2.3871(best: 2.8073), Xent 1.5843, Loss 3.1792, Error 0.3630(best: 0.2522)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0029 | Time 27.6181(41.5623) | Bit/dim 2.3978(10.8582) | Xent 1.6006(2.0301) | Loss 3.1981(11.8733) | Error 0.3772(0.6711) Steps 380(355.29) | Grad Norm 4.6913(71.3967) | Total Time 10.00(10.00)\n",
      "Iter 0030 | Time 27.8741(41.1516) | Bit/dim 2.3811(10.6039) | Xent 1.3580(2.0099) | Loss 3.0601(11.6089) | Error 0.3205(0.6606) Steps 386(356.21) | Grad Norm 4.6574(69.3945) | Total Time 10.00(10.00)\n",
      "Iter 0031 | Time 27.6828(40.7475) | Bit/dim 2.4101(10.3581) | Xent 1.1072(1.9828) | Loss 2.9637(11.3495) | Error 0.2734(0.6489) Steps 380(356.93) | Grad Norm 4.3790(67.4440) | Total Time 10.00(10.00)\n",
      "Iter 0032 | Time 26.2414(40.3124) | Bit/dim 2.5113(10.1227) | Xent 0.8775(1.9497) | Loss 2.9500(11.0975) | Error 0.2319(0.6364) Steps 368(357.26) | Grad Norm 4.0113(65.5411) | Total Time 10.00(10.00)\n",
      "Iter 0033 | Time 25.1718(39.8581) | Bit/dim 2.6192(9.8976) | Xent 0.7416(1.9134) | Loss 2.9900(10.8543) | Error 0.2110(0.6237) Steps 356(357.22) | Grad Norm 6.5006(63.7698) | Total Time 10.00(10.00)\n",
      "Iter 0034 | Time 23.3562(39.3631) | Bit/dim 2.6472(9.6801) | Xent 0.7191(1.8776) | Loss 3.0068(10.6189) | Error 0.2181(0.6115) Steps 344(356.83) | Grad Norm 9.0548(62.1284) | Total Time 10.00(10.00)\n",
      "Iter 0035 | Time 23.6282(38.8910) | Bit/dim 2.5489(9.4661) | Xent 0.7229(1.8430) | Loss 2.9104(10.3876) | Error 0.2120(0.5995) Steps 350(356.62) | Grad Norm 7.6156(60.4930) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0005 | Time 14.4373, Epoch Time 208.3635(244.9649), Bit/dim 2.4001(best: 2.3871), Xent 0.7297, Loss 2.7650, Error 0.1983(best: 0.2522)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0036 | Time 25.0535(38.4759) | Bit/dim 2.4021(9.2542) | Xent 0.7432(1.8100) | Loss 2.7737(10.1592) | Error 0.1979(0.5875) Steps 356(356.60) | Grad Norm 3.6453(58.7876) | Total Time 10.00(10.00)\n",
      "Iter 0037 | Time 26.7413(38.1239) | Bit/dim 2.2946(9.0454) | Xent 0.8460(1.7810) | Loss 2.7177(9.9360) | Error 0.2089(0.5761) Steps 386(357.48) | Grad Norm 1.4506(57.0675) | Total Time 10.00(10.00)\n",
      "Iter 0038 | Time 26.7492(37.7826) | Bit/dim 2.2357(8.8411) | Xent 0.9562(1.7563) | Loss 2.7138(9.7193) | Error 0.2164(0.5653) Steps 386(358.34) | Grad Norm 3.1087(55.4487) | Total Time 10.00(10.00)\n",
      "Iter 0039 | Time 25.9783(37.4285) | Bit/dim 2.2071(8.6421) | Xent 1.0274(1.7344) | Loss 2.7208(9.5093) | Error 0.2198(0.5549) Steps 380(358.99) | Grad Norm 4.2764(53.9135) | Total Time 10.00(10.00)\n",
      "Iter 0040 | Time 25.2993(37.0646) | Bit/dim 2.1964(8.4488) | Xent 1.0436(1.7137) | Loss 2.7182(9.3056) | Error 0.2202(0.5449) Steps 374(359.44) | Grad Norm 4.9255(52.4439) | Total Time 10.00(10.00)\n",
      "Iter 0041 | Time 26.4331(36.7457) | Bit/dim 2.1820(8.2607) | Xent 0.9857(1.6919) | Loss 2.6748(9.1067) | Error 0.2114(0.5349) Steps 374(359.88) | Grad Norm 4.7620(51.0134) | Total Time 10.00(10.00)\n",
      "Iter 0042 | Time 25.6107(36.4116) | Bit/dim 2.1639(8.0778) | Xent 0.8524(1.6667) | Loss 2.5901(8.9112) | Error 0.1995(0.5248) Steps 374(360.30) | Grad Norm 3.2480(49.5805) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0006 | Time 14.4969, Epoch Time 208.8846(243.8825), Bit/dim 2.1815(best: 2.3871), Xent 0.7090, Loss 2.5360, Error 0.1833(best: 0.1983)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0043 | Time 24.3834(36.0508) | Bit/dim 2.1880(7.9011) | Xent 0.7194(1.6383) | Loss 2.5477(8.7203) | Error 0.1871(0.5147) Steps 368(360.53) | Grad Norm 3.1045(48.1862) | Total Time 10.00(10.00)\n",
      "Iter 0044 | Time 24.6135(35.7077) | Bit/dim 2.2082(7.7304) | Xent 0.6364(1.6082) | Loss 2.5264(8.5345) | Error 0.1777(0.5046) Steps 368(360.76) | Grad Norm 3.8672(46.8566) | Total Time 10.00(10.00)\n",
      "Iter 0045 | Time 24.0892(35.3591) | Bit/dim 2.2257(7.5652) | Xent 0.5849(1.5775) | Loss 2.5181(8.3540) | Error 0.1735(0.4947) Steps 356(360.61) | Grad Norm 3.3396(45.5511) | Total Time 10.00(10.00)\n",
      "Iter 0046 | Time 23.3813(34.9998) | Bit/dim 2.2151(7.4047) | Xent 0.5918(1.5479) | Loss 2.5110(8.1787) | Error 0.1805(0.4852) Steps 350(360.29) | Grad Norm 4.1271(44.3084) | Total Time 10.00(10.00)\n",
      "Iter 0047 | Time 23.6843(34.6603) | Bit/dim 2.1693(7.2477) | Xent 0.5785(1.5189) | Loss 2.4586(8.0071) | Error 0.1671(0.4757) Steps 350(359.99) | Grad Norm 3.4678(43.0832) | Total Time 10.00(10.00)\n",
      "Iter 0048 | Time 23.7322(34.3325) | Bit/dim 2.1222(7.0939) | Xent 0.5940(1.4911) | Loss 2.4192(7.8394) | Error 0.1629(0.4663) Steps 350(359.69) | Grad Norm 1.3611(41.8315) | Total Time 10.00(10.00)\n",
      "Iter 0049 | Time 23.7326(34.0145) | Bit/dim 2.0812(6.9435) | Xent 0.6519(1.4659) | Loss 2.4072(7.6765) | Error 0.1833(0.4578) Steps 350(359.40) | Grad Norm 2.1684(40.6416) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0007 | Time 14.0235, Epoch Time 194.1336(242.3901), Bit/dim 2.0697(best: 2.1815), Xent 0.6311, Loss 2.3852, Error 0.1673(best: 0.1833)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0050 | Time 23.0255(33.6848) | Bit/dim 2.0739(6.7974) | Xent 0.6235(1.4407) | Loss 2.3857(7.5178) | Error 0.1669(0.4491) Steps 350(359.11) | Grad Norm 2.3623(39.4932) | Total Time 10.00(10.00)\n",
      "Iter 0051 | Time 23.7573(33.3870) | Bit/dim 2.0653(6.6555) | Xent 0.6265(1.4162) | Loss 2.3786(7.3636) | Error 0.1675(0.4406) Steps 356(359.02) | Grad Norm 2.0950(38.3713) | Total Time 10.00(10.00)\n",
      "Iter 0052 | Time 23.9115(33.1027) | Bit/dim 2.0572(6.5175) | Xent 0.5773(1.3911) | Loss 2.3458(7.2130) | Error 0.1596(0.4322) Steps 362(359.11) | Grad Norm 1.9618(37.2790) | Total Time 10.00(10.00)\n",
      "Iter 0053 | Time 25.0740(32.8619) | Bit/dim 2.0546(6.3836) | Xent 0.5573(1.3661) | Loss 2.3333(7.0667) | Error 0.1595(0.4240) Steps 368(359.38) | Grad Norm 1.7339(36.2127) | Total Time 10.00(10.00)\n",
      "Iter 0054 | Time 24.5436(32.6123) | Bit/dim 2.0455(6.2535) | Xent 0.5109(1.3404) | Loss 2.3009(6.9237) | Error 0.1475(0.4157) Steps 368(359.63) | Grad Norm 1.5757(35.1735) | Total Time 10.00(10.00)\n",
      "Iter 0055 | Time 25.1650(32.3889) | Bit/dim 2.0310(6.1268) | Xent 0.5371(1.3163) | Loss 2.2996(6.7850) | Error 0.1552(0.4079) Steps 362(359.71) | Grad Norm 1.7736(34.1715) | Total Time 10.00(10.00)\n",
      "Iter 0056 | Time 25.1457(32.1716) | Bit/dim 2.0176(6.0035) | Xent 0.5326(1.2928) | Loss 2.2839(6.6499) | Error 0.1536(0.4003) Steps 362(359.77) | Grad Norm 2.1297(33.2103) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0008 | Time 14.2444, Epoch Time 197.3384(241.0385), Bit/dim 1.9995(best: 2.0697), Xent 0.5035, Loss 2.2513, Error 0.1405(best: 0.1673)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0057 | Time 25.8533(31.9820) | Bit/dim 1.9998(5.8834) | Xent 0.5062(1.2692) | Loss 2.2529(6.5180) | Error 0.1449(0.3926) Steps 362(359.84) | Grad Norm 1.2502(32.2515) | Total Time 10.00(10.00)\n",
      "Iter 0058 | Time 25.0410(31.7738) | Bit/dim 1.9933(5.7667) | Xent 0.5008(1.2461) | Loss 2.2437(6.3898) | Error 0.1468(0.3853) Steps 362(359.91) | Grad Norm 2.3913(31.3557) | Total Time 10.00(10.00)\n",
      "Iter 0059 | Time 24.9786(31.5700) | Bit/dim 1.9731(5.6529) | Xent 0.5103(1.2241) | Loss 2.2283(6.2649) | Error 0.1401(0.3779) Steps 362(359.97) | Grad Norm 2.0533(30.4766) | Total Time 10.00(10.00)\n",
      "Iter 0060 | Time 25.8000(31.3969) | Bit/dim 1.9608(5.5421) | Xent 0.4949(1.2022) | Loss 2.2082(6.1432) | Error 0.1426(0.3708) Steps 374(360.39) | Grad Norm 1.8573(29.6180) | Total Time 10.00(10.00)\n",
      "Iter 0061 | Time 26.6087(31.2532) | Bit/dim 1.9565(5.4346) | Xent 0.5040(1.1812) | Loss 2.2085(6.0252) | Error 0.1502(0.3642) Steps 374(360.80) | Grad Norm 1.3034(28.7686) | Total Time 10.00(10.00)\n",
      "Iter 0062 | Time 26.3006(31.1046) | Bit/dim 1.9544(5.3302) | Xent 0.4763(1.1601) | Loss 2.1926(5.9102) | Error 0.1388(0.3575) Steps 380(361.37) | Grad Norm 2.3475(27.9760) | Total Time 10.00(10.00)\n",
      "Iter 0063 | Time 27.3482(30.9919) | Bit/dim 1.9527(5.2288) | Xent 0.4462(1.1387) | Loss 2.1758(5.7982) | Error 0.1365(0.3508) Steps 380(361.93) | Grad Norm 2.0051(27.1968) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0009 | Time 14.4509, Epoch Time 208.8487(240.0728), Bit/dim 1.9308(best: 1.9995), Xent 0.4372, Loss 2.1494, Error 0.1314(best: 0.1405)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0064 | Time 26.3851(30.8537) | Bit/dim 1.9362(5.1301) | Xent 0.4735(1.1187) | Loss 2.1730(5.6894) | Error 0.1405(0.3445) Steps 374(362.29) | Grad Norm 1.8421(26.4362) | Total Time 10.00(10.00)\n",
      "Iter 0065 | Time 25.9157(30.7056) | Bit/dim 1.9250(5.0339) | Xent 0.4520(1.0987) | Loss 2.1510(5.5833) | Error 0.1352(0.3382) Steps 374(362.65) | Grad Norm 1.8770(25.6994) | Total Time 10.00(10.00)\n",
      "Iter 0066 | Time 26.1424(30.5687) | Bit/dim 1.9066(4.9401) | Xent 0.4805(1.0802) | Loss 2.1468(5.4802) | Error 0.1348(0.3321) Steps 374(362.99) | Grad Norm 3.1190(25.0220) | Total Time 10.00(10.00)\n",
      "Iter 0067 | Time 26.0112(30.4320) | Bit/dim 1.9404(4.8501) | Xent 0.4497(1.0613) | Loss 2.1653(5.3807) | Error 0.1366(0.3263) Steps 374(363.32) | Grad Norm 10.0722(24.5735) | Total Time 10.00(10.00)\n",
      "Iter 0068 | Time 26.8797(30.3254) | Bit/dim 2.0704(4.7667) | Xent 0.8141(1.0538) | Loss 2.4774(5.2936) | Error 0.2411(0.3237) Steps 368(363.46) | Grad Norm 22.0799(24.4987) | Total Time 10.00(10.00)\n",
      "Iter 0069 | Time 24.4186(30.1482) | Bit/dim 2.2277(4.6905) | Xent 0.5467(1.0386) | Loss 2.5010(5.2099) | Error 0.1466(0.3184) Steps 356(363.23) | Grad Norm 31.3348(24.7038) | Total Time 10.00(10.00)\n",
      "Iter 0070 | Time 25.6760(30.0140) | Bit/dim 1.9723(4.6090) | Xent 1.1928(1.0433) | Loss 2.5687(5.1306) | Error 0.3830(0.3203) Steps 368(363.38) | Grad Norm 29.6769(24.8530) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0010 | Time 14.2139, Epoch Time 208.5518(239.1272), Bit/dim 2.0755(best: 1.9308), Xent 1.5933, Loss 2.8722, Error 0.4407(best: 0.1314)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0071 | Time 25.4780(29.8780) | Bit/dim 2.0841(4.5332) | Xent 1.6096(1.0602) | Loss 2.8889(5.0634) | Error 0.4453(0.3241) Steps 368(363.52) | Grad Norm 19.9844(24.7069) | Total Time 10.00(10.00)\n",
      "Iter 0072 | Time 23.2609(29.6794) | Bit/dim 2.2960(4.4661) | Xent 0.6340(1.0475) | Loss 2.6130(4.9899) | Error 0.1676(0.3194) Steps 350(363.11) | Grad Norm 23.5744(24.6730) | Total Time 10.00(10.00)\n",
      "Iter 0073 | Time 25.7520(29.5616) | Bit/dim 2.0789(4.3945) | Xent 0.7162(1.0375) | Loss 2.4370(4.9133) | Error 0.2261(0.3166) Steps 356(362.90) | Grad Norm 10.1714(24.2379) | Total Time 10.00(10.00)\n",
      "Iter 0074 | Time 25.9392(29.4529) | Bit/dim 2.0854(4.3252) | Xent 0.6140(1.0248) | Loss 2.3924(4.8376) | Error 0.1973(0.3130) Steps 374(363.23) | Grad Norm 6.3991(23.7027) | Total Time 10.00(10.00)\n",
      "Iter 0075 | Time 28.0640(29.4113) | Bit/dim 2.1420(4.2597) | Xent 0.9131(1.0215) | Loss 2.5986(4.7705) | Error 0.2804(0.3120) Steps 386(363.91) | Grad Norm 12.7667(23.3747) | Total Time 10.00(10.00)\n",
      "Iter 0076 | Time 27.1043(29.3421) | Bit/dim 2.0275(4.1928) | Xent 0.5002(1.0058) | Loss 2.2776(4.6957) | Error 0.1519(0.3072) Steps 380(364.40) | Grad Norm 3.0194(22.7640) | Total Time 10.00(10.00)\n",
      "Iter 0077 | Time 25.7497(29.2343) | Bit/dim 2.0542(4.1286) | Xent 0.5758(0.9929) | Loss 2.3421(4.6251) | Error 0.1636(0.3029) Steps 380(364.86) | Grad Norm 5.8346(22.2561) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0011 | Time 15.4064, Epoch Time 209.4767(238.2377), Bit/dim 2.0688(best: 1.9308), Xent 0.4714, Loss 2.3044, Error 0.1406(best: 0.1314)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0078 | Time 27.7191(29.1888) | Bit/dim 2.0728(4.0669) | Xent 0.5000(0.9781) | Loss 2.3228(4.5560) | Error 0.1446(0.2982) Steps 410(366.22) | Grad Norm 5.9740(21.7677) | Total Time 10.00(10.00)\n",
      "Iter 0079 | Time 26.0345(29.0942) | Bit/dim 2.0363(4.0060) | Xent 0.4441(0.9621) | Loss 2.2584(4.4871) | Error 0.1298(0.2931) Steps 404(367.35) | Grad Norm 4.6510(21.2542) | Total Time 10.00(10.00)\n",
      "Iter 0080 | Time 27.3776(29.0427) | Bit/dim 2.0165(3.9463) | Xent 0.4762(0.9475) | Loss 2.2546(4.4201) | Error 0.1409(0.2886) Steps 392(368.09) | Grad Norm 4.5155(20.7520) | Total Time 10.00(10.00)\n",
      "Iter 0081 | Time 27.1423(28.9857) | Bit/dim 2.0056(3.8881) | Xent 0.5991(0.9371) | Loss 2.3052(4.3567) | Error 0.2015(0.2859) Steps 398(368.99) | Grad Norm 8.0607(20.3713) | Total Time 10.00(10.00)\n",
      "Iter 0082 | Time 27.5267(28.9419) | Bit/dim 1.9986(3.8314) | Xent 0.5771(0.9263) | Loss 2.2872(4.2946) | Error 0.1744(0.2826) Steps 398(369.86) | Grad Norm 6.2397(19.9473) | Total Time 10.00(10.00)\n",
      "Iter 0083 | Time 25.5507(28.8402) | Bit/dim 2.0237(3.7772) | Xent 0.4633(0.9124) | Loss 2.2554(4.2334) | Error 0.1394(0.2783) Steps 380(370.16) | Grad Norm 5.4546(19.5125) | Total Time 10.00(10.00)\n",
      "Iter 0084 | Time 25.4749(28.7392) | Bit/dim 2.0069(3.7241) | Xent 0.4867(0.8996) | Loss 2.2502(4.1739) | Error 0.1480(0.2744) Steps 380(370.46) | Grad Norm 6.9099(19.1345) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0012 | Time 14.7370, Epoch Time 214.1789(237.5159), Bit/dim 1.9656(best: 1.9308), Xent 0.4491, Loss 2.1902, Error 0.1366(best: 0.1314)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0085 | Time 24.7875(28.6207) | Bit/dim 1.9654(3.6713) | Xent 0.4503(0.8861) | Loss 2.1906(4.1144) | Error 0.1329(0.2701) Steps 368(370.38) | Grad Norm 3.1078(18.6537) | Total Time 10.00(10.00)\n",
      "Iter 0086 | Time 24.5825(28.4995) | Bit/dim 1.9562(3.6199) | Xent 0.4912(0.8743) | Loss 2.2018(4.0570) | Error 0.1496(0.2665) Steps 362(370.13) | Grad Norm 6.4834(18.2886) | Total Time 10.00(10.00)\n",
      "Iter 0087 | Time 25.5016(28.4096) | Bit/dim 1.9484(3.5697) | Xent 0.4480(0.8615) | Loss 2.1724(4.0005) | Error 0.1398(0.2627) Steps 368(370.07) | Grad Norm 3.5433(17.8462) | Total Time 10.00(10.00)\n",
      "Iter 0088 | Time 24.9776(28.3066) | Bit/dim 1.9585(3.5214) | Xent 0.4380(0.8488) | Loss 2.1775(3.9458) | Error 0.1338(0.2589) Steps 362(369.83) | Grad Norm 2.7763(17.3941) | Total Time 10.00(10.00)\n",
      "Iter 0089 | Time 25.1075(28.2107) | Bit/dim 1.9571(3.4745) | Xent 0.4150(0.8358) | Loss 2.1645(3.8924) | Error 0.1301(0.2550) Steps 362(369.59) | Grad Norm 2.7612(16.9551) | Total Time 10.00(10.00)\n",
      "Iter 0090 | Time 25.3199(28.1239) | Bit/dim 1.9471(3.4286) | Xent 0.4064(0.8229) | Loss 2.1503(3.8401) | Error 0.1261(0.2511) Steps 362(369.36) | Grad Norm 2.2242(16.5132) | Total Time 10.00(10.00)\n",
      "Iter 0091 | Time 24.2245(28.0070) | Bit/dim 1.9320(3.3837) | Xent 0.3857(0.8098) | Loss 2.1249(3.7886) | Error 0.1162(0.2471) Steps 350(368.78) | Grad Norm 2.0558(16.0795) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0013 | Time 13.6972, Epoch Time 201.2138(236.4268), Bit/dim 1.9220(best: 1.9308), Xent 0.4113, Loss 2.1277, Error 0.1245(best: 0.1314)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0092 | Time 26.1535(27.9514) | Bit/dim 1.9289(3.3401) | Xent 0.4075(0.7977) | Loss 2.1327(3.7390) | Error 0.1239(0.2434) Steps 374(368.94) | Grad Norm 3.4914(15.7018) | Total Time 10.00(10.00)\n",
      "Iter 0093 | Time 26.5517(27.9094) | Bit/dim 1.9135(3.2973) | Xent 0.3804(0.7852) | Loss 2.1037(3.6899) | Error 0.1132(0.2395) Steps 380(369.27) | Grad Norm 1.2289(15.2676) | Total Time 10.00(10.00)\n",
      "Iter 0094 | Time 25.6856(27.8427) | Bit/dim 1.9108(3.2557) | Xent 0.3993(0.7736) | Loss 2.1105(3.6425) | Error 0.1192(0.2359) Steps 368(369.23) | Grad Norm 2.8795(14.8960) | Total Time 10.00(10.00)\n",
      "Iter 0095 | Time 25.5217(27.7730) | Bit/dim 1.9056(3.2152) | Xent 0.3929(0.7622) | Loss 2.1021(3.5963) | Error 0.1184(0.2324) Steps 368(369.20) | Grad Norm 2.2265(14.5159) | Total Time 10.00(10.00)\n",
      "Iter 0096 | Time 25.8979(27.7168) | Bit/dim 1.8946(3.1756) | Xent 0.4002(0.7513) | Loss 2.0947(3.5513) | Error 0.1192(0.2290) Steps 368(369.16) | Grad Norm 1.7610(14.1333) | Total Time 10.00(10.00)\n",
      "Iter 0097 | Time 26.7192(27.6868) | Bit/dim 1.8772(3.1366) | Xent 0.4049(0.7409) | Loss 2.0796(3.5071) | Error 0.1190(0.2257) Steps 380(369.49) | Grad Norm 2.8823(13.7957) | Total Time 10.00(10.00)\n",
      "Iter 0098 | Time 24.7609(27.5991) | Bit/dim 1.8732(3.0987) | Xent 0.3876(0.7303) | Loss 2.0670(3.4639) | Error 0.1196(0.2225) Steps 356(369.08) | Grad Norm 0.8909(13.4086) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0014 | Time 14.2147, Epoch Time 208.2467(235.5814), Bit/dim 1.8689(best: 1.9220), Xent 0.3886, Loss 2.0632, Error 0.1121(best: 0.1245)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0099 | Time 24.7279(27.5129) | Bit/dim 1.8803(3.0622) | Xent 0.3868(0.7200) | Loss 2.0737(3.4222) | Error 0.1158(0.2193) Steps 356(368.69) | Grad Norm 2.5027(13.0814) | Total Time 10.00(10.00)\n",
      "Iter 0100 | Time 24.2592(27.4153) | Bit/dim 1.8659(3.0263) | Xent 0.3724(0.7096) | Loss 2.0521(3.3811) | Error 0.1095(0.2160) Steps 350(368.13) | Grad Norm 1.4465(12.7324) | Total Time 10.00(10.00)\n",
      "Iter 0101 | Time 26.9560(27.4015) | Bit/dim 1.8622(2.9914) | Xent 0.3853(0.6999) | Loss 2.0548(3.3413) | Error 0.1188(0.2131) Steps 362(367.94) | Grad Norm 2.0787(12.4128) | Total Time 10.00(10.00)\n",
      "Iter 0102 | Time 25.6111(27.3478) | Bit/dim 1.8546(2.9573) | Xent 0.3915(0.6906) | Loss 2.0503(3.3026) | Error 0.1196(0.2103) Steps 362(367.77) | Grad Norm 1.6696(12.0905) | Total Time 10.00(10.00)\n",
      "Iter 0103 | Time 25.4333(27.2904) | Bit/dim 1.8472(2.9240) | Xent 0.3710(0.6810) | Loss 2.0327(3.2645) | Error 0.1098(0.2072) Steps 356(367.41) | Grad Norm 2.3675(11.7988) | Total Time 10.00(10.00)\n",
      "Iter 0104 | Time 25.1728(27.2269) | Bit/dim 1.8315(2.8912) | Xent 0.3691(0.6717) | Loss 2.0161(3.2270) | Error 0.1125(0.2044) Steps 350(366.89) | Grad Norm 1.6960(11.4957) | Total Time 10.00(10.00)\n",
      "Iter 0105 | Time 25.4008(27.1721) | Bit/dim 1.8304(2.8594) | Xent 0.3657(0.6625) | Loss 2.0133(3.1906) | Error 0.1065(0.2015) Steps 350(366.38) | Grad Norm 2.8003(11.2348) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0015 | Time 13.8021, Epoch Time 203.7644(234.6269), Bit/dim 1.8120(best: 1.8689), Xent 0.3537, Loss 1.9889, Error 0.1042(best: 0.1121)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0106 | Time 24.7795(27.1003) | Bit/dim 1.8189(2.8282) | Xent 0.3685(0.6537) | Loss 2.0032(3.1550) | Error 0.1105(0.1987) Steps 344(365.71) | Grad Norm 0.9968(10.9277) | Total Time 10.00(10.00)\n",
      "Iter 0107 | Time 24.2295(27.0142) | Bit/dim 1.8225(2.7980) | Xent 0.3448(0.6444) | Loss 1.9949(3.1202) | Error 0.1042(0.1959) Steps 344(365.06) | Grad Norm 3.3181(10.6994) | Total Time 10.00(10.00)\n",
      "Iter 0108 | Time 24.6208(26.9424) | Bit/dim 1.8021(2.7681) | Xent 0.3588(0.6359) | Loss 1.9815(3.0860) | Error 0.1046(0.1932) Steps 344(364.43) | Grad Norm 2.7910(10.4621) | Total Time 10.00(10.00)\n",
      "Iter 0109 | Time 23.7227(26.8458) | Bit/dim 1.7950(2.7389) | Xent 0.3514(0.6273) | Loss 1.9707(3.0526) | Error 0.1056(0.1905) Steps 344(363.82) | Grad Norm 1.0623(10.1801) | Total Time 10.00(10.00)\n",
      "Iter 0110 | Time 24.7791(26.7838) | Bit/dim 1.7918(2.7105) | Xent 0.3586(0.6193) | Loss 1.9711(3.0201) | Error 0.1064(0.1880) Steps 344(363.22) | Grad Norm 3.2674(9.9728) | Total Time 10.00(10.00)\n",
      "Iter 0111 | Time 24.0176(26.7008) | Bit/dim 1.7729(2.6824) | Xent 0.3963(0.6126) | Loss 1.9710(2.9887) | Error 0.1184(0.1859) Steps 350(362.82) | Grad Norm 5.0539(9.8252) | Total Time 10.00(10.00)\n",
      "Iter 0112 | Time 24.2541(26.6274) | Bit/dim 1.7715(2.6550) | Xent 0.3418(0.6044) | Loss 1.9424(2.9573) | Error 0.1001(0.1834) Steps 344(362.26) | Grad Norm 6.1454(9.7148) | Total Time 10.00(10.00)\n",
      "validating...\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf.py --data mnist --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 8000 --test_batch_size 5000 --save ../experiments_published/cnf_conditional_8K_run1 --seed 1 --conditional True --controlled_tol True --train_mode semisup --lr 0.01 --warmup_iters 113 --atol 1e-4  --rtol 1e-4 --weight_y 0.5\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
