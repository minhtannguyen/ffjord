{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='4,5,6,7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_rl.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time, count_nfe_gate\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "GATES = [\"cnn1\", \"cnn2\", \"rnn\"]\n",
      "\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--gate', type=str, default='cnn1', choices=GATES)\n",
      "parser.add_argument('--scale', type=float, default=1.0)\n",
      "parser.add_argument('--scale_fac', type=float, default=1.0)\n",
      "parser.add_argument('--scale_std', type=float, default=1.0)\n",
      "parser.add_argument('--eta', default=0.1, type=float,\n",
      "                        help='tuning parameter that allows us to trade-off the competing goals of' \n",
      "                                'minimizing the prediction loss and maximizing the gate rewards ')\n",
      "parser.add_argument('--rl-weight', default=0.01, type=float,\n",
      "                        help='rl weight')\n",
      "\n",
      "parser.add_argument('--gamma', default=0.99, type=float,\n",
      "                        help='discount factor, default: (0.99)')\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "import lib.odenvp_conditional_rl as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol, \"scale\": args.scale, \"scale_fac\": args.scale_fac, \"scale_std\": args.scale_std, \"gate\": args.gate},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            # re-weight the gate rewards\n",
      "            normalized_eta = args.eta / len(logp_actions)\n",
      "            \n",
      "            # collect cumulative future rewards\n",
      "            R = - loss\n",
      "            cum_rewards = []\n",
      "            for r in nfe[::-1]:\n",
      "                R = -normalized_eta * r.view(-1,1) + args.gamma * R\n",
      "                cum_rewards.insert(0,R)\n",
      "            \n",
      "            # apply REINFORCE\n",
      "            rl_loss = 0\n",
      "            for lpa, r in zip(logp_actions, cum_rewards):\n",
      "                rl_loss = rl_loss - lpa.view(-1,1) * args.rl_weight * r\n",
      "                \n",
      "            loss = loss + rl_loss.mean()\n",
      "            \n",
      "            loss.backward()\n",
      "            \n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe_gate(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            for idx in range(len(model.module.transforms)):\n",
      "                for layer in model.module.transforms[idx].chain:\n",
      "                    if hasattr(layer, 'atol'):\n",
      "                        layer.odefunc.after_odeint()\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'train': atol[tol_indx].mean()}, itr)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'train': rtol[tol_indx].mean()}, itr)\n",
      "                    \n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples, atol, rtol, logp_actions, nfe = model(fixed_z, reverse=True)\n",
      "            generated_samples = generated_samples.view(-1, *data_shape)\n",
      "            for tol_indx in range(len(atol)):\n",
      "                writer.add_scalars('atol_gen_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                writer.add_scalars('rtol_gen_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, condition_ratio=0.5, conditional=True, controlled_tol=False, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, eta=0.1, gamma=0.99, gate='cnn1', imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume=None, rl_weight=0.01, rtol=1e-05, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_15_run3', scale=1.0, scale_fac=1.0, scale_std=15.0, seed=3, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1450732\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc_rl.py:291: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Iter 0010 | Time 10.8439(29.5445) | Bit/dim 8.9021(9.0691) | Xent 2.2767(2.2999) | Loss 20.5938(21.1783) | Error 0.7511(0.8713) Steps 430(446.98) | Grad Norm 20.4537(29.4832) | Total Time 0.00(0.00)\n",
      "Iter 0020 | Time 11.1403(24.6697) | Bit/dim 8.5319(8.9467) | Xent 2.2349(2.2872) | Loss 19.9767(20.8861) | Error 0.7667(0.8425) Steps 436(445.88) | Grad Norm 9.1355(24.9168) | Total Time 0.00(0.00)\n",
      "Iter 0030 | Time 11.3351(21.0234) | Bit/dim 8.3453(8.8026) | Xent 2.1771(2.2633) | Loss 19.4227(20.5389) | Error 0.7544(0.8179) Steps 454(445.06) | Grad Norm 7.4830(20.5828) | Total Time 0.00(0.00)\n",
      "Iter 0040 | Time 10.7807(18.3438) | Bit/dim 8.1211(8.6444) | Xent 2.1230(2.2349) | Loss 19.1328(20.1905) | Error 0.7300(0.7980) Steps 418(443.93) | Grad Norm 6.4886(16.8103) | Total Time 0.00(0.00)\n",
      "Iter 0050 | Time 11.1732(16.3677) | Bit/dim 7.8695(8.4705) | Xent 2.0932(2.2046) | Loss 18.6246(19.8178) | Error 0.7200(0.7791) Steps 478(445.24) | Grad Norm 5.5629(13.8812) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0001 | Time 66.3541, Epoch Time 704.9515(704.9515), Bit/dim 7.7088(best: inf), Xent 2.0815, Loss 8.7496, Error 0.6980(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0060 | Time 11.7288(14.9535) | Bit/dim 7.5658(8.2749) | Xent 2.0670(2.1738) | Loss 17.9629(19.8463) | Error 0.7000(0.7599) Steps 442(444.55) | Grad Norm 4.8767(11.6348) | Total Time 0.00(0.00)\n",
      "Iter 0070 | Time 11.6981(14.0372) | Bit/dim 7.3302(8.0559) | Xent 2.0629(2.1473) | Loss 17.3206(19.2846) | Error 0.6833(0.7408) Steps 484(448.96) | Grad Norm 3.9273(9.8115) | Total Time 0.00(0.00)\n",
      "Iter 0080 | Time 12.4577(13.3956) | Bit/dim 7.1442(7.8358) | Xent 2.0764(2.1296) | Loss 17.2717(18.7646) | Error 0.6878(0.7266) Steps 496(453.12) | Grad Norm 2.7744(8.1111) | Total Time 0.00(0.00)\n",
      "Iter 0090 | Time 11.1783(12.9338) | Bit/dim 7.0745(7.6434) | Xent 2.1014(2.1214) | Loss 16.9317(18.3140) | Error 0.7167(0.7212) Steps 472(458.30) | Grad Norm 3.1130(6.8348) | Total Time 0.00(0.00)\n",
      "Iter 0100 | Time 11.8618(12.5756) | Bit/dim 7.0379(7.4869) | Xent 2.0964(2.1128) | Loss 17.0073(17.9468) | Error 0.7300(0.7200) Steps 466(458.06) | Grad Norm 2.3392(5.8018) | Total Time 0.00(0.00)\n",
      "Iter 0110 | Time 11.8323(12.3557) | Bit/dim 6.9918(7.3624) | Xent 2.0755(2.1022) | Loss 16.7439(17.6464) | Error 0.7056(0.7166) Steps 484(461.20) | Grad Norm 3.0189(5.1056) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0002 | Time 65.2687, Epoch Time 719.0117(705.3733), Bit/dim 6.9959(best: 7.7088), Xent 2.0523, Loss 8.0221, Error 0.6911(best: 0.6980)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0120 | Time 11.2246(12.1808) | Bit/dim 6.9556(7.2617) | Xent 2.0450(2.0897) | Loss 16.4712(17.8126) | Error 0.6856(0.7106) Steps 466(463.14) | Grad Norm 3.9921(4.6724) | Total Time 0.00(0.00)\n",
      "Iter 0130 | Time 11.2991(12.0804) | Bit/dim 6.9353(7.1766) | Xent 2.0547(2.0783) | Loss 16.4653(17.4958) | Error 0.7167(0.7063) Steps 466(467.33) | Grad Norm 7.8423(4.6063) | Total Time 0.00(0.00)\n",
      "Iter 0140 | Time 11.7463(12.0657) | Bit/dim 6.8863(7.1054) | Xent 2.0401(2.0728) | Loss 16.5932(17.2715) | Error 0.6867(0.7054) Steps 490(472.69) | Grad Norm 8.0890(5.6839) | Total Time 0.00(0.00)\n",
      "Iter 0150 | Time 12.7498(12.0912) | Bit/dim 6.8106(7.0369) | Xent 2.0420(2.0623) | Loss 16.2185(17.0453) | Error 0.7011(0.7023) Steps 502(475.68) | Grad Norm 12.4702(6.4615) | Total Time 0.00(0.00)\n",
      "Iter 0160 | Time 11.7141(12.1611) | Bit/dim 6.7522(6.9687) | Xent 2.0626(2.0528) | Loss 16.3336(16.8614) | Error 0.7411(0.6987) Steps 478(477.03) | Grad Norm 17.9503(8.0483) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0003 | Time 66.3654, Epoch Time 746.3779(706.6035), Bit/dim 6.6863(best: 6.9959), Xent 1.9986, Loss 7.6857, Error 0.6645(best: 0.6911)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0170 | Time 12.5620(12.2161) | Bit/dim 6.6466(6.8982) | Xent 2.0242(2.0525) | Loss 15.9306(17.1502) | Error 0.6911(0.7015) Steps 460(477.82) | Grad Norm 30.6606(13.7727) | Total Time 0.00(0.00)\n",
      "Iter 0180 | Time 12.3335(12.3406) | Bit/dim 6.5064(6.8116) | Xent 2.0093(2.0409) | Loss 15.8469(16.8269) | Error 0.6889(0.6963) Steps 490(484.91) | Grad Norm 15.4918(14.9917) | Total Time 0.00(0.00)\n",
      "Iter 0190 | Time 12.0060(12.3645) | Bit/dim 6.3453(6.7101) | Xent 2.0509(2.0459) | Loss 15.3974(16.5057) | Error 0.7289(0.7059) Steps 472(487.33) | Grad Norm 54.6608(23.1252) | Total Time 0.00(0.00)\n",
      "Iter 0200 | Time 12.6111(12.5059) | Bit/dim 6.2067(6.5889) | Xent 2.1529(2.0582) | Loss 15.5175(16.2190) | Error 0.7600(0.7191) Steps 520(488.06) | Grad Norm 112.0735(37.9618) | Total Time 0.00(0.00)\n",
      "Iter 0210 | Time 12.5375(12.5715) | Bit/dim 5.9838(6.4475) | Xent 2.1173(2.0566) | Loss 15.1018(15.8941) | Error 0.7856(0.7191) Steps 526(493.02) | Grad Norm 66.4967(41.5730) | Total Time 0.00(0.00)\n",
      "Iter 0220 | Time 12.1192(12.5198) | Bit/dim 6.1898(6.3199) | Xent 2.3213(2.0635) | Loss 15.4188(15.6178) | Error 0.8033(0.7218) Steps 484(495.02) | Grad Norm 273.4795(54.7720) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0004 | Time 68.9674, Epoch Time 780.1883(708.8110), Bit/dim 6.0777(best: 6.6863), Xent 2.1551, Loss 7.1553, Error 0.7637(best: 0.6645)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0230 | Time 12.3958(12.6563) | Bit/dim 5.8674(6.2222) | Xent 2.0789(2.0618) | Loss 14.6663(15.8056) | Error 0.7400(0.7204) Steps 490(495.50) | Grad Norm 46.6788(56.0992) | Total Time 0.00(0.00)\n",
      "Iter 0240 | Time 12.2350(12.5861) | Bit/dim 5.7168(6.1065) | Xent 2.0141(2.0574) | Loss 14.2345(15.4342) | Error 0.6744(0.7166) Steps 508(494.42) | Grad Norm 23.3851(48.2765) | Total Time 0.00(0.00)\n",
      "Iter 0250 | Time 12.7874(12.5623) | Bit/dim 5.6586(6.0034) | Xent 1.9945(2.0511) | Loss 13.9927(15.1091) | Error 0.6778(0.7140) Steps 484(493.80) | Grad Norm 25.5513(44.4641) | Total Time 0.00(0.00)\n",
      "Iter 0260 | Time 12.3151(12.5655) | Bit/dim 5.6129(5.9123) | Xent 1.9833(2.0366) | Loss 13.9637(14.8390) | Error 0.6489(0.7041) Steps 526(496.59) | Grad Norm 7.1936(38.2432) | Total Time 0.00(0.00)\n",
      "Iter 0270 | Time 13.2669(12.5565) | Bit/dim 5.5927(5.8336) | Xent 1.9450(2.0195) | Loss 13.8321(14.6054) | Error 0.6567(0.6953) Steps 490(494.50) | Grad Norm 3.8255(31.4561) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0005 | Time 68.2473, Epoch Time 777.2097(710.8630), Bit/dim 5.5795(best: 6.0777), Xent 1.9303, Loss 6.5447, Error 0.6416(best: 0.6645)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0280 | Time 12.4227(12.5155) | Bit/dim 5.5631(5.7699) | Xent 1.9696(2.0031) | Loss 13.9941(14.9192) | Error 0.6622(0.6883) Steps 496(495.55) | Grad Norm 16.3185(27.9338) | Total Time 0.00(0.00)\n",
      "Iter 0290 | Time 13.2100(12.5889) | Bit/dim 5.4839(5.7094) | Xent 1.9250(1.9849) | Loss 13.7382(14.6376) | Error 0.6467(0.6813) Steps 466(496.14) | Grad Norm 15.7185(26.7061) | Total Time 0.00(0.00)\n",
      "Iter 0300 | Time 12.4842(12.6706) | Bit/dim 5.5994(5.6739) | Xent 2.0387(2.0081) | Loss 13.9374(14.4703) | Error 0.7233(0.6907) Steps 508(498.98) | Grad Norm 63.4086(40.0922) | Total Time 0.00(0.00)\n",
      "Iter 0310 | Time 12.7941(12.6302) | Bit/dim 5.5423(5.6334) | Xent 2.1074(2.0049) | Loss 14.1603(14.3041) | Error 0.7300(0.6934) Steps 496(499.94) | Grad Norm 77.8788(39.3579) | Total Time 0.00(0.00)\n",
      "Iter 0320 | Time 12.7907(12.6124) | Bit/dim 5.4372(5.5937) | Xent 1.9654(1.9997) | Loss 13.6138(14.1485) | Error 0.6644(0.6919) Steps 508(498.74) | Grad Norm 6.3112(34.2367) | Total Time 0.00(0.00)\n",
      "Iter 0330 | Time 12.5493(12.6209) | Bit/dim 5.4155(5.5497) | Xent 1.9088(1.9897) | Loss 13.5886(14.0076) | Error 0.6733(0.6905) Steps 502(499.38) | Grad Norm 6.9851(31.2248) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0006 | Time 69.7664, Epoch Time 785.1267(713.0909), Bit/dim 5.4109(best: 5.5795), Xent 1.9857, Loss 6.4038, Error 0.7112(best: 0.6416)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0340 | Time 12.7553(12.6018) | Bit/dim 5.4010(5.5101) | Xent 2.0091(1.9874) | Loss 13.6867(14.3333) | Error 0.6978(0.6918) Steps 496(501.04) | Grad Norm 26.0521(31.9460) | Total Time 0.00(0.00)\n",
      "Iter 0350 | Time 12.5258(12.6768) | Bit/dim 5.3138(5.4697) | Xent 1.8999(1.9777) | Loss 13.2407(14.0928) | Error 0.6522(0.6876) Steps 526(505.30) | Grad Norm 18.1118(28.8764) | Total Time 0.00(0.00)\n",
      "Iter 0360 | Time 13.3890(12.7320) | Bit/dim 5.3535(5.4327) | Xent 1.9332(1.9705) | Loss 13.3859(13.9224) | Error 0.6500(0.6837) Steps 496(506.73) | Grad Norm 12.0942(26.0394) | Total Time 0.00(0.00)\n",
      "Iter 0370 | Time 13.1407(12.7481) | Bit/dim 5.2785(5.3949) | Xent 1.8850(1.9573) | Loss 13.3618(13.7650) | Error 0.6511(0.6774) Steps 544(507.10) | Grad Norm 20.3623(22.9936) | Total Time 0.00(0.00)\n",
      "Iter 0380 | Time 13.0305(12.7595) | Bit/dim 5.2590(5.3591) | Xent 1.8888(1.9421) | Loss 13.3197(13.6274) | Error 0.6400(0.6708) Steps 496(507.35) | Grad Norm 12.2324(20.0092) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0007 | Time 72.0251, Epoch Time 793.5011(715.5032), Bit/dim 5.2325(best: 5.4109), Xent 1.8658, Loss 6.1655, Error 0.6319(best: 0.6416)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0390 | Time 13.2229(12.8423) | Bit/dim 5.2692(5.3328) | Xent 1.8931(1.9300) | Loss 13.4347(14.0407) | Error 0.6711(0.6654) Steps 490(506.77) | Grad Norm 7.8168(17.6604) | Total Time 0.00(0.00)\n",
      "Iter 0400 | Time 13.1547(12.9634) | Bit/dim 5.2219(5.3061) | Xent 1.9481(1.9334) | Loss 13.0957(13.8397) | Error 0.6744(0.6701) Steps 502(507.79) | Grad Norm 33.5337(21.9926) | Total Time 0.00(0.00)\n",
      "Iter 0410 | Time 14.1309(13.0221) | Bit/dim 5.1839(5.2760) | Xent 1.9045(1.9318) | Loss 13.1488(13.6415) | Error 0.6844(0.6742) Steps 556(511.97) | Grad Norm 28.1724(23.6967) | Total Time 0.00(0.00)\n",
      "Iter 0420 | Time 12.9355(13.0786) | Bit/dim 5.1544(5.2506) | Xent 1.8843(1.9194) | Loss 12.9918(13.4916) | Error 0.6267(0.6688) Steps 478(510.53) | Grad Norm 14.4926(22.4951) | Total Time 0.00(0.00)\n",
      "Iter 0430 | Time 13.1948(13.1607) | Bit/dim 5.1592(5.2223) | Xent 1.8969(1.9143) | Loss 13.0663(13.3710) | Error 0.6567(0.6667) Steps 526(513.00) | Grad Norm 5.0168(22.0625) | Total Time 0.00(0.00)\n",
      "Iter 0440 | Time 13.1858(13.2058) | Bit/dim 5.1254(5.1990) | Xent 1.9397(1.9161) | Loss 13.1155(13.2816) | Error 0.6667(0.6676) Steps 502(513.48) | Grad Norm 29.5754(26.1702) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0008 | Time 71.6902, Epoch Time 819.3966(718.6200), Bit/dim 5.1028(best: 5.2325), Xent 1.8558, Loss 6.0307, Error 0.6345(best: 0.6319)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0450 | Time 13.4433(13.1807) | Bit/dim 5.0610(5.1758) | Xent 1.8951(1.9101) | Loss 12.8968(13.6278) | Error 0.6800(0.6673) Steps 538(513.16) | Grad Norm 23.6031(24.6526) | Total Time 0.00(0.00)\n",
      "Iter 0460 | Time 13.6381(13.2178) | Bit/dim 5.0337(5.1523) | Xent 1.8201(1.8992) | Loss 12.7715(13.4322) | Error 0.6189(0.6616) Steps 562(515.09) | Grad Norm 19.4680(23.7961) | Total Time 0.00(0.00)\n",
      "Iter 0470 | Time 13.3217(13.2833) | Bit/dim 5.0491(5.1247) | Xent 1.9830(1.8991) | Loss 12.9079(13.2704) | Error 0.7189(0.6642) Steps 526(515.72) | Grad Norm 56.1204(26.3159) | Total Time 0.00(0.00)\n",
      "Iter 0480 | Time 13.1806(13.2871) | Bit/dim 5.0137(5.0942) | Xent 1.9253(1.8999) | Loss 12.7240(13.1270) | Error 0.7000(0.6672) Steps 520(517.19) | Grad Norm 37.4615(28.4748) | Total Time 0.00(0.00)\n",
      "Iter 0490 | Time 13.9865(13.3657) | Bit/dim 4.9517(5.0660) | Xent 1.8923(1.8993) | Loss 12.4901(13.0125) | Error 0.6644(0.6658) Steps 526(519.08) | Grad Norm 23.9037(28.3623) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0009 | Time 72.0081, Epoch Time 825.7015(721.8324), Bit/dim 4.9424(best: 5.1028), Xent 1.8183, Loss 5.8516, Error 0.6227(best: 0.6319)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0500 | Time 14.0059(13.5197) | Bit/dim 4.9319(5.0334) | Xent 1.8113(1.8888) | Loss 12.6057(13.4078) | Error 0.6500(0.6604) Steps 526(521.84) | Grad Norm 6.2557(24.2834) | Total Time 0.00(0.00)\n",
      "Iter 0510 | Time 14.2684(13.6026) | Bit/dim 4.8733(5.0066) | Xent 1.8636(1.8821) | Loss 12.4089(13.1783) | Error 0.6567(0.6571) Steps 502(521.89) | Grad Norm 20.1678(25.2269) | Total Time 0.00(0.00)\n",
      "Iter 0520 | Time 14.9855(13.6935) | Bit/dim 4.9056(4.9827) | Xent 1.9153(1.8717) | Loss 12.6213(12.9930) | Error 0.6667(0.6551) Steps 580(523.97) | Grad Norm 45.3597(27.7433) | Total Time 0.00(0.00)\n",
      "Iter 0530 | Time 14.3895(13.7881) | Bit/dim 4.8462(4.9558) | Xent 1.8706(1.8836) | Loss 12.4647(12.8719) | Error 0.6678(0.6597) Steps 550(527.50) | Grad Norm 25.1401(29.6627) | Total Time 0.00(0.00)\n",
      "Iter 0540 | Time 13.7653(13.8920) | Bit/dim 4.8477(4.9267) | Xent 1.8764(1.8705) | Loss 12.2604(12.7117) | Error 0.6956(0.6565) Steps 520(529.83) | Grad Norm 26.1187(26.6699) | Total Time 0.00(0.00)\n",
      "Iter 0550 | Time 13.3844(13.9666) | Bit/dim 4.9039(4.9161) | Xent 1.8607(1.8712) | Loss 12.4957(12.6552) | Error 0.6611(0.6603) Steps 556(534.71) | Grad Norm 38.6675(30.6793) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0010 | Time 75.6568, Epoch Time 865.9713(726.1566), Bit/dim 4.8436(best: 4.9424), Xent 1.8120, Loss 5.7496, Error 0.6388(best: 0.6227)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0560 | Time 14.2339(13.9672) | Bit/dim 4.7934(4.8969) | Xent 1.8568(1.8671) | Loss 12.2460(13.0298) | Error 0.6633(0.6589) Steps 538(535.31) | Grad Norm 29.8754(30.3598) | Total Time 0.00(0.00)\n",
      "Iter 0570 | Time 14.5595(14.0550) | Bit/dim 4.8175(4.8719) | Xent 1.8192(1.8506) | Loss 12.4329(12.8340) | Error 0.6567(0.6533) Steps 538(540.28) | Grad Norm 16.9936(26.9243) | Total Time 0.00(0.00)\n",
      "Iter 0580 | Time 14.9268(14.1178) | Bit/dim 4.8080(4.8508) | Xent 1.8577(1.8404) | Loss 12.3294(12.6841) | Error 0.6589(0.6500) Steps 544(541.57) | Grad Norm 50.3769(29.7279) | Total Time 0.00(0.00)\n",
      "Iter 0590 | Time 15.3297(14.3099) | Bit/dim 4.7821(4.8320) | Xent 1.8362(1.8496) | Loss 12.1386(12.5793) | Error 0.6544(0.6544) Steps 508(543.94) | Grad Norm 30.7045(31.8987) | Total Time 0.00(0.00)\n",
      "Iter 0600 | Time 15.1031(14.4082) | Bit/dim 4.7302(4.8155) | Xent 1.7722(1.8324) | Loss 12.1403(12.4879) | Error 0.6044(0.6498) Steps 556(551.48) | Grad Norm 26.1164(29.4402) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0011 | Time 77.8768, Epoch Time 889.3141(731.0513), Bit/dim 4.7331(best: 4.8436), Xent 1.6827, Loss 5.5745, Error 0.5871(best: 0.6227)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0610 | Time 14.9635(14.5117) | Bit/dim 4.7189(4.7935) | Xent 1.7376(1.8067) | Loss 12.0573(12.9291) | Error 0.6178(0.6389) Steps 574(556.57) | Grad Norm 34.8719(26.9863) | Total Time 0.00(0.00)\n",
      "Iter 0620 | Time 15.0290(14.5353) | Bit/dim 4.7321(4.7772) | Xent 1.7229(1.7955) | Loss 12.1959(12.7057) | Error 0.5922(0.6342) Steps 598(559.72) | Grad Norm 29.0256(28.0927) | Total Time 0.00(0.00)\n",
      "Iter 0630 | Time 14.5434(14.5991) | Bit/dim 4.7854(4.7693) | Xent 1.8851(1.8076) | Loss 12.3588(12.5874) | Error 0.6867(0.6398) Steps 574(565.90) | Grad Norm 46.6121(33.2086) | Total Time 0.00(0.00)\n",
      "Iter 0640 | Time 14.8930(14.6097) | Bit/dim 4.6850(4.7607) | Xent 1.7716(1.8143) | Loss 12.0842(12.4812) | Error 0.6289(0.6413) Steps 586(567.19) | Grad Norm 9.2539(31.8986) | Total Time 0.00(0.00)\n",
      "Iter 0650 | Time 15.6592(14.6218) | Bit/dim 4.6510(4.7415) | Xent 1.7582(1.8115) | Loss 11.9517(12.3735) | Error 0.6378(0.6422) Steps 598(567.02) | Grad Norm 7.7653(28.0987) | Total Time 0.00(0.00)\n",
      "Iter 0660 | Time 14.6540(14.6241) | Bit/dim 4.7331(4.7264) | Xent 1.7626(1.7932) | Loss 12.0812(12.2714) | Error 0.6044(0.6340) Steps 568(566.77) | Grad Norm 30.5463(24.7577) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0012 | Time 76.2822, Epoch Time 902.0597(736.1816), Bit/dim 4.6794(best: 4.7331), Xent 1.6614, Loss 5.5101, Error 0.5798(best: 0.5871)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0670 | Time 15.2726(14.7075) | Bit/dim 4.6688(4.7126) | Xent 1.7235(1.7768) | Loss 11.8314(12.6393) | Error 0.6256(0.6308) Steps 610(569.04) | Grad Norm 32.3188(25.9878) | Total Time 0.00(0.00)\n",
      "Iter 0680 | Time 15.1026(14.7114) | Bit/dim 4.6247(4.6975) | Xent 1.7011(1.7670) | Loss 11.9735(12.4439) | Error 0.6000(0.6260) Steps 580(567.68) | Grad Norm 12.7302(25.9523) | Total Time 0.00(0.00)\n",
      "Iter 0690 | Time 15.0122(14.7599) | Bit/dim 4.6670(4.6874) | Xent 1.6974(1.7531) | Loss 11.8021(12.2927) | Error 0.6089(0.6209) Steps 544(566.89) | Grad Norm 20.0898(26.8907) | Total Time 0.00(0.00)\n",
      "Iter 0700 | Time 15.5555(14.8046) | Bit/dim 4.5901(4.6736) | Xent 1.7145(1.7483) | Loss 11.8295(12.1799) | Error 0.6056(0.6199) Steps 616(569.52) | Grad Norm 11.2696(27.0418) | Total Time 0.00(0.00)\n",
      "Iter 0710 | Time 14.3164(14.7968) | Bit/dim 4.6088(4.6587) | Xent 1.6582(1.7289) | Loss 11.7357(12.0632) | Error 0.5756(0.6138) Steps 568(571.68) | Grad Norm 22.1321(24.0150) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0013 | Time 77.8956, Epoch Time 911.5257(741.4419), Bit/dim 4.6007(best: 4.6794), Xent 1.6018, Loss 5.4016, Error 0.5681(best: 0.5798)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0720 | Time 15.1467(14.8582) | Bit/dim 4.6001(4.6471) | Xent 1.7037(1.7225) | Loss 11.8377(12.5576) | Error 0.6122(0.6126) Steps 598(573.95) | Grad Norm 28.4059(24.5656) | Total Time 0.00(0.00)\n",
      "Iter 0730 | Time 14.7460(14.9362) | Bit/dim 4.6266(4.6389) | Xent 1.6398(1.7217) | Loss 11.6948(12.3519) | Error 0.5733(0.6114) Steps 586(580.59) | Grad Norm 12.2955(25.6778) | Total Time 0.00(0.00)\n",
      "Iter 0740 | Time 14.7076(14.9034) | Bit/dim 4.5957(4.6265) | Xent 1.6556(1.7070) | Loss 11.6430(12.1900) | Error 0.5711(0.6073) Steps 544(583.96) | Grad Norm 14.2484(23.8851) | Total Time 0.00(0.00)\n",
      "Iter 0750 | Time 14.5314(14.9055) | Bit/dim 4.6434(4.6274) | Xent 1.8708(1.7136) | Loss 12.1244(12.1091) | Error 0.6478(0.6095) Steps 586(583.21) | Grad Norm 44.6931(26.9290) | Total Time 0.00(0.00)\n",
      "Iter 0760 | Time 14.5035(14.8962) | Bit/dim 4.5885(4.6323) | Xent 1.7000(1.7238) | Loss 11.7648(12.0705) | Error 0.5889(0.6113) Steps 568(585.96) | Grad Norm 7.4306(26.8515) | Total Time 0.00(0.00)\n",
      "Iter 0770 | Time 14.6827(14.8221) | Bit/dim 4.5689(4.6187) | Xent 1.7198(1.7195) | Loss 11.6461(11.9771) | Error 0.5967(0.6103) Steps 562(581.61) | Grad Norm 12.7564(23.6845) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0014 | Time 78.2237, Epoch Time 915.8940(746.6755), Bit/dim 4.5671(best: 4.6007), Xent 1.6022, Loss 5.3682, Error 0.5697(best: 0.5681)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0780 | Time 14.5704(14.8728) | Bit/dim 4.5291(4.6005) | Xent 1.6660(1.6940) | Loss 11.6054(12.3328) | Error 0.6078(0.6034) Steps 592(582.16) | Grad Norm 20.8744(21.4378) | Total Time 0.00(0.00)\n",
      "Iter 0790 | Time 15.0802(14.8999) | Bit/dim 4.5704(4.5964) | Xent 1.6854(1.6863) | Loss 11.7805(12.1807) | Error 0.6133(0.6014) Steps 604(580.91) | Grad Norm 21.9264(22.6071) | Total Time 0.00(0.00)\n",
      "Iter 0800 | Time 15.4485(14.8690) | Bit/dim 4.5200(4.5879) | Xent 1.6504(1.6777) | Loss 11.5846(12.0419) | Error 0.5856(0.5987) Steps 562(580.50) | Grad Norm 15.5553(21.5401) | Total Time 0.00(0.00)\n",
      "Iter 0810 | Time 15.2798(14.8032) | Bit/dim 4.5203(4.5718) | Xent 1.6176(1.6715) | Loss 11.6492(11.9230) | Error 0.5889(0.5976) Steps 550(575.98) | Grad Norm 13.1392(19.5148) | Total Time 0.00(0.00)\n",
      "Iter 0820 | Time 14.3787(14.7478) | Bit/dim 4.4785(4.5523) | Xent 1.6396(1.6588) | Loss 11.3743(11.8014) | Error 0.5833(0.5929) Steps 574(576.77) | Grad Norm 4.3630(17.6985) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0015 | Time 77.4082, Epoch Time 907.3711(751.4963), Bit/dim 4.4949(best: 4.5671), Xent 1.5378, Loss 5.2638, Error 0.5522(best: 0.5681)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0830 | Time 15.1485(14.7211) | Bit/dim 4.5162(4.5410) | Xent 1.7412(1.6561) | Loss 11.7263(12.2785) | Error 0.6089(0.5922) Steps 544(575.57) | Grad Norm 42.6699(20.3495) | Total Time 0.00(0.00)\n",
      "Iter 0840 | Time 14.2634(14.6865) | Bit/dim 4.4998(4.5248) | Xent 1.6065(1.6499) | Loss 11.5129(12.0705) | Error 0.5856(0.5912) Steps 556(577.10) | Grad Norm 24.3770(19.6828) | Total Time 0.00(0.00)\n",
      "Iter 0850 | Time 14.5040(14.6911) | Bit/dim 4.4585(4.5155) | Xent 1.6473(1.6386) | Loss 11.4454(11.9164) | Error 0.5767(0.5857) Steps 562(579.24) | Grad Norm 30.6861(19.9750) | Total Time 0.00(0.00)\n",
      "Iter 0860 | Time 15.3189(14.8029) | Bit/dim 4.5334(4.5084) | Xent 1.8987(1.6444) | Loss 11.8530(11.8162) | Error 0.6644(0.5864) Steps 592(584.72) | Grad Norm 58.7827(23.0784) | Total Time 0.00(0.00)\n",
      "Iter 0870 | Time 14.2290(14.7642) | Bit/dim 4.4803(4.5120) | Xent 1.6852(1.6653) | Loss 11.2324(11.7588) | Error 0.6311(0.5949) Steps 568(585.01) | Grad Norm 8.2625(23.7900) | Total Time 0.00(0.00)\n",
      "Iter 0880 | Time 14.4683(14.7521) | Bit/dim 4.4722(4.5000) | Xent 1.6405(1.6673) | Loss 11.5897(11.7064) | Error 0.5944(0.5970) Steps 580(586.98) | Grad Norm 18.1515(21.2615) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0016 | Time 79.3794, Epoch Time 908.9641(756.2204), Bit/dim 4.4505(best: 4.4949), Xent 1.5222, Loss 5.2116, Error 0.5453(best: 0.5522)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0890 | Time 15.1947(14.7306) | Bit/dim 4.4456(4.4898) | Xent 1.6251(1.6550) | Loss 11.3842(12.0983) | Error 0.6122(0.5941) Steps 610(584.63) | Grad Norm 22.8880(20.3857) | Total Time 0.00(0.00)\n",
      "Iter 0900 | Time 14.0370(14.7225) | Bit/dim 4.4138(4.4781) | Xent 1.5277(1.6387) | Loss 11.2540(11.9119) | Error 0.5444(0.5884) Steps 556(583.35) | Grad Norm 19.9982(20.3835) | Total Time 0.00(0.00)\n",
      "Iter 0910 | Time 14.7570(14.6955) | Bit/dim 4.4043(4.4606) | Xent 1.5636(1.6149) | Loss 11.1428(11.7299) | Error 0.5611(0.5787) Steps 586(581.29) | Grad Norm 10.2769(18.4949) | Total Time 0.00(0.00)\n",
      "Iter 0920 | Time 15.7252(14.6578) | Bit/dim 4.3921(4.4434) | Xent 1.5514(1.6025) | Loss 11.3098(11.5946) | Error 0.5633(0.5760) Steps 574(577.01) | Grad Norm 14.7397(17.4110) | Total Time 0.00(0.00)\n",
      "Iter 0930 | Time 14.5225(14.6411) | Bit/dim 4.4481(4.4480) | Xent 1.6788(1.6270) | Loss 11.4686(11.5677) | Error 0.6067(0.5851) Steps 598(578.24) | Grad Norm 22.2105(20.9708) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0017 | Time 78.9457, Epoch Time 899.9409(760.5320), Bit/dim 4.4106(best: 4.4505), Xent 1.5499, Loss 5.1855, Error 0.5667(best: 0.5453)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0940 | Time 15.0058(14.6450) | Bit/dim 4.3871(4.4376) | Xent 1.5544(1.6193) | Loss 11.2027(12.0332) | Error 0.5878(0.5837) Steps 604(579.00) | Grad Norm 12.3187(18.9070) | Total Time 0.00(0.00)\n",
      "Iter 0950 | Time 14.9282(14.6604) | Bit/dim 4.4014(4.4228) | Xent 1.5253(1.6040) | Loss 11.1487(11.8259) | Error 0.5467(0.5793) Steps 616(581.13) | Grad Norm 14.7848(17.7775) | Total Time 0.00(0.00)\n",
      "Iter 0960 | Time 14.6667(14.7160) | Bit/dim 4.3514(4.4053) | Xent 1.5755(1.5815) | Loss 11.0954(11.6405) | Error 0.5556(0.5698) Steps 556(578.00) | Grad Norm 31.9611(16.9912) | Total Time 0.00(0.00)\n",
      "Iter 0970 | Time 15.3230(14.7219) | Bit/dim 4.3689(4.3923) | Xent 1.5538(1.5765) | Loss 11.1324(11.5175) | Error 0.5544(0.5692) Steps 526(576.08) | Grad Norm 18.9275(17.0031) | Total Time 0.00(0.00)\n",
      "Iter 0980 | Time 15.5920(14.7620) | Bit/dim 4.3326(4.3735) | Xent 1.4758(1.5652) | Loss 11.1051(11.3875) | Error 0.5567(0.5650) Steps 586(577.37) | Grad Norm 5.9331(15.3268) | Total Time 0.00(0.00)\n",
      "Iter 0990 | Time 14.1623(14.6616) | Bit/dim 4.6373(4.3896) | Xent 1.7218(1.5838) | Loss 11.9302(11.3797) | Error 0.6222(0.5703) Steps 598(575.82) | Grad Norm 28.2351(18.8836) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0018 | Time 80.6192, Epoch Time 907.7102(764.9473), Bit/dim 4.4262(best: 4.4106), Xent 1.6801, Loss 5.2662, Error 0.6072(best: 0.5453)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1000 | Time 14.8179(14.6717) | Bit/dim 4.4497(4.4080) | Xent 1.6631(1.6259) | Loss 11.3244(11.9050) | Error 0.6056(0.5853) Steps 568(576.34) | Grad Norm 19.3119(22.1093) | Total Time 0.00(0.00)\n",
      "Iter 1010 | Time 14.9171(14.6910) | Bit/dim 4.3403(4.3982) | Xent 1.5807(1.6207) | Loss 11.0332(11.7270) | Error 0.5622(0.5829) Steps 556(575.81) | Grad Norm 7.7334(19.1995) | Total Time 0.00(0.00)\n",
      "Iter 1020 | Time 14.9641(14.6926) | Bit/dim 4.3046(4.3772) | Xent 1.5682(1.6002) | Loss 11.1436(11.5632) | Error 0.5489(0.5759) Steps 604(577.97) | Grad Norm 6.1296(16.3138) | Total Time 0.00(0.00)\n",
      "Iter 1030 | Time 14.6210(14.7011) | Bit/dim 4.2807(4.3561) | Xent 1.4949(1.5820) | Loss 11.0152(11.4178) | Error 0.5556(0.5710) Steps 598(578.74) | Grad Norm 14.4076(14.3941) | Total Time 0.00(0.00)\n",
      "Iter 1040 | Time 15.3824(14.7156) | Bit/dim 4.3130(4.3370) | Xent 1.4778(1.5671) | Loss 11.0062(11.3005) | Error 0.5244(0.5655) Steps 586(579.11) | Grad Norm 12.6071(13.2871) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0019 | Time 78.5922, Epoch Time 907.8757(769.2352), Bit/dim 4.2808(best: 4.4106), Xent 1.4357, Loss 4.9987, Error 0.5208(best: 0.5453)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1050 | Time 13.5419(14.7169) | Bit/dim 4.2431(4.3212) | Xent 1.4653(1.5491) | Loss 10.8212(11.7321) | Error 0.5400(0.5584) Steps 568(577.78) | Grad Norm 23.0585(13.3129) | Total Time 0.00(0.00)\n",
      "Iter 1060 | Time 14.7891(14.7478) | Bit/dim 4.2752(4.3043) | Xent 1.4325(1.5270) | Loss 10.8770(11.4979) | Error 0.5278(0.5513) Steps 604(579.23) | Grad Norm 7.8912(12.0814) | Total Time 0.00(0.00)\n",
      "Iter 1070 | Time 14.3577(14.7311) | Bit/dim 4.4622(4.3053) | Xent 1.8077(1.5337) | Loss 11.6035(11.3814) | Error 0.5767(0.5518) Steps 544(576.37) | Grad Norm 50.2025(15.6411) | Total Time 0.00(0.00)\n",
      "Iter 1080 | Time 13.9316(14.5523) | Bit/dim 4.2784(4.3056) | Xent 1.5520(1.5362) | Loss 10.8215(11.2876) | Error 0.5367(0.5530) Steps 562(574.33) | Grad Norm 19.7728(15.1621) | Total Time 0.00(0.00)\n",
      "Iter 1090 | Time 14.4323(14.5612) | Bit/dim 4.2394(4.2917) | Xent 1.4689(1.5341) | Loss 10.8684(11.1842) | Error 0.5589(0.5535) Steps 592(575.34) | Grad Norm 10.0851(14.4091) | Total Time 0.00(0.00)\n",
      "Iter 1100 | Time 14.4197(14.6618) | Bit/dim 4.2535(4.2787) | Xent 1.4011(1.5272) | Loss 10.8494(11.1082) | Error 0.5067(0.5515) Steps 550(576.84) | Grad Norm 12.2974(13.7898) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0020 | Time 77.1072, Epoch Time 898.1547(773.1028), Bit/dim 4.2248(best: 4.2808), Xent 1.3971, Loss 4.9233, Error 0.5065(best: 0.5208)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1110 | Time 15.7565(14.6502) | Bit/dim 4.2322(4.2636) | Xent 1.5005(1.5067) | Loss 10.7342(11.4774) | Error 0.5200(0.5437) Steps 628(581.30) | Grad Norm 18.5386(13.7301) | Total Time 0.00(0.00)\n",
      "Iter 1120 | Time 15.7918(14.6567) | Bit/dim 4.1822(4.2493) | Xent 1.4649(1.4948) | Loss 10.7340(11.2875) | Error 0.5322(0.5396) Steps 544(579.32) | Grad Norm 12.2302(13.1259) | Total Time 0.00(0.00)\n",
      "Iter 1130 | Time 15.5609(14.6671) | Bit/dim 4.2087(4.2384) | Xent 1.4280(1.4728) | Loss 10.7656(11.1424) | Error 0.5200(0.5307) Steps 586(580.20) | Grad Norm 4.5262(12.4636) | Total Time 0.00(0.00)\n",
      "Iter 1140 | Time 14.7453(14.6239) | Bit/dim 4.1922(4.2280) | Xent 1.4293(1.4667) | Loss 10.6587(11.0293) | Error 0.5333(0.5280) Steps 544(579.57) | Grad Norm 11.3661(13.9671) | Total Time 0.00(0.00)\n",
      "Iter 1150 | Time 14.9985(14.6161) | Bit/dim 4.1856(4.2170) | Xent 1.4500(1.4658) | Loss 10.7999(10.9517) | Error 0.5411(0.5280) Steps 604(580.58) | Grad Norm 7.7621(13.6667) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0021 | Time 78.6830, Epoch Time 901.9945(776.9695), Bit/dim 4.1917(best: 4.2248), Xent 1.3510, Loss 4.8673, Error 0.4917(best: 0.5065)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1160 | Time 15.4183(14.6598) | Bit/dim 4.2003(4.2124) | Xent 1.4033(1.4519) | Loss 10.8645(11.4458) | Error 0.5056(0.5254) Steps 604(580.98) | Grad Norm 22.5623(14.5961) | Total Time 0.00(0.00)\n",
      "Iter 1170 | Time 14.4608(14.5913) | Bit/dim 4.1884(4.2041) | Xent 1.3900(1.4426) | Loss 10.4304(11.2345) | Error 0.4822(0.5215) Steps 556(580.37) | Grad Norm 8.7014(15.0699) | Total Time 0.00(0.00)\n",
      "Iter 1180 | Time 15.9110(14.5277) | Bit/dim 4.1321(4.1936) | Xent 1.4307(1.4344) | Loss 10.7245(11.0770) | Error 0.5100(0.5186) Steps 646(583.92) | Grad Norm 16.1095(15.4686) | Total Time 0.00(0.00)\n",
      "Iter 1190 | Time 15.0910(14.5238) | Bit/dim 4.1841(4.1859) | Xent 1.3884(1.4243) | Loss 10.5738(10.9491) | Error 0.5178(0.5152) Steps 562(580.71) | Grad Norm 17.6568(14.8488) | Total Time 0.00(0.00)\n",
      "Iter 1200 | Time 13.5611(14.5159) | Bit/dim 4.1507(4.1773) | Xent 1.5060(1.4313) | Loss 10.6536(10.8667) | Error 0.5456(0.5159) Steps 556(579.22) | Grad Norm 33.8372(16.0079) | Total Time 0.00(0.00)\n",
      "Iter 1210 | Time 15.0266(14.5921) | Bit/dim 4.1706(4.1706) | Xent 1.3977(1.4349) | Loss 10.6345(10.8118) | Error 0.5000(0.5170) Steps 574(576.96) | Grad Norm 10.4023(16.4677) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0022 | Time 77.5821, Epoch Time 893.7159(780.4719), Bit/dim 4.1528(best: 4.1917), Xent 1.3310, Loss 4.8183, Error 0.4825(best: 0.4917)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1220 | Time 14.3745(14.5721) | Bit/dim 4.1450(4.1623) | Xent 1.3846(1.4157) | Loss 10.5370(11.1901) | Error 0.4900(0.5110) Steps 598(580.93) | Grad Norm 10.1898(15.3887) | Total Time 0.00(0.00)\n",
      "Iter 1230 | Time 13.8718(14.5344) | Bit/dim 4.1889(4.1623) | Xent 1.6660(1.4261) | Loss 10.9133(11.0589) | Error 0.5856(0.5114) Steps 568(581.52) | Grad Norm 32.7797(17.1525) | Total Time 0.00(0.00)\n",
      "Iter 1240 | Time 14.6853(14.4372) | Bit/dim 4.1589(4.1633) | Xent 1.4165(1.4367) | Loss 10.5308(10.9433) | Error 0.5111(0.5161) Steps 604(580.97) | Grad Norm 9.6908(16.3437) | Total Time 0.00(0.00)\n",
      "Iter 1250 | Time 14.5442(14.4518) | Bit/dim 4.1365(4.1566) | Xent 1.4604(1.4354) | Loss 10.6296(10.8494) | Error 0.5344(0.5170) Steps 598(581.25) | Grad Norm 13.9143(15.4077) | Total Time 0.00(0.00)\n",
      "Iter 1260 | Time 14.3064(14.4759) | Bit/dim 4.1456(4.1491) | Xent 1.3725(1.4269) | Loss 10.5848(10.7607) | Error 0.4833(0.5157) Steps 562(583.56) | Grad Norm 10.1392(14.6542) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0023 | Time 78.3655, Epoch Time 893.3041(783.8569), Bit/dim 4.1056(best: 4.1528), Xent 1.2958, Loss 4.7535, Error 0.4726(best: 0.4825)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1270 | Time 14.8354(14.6055) | Bit/dim 4.0941(4.1368) | Xent 1.3838(1.4146) | Loss 10.5458(11.2317) | Error 0.4922(0.5101) Steps 604(589.20) | Grad Norm 12.0963(14.0120) | Total Time 0.00(0.00)\n",
      "Iter 1280 | Time 14.8384(14.6264) | Bit/dim 4.0751(4.1240) | Xent 1.3292(1.3988) | Loss 10.4465(11.0216) | Error 0.4767(0.5047) Steps 598(588.27) | Grad Norm 11.6911(13.2593) | Total Time 0.00(0.00)\n",
      "Iter 1290 | Time 14.2371(14.6201) | Bit/dim 4.1575(4.1250) | Xent 1.3597(1.3893) | Loss 10.5996(10.8874) | Error 0.4756(0.5015) Steps 574(587.03) | Grad Norm 22.5128(15.1074) | Total Time 0.00(0.00)\n",
      "Iter 1300 | Time 15.0901(14.7187) | Bit/dim 4.0978(4.1204) | Xent 1.3666(1.3906) | Loss 10.4467(10.7712) | Error 0.4944(0.5032) Steps 604(588.16) | Grad Norm 5.5739(15.3697) | Total Time 0.00(0.00)\n",
      "Iter 1310 | Time 14.5071(14.6628) | Bit/dim 4.1054(4.1129) | Xent 1.3406(1.3813) | Loss 10.3425(10.6663) | Error 0.4856(0.5008) Steps 574(587.06) | Grad Norm 9.5127(14.5112) | Total Time 0.00(0.00)\n",
      "Iter 1320 | Time 15.2025(14.5994) | Bit/dim 4.0937(4.1059) | Xent 1.3642(1.3683) | Loss 10.4518(10.5965) | Error 0.4978(0.4961) Steps 586(586.22) | Grad Norm 11.3234(13.3076) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0024 | Time 77.9214, Epoch Time 901.5221(787.3868), Bit/dim 4.0816(best: 4.1056), Xent 1.2485, Loss 4.7059, Error 0.4519(best: 0.4726)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1330 | Time 15.3434(14.6160) | Bit/dim 4.0955(4.0961) | Xent 1.2537(1.3522) | Loss 10.3457(10.9986) | Error 0.4733(0.4916) Steps 622(586.00) | Grad Norm 6.0477(12.5129) | Total Time 0.00(0.00)\n",
      "Iter 1340 | Time 14.1173(14.5260) | Bit/dim 4.0482(4.0919) | Xent 1.3112(1.3452) | Loss 10.1978(10.8276) | Error 0.4733(0.4883) Steps 574(583.10) | Grad Norm 18.7176(13.6014) | Total Time 0.00(0.00)\n",
      "Iter 1350 | Time 14.4829(14.5613) | Bit/dim 4.0446(4.0843) | Xent 1.3056(1.3342) | Loss 10.2777(10.6964) | Error 0.4689(0.4842) Steps 592(584.69) | Grad Norm 8.2111(13.4650) | Total Time 0.00(0.00)\n",
      "Iter 1360 | Time 15.2718(14.5437) | Bit/dim 4.0586(4.0780) | Xent 1.2665(1.3273) | Loss 10.3351(10.6016) | Error 0.4678(0.4818) Steps 592(585.39) | Grad Norm 16.1304(12.8640) | Total Time 0.00(0.00)\n",
      "Iter 1370 | Time 13.9859(14.5194) | Bit/dim 4.0325(4.0738) | Xent 1.3888(1.3304) | Loss 10.2196(10.5350) | Error 0.4956(0.4828) Steps 568(583.32) | Grad Norm 14.3805(13.9988) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0025 | Time 78.6951, Epoch Time 894.5261(790.6010), Bit/dim 4.0449(best: 4.0816), Xent 1.2452, Loss 4.6675, Error 0.4504(best: 0.4519)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1380 | Time 14.9611(14.5710) | Bit/dim 4.0511(4.0706) | Xent 1.1838(1.3270) | Loss 10.1721(11.0352) | Error 0.4144(0.4794) Steps 592(585.11) | Grad Norm 7.0700(14.2150) | Total Time 0.00(0.00)\n",
      "Iter 1390 | Time 13.7625(14.5401) | Bit/dim 4.0676(4.0655) | Xent 1.2950(1.3199) | Loss 10.4174(10.8454) | Error 0.4700(0.4772) Steps 574(585.87) | Grad Norm 18.3659(14.2765) | Total Time 0.00(0.00)\n",
      "Iter 1400 | Time 14.1225(14.5311) | Bit/dim 4.0276(4.0585) | Xent 1.2677(1.3126) | Loss 10.1882(10.6880) | Error 0.4600(0.4742) Steps 598(584.53) | Grad Norm 6.5484(13.4407) | Total Time 0.00(0.00)\n",
      "Iter 1410 | Time 13.8186(14.5271) | Bit/dim 4.0684(4.0537) | Xent 1.3597(1.3126) | Loss 10.4175(10.5803) | Error 0.4944(0.4747) Steps 568(582.38) | Grad Norm 13.8947(14.1525) | Total Time 0.00(0.00)\n",
      "Iter 1420 | Time 14.7692(14.5225) | Bit/dim 4.0004(4.0478) | Xent 1.2634(1.3081) | Loss 10.1609(10.4906) | Error 0.4756(0.4736) Steps 562(580.81) | Grad Norm 5.6556(13.7087) | Total Time 0.00(0.00)\n",
      "Iter 1430 | Time 13.4509(14.5214) | Bit/dim 4.0317(4.0436) | Xent 1.2597(1.2982) | Loss 10.1267(10.4195) | Error 0.4378(0.4705) Steps 580(582.10) | Grad Norm 11.1307(12.9539) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0026 | Time 78.5344, Epoch Time 895.8892(793.7597), Bit/dim 4.0155(best: 4.0449), Xent 1.2149, Loss 4.6230, Error 0.4356(best: 0.4504)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1440 | Time 14.1027(14.4978) | Bit/dim 4.0462(4.0387) | Xent 1.2894(1.2845) | Loss 10.1447(10.8336) | Error 0.4767(0.4648) Steps 550(580.12) | Grad Norm 6.5175(12.2996) | Total Time 0.00(0.00)\n",
      "Iter 1450 | Time 14.5172(14.4728) | Bit/dim 4.0297(4.0354) | Xent 1.3797(1.2829) | Loss 10.2478(10.6669) | Error 0.5000(0.4641) Steps 556(581.00) | Grad Norm 29.0746(13.8135) | Total Time 0.00(0.00)\n",
      "Iter 1460 | Time 13.9324(14.3998) | Bit/dim 4.0118(4.0323) | Xent 1.2665(1.2883) | Loss 10.2515(10.5644) | Error 0.4522(0.4647) Steps 592(579.34) | Grad Norm 10.5035(13.9400) | Total Time 0.00(0.00)\n",
      "Iter 1470 | Time 14.4409(14.4199) | Bit/dim 4.0081(4.0294) | Xent 1.2427(1.2837) | Loss 10.2109(10.4750) | Error 0.4456(0.4632) Steps 628(583.21) | Grad Norm 13.5747(14.1926) | Total Time 0.00(0.00)\n",
      "Iter 1480 | Time 14.5187(14.4815) | Bit/dim 4.0175(4.0248) | Xent 1.2981(1.2909) | Loss 10.2365(10.4104) | Error 0.4767(0.4659) Steps 568(581.85) | Grad Norm 16.0213(14.3140) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0027 | Time 77.7141, Epoch Time 889.7328(796.6389), Bit/dim 4.0139(best: 4.0155), Xent 1.2059, Loss 4.6168, Error 0.4354(best: 0.4356)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1490 | Time 14.1089(14.4429) | Bit/dim 3.9889(4.0193) | Xent 1.2133(1.2804) | Loss 9.9188(10.8768) | Error 0.4456(0.4630) Steps 574(583.47) | Grad Norm 8.0790(13.5850) | Total Time 0.00(0.00)\n",
      "Iter 1500 | Time 14.2649(14.4701) | Bit/dim 3.9789(4.0152) | Xent 1.2053(1.2664) | Loss 9.9238(10.6626) | Error 0.4422(0.4572) Steps 586(581.81) | Grad Norm 10.0027(13.1079) | Total Time 0.00(0.00)\n",
      "Iter 1510 | Time 13.9519(14.4386) | Bit/dim 4.0258(4.0137) | Xent 1.3408(1.2660) | Loss 10.2603(10.5376) | Error 0.4722(0.4568) Steps 580(583.64) | Grad Norm 26.8140(13.3480) | Total Time 0.00(0.00)\n",
      "Iter 1520 | Time 14.1226(14.4505) | Bit/dim 4.0174(4.0086) | Xent 1.2195(1.2623) | Loss 10.1383(10.4276) | Error 0.4200(0.4552) Steps 544(581.89) | Grad Norm 13.0295(13.4350) | Total Time 0.00(0.00)\n",
      "Iter 1530 | Time 15.3877(14.5074) | Bit/dim 3.9571(3.9997) | Xent 1.2100(1.2561) | Loss 10.0709(10.3302) | Error 0.4233(0.4527) Steps 580(582.34) | Grad Norm 7.9823(12.8836) | Total Time 0.00(0.00)\n",
      "Iter 1540 | Time 14.3003(14.4671) | Bit/dim 3.9806(3.9953) | Xent 1.2205(1.2498) | Loss 10.0150(10.2683) | Error 0.4356(0.4507) Steps 556(582.58) | Grad Norm 12.2127(12.5795) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0028 | Time 77.7887, Epoch Time 890.2642(799.4476), Bit/dim 3.9741(best: 4.0139), Xent 1.1493, Loss 4.5488, Error 0.4159(best: 0.4354)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1550 | Time 14.5750(14.5251) | Bit/dim 3.9861(3.9910) | Xent 1.1160(1.2321) | Loss 10.0332(10.6565) | Error 0.4244(0.4445) Steps 610(583.74) | Grad Norm 6.2430(11.7981) | Total Time 0.00(0.00)\n",
      "Iter 1560 | Time 14.6088(14.5426) | Bit/dim 3.9626(3.9876) | Xent 1.2070(1.2227) | Loss 9.9379(10.4880) | Error 0.4456(0.4413) Steps 574(584.05) | Grad Norm 9.1869(11.3963) | Total Time 0.00(0.00)\n",
      "Iter 1570 | Time 15.1785(14.6092) | Bit/dim 3.9888(3.9841) | Xent 1.1827(1.2226) | Loss 10.1255(10.3852) | Error 0.4311(0.4413) Steps 580(584.44) | Grad Norm 9.3661(12.5964) | Total Time 0.00(0.00)\n",
      "Iter 1580 | Time 14.5555(14.6000) | Bit/dim 4.0059(3.9850) | Xent 1.4027(1.2466) | Loss 10.2531(10.3217) | Error 0.4878(0.4477) Steps 598(582.64) | Grad Norm 25.1122(14.8832) | Total Time 0.00(0.00)\n",
      "Iter 1590 | Time 13.5607(14.5524) | Bit/dim 3.9810(3.9863) | Xent 1.2549(1.2516) | Loss 10.0770(10.2676) | Error 0.4600(0.4501) Steps 586(583.24) | Grad Norm 8.3978(13.8094) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0029 | Time 79.4897, Epoch Time 900.4214(802.4768), Bit/dim 3.9752(best: 3.9741), Xent 1.1590, Loss 4.5547, Error 0.4146(best: 0.4159)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1600 | Time 14.1440(14.5297) | Bit/dim 3.9610(3.9810) | Xent 1.2206(1.2447) | Loss 10.0103(10.7490) | Error 0.4267(0.4470) Steps 592(584.43) | Grad Norm 9.4437(12.5916) | Total Time 0.00(0.00)\n",
      "Iter 1610 | Time 14.7039(14.4899) | Bit/dim 3.9998(3.9764) | Xent 1.1660(1.2325) | Loss 9.7930(10.5474) | Error 0.4322(0.4423) Steps 556(581.99) | Grad Norm 7.1973(11.8278) | Total Time 0.00(0.00)\n",
      "Iter 1620 | Time 15.0155(14.4725) | Bit/dim 3.9411(3.9719) | Xent 1.1519(1.2217) | Loss 9.8546(10.3917) | Error 0.3978(0.4378) Steps 592(581.65) | Grad Norm 6.8974(11.6005) | Total Time 0.00(0.00)\n",
      "Iter 1630 | Time 14.4020(14.4161) | Bit/dim 3.9411(3.9653) | Xent 1.1990(1.2091) | Loss 9.8911(10.2698) | Error 0.4278(0.4345) Steps 574(582.16) | Grad Norm 18.2854(11.3670) | Total Time 0.00(0.00)\n",
      "Iter 1640 | Time 14.5361(14.4961) | Bit/dim 3.9557(3.9618) | Xent 1.1524(1.2107) | Loss 9.9338(10.1988) | Error 0.4278(0.4351) Steps 592(584.01) | Grad Norm 16.0678(12.2606) | Total Time 0.00(0.00)\n",
      "Iter 1650 | Time 14.8902(14.5112) | Bit/dim 3.9662(3.9592) | Xent 1.1813(1.2099) | Loss 9.9970(10.1460) | Error 0.4067(0.4326) Steps 586(583.31) | Grad Norm 11.2270(13.1013) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0030 | Time 78.6529, Epoch Time 891.8291(805.1574), Bit/dim 3.9504(best: 3.9741), Xent 1.1296, Loss 4.5152, Error 0.4063(best: 0.4146)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1660 | Time 14.8813(14.5631) | Bit/dim 3.9134(3.9548) | Xent 1.1797(1.2040) | Loss 9.8413(10.5724) | Error 0.4189(0.4312) Steps 586(585.85) | Grad Norm 11.1774(13.0690) | Total Time 0.00(0.00)\n",
      "Iter 1670 | Time 13.8808(14.5518) | Bit/dim 3.9213(3.9525) | Xent 1.1496(1.1965) | Loss 9.8544(10.4236) | Error 0.4178(0.4287) Steps 586(585.17) | Grad Norm 8.7783(12.5197) | Total Time 0.00(0.00)\n",
      "Iter 1680 | Time 14.1412(14.5367) | Bit/dim 3.9345(3.9484) | Xent 1.2515(1.1915) | Loss 10.0472(10.2951) | Error 0.4267(0.4255) Steps 598(585.53) | Grad Norm 18.5804(12.8342) | Total Time 0.00(0.00)\n",
      "Iter 1690 | Time 13.6458(14.4993) | Bit/dim 3.9851(3.9503) | Xent 1.1860(1.1929) | Loss 9.9606(10.2034) | Error 0.4356(0.4256) Steps 568(583.63) | Grad Norm 10.3401(14.3688) | Total Time 0.00(0.00)\n",
      "Iter 1700 | Time 14.8734(14.5006) | Bit/dim 3.9265(3.9502) | Xent 1.2091(1.1979) | Loss 9.9698(10.1446) | Error 0.4422(0.4268) Steps 604(583.01) | Grad Norm 14.8230(14.5054) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0031 | Time 79.1332, Epoch Time 897.3209(807.9223), Bit/dim 3.9373(best: 3.9504), Xent 1.1012, Loss 4.4879, Error 0.3966(best: 0.4063)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1710 | Time 14.2456(14.5493) | Bit/dim 3.9363(3.9482) | Xent 1.1620(1.1855) | Loss 9.8735(10.6484) | Error 0.4200(0.4235) Steps 592(585.07) | Grad Norm 7.7302(13.6567) | Total Time 0.00(0.00)\n",
      "Iter 1720 | Time 14.6387(14.5270) | Bit/dim 3.9280(3.9458) | Xent 1.1830(1.1771) | Loss 9.9827(10.4588) | Error 0.4289(0.4205) Steps 562(582.36) | Grad Norm 16.2662(13.1361) | Total Time 0.00(0.00)\n",
      "Iter 1730 | Time 14.4880(14.5069) | Bit/dim 3.9347(3.9417) | Xent 1.1416(1.1683) | Loss 10.0272(10.3169) | Error 0.4044(0.4159) Steps 598(580.27) | Grad Norm 11.8842(12.7852) | Total Time 0.00(0.00)\n",
      "Iter 1740 | Time 14.1634(14.4895) | Bit/dim 3.9122(3.9388) | Xent 1.1501(1.1642) | Loss 9.9038(10.1899) | Error 0.4022(0.4159) Steps 598(578.65) | Grad Norm 9.2802(12.6209) | Total Time 0.00(0.00)\n",
      "Iter 1750 | Time 15.1055(14.5541) | Bit/dim 3.9334(3.9355) | Xent 1.1407(1.1570) | Loss 10.0137(10.1175) | Error 0.4156(0.4137) Steps 586(579.23) | Grad Norm 13.3181(12.8511) | Total Time 0.00(0.00)\n",
      "Iter 1760 | Time 14.2647(14.5311) | Bit/dim 3.9035(3.9302) | Xent 1.1160(1.1556) | Loss 9.9239(10.0567) | Error 0.3978(0.4123) Steps 586(580.70) | Grad Norm 15.2153(12.5516) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0032 | Time 78.3518, Epoch Time 894.5501(810.5211), Bit/dim 3.9156(best: 3.9373), Xent 1.0972, Loss 4.4642, Error 0.3858(best: 0.3966)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1770 | Time 14.8311(14.5764) | Bit/dim 3.8802(3.9259) | Xent 1.1083(1.1442) | Loss 9.8507(10.4857) | Error 0.4111(0.4085) Steps 598(580.17) | Grad Norm 7.7535(12.2664) | Total Time 0.00(0.00)\n",
      "Iter 1780 | Time 14.1848(14.5766) | Bit/dim 3.9150(3.9230) | Xent 1.1160(1.1334) | Loss 9.8246(10.3183) | Error 0.3878(0.4054) Steps 580(582.41) | Grad Norm 7.9434(11.4277) | Total Time 0.00(0.00)\n",
      "Iter 1790 | Time 14.1944(14.6140) | Bit/dim 3.8997(3.9200) | Xent 1.1598(1.1290) | Loss 9.7814(10.1828) | Error 0.4133(0.4032) Steps 574(584.76) | Grad Norm 15.1159(11.7810) | Total Time 0.00(0.00)\n",
      "Iter 1800 | Time 14.5771(14.6106) | Bit/dim 3.9131(3.9183) | Xent 1.1875(1.1297) | Loss 9.8017(10.0920) | Error 0.4033(0.4034) Steps 592(583.40) | Grad Norm 19.9766(12.8093) | Total Time 0.00(0.00)\n",
      "Iter 1810 | Time 14.6948(14.6699) | Bit/dim 3.8959(3.9196) | Xent 1.1792(1.1412) | Loss 9.6125(10.0295) | Error 0.4367(0.4082) Steps 538(583.51) | Grad Norm 10.5599(13.2525) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0033 | Time 77.1294, Epoch Time 901.2754(813.2438), Bit/dim 3.8982(best: 3.9156), Xent 1.0695, Loss 4.4329, Error 0.3800(best: 0.3858)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1820 | Time 14.2976(14.6317) | Bit/dim 3.8877(3.9154) | Xent 1.1282(1.1363) | Loss 9.8140(10.5035) | Error 0.3967(0.4067) Steps 592(582.80) | Grad Norm 16.2132(12.4110) | Total Time 0.00(0.00)\n",
      "Iter 1830 | Time 13.9838(14.5734) | Bit/dim 3.9170(3.9121) | Xent 1.1606(1.1279) | Loss 9.8211(10.3079) | Error 0.4211(0.4039) Steps 562(582.94) | Grad Norm 10.2439(11.8562) | Total Time 0.00(0.00)\n",
      "Iter 1840 | Time 15.2121(14.5765) | Bit/dim 3.9137(3.9110) | Xent 1.0886(1.1188) | Loss 9.8913(10.1821) | Error 0.3756(0.4003) Steps 574(581.80) | Grad Norm 9.8281(11.6854) | Total Time 0.00(0.00)\n",
      "Iter 1850 | Time 14.2972(14.5976) | Bit/dim 3.8709(3.9060) | Xent 1.1338(1.1112) | Loss 9.8383(10.0718) | Error 0.3867(0.3967) Steps 568(581.96) | Grad Norm 6.9618(11.1590) | Total Time 0.00(0.00)\n",
      "Iter 1860 | Time 13.4371(14.5795) | Bit/dim 3.9164(3.9031) | Xent 1.1787(1.1122) | Loss 9.8629(9.9951) | Error 0.4267(0.3965) Steps 562(582.21) | Grad Norm 21.4359(11.1210) | Total Time 0.00(0.00)\n",
      "Iter 1870 | Time 14.5815(14.6072) | Bit/dim 3.8465(3.8973) | Xent 1.0355(1.1075) | Loss 9.6879(9.9366) | Error 0.3778(0.3952) Steps 574(580.82) | Grad Norm 6.1774(11.6543) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0034 | Time 77.2990, Epoch Time 897.3370(815.7666), Bit/dim 3.8959(best: 3.8982), Xent 1.0375, Loss 4.4146, Error 0.3719(best: 0.3800)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1880 | Time 14.6514(14.6117) | Bit/dim 3.8616(3.8964) | Xent 1.1193(1.1052) | Loss 9.7091(10.3492) | Error 0.3911(0.3944) Steps 610(583.14) | Grad Norm 9.2867(11.6308) | Total Time 0.00(0.00)\n",
      "Iter 1890 | Time 14.7648(14.6588) | Bit/dim 3.8990(3.8969) | Xent 1.0873(1.0947) | Loss 9.7909(10.1890) | Error 0.3944(0.3916) Steps 592(583.10) | Grad Norm 12.7014(11.2442) | Total Time 0.00(0.00)\n",
      "Iter 1900 | Time 14.4070(14.5752) | Bit/dim 3.8896(3.8937) | Xent 1.0136(1.0876) | Loss 9.6511(10.0547) | Error 0.3700(0.3896) Steps 580(581.64) | Grad Norm 8.8634(11.0141) | Total Time 0.00(0.00)\n",
      "Iter 1910 | Time 16.0482(14.6767) | Bit/dim 3.8769(3.8897) | Xent 1.1190(1.0871) | Loss 9.9207(9.9797) | Error 0.3956(0.3888) Steps 598(581.69) | Grad Norm 8.1046(11.6301) | Total Time 0.00(0.00)\n",
      "Iter 1920 | Time 13.7454(14.5984) | Bit/dim 3.9052(3.8883) | Xent 1.0510(1.0912) | Loss 9.7650(9.9048) | Error 0.3667(0.3893) Steps 574(579.76) | Grad Norm 13.2383(12.5981) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0035 | Time 77.5477, Epoch Time 899.0820(818.2660), Bit/dim 3.8906(best: 3.8959), Xent 1.0856, Loss 4.4334, Error 0.3847(best: 0.3719)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1930 | Time 14.5965(14.6257) | Bit/dim 3.8918(3.8877) | Xent 1.0806(1.0925) | Loss 9.7508(10.4173) | Error 0.3833(0.3884) Steps 568(578.39) | Grad Norm 9.5801(13.1743) | Total Time 0.00(0.00)\n",
      "Iter 1940 | Time 15.1073(14.6288) | Bit/dim 3.8820(3.8835) | Xent 0.9930(1.0907) | Loss 9.6915(10.2344) | Error 0.3522(0.3863) Steps 544(578.52) | Grad Norm 7.0835(12.2512) | Total Time 0.00(0.00)\n",
      "Iter 1950 | Time 14.4985(14.6054) | Bit/dim 3.8547(3.8817) | Xent 1.1000(1.0833) | Loss 9.6918(10.0898) | Error 0.3778(0.3850) Steps 586(578.84) | Grad Norm 9.6287(11.7918) | Total Time 0.00(0.00)\n",
      "Iter 1960 | Time 14.9779(14.6114) | Bit/dim 3.8530(3.8771) | Xent 1.1238(1.0775) | Loss 9.8174(9.9799) | Error 0.3989(0.3833) Steps 616(578.20) | Grad Norm 16.4172(10.9204) | Total Time 0.00(0.00)\n",
      "Iter 1970 | Time 15.5641(14.6594) | Bit/dim 3.8551(3.8754) | Xent 1.1335(1.0865) | Loss 9.7721(9.9290) | Error 0.4000(0.3861) Steps 628(582.37) | Grad Norm 12.7149(11.4752) | Total Time 0.00(0.00)\n",
      "Iter 1980 | Time 15.7072(14.7177) | Bit/dim 3.8613(3.8745) | Xent 1.0710(1.0849) | Loss 9.6680(9.8657) | Error 0.3833(0.3876) Steps 592(582.03) | Grad Norm 5.6951(10.6233) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0036 | Time 77.1756, Epoch Time 903.3490(820.8185), Bit/dim 3.8616(best: 3.8906), Xent 1.0043, Loss 4.3638, Error 0.3564(best: 0.3719)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1990 | Time 14.5630(14.7066) | Bit/dim 3.8751(3.8731) | Xent 1.0600(1.0724) | Loss 9.6105(10.2713) | Error 0.3733(0.3827) Steps 598(583.00) | Grad Norm 20.5295(10.3134) | Total Time 0.00(0.00)\n",
      "Iter 2000 | Time 13.9978(14.7193) | Bit/dim 3.8775(3.8731) | Xent 1.0807(1.0701) | Loss 9.6847(10.1162) | Error 0.3889(0.3821) Steps 574(582.43) | Grad Norm 11.4407(11.3143) | Total Time 0.00(0.00)\n",
      "Iter 2010 | Time 14.6317(14.6617) | Bit/dim 3.8560(3.8703) | Xent 1.0183(1.0627) | Loss 9.6002(9.9886) | Error 0.3678(0.3789) Steps 598(581.14) | Grad Norm 8.0332(10.5743) | Total Time 0.00(0.00)\n",
      "Iter 2020 | Time 14.5997(14.6742) | Bit/dim 3.8622(3.8655) | Xent 0.9812(1.0513) | Loss 9.6023(9.8920) | Error 0.3600(0.3752) Steps 568(582.14) | Grad Norm 6.4090(9.6790) | Total Time 0.00(0.00)\n",
      "Iter 2030 | Time 14.3270(14.5793) | Bit/dim 3.8458(3.8635) | Xent 1.1180(1.0501) | Loss 9.7216(9.8212) | Error 0.3978(0.3756) Steps 592(581.35) | Grad Norm 10.1118(9.4875) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0037 | Time 77.1671, Epoch Time 897.6260(823.1227), Bit/dim 3.8625(best: 3.8616), Xent 1.0052, Loss 4.3651, Error 0.3568(best: 0.3564)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2040 | Time 14.5084(14.6287) | Bit/dim 3.8845(3.8610) | Xent 1.0506(1.0537) | Loss 9.7691(10.3154) | Error 0.3678(0.3754) Steps 568(582.03) | Grad Norm 10.2245(10.6416) | Total Time 0.00(0.00)\n",
      "Iter 2050 | Time 14.6134(14.6940) | Bit/dim 3.8615(3.8591) | Xent 1.1034(1.0521) | Loss 9.7191(10.1436) | Error 0.3700(0.3741) Steps 586(578.94) | Grad Norm 12.0044(10.6038) | Total Time 0.00(0.00)\n",
      "Iter 2060 | Time 14.9857(14.6894) | Bit/dim 3.8085(3.8560) | Xent 1.0205(1.0532) | Loss 9.4135(10.0024) | Error 0.3633(0.3746) Steps 574(578.17) | Grad Norm 13.2806(10.7561) | Total Time 0.00(0.00)\n",
      "Iter 2070 | Time 14.7476(14.7038) | Bit/dim 3.8772(3.8600) | Xent 1.0579(1.0660) | Loss 9.6769(9.9304) | Error 0.3700(0.3785) Steps 556(580.68) | Grad Norm 19.9661(13.1606) | Total Time 0.00(0.00)\n",
      "Iter 2080 | Time 13.9796(14.7086) | Bit/dim 3.8740(3.8601) | Xent 1.0950(1.0665) | Loss 9.6707(9.8625) | Error 0.3889(0.3785) Steps 562(584.40) | Grad Norm 20.9189(13.3383) | Total Time 0.00(0.00)\n",
      "Iter 2090 | Time 14.4237(14.6596) | Bit/dim 3.7944(3.8560) | Xent 1.0155(1.0616) | Loss 9.4156(9.7827) | Error 0.3600(0.3778) Steps 574(581.11) | Grad Norm 10.8374(12.9664) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0038 | Time 77.3476, Epoch Time 905.0527(825.5806), Bit/dim 3.8568(best: 3.8616), Xent 0.9781, Loss 4.3459, Error 0.3499(best: 0.3564)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2100 | Time 14.0325(14.6413) | Bit/dim 3.8353(3.8548) | Xent 1.0181(1.0508) | Loss 9.5596(10.1811) | Error 0.3600(0.3752) Steps 568(579.55) | Grad Norm 10.4207(11.8072) | Total Time 0.00(0.00)\n",
      "Iter 2110 | Time 14.9221(14.6425) | Bit/dim 3.8282(3.8528) | Xent 1.0225(1.0397) | Loss 9.5366(10.0189) | Error 0.3567(0.3710) Steps 592(581.64) | Grad Norm 12.9678(10.9366) | Total Time 0.00(0.00)\n",
      "Iter 2120 | Time 15.0310(14.6098) | Bit/dim 3.8431(3.8493) | Xent 1.0104(1.0347) | Loss 9.4940(9.9023) | Error 0.3600(0.3686) Steps 598(580.26) | Grad Norm 6.2215(11.0210) | Total Time 0.00(0.00)\n",
      "Iter 2130 | Time 15.2540(14.6484) | Bit/dim 3.8364(3.8474) | Xent 1.0184(1.0372) | Loss 9.5059(9.8233) | Error 0.3556(0.3677) Steps 598(580.52) | Grad Norm 12.2635(11.5614) | Total Time 0.00(0.00)\n",
      "Iter 2140 | Time 15.4108(14.6782) | Bit/dim 3.7999(3.8417) | Xent 1.0609(1.0375) | Loss 9.5706(9.7560) | Error 0.3889(0.3694) Steps 604(582.37) | Grad Norm 12.1241(11.8826) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0039 | Time 78.7067, Epoch Time 901.4513(827.8568), Bit/dim 3.8399(best: 3.8568), Xent 0.9941, Loss 4.3370, Error 0.3568(best: 0.3499)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2150 | Time 14.7139(14.7009) | Bit/dim 3.8313(3.8413) | Xent 1.0521(1.0386) | Loss 9.6704(10.2579) | Error 0.3667(0.3695) Steps 604(583.06) | Grad Norm 15.7857(12.2122) | Total Time 0.00(0.00)\n",
      "Iter 2160 | Time 14.7560(14.6600) | Bit/dim 3.8345(3.8417) | Xent 1.0350(1.0328) | Loss 9.5735(10.0828) | Error 0.3733(0.3686) Steps 574(581.64) | Grad Norm 15.5805(11.9824) | Total Time 0.00(0.00)\n",
      "Iter 2170 | Time 13.8442(14.6170) | Bit/dim 3.8530(3.8447) | Xent 1.0994(1.0285) | Loss 9.6689(9.9594) | Error 0.3811(0.3663) Steps 580(581.30) | Grad Norm 10.5417(11.7378) | Total Time 0.00(0.00)\n",
      "Iter 2180 | Time 15.2058(14.6927) | Bit/dim 3.8348(3.8431) | Xent 1.0057(1.0301) | Loss 9.5692(9.8575) | Error 0.3533(0.3662) Steps 580(580.22) | Grad Norm 11.1609(11.4752) | Total Time 0.00(0.00)\n",
      "Iter 2190 | Time 15.6764(14.6856) | Bit/dim 3.8117(3.8393) | Xent 1.0063(1.0295) | Loss 9.5226(9.7659) | Error 0.3544(0.3670) Steps 580(580.93) | Grad Norm 9.4568(11.0263) | Total Time 0.00(0.00)\n",
      "Iter 2200 | Time 15.3768(14.7471) | Bit/dim 3.7969(3.8331) | Xent 0.9912(1.0226) | Loss 9.5258(9.7092) | Error 0.3578(0.3657) Steps 610(580.81) | Grad Norm 9.1270(10.1443) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0040 | Time 76.2289, Epoch Time 903.7581(830.1338), Bit/dim 3.8323(best: 3.8399), Xent 0.9826, Loss 4.3236, Error 0.3447(best: 0.3499)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2210 | Time 13.8877(14.6699) | Bit/dim 3.8235(3.8299) | Xent 1.0770(1.0155) | Loss 9.6583(10.0934) | Error 0.3733(0.3620) Steps 568(578.58) | Grad Norm 11.2472(10.5206) | Total Time 0.00(0.00)\n",
      "Iter 2220 | Time 14.8584(14.6393) | Bit/dim 3.8470(3.8306) | Xent 0.9902(1.0106) | Loss 9.6910(9.9571) | Error 0.3500(0.3604) Steps 586(578.07) | Grad Norm 10.2417(10.1617) | Total Time 0.00(0.00)\n",
      "Iter 2230 | Time 14.1449(14.6237) | Bit/dim 3.7982(3.8288) | Xent 0.9481(1.0113) | Loss 9.5269(9.8491) | Error 0.3467(0.3610) Steps 580(576.84) | Grad Norm 14.3631(10.2707) | Total Time 0.00(0.00)\n",
      "Iter 2240 | Time 14.2159(14.6132) | Bit/dim 3.7776(3.8266) | Xent 0.8983(1.0078) | Loss 9.3044(9.7529) | Error 0.3044(0.3593) Steps 544(577.72) | Grad Norm 7.3292(10.3940) | Total Time 0.00(0.00)\n",
      "Iter 2250 | Time 14.8536(14.6990) | Bit/dim 3.8215(3.8236) | Xent 0.9444(1.0073) | Loss 9.5672(9.6864) | Error 0.3389(0.3598) Steps 580(578.60) | Grad Norm 12.2401(10.5257) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0041 | Time 77.4076, Epoch Time 899.0501(832.2013), Bit/dim 3.8242(best: 3.8323), Xent 0.9671, Loss 4.3077, Error 0.3381(best: 0.3447)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2260 | Time 14.9458(14.6856) | Bit/dim 3.8229(3.8246) | Xent 0.9478(1.0088) | Loss 9.5485(10.2053) | Error 0.3433(0.3595) Steps 610(581.40) | Grad Norm 18.2065(11.4919) | Total Time 0.00(0.00)\n",
      "Iter 2270 | Time 15.2531(14.6587) | Bit/dim 3.8554(3.8292) | Xent 1.0167(1.0138) | Loss 9.5345(10.0382) | Error 0.3778(0.3617) Steps 616(578.91) | Grad Norm 8.2543(11.8757) | Total Time 0.00(0.00)\n",
      "Iter 2280 | Time 15.5885(14.7111) | Bit/dim 3.8066(3.8292) | Xent 0.9763(1.0133) | Loss 9.3752(9.9130) | Error 0.3467(0.3625) Steps 592(579.16) | Grad Norm 8.6133(12.0057) | Total Time 0.00(0.00)\n",
      "Iter 2290 | Time 14.5941(14.6558) | Bit/dim 3.8109(3.8257) | Xent 0.8722(1.0045) | Loss 9.4596(9.8053) | Error 0.3344(0.3590) Steps 586(580.52) | Grad Norm 6.9025(11.4595) | Total Time 0.00(0.00)\n",
      "Iter 2300 | Time 14.2614(14.6277) | Bit/dim 3.8033(3.8225) | Xent 0.9397(1.0035) | Loss 9.4366(9.7240) | Error 0.3389(0.3574) Steps 556(577.60) | Grad Norm 7.0559(12.0525) | Total Time 0.00(0.00)\n",
      "Iter 2310 | Time 15.4093(14.6765) | Bit/dim 3.8479(3.8239) | Xent 0.9574(1.0077) | Loss 9.4667(9.6733) | Error 0.3456(0.3587) Steps 610(577.51) | Grad Norm 12.9675(13.2118) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0042 | Time 77.8283, Epoch Time 901.3706(834.2764), Bit/dim 3.8295(best: 3.8242), Xent 0.9524, Loss 4.3056, Error 0.3390(best: 0.3381)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2320 | Time 15.0557(14.7148) | Bit/dim 3.8058(3.8194) | Xent 0.8979(1.0002) | Loss 9.4105(10.0745) | Error 0.3344(0.3568) Steps 592(578.56) | Grad Norm 6.7570(12.3459) | Total Time 0.00(0.00)\n",
      "Iter 2330 | Time 15.3355(14.7092) | Bit/dim 3.8205(3.8180) | Xent 1.0643(1.0025) | Loss 9.6771(9.9299) | Error 0.4078(0.3590) Steps 598(577.75) | Grad Norm 9.5441(11.7914) | Total Time 0.00(0.00)\n",
      "Iter 2340 | Time 14.5274(14.7232) | Bit/dim 3.7879(3.8155) | Xent 1.0020(1.0008) | Loss 9.4446(9.8032) | Error 0.3678(0.3591) Steps 580(577.40) | Grad Norm 7.1178(10.7206) | Total Time 0.00(0.00)\n",
      "Iter 2350 | Time 14.1501(14.7314) | Bit/dim 3.8205(3.8144) | Xent 0.9177(0.9883) | Loss 9.3055(9.7090) | Error 0.3400(0.3553) Steps 574(577.54) | Grad Norm 5.6382(9.6703) | Total Time 0.00(0.00)\n",
      "Iter 2360 | Time 15.2438(14.7994) | Bit/dim 3.8090(3.8113) | Xent 0.9705(0.9838) | Loss 9.5598(9.6508) | Error 0.3522(0.3529) Steps 580(580.57) | Grad Norm 5.0331(10.0955) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0043 | Time 76.5704, Epoch Time 906.1859(836.4336), Bit/dim 3.8070(best: 3.8242), Xent 0.9819, Loss 4.2979, Error 0.3516(best: 0.3381)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2370 | Time 14.5911(14.7368) | Bit/dim 3.7942(3.8078) | Xent 0.9369(0.9893) | Loss 9.4496(10.1538) | Error 0.3300(0.3518) Steps 562(577.66) | Grad Norm 11.3415(11.1657) | Total Time 0.00(0.00)\n",
      "Iter 2380 | Time 14.6705(14.7636) | Bit/dim 3.8367(3.8091) | Xent 0.9393(0.9849) | Loss 9.3499(9.9771) | Error 0.3433(0.3511) Steps 550(576.43) | Grad Norm 7.6542(11.0834) | Total Time 0.00(0.00)\n",
      "Iter 2390 | Time 15.4418(14.8113) | Bit/dim 3.7933(3.8031) | Xent 1.0009(0.9797) | Loss 9.4497(9.8277) | Error 0.3411(0.3490) Steps 568(578.08) | Grad Norm 7.2862(10.1949) | Total Time 0.00(0.00)\n",
      "Iter 2400 | Time 14.8009(14.7460) | Bit/dim 3.8108(3.8025) | Xent 0.8766(0.9710) | Loss 9.1885(9.7127) | Error 0.3356(0.3460) Steps 598(579.68) | Grad Norm 9.2761(10.2033) | Total Time 0.00(0.00)\n",
      "Iter 2410 | Time 15.1776(14.7608) | Bit/dim 3.8275(3.8007) | Xent 0.9993(0.9710) | Loss 9.6428(9.6526) | Error 0.3600(0.3470) Steps 556(579.32) | Grad Norm 10.9669(9.8206) | Total Time 0.00(0.00)\n",
      "Iter 2420 | Time 14.7219(14.7749) | Bit/dim 3.7808(3.8033) | Xent 0.9939(0.9747) | Loss 9.3670(9.6013) | Error 0.3611(0.3475) Steps 592(581.08) | Grad Norm 12.1338(11.0608) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0044 | Time 77.4461, Epoch Time 908.3279(838.5905), Bit/dim 3.8012(best: 3.8070), Xent 0.9597, Loss 4.2810, Error 0.3375(best: 0.3381)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2430 | Time 15.7951(14.8071) | Bit/dim 3.8074(3.8052) | Xent 0.9116(0.9661) | Loss 9.5149(10.0233) | Error 0.3267(0.3451) Steps 598(582.42) | Grad Norm 5.2975(10.5180) | Total Time 0.00(0.00)\n",
      "Iter 2440 | Time 15.6209(14.8712) | Bit/dim 3.7812(3.7994) | Xent 0.9886(0.9643) | Loss 9.4441(9.8730) | Error 0.3489(0.3426) Steps 586(584.09) | Grad Norm 12.2182(10.8639) | Total Time 0.00(0.00)\n",
      "Iter 2450 | Time 14.9116(14.8437) | Bit/dim 3.8142(3.7992) | Xent 1.0534(0.9648) | Loss 9.5480(9.7562) | Error 0.3767(0.3435) Steps 586(583.14) | Grad Norm 6.3899(10.6505) | Total Time 0.00(0.00)\n",
      "Iter 2460 | Time 15.2674(14.8900) | Bit/dim 3.8172(3.7988) | Xent 0.9570(0.9623) | Loss 9.5132(9.6738) | Error 0.3389(0.3428) Steps 598(584.02) | Grad Norm 7.5149(10.0736) | Total Time 0.00(0.00)\n",
      "Iter 2470 | Time 15.0077(14.9313) | Bit/dim 3.8092(3.7973) | Xent 1.0327(0.9637) | Loss 9.5616(9.6254) | Error 0.3600(0.3429) Steps 598(585.84) | Grad Norm 12.7085(10.5065) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0045 | Time 78.9498, Epoch Time 919.1513(841.0073), Bit/dim 3.7947(best: 3.8012), Xent 0.9226, Loss 4.2560, Error 0.3250(best: 0.3375)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2480 | Time 15.4344(14.9246) | Bit/dim 3.7939(3.7956) | Xent 0.9115(0.9554) | Loss 9.3867(10.1168) | Error 0.3311(0.3393) Steps 556(582.96) | Grad Norm 7.0788(10.1863) | Total Time 0.00(0.00)\n",
      "Iter 2490 | Time 14.1391(14.8929) | Bit/dim 3.8247(3.7956) | Xent 0.9725(0.9490) | Loss 9.4599(9.9210) | Error 0.3333(0.3373) Steps 586(582.28) | Grad Norm 12.2999(10.1755) | Total Time 0.00(0.00)\n",
      "Iter 2500 | Time 15.1204(14.9537) | Bit/dim 3.7870(3.7962) | Xent 0.9116(0.9467) | Loss 9.4192(9.7857) | Error 0.3322(0.3362) Steps 598(583.40) | Grad Norm 7.7613(10.1773) | Total Time 0.00(0.00)\n",
      "Iter 2510 | Time 15.4884(14.9372) | Bit/dim 3.7821(3.7932) | Xent 0.9378(0.9479) | Loss 9.5062(9.6873) | Error 0.3467(0.3376) Steps 610(583.37) | Grad Norm 7.6636(10.8003) | Total Time 0.00(0.00)\n",
      "Iter 2520 | Time 14.6653(14.8813) | Bit/dim 3.7782(3.7924) | Xent 0.9232(0.9501) | Loss 9.4294(9.6199) | Error 0.3356(0.3380) Steps 598(581.70) | Grad Norm 9.8725(10.7433) | Total Time 0.00(0.00)\n",
      "Iter 2530 | Time 14.7860(14.9045) | Bit/dim 3.7801(3.7926) | Xent 0.9849(0.9512) | Loss 9.4881(9.5709) | Error 0.3489(0.3384) Steps 586(579.60) | Grad Norm 12.6980(11.7479) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0046 | Time 78.1950, Epoch Time 915.2506(843.2346), Bit/dim 3.8015(best: 3.7947), Xent 0.9259, Loss 4.2644, Error 0.3281(best: 0.3250)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2540 | Time 15.0810(14.8848) | Bit/dim 3.7759(3.7901) | Xent 0.9630(0.9476) | Loss 9.3555(9.9697) | Error 0.3278(0.3371) Steps 586(580.66) | Grad Norm 13.1608(11.4314) | Total Time 0.00(0.00)\n",
      "Iter 2550 | Time 15.1307(14.9218) | Bit/dim 3.7823(3.7880) | Xent 0.9618(0.9442) | Loss 9.4953(9.8222) | Error 0.3311(0.3348) Steps 568(581.11) | Grad Norm 7.8493(11.0371) | Total Time 0.00(0.00)\n",
      "Iter 2560 | Time 14.8325(14.9028) | Bit/dim 3.7866(3.7902) | Xent 0.9445(0.9412) | Loss 9.3528(9.7139) | Error 0.3456(0.3351) Steps 586(579.09) | Grad Norm 17.5962(11.2390) | Total Time 0.00(0.00)\n",
      "Iter 2570 | Time 14.4474(14.8017) | Bit/dim 3.7720(3.7889) | Xent 0.9660(0.9383) | Loss 9.4108(9.6205) | Error 0.3633(0.3343) Steps 592(579.22) | Grad Norm 10.0528(10.8482) | Total Time 0.00(0.00)\n",
      "Iter 2580 | Time 15.8890(14.8325) | Bit/dim 3.7786(3.7895) | Xent 0.9890(0.9407) | Loss 9.6011(9.5727) | Error 0.3433(0.3346) Steps 580(579.05) | Grad Norm 13.0256(11.2276) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0047 | Time 77.7220, Epoch Time 912.5087(845.3128), Bit/dim 3.7901(best: 3.7947), Xent 0.9489, Loss 4.2646, Error 0.3400(best: 0.3250)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2590 | Time 15.5796(14.8588) | Bit/dim 3.7607(3.7882) | Xent 0.9561(0.9455) | Loss 9.3463(10.0598) | Error 0.3533(0.3370) Steps 604(581.51) | Grad Norm 11.0641(10.9916) | Total Time 0.00(0.00)\n",
      "Iter 2600 | Time 13.8909(14.8533) | Bit/dim 3.8352(3.7856) | Xent 0.9614(0.9439) | Loss 9.5154(9.8893) | Error 0.3411(0.3370) Steps 562(582.33) | Grad Norm 13.6793(10.9279) | Total Time 0.00(0.00)\n",
      "Iter 2610 | Time 14.7145(14.8382) | Bit/dim 3.7795(3.7864) | Xent 0.9057(0.9413) | Loss 9.3698(9.7691) | Error 0.3300(0.3362) Steps 550(583.41) | Grad Norm 9.7189(10.3657) | Total Time 0.00(0.00)\n",
      "Iter 2620 | Time 16.0827(14.9196) | Bit/dim 3.7910(3.7831) | Xent 0.8888(0.9356) | Loss 9.5260(9.6586) | Error 0.3167(0.3328) Steps 598(583.30) | Grad Norm 7.3067(10.2822) | Total Time 0.00(0.00)\n",
      "Iter 2630 | Time 15.0189(14.9367) | Bit/dim 3.7879(3.7823) | Xent 0.9050(0.9383) | Loss 9.3712(9.6008) | Error 0.3311(0.3334) Steps 592(583.26) | Grad Norm 6.7293(10.2909) | Total Time 0.00(0.00)\n",
      "Iter 2640 | Time 15.4556(14.9707) | Bit/dim 3.7774(3.7782) | Xent 0.8889(0.9322) | Loss 9.3221(9.5441) | Error 0.3289(0.3324) Steps 604(586.43) | Grad Norm 5.9122(9.9225) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0048 | Time 79.5562, Epoch Time 918.9436(847.5217), Bit/dim 3.7881(best: 3.7901), Xent 0.9036, Loss 4.2399, Error 0.3187(best: 0.3250)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2650 | Time 14.3013(15.0186) | Bit/dim 3.7911(3.7775) | Xent 0.8756(0.9237) | Loss 9.3364(9.9575) | Error 0.3156(0.3297) Steps 574(587.54) | Grad Norm 6.5486(9.9675) | Total Time 0.00(0.00)\n",
      "Iter 2660 | Time 15.3678(15.0589) | Bit/dim 3.7854(3.7774) | Xent 0.9119(0.9236) | Loss 9.3215(9.8002) | Error 0.3333(0.3284) Steps 610(587.88) | Grad Norm 14.8173(10.3327) | Total Time 0.00(0.00)\n",
      "Iter 2670 | Time 14.7290(15.0067) | Bit/dim 3.7552(3.7772) | Xent 0.9710(0.9305) | Loss 9.4529(9.7067) | Error 0.3333(0.3311) Steps 586(586.32) | Grad Norm 17.3059(11.4716) | Total Time 0.00(0.00)\n",
      "Iter 2680 | Time 14.1653(14.9703) | Bit/dim 3.7754(3.7775) | Xent 0.8888(0.9308) | Loss 9.2349(9.6212) | Error 0.3400(0.3316) Steps 562(585.92) | Grad Norm 6.2998(11.5300) | Total Time 0.00(0.00)\n",
      "Iter 2690 | Time 14.9173(14.9739) | Bit/dim 3.7614(3.7752) | Xent 0.9689(0.9259) | Loss 9.4409(9.5461) | Error 0.3433(0.3308) Steps 616(584.03) | Grad Norm 8.6340(10.2816) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0049 | Time 79.3480, Epoch Time 919.4748(849.6803), Bit/dim 3.7864(best: 3.7881), Xent 0.9223, Loss 4.2476, Error 0.3310(best: 0.3187)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2700 | Time 15.2905(14.8897) | Bit/dim 3.7557(3.7720) | Xent 0.8513(0.9182) | Loss 9.1917(10.0396) | Error 0.2944(0.3268) Steps 586(582.73) | Grad Norm 7.5208(9.9849) | Total Time 0.00(0.00)\n",
      "Iter 2710 | Time 14.7726(14.9529) | Bit/dim 3.7699(3.7705) | Xent 0.9384(0.9099) | Loss 9.3388(9.8486) | Error 0.3222(0.3244) Steps 568(585.14) | Grad Norm 10.8759(9.8072) | Total Time 0.00(0.00)\n",
      "Iter 2720 | Time 15.1765(14.9322) | Bit/dim 3.7699(3.7684) | Xent 0.8160(0.9104) | Loss 9.3185(9.7165) | Error 0.3078(0.3257) Steps 598(585.99) | Grad Norm 13.5367(9.8280) | Total Time 0.00(0.00)\n",
      "Iter 2730 | Time 14.6623(14.9261) | Bit/dim 3.7734(3.7690) | Xent 0.9227(0.9085) | Loss 9.4642(9.6263) | Error 0.3244(0.3241) Steps 592(585.18) | Grad Norm 16.4267(10.5926) | Total Time 0.00(0.00)\n",
      "Iter 2740 | Time 15.3216(14.9647) | Bit/dim 3.7657(3.7716) | Xent 0.9207(0.9175) | Loss 9.3647(9.5700) | Error 0.3456(0.3277) Steps 598(585.84) | Grad Norm 7.9098(10.8979) | Total Time 0.00(0.00)\n",
      "Iter 2750 | Time 14.6900(14.9668) | Bit/dim 3.7417(3.7715) | Xent 0.9726(0.9236) | Loss 9.2952(9.5225) | Error 0.3333(0.3293) Steps 586(584.94) | Grad Norm 11.9196(10.9526) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0050 | Time 78.0802, Epoch Time 919.6861(851.7805), Bit/dim 3.7728(best: 3.7864), Xent 0.9534, Loss 4.2495, Error 0.3367(best: 0.3187)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2760 | Time 15.8952(14.9632) | Bit/dim 3.7677(3.7711) | Xent 0.9239(0.9238) | Loss 9.5603(9.9542) | Error 0.3222(0.3287) Steps 616(586.51) | Grad Norm 14.5581(11.2720) | Total Time 0.00(0.00)\n",
      "Iter 2770 | Time 15.6563(15.0347) | Bit/dim 3.7845(3.7696) | Xent 0.8518(0.9161) | Loss 9.4791(9.7977) | Error 0.3033(0.3253) Steps 616(588.29) | Grad Norm 11.0276(10.8873) | Total Time 0.00(0.00)\n",
      "Iter 2780 | Time 14.8910(15.0222) | Bit/dim 3.7530(3.7681) | Xent 0.9061(0.9114) | Loss 9.2439(9.6793) | Error 0.3289(0.3245) Steps 598(590.36) | Grad Norm 14.1162(10.5899) | Total Time 0.00(0.00)\n",
      "Iter 2790 | Time 15.1860(15.0427) | Bit/dim 3.7678(3.7693) | Xent 0.9153(0.9135) | Loss 9.4646(9.6072) | Error 0.3267(0.3255) Steps 610(591.49) | Grad Norm 13.8607(10.8798) | Total Time 0.00(0.00)\n",
      "Iter 2800 | Time 16.5656(15.0111) | Bit/dim 3.7947(3.7685) | Xent 0.9924(0.9208) | Loss 9.4199(9.5389) | Error 0.3656(0.3283) Steps 616(590.08) | Grad Norm 13.1900(11.0899) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0051 | Time 78.6945, Epoch Time 923.9425(853.9454), Bit/dim 3.7694(best: 3.7728), Xent 0.9060, Loss 4.2224, Error 0.3187(best: 0.3187)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2810 | Time 14.9791(15.1131) | Bit/dim 3.7541(3.7684) | Xent 0.8676(0.9182) | Loss 9.2430(10.0309) | Error 0.3022(0.3269) Steps 580(593.10) | Grad Norm 7.8077(11.0741) | Total Time 0.00(0.00)\n",
      "Iter 2820 | Time 14.8356(15.0906) | Bit/dim 3.7967(3.7683) | Xent 0.9131(0.9160) | Loss 9.5000(9.8546) | Error 0.3256(0.3268) Steps 610(592.25) | Grad Norm 13.4121(11.1769) | Total Time 0.00(0.00)\n",
      "Iter 2830 | Time 15.0468(15.1203) | Bit/dim 3.7352(3.7680) | Xent 0.8963(0.9146) | Loss 9.2169(9.7227) | Error 0.3189(0.3263) Steps 604(592.60) | Grad Norm 6.3994(11.0031) | Total Time 0.00(0.00)\n",
      "Iter 2840 | Time 15.5177(15.1294) | Bit/dim 3.7730(3.7650) | Xent 0.8703(0.9053) | Loss 9.4157(9.6178) | Error 0.3278(0.3239) Steps 592(593.66) | Grad Norm 13.9553(11.2851) | Total Time 0.00(0.00)\n",
      "Iter 2850 | Time 15.9527(15.1103) | Bit/dim 3.7326(3.7613) | Xent 0.8359(0.8984) | Loss 9.3092(9.5538) | Error 0.2989(0.3192) Steps 592(592.71) | Grad Norm 7.7805(11.0126) | Total Time 0.00(0.00)\n",
      "Iter 2860 | Time 15.3933(15.0354) | Bit/dim 3.7922(3.7658) | Xent 0.9306(0.9016) | Loss 9.4267(9.5159) | Error 0.3111(0.3206) Steps 598(593.55) | Grad Norm 9.5856(11.1825) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0052 | Time 80.3754, Epoch Time 928.6688(856.1871), Bit/dim 3.7608(best: 3.7694), Xent 0.9185, Loss 4.2201, Error 0.3207(best: 0.3187)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2870 | Time 15.2494(15.0109) | Bit/dim 3.7474(3.7658) | Xent 0.9662(0.9012) | Loss 9.4380(9.9617) | Error 0.3322(0.3207) Steps 598(591.51) | Grad Norm 16.4012(11.7445) | Total Time 0.00(0.00)\n",
      "Iter 2880 | Time 14.6624(14.9637) | Bit/dim 3.7558(3.7637) | Xent 0.8831(0.8932) | Loss 9.3629(9.7989) | Error 0.3033(0.3174) Steps 616(591.88) | Grad Norm 5.6211(10.7082) | Total Time 0.00(0.00)\n",
      "Iter 2890 | Time 15.0743(14.9199) | Bit/dim 3.7605(3.7619) | Xent 0.9217(0.8904) | Loss 9.4134(9.6710) | Error 0.3189(0.3145) Steps 592(592.80) | Grad Norm 6.9302(10.1296) | Total Time 0.00(0.00)\n",
      "Iter 2900 | Time 15.6222(15.0184) | Bit/dim 3.7625(3.7584) | Xent 0.8531(0.8813) | Loss 9.4401(9.5778) | Error 0.3167(0.3139) Steps 604(594.45) | Grad Norm 8.6399(9.8386) | Total Time 0.00(0.00)\n",
      "Iter 2910 | Time 14.8553(15.0403) | Bit/dim 3.7860(3.7577) | Xent 0.8341(0.8802) | Loss 9.4397(9.5152) | Error 0.3022(0.3143) Steps 616(596.88) | Grad Norm 9.1771(10.1063) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0053 | Time 79.5929, Epoch Time 922.2535(858.1691), Bit/dim 3.7576(best: 3.7608), Xent 0.8704, Loss 4.1928, Error 0.3073(best: 0.3187)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2920 | Time 16.5356(15.0594) | Bit/dim 3.7421(3.7566) | Xent 0.8442(0.8798) | Loss 9.2458(10.0297) | Error 0.2900(0.3133) Steps 652(597.13) | Grad Norm 6.5977(10.1105) | Total Time 0.00(0.00)\n",
      "Iter 2930 | Time 15.2587(15.0842) | Bit/dim 3.7728(3.7613) | Xent 0.9821(0.8840) | Loss 9.6151(9.8710) | Error 0.3456(0.3147) Steps 604(598.99) | Grad Norm 10.5215(10.5362) | Total Time 0.00(0.00)\n",
      "Iter 2940 | Time 14.9270(15.0593) | Bit/dim 3.7553(3.7595) | Xent 0.8597(0.8823) | Loss 9.3321(9.7318) | Error 0.2933(0.3138) Steps 586(601.18) | Grad Norm 10.9370(10.5644) | Total Time 0.00(0.00)\n",
      "Iter 2950 | Time 14.3034(15.0749) | Bit/dim 3.7192(3.7565) | Xent 0.9009(0.8822) | Loss 9.2077(9.6249) | Error 0.3156(0.3142) Steps 586(601.77) | Grad Norm 8.0437(9.9400) | Total Time 0.00(0.00)\n",
      "Iter 2960 | Time 16.3285(15.0733) | Bit/dim 3.7677(3.7530) | Xent 0.8797(0.8778) | Loss 9.4221(9.5454) | Error 0.3222(0.3119) Steps 604(600.47) | Grad Norm 12.7306(9.7905) | Total Time 0.00(0.00)\n",
      "Iter 2970 | Time 15.3804(15.1143) | Bit/dim 3.7191(3.7500) | Xent 0.8331(0.8700) | Loss 9.2072(9.4723) | Error 0.2833(0.3090) Steps 622(601.28) | Grad Norm 8.5817(9.9412) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0054 | Time 81.1142, Epoch Time 930.6627(860.3439), Bit/dim 3.7563(best: 3.7576), Xent 0.8420, Loss 4.1773, Error 0.2997(best: 0.3073)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2980 | Time 15.7624(15.2278) | Bit/dim 3.7682(3.7495) | Xent 0.8462(0.8624) | Loss 9.3642(9.9244) | Error 0.3000(0.3064) Steps 604(601.54) | Grad Norm 6.8935(9.2009) | Total Time 0.00(0.00)\n",
      "Iter 2990 | Time 15.7230(15.2366) | Bit/dim 3.7582(3.7476) | Xent 0.8352(0.8571) | Loss 9.2244(9.7484) | Error 0.2944(0.3058) Steps 616(601.50) | Grad Norm 7.4094(8.7715) | Total Time 0.00(0.00)\n",
      "Iter 3000 | Time 15.2894(15.2289) | Bit/dim 3.7141(3.7467) | Xent 0.8287(0.8503) | Loss 9.3223(9.6274) | Error 0.2811(0.3031) Steps 610(601.21) | Grad Norm 5.1947(8.5533) | Total Time 0.00(0.00)\n",
      "Iter 3010 | Time 14.8024(15.2655) | Bit/dim 3.7255(3.7440) | Xent 0.8226(0.8531) | Loss 9.1243(9.5373) | Error 0.2789(0.3029) Steps 598(604.13) | Grad Norm 5.5049(8.7897) | Total Time 0.00(0.00)\n",
      "Iter 3020 | Time 15.6794(15.2678) | Bit/dim 3.7507(3.7420) | Xent 0.8610(0.8608) | Loss 9.3423(9.4732) | Error 0.3056(0.3066) Steps 592(603.49) | Grad Norm 6.6461(8.8815) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0055 | Time 80.0165, Epoch Time 941.3585(862.7743), Bit/dim 3.7468(best: 3.7563), Xent 0.8532, Loss 4.1734, Error 0.3037(best: 0.2997)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3030 | Time 16.0926(15.2858) | Bit/dim 3.7594(3.7446) | Xent 0.8190(0.8596) | Loss 9.2599(9.9937) | Error 0.3044(0.3060) Steps 586(602.99) | Grad Norm 12.8180(9.0498) | Total Time 0.00(0.00)\n",
      "Iter 3040 | Time 15.5546(15.3017) | Bit/dim 3.7232(3.7444) | Xent 0.8653(0.8618) | Loss 9.3222(9.8248) | Error 0.3067(0.3056) Steps 592(604.31) | Grad Norm 14.1431(9.8329) | Total Time 0.00(0.00)\n",
      "Iter 3050 | Time 15.3096(15.2805) | Bit/dim 3.7481(3.7438) | Xent 0.8193(0.8564) | Loss 9.1767(9.6860) | Error 0.2878(0.3038) Steps 616(604.45) | Grad Norm 8.0838(9.5602) | Total Time 0.00(0.00)\n",
      "Iter 3060 | Time 14.8074(15.2375) | Bit/dim 3.7290(3.7447) | Xent 0.8041(0.8550) | Loss 9.1640(9.5892) | Error 0.2822(0.3036) Steps 610(604.08) | Grad Norm 9.1202(9.6486) | Total Time 0.00(0.00)\n",
      "Iter 3070 | Time 14.9924(15.1578) | Bit/dim 3.7603(3.7432) | Xent 0.8722(0.8546) | Loss 9.3841(9.5046) | Error 0.2911(0.3038) Steps 604(604.43) | Grad Norm 7.8502(9.4508) | Total Time 0.00(0.00)\n",
      "Iter 3080 | Time 16.0669(15.2400) | Bit/dim 3.7372(3.7428) | Xent 0.8209(0.8569) | Loss 9.2830(9.4533) | Error 0.3100(0.3057) Steps 634(606.53) | Grad Norm 9.7163(9.8712) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0056 | Time 80.3835, Epoch Time 934.8641(864.9370), Bit/dim 3.7482(best: 3.7468), Xent 0.8764, Loss 4.1864, Error 0.3115(best: 0.2997)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3090 | Time 16.0334(15.2659) | Bit/dim 3.7591(3.7434) | Xent 0.9079(0.8519) | Loss 9.4206(9.8747) | Error 0.3322(0.3040) Steps 640(608.42) | Grad Norm 16.0747(9.9292) | Total Time 0.00(0.00)\n",
      "Iter 3100 | Time 16.0618(15.2657) | Bit/dim 3.7515(3.7445) | Xent 0.8360(0.8439) | Loss 9.2789(9.7169) | Error 0.2956(0.3008) Steps 568(605.84) | Grad Norm 10.4975(9.1831) | Total Time 0.00(0.00)\n",
      "Iter 3110 | Time 15.0981(15.2714) | Bit/dim 3.7371(3.7434) | Xent 0.8763(0.8416) | Loss 9.3578(9.6068) | Error 0.3267(0.3005) Steps 616(605.96) | Grad Norm 10.0217(9.4066) | Total Time 0.00(0.00)\n",
      "Iter 3120 | Time 15.5026(15.3105) | Bit/dim 3.7236(3.7425) | Xent 0.9313(0.8534) | Loss 9.3301(9.5253) | Error 0.3333(0.3044) Steps 610(607.43) | Grad Norm 9.7669(10.0191) | Total Time 0.00(0.00)\n",
      "Iter 3130 | Time 15.8079(15.3717) | Bit/dim 3.7133(3.7410) | Xent 0.8923(0.8615) | Loss 9.3057(9.4815) | Error 0.3111(0.3074) Steps 652(611.58) | Grad Norm 13.7638(10.8508) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0057 | Time 81.1307, Epoch Time 944.9915(867.3386), Bit/dim 3.7529(best: 3.7468), Xent 0.8688, Loss 4.1873, Error 0.3082(best: 0.2997)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3140 | Time 15.6252(15.3923) | Bit/dim 3.7302(3.7419) | Xent 0.8046(0.8611) | Loss 9.1777(10.0078) | Error 0.2967(0.3083) Steps 628(612.23) | Grad Norm 9.0357(10.8443) | Total Time 0.00(0.00)\n",
      "Iter 3150 | Time 14.1699(15.3226) | Bit/dim 3.7517(3.7454) | Xent 0.8178(0.8534) | Loss 9.1641(9.8067) | Error 0.2967(0.3056) Steps 586(611.86) | Grad Norm 10.6186(10.7025) | Total Time 0.00(0.00)\n",
      "Iter 3160 | Time 15.0140(15.2617) | Bit/dim 3.7269(3.7405) | Xent 0.8018(0.8474) | Loss 9.2580(9.6673) | Error 0.2956(0.3044) Steps 610(609.75) | Grad Norm 8.0713(10.1424) | Total Time 0.00(0.00)\n",
      "Iter 3170 | Time 15.6316(15.3710) | Bit/dim 3.7154(3.7400) | Xent 0.8991(0.8445) | Loss 9.2467(9.5683) | Error 0.3067(0.3021) Steps 592(613.68) | Grad Norm 13.1398(9.7740) | Total Time 0.00(0.00)\n",
      "Iter 3180 | Time 16.1685(15.4007) | Bit/dim 3.7601(3.7405) | Xent 0.8275(0.8401) | Loss 9.3772(9.4965) | Error 0.2856(0.2999) Steps 610(613.52) | Grad Norm 8.9644(9.6923) | Total Time 0.00(0.00)\n",
      "Iter 3190 | Time 15.0442(15.4760) | Bit/dim 3.7420(3.7412) | Xent 0.8428(0.8449) | Loss 9.2868(9.4546) | Error 0.3144(0.3015) Steps 610(615.49) | Grad Norm 7.4276(10.0558) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0058 | Time 82.7156, Epoch Time 948.1343(869.7625), Bit/dim 3.7313(best: 3.7468), Xent 0.8241, Loss 4.1434, Error 0.2922(best: 0.2997)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3200 | Time 15.4600(15.4782) | Bit/dim 3.7375(3.7395) | Xent 0.8019(0.8324) | Loss 9.2297(9.8983) | Error 0.2878(0.2974) Steps 598(614.37) | Grad Norm 9.1981(9.6368) | Total Time 0.00(0.00)\n",
      "Iter 3210 | Time 15.6219(15.4861) | Bit/dim 3.7143(3.7383) | Xent 0.7686(0.8304) | Loss 9.2408(9.7259) | Error 0.2678(0.2962) Steps 628(616.15) | Grad Norm 10.2874(9.8578) | Total Time 0.00(0.00)\n",
      "Iter 3220 | Time 14.9800(15.5017) | Bit/dim 3.7007(3.7352) | Xent 0.8579(0.8246) | Loss 9.2436(9.5867) | Error 0.3111(0.2945) Steps 610(617.45) | Grad Norm 9.4509(9.7040) | Total Time 0.00(0.00)\n",
      "Iter 3230 | Time 16.0945(15.5120) | Bit/dim 3.7060(3.7348) | Xent 0.8437(0.8230) | Loss 9.3110(9.4998) | Error 0.2844(0.2931) Steps 646(619.29) | Grad Norm 14.0056(9.8581) | Total Time 0.00(0.00)\n",
      "Iter 3240 | Time 15.5466(15.5255) | Bit/dim 3.7572(3.7336) | Xent 0.8651(0.8269) | Loss 9.4322(9.4399) | Error 0.3144(0.2954) Steps 628(619.79) | Grad Norm 12.8891(10.6194) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0059 | Time 84.0083, Epoch Time 955.9606(872.3485), Bit/dim 3.7296(best: 3.7313), Xent 0.8322, Loss 4.1457, Error 0.2976(best: 0.2922)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3250 | Time 15.5748(15.5668) | Bit/dim 3.7574(3.7343) | Xent 0.7504(0.8258) | Loss 9.2861(9.9829) | Error 0.2822(0.2965) Steps 628(622.54) | Grad Norm 7.1925(10.0639) | Total Time 0.00(0.00)\n",
      "Iter 3260 | Time 15.7437(15.6335) | Bit/dim 3.7201(3.7356) | Xent 0.9502(0.8279) | Loss 9.3442(9.8072) | Error 0.3356(0.2965) Steps 628(625.88) | Grad Norm 12.2567(10.2498) | Total Time 0.00(0.00)\n",
      "Iter 3270 | Time 15.9462(15.7408) | Bit/dim 3.7223(3.7330) | Xent 0.7183(0.8272) | Loss 9.2031(9.6604) | Error 0.2544(0.2958) Steps 598(628.30) | Grad Norm 5.8931(10.1962) | Total Time 0.00(0.00)\n",
      "Iter 3280 | Time 15.7383(15.7397) | Bit/dim 3.7445(3.7333) | Xent 0.7288(0.8222) | Loss 9.1434(9.5544) | Error 0.2622(0.2936) Steps 634(629.09) | Grad Norm 9.9680(9.9979) | Total Time 0.00(0.00)\n",
      "Iter 3290 | Time 16.2777(15.7311) | Bit/dim 3.7252(3.7353) | Xent 0.7651(0.8197) | Loss 9.2647(9.4734) | Error 0.2744(0.2925) Steps 622(627.69) | Grad Norm 5.6279(9.9271) | Total Time 0.00(0.00)\n",
      "Iter 3300 | Time 15.8634(15.7333) | Bit/dim 3.7090(3.7314) | Xent 0.8619(0.8235) | Loss 9.2421(9.4194) | Error 0.3100(0.2942) Steps 616(624.84) | Grad Norm 16.0236(9.5686) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0060 | Time 83.3349, Epoch Time 970.4511(875.2915), Bit/dim 3.7298(best: 3.7296), Xent 0.8331, Loss 4.1463, Error 0.2953(best: 0.2922)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3310 | Time 15.7378(15.7237) | Bit/dim 3.7194(3.7325) | Xent 0.8046(0.8210) | Loss 9.2267(9.8989) | Error 0.2967(0.2943) Steps 616(626.15) | Grad Norm 6.2179(9.5973) | Total Time 0.00(0.00)\n",
      "Iter 3320 | Time 16.3325(15.8702) | Bit/dim 3.7337(3.7315) | Xent 0.8128(0.8106) | Loss 9.2508(9.7190) | Error 0.2933(0.2907) Steps 646(628.34) | Grad Norm 14.8596(9.4591) | Total Time 0.00(0.00)\n",
      "Iter 3330 | Time 15.4150(15.8483) | Bit/dim 3.7078(3.7296) | Xent 0.8538(0.8154) | Loss 9.1762(9.6029) | Error 0.3111(0.2928) Steps 622(626.45) | Grad Norm 12.3937(10.0556) | Total Time 0.00(0.00)\n",
      "Iter 3340 | Time 15.1787(15.8164) | Bit/dim 3.7634(3.7265) | Xent 0.7535(0.8154) | Loss 9.2008(9.5042) | Error 0.2711(0.2937) Steps 628(627.99) | Grad Norm 6.7787(10.1688) | Total Time 0.00(0.00)\n",
      "Iter 3350 | Time 15.0733(15.7868) | Bit/dim 3.7243(3.7275) | Xent 0.8176(0.8215) | Loss 9.2300(9.4423) | Error 0.2711(0.2945) Steps 634(626.98) | Grad Norm 14.2929(10.1634) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0061 | Time 82.2467, Epoch Time 971.1462(878.1672), Bit/dim 3.7300(best: 3.7296), Xent 0.8367, Loss 4.1484, Error 0.2965(best: 0.2922)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3360 | Time 15.4950(15.8163) | Bit/dim 3.7152(3.7308) | Xent 0.7455(0.8201) | Loss 9.0997(9.9901) | Error 0.2733(0.2931) Steps 610(626.43) | Grad Norm 9.6010(10.2928) | Total Time 0.00(0.00)\n",
      "Iter 3370 | Time 15.4466(15.7526) | Bit/dim 3.7361(3.7297) | Xent 0.8809(0.8349) | Loss 9.2834(9.7991) | Error 0.3100(0.2975) Steps 610(624.66) | Grad Norm 9.8994(11.0000) | Total Time 0.00(0.00)\n",
      "Iter 3380 | Time 15.9356(15.8025) | Bit/dim 3.7525(3.7289) | Xent 0.8087(0.8353) | Loss 9.2443(9.6588) | Error 0.2889(0.2974) Steps 610(626.40) | Grad Norm 10.1450(10.8431) | Total Time 0.00(0.00)\n",
      "Iter 3390 | Time 16.4054(15.8501) | Bit/dim 3.7072(3.7296) | Xent 0.8549(0.8335) | Loss 9.2663(9.5568) | Error 0.3011(0.2968) Steps 628(627.32) | Grad Norm 12.2496(10.0626) | Total Time 0.00(0.00)\n",
      "Iter 3400 | Time 16.3594(15.7975) | Bit/dim 3.7283(3.7267) | Xent 0.8268(0.8255) | Loss 9.3723(9.4726) | Error 0.3056(0.2945) Steps 628(627.28) | Grad Norm 7.1987(9.4792) | Total Time 0.00(0.00)\n",
      "Iter 3410 | Time 16.2499(15.7628) | Bit/dim 3.6720(3.7253) | Xent 0.8774(0.8254) | Loss 9.1881(9.4154) | Error 0.3078(0.2939) Steps 676(627.35) | Grad Norm 9.1111(9.1451) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0062 | Time 81.9732, Epoch Time 967.5216(880.8478), Bit/dim 3.7227(best: 3.7296), Xent 0.8171, Loss 4.1313, Error 0.2903(best: 0.2922)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3420 | Time 15.5430(15.7450) | Bit/dim 3.7224(3.7240) | Xent 0.8229(0.8190) | Loss 9.0341(9.8508) | Error 0.2733(0.2916) Steps 580(625.73) | Grad Norm 7.6628(9.0283) | Total Time 0.00(0.00)\n",
      "Iter 3430 | Time 15.6494(15.7492) | Bit/dim 3.7124(3.7234) | Xent 0.7818(0.8032) | Loss 9.1969(9.6669) | Error 0.2922(0.2872) Steps 622(625.46) | Grad Norm 9.1380(8.5782) | Total Time 0.00(0.00)\n",
      "Iter 3440 | Time 15.6492(15.8590) | Bit/dim 3.6733(3.7224) | Xent 0.8244(0.8049) | Loss 9.1359(9.5564) | Error 0.2822(0.2855) Steps 634(627.93) | Grad Norm 8.8598(9.1746) | Total Time 0.00(0.00)\n",
      "Iter 3450 | Time 15.2547(15.8280) | Bit/dim 3.7260(3.7211) | Xent 0.8094(0.8056) | Loss 9.2653(9.4645) | Error 0.2878(0.2866) Steps 634(629.07) | Grad Norm 8.9838(9.1548) | Total Time 0.00(0.00)\n",
      "Iter 3460 | Time 15.3091(15.8739) | Bit/dim 3.7271(3.7196) | Xent 0.8098(0.8011) | Loss 9.2317(9.3915) | Error 0.2811(0.2841) Steps 628(629.82) | Grad Norm 9.8800(9.2140) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0063 | Time 82.9619, Epoch Time 973.6307(883.6313), Bit/dim 3.7265(best: 3.7227), Xent 0.8561, Loss 4.1545, Error 0.3065(best: 0.2903)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3470 | Time 15.7580(15.8879) | Bit/dim 3.7265(3.7196) | Xent 0.8365(0.8088) | Loss 9.2045(9.9648) | Error 0.2978(0.2885) Steps 604(630.73) | Grad Norm 16.3674(9.4221) | Total Time 0.00(0.00)\n",
      "Iter 3480 | Time 15.3916(15.8548) | Bit/dim 3.6724(3.7191) | Xent 0.7854(0.8064) | Loss 9.1407(9.7724) | Error 0.2767(0.2874) Steps 622(631.72) | Grad Norm 7.6642(9.2361) | Total Time 0.00(0.00)\n",
      "Iter 3490 | Time 15.3420(15.8548) | Bit/dim 3.7461(3.7209) | Xent 0.7123(0.7991) | Loss 9.1783(9.6145) | Error 0.2322(0.2843) Steps 616(631.93) | Grad Norm 8.7011(8.9482) | Total Time 0.00(0.00)\n",
      "Iter 3500 | Time 15.6955(15.8102) | Bit/dim 3.6998(3.7187) | Xent 0.8207(0.7892) | Loss 9.1454(9.4867) | Error 0.2922(0.2807) Steps 610(627.93) | Grad Norm 9.1900(8.7601) | Total Time 0.00(0.00)\n",
      "Iter 3510 | Time 15.5311(15.8314) | Bit/dim 3.7023(3.7163) | Xent 0.7867(0.7931) | Loss 9.3115(9.4279) | Error 0.2844(0.2809) Steps 640(629.72) | Grad Norm 10.4089(9.6007) | Total Time 0.00(0.00)\n",
      "Iter 3520 | Time 15.7877(15.7894) | Bit/dim 3.7176(3.7160) | Xent 0.8151(0.7954) | Loss 9.1549(9.3646) | Error 0.2978(0.2830) Steps 616(629.42) | Grad Norm 7.1549(9.6324) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0064 | Time 83.5159, Epoch Time 970.4404(886.2356), Bit/dim 3.7199(best: 3.7227), Xent 0.8034, Loss 4.1216, Error 0.2845(best: 0.2903)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3530 | Time 15.3710(15.8597) | Bit/dim 3.7062(3.7143) | Xent 0.7041(0.7877) | Loss 9.1422(9.8271) | Error 0.2578(0.2806) Steps 634(632.42) | Grad Norm 7.0258(9.1638) | Total Time 0.00(0.00)\n",
      "Iter 3540 | Time 16.5971(16.0087) | Bit/dim 3.7298(3.7154) | Xent 0.8439(0.7897) | Loss 9.2535(9.6778) | Error 0.2900(0.2805) Steps 640(633.97) | Grad Norm 9.2404(9.4767) | Total Time 0.00(0.00)\n",
      "Iter 3550 | Time 16.2845(16.0180) | Bit/dim 3.7139(3.7149) | Xent 0.8231(0.7939) | Loss 9.2235(9.5600) | Error 0.2900(0.2827) Steps 616(634.18) | Grad Norm 12.8057(10.1334) | Total Time 0.00(0.00)\n",
      "Iter 3560 | Time 16.3792(16.0769) | Bit/dim 3.6785(3.7143) | Xent 0.7997(0.7923) | Loss 9.1595(9.4659) | Error 0.2811(0.2811) Steps 628(635.70) | Grad Norm 8.6855(9.8522) | Total Time 0.00(0.00)\n",
      "Iter 3570 | Time 15.6387(16.0164) | Bit/dim 3.7210(3.7155) | Xent 0.7905(0.7889) | Loss 9.3003(9.4073) | Error 0.2878(0.2818) Steps 616(636.13) | Grad Norm 8.0743(9.3326) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0065 | Time 83.9536, Epoch Time 986.1730(889.2337), Bit/dim 3.7147(best: 3.7199), Xent 0.7953, Loss 4.1123, Error 0.2837(best: 0.2845)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3580 | Time 16.5430(16.0574) | Bit/dim 3.7468(3.7132) | Xent 0.7849(0.7848) | Loss 9.2874(9.9450) | Error 0.2789(0.2793) Steps 634(636.01) | Grad Norm 11.2142(9.1048) | Total Time 0.00(0.00)\n",
      "Iter 3590 | Time 16.6292(16.0160) | Bit/dim 3.6867(3.7137) | Xent 0.8205(0.7778) | Loss 9.2173(9.7408) | Error 0.2767(0.2758) Steps 628(635.95) | Grad Norm 16.9384(9.3583) | Total Time 0.00(0.00)\n",
      "Iter 3600 | Time 16.5177(16.0087) | Bit/dim 3.6944(3.7135) | Xent 0.8270(0.7804) | Loss 9.2774(9.5922) | Error 0.2978(0.2775) Steps 634(636.29) | Grad Norm 9.8839(9.5342) | Total Time 0.00(0.00)\n",
      "Iter 3610 | Time 16.5665(16.0072) | Bit/dim 3.6774(3.7115) | Xent 0.7509(0.7787) | Loss 9.0677(9.4801) | Error 0.2767(0.2790) Steps 646(634.94) | Grad Norm 6.6279(8.8211) | Total Time 0.00(0.00)\n",
      "Iter 3620 | Time 15.4401(16.0339) | Bit/dim 3.6907(3.7109) | Xent 0.8100(0.7812) | Loss 9.1360(9.4095) | Error 0.2700(0.2782) Steps 628(636.91) | Grad Norm 8.0603(8.8147) | Total Time 0.00(0.00)\n",
      "Iter 3630 | Time 16.6595(16.1081) | Bit/dim 3.6920(3.7085) | Xent 0.7795(0.7780) | Loss 9.0769(9.3357) | Error 0.2756(0.2767) Steps 628(634.60) | Grad Norm 13.7448(8.7093) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0066 | Time 82.5240, Epoch Time 986.6683(892.1567), Bit/dim 3.7164(best: 3.7147), Xent 0.7671, Loss 4.1000, Error 0.2708(best: 0.2837)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3640 | Time 16.3479(16.0714) | Bit/dim 3.6839(3.7087) | Xent 0.7447(0.7738) | Loss 9.2315(9.7989) | Error 0.2556(0.2749) Steps 658(635.50) | Grad Norm 8.0792(9.2934) | Total Time 0.00(0.00)\n",
      "Iter 3650 | Time 16.6744(16.0921) | Bit/dim 3.7548(3.7074) | Xent 0.7618(0.7696) | Loss 9.2477(9.6290) | Error 0.2856(0.2735) Steps 652(636.38) | Grad Norm 12.0336(9.0975) | Total Time 0.00(0.00)\n",
      "Iter 3660 | Time 16.8609(16.1130) | Bit/dim 3.7026(3.7086) | Xent 0.6925(0.7691) | Loss 9.1971(9.5139) | Error 0.2456(0.2743) Steps 670(636.56) | Grad Norm 7.0602(9.0765) | Total Time 0.00(0.00)\n",
      "Iter 3670 | Time 16.1299(16.1985) | Bit/dim 3.6875(3.7065) | Xent 0.7550(0.7732) | Loss 9.1245(9.4269) | Error 0.2622(0.2764) Steps 622(637.07) | Grad Norm 6.1482(9.5125) | Total Time 0.00(0.00)\n",
      "Iter 3680 | Time 16.5339(16.1653) | Bit/dim 3.7129(3.7101) | Xent 0.7561(0.7676) | Loss 9.2604(9.3609) | Error 0.2567(0.2726) Steps 634(638.05) | Grad Norm 9.0532(9.0477) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0067 | Time 84.3434, Epoch Time 991.9240(895.1497), Bit/dim 3.7040(best: 3.7147), Xent 0.7865, Loss 4.0972, Error 0.2759(best: 0.2708)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3690 | Time 16.2406(16.2139) | Bit/dim 3.7292(3.7095) | Xent 0.7901(0.7641) | Loss 9.3302(9.8959) | Error 0.2944(0.2716) Steps 658(637.50) | Grad Norm 10.2931(9.2805) | Total Time 0.00(0.00)\n",
      "Iter 3700 | Time 16.2161(16.1867) | Bit/dim 3.6854(3.7077) | Xent 0.7022(0.7627) | Loss 9.0654(9.6898) | Error 0.2611(0.2719) Steps 628(635.59) | Grad Norm 6.1895(9.5495) | Total Time 0.00(0.00)\n",
      "Iter 3710 | Time 15.7636(16.1980) | Bit/dim 3.7250(3.7083) | Xent 0.8836(0.7665) | Loss 9.2897(9.5532) | Error 0.2989(0.2738) Steps 592(635.61) | Grad Norm 12.8836(9.8000) | Total Time 0.00(0.00)\n",
      "Iter 3720 | Time 15.3899(16.2177) | Bit/dim 3.6963(3.7117) | Xent 0.7639(0.7712) | Loss 9.0886(9.4476) | Error 0.2500(0.2756) Steps 610(636.56) | Grad Norm 8.0232(9.8170) | Total Time 0.00(0.00)\n",
      "Iter 3730 | Time 16.9024(16.1943) | Bit/dim 3.7243(3.7097) | Xent 0.7502(0.7732) | Loss 9.2490(9.3701) | Error 0.2700(0.2749) Steps 664(640.13) | Grad Norm 9.7175(9.6901) | Total Time 0.00(0.00)\n",
      "Iter 3740 | Time 16.0314(16.2508) | Bit/dim 3.7049(3.7065) | Xent 0.7420(0.7692) | Loss 9.2534(9.3149) | Error 0.2744(0.2735) Steps 664(643.09) | Grad Norm 12.5270(9.3277) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0068 | Time 84.2104, Epoch Time 995.1548(898.1499), Bit/dim 3.7200(best: 3.7040), Xent 0.7623, Loss 4.1012, Error 0.2678(best: 0.2708)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3750 | Time 16.6442(16.3015) | Bit/dim 3.6829(3.7065) | Xent 0.7748(0.7600) | Loss 9.1383(9.7646) | Error 0.2556(0.2697) Steps 628(641.23) | Grad Norm 10.8030(9.5438) | Total Time 0.00(0.00)\n",
      "Iter 3760 | Time 17.1569(16.3457) | Bit/dim 3.7220(3.7065) | Xent 0.8103(0.7580) | Loss 9.1068(9.6021) | Error 0.2967(0.2694) Steps 664(644.02) | Grad Norm 9.0952(9.1711) | Total Time 0.00(0.00)\n",
      "Iter 3770 | Time 16.8831(16.2983) | Bit/dim 3.7045(3.7033) | Xent 0.8039(0.7609) | Loss 9.2452(9.4798) | Error 0.2722(0.2688) Steps 634(645.15) | Grad Norm 15.5279(9.4481) | Total Time 0.00(0.00)\n",
      "Iter 3780 | Time 17.2264(16.3803) | Bit/dim 3.7137(3.7061) | Xent 0.7573(0.7525) | Loss 9.1376(9.3889) | Error 0.2722(0.2656) Steps 634(644.35) | Grad Norm 7.8182(9.0956) | Total Time 0.00(0.00)\n",
      "Iter 3790 | Time 16.6259(16.4013) | Bit/dim 3.6988(3.7076) | Xent 0.7095(0.7529) | Loss 9.2485(9.3349) | Error 0.2333(0.2652) Steps 658(647.91) | Grad Norm 8.5186(8.7865) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0069 | Time 84.5313, Epoch Time 1005.2247(901.3621), Bit/dim 3.7037(best: 3.7040), Xent 0.7683, Loss 4.0879, Error 0.2705(best: 0.2678)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3800 | Time 15.1423(16.3619) | Bit/dim 3.7361(3.7095) | Xent 0.6654(0.7434) | Loss 9.0951(9.8605) | Error 0.2333(0.2624) Steps 628(648.11) | Grad Norm 5.1163(8.3452) | Total Time 0.00(0.00)\n",
      "Iter 3810 | Time 16.4221(16.3757) | Bit/dim 3.6909(3.7059) | Xent 0.7807(0.7464) | Loss 9.1703(9.6745) | Error 0.2800(0.2630) Steps 622(648.03) | Grad Norm 5.0528(8.5406) | Total Time 0.00(0.00)\n",
      "Iter 3820 | Time 15.8634(16.3760) | Bit/dim 3.7243(3.7053) | Xent 0.7731(0.7501) | Loss 9.2098(9.5403) | Error 0.2844(0.2656) Steps 628(645.66) | Grad Norm 6.9987(9.2364) | Total Time 0.00(0.00)\n",
      "Iter 3830 | Time 16.0992(16.3782) | Bit/dim 3.7097(3.7053) | Xent 0.7371(0.7454) | Loss 9.1489(9.4275) | Error 0.2567(0.2627) Steps 646(645.58) | Grad Norm 11.0508(9.2364) | Total Time 0.00(0.00)\n",
      "Iter 3840 | Time 15.7657(16.3363) | Bit/dim 3.6992(3.7028) | Xent 0.7843(0.7406) | Loss 9.1278(9.3380) | Error 0.2856(0.2609) Steps 640(642.83) | Grad Norm 10.4219(8.9387) | Total Time 0.00(0.00)\n",
      "Iter 3850 | Time 16.3251(16.2995) | Bit/dim 3.6984(3.7014) | Xent 0.7312(0.7454) | Loss 9.2177(9.2895) | Error 0.2511(0.2640) Steps 646(642.87) | Grad Norm 6.2708(8.9138) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0070 | Time 83.2039, Epoch Time 998.3945(904.2731), Bit/dim 3.7016(best: 3.7037), Xent 0.7777, Loss 4.0904, Error 0.2757(best: 0.2678)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3860 | Time 15.8255(16.2934) | Bit/dim 3.7274(3.7038) | Xent 0.7224(0.7376) | Loss 9.1361(9.7595) | Error 0.2533(0.2611) Steps 634(642.31) | Grad Norm 6.9694(8.5104) | Total Time 0.00(0.00)\n",
      "Iter 3870 | Time 17.1235(16.3586) | Bit/dim 3.7245(3.7026) | Xent 0.7375(0.7361) | Loss 9.1634(9.5937) | Error 0.2667(0.2617) Steps 652(644.17) | Grad Norm 16.2210(8.9420) | Total Time 0.00(0.00)\n",
      "Iter 3880 | Time 16.2002(16.3585) | Bit/dim 3.7277(3.7019) | Xent 0.7320(0.7338) | Loss 9.1806(9.4743) | Error 0.2700(0.2596) Steps 634(646.00) | Grad Norm 8.2943(9.0110) | Total Time 0.00(0.00)\n",
      "Iter 3890 | Time 16.5502(16.4069) | Bit/dim 3.6059(3.6967) | Xent 0.7826(0.7423) | Loss 8.9945(9.3793) | Error 0.2656(0.2628) Steps 664(648.20) | Grad Norm 10.0404(9.7010) | Total Time 0.00(0.00)\n",
      "Iter 3900 | Time 15.5703(16.3268) | Bit/dim 3.7137(3.7004) | Xent 0.8285(0.7535) | Loss 9.2231(9.3255) | Error 0.3100(0.2671) Steps 640(648.12) | Grad Norm 10.3886(9.8890) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0071 | Time 85.7874, Epoch Time 1003.6680(907.2550), Bit/dim 3.7090(best: 3.7016), Xent 0.7722, Loss 4.0951, Error 0.2710(best: 0.2678)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3910 | Time 15.9622(16.3253) | Bit/dim 3.7136(3.7022) | Xent 0.7305(0.7508) | Loss 9.1564(9.8829) | Error 0.2622(0.2661) Steps 646(647.49) | Grad Norm 15.1244(10.0308) | Total Time 0.00(0.00)\n",
      "Iter 3920 | Time 17.4225(16.3687) | Bit/dim 3.7066(3.7048) | Xent 0.6823(0.7492) | Loss 9.1444(9.7016) | Error 0.2656(0.2684) Steps 652(646.04) | Grad Norm 13.5181(10.2474) | Total Time 0.00(0.00)\n",
      "Iter 3930 | Time 17.3720(16.4191) | Bit/dim 3.6871(3.7038) | Xent 0.6814(0.7483) | Loss 9.0150(9.5545) | Error 0.2389(0.2680) Steps 682(648.37) | Grad Norm 6.2769(10.2903) | Total Time 0.00(0.00)\n",
      "Iter 3940 | Time 16.2352(16.4384) | Bit/dim 3.7179(3.7044) | Xent 0.7462(0.7479) | Loss 9.0888(9.4487) | Error 0.2600(0.2672) Steps 658(649.30) | Grad Norm 8.9633(9.9797) | Total Time 0.00(0.00)\n",
      "Iter 3950 | Time 16.0339(16.4424) | Bit/dim 3.6912(3.7009) | Xent 0.6993(0.7435) | Loss 9.1281(9.3618) | Error 0.2400(0.2660) Steps 652(650.53) | Grad Norm 8.5300(9.2227) | Total Time 0.00(0.00)\n",
      "Iter 3960 | Time 16.6265(16.4996) | Bit/dim 3.7006(3.7029) | Xent 0.7305(0.7446) | Loss 9.1428(9.3114) | Error 0.2456(0.2664) Steps 676(651.23) | Grad Norm 11.3837(9.7966) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0072 | Time 84.0804, Epoch Time 1009.7859(910.3309), Bit/dim 3.7023(best: 3.7016), Xent 0.7747, Loss 4.0896, Error 0.2719(best: 0.2678)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3970 | Time 16.0241(16.3769) | Bit/dim 3.6777(3.7034) | Xent 0.7160(0.7391) | Loss 8.9395(9.7727) | Error 0.2456(0.2635) Steps 658(647.55) | Grad Norm 11.2786(9.4564) | Total Time 0.00(0.00)\n",
      "Iter 3980 | Time 16.5591(16.3365) | Bit/dim 3.6905(3.6989) | Xent 0.6786(0.7298) | Loss 9.0409(9.5824) | Error 0.2356(0.2598) Steps 640(644.78) | Grad Norm 9.4378(9.1604) | Total Time 0.00(0.00)\n",
      "Iter 3990 | Time 15.8316(16.3198) | Bit/dim 3.6920(3.6994) | Xent 0.7366(0.7286) | Loss 8.9935(9.4543) | Error 0.2667(0.2594) Steps 658(645.42) | Grad Norm 6.7168(9.1117) | Total Time 0.00(0.00)\n",
      "Iter 4000 | Time 16.1078(16.3361) | Bit/dim 3.6937(3.6996) | Xent 0.7172(0.7255) | Loss 8.9480(9.3592) | Error 0.2411(0.2579) Steps 628(646.64) | Grad Norm 11.0914(8.7673) | Total Time 0.00(0.00)\n",
      "Iter 4010 | Time 16.9018(16.3562) | Bit/dim 3.6789(3.6990) | Xent 0.7102(0.7208) | Loss 9.1022(9.2886) | Error 0.2644(0.2565) Steps 652(647.88) | Grad Norm 8.6397(8.6210) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0073 | Time 84.2152, Epoch Time 997.3399(912.9412), Bit/dim 3.6939(best: 3.7016), Xent 0.7662, Loss 4.0770, Error 0.2676(best: 0.2678)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4020 | Time 16.2837(16.3908) | Bit/dim 3.6603(3.6947) | Xent 0.7374(0.7148) | Loss 9.1153(9.8220) | Error 0.2767(0.2551) Steps 652(648.40) | Grad Norm 10.5079(8.8796) | Total Time 0.00(0.00)\n",
      "Iter 4030 | Time 16.6864(16.3506) | Bit/dim 3.6871(3.6927) | Xent 0.7082(0.7183) | Loss 9.2520(9.6329) | Error 0.2511(0.2564) Steps 652(646.97) | Grad Norm 8.8892(9.3196) | Total Time 0.00(0.00)\n",
      "Iter 4040 | Time 16.5687(16.3696) | Bit/dim 3.6811(3.6933) | Xent 0.7471(0.7193) | Loss 9.1461(9.4911) | Error 0.2656(0.2577) Steps 664(650.28) | Grad Norm 9.4837(9.2459) | Total Time 0.00(0.00)\n",
      "Iter 4050 | Time 16.4665(16.3783) | Bit/dim 3.7304(3.6949) | Xent 0.6880(0.7213) | Loss 9.1669(9.3922) | Error 0.2367(0.2576) Steps 646(647.48) | Grad Norm 7.6900(9.1866) | Total Time 0.00(0.00)\n",
      "Iter 4060 | Time 16.8153(16.4198) | Bit/dim 3.6720(3.6971) | Xent 0.6994(0.7211) | Loss 9.0697(9.3189) | Error 0.2522(0.2580) Steps 628(646.64) | Grad Norm 9.9447(9.0810) | Total Time 0.00(0.00)\n",
      "Iter 4070 | Time 15.7737(16.2914) | Bit/dim 3.7100(3.6996) | Xent 0.8907(0.7263) | Loss 9.1597(9.2676) | Error 0.2933(0.2593) Steps 646(644.70) | Grad Norm 13.7097(9.5202) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0074 | Time 85.1034, Epoch Time 1001.0319(915.5839), Bit/dim 3.6948(best: 3.6939), Xent 0.7813, Loss 4.0854, Error 0.2698(best: 0.2676)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4080 | Time 16.7060(16.3575) | Bit/dim 3.7078(3.6997) | Xent 0.6901(0.7246) | Loss 9.0949(9.7406) | Error 0.2478(0.2588) Steps 628(645.14) | Grad Norm 10.3152(9.9005) | Total Time 0.00(0.00)\n",
      "Iter 4090 | Time 15.6761(16.3353) | Bit/dim 3.7021(3.6993) | Xent 0.7413(0.7199) | Loss 8.9949(9.5681) | Error 0.2733(0.2575) Steps 622(643.74) | Grad Norm 6.2462(9.7223) | Total Time 0.00(0.00)\n",
      "Iter 4100 | Time 14.9457(16.2920) | Bit/dim 3.6992(3.6980) | Xent 0.7398(0.7198) | Loss 9.0115(9.4325) | Error 0.2656(0.2578) Steps 628(644.36) | Grad Norm 8.3400(9.7024) | Total Time 0.00(0.00)\n",
      "Iter 4110 | Time 16.3382(16.3463) | Bit/dim 3.7111(3.6981) | Xent 0.8012(0.7294) | Loss 9.2594(9.3561) | Error 0.2756(0.2602) Steps 664(645.54) | Grad Norm 11.8638(9.7152) | Total Time 0.00(0.00)\n",
      "Iter 4120 | Time 16.0243(16.3714) | Bit/dim 3.7175(3.6991) | Xent 0.7117(0.7283) | Loss 8.9719(9.2857) | Error 0.2522(0.2614) Steps 616(644.00) | Grad Norm 5.3063(9.5328) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0075 | Time 85.3587, Epoch Time 1005.7948(918.2902), Bit/dim 3.6962(best: 3.6939), Xent 0.7439, Loss 4.0682, Error 0.2587(best: 0.2676)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4130 | Time 16.2188(16.3822) | Bit/dim 3.6761(3.7001) | Xent 0.7177(0.7267) | Loss 9.0764(9.8664) | Error 0.2533(0.2605) Steps 646(646.37) | Grad Norm 8.1690(9.3605) | Total Time 0.00(0.00)\n",
      "Iter 4140 | Time 16.9400(16.3831) | Bit/dim 3.6656(3.6957) | Xent 0.7028(0.7210) | Loss 9.0906(9.6630) | Error 0.2500(0.2585) Steps 634(649.14) | Grad Norm 7.2610(8.9969) | Total Time 0.00(0.00)\n",
      "Iter 4150 | Time 16.5788(16.4122) | Bit/dim 3.6798(3.6928) | Xent 0.7072(0.7167) | Loss 9.0451(9.5003) | Error 0.2433(0.2572) Steps 664(649.48) | Grad Norm 8.1317(8.9483) | Total Time 0.00(0.00)\n",
      "Iter 4160 | Time 15.7804(16.4942) | Bit/dim 3.6851(3.6911) | Xent 0.6800(0.7107) | Loss 9.0937(9.3960) | Error 0.2411(0.2537) Steps 652(655.44) | Grad Norm 11.6355(8.9158) | Total Time 0.00(0.00)\n",
      "Iter 4170 | Time 16.2214(16.4502) | Bit/dim 3.7017(3.6929) | Xent 0.6686(0.7120) | Loss 9.0002(9.3138) | Error 0.2344(0.2539) Steps 652(653.59) | Grad Norm 5.9201(8.5748) | Total Time 0.00(0.00)\n",
      "Iter 4180 | Time 17.1999(16.4376) | Bit/dim 3.6851(3.6923) | Xent 0.7060(0.7055) | Loss 9.0229(9.2307) | Error 0.2678(0.2516) Steps 640(651.51) | Grad Norm 8.4473(8.3858) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0076 | Time 85.2891, Epoch Time 1007.2936(920.9603), Bit/dim 3.6912(best: 3.6939), Xent 0.7499, Loss 4.0662, Error 0.2611(best: 0.2587)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4190 | Time 16.1594(16.4773) | Bit/dim 3.7001(3.6903) | Xent 0.7044(0.7083) | Loss 8.9686(9.6993) | Error 0.2433(0.2524) Steps 646(651.45) | Grad Norm 5.6465(8.4681) | Total Time 0.00(0.00)\n",
      "Iter 4200 | Time 15.8523(16.4838) | Bit/dim 3.7065(3.6887) | Xent 0.7038(0.7013) | Loss 9.1021(9.5268) | Error 0.2622(0.2504) Steps 646(650.62) | Grad Norm 8.6965(8.3332) | Total Time 0.00(0.00)\n",
      "Iter 4210 | Time 16.6462(16.4600) | Bit/dim 3.6857(3.6909) | Xent 0.6799(0.6986) | Loss 8.9971(9.4041) | Error 0.2578(0.2492) Steps 640(648.57) | Grad Norm 6.0591(8.3508) | Total Time 0.00(0.00)\n",
      "Iter 4220 | Time 16.6132(16.4647) | Bit/dim 3.6786(3.6898) | Xent 0.7619(0.7209) | Loss 9.0869(9.3312) | Error 0.2733(0.2559) Steps 640(646.90) | Grad Norm 7.0425(9.6715) | Total Time 0.00(0.00)\n",
      "Iter 4230 | Time 16.2845(16.4974) | Bit/dim 3.6596(3.6915) | Xent 0.6793(0.7190) | Loss 8.9473(9.2607) | Error 0.2444(0.2563) Steps 652(644.70) | Grad Norm 7.8973(9.2627) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0077 | Time 85.2339, Epoch Time 1010.8317(923.6565), Bit/dim 3.6893(best: 3.6912), Xent 0.7614, Loss 4.0700, Error 0.2703(best: 0.2587)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4240 | Time 16.5189(16.4494) | Bit/dim 3.6643(3.6896) | Xent 0.6515(0.7158) | Loss 8.8497(9.8413) | Error 0.2378(0.2553) Steps 652(647.91) | Grad Norm 9.9687(9.4255) | Total Time 0.00(0.00)\n",
      "Iter 4250 | Time 17.1425(16.4925) | Bit/dim 3.6624(3.6882) | Xent 0.6466(0.7149) | Loss 8.9780(9.6405) | Error 0.2222(0.2554) Steps 688(651.95) | Grad Norm 6.5432(8.8246) | Total Time 0.00(0.00)\n",
      "Iter 4260 | Time 15.7727(16.4737) | Bit/dim 3.6904(3.6897) | Xent 0.6550(0.7092) | Loss 9.0777(9.4968) | Error 0.2256(0.2529) Steps 640(652.99) | Grad Norm 11.2607(9.1870) | Total Time 0.00(0.00)\n",
      "Iter 4270 | Time 16.1892(16.4945) | Bit/dim 3.7116(3.6875) | Xent 0.7019(0.7078) | Loss 9.1913(9.3865) | Error 0.2511(0.2521) Steps 634(651.74) | Grad Norm 8.3324(9.3300) | Total Time 0.00(0.00)\n",
      "Iter 4280 | Time 16.2436(16.4858) | Bit/dim 3.6858(3.6873) | Xent 0.6876(0.7077) | Loss 8.9800(9.3044) | Error 0.2389(0.2524) Steps 634(651.84) | Grad Norm 5.8457(9.0627) | Total Time 0.00(0.00)\n",
      "Iter 4290 | Time 16.7913(16.5284) | Bit/dim 3.6843(3.6876) | Xent 0.6933(0.7034) | Loss 8.9741(9.2300) | Error 0.2589(0.2506) Steps 652(653.68) | Grad Norm 7.1382(8.6510) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0078 | Time 84.1087, Epoch Time 1010.1471(926.2512), Bit/dim 3.6840(best: 3.6893), Xent 0.7449, Loss 4.0564, Error 0.2585(best: 0.2587)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4300 | Time 16.7344(16.5907) | Bit/dim 3.6695(3.6872) | Xent 0.7235(0.7017) | Loss 8.9972(9.7105) | Error 0.2578(0.2493) Steps 676(655.25) | Grad Norm 8.3917(8.7770) | Total Time 0.00(0.00)\n",
      "Iter 4310 | Time 16.4285(16.5821) | Bit/dim 3.6761(3.6852) | Xent 0.6490(0.6951) | Loss 8.9393(9.5356) | Error 0.2211(0.2468) Steps 646(656.71) | Grad Norm 14.7763(8.9751) | Total Time 0.00(0.00)\n",
      "Iter 4320 | Time 16.5472(16.5876) | Bit/dim 3.6921(3.6852) | Xent 0.7826(0.6982) | Loss 9.0967(9.4109) | Error 0.2678(0.2483) Steps 640(658.28) | Grad Norm 10.0788(9.3583) | Total Time 0.00(0.00)\n",
      "Iter 4330 | Time 16.3821(16.6536) | Bit/dim 3.6774(3.6855) | Xent 0.7584(0.7033) | Loss 9.1328(9.3219) | Error 0.2689(0.2495) Steps 664(661.59) | Grad Norm 11.2773(9.5675) | Total Time 0.00(0.00)\n",
      "Iter 4340 | Time 16.1509(16.5772) | Bit/dim 3.6562(3.6847) | Xent 0.6645(0.7042) | Loss 9.0064(9.2571) | Error 0.2400(0.2498) Steps 652(660.72) | Grad Norm 11.6455(9.5740) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0079 | Time 85.7968, Epoch Time 1018.6665(929.0236), Bit/dim 3.6918(best: 3.6840), Xent 0.7639, Loss 4.0738, Error 0.2651(best: 0.2585)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4350 | Time 16.2123(16.5996) | Bit/dim 3.7026(3.6869) | Xent 0.5953(0.6969) | Loss 8.9867(9.8147) | Error 0.2167(0.2475) Steps 658(658.84) | Grad Norm 5.7007(9.3323) | Total Time 0.00(0.00)\n",
      "Iter 4360 | Time 16.4097(16.6574) | Bit/dim 3.6755(3.6848) | Xent 0.6254(0.6952) | Loss 8.8186(9.6121) | Error 0.2233(0.2475) Steps 640(656.65) | Grad Norm 12.1570(9.7015) | Total Time 0.00(0.00)\n",
      "Iter 4370 | Time 16.6633(16.6612) | Bit/dim 3.6885(3.6847) | Xent 0.6836(0.6929) | Loss 9.1241(9.4689) | Error 0.2511(0.2469) Steps 658(656.79) | Grad Norm 6.3478(9.1875) | Total Time 0.00(0.00)\n",
      "Iter 4380 | Time 16.5842(16.6638) | Bit/dim 3.7161(3.6857) | Xent 0.7364(0.6919) | Loss 9.1661(9.3745) | Error 0.2656(0.2462) Steps 634(656.18) | Grad Norm 9.6317(9.1938) | Total Time 0.00(0.00)\n",
      "Iter 4390 | Time 16.9434(16.6615) | Bit/dim 3.7027(3.6853) | Xent 0.7131(0.6929) | Loss 9.1074(9.2980) | Error 0.2478(0.2455) Steps 688(657.13) | Grad Norm 8.6323(9.4819) | Total Time 0.00(0.00)\n",
      "Iter 4400 | Time 16.8674(16.7116) | Bit/dim 3.6648(3.6846) | Xent 0.6852(0.6950) | Loss 9.0490(9.2418) | Error 0.2489(0.2466) Steps 646(660.10) | Grad Norm 13.3952(9.7477) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0080 | Time 84.8481, Epoch Time 1023.1256(931.8467), Bit/dim 3.6897(best: 3.6840), Xent 0.7292, Loss 4.0542, Error 0.2576(best: 0.2585)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4410 | Time 16.0067(16.7194) | Bit/dim 3.6869(3.6870) | Xent 0.7270(0.6897) | Loss 9.1649(9.6980) | Error 0.2500(0.2460) Steps 652(661.48) | Grad Norm 9.0560(9.4796) | Total Time 0.00(0.00)\n",
      "Iter 4420 | Time 17.0146(16.6918) | Bit/dim 3.6776(3.6826) | Xent 0.7072(0.6923) | Loss 9.1164(9.5255) | Error 0.2422(0.2470) Steps 658(661.24) | Grad Norm 9.7186(9.3744) | Total Time 0.00(0.00)\n",
      "Iter 4430 | Time 16.6084(16.7799) | Bit/dim 3.6698(3.6861) | Xent 0.7008(0.6909) | Loss 9.0035(9.4199) | Error 0.2500(0.2458) Steps 658(663.89) | Grad Norm 14.9963(9.8002) | Total Time 0.00(0.00)\n",
      "Iter 4440 | Time 16.8034(16.7871) | Bit/dim 3.6764(3.6852) | Xent 0.7056(0.6859) | Loss 9.0641(9.3249) | Error 0.2444(0.2433) Steps 670(668.17) | Grad Norm 12.8505(9.4554) | Total Time 0.00(0.00)\n",
      "Iter 4450 | Time 16.3028(16.7651) | Bit/dim 3.6433(3.6826) | Xent 0.6167(0.6818) | Loss 8.9626(9.2480) | Error 0.2244(0.2424) Steps 640(665.14) | Grad Norm 7.5719(9.1976) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0081 | Time 84.9946, Epoch Time 1028.4634(934.7452), Bit/dim 3.6758(best: 3.6840), Xent 0.7344, Loss 4.0430, Error 0.2571(best: 0.2576)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4460 | Time 16.9946(16.8201) | Bit/dim 3.6927(3.6817) | Xent 0.6579(0.6783) | Loss 9.0797(9.7859) | Error 0.2222(0.2410) Steps 646(665.48) | Grad Norm 7.9036(8.5717) | Total Time 0.00(0.00)\n",
      "Iter 4470 | Time 17.5940(16.8281) | Bit/dim 3.6849(3.6808) | Xent 0.7287(0.6806) | Loss 9.1052(9.5954) | Error 0.2556(0.2423) Steps 700(667.03) | Grad Norm 12.6420(8.5222) | Total Time 0.00(0.00)\n",
      "Iter 4480 | Time 17.0921(16.8147) | Bit/dim 3.6324(3.6787) | Xent 0.6864(0.6796) | Loss 8.9389(9.4401) | Error 0.2422(0.2416) Steps 694(667.28) | Grad Norm 7.5105(8.4947) | Total Time 0.00(0.00)\n",
      "Iter 4490 | Time 17.6871(16.9297) | Bit/dim 3.6883(3.6808) | Xent 0.6828(0.6801) | Loss 9.0279(9.3501) | Error 0.2467(0.2426) Steps 670(668.66) | Grad Norm 8.8034(8.4964) | Total Time 0.00(0.00)\n",
      "Iter 4500 | Time 17.7869(16.9293) | Bit/dim 3.6322(3.6806) | Xent 0.6372(0.6860) | Loss 8.9562(9.2703) | Error 0.2244(0.2439) Steps 700(667.59) | Grad Norm 6.2236(9.0847) | Total Time 0.00(0.00)\n",
      "Iter 4510 | Time 16.9725(16.9251) | Bit/dim 3.6805(3.6807) | Xent 0.6425(0.6842) | Loss 8.9991(9.2058) | Error 0.2356(0.2442) Steps 676(667.06) | Grad Norm 6.4228(9.0993) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0082 | Time 84.4755, Epoch Time 1032.4369(937.6759), Bit/dim 3.6875(best: 3.6758), Xent 0.7190, Loss 4.0471, Error 0.2545(best: 0.2571)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4520 | Time 16.7275(16.9520) | Bit/dim 3.7177(3.6815) | Xent 0.6420(0.6773) | Loss 9.1414(9.6844) | Error 0.2389(0.2418) Steps 688(666.66) | Grad Norm 9.5948(8.7364) | Total Time 0.00(0.00)\n",
      "Iter 4530 | Time 16.7286(16.9640) | Bit/dim 3.6856(3.6791) | Xent 0.6544(0.6760) | Loss 8.9223(9.5057) | Error 0.2278(0.2407) Steps 646(665.67) | Grad Norm 12.2511(9.0590) | Total Time 0.00(0.00)\n",
      "Iter 4540 | Time 17.4693(17.0518) | Bit/dim 3.6685(3.6799) | Xent 0.6703(0.6746) | Loss 9.0745(9.3873) | Error 0.2511(0.2391) Steps 700(669.08) | Grad Norm 8.6891(9.1471) | Total Time 0.00(0.00)\n",
      "Iter 4550 | Time 18.1310(17.1007) | Bit/dim 3.7004(3.6783) | Xent 0.6795(0.6731) | Loss 8.9651(9.2900) | Error 0.2522(0.2393) Steps 670(667.18) | Grad Norm 9.7979(8.8915) | Total Time 0.00(0.00)\n",
      "Iter 4560 | Time 16.7047(17.0410) | Bit/dim 3.6690(3.6798) | Xent 0.6816(0.6735) | Loss 9.0052(9.2294) | Error 0.2478(0.2397) Steps 670(667.44) | Grad Norm 11.2453(8.7641) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0083 | Time 83.3774, Epoch Time 1042.1609(940.8105), Bit/dim 3.6926(best: 3.6758), Xent 0.8385, Loss 4.1118, Error 0.2854(best: 0.2545)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4570 | Time 17.2003(17.1685) | Bit/dim 3.6759(3.6793) | Xent 0.6835(0.6820) | Loss 9.0555(9.7953) | Error 0.2267(0.2422) Steps 682(672.30) | Grad Norm 10.5462(9.4175) | Total Time 0.00(0.00)\n",
      "Iter 4580 | Time 16.2898(17.1177) | Bit/dim 3.6332(3.6819) | Xent 0.6494(0.6803) | Loss 8.7690(9.5925) | Error 0.2422(0.2422) Steps 664(671.87) | Grad Norm 6.5477(8.9100) | Total Time 0.00(0.00)\n",
      "Iter 4590 | Time 16.8094(17.0899) | Bit/dim 3.6816(3.6814) | Xent 0.6648(0.6717) | Loss 9.0645(9.4422) | Error 0.2433(0.2390) Steps 682(672.67) | Grad Norm 6.0138(8.6154) | Total Time 0.00(0.00)\n",
      "Iter 4600 | Time 17.1602(17.0878) | Bit/dim 3.6752(3.6784) | Xent 0.6622(0.6652) | Loss 8.9788(9.3183) | Error 0.2478(0.2359) Steps 706(673.54) | Grad Norm 8.8665(8.4818) | Total Time 0.00(0.00)\n",
      "Iter 4610 | Time 16.5211(17.1087) | Bit/dim 3.7009(3.6774) | Xent 0.6563(0.6636) | Loss 8.9746(9.2389) | Error 0.2367(0.2348) Steps 664(671.47) | Grad Norm 11.0775(8.7092) | Total Time 0.00(0.00)\n",
      "Iter 4620 | Time 16.9132(17.1162) | Bit/dim 3.7128(3.6738) | Xent 0.6575(0.6692) | Loss 9.1335(9.1777) | Error 0.2344(0.2379) Steps 670(670.33) | Grad Norm 7.2233(8.6389) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0084 | Time 85.6709, Epoch Time 1045.6064(943.9544), Bit/dim 3.6731(best: 3.6758), Xent 0.7197, Loss 4.0330, Error 0.2511(best: 0.2545)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4630 | Time 16.4052(17.1036) | Bit/dim 3.6674(3.6738) | Xent 0.6983(0.6588) | Loss 9.0440(9.6517) | Error 0.2522(0.2341) Steps 658(668.44) | Grad Norm 10.2798(8.2236) | Total Time 0.00(0.00)\n",
      "Iter 4640 | Time 17.8970(17.0990) | Bit/dim 3.6453(3.6723) | Xent 0.5988(0.6530) | Loss 8.9176(9.4769) | Error 0.2111(0.2317) Steps 670(670.38) | Grad Norm 6.0211(7.9573) | Total Time 0.00(0.00)\n",
      "Iter 4650 | Time 16.3844(17.0883) | Bit/dim 3.6729(3.6710) | Xent 0.7103(0.6542) | Loss 9.0576(9.3552) | Error 0.2511(0.2329) Steps 664(671.54) | Grad Norm 10.4942(8.4975) | Total Time 0.00(0.00)\n",
      "Iter 4660 | Time 17.5679(17.0697) | Bit/dim 3.6668(3.6716) | Xent 0.6788(0.6570) | Loss 9.0902(9.2577) | Error 0.2400(0.2329) Steps 676(671.11) | Grad Norm 8.2180(8.5078) | Total Time 0.00(0.00)\n",
      "Iter 4670 | Time 16.7187(17.0345) | Bit/dim 3.6808(3.6720) | Xent 0.6669(0.6619) | Loss 9.1138(9.2021) | Error 0.2389(0.2339) Steps 664(671.59) | Grad Norm 7.3219(8.1489) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0085 | Time 85.7840, Epoch Time 1040.7637(946.8587), Bit/dim 3.6811(best: 3.6731), Xent 0.7325, Loss 4.0473, Error 0.2608(best: 0.2511)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4680 | Time 17.0449(17.0457) | Bit/dim 3.6632(3.6724) | Xent 0.6892(0.6551) | Loss 8.9910(9.7551) | Error 0.2433(0.2316) Steps 658(671.96) | Grad Norm 10.6011(8.0266) | Total Time 0.00(0.00)\n",
      "Iter 4690 | Time 17.6358(17.1366) | Bit/dim 3.6613(3.6721) | Xent 0.6919(0.6520) | Loss 9.0215(9.5654) | Error 0.2511(0.2313) Steps 646(676.58) | Grad Norm 7.4382(7.9860) | Total Time 0.00(0.00)\n",
      "Iter 4700 | Time 17.8798(17.1363) | Bit/dim 3.6769(3.6715) | Xent 0.6299(0.6492) | Loss 9.0389(9.4132) | Error 0.2289(0.2304) Steps 682(672.75) | Grad Norm 8.5914(8.0446) | Total Time 0.00(0.00)\n",
      "Iter 4710 | Time 16.3110(17.1389) | Bit/dim 3.6938(3.6715) | Xent 0.7013(0.6499) | Loss 9.1478(9.3149) | Error 0.2367(0.2303) Steps 682(673.67) | Grad Norm 7.8401(7.9346) | Total Time 0.00(0.00)\n",
      "Iter 4720 | Time 16.2164(17.0839) | Bit/dim 3.6436(3.6695) | Xent 0.6763(0.6516) | Loss 8.9784(9.2262) | Error 0.2500(0.2312) Steps 658(672.56) | Grad Norm 5.9960(8.3745) | Total Time 0.00(0.00)\n",
      "Iter 4730 | Time 17.3755(17.1337) | Bit/dim 3.6961(3.6720) | Xent 0.6394(0.6469) | Loss 8.9821(9.1694) | Error 0.2156(0.2284) Steps 664(677.04) | Grad Norm 6.9493(8.1418) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0086 | Time 85.6361, Epoch Time 1048.7080(949.9141), Bit/dim 3.6723(best: 3.6731), Xent 0.7180, Loss 4.0313, Error 0.2559(best: 0.2511)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4740 | Time 17.9193(17.0287) | Bit/dim 3.6540(3.6724) | Xent 0.6289(0.6404) | Loss 8.9290(9.6370) | Error 0.2067(0.2262) Steps 670(676.98) | Grad Norm 7.5843(8.2346) | Total Time 0.00(0.00)\n",
      "Iter 4750 | Time 16.9705(17.1217) | Bit/dim 3.6591(3.6705) | Xent 0.6968(0.6364) | Loss 8.9816(9.4552) | Error 0.2522(0.2260) Steps 676(679.01) | Grad Norm 12.9914(7.9787) | Total Time 0.00(0.00)\n",
      "Iter 4760 | Time 16.6263(17.2213) | Bit/dim 3.6527(3.6704) | Xent 0.6659(0.6440) | Loss 8.9103(9.3350) | Error 0.2211(0.2283) Steps 676(678.97) | Grad Norm 9.1182(8.6954) | Total Time 0.00(0.00)\n",
      "Iter 4770 | Time 18.1155(17.1907) | Bit/dim 3.6732(3.6727) | Xent 0.6601(0.6551) | Loss 9.0395(9.2678) | Error 0.2178(0.2308) Steps 676(678.21) | Grad Norm 11.5742(9.1725) | Total Time 0.00(0.00)\n",
      "Iter 4780 | Time 16.7277(17.1714) | Bit/dim 3.6721(3.6719) | Xent 0.6392(0.6516) | Loss 8.9194(9.1909) | Error 0.2278(0.2294) Steps 676(677.02) | Grad Norm 7.5619(8.6974) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0087 | Time 86.2192, Epoch Time 1047.5725(952.8439), Bit/dim 3.6741(best: 3.6723), Xent 0.7092, Loss 4.0287, Error 0.2519(best: 0.2511)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4790 | Time 17.6280(17.2021) | Bit/dim 3.6803(3.6746) | Xent 0.5729(0.6485) | Loss 8.8716(9.7472) | Error 0.2156(0.2305) Steps 670(676.33) | Grad Norm 9.1250(8.8322) | Total Time 0.00(0.00)\n",
      "Iter 4800 | Time 17.2137(17.2020) | Bit/dim 3.7002(3.6775) | Xent 0.7198(0.6503) | Loss 9.1354(9.5597) | Error 0.2522(0.2306) Steps 670(675.22) | Grad Norm 9.2261(8.8646) | Total Time 0.00(0.00)\n",
      "Iter 4810 | Time 18.0195(17.2033) | Bit/dim 3.6471(3.6759) | Xent 0.6452(0.6476) | Loss 9.0408(9.4081) | Error 0.2178(0.2290) Steps 706(678.74) | Grad Norm 10.1689(9.0316) | Total Time 0.00(0.00)\n",
      "Iter 4820 | Time 17.2544(17.2038) | Bit/dim 3.6353(3.6725) | Xent 0.6452(0.6460) | Loss 9.0453(9.3037) | Error 0.2300(0.2276) Steps 658(678.86) | Grad Norm 7.0104(8.9116) | Total Time 0.00(0.00)\n",
      "Iter 4830 | Time 17.2251(17.1279) | Bit/dim 3.6666(3.6723) | Xent 0.6797(0.6431) | Loss 9.0653(9.2227) | Error 0.2433(0.2263) Steps 694(678.63) | Grad Norm 6.2164(8.3758) | Total Time 0.00(0.00)\n",
      "Iter 4840 | Time 16.7058(17.1385) | Bit/dim 3.6749(3.6691) | Xent 0.7061(0.6465) | Loss 8.9992(9.1569) | Error 0.2533(0.2278) Steps 670(677.88) | Grad Norm 12.2946(7.9173) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0088 | Time 84.8763, Epoch Time 1047.5284(955.6844), Bit/dim 3.6694(best: 3.6723), Xent 0.7385, Loss 4.0387, Error 0.2607(best: 0.2511)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4850 | Time 16.8202(17.1354) | Bit/dim 3.7245(3.6688) | Xent 0.6313(0.6485) | Loss 8.9645(9.6414) | Error 0.2333(0.2293) Steps 694(680.78) | Grad Norm 10.0835(8.4699) | Total Time 0.00(0.00)\n",
      "Iter 4860 | Time 17.8003(17.2030) | Bit/dim 3.6783(3.6705) | Xent 0.6487(0.6425) | Loss 8.9708(9.4574) | Error 0.2200(0.2277) Steps 700(681.60) | Grad Norm 10.5597(8.5988) | Total Time 0.00(0.00)\n",
      "Iter 4870 | Time 16.6937(17.1928) | Bit/dim 3.6821(3.6709) | Xent 0.6197(0.6385) | Loss 9.0611(9.3297) | Error 0.2211(0.2272) Steps 706(681.54) | Grad Norm 7.5429(8.9520) | Total Time 0.00(0.00)\n",
      "Iter 4880 | Time 16.9172(17.2236) | Bit/dim 3.6485(3.6711) | Xent 0.6461(0.6413) | Loss 8.8385(9.2380) | Error 0.2344(0.2279) Steps 664(677.04) | Grad Norm 5.1329(8.6322) | Total Time 0.00(0.00)\n",
      "Iter 4890 | Time 17.0377(17.2704) | Bit/dim 3.6558(3.6688) | Xent 0.6830(0.6404) | Loss 9.0589(9.1716) | Error 0.2400(0.2285) Steps 652(679.45) | Grad Norm 13.2195(8.5566) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0089 | Time 87.0372, Epoch Time 1055.2684(958.6719), Bit/dim 3.6678(best: 3.6694), Xent 0.7546, Loss 4.0451, Error 0.2663(best: 0.2511)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4900 | Time 17.5482(17.2872) | Bit/dim 3.6692(3.6693) | Xent 0.6402(0.6419) | Loss 8.9927(9.7530) | Error 0.2300(0.2279) Steps 688(681.28) | Grad Norm 11.0353(9.3221) | Total Time 0.00(0.00)\n",
      "Iter 4910 | Time 17.5165(17.3366) | Bit/dim 3.6539(3.6675) | Xent 0.6150(0.6331) | Loss 8.9746(9.5391) | Error 0.2233(0.2250) Steps 706(684.36) | Grad Norm 4.6997(8.5490) | Total Time 0.00(0.00)\n",
      "Iter 4920 | Time 17.4785(17.3458) | Bit/dim 3.6899(3.6683) | Xent 0.6031(0.6263) | Loss 9.0582(9.3781) | Error 0.2222(0.2231) Steps 724(686.07) | Grad Norm 5.6217(8.3649) | Total Time 0.00(0.00)\n",
      "Iter 4930 | Time 17.4928(17.4071) | Bit/dim 3.6782(3.6671) | Xent 0.5894(0.6285) | Loss 8.9514(9.2693) | Error 0.2122(0.2238) Steps 682(687.26) | Grad Norm 4.7316(8.0651) | Total Time 0.00(0.00)\n",
      "Iter 4940 | Time 17.7847(17.4044) | Bit/dim 3.6763(3.6663) | Xent 0.6185(0.6271) | Loss 8.8187(9.1848) | Error 0.2200(0.2242) Steps 676(686.98) | Grad Norm 4.6861(7.9585) | Total Time 0.00(0.00)\n",
      "Iter 4950 | Time 17.7964(17.3946) | Bit/dim 3.6419(3.6631) | Xent 0.6682(0.6290) | Loss 8.9686(9.1210) | Error 0.2367(0.2244) Steps 658(685.04) | Grad Norm 8.1776(7.7997) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0090 | Time 86.6494, Epoch Time 1063.6235(961.8205), Bit/dim 3.6687(best: 3.6678), Xent 0.7047, Loss 4.0210, Error 0.2436(best: 0.2511)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4960 | Time 17.5097(17.4537) | Bit/dim 3.6960(3.6660) | Xent 0.6610(0.6247) | Loss 9.0044(9.6054) | Error 0.2233(0.2227) Steps 664(687.23) | Grad Norm 9.7366(7.8559) | Total Time 0.00(0.00)\n",
      "Iter 4970 | Time 17.5981(17.4721) | Bit/dim 3.6809(3.6658) | Xent 0.7609(0.6291) | Loss 9.1751(9.4472) | Error 0.2600(0.2232) Steps 658(687.56) | Grad Norm 22.8195(8.5713) | Total Time 0.00(0.00)\n",
      "Iter 4980 | Time 18.4948(17.5037) | Bit/dim 3.6712(3.6679) | Xent 0.6546(0.6368) | Loss 9.1770(9.3412) | Error 0.2200(0.2253) Steps 718(690.16) | Grad Norm 8.9239(9.0729) | Total Time 0.00(0.00)\n",
      "Iter 4990 | Time 16.4358(17.5136) | Bit/dim 3.6554(3.6682) | Xent 0.6438(0.6466) | Loss 8.9635(9.2577) | Error 0.2278(0.2294) Steps 676(688.27) | Grad Norm 7.3391(8.8547) | Total Time 0.00(0.00)\n",
      "Iter 5000 | Time 17.9693(17.5799) | Bit/dim 3.6741(3.6672) | Xent 0.6163(0.6444) | Loss 9.0652(9.1854) | Error 0.2278(0.2290) Steps 718(689.27) | Grad Norm 5.2045(8.5625) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0091 | Time 87.7219, Epoch Time 1072.1434(965.1302), Bit/dim 3.6760(best: 3.6678), Xent 0.7116, Loss 4.0318, Error 0.2491(best: 0.2436)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5010 | Time 17.1933(17.5313) | Bit/dim 3.7037(3.6662) | Xent 0.5993(0.6396) | Loss 9.0235(9.7552) | Error 0.2178(0.2283) Steps 682(690.56) | Grad Norm 6.4873(8.5235) | Total Time 0.00(0.00)\n",
      "Iter 5020 | Time 16.4273(17.4587) | Bit/dim 3.6687(3.6667) | Xent 0.5894(0.6300) | Loss 8.9282(9.5536) | Error 0.2111(0.2256) Steps 664(687.90) | Grad Norm 8.5326(8.5738) | Total Time 0.00(0.00)\n",
      "Iter 5030 | Time 16.3453(17.4096) | Bit/dim 3.6349(3.6676) | Xent 0.6429(0.6252) | Loss 8.8754(9.3967) | Error 0.2211(0.2231) Steps 652(684.17) | Grad Norm 10.5118(8.4116) | Total Time 0.00(0.00)\n",
      "Iter 5040 | Time 17.6215(17.4322) | Bit/dim 3.6553(3.6665) | Xent 0.5925(0.6225) | Loss 9.0367(9.2828) | Error 0.2111(0.2224) Steps 706(685.05) | Grad Norm 5.5291(8.1877) | Total Time 0.00(0.00)\n",
      "Iter 5050 | Time 17.4604(17.4857) | Bit/dim 3.6663(3.6655) | Xent 0.6757(0.6289) | Loss 8.9667(9.1990) | Error 0.2289(0.2237) Steps 670(688.09) | Grad Norm 10.6128(8.6145) | Total Time 0.00(0.00)\n",
      "Iter 5060 | Time 17.1276(17.4745) | Bit/dim 3.6756(3.6641) | Xent 0.6077(0.6273) | Loss 8.9540(9.1329) | Error 0.2111(0.2227) Steps 694(686.19) | Grad Norm 11.1766(8.8356) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0092 | Time 87.2277, Epoch Time 1063.8229(968.0910), Bit/dim 3.6788(best: 3.6678), Xent 0.7125, Loss 4.0351, Error 0.2453(best: 0.2436)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5070 | Time 17.1671(17.4461) | Bit/dim 3.6660(3.6622) | Xent 0.6391(0.6263) | Loss 8.9433(9.6009) | Error 0.2233(0.2231) Steps 706(688.60) | Grad Norm 9.8073(8.7083) | Total Time 0.00(0.00)\n",
      "Iter 5080 | Time 17.1829(17.5082) | Bit/dim 3.6570(3.6622) | Xent 0.6449(0.6209) | Loss 8.9872(9.4380) | Error 0.2122(0.2195) Steps 682(688.74) | Grad Norm 7.3381(8.7475) | Total Time 0.00(0.00)\n",
      "Iter 5090 | Time 17.7185(17.5127) | Bit/dim 3.6217(3.6617) | Xent 0.6546(0.6206) | Loss 8.9184(9.3143) | Error 0.2478(0.2205) Steps 712(690.95) | Grad Norm 10.2510(8.6814) | Total Time 0.00(0.00)\n",
      "Iter 5100 | Time 16.9172(17.4873) | Bit/dim 3.6828(3.6660) | Xent 0.6224(0.6171) | Loss 8.8822(9.2266) | Error 0.2211(0.2189) Steps 664(691.19) | Grad Norm 6.9772(7.9788) | Total Time 0.00(0.00)\n",
      "Iter 5110 | Time 18.2665(17.5103) | Bit/dim 3.6679(3.6614) | Xent 0.5989(0.6135) | Loss 8.8956(9.1406) | Error 0.2089(0.2175) Steps 688(689.43) | Grad Norm 6.4984(7.9498) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0093 | Time 88.9476, Epoch Time 1071.4968(971.1931), Bit/dim 3.6558(best: 3.6678), Xent 0.6854, Loss 3.9985, Error 0.2398(best: 0.2436)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5120 | Time 17.1028(17.5495) | Bit/dim 3.6707(3.6618) | Xent 0.6059(0.6082) | Loss 8.9657(9.6976) | Error 0.2122(0.2155) Steps 652(688.21) | Grad Norm 6.2067(7.9835) | Total Time 0.00(0.00)\n",
      "Iter 5130 | Time 17.5709(17.5171) | Bit/dim 3.6965(3.6647) | Xent 0.6434(0.6148) | Loss 9.0845(9.5234) | Error 0.2167(0.2182) Steps 700(687.38) | Grad Norm 14.2104(8.7223) | Total Time 0.00(0.00)\n",
      "Iter 5140 | Time 18.1954(17.6209) | Bit/dim 3.6721(3.6641) | Xent 0.6212(0.6225) | Loss 9.0536(9.3933) | Error 0.2144(0.2208) Steps 724(693.29) | Grad Norm 7.2722(8.9996) | Total Time 0.00(0.00)\n",
      "Iter 5150 | Time 17.4393(17.5983) | Bit/dim 3.6722(3.6623) | Xent 0.5909(0.6223) | Loss 9.0499(9.2792) | Error 0.2156(0.2206) Steps 694(691.68) | Grad Norm 7.1869(8.5768) | Total Time 0.00(0.00)\n",
      "Iter 5160 | Time 17.5271(17.5937) | Bit/dim 3.6657(3.6623) | Xent 0.6265(0.6168) | Loss 8.8999(9.1865) | Error 0.2089(0.2185) Steps 688(689.12) | Grad Norm 8.5247(8.3114) | Total Time 0.00(0.00)\n",
      "Iter 5170 | Time 17.2553(17.6678) | Bit/dim 3.6725(3.6614) | Xent 0.6702(0.6224) | Loss 8.9648(9.1367) | Error 0.2300(0.2209) Steps 676(692.09) | Grad Norm 9.5990(8.6276) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0094 | Time 89.0312, Epoch Time 1078.0310(974.3983), Bit/dim 3.6617(best: 3.6558), Xent 0.6880, Loss 4.0057, Error 0.2423(best: 0.2398)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5180 | Time 17.7996(17.7112) | Bit/dim 3.6649(3.6618) | Xent 0.6532(0.6155) | Loss 9.0396(9.6169) | Error 0.2356(0.2186) Steps 706(695.36) | Grad Norm 10.1406(8.4052) | Total Time 0.00(0.00)\n",
      "Iter 5190 | Time 17.4284(17.7001) | Bit/dim 3.6682(3.6637) | Xent 0.5855(0.6112) | Loss 8.8631(9.4482) | Error 0.2200(0.2172) Steps 682(693.92) | Grad Norm 7.3438(8.1906) | Total Time 0.00(0.00)\n",
      "Iter 5200 | Time 17.7194(17.6789) | Bit/dim 3.6861(3.6632) | Xent 0.6264(0.6159) | Loss 9.0022(9.3188) | Error 0.2267(0.2198) Steps 682(693.51) | Grad Norm 12.5824(8.6223) | Total Time 0.00(0.00)\n",
      "Iter 5210 | Time 17.4595(17.6981) | Bit/dim 3.6649(3.6611) | Xent 0.6072(0.6183) | Loss 9.0586(9.2277) | Error 0.2233(0.2198) Steps 670(692.04) | Grad Norm 8.8089(9.0624) | Total Time 0.00(0.00)\n",
      "Iter 5220 | Time 17.6718(17.7092) | Bit/dim 3.6605(3.6594) | Xent 0.6740(0.6200) | Loss 8.9241(9.1594) | Error 0.2378(0.2197) Steps 700(694.17) | Grad Norm 7.7749(9.6334) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0095 | Time 87.9796, Epoch Time 1081.4171(977.6088), Bit/dim 3.6653(best: 3.6558), Xent 0.6919, Loss 4.0113, Error 0.2401(best: 0.2398)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5230 | Time 17.7324(17.6927) | Bit/dim 3.6843(3.6619) | Xent 0.6485(0.6143) | Loss 8.7368(9.6932) | Error 0.2311(0.2173) Steps 694(692.78) | Grad Norm 5.9466(8.7901) | Total Time 0.00(0.00)\n",
      "Iter 5240 | Time 17.9082(17.6847) | Bit/dim 3.6796(3.6632) | Xent 0.5490(0.6111) | Loss 9.0439(9.5076) | Error 0.2011(0.2159) Steps 700(694.57) | Grad Norm 7.9338(8.5108) | Total Time 0.00(0.00)\n",
      "Iter 5250 | Time 17.7052(17.6820) | Bit/dim 3.6383(3.6592) | Xent 0.5478(0.6086) | Loss 8.7692(9.3501) | Error 0.1900(0.2155) Steps 694(696.53) | Grad Norm 6.9167(8.6027) | Total Time 0.00(0.00)\n",
      "Iter 5260 | Time 17.2842(17.6679) | Bit/dim 3.6793(3.6600) | Xent 0.6027(0.6091) | Loss 9.0062(9.2431) | Error 0.2167(0.2152) Steps 706(699.42) | Grad Norm 9.6460(8.7932) | Total Time 0.00(0.00)\n",
      "Iter 5270 | Time 16.8834(17.5598) | Bit/dim 3.6684(3.6618) | Xent 0.6361(0.6053) | Loss 9.0554(9.1648) | Error 0.2367(0.2146) Steps 676(696.25) | Grad Norm 14.9925(9.1250) | Total Time 0.00(0.00)\n",
      "Iter 5280 | Time 17.7048(17.6513) | Bit/dim 3.6150(3.6605) | Xent 0.6371(0.6113) | Loss 8.9249(9.1145) | Error 0.2422(0.2179) Steps 682(696.60) | Grad Norm 8.0095(9.2259) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0096 | Time 87.4815, Epoch Time 1074.3985(980.5125), Bit/dim 3.6623(best: 3.6558), Xent 0.6922, Loss 4.0084, Error 0.2417(best: 0.2398)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5290 | Time 17.8384(17.6747) | Bit/dim 3.6224(3.6570) | Xent 0.6132(0.6082) | Loss 8.8314(9.5989) | Error 0.2300(0.2171) Steps 694(698.21) | Grad Norm 5.6119(8.6312) | Total Time 0.00(0.00)\n",
      "Iter 5300 | Time 17.1080(17.6470) | Bit/dim 3.6513(3.6597) | Xent 0.5862(0.6037) | Loss 8.9034(9.4250) | Error 0.2133(0.2165) Steps 700(698.72) | Grad Norm 6.7382(8.2998) | Total Time 0.00(0.00)\n",
      "Iter 5310 | Time 18.3679(17.7177) | Bit/dim 3.6414(3.6563) | Xent 0.6048(0.6012) | Loss 8.9283(9.2874) | Error 0.2111(0.2143) Steps 706(698.53) | Grad Norm 12.1101(8.3675) | Total Time 0.00(0.00)\n",
      "Iter 5320 | Time 18.5912(17.7460) | Bit/dim 3.6392(3.6568) | Xent 0.4968(0.5954) | Loss 8.8260(9.1963) | Error 0.1844(0.2119) Steps 694(697.82) | Grad Norm 6.4834(8.4585) | Total Time 0.00(0.00)\n",
      "Iter 5330 | Time 18.8171(17.8185) | Bit/dim 3.6633(3.6598) | Xent 0.5933(0.5922) | Loss 9.1098(9.1357) | Error 0.2067(0.2102) Steps 688(698.52) | Grad Norm 9.3475(8.2716) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0097 | Time 88.3208, Epoch Time 1084.4506(983.6307), Bit/dim 3.6605(best: 3.6558), Xent 0.6889, Loss 4.0050, Error 0.2358(best: 0.2398)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5340 | Time 17.9045(17.7700) | Bit/dim 3.6922(3.6576) | Xent 0.5589(0.5879) | Loss 8.9723(9.6891) | Error 0.1978(0.2086) Steps 670(695.19) | Grad Norm 10.6318(8.0301) | Total Time 0.00(0.00)\n",
      "Iter 5350 | Time 17.5245(17.7873) | Bit/dim 3.6527(3.6563) | Xent 0.5481(0.5905) | Loss 8.8113(9.4887) | Error 0.1911(0.2101) Steps 682(693.55) | Grad Norm 6.7622(8.1909) | Total Time 0.00(0.00)\n",
      "Iter 5360 | Time 18.6839(17.7749) | Bit/dim 3.7124(3.6573) | Xent 0.5131(0.5918) | Loss 8.9002(9.3441) | Error 0.1967(0.2099) Steps 682(693.85) | Grad Norm 7.9183(8.4087) | Total Time 0.00(0.00)\n",
      "Iter 5370 | Time 17.8344(17.8048) | Bit/dim 3.6407(3.6566) | Xent 0.5354(0.5904) | Loss 8.7526(9.2280) | Error 0.1844(0.2093) Steps 724(698.63) | Grad Norm 5.6095(8.2768) | Total Time 0.00(0.00)\n",
      "Iter 5380 | Time 18.0622(17.7995) | Bit/dim 3.6627(3.6559) | Xent 0.6201(0.5951) | Loss 8.9377(9.1523) | Error 0.2122(0.2109) Steps 676(697.06) | Grad Norm 13.5563(8.3803) | Total Time 0.00(0.00)\n",
      "Iter 5390 | Time 17.6281(17.7968) | Bit/dim 3.6562(3.6552) | Xent 0.6350(0.6047) | Loss 9.0001(9.1061) | Error 0.2189(0.2140) Steps 706(699.34) | Grad Norm 9.0645(8.7760) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0098 | Time 88.4051, Epoch Time 1085.6230(986.6904), Bit/dim 3.6589(best: 3.6558), Xent 0.6927, Loss 4.0052, Error 0.2358(best: 0.2358)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5400 | Time 18.2101(17.8640) | Bit/dim 3.6688(3.6576) | Xent 0.5877(0.5958) | Loss 9.0211(9.5731) | Error 0.1967(0.2102) Steps 718(703.03) | Grad Norm 7.9194(8.8527) | Total Time 0.00(0.00)\n",
      "Iter 5410 | Time 18.5789(17.8117) | Bit/dim 3.6631(3.6587) | Xent 0.5888(0.5976) | Loss 8.9342(9.4133) | Error 0.2144(0.2127) Steps 736(705.57) | Grad Norm 11.5373(8.8952) | Total Time 0.00(0.00)\n",
      "Iter 5420 | Time 17.2314(17.8281) | Bit/dim 3.6720(3.6580) | Xent 0.5185(0.5988) | Loss 8.8363(9.2887) | Error 0.1789(0.2146) Steps 682(703.20) | Grad Norm 10.8825(9.3574) | Total Time 0.00(0.00)\n",
      "Iter 5430 | Time 19.4459(17.9164) | Bit/dim 3.6272(3.6560) | Xent 0.5612(0.5966) | Loss 8.8407(9.1916) | Error 0.2111(0.2134) Steps 694(705.82) | Grad Norm 7.7637(8.7992) | Total Time 0.00(0.00)\n",
      "Iter 5440 | Time 18.0659(17.9476) | Bit/dim 3.6305(3.6552) | Xent 0.5824(0.5961) | Loss 8.7945(9.1249) | Error 0.1889(0.2121) Steps 706(704.43) | Grad Norm 4.9774(8.8073) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0099 | Time 87.3640, Epoch Time 1091.5352(989.8358), Bit/dim 3.6610(best: 3.6558), Xent 0.6912, Loss 4.0066, Error 0.2375(best: 0.2358)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5450 | Time 18.0996(17.9262) | Bit/dim 3.6921(3.6570) | Xent 0.5374(0.5914) | Loss 8.9894(9.6986) | Error 0.1767(0.2104) Steps 700(707.89) | Grad Norm 12.3954(8.8011) | Total Time 0.00(0.00)\n",
      "Iter 5460 | Time 17.6882(17.8918) | Bit/dim 3.6657(3.6532) | Xent 0.5119(0.5850) | Loss 8.8609(9.4948) | Error 0.1844(0.2080) Steps 724(708.92) | Grad Norm 6.8733(8.5551) | Total Time 0.00(0.00)\n",
      "Iter 5470 | Time 19.5032(17.8949) | Bit/dim 3.6719(3.6499) | Xent 0.5746(0.5858) | Loss 8.9649(9.3370) | Error 0.1933(0.2077) Steps 736(708.48) | Grad Norm 8.4839(8.3953) | Total Time 0.00(0.00)\n",
      "Iter 5480 | Time 16.8409(17.8392) | Bit/dim 3.6052(3.6505) | Xent 0.5875(0.5846) | Loss 8.7763(9.2340) | Error 0.1956(0.2070) Steps 694(708.51) | Grad Norm 4.8356(7.8077) | Total Time 0.00(0.00)\n",
      "Iter 5490 | Time 17.6327(17.7291) | Bit/dim 3.6516(3.6493) | Xent 0.5928(0.5868) | Loss 8.9597(9.1552) | Error 0.2044(0.2087) Steps 724(709.45) | Grad Norm 9.1820(8.0203) | Total Time 0.00(0.00)\n",
      "Iter 5500 | Time 17.7204(17.7429) | Bit/dim 3.6486(3.6534) | Xent 0.5838(0.5882) | Loss 8.8762(9.1002) | Error 0.1967(0.2083) Steps 724(713.70) | Grad Norm 11.3940(8.2134) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0100 | Time 84.7631, Epoch Time 1078.0302(992.4816), Bit/dim 3.6551(best: 3.6558), Xent 0.7143, Loss 4.0122, Error 0.2499(best: 0.2358)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5510 | Time 17.4614(17.6464) | Bit/dim 3.6188(3.6511) | Xent 0.5635(0.5905) | Loss 8.8442(9.5857) | Error 0.2056(0.2088) Steps 700(713.23) | Grad Norm 10.6084(8.5412) | Total Time 0.00(0.00)\n",
      "Iter 5520 | Time 17.7800(17.6408) | Bit/dim 3.6374(3.6512) | Xent 0.6113(0.5908) | Loss 8.9426(9.4095) | Error 0.2011(0.2093) Steps 736(713.30) | Grad Norm 8.2183(8.8133) | Total Time 0.00(0.00)\n",
      "Iter 5530 | Time 17.9933(17.6849) | Bit/dim 3.6683(3.6561) | Xent 0.6113(0.5838) | Loss 9.0589(9.2895) | Error 0.2267(0.2068) Steps 694(712.99) | Grad Norm 12.0101(8.6179) | Total Time 0.00(0.00)\n",
      "Iter 5540 | Time 18.0956(17.6483) | Bit/dim 3.6310(3.6560) | Xent 0.5889(0.5888) | Loss 8.8970(9.1991) | Error 0.2122(0.2099) Steps 748(713.33) | Grad Norm 8.8332(9.0288) | Total Time 0.00(0.00)\n",
      "Iter 5550 | Time 17.4560(17.5963) | Bit/dim 3.6207(3.6546) | Xent 0.6071(0.5853) | Loss 8.9488(9.1203) | Error 0.2278(0.2084) Steps 682(711.83) | Grad Norm 8.8623(8.8464) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0101 | Time 85.9465, Epoch Time 1071.2627(994.8450), Bit/dim 3.6519(best: 3.6551), Xent 0.7059, Loss 4.0048, Error 0.2470(best: 0.2358)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5560 | Time 19.0192(17.7556) | Bit/dim 3.6170(3.6540) | Xent 0.5878(0.5900) | Loss 8.9142(9.7035) | Error 0.2122(0.2107) Steps 706(713.12) | Grad Norm 10.9023(8.8499) | Total Time 0.00(0.00)\n",
      "Iter 5570 | Time 18.1566(17.7296) | Bit/dim 3.6529(3.6542) | Xent 0.5804(0.5872) | Loss 9.0757(9.4964) | Error 0.2167(0.2097) Steps 712(714.58) | Grad Norm 5.3348(8.3199) | Total Time 0.00(0.00)\n",
      "Iter 5580 | Time 17.1262(17.7176) | Bit/dim 3.6164(3.6519) | Xent 0.5613(0.5847) | Loss 8.7916(9.3478) | Error 0.1956(0.2076) Steps 718(713.69) | Grad Norm 9.2123(8.4758) | Total Time 0.00(0.00)\n",
      "Iter 5590 | Time 17.7607(17.7510) | Bit/dim 3.6629(3.6545) | Xent 0.6439(0.5850) | Loss 9.1010(9.2415) | Error 0.2411(0.2084) Steps 724(712.73) | Grad Norm 15.2044(8.9486) | Total Time 0.00(0.00)\n",
      "Iter 5600 | Time 17.3169(17.7194) | Bit/dim 3.6471(3.6530) | Xent 0.4909(0.5840) | Loss 8.8592(9.1593) | Error 0.1711(0.2086) Steps 724(713.43) | Grad Norm 6.9443(8.7704) | Total Time 0.00(0.00)\n",
      "Iter 5610 | Time 16.3327(17.6590) | Bit/dim 3.6439(3.6513) | Xent 0.6237(0.5886) | Loss 8.9138(9.1002) | Error 0.2256(0.2100) Steps 688(713.14) | Grad Norm 11.3919(8.5125) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0102 | Time 84.0546, Epoch Time 1073.8656(997.2157), Bit/dim 3.6562(best: 3.6519), Xent 0.7281, Loss 4.0202, Error 0.2465(best: 0.2358)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5620 | Time 17.2262(17.6912) | Bit/dim 3.6524(3.6512) | Xent 0.5493(0.5834) | Loss 8.9440(9.5818) | Error 0.1978(0.2074) Steps 712(713.60) | Grad Norm 5.0541(8.6970) | Total Time 0.00(0.00)\n",
      "Iter 5630 | Time 18.5582(17.7398) | Bit/dim 3.6913(3.6510) | Xent 0.5349(0.5746) | Loss 9.0741(9.4059) | Error 0.1911(0.2054) Steps 742(716.80) | Grad Norm 8.6709(8.4760) | Total Time 0.00(0.00)\n",
      "Iter 5640 | Time 18.2103(17.7833) | Bit/dim 3.6509(3.6499) | Xent 0.5088(0.5759) | Loss 8.9096(9.2814) | Error 0.1800(0.2057) Steps 724(718.19) | Grad Norm 7.1136(8.8346) | Total Time 0.00(0.00)\n",
      "Iter 5650 | Time 18.0109(17.7993) | Bit/dim 3.6331(3.6478) | Xent 0.5909(0.5819) | Loss 8.7828(9.1770) | Error 0.2200(0.2066) Steps 694(714.58) | Grad Norm 11.4977(9.1014) | Total Time 0.00(0.00)\n",
      "Iter 5660 | Time 18.4120(17.8259) | Bit/dim 3.6221(3.6502) | Xent 0.5750(0.5931) | Loss 8.7834(9.1314) | Error 0.2056(0.2098) Steps 742(717.52) | Grad Norm 5.3142(9.7188) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0103 | Time 84.4509, Epoch Time 1081.2070(999.7354), Bit/dim 3.6674(best: 3.6519), Xent 0.6907, Loss 4.0128, Error 0.2366(best: 0.2358)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5670 | Time 18.5699(17.8304) | Bit/dim 3.6519(3.6533) | Xent 0.5808(0.5875) | Loss 8.9240(9.7071) | Error 0.2022(0.2082) Steps 730(718.78) | Grad Norm 8.3492(9.6166) | Total Time 0.00(0.00)\n",
      "Iter 5680 | Time 18.3541(17.8497) | Bit/dim 3.6619(3.6526) | Xent 0.5755(0.5856) | Loss 9.0053(9.5035) | Error 0.2100(0.2085) Steps 724(720.36) | Grad Norm 9.3916(8.9429) | Total Time 0.00(0.00)\n",
      "Iter 5690 | Time 18.9324(17.9691) | Bit/dim 3.6334(3.6498) | Xent 0.5789(0.5806) | Loss 8.9369(9.3448) | Error 0.1867(0.2054) Steps 754(721.82) | Grad Norm 10.4246(8.5222) | Total Time 0.00(0.00)\n",
      "Iter 5700 | Time 18.6537(18.1019) | Bit/dim 3.6545(3.6473) | Xent 0.5696(0.5762) | Loss 8.9391(9.2217) | Error 0.1978(0.2043) Steps 712(720.44) | Grad Norm 5.9962(8.0712) | Total Time 0.00(0.00)\n",
      "Iter 5710 | Time 18.4826(18.2142) | Bit/dim 3.6726(3.6452) | Xent 0.5501(0.5704) | Loss 8.7954(9.1333) | Error 0.1989(0.2019) Steps 694(718.91) | Grad Norm 10.9290(7.9868) | Total Time 0.00(0.00)\n",
      "Iter 5720 | Time 18.7275(18.2854) | Bit/dim 3.6971(3.6441) | Xent 0.5876(0.5693) | Loss 8.9905(9.0768) | Error 0.2089(0.2020) Steps 718(720.75) | Grad Norm 10.2013(8.1914) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0104 | Time 88.5817, Epoch Time 1113.4890(1003.1480), Bit/dim 3.6474(best: 3.6519), Xent 0.7146, Loss 4.0048, Error 0.2429(best: 0.2358)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5730 | Time 18.5214(18.2741) | Bit/dim 3.6460(3.6422) | Xent 0.5704(0.5653) | Loss 8.9315(9.5631) | Error 0.1989(0.2018) Steps 712(719.19) | Grad Norm 11.1478(8.4174) | Total Time 0.00(0.00)\n",
      "Iter 5740 | Time 18.5534(18.3167) | Bit/dim 3.6743(3.6434) | Xent 0.5619(0.5647) | Loss 8.9580(9.3932) | Error 0.2100(0.2005) Steps 700(718.60) | Grad Norm 9.2279(8.3465) | Total Time 0.00(0.00)\n",
      "Iter 5750 | Time 18.7158(18.3707) | Bit/dim 3.6342(3.6443) | Xent 0.6110(0.5663) | Loss 8.9971(9.2670) | Error 0.2122(0.2007) Steps 694(719.56) | Grad Norm 8.0024(8.1605) | Total Time 0.00(0.00)\n",
      "Iter 5760 | Time 18.6640(18.4204) | Bit/dim 3.6718(3.6449) | Xent 0.5540(0.5626) | Loss 9.1096(9.1757) | Error 0.1900(0.1998) Steps 718(722.24) | Grad Norm 10.6789(8.0311) | Total Time 0.00(0.00)\n",
      "Iter 5770 | Time 17.5893(18.3616) | Bit/dim 3.6737(3.6474) | Xent 0.5924(0.5664) | Loss 9.0549(9.1165) | Error 0.2189(0.2017) Steps 718(722.39) | Grad Norm 7.0012(8.5172) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0105 | Time 89.1969, Epoch Time 1119.4412(1006.6368), Bit/dim 3.6419(best: 3.6474), Xent 0.6745, Loss 3.9791, Error 0.2359(best: 0.2358)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5780 | Time 18.1722(18.3673) | Bit/dim 3.6760(3.6466) | Xent 0.5357(0.5650) | Loss 8.9362(9.6867) | Error 0.1856(0.2011) Steps 700(719.93) | Grad Norm 7.0442(8.2663) | Total Time 0.00(0.00)\n",
      "Iter 5790 | Time 17.5975(18.3414) | Bit/dim 3.6482(3.6484) | Xent 0.4924(0.5519) | Loss 8.7973(9.4644) | Error 0.1678(0.1951) Steps 718(719.90) | Grad Norm 6.8085(7.7936) | Total Time 0.00(0.00)\n",
      "Iter 5800 | Time 19.0929(18.3053) | Bit/dim 3.6822(3.6475) | Xent 0.5346(0.5503) | Loss 8.9855(9.3157) | Error 0.1922(0.1945) Steps 730(720.26) | Grad Norm 9.4191(7.7085) | Total Time 0.00(0.00)\n",
      "Iter 5810 | Time 18.0372(18.3087) | Bit/dim 3.6172(3.6455) | Xent 0.5805(0.5510) | Loss 8.9945(9.2103) | Error 0.2178(0.1950) Steps 736(719.30) | Grad Norm 10.7330(7.6455) | Total Time 0.00(0.00)\n",
      "Iter 5820 | Time 17.6550(18.3298) | Bit/dim 3.6145(3.6449) | Xent 0.5324(0.5524) | Loss 8.8501(9.1312) | Error 0.1756(0.1958) Steps 694(720.36) | Grad Norm 11.5874(7.5762) | Total Time 0.00(0.00)\n",
      "Iter 5830 | Time 18.1196(18.3015) | Bit/dim 3.6447(3.6456) | Xent 0.5623(0.5515) | Loss 8.9844(9.0678) | Error 0.1956(0.1948) Steps 718(720.84) | Grad Norm 8.1713(7.6952) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0106 | Time 89.7408, Epoch Time 1112.9883(1009.8274), Bit/dim 3.6489(best: 3.6419), Xent 0.6599, Loss 3.9788, Error 0.2265(best: 0.2358)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5840 | Time 17.6863(18.3084) | Bit/dim 3.6620(3.6471) | Xent 0.5846(0.5489) | Loss 8.9518(9.5637) | Error 0.1989(0.1941) Steps 724(722.69) | Grad Norm 8.9136(7.7905) | Total Time 0.00(0.00)\n",
      "Iter 5850 | Time 18.2715(18.3325) | Bit/dim 3.6186(3.6452) | Xent 0.5810(0.5542) | Loss 8.7857(9.3911) | Error 0.1989(0.1955) Steps 706(722.24) | Grad Norm 10.1243(8.6884) | Total Time 0.00(0.00)\n",
      "Iter 5860 | Time 18.3875(18.4213) | Bit/dim 3.6412(3.6487) | Xent 0.5795(0.5676) | Loss 9.0135(9.2891) | Error 0.2189(0.2005) Steps 724(727.07) | Grad Norm 8.2645(8.9261) | Total Time 0.00(0.00)\n",
      "Iter 5870 | Time 18.6636(18.4371) | Bit/dim 3.6389(3.6478) | Xent 0.5620(0.5720) | Loss 8.9210(9.1954) | Error 0.1989(0.2018) Steps 742(724.56) | Grad Norm 5.4616(8.7237) | Total Time 0.00(0.00)\n",
      "Iter 5880 | Time 17.8053(18.4598) | Bit/dim 3.6377(3.6474) | Xent 0.5654(0.5725) | Loss 8.9503(9.1161) | Error 0.1967(0.2027) Steps 724(724.36) | Grad Norm 5.4164(8.1815) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0107 | Time 88.7480, Epoch Time 1123.8058(1013.2467), Bit/dim 3.6470(best: 3.6419), Xent 0.6653, Loss 3.9796, Error 0.2310(best: 0.2265)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5890 | Time 19.0204(18.4864) | Bit/dim 3.6248(3.6438) | Xent 0.5037(0.5626) | Loss 8.9258(9.6768) | Error 0.1833(0.1994) Steps 724(727.03) | Grad Norm 6.6467(7.7140) | Total Time 0.00(0.00)\n",
      "Iter 5900 | Time 18.8942(18.4960) | Bit/dim 3.6486(3.6423) | Xent 0.5756(0.5599) | Loss 8.9483(9.4672) | Error 0.2033(0.1992) Steps 712(726.70) | Grad Norm 13.0376(8.1236) | Total Time 0.00(0.00)\n",
      "Iter 5910 | Time 19.3884(18.6069) | Bit/dim 3.6855(3.6436) | Xent 0.5982(0.5597) | Loss 9.1027(9.3194) | Error 0.2111(0.1988) Steps 766(727.45) | Grad Norm 12.7308(8.5775) | Total Time 0.00(0.00)\n",
      "Iter 5920 | Time 18.4011(18.6667) | Bit/dim 3.6602(3.6440) | Xent 0.4924(0.5523) | Loss 8.9570(9.2075) | Error 0.1844(0.1958) Steps 718(730.35) | Grad Norm 7.2488(8.4066) | Total Time 0.00(0.00)\n",
      "Iter 5930 | Time 18.6116(18.6691) | Bit/dim 3.6206(3.6428) | Xent 0.5520(0.5484) | Loss 8.8103(9.1205) | Error 0.1978(0.1945) Steps 754(729.85) | Grad Norm 7.9505(8.2174) | Total Time 0.00(0.00)\n",
      "Iter 5940 | Time 19.0177(18.6332) | Bit/dim 3.6513(3.6433) | Xent 0.6120(0.5629) | Loss 9.0440(9.0796) | Error 0.2278(0.1994) Steps 748(729.70) | Grad Norm 16.3959(8.6883) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0108 | Time 89.9470, Epoch Time 1134.7335(1016.8913), Bit/dim 3.6569(best: 3.6419), Xent 0.7117, Loss 4.0128, Error 0.2467(best: 0.2265)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5950 | Time 18.1628(18.5796) | Bit/dim 3.6378(3.6450) | Xent 0.5610(0.5615) | Loss 8.8911(9.5730) | Error 0.1989(0.1977) Steps 712(728.29) | Grad Norm 9.6158(8.8122) | Total Time 0.00(0.00)\n",
      "Iter 5960 | Time 17.9735(18.5418) | Bit/dim 3.6209(3.6442) | Xent 0.5533(0.5569) | Loss 8.9475(9.3948) | Error 0.2000(0.1970) Steps 736(729.91) | Grad Norm 5.9610(8.4534) | Total Time 0.00(0.00)\n",
      "Iter 5970 | Time 17.8271(18.5603) | Bit/dim 3.6453(3.6408) | Xent 0.4994(0.5498) | Loss 8.8053(9.2521) | Error 0.1789(0.1954) Steps 682(727.07) | Grad Norm 5.8670(8.2330) | Total Time 0.00(0.00)\n",
      "Iter 5980 | Time 18.8561(18.6079) | Bit/dim 3.6670(3.6435) | Xent 0.5325(0.5486) | Loss 8.9416(9.1683) | Error 0.1889(0.1948) Steps 760(731.13) | Grad Norm 5.1459(8.4726) | Total Time 0.00(0.00)\n",
      "Iter 5990 | Time 18.6763(18.6563) | Bit/dim 3.6551(3.6428) | Xent 0.5670(0.5501) | Loss 8.8121(9.0858) | Error 0.1922(0.1948) Steps 730(727.96) | Grad Norm 9.3694(8.4262) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0109 | Time 89.6591, Epoch Time 1132.4768(1020.3589), Bit/dim 3.6448(best: 3.6419), Xent 0.6899, Loss 3.9897, Error 0.2313(best: 0.2265)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6000 | Time 19.7064(18.6965) | Bit/dim 3.6624(3.6441) | Xent 0.5564(0.5511) | Loss 8.9863(9.6681) | Error 0.1911(0.1962) Steps 766(732.12) | Grad Norm 9.5306(8.9105) | Total Time 0.00(0.00)\n",
      "Iter 6010 | Time 18.3577(18.6665) | Bit/dim 3.6604(3.6456) | Xent 0.5645(0.5506) | Loss 8.9659(9.4698) | Error 0.2044(0.1965) Steps 742(733.52) | Grad Norm 12.4113(9.0013) | Total Time 0.00(0.00)\n",
      "Iter 6020 | Time 19.1023(18.6779) | Bit/dim 3.6262(3.6436) | Xent 0.5227(0.5512) | Loss 8.9357(9.3195) | Error 0.2089(0.1972) Steps 718(734.19) | Grad Norm 6.7552(9.0188) | Total Time 0.00(0.00)\n",
      "Iter 6030 | Time 19.0826(18.6859) | Bit/dim 3.6024(3.6390) | Xent 0.5309(0.5437) | Loss 8.8464(9.1955) | Error 0.1878(0.1949) Steps 742(734.26) | Grad Norm 6.6769(8.5394) | Total Time 0.00(0.00)\n",
      "Iter 6040 | Time 20.3755(18.7212) | Bit/dim 3.6174(3.6379) | Xent 0.5598(0.5423) | Loss 8.7868(9.1131) | Error 0.1922(0.1931) Steps 790(734.43) | Grad Norm 6.7704(8.1671) | Total Time 0.00(0.00)\n",
      "Iter 6050 | Time 19.4344(18.6846) | Bit/dim 3.6315(3.6407) | Xent 0.5647(0.5459) | Loss 8.9479(9.0681) | Error 0.2089(0.1954) Steps 688(730.14) | Grad Norm 7.3435(7.9776) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0110 | Time 90.6052, Epoch Time 1135.2040(1023.8042), Bit/dim 3.6453(best: 3.6419), Xent 0.6795, Loss 3.9850, Error 0.2324(best: 0.2265)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6060 | Time 18.9514(18.6676) | Bit/dim 3.6274(3.6405) | Xent 0.5335(0.5475) | Loss 8.9338(9.5960) | Error 0.1933(0.1955) Steps 736(730.28) | Grad Norm 5.9062(8.3017) | Total Time 0.00(0.00)\n",
      "Iter 6070 | Time 19.0333(18.6473) | Bit/dim 3.6718(3.6443) | Xent 0.4887(0.5538) | Loss 8.9595(9.4261) | Error 0.1889(0.1975) Steps 748(732.63) | Grad Norm 10.4068(9.1286) | Total Time 0.00(0.00)\n",
      "Iter 6080 | Time 18.7745(18.6169) | Bit/dim 3.6393(3.6468) | Xent 0.5179(0.5546) | Loss 8.9394(9.3002) | Error 0.1889(0.1973) Steps 778(733.51) | Grad Norm 7.2833(9.1401) | Total Time 0.00(0.00)\n",
      "Iter 6090 | Time 19.2483(18.6384) | Bit/dim 3.6357(3.6445) | Xent 0.5460(0.5513) | Loss 8.9402(9.1846) | Error 0.1956(0.1958) Steps 772(731.83) | Grad Norm 8.5699(8.5727) | Total Time 0.00(0.00)\n",
      "Iter 6100 | Time 19.6226(18.7241) | Bit/dim 3.6595(3.6407) | Xent 0.5057(0.5421) | Loss 8.9576(9.0952) | Error 0.1789(0.1927) Steps 706(732.37) | Grad Norm 5.6644(7.8005) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0111 | Time 88.6839, Epoch Time 1130.4385(1027.0033), Bit/dim 3.6357(best: 3.6419), Xent 0.6921, Loss 3.9818, Error 0.2408(best: 0.2265)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6110 | Time 19.0432(18.6790) | Bit/dim 3.6061(3.6385) | Xent 0.5083(0.5433) | Loss 8.8698(9.6912) | Error 0.1800(0.1933) Steps 754(732.85) | Grad Norm 11.0072(8.2164) | Total Time 0.00(0.00)\n",
      "Iter 6120 | Time 18.7129(18.6730) | Bit/dim 3.6246(3.6380) | Xent 0.4839(0.5356) | Loss 8.9096(9.4721) | Error 0.1789(0.1901) Steps 754(734.86) | Grad Norm 6.7256(8.3143) | Total Time 0.00(0.00)\n",
      "Iter 6130 | Time 18.0610(18.7040) | Bit/dim 3.6345(3.6378) | Xent 0.5434(0.5370) | Loss 8.8263(9.3178) | Error 0.1989(0.1904) Steps 712(734.99) | Grad Norm 6.7192(8.3547) | Total Time 0.00(0.00)\n",
      "Iter 6140 | Time 18.7609(18.7371) | Bit/dim 3.6646(3.6383) | Xent 0.5297(0.5335) | Loss 8.8786(9.1981) | Error 0.1744(0.1892) Steps 724(735.09) | Grad Norm 4.8000(7.7651) | Total Time 0.00(0.00)\n",
      "Iter 6150 | Time 18.6837(18.7597) | Bit/dim 3.6178(3.6356) | Xent 0.5670(0.5333) | Loss 8.9593(9.1209) | Error 0.1978(0.1888) Steps 730(736.48) | Grad Norm 12.1082(7.8477) | Total Time 0.00(0.00)\n",
      "Iter 6160 | Time 18.3829(18.6382) | Bit/dim 3.6352(3.6371) | Xent 0.5691(0.5429) | Loss 8.9172(9.0625) | Error 0.2089(0.1916) Steps 754(734.60) | Grad Norm 10.5190(8.4020) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0112 | Time 88.9583, Epoch Time 1134.5319(1030.2291), Bit/dim 3.6427(best: 3.6357), Xent 0.6922, Loss 3.9888, Error 0.2371(best: 0.2265)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6170 | Time 18.0465(18.7007) | Bit/dim 3.6753(3.6427) | Xent 0.5539(0.5483) | Loss 8.9123(9.5778) | Error 0.1944(0.1931) Steps 724(738.12) | Grad Norm 12.2431(8.8195) | Total Time 0.00(0.00)\n",
      "Iter 6180 | Time 18.9082(18.7455) | Bit/dim 3.6241(3.6391) | Xent 0.5058(0.5437) | Loss 8.8329(9.3843) | Error 0.1733(0.1916) Steps 742(737.14) | Grad Norm 5.2426(8.3888) | Total Time 0.00(0.00)\n",
      "Iter 6190 | Time 19.6479(18.7609) | Bit/dim 3.6336(3.6380) | Xent 0.5105(0.5356) | Loss 8.9350(9.2601) | Error 0.1711(0.1886) Steps 766(739.81) | Grad Norm 7.1727(8.3080) | Total Time 0.00(0.00)\n",
      "Iter 6200 | Time 19.5963(18.7580) | Bit/dim 3.6191(3.6397) | Xent 0.5951(0.5426) | Loss 8.9406(9.1853) | Error 0.2122(0.1917) Steps 736(741.13) | Grad Norm 8.4777(8.6446) | Total Time 0.00(0.00)\n",
      "Iter 6210 | Time 18.5816(18.6993) | Bit/dim 3.6516(3.6383) | Xent 0.5291(0.5491) | Loss 8.8754(9.1073) | Error 0.1778(0.1940) Steps 760(741.24) | Grad Norm 5.5771(8.1748) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0113 | Time 89.0013, Epoch Time 1136.9360(1033.4303), Bit/dim 3.6395(best: 3.6357), Xent 0.6500, Loss 3.9645, Error 0.2213(best: 0.2265)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6220 | Time 18.7844(18.6844) | Bit/dim 3.6262(3.6411) | Xent 0.5087(0.5382) | Loss 8.6391(9.6690) | Error 0.1744(0.1908) Steps 736(744.54) | Grad Norm 7.6176(8.0475) | Total Time 0.00(0.00)\n",
      "Iter 6230 | Time 19.5092(18.7290) | Bit/dim 3.6823(3.6412) | Xent 0.4981(0.5368) | Loss 8.8789(9.4716) | Error 0.1767(0.1906) Steps 742(746.13) | Grad Norm 9.9186(8.3958) | Total Time 0.00(0.00)\n",
      "Iter 6240 | Time 18.5339(18.7738) | Bit/dim 3.6262(3.6409) | Xent 0.5272(0.5373) | Loss 8.8835(9.3220) | Error 0.1744(0.1911) Steps 760(748.92) | Grad Norm 7.2437(8.2742) | Total Time 0.00(0.00)\n",
      "Iter 6250 | Time 18.8243(18.7611) | Bit/dim 3.6062(3.6388) | Xent 0.4760(0.5307) | Loss 8.7506(9.1979) | Error 0.1622(0.1873) Steps 766(747.25) | Grad Norm 4.5989(8.1001) | Total Time 0.00(0.00)\n",
      "Iter 6260 | Time 18.6853(18.8236) | Bit/dim 3.6103(3.6366) | Xent 0.5072(0.5294) | Loss 8.8730(9.1149) | Error 0.1756(0.1870) Steps 736(746.89) | Grad Norm 6.0181(7.6979) | Total Time 0.00(0.00)\n",
      "Iter 6270 | Time 18.2364(18.8032) | Bit/dim 3.6435(3.6360) | Xent 0.5436(0.5316) | Loss 8.9636(9.0585) | Error 0.1922(0.1875) Steps 766(744.92) | Grad Norm 7.3789(7.7955) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0114 | Time 89.7098, Epoch Time 1142.2742(1036.6956), Bit/dim 3.6381(best: 3.6357), Xent 0.6733, Loss 3.9748, Error 0.2304(best: 0.2213)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6280 | Time 18.3623(18.7776) | Bit/dim 3.6499(3.6353) | Xent 0.5363(0.5234) | Loss 8.9441(9.5590) | Error 0.1956(0.1846) Steps 742(744.43) | Grad Norm 14.3470(7.9346) | Total Time 0.00(0.00)\n",
      "Iter 6290 | Time 19.0310(18.7591) | Bit/dim 3.6105(3.6326) | Xent 0.4742(0.5190) | Loss 8.6852(9.3637) | Error 0.1589(0.1837) Steps 772(746.33) | Grad Norm 6.4978(7.8538) | Total Time 0.00(0.00)\n",
      "Iter 6300 | Time 17.9770(18.7422) | Bit/dim 3.6502(3.6344) | Xent 0.5505(0.5233) | Loss 8.8297(9.2462) | Error 0.2011(0.1867) Steps 712(746.23) | Grad Norm 10.3941(7.9962) | Total Time 0.00(0.00)\n",
      "Iter 6310 | Time 20.1767(18.8101) | Bit/dim 3.6443(3.6335) | Xent 0.5791(0.5313) | Loss 8.9380(9.1567) | Error 0.2089(0.1893) Steps 760(746.04) | Grad Norm 15.9630(8.6663) | Total Time 0.00(0.00)\n",
      "Iter 6320 | Time 18.7134(18.8122) | Bit/dim 3.6664(3.6364) | Xent 0.5469(0.5350) | Loss 8.9371(9.0926) | Error 0.1944(0.1910) Steps 748(744.77) | Grad Norm 7.4014(8.8169) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0115 | Time 89.2260, Epoch Time 1140.2740(1039.8030), Bit/dim 3.6424(best: 3.6357), Xent 0.6695, Loss 3.9771, Error 0.2282(best: 0.2213)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6330 | Time 17.9256(18.7986) | Bit/dim 3.6312(3.6375) | Xent 0.4644(0.5293) | Loss 8.8708(9.6957) | Error 0.1644(0.1890) Steps 730(745.89) | Grad Norm 5.0400(8.5596) | Total Time 0.00(0.00)\n",
      "Iter 6340 | Time 18.5213(18.7985) | Bit/dim 3.6589(3.6369) | Xent 0.4706(0.5181) | Loss 8.7795(9.4732) | Error 0.1711(0.1851) Steps 730(745.87) | Grad Norm 8.8381(8.4259) | Total Time 0.00(0.00)\n",
      "Iter 6350 | Time 19.3612(18.8074) | Bit/dim 3.6727(3.6409) | Xent 0.4923(0.5168) | Loss 8.8739(9.3234) | Error 0.1867(0.1841) Steps 718(743.47) | Grad Norm 9.9000(8.6908) | Total Time 0.00(0.00)\n",
      "Iter 6360 | Time 19.6451(18.8681) | Bit/dim 3.6484(3.6428) | Xent 0.6071(0.5232) | Loss 9.0115(9.2210) | Error 0.2056(0.1857) Steps 748(744.64) | Grad Norm 13.2419(8.8210) | Total Time 0.00(0.00)\n",
      "Iter 6370 | Time 18.2492(18.8835) | Bit/dim 3.6413(3.6399) | Xent 0.5413(0.5335) | Loss 8.8662(9.1412) | Error 0.2022(0.1883) Steps 748(744.57) | Grad Norm 12.3110(9.4741) | Total Time 0.00(0.00)\n",
      "Iter 6380 | Time 18.7214(18.9261) | Bit/dim 3.6520(3.6407) | Xent 0.5260(0.5319) | Loss 9.0190(9.0825) | Error 0.1889(0.1889) Steps 766(750.42) | Grad Norm 7.2445(9.0419) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0116 | Time 89.8086, Epoch Time 1145.9708(1042.9880), Bit/dim 3.6478(best: 3.6357), Xent 0.7019, Loss 3.9987, Error 0.2335(best: 0.2213)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6390 | Time 18.0925(18.9490) | Bit/dim 3.6324(3.6380) | Xent 0.5130(0.5265) | Loss 8.8558(9.5540) | Error 0.1833(0.1867) Steps 730(751.59) | Grad Norm 9.5675(9.1389) | Total Time 0.00(0.00)\n",
      "Iter 6400 | Time 19.1534(18.9228) | Bit/dim 3.6739(3.6384) | Xent 0.4659(0.5198) | Loss 8.9021(9.3705) | Error 0.1444(0.1831) Steps 760(751.81) | Grad Norm 11.9287(8.6680) | Total Time 0.00(0.00)\n",
      "Iter 6410 | Time 18.7761(18.9213) | Bit/dim 3.6681(3.6374) | Xent 0.4682(0.5118) | Loss 8.8172(9.2354) | Error 0.1800(0.1808) Steps 748(749.51) | Grad Norm 6.2580(8.2085) | Total Time 0.00(0.00)\n",
      "Iter 6420 | Time 18.2340(18.9418) | Bit/dim 3.6655(3.6344) | Xent 0.5958(0.5104) | Loss 8.8882(9.1336) | Error 0.2156(0.1805) Steps 718(751.07) | Grad Norm 12.6635(8.4629) | Total Time 0.00(0.00)\n",
      "Iter 6430 | Time 19.3369(18.8633) | Bit/dim 3.6391(3.6350) | Xent 0.5218(0.5179) | Loss 8.9121(9.0639) | Error 0.1756(0.1818) Steps 790(749.20) | Grad Norm 7.1799(8.6621) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0117 | Time 90.7053, Epoch Time 1144.7424(1046.0407), Bit/dim 3.6376(best: 3.6357), Xent 0.6858, Loss 3.9805, Error 0.2388(best: 0.2213)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6440 | Time 19.5653(18.8487) | Bit/dim 3.6698(3.6393) | Xent 0.4786(0.5178) | Loss 8.9533(9.6756) | Error 0.1744(0.1824) Steps 772(751.75) | Grad Norm 9.3347(8.7346) | Total Time 0.00(0.00)\n",
      "Iter 6450 | Time 18.4371(18.8484) | Bit/dim 3.6346(3.6377) | Xent 0.5711(0.5175) | Loss 8.8071(9.4593) | Error 0.2089(0.1824) Steps 748(754.69) | Grad Norm 10.8240(8.5162) | Total Time 0.00(0.00)\n",
      "Iter 6460 | Time 18.4371(18.8060) | Bit/dim 3.6398(3.6367) | Xent 0.5388(0.5131) | Loss 8.8244(9.2888) | Error 0.1900(0.1815) Steps 766(752.32) | Grad Norm 9.1184(8.2311) | Total Time 0.00(0.00)\n",
      "Iter 6470 | Time 18.7717(18.8656) | Bit/dim 3.6062(3.6345) | Xent 0.6152(0.5146) | Loss 9.0474(9.1887) | Error 0.2067(0.1816) Steps 736(752.71) | Grad Norm 18.8089(8.7761) | Total Time 0.00(0.00)\n",
      "Iter 6480 | Time 18.6861(18.9260) | Bit/dim 3.6444(3.6372) | Xent 0.5817(0.5310) | Loss 9.0465(9.1303) | Error 0.2144(0.1887) Steps 772(754.09) | Grad Norm 7.6637(8.8160) | Total Time 0.00(0.00)\n",
      "Iter 6490 | Time 18.9045(18.9134) | Bit/dim 3.6805(3.6381) | Xent 0.4980(0.5310) | Loss 8.9466(9.0664) | Error 0.1789(0.1889) Steps 754(750.68) | Grad Norm 6.9041(8.2797) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0118 | Time 89.2425, Epoch Time 1147.2888(1049.0781), Bit/dim 3.6400(best: 3.6357), Xent 0.6720, Loss 3.9760, Error 0.2243(best: 0.2213)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6500 | Time 18.8422(18.9235) | Bit/dim 3.6713(3.6401) | Xent 0.5040(0.5197) | Loss 8.9619(9.5670) | Error 0.1889(0.1856) Steps 766(750.38) | Grad Norm 6.0475(8.0302) | Total Time 0.00(0.00)\n",
      "Iter 6510 | Time 18.6497(19.0262) | Bit/dim 3.6391(3.6387) | Xent 0.5039(0.5139) | Loss 8.9478(9.3870) | Error 0.1889(0.1836) Steps 736(753.11) | Grad Norm 15.2843(7.9766) | Total Time 0.00(0.00)\n",
      "Iter 6520 | Time 18.5789(19.0228) | Bit/dim 3.6011(3.6364) | Xent 0.4862(0.5095) | Loss 8.7270(9.2444) | Error 0.1711(0.1818) Steps 712(751.52) | Grad Norm 5.4860(7.5387) | Total Time 0.00(0.00)\n",
      "Iter 6530 | Time 18.3231(18.9858) | Bit/dim 3.6024(3.6333) | Xent 0.4940(0.5072) | Loss 8.9074(9.1354) | Error 0.1833(0.1811) Steps 754(750.15) | Grad Norm 11.3011(7.5590) | Total Time 0.00(0.00)\n",
      "Iter 6540 | Time 19.5381(19.0542) | Bit/dim 3.6261(3.6322) | Xent 0.5207(0.5129) | Loss 8.8808(9.0690) | Error 0.1778(0.1831) Steps 736(748.13) | Grad Norm 13.0081(8.1507) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0119 | Time 90.5656, Epoch Time 1158.6864(1052.3663), Bit/dim 3.6415(best: 3.6357), Xent 0.7268, Loss 4.0049, Error 0.2436(best: 0.2213)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6550 | Time 18.9908(19.0496) | Bit/dim 3.6515(3.6369) | Xent 0.5745(0.5183) | Loss 9.1075(9.6915) | Error 0.2011(0.1850) Steps 754(749.03) | Grad Norm 9.5217(8.2960) | Total Time 0.00(0.00)\n",
      "Iter 6560 | Time 18.7138(19.0339) | Bit/dim 3.6423(3.6356) | Xent 0.6581(0.5261) | Loss 8.9402(9.4844) | Error 0.2344(0.1882) Steps 760(755.77) | Grad Norm 13.8379(9.3573) | Total Time 0.00(0.00)\n",
      "Iter 6570 | Time 20.0062(19.0012) | Bit/dim 3.6677(3.6381) | Xent 0.5092(0.5336) | Loss 8.9641(9.3434) | Error 0.1667(0.1891) Steps 736(755.95) | Grad Norm 7.1979(9.4706) | Total Time 0.00(0.00)\n",
      "Iter 6580 | Time 18.7719(18.9248) | Bit/dim 3.6292(3.6364) | Xent 0.5357(0.5257) | Loss 8.9704(9.2138) | Error 0.1922(0.1859) Steps 712(752.35) | Grad Norm 6.6296(8.7741) | Total Time 0.00(0.00)\n",
      "Iter 6590 | Time 19.2597(18.9806) | Bit/dim 3.6426(3.6352) | Xent 0.5047(0.5231) | Loss 8.8178(9.1209) | Error 0.1756(0.1835) Steps 712(753.58) | Grad Norm 6.8923(8.2373) | Total Time 0.00(0.00)\n",
      "Iter 6600 | Time 18.8783(18.9502) | Bit/dim 3.6271(3.6344) | Xent 0.4798(0.5267) | Loss 8.7882(9.0577) | Error 0.1756(0.1861) Steps 736(753.55) | Grad Norm 7.3418(8.2161) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0120 | Time 90.6422, Epoch Time 1147.5937(1055.2232), Bit/dim 3.6363(best: 3.6357), Xent 0.6909, Loss 3.9817, Error 0.2360(best: 0.2213)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6610 | Time 18.4343(18.9552) | Bit/dim 3.6329(3.6370) | Xent 0.4952(0.5209) | Loss 8.7062(9.5855) | Error 0.1789(0.1846) Steps 760(753.27) | Grad Norm 9.2824(8.3094) | Total Time 0.00(0.00)\n",
      "Iter 6620 | Time 18.6552(18.9958) | Bit/dim 3.6002(3.6345) | Xent 0.5210(0.5192) | Loss 8.8107(9.3961) | Error 0.1811(0.1841) Steps 760(757.16) | Grad Norm 6.2531(7.7067) | Total Time 0.00(0.00)\n",
      "Iter 6630 | Time 18.9337(19.0144) | Bit/dim 3.5985(3.6339) | Xent 0.4945(0.5162) | Loss 8.7607(9.2525) | Error 0.1722(0.1825) Steps 760(757.79) | Grad Norm 9.9827(7.6931) | Total Time 0.00(0.00)\n",
      "Iter 6640 | Time 19.8110(19.0018) | Bit/dim 3.6496(3.6307) | Xent 0.5053(0.5051) | Loss 8.9635(9.1302) | Error 0.1789(0.1785) Steps 820(758.83) | Grad Norm 7.7181(7.3643) | Total Time 0.00(0.00)\n",
      "Iter 6650 | Time 19.3295(18.9810) | Bit/dim 3.5908(3.6281) | Xent 0.4919(0.5021) | Loss 8.7792(9.0520) | Error 0.1767(0.1772) Steps 790(759.79) | Grad Norm 9.5511(7.4437) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0121 | Time 89.8886, Epoch Time 1153.4053(1058.1686), Bit/dim 3.6339(best: 3.6357), Xent 0.6479, Loss 3.9578, Error 0.2210(best: 0.2213)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6660 | Time 19.3176(19.1141) | Bit/dim 3.6242(3.6294) | Xent 0.4957(0.4968) | Loss 8.9139(9.6526) | Error 0.1689(0.1754) Steps 772(763.27) | Grad Norm 6.5851(7.3280) | Total Time 0.00(0.00)\n",
      "Iter 6670 | Time 18.6093(19.0353) | Bit/dim 3.6118(3.6290) | Xent 0.5430(0.4978) | Loss 8.7873(9.4420) | Error 0.1767(0.1760) Steps 730(759.42) | Grad Norm 6.2446(7.2674) | Total Time 0.00(0.00)\n",
      "Iter 6680 | Time 19.2713(19.0712) | Bit/dim 3.6387(3.6318) | Xent 0.4814(0.4963) | Loss 8.8401(9.2904) | Error 0.1622(0.1754) Steps 796(760.80) | Grad Norm 12.5544(7.4504) | Total Time 0.00(0.00)\n",
      "Iter 6690 | Time 17.8319(19.0557) | Bit/dim 3.6398(3.6330) | Xent 0.4674(0.4948) | Loss 8.7763(9.1776) | Error 0.1700(0.1742) Steps 736(760.74) | Grad Norm 7.0034(7.5285) | Total Time 0.00(0.00)\n",
      "Iter 6700 | Time 18.2145(19.0228) | Bit/dim 3.6264(3.6311) | Xent 0.4817(0.4918) | Loss 8.7821(9.0770) | Error 0.1644(0.1734) Steps 766(759.76) | Grad Norm 5.6154(7.1854) | Total Time 0.00(0.00)\n",
      "Iter 6710 | Time 18.0073(19.0073) | Bit/dim 3.6157(3.6307) | Xent 0.5158(0.4999) | Loss 8.6648(9.0285) | Error 0.1844(0.1754) Steps 718(757.72) | Grad Norm 8.3141(7.5980) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0122 | Time 90.3914, Epoch Time 1155.1859(1061.0792), Bit/dim 3.6241(best: 3.6339), Xent 0.6578, Loss 3.9530, Error 0.2300(best: 0.2210)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6720 | Time 18.7232(18.9983) | Bit/dim 3.6449(3.6294) | Xent 0.5348(0.4962) | Loss 9.0066(9.5354) | Error 0.1811(0.1752) Steps 766(758.30) | Grad Norm 15.5023(8.0621) | Total Time 0.00(0.00)\n",
      "Iter 6730 | Time 18.7725(19.0239) | Bit/dim 3.6194(3.6281) | Xent 0.4552(0.4896) | Loss 8.7101(9.3375) | Error 0.1711(0.1726) Steps 778(760.54) | Grad Norm 5.0061(8.0362) | Total Time 0.00(0.00)\n",
      "Iter 6740 | Time 19.5143(19.0301) | Bit/dim 3.6154(3.6263) | Xent 0.4978(0.4901) | Loss 8.9098(9.2158) | Error 0.1767(0.1724) Steps 772(761.32) | Grad Norm 5.9398(8.1337) | Total Time 0.00(0.00)\n",
      "Iter 6750 | Time 20.3204(19.1205) | Bit/dim 3.6193(3.6268) | Xent 0.4619(0.4908) | Loss 8.8099(9.1122) | Error 0.1633(0.1727) Steps 772(764.28) | Grad Norm 5.4615(7.8624) | Total Time 0.00(0.00)\n",
      "Iter 6760 | Time 18.3398(19.0624) | Bit/dim 3.6465(3.6291) | Xent 0.4267(0.4913) | Loss 8.7660(9.0345) | Error 0.1422(0.1730) Steps 730(761.63) | Grad Norm 7.9795(7.9058) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0123 | Time 90.7876, Epoch Time 1158.0265(1063.9876), Bit/dim 3.6388(best: 3.6241), Xent 0.6809, Loss 3.9792, Error 0.2273(best: 0.2210)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6770 | Time 18.9233(19.0750) | Bit/dim 3.6215(3.6278) | Xent 0.4836(0.4931) | Loss 8.9132(9.6230) | Error 0.1767(0.1746) Steps 772(762.00) | Grad Norm 7.5234(8.0046) | Total Time 0.00(0.00)\n",
      "Iter 6780 | Time 19.2881(19.0204) | Bit/dim 3.6219(3.6287) | Xent 0.4689(0.4888) | Loss 8.8493(9.4130) | Error 0.1611(0.1722) Steps 784(762.45) | Grad Norm 8.2442(7.8636) | Total Time 0.00(0.00)\n",
      "Iter 6790 | Time 18.2146(19.0236) | Bit/dim 3.6153(3.6273) | Xent 0.4469(0.4831) | Loss 8.8901(9.2619) | Error 0.1711(0.1714) Steps 766(763.34) | Grad Norm 13.4477(7.9488) | Total Time 0.00(0.00)\n",
      "Iter 6800 | Time 19.6334(19.0800) | Bit/dim 3.6008(3.6280) | Xent 0.4891(0.4846) | Loss 8.7772(9.1478) | Error 0.1778(0.1710) Steps 778(764.53) | Grad Norm 7.2909(8.0473) | Total Time 0.00(0.00)\n",
      "Iter 6810 | Time 19.6726(19.2144) | Bit/dim 3.6576(3.6280) | Xent 0.4720(0.4879) | Loss 8.9440(9.0728) | Error 0.1711(0.1722) Steps 772(766.01) | Grad Norm 10.7829(8.5820) | Total Time 0.00(0.00)\n",
      "Iter 6820 | Time 19.3814(19.2509) | Bit/dim 3.6199(3.6252) | Xent 0.5329(0.4878) | Loss 8.8740(9.0094) | Error 0.1833(0.1724) Steps 772(763.44) | Grad Norm 9.4699(8.4405) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0124 | Time 91.0618, Epoch Time 1164.3895(1066.9996), Bit/dim 3.6274(best: 3.6241), Xent 0.6959, Loss 3.9754, Error 0.2383(best: 0.2210)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6830 | Time 18.8202(19.2461) | Bit/dim 3.6250(3.6267) | Xent 0.4915(0.4960) | Loss 8.8365(9.5305) | Error 0.1778(0.1750) Steps 754(761.11) | Grad Norm 7.2240(8.6642) | Total Time 0.00(0.00)\n",
      "Iter 6840 | Time 18.8819(19.1980) | Bit/dim 3.6469(3.6245) | Xent 0.4425(0.4901) | Loss 8.6753(9.3324) | Error 0.1533(0.1727) Steps 718(762.57) | Grad Norm 5.8265(8.1910) | Total Time 0.00(0.00)\n",
      "Iter 6850 | Time 18.9611(19.1788) | Bit/dim 3.6585(3.6245) | Xent 0.4249(0.4863) | Loss 8.8076(9.2084) | Error 0.1589(0.1726) Steps 730(763.65) | Grad Norm 10.9601(8.1387) | Total Time 0.00(0.00)\n",
      "Iter 6860 | Time 19.1893(19.1783) | Bit/dim 3.6036(3.6258) | Xent 0.4987(0.4798) | Loss 8.7676(9.1047) | Error 0.1744(0.1707) Steps 754(760.60) | Grad Norm 6.9682(7.6321) | Total Time 0.00(0.00)\n",
      "Iter 6870 | Time 18.4203(19.0685) | Bit/dim 3.6120(3.6256) | Xent 0.4900(0.4765) | Loss 8.8002(9.0214) | Error 0.1833(0.1701) Steps 748(757.46) | Grad Norm 8.9869(7.5981) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0125 | Time 91.3156, Epoch Time 1157.3134(1069.7090), Bit/dim 3.6302(best: 3.6241), Xent 0.6607, Loss 3.9606, Error 0.2243(best: 0.2210)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6880 | Time 19.0228(19.0820) | Bit/dim 3.6079(3.6246) | Xent 0.5314(0.4807) | Loss 8.8287(9.6362) | Error 0.1944(0.1714) Steps 736(757.45) | Grad Norm 9.4274(7.9371) | Total Time 0.00(0.00)\n",
      "Iter 6890 | Time 19.7637(19.0898) | Bit/dim 3.6100(3.6228) | Xent 0.4905(0.4766) | Loss 8.7550(9.4089) | Error 0.1756(0.1702) Steps 754(756.04) | Grad Norm 9.7731(7.5703) | Total Time 0.00(0.00)\n",
      "Iter 6900 | Time 18.9399(19.1207) | Bit/dim 3.6318(3.6218) | Xent 0.4877(0.4777) | Loss 8.8147(9.2500) | Error 0.1656(0.1695) Steps 778(759.66) | Grad Norm 7.6143(7.5379) | Total Time 0.00(0.00)\n",
      "Iter 6910 | Time 19.9956(19.0717) | Bit/dim 3.6183(3.6239) | Xent 0.4576(0.4762) | Loss 8.8734(9.1434) | Error 0.1456(0.1683) Steps 790(760.03) | Grad Norm 7.7825(7.7151) | Total Time 0.00(0.00)\n",
      "Iter 6920 | Time 19.0548(19.1175) | Bit/dim 3.6192(3.6275) | Xent 0.4276(0.4724) | Loss 8.8111(9.0621) | Error 0.1511(0.1672) Steps 742(758.26) | Grad Norm 9.0874(7.7371) | Total Time 0.00(0.00)\n",
      "Iter 6930 | Time 18.6221(19.1266) | Bit/dim 3.6422(3.6268) | Xent 0.5348(0.4775) | Loss 8.9806(9.0000) | Error 0.1911(0.1694) Steps 772(758.49) | Grad Norm 12.8530(8.2283) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0126 | Time 89.8939, Epoch Time 1159.3951(1072.3996), Bit/dim 3.6228(best: 3.6241), Xent 0.6289, Loss 3.9373, Error 0.2165(best: 0.2210)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6940 | Time 19.6564(19.1427) | Bit/dim 3.6320(3.6288) | Xent 0.4728(0.4752) | Loss 8.8663(9.4969) | Error 0.1833(0.1689) Steps 784(759.17) | Grad Norm 9.9007(8.2645) | Total Time 0.00(0.00)\n",
      "Iter 6950 | Time 19.4687(19.2124) | Bit/dim 3.5908(3.6266) | Xent 0.4838(0.4802) | Loss 8.6733(9.3257) | Error 0.1722(0.1718) Steps 790(763.96) | Grad Norm 10.6483(8.8234) | Total Time 0.00(0.00)\n",
      "Iter 6960 | Time 19.8794(19.2890) | Bit/dim 3.6304(3.6283) | Xent 0.4662(0.4775) | Loss 8.7995(9.1994) | Error 0.1689(0.1703) Steps 772(762.92) | Grad Norm 8.8140(8.7015) | Total Time 0.00(0.00)\n",
      "Iter 6970 | Time 19.6312(19.2914) | Bit/dim 3.6673(3.6273) | Xent 0.4763(0.4747) | Loss 8.9552(9.1019) | Error 0.1711(0.1685) Steps 748(763.24) | Grad Norm 9.2747(8.5165) | Total Time 0.00(0.00)\n",
      "Iter 6980 | Time 18.9204(19.2657) | Bit/dim 3.6348(3.6265) | Xent 0.4883(0.4774) | Loss 8.8095(9.0223) | Error 0.1656(0.1689) Steps 748(760.28) | Grad Norm 5.0326(8.3035) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0127 | Time 91.4580, Epoch Time 1167.8880(1075.2643), Bit/dim 3.6295(best: 3.6228), Xent 0.6433, Loss 3.9511, Error 0.2191(best: 0.2165)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6990 | Time 18.5939(19.2184) | Bit/dim 3.6367(3.6266) | Xent 0.4444(0.4762) | Loss 8.8464(9.6246) | Error 0.1611(0.1688) Steps 760(760.83) | Grad Norm 4.4745(8.2241) | Total Time 0.00(0.00)\n",
      "Iter 7000 | Time 19.3335(19.2197) | Bit/dim 3.6580(3.6285) | Xent 0.4406(0.4670) | Loss 8.9851(9.4158) | Error 0.1433(0.1638) Steps 796(764.41) | Grad Norm 6.8540(7.7969) | Total Time 0.00(0.00)\n",
      "Iter 7010 | Time 18.3901(19.2588) | Bit/dim 3.6285(3.6268) | Xent 0.5140(0.4637) | Loss 8.8273(9.2461) | Error 0.1811(0.1632) Steps 766(767.17) | Grad Norm 10.8497(7.5670) | Total Time 0.00(0.00)\n",
      "Iter 7020 | Time 19.6341(19.3020) | Bit/dim 3.6456(3.6266) | Xent 0.4787(0.4740) | Loss 8.9485(9.1444) | Error 0.1656(0.1679) Steps 784(769.66) | Grad Norm 6.7707(8.3915) | Total Time 0.00(0.00)\n",
      "Iter 7030 | Time 19.0709(19.2835) | Bit/dim 3.6408(3.6280) | Xent 0.4770(0.4779) | Loss 8.8799(9.0611) | Error 0.1700(0.1683) Steps 778(768.32) | Grad Norm 10.7437(8.3868) | Total Time 0.00(0.00)\n",
      "Iter 7040 | Time 19.5762(19.2406) | Bit/dim 3.6373(3.6263) | Xent 0.5083(0.4852) | Loss 9.0059(9.0187) | Error 0.1767(0.1716) Steps 808(770.20) | Grad Norm 9.0164(8.6660) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0128 | Time 93.1568, Epoch Time 1170.4966(1078.1212), Bit/dim 3.6280(best: 3.6228), Xent 0.6545, Loss 3.9553, Error 0.2221(best: 0.2165)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7050 | Time 19.6687(19.2955) | Bit/dim 3.5898(3.6246) | Xent 0.4959(0.4835) | Loss 8.8957(9.5199) | Error 0.1644(0.1704) Steps 784(769.60) | Grad Norm 14.1679(8.8507) | Total Time 0.00(0.00)\n",
      "Iter 7060 | Time 18.1660(19.2525) | Bit/dim 3.6261(3.6258) | Xent 0.4635(0.4841) | Loss 8.6816(9.3320) | Error 0.1444(0.1701) Steps 748(769.91) | Grad Norm 5.7698(8.7485) | Total Time 0.00(0.00)\n",
      "Iter 7070 | Time 19.6089(19.2624) | Bit/dim 3.6248(3.6243) | Xent 0.5374(0.4897) | Loss 8.8804(9.2008) | Error 0.2000(0.1731) Steps 748(767.97) | Grad Norm 9.5676(8.9952) | Total Time 0.00(0.00)\n",
      "Iter 7080 | Time 19.1441(19.2918) | Bit/dim 3.6406(3.6237) | Xent 0.4784(0.4897) | Loss 8.9029(9.1015) | Error 0.1700(0.1738) Steps 778(769.48) | Grad Norm 6.7474(8.7057) | Total Time 0.00(0.00)\n",
      "Iter 7090 | Time 19.5331(19.2948) | Bit/dim 3.6311(3.6255) | Xent 0.4768(0.4865) | Loss 8.8722(9.0242) | Error 0.1656(0.1732) Steps 760(768.63) | Grad Norm 6.6931(8.4145) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0129 | Time 92.0454, Epoch Time 1172.1955(1080.9435), Bit/dim 3.6266(best: 3.6228), Xent 0.6575, Loss 3.9554, Error 0.2229(best: 0.2165)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7100 | Time 19.4313(19.3190) | Bit/dim 3.6174(3.6244) | Xent 0.4499(0.4757) | Loss 8.7590(9.6144) | Error 0.1489(0.1698) Steps 772(770.10) | Grad Norm 9.2067(7.9939) | Total Time 0.00(0.00)\n",
      "Iter 7110 | Time 19.0899(19.3304) | Bit/dim 3.6478(3.6292) | Xent 0.4330(0.4754) | Loss 8.9718(9.4152) | Error 0.1500(0.1693) Steps 766(767.52) | Grad Norm 10.6437(8.6365) | Total Time 0.00(0.00)\n",
      "Iter 7120 | Time 18.7454(19.2534) | Bit/dim 3.6447(3.6290) | Xent 0.4693(0.4710) | Loss 8.7526(9.2485) | Error 0.1689(0.1670) Steps 748(766.59) | Grad Norm 7.6984(8.3664) | Total Time 0.00(0.00)\n",
      "Iter 7130 | Time 19.4205(19.3181) | Bit/dim 3.6058(3.6262) | Xent 0.4726(0.4724) | Loss 8.8473(9.1385) | Error 0.1889(0.1678) Steps 772(768.39) | Grad Norm 9.3771(8.3675) | Total Time 0.00(0.00)\n",
      "Iter 7140 | Time 19.1362(19.3521) | Bit/dim 3.6244(3.6274) | Xent 0.4475(0.4679) | Loss 8.7195(9.0494) | Error 0.1622(0.1669) Steps 784(769.93) | Grad Norm 8.0172(8.0280) | Total Time 0.00(0.00)\n",
      "Iter 7150 | Time 19.4358(19.4439) | Bit/dim 3.6357(3.6244) | Xent 0.4788(0.4731) | Loss 8.8443(8.9965) | Error 0.1689(0.1686) Steps 760(770.46) | Grad Norm 6.2143(8.4708) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0130 | Time 93.9582, Epoch Time 1178.4170(1083.8677), Bit/dim 3.6373(best: 3.6228), Xent 0.7369, Loss 4.0057, Error 0.2482(best: 0.2165)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7160 | Time 19.7878(19.4326) | Bit/dim 3.6167(3.6223) | Xent 0.4542(0.4733) | Loss 8.7685(9.5183) | Error 0.1611(0.1679) Steps 790(773.04) | Grad Norm 5.0497(8.5111) | Total Time 0.00(0.00)\n",
      "Iter 7170 | Time 17.6770(19.4491) | Bit/dim 3.6352(3.6228) | Xent 0.3964(0.4662) | Loss 8.6595(9.3193) | Error 0.1500(0.1649) Steps 748(773.29) | Grad Norm 6.2566(8.0072) | Total Time 0.00(0.00)\n",
      "Iter 7180 | Time 19.9747(19.5251) | Bit/dim 3.6311(3.6243) | Xent 0.4256(0.4586) | Loss 8.9253(9.1818) | Error 0.1478(0.1621) Steps 796(774.13) | Grad Norm 9.6467(7.9143) | Total Time 0.00(0.00)\n",
      "Iter 7190 | Time 19.1763(19.4958) | Bit/dim 3.6543(3.6232) | Xent 0.5096(0.4604) | Loss 9.0310(9.0873) | Error 0.1822(0.1623) Steps 742(773.82) | Grad Norm 8.9473(7.7076) | Total Time 0.00(0.00)\n",
      "Iter 7200 | Time 19.9109(19.5569) | Bit/dim 3.6314(3.6237) | Xent 0.4810(0.4621) | Loss 8.9263(9.0189) | Error 0.1789(0.1632) Steps 814(774.44) | Grad Norm 13.0618(8.3347) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0131 | Time 92.1675, Epoch Time 1185.7935(1086.9255), Bit/dim 3.6210(best: 3.6228), Xent 0.6713, Loss 3.9567, Error 0.2204(best: 0.2165)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7210 | Time 18.9747(19.5274) | Bit/dim 3.6214(3.6263) | Xent 0.4271(0.4636) | Loss 8.8007(9.6274) | Error 0.1544(0.1631) Steps 760(775.00) | Grad Norm 8.2219(8.2114) | Total Time 0.00(0.00)\n",
      "Iter 7220 | Time 18.7177(19.4759) | Bit/dim 3.6071(3.6229) | Xent 0.5163(0.4672) | Loss 8.8517(9.4187) | Error 0.1667(0.1640) Steps 796(776.29) | Grad Norm 6.0016(8.1131) | Total Time 0.00(0.00)\n",
      "Iter 7230 | Time 19.2746(19.5039) | Bit/dim 3.6160(3.6240) | Xent 0.3942(0.4614) | Loss 8.7254(9.2570) | Error 0.1411(0.1616) Steps 784(773.81) | Grad Norm 5.2802(7.7910) | Total Time 0.00(0.00)\n",
      "Iter 7240 | Time 18.9168(19.5002) | Bit/dim 3.5897(3.6219) | Xent 0.4773(0.4588) | Loss 8.7099(9.1376) | Error 0.1733(0.1614) Steps 766(774.17) | Grad Norm 6.4487(7.4831) | Total Time 0.00(0.00)\n",
      "Iter 7250 | Time 20.3172(19.4747) | Bit/dim 3.6193(3.6192) | Xent 0.5084(0.4591) | Loss 8.8934(9.0430) | Error 0.1611(0.1609) Steps 778(770.38) | Grad Norm 10.9653(8.1393) | Total Time 0.00(0.00)\n",
      "Iter 7260 | Time 20.3908(19.5676) | Bit/dim 3.6596(3.6225) | Xent 0.4436(0.4633) | Loss 8.8805(8.9920) | Error 0.1489(0.1624) Steps 766(771.64) | Grad Norm 5.9587(8.0889) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0132 | Time 93.0500, Epoch Time 1182.7161(1089.7992), Bit/dim 3.6239(best: 3.6210), Xent 0.6595, Loss 3.9537, Error 0.2236(best: 0.2165)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7270 | Time 19.6615(19.5126) | Bit/dim 3.5939(3.6210) | Xent 0.4496(0.4594) | Loss 8.8722(9.5069) | Error 0.1689(0.1616) Steps 766(771.59) | Grad Norm 7.6452(7.8792) | Total Time 0.00(0.00)\n",
      "Iter 7280 | Time 19.3600(19.5493) | Bit/dim 3.5946(3.6206) | Xent 0.4451(0.4609) | Loss 8.6201(9.3132) | Error 0.1600(0.1617) Steps 778(771.63) | Grad Norm 14.4447(8.5146) | Total Time 0.00(0.00)\n",
      "Iter 7290 | Time 19.0797(19.5710) | Bit/dim 3.6192(3.6244) | Xent 0.4638(0.4657) | Loss 8.8411(9.1875) | Error 0.1733(0.1641) Steps 760(769.66) | Grad Norm 7.6861(8.7680) | Total Time 0.00(0.00)\n",
      "Iter 7300 | Time 18.6045(19.4576) | Bit/dim 3.6295(3.6255) | Xent 0.4535(0.4615) | Loss 8.6851(9.0872) | Error 0.1644(0.1628) Steps 760(769.64) | Grad Norm 10.0724(8.2731) | Total Time 0.00(0.00)\n",
      "Iter 7310 | Time 19.1421(19.4969) | Bit/dim 3.6111(3.6225) | Xent 0.4644(0.4631) | Loss 8.7411(9.0050) | Error 0.1578(0.1625) Steps 772(772.85) | Grad Norm 11.4507(8.0826) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0133 | Time 91.4650, Epoch Time 1179.4580(1092.4889), Bit/dim 3.6249(best: 3.6210), Xent 0.6708, Loss 3.9603, Error 0.2255(best: 0.2165)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7320 | Time 19.3974(19.4166) | Bit/dim 3.6108(3.6232) | Xent 0.4465(0.4701) | Loss 8.7555(9.5987) | Error 0.1689(0.1660) Steps 754(770.22) | Grad Norm 10.9557(9.0843) | Total Time 0.00(0.00)\n",
      "Iter 7330 | Time 19.6554(19.4975) | Bit/dim 3.6190(3.6220) | Xent 0.3957(0.4589) | Loss 8.7879(9.3837) | Error 0.1256(0.1622) Steps 796(770.71) | Grad Norm 7.0431(8.7503) | Total Time 0.00(0.00)\n",
      "Iter 7340 | Time 19.0048(19.4957) | Bit/dim 3.6138(3.6226) | Xent 0.4531(0.4565) | Loss 8.7722(9.2340) | Error 0.1533(0.1616) Steps 754(770.82) | Grad Norm 6.2286(8.3580) | Total Time 0.00(0.00)\n",
      "Iter 7350 | Time 19.0253(19.3696) | Bit/dim 3.6143(3.6229) | Xent 0.4989(0.4552) | Loss 8.8684(9.1154) | Error 0.1700(0.1607) Steps 754(766.89) | Grad Norm 8.1850(8.5288) | Total Time 0.00(0.00)\n",
      "Iter 7360 | Time 20.2659(19.4414) | Bit/dim 3.6465(3.6223) | Xent 0.4606(0.4637) | Loss 8.9527(9.0537) | Error 0.1667(0.1642) Steps 778(768.22) | Grad Norm 9.3694(8.6917) | Total Time 0.00(0.00)\n",
      "Iter 7370 | Time 19.4863(19.5131) | Bit/dim 3.6363(3.6249) | Xent 0.4394(0.4628) | Loss 8.8127(8.9913) | Error 0.1422(0.1632) Steps 802(770.66) | Grad Norm 7.3098(8.5972) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0134 | Time 93.3206, Epoch Time 1181.5510(1095.1608), Bit/dim 3.6325(best: 3.6210), Xent 0.6435, Loss 3.9542, Error 0.2160(best: 0.2165)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7380 | Time 19.5908(19.5891) | Bit/dim 3.6119(3.6250) | Xent 0.4667(0.4560) | Loss 8.7630(9.5125) | Error 0.1722(0.1606) Steps 784(775.72) | Grad Norm 6.2125(8.5730) | Total Time 0.00(0.00)\n",
      "Iter 7390 | Time 18.7742(19.5481) | Bit/dim 3.6344(3.6226) | Xent 0.4558(0.4502) | Loss 8.6522(9.3208) | Error 0.1611(0.1584) Steps 736(773.82) | Grad Norm 7.7511(8.2969) | Total Time 0.00(0.00)\n",
      "Iter 7400 | Time 19.6405(19.5815) | Bit/dim 3.6576(3.6219) | Xent 0.4732(0.4569) | Loss 8.6967(9.1888) | Error 0.1578(0.1606) Steps 796(775.63) | Grad Norm 6.3326(8.5150) | Total Time 0.00(0.00)\n",
      "Iter 7410 | Time 19.6618(19.5782) | Bit/dim 3.6516(3.6232) | Xent 0.4827(0.4618) | Loss 8.9468(9.0934) | Error 0.1844(0.1629) Steps 784(774.18) | Grad Norm 10.3816(8.9589) | Total Time 0.00(0.00)\n",
      "Iter 7420 | Time 19.6521(19.5428) | Bit/dim 3.6131(3.6249) | Xent 0.4784(0.4613) | Loss 8.8719(9.0176) | Error 0.1633(0.1629) Steps 790(774.12) | Grad Norm 7.7478(8.8432) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0135 | Time 91.4849, Epoch Time 1186.4237(1097.8987), Bit/dim 3.6245(best: 3.6210), Xent 0.6881, Loss 3.9685, Error 0.2239(best: 0.2160)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7430 | Time 18.6517(19.5182) | Bit/dim 3.6228(3.6216) | Xent 0.4103(0.4529) | Loss 8.8202(9.6104) | Error 0.1500(0.1607) Steps 778(775.11) | Grad Norm 6.5795(8.2094) | Total Time 0.00(0.00)\n",
      "Iter 7440 | Time 19.2210(19.5154) | Bit/dim 3.5857(3.6218) | Xent 0.4129(0.4513) | Loss 8.7933(9.3932) | Error 0.1556(0.1612) Steps 772(774.30) | Grad Norm 7.4029(7.9046) | Total Time 0.00(0.00)\n",
      "Iter 7450 | Time 18.9930(19.4766) | Bit/dim 3.6161(3.6214) | Xent 0.4033(0.4491) | Loss 8.7983(9.2474) | Error 0.1311(0.1590) Steps 778(774.53) | Grad Norm 6.4884(7.9732) | Total Time 0.00(0.00)\n",
      "Iter 7460 | Time 19.9784(19.4913) | Bit/dim 3.6741(3.6215) | Xent 0.4406(0.4444) | Loss 8.9296(9.1245) | Error 0.1511(0.1569) Steps 814(777.62) | Grad Norm 7.7781(7.8111) | Total Time 0.00(0.00)\n",
      "Iter 7470 | Time 19.9306(19.4428) | Bit/dim 3.6326(3.6199) | Xent 0.4576(0.4388) | Loss 8.7537(9.0157) | Error 0.1622(0.1556) Steps 760(774.98) | Grad Norm 9.6131(8.0278) | Total Time 0.00(0.00)\n",
      "Iter 7480 | Time 18.8205(19.3749) | Bit/dim 3.6047(3.6194) | Xent 0.4896(0.4430) | Loss 8.8602(8.9617) | Error 0.1733(0.1560) Steps 772(774.90) | Grad Norm 13.2059(7.9011) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0136 | Time 91.8278, Epoch Time 1174.3031(1100.1908), Bit/dim 3.6308(best: 3.6210), Xent 0.7256, Loss 3.9936, Error 0.2385(best: 0.2160)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7490 | Time 19.0785(19.3830) | Bit/dim 3.6456(3.6216) | Xent 0.4412(0.4426) | Loss 8.8300(9.4830) | Error 0.1600(0.1562) Steps 760(775.00) | Grad Norm 13.2334(8.4992) | Total Time 0.00(0.00)\n",
      "Iter 7500 | Time 18.6848(19.3229) | Bit/dim 3.6504(3.6205) | Xent 0.4077(0.4403) | Loss 8.8162(9.3003) | Error 0.1456(0.1563) Steps 766(774.09) | Grad Norm 5.3378(8.1787) | Total Time 0.00(0.00)\n",
      "Iter 7510 | Time 19.8468(19.3757) | Bit/dim 3.6032(3.6201) | Xent 0.4239(0.4365) | Loss 8.7360(9.1533) | Error 0.1367(0.1545) Steps 826(778.48) | Grad Norm 5.7613(8.0318) | Total Time 0.00(0.00)\n",
      "Iter 7520 | Time 19.2956(19.4207) | Bit/dim 3.6112(3.6206) | Xent 0.4651(0.4380) | Loss 8.7641(9.0623) | Error 0.1633(0.1552) Steps 754(779.77) | Grad Norm 7.3224(7.8941) | Total Time 0.00(0.00)\n",
      "Iter 7530 | Time 19.0616(19.4229) | Bit/dim 3.6626(3.6204) | Xent 0.4400(0.4430) | Loss 8.9004(8.9942) | Error 0.1533(0.1565) Steps 784(778.93) | Grad Norm 7.1759(8.0787) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0137 | Time 92.8925, Epoch Time 1177.1261(1102.4989), Bit/dim 3.6211(best: 3.6210), Xent 0.6518, Loss 3.9469, Error 0.2211(best: 0.2160)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7540 | Time 18.6738(19.3997) | Bit/dim 3.6351(3.6241) | Xent 0.3940(0.4450) | Loss 8.7492(9.6026) | Error 0.1522(0.1572) Steps 784(777.81) | Grad Norm 6.4057(7.9458) | Total Time 0.00(0.00)\n",
      "Iter 7550 | Time 19.5723(19.4429) | Bit/dim 3.6155(3.6209) | Xent 0.4109(0.4359) | Loss 8.8326(9.3816) | Error 0.1456(0.1535) Steps 808(777.84) | Grad Norm 8.5666(8.0424) | Total Time 0.00(0.00)\n",
      "Iter 7560 | Time 19.2866(19.5661) | Bit/dim 3.6036(3.6182) | Xent 0.4191(0.4368) | Loss 8.6809(9.2228) | Error 0.1500(0.1541) Steps 778(781.04) | Grad Norm 6.8024(8.1778) | Total Time 0.00(0.00)\n",
      "Iter 7570 | Time 19.2611(19.4979) | Bit/dim 3.6259(3.6181) | Xent 0.4608(0.4359) | Loss 8.8976(9.1136) | Error 0.1722(0.1537) Steps 778(777.38) | Grad Norm 8.9179(8.0126) | Total Time 0.00(0.00)\n",
      "Iter 7580 | Time 19.6487(19.5779) | Bit/dim 3.6196(3.6148) | Xent 0.4312(0.4335) | Loss 8.6701(9.0205) | Error 0.1689(0.1539) Steps 784(777.42) | Grad Norm 4.9414(7.4517) | Total Time 0.00(0.00)\n",
      "Iter 7590 | Time 19.7351(19.5955) | Bit/dim 3.6124(3.6159) | Xent 0.4629(0.4333) | Loss 8.9264(8.9448) | Error 0.1656(0.1527) Steps 790(777.68) | Grad Norm 9.7604(7.2926) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0138 | Time 91.9258, Epoch Time 1188.7566(1105.0866), Bit/dim 3.6172(best: 3.6210), Xent 0.6439, Loss 3.9391, Error 0.2135(best: 0.2160)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7600 | Time 19.0308(19.4951) | Bit/dim 3.6092(3.6168) | Xent 0.4138(0.4264) | Loss 8.5696(9.4325) | Error 0.1511(0.1513) Steps 736(775.61) | Grad Norm 6.8011(7.1908) | Total Time 0.00(0.00)\n",
      "Iter 7610 | Time 19.6507(19.4931) | Bit/dim 3.6190(3.6140) | Xent 0.4451(0.4261) | Loss 8.8330(9.2487) | Error 0.1578(0.1508) Steps 772(775.28) | Grad Norm 7.3733(7.6734) | Total Time 0.00(0.00)\n",
      "Iter 7620 | Time 19.4057(19.4809) | Bit/dim 3.6013(3.6130) | Xent 0.4125(0.4321) | Loss 8.6679(9.1206) | Error 0.1411(0.1520) Steps 790(777.00) | Grad Norm 9.0793(7.8518) | Total Time 0.00(0.00)\n",
      "Iter 7630 | Time 20.3018(19.6588) | Bit/dim 3.6176(3.6165) | Xent 0.4141(0.4323) | Loss 8.8137(9.0430) | Error 0.1467(0.1525) Steps 778(779.40) | Grad Norm 8.6858(7.7844) | Total Time 0.00(0.00)\n",
      "Iter 7640 | Time 18.7229(19.6294) | Bit/dim 3.6227(3.6207) | Xent 0.4682(0.4396) | Loss 8.7716(8.9827) | Error 0.1700(0.1551) Steps 790(781.55) | Grad Norm 10.7614(8.2992) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0139 | Time 93.4818, Epoch Time 1186.5398(1107.5302), Bit/dim 3.6284(best: 3.6172), Xent 0.6911, Loss 3.9739, Error 0.2252(best: 0.2135)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7650 | Time 19.9295(19.5812) | Bit/dim 3.6469(3.6238) | Xent 0.4577(0.4419) | Loss 9.0182(9.6153) | Error 0.1578(0.1553) Steps 808(780.79) | Grad Norm 12.0309(8.3270) | Total Time 0.00(0.00)\n",
      "Iter 7660 | Time 19.1928(19.5465) | Bit/dim 3.6081(3.6211) | Xent 0.4607(0.4456) | Loss 8.9037(9.4060) | Error 0.1689(0.1575) Steps 802(781.24) | Grad Norm 8.5002(8.4629) | Total Time 0.00(0.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_rl.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_15_annealing_run3 --seed 3 --lr 0.001 --conditional True --controlled_tol False --train_mode semisup --log_freq 10 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5 --scale_fac 1.0 --scale_std 15.0 --annealing_std True\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
