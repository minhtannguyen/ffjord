{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0,1,2,3,4,5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.odenvp as odenvp\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=True, choices=[True, False])\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"./data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"./data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"./data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"./data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"./data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"./data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            './data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            './data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "\n",
      "def compute_bits_per_dim_and_xent(x, y, model, size_cond=10):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "    z_noise, z_cond = z[:,:-size_cond], z[:,-size_cond:] # split z into z for noise and z for conditional signal\n",
      "    \n",
      "    # compute bits_per_dim\n",
      "    logpz = standard_normal_logprob(z_noise).view(z_noise.shape[0], -1).sum(1, keepdim=True)  # logp(z_noise)\n",
      "    logpx = logpz - delta_logp\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    L_xent = torch.nn.CrossEntropyLoss()\n",
      "    loss_xent = L_xent(z_cond, y.to(x.get_device()))\n",
      "\n",
      "    return bits_per_dim, loss_xent\n",
      "\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns},\n",
      "        )\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97)\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss = float(\"inf\")\n",
      "    itr = 0\n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent = compute_bits_per_dim_and_xent(x, y, model, size_cond=10)\n",
      "                loss = loss_nll + loss_xent\n",
      "            else:\n",
      "                loss = compute_bits_per_dim(x, model)\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            loss.backward()\n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "\n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, loss_meter.val, loss_meter.avg, steps_meter.val,\n",
      "                        steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "\n",
      "            itr += 1\n",
      "\n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                losses = []\n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    loss = compute_bits_per_dim(x, model)\n",
      "                    losses.append(loss.cpu().numpy())\n",
      "                \n",
      "                loss = np.mean(losses)\n",
      "                logger.info(\"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}, Bit/dim {:.4f}\".format(epoch, time.time() - start, time.time() - start_epoch, loss))\n",
      "                \n",
      "                if loss < best_loss:\n",
      "                    best_loss = loss\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                    }, os.path.join(args.save, \"checkpt.pth\"))\n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, conditional=False, conv=True, data='mnist', dims='64,64,64', divergence_fn='approximate', dl2int=None, imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume=None, rtol=1e-05, save='experiments/cnf_nocond_bs900', solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=900, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, val_freq=1, warmup_iters=1000, weight_decay=0.0)\n",
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Number of trainable parameters: 800646\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc.py:286: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "Iter 0000 | Time 90.9662(90.9662) | Bit/dim 24.7685(24.7685) | Steps 410(410.00) | Grad Norm 208.0642(208.0642) | Total Time 10.00(10.00)\n",
      "Iter 0010 | Time 20.4376(72.5927) | Bit/dim 22.3413(24.4940) | Steps 410(410.00) | Grad Norm 185.3630(205.4906) | Total Time 10.00(10.00)\n",
      "Iter 0020 | Time 21.0673(59.0740) | Bit/dim 17.2792(23.1884) | Steps 410(410.00) | Grad Norm 132.6199(192.7477) | Total Time 10.00(10.00)\n",
      "Iter 0030 | Time 21.0640(49.0711) | Bit/dim 12.8478(20.9235) | Steps 410(410.00) | Grad Norm 69.9917(167.4295) | Total Time 10.00(10.00)\n",
      "Iter 0040 | Time 20.6318(41.6828) | Bit/dim 10.5481(18.4379) | Steps 410(410.00) | Grad Norm 26.1015(134.2328) | Total Time 10.00(10.00)\n",
      "Iter 0050 | Time 21.0897(36.2732) | Bit/dim 9.2281(16.1830) | Steps 410(410.00) | Grad Norm 16.4812(104.0326) | Total Time 10.00(10.00)\n",
      "Iter 0060 | Time 21.4137(32.3030) | Bit/dim 7.8260(14.1422) | Steps 410(410.00) | Grad Norm 14.1963(80.4887) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0001 | Time 76.0401, Epoch Time 1550.0532, Bit/dim 7.1371\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0070 | Time 21.1436(29.3586) | Bit/dim 6.6220(12.2960) | Steps 410(410.00) | Grad Norm 16.0166(63.4827) | Total Time 10.00(10.00)\n",
      "Iter 0080 | Time 21.7793(27.2069) | Bit/dim 5.2779(10.5993) | Steps 410(410.00) | Grad Norm 12.7735(50.5962) | Total Time 10.00(10.00)\n",
      "Iter 0090 | Time 20.9723(25.5696) | Bit/dim 4.2513(9.0454) | Steps 410(410.00) | Grad Norm 10.1890(40.2373) | Total Time 10.00(10.00)\n",
      "Iter 0100 | Time 21.2190(24.4178) | Bit/dim 3.4527(7.6531) | Steps 416(410.52) | Grad Norm 8.6171(32.1001) | Total Time 10.00(10.00)\n",
      "Iter 0110 | Time 22.4110(23.7491) | Bit/dim 2.9141(6.4566) | Steps 422(413.26) | Grad Norm 5.9426(25.5421) | Total Time 10.00(10.00)\n",
      "Iter 0120 | Time 21.3139(23.2169) | Bit/dim 2.6137(5.4743) | Steps 428(415.73) | Grad Norm 3.5575(20.0033) | Total Time 10.00(10.00)\n",
      "Iter 0130 | Time 22.1364(23.0051) | Bit/dim 2.4551(4.6956) | Steps 440(421.68) | Grad Norm 2.1332(15.4491) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0002 | Time 83.2531, Epoch Time 1520.5064, Bit/dim 2.4154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0140 | Time 23.7038(23.0487) | Bit/dim 2.3615(4.0908) | Steps 446(427.79) | Grad Norm 1.3848(11.8211) | Total Time 10.00(10.00)\n",
      "Iter 0150 | Time 23.5858(23.0082) | Bit/dim 2.3073(3.6264) | Steps 446(432.57) | Grad Norm 1.0947(9.0326) | Total Time 10.00(10.00)\n",
      "Iter 0160 | Time 22.8183(23.0106) | Bit/dim 2.2698(3.2721) | Steps 446(436.10) | Grad Norm 0.9120(6.9167) | Total Time 10.00(10.00)\n",
      "Iter 0170 | Time 23.0652(22.9863) | Bit/dim 2.2378(3.0051) | Steps 446(438.70) | Grad Norm 0.7988(5.3249) | Total Time 10.00(10.00)\n",
      "Iter 0180 | Time 22.7857(22.9456) | Bit/dim 2.2257(2.8019) | Steps 446(440.61) | Grad Norm 0.6945(4.1212) | Total Time 10.00(10.00)\n",
      "Iter 0190 | Time 22.8705(22.9240) | Bit/dim 2.2124(2.6478) | Steps 446(442.03) | Grad Norm 0.6130(3.2105) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0003 | Time 83.1196, Epoch Time 1613.2877, Bit/dim 2.1928\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0200 | Time 23.0874(22.9046) | Bit/dim 2.2227(2.5311) | Steps 440(441.92) | Grad Norm 0.5647(2.5241) | Total Time 10.00(10.00)\n",
      "Iter 0210 | Time 22.2391(22.7514) | Bit/dim 2.1954(2.4433) | Steps 440(441.42) | Grad Norm 0.5412(2.0083) | Total Time 10.00(10.00)\n",
      "Iter 0220 | Time 23.0865(22.7144) | Bit/dim 2.1857(2.3743) | Steps 440(441.05) | Grad Norm 0.5304(1.6214) | Total Time 10.00(10.00)\n",
      "Iter 0230 | Time 22.9892(22.6797) | Bit/dim 2.1633(2.3186) | Steps 440(440.77) | Grad Norm 0.4935(1.3315) | Total Time 10.00(10.00)\n",
      "Iter 0240 | Time 22.0108(22.6046) | Bit/dim 2.1498(2.2736) | Steps 440(440.57) | Grad Norm 0.4893(1.1145) | Total Time 10.00(10.00)\n",
      "Iter 0250 | Time 21.3151(22.4822) | Bit/dim 2.1298(2.2403) | Steps 434(440.07) | Grad Norm 0.4862(0.9464) | Total Time 10.00(10.00)\n",
      "Iter 0260 | Time 22.1507(22.4377) | Bit/dim 2.1268(2.2108) | Steps 434(438.48) | Grad Norm 0.4293(0.8206) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0004 | Time 81.4024, Epoch Time 1576.1233, Bit/dim 2.1155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0270 | Time 22.0118(22.3865) | Bit/dim 2.1155(2.1857) | Steps 434(437.30) | Grad Norm 0.4618(0.7266) | Total Time 10.00(10.00)\n",
      "Iter 0280 | Time 22.3404(22.3923) | Bit/dim 2.1040(2.1643) | Steps 434(436.43) | Grad Norm 0.4217(0.6482) | Total Time 10.00(10.00)\n",
      "Iter 0290 | Time 22.5926(22.3352) | Bit/dim 2.0933(2.1461) | Steps 434(435.79) | Grad Norm 0.3910(0.5845) | Total Time 10.00(10.00)\n",
      "Iter 0300 | Time 21.8968(22.3190) | Bit/dim 2.0681(2.1291) | Steps 434(435.32) | Grad Norm 0.4054(0.5377) | Total Time 10.00(10.00)\n",
      "Iter 0310 | Time 22.0715(22.3109) | Bit/dim 2.0371(2.1111) | Steps 434(434.98) | Grad Norm 0.4025(0.5024) | Total Time 10.00(10.00)\n",
      "Iter 0320 | Time 22.0948(22.2873) | Bit/dim 2.0591(2.0961) | Steps 434(434.72) | Grad Norm 0.4372(0.4757) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0005 | Time 82.4370, Epoch Time 1569.9850, Bit/dim 2.0257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0330 | Time 22.0281(22.3130) | Bit/dim 2.0358(2.0812) | Steps 434(434.68) | Grad Norm 0.4443(0.4647) | Total Time 10.00(10.00)\n",
      "Iter 0340 | Time 22.1706(22.3154) | Bit/dim 2.0226(2.0655) | Steps 434(434.50) | Grad Norm 0.4071(0.4555) | Total Time 10.00(10.00)\n",
      "Iter 0350 | Time 22.0388(22.3046) | Bit/dim 1.9906(2.0487) | Steps 434(434.37) | Grad Norm 0.4137(0.4475) | Total Time 10.00(10.00)\n",
      "Iter 0360 | Time 22.2564(22.3318) | Bit/dim 1.9773(2.0320) | Steps 434(434.27) | Grad Norm 0.3957(0.4363) | Total Time 10.00(10.00)\n",
      "Iter 0370 | Time 21.7754(22.3278) | Bit/dim 1.9735(2.0159) | Steps 434(434.20) | Grad Norm 0.4249(0.4304) | Total Time 10.00(10.00)\n",
      "Iter 0380 | Time 22.7590(22.3961) | Bit/dim 1.9618(2.0012) | Steps 446(435.99) | Grad Norm 0.4261(0.4265) | Total Time 10.00(10.00)\n",
      "Iter 0390 | Time 23.3115(22.4862) | Bit/dim 1.9314(1.9870) | Steps 446(438.62) | Grad Norm 0.3780(0.4113) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0006 | Time 85.3465, Epoch Time 1581.9767, Bit/dim 1.9266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do _generate\n",
      "do _generatedo _generate\n",
      "\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0400 | Time 22.9327(22.5282) | Bit/dim 1.9208(1.9723) | Steps 446(440.56) | Grad Norm 0.3634(0.3886) | Total Time 10.00(10.00)\n",
      "Iter 0410 | Time 23.4801(22.6486) | Bit/dim 1.9215(1.9584) | Steps 446(441.99) | Grad Norm 0.2542(0.3683) | Total Time 10.00(10.00)\n",
      "Iter 0420 | Time 23.0661(22.8916) | Bit/dim 1.9146(1.9470) | Steps 464(447.03) | Grad Norm 0.2903(0.3432) | Total Time 10.00(10.00)\n",
      "Iter 0430 | Time 23.7536(23.0687) | Bit/dim 1.9047(1.9348) | Steps 464(451.49) | Grad Norm 0.3015(0.3259) | Total Time 10.00(10.00)\n",
      "Iter 0440 | Time 23.1937(23.2588) | Bit/dim 1.8987(1.9248) | Steps 464(454.77) | Grad Norm 0.2079(0.3080) | Total Time 10.00(10.00)\n",
      "Iter 0450 | Time 23.4940(23.3992) | Bit/dim 1.8804(1.9155) | Steps 464(457.20) | Grad Norm 0.2708(0.3010) | Total Time 10.00(10.00)\n",
      "Iter 0460 | Time 23.1232(23.4377) | Bit/dim 1.8564(1.9055) | Steps 464(458.98) | Grad Norm 0.2143(0.2826) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0007 | Time 88.1546, Epoch Time 1655.0601, Bit/dim 1.8721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0470 | Time 23.4224(23.5479) | Bit/dim 1.8663(1.8973) | Steps 470(461.60) | Grad Norm 0.3072(0.2795) | Total Time 10.00(10.00)\n",
      "Iter 0480 | Time 24.3198(23.7779) | Bit/dim 1.8562(1.8877) | Steps 476(465.09) | Grad Norm 0.3490(0.2844) | Total Time 10.00(10.00)\n",
      "Iter 0490 | Time 24.0100(23.9707) | Bit/dim 1.8544(1.8792) | Steps 476(467.95) | Grad Norm 0.2913(0.2831) | Total Time 10.00(10.00)\n",
      "Iter 0500 | Time 24.6510(24.0630) | Bit/dim 1.8429(1.8712) | Steps 476(470.07) | Grad Norm 0.2433(0.2793) | Total Time 10.00(10.00)\n",
      "Iter 0510 | Time 24.5964(24.0862) | Bit/dim 1.8244(1.8625) | Steps 476(471.62) | Grad Norm 0.2706(0.2855) | Total Time 10.00(10.00)\n",
      "Iter 0520 | Time 24.5115(24.1475) | Bit/dim 1.7984(1.8510) | Steps 476(472.77) | Grad Norm 0.3671(0.2948) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0008 | Time 89.9465, Epoch Time 1709.1328, Bit/dim 1.7867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0530 | Time 24.5545(24.2169) | Bit/dim 1.7862(1.8355) | Steps 470(473.44) | Grad Norm 0.4364(0.3188) | Total Time 10.00(10.00)\n",
      "Iter 0540 | Time 24.0986(24.1744) | Bit/dim 1.7502(1.8189) | Steps 464(472.49) | Grad Norm 0.3032(0.3375) | Total Time 10.00(10.00)\n",
      "Iter 0550 | Time 23.6855(24.0686) | Bit/dim 1.7376(1.8003) | Steps 464(471.00) | Grad Norm 0.6614(0.3623) | Total Time 10.00(10.00)\n",
      "Iter 0560 | Time 23.6741(24.0137) | Bit/dim 1.6914(1.7783) | Steps 464(469.30) | Grad Norm 0.3359(0.4166) | Total Time 10.00(10.00)\n",
      "Iter 0570 | Time 23.7937(24.0154) | Bit/dim 1.6687(1.7539) | Steps 464(468.06) | Grad Norm 1.0193(0.4447) | Total Time 10.00(10.00)\n",
      "Iter 0580 | Time 24.2578(24.0076) | Bit/dim 1.6323(1.7282) | Steps 470(467.80) | Grad Norm 5.1878(1.0939) | Total Time 10.00(10.00)\n",
      "Iter 0590 | Time 23.5167(24.0009) | Bit/dim 1.6227(1.7021) | Steps 464(467.74) | Grad Norm 2.7198(1.8944) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0009 | Time 89.7335, Epoch Time 1686.4931, Bit/dim 1.6066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0600 | Time 23.9786(23.9931) | Bit/dim 1.5699(1.6753) | Steps 464(467.56) | Grad Norm 2.0878(2.7448) | Total Time 10.00(10.00)\n",
      "Iter 0610 | Time 24.1459(23.9861) | Bit/dim 1.5615(1.6518) | Steps 470(467.43) | Grad Norm 0.3466(3.2163) | Total Time 10.00(10.00)\n",
      "Iter 0620 | Time 24.1154(24.0505) | Bit/dim 1.6677(1.6372) | Steps 464(468.18) | Grad Norm 35.7894(5.9168) | Total Time 10.00(10.00)\n",
      "Iter 0630 | Time 24.1288(24.1443) | Bit/dim 1.5802(1.6325) | Steps 470(469.76) | Grad Norm 13.4382(8.9151) | Total Time 10.00(10.00)\n",
      "Iter 0640 | Time 24.2769(24.1044) | Bit/dim 1.5496(1.6164) | Steps 470(469.56) | Grad Norm 4.4047(8.6471) | Total Time 10.00(10.00)\n",
      "Iter 0650 | Time 24.6000(24.1107) | Bit/dim 1.5365(1.5980) | Steps 476(470.82) | Grad Norm 3.0137(7.7591) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0010 | Time 91.8639, Epoch Time 1700.8910, Bit/dim 1.5174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0660 | Time 24.3470(24.1244) | Bit/dim 1.5389(1.5798) | Steps 476(472.18) | Grad Norm 1.8244(6.7377) | Total Time 10.00(10.00)\n",
      "Iter 0670 | Time 24.4208(24.1215) | Bit/dim 1.5081(1.5627) | Steps 476(473.18) | Grad Norm 4.3217(5.7384) | Total Time 10.00(10.00)\n",
      "Iter 0680 | Time 24.0213(24.1232) | Bit/dim 1.4949(1.5483) | Steps 476(473.92) | Grad Norm 4.1384(5.0067) | Total Time 10.00(10.00)\n",
      "Iter 0690 | Time 24.5602(24.1138) | Bit/dim 1.5249(1.5392) | Steps 482(474.80) | Grad Norm 19.1700(6.9097) | Total Time 10.00(10.00)\n",
      "Iter 0700 | Time 23.8423(24.0389) | Bit/dim 1.4911(1.5277) | Steps 470(473.54) | Grad Norm 12.4462(7.3924) | Total Time 10.00(10.00)\n",
      "Iter 0710 | Time 23.7760(24.0173) | Bit/dim 1.4980(1.5179) | Steps 470(472.79) | Grad Norm 12.1481(8.2366) | Total Time 10.00(10.00)\n",
      "Iter 0720 | Time 24.0878(24.0106) | Bit/dim 1.4757(1.5125) | Steps 476(473.49) | Grad Norm 4.7121(9.3042) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0011 | Time 90.4817, Epoch Time 1693.2860, Bit/dim 1.4673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0730 | Time 23.7035(24.0054) | Bit/dim 1.5047(1.5060) | Steps 476(474.15) | Grad Norm 14.2876(9.8744) | Total Time 10.00(10.00)\n",
      "Iter 0740 | Time 24.2733(24.0353) | Bit/dim 1.4628(1.4973) | Steps 476(474.64) | Grad Norm 2.0032(9.5170) | Total Time 10.00(10.00)\n",
      "Iter 0750 | Time 24.0112(24.0456) | Bit/dim 1.4867(1.4948) | Steps 476(474.99) | Grad Norm 17.4967(11.0276) | Total Time 10.00(10.00)\n",
      "Iter 0760 | Time 24.9088(24.1159) | Bit/dim 1.4632(1.4880) | Steps 482(475.88) | Grad Norm 7.5968(11.1995) | Total Time 10.00(10.00)\n",
      "Iter 0770 | Time 24.1612(24.1319) | Bit/dim 1.4601(1.4803) | Steps 476(477.17) | Grad Norm 16.4686(11.1074) | Total Time 10.00(10.00)\n",
      "Iter 0780 | Time 23.7598(24.0683) | Bit/dim 1.5258(1.4771) | Steps 482(478.13) | Grad Norm 29.3855(11.8492) | Total Time 10.00(10.00)\n",
      "Iter 0790 | Time 24.2703(24.0727) | Bit/dim 1.4386(1.4712) | Steps 482(479.14) | Grad Norm 5.9291(12.2029) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0012 | Time 92.1229, Epoch Time 1697.7538, Bit/dim 1.4420\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0800 | Time 23.9510(24.0820) | Bit/dim 1.4424(1.4646) | Steps 482(479.89) | Grad Norm 12.9033(11.7421) | Total Time 10.00(10.00)\n",
      "Iter 0810 | Time 24.3355(24.1651) | Bit/dim 1.4902(1.4619) | Steps 482(480.45) | Grad Norm 25.3818(12.8091) | Total Time 10.00(10.00)\n",
      "Iter 0820 | Time 24.9733(24.2727) | Bit/dim 1.4372(1.4569) | Steps 482(481.02) | Grad Norm 2.6926(12.7231) | Total Time 10.00(10.00)\n",
      "Iter 0830 | Time 23.7117(24.2067) | Bit/dim 1.4263(1.4504) | Steps 482(481.28) | Grad Norm 13.3580(12.5794) | Total Time 10.00(10.00)\n",
      "Iter 0840 | Time 24.4621(24.2561) | Bit/dim 1.4240(1.4488) | Steps 482(482.16) | Grad Norm 13.2399(13.1175) | Total Time 10.00(10.00)\n",
      "Iter 0850 | Time 24.3124(24.3059) | Bit/dim 1.4146(1.4466) | Steps 482(483.01) | Grad Norm 6.0104(13.4011) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0013 | Time 94.2851, Epoch Time 1717.0411, Bit/dim 1.4142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0860 | Time 24.9024(24.3787) | Bit/dim 1.4227(1.4392) | Steps 482(483.63) | Grad Norm 12.6845(12.6244) | Total Time 10.00(10.00)\n",
      "Iter 0870 | Time 24.6732(24.2964) | Bit/dim 1.4120(1.4329) | Steps 488(483.89) | Grad Norm 12.9431(12.5943) | Total Time 10.00(10.00)\n",
      "Iter 0880 | Time 25.1081(24.3707) | Bit/dim 1.4187(1.4279) | Steps 494(485.14) | Grad Norm 16.3479(12.2554) | Total Time 10.00(10.00)\n",
      "Iter 0890 | Time 24.5817(24.4398) | Bit/dim 1.4134(1.4243) | Steps 482(486.19) | Grad Norm 6.6438(12.5511) | Total Time 10.00(10.00)\n",
      "Iter 0900 | Time 25.2175(24.4647) | Bit/dim 1.4053(1.4214) | Steps 488(487.77) | Grad Norm 15.6109(12.8012) | Total Time 10.00(10.00)\n",
      "Iter 0910 | Time 24.8022(24.5232) | Bit/dim 1.4111(1.4200) | Steps 494(489.92) | Grad Norm 11.0331(13.2506) | Total Time 10.00(10.00)\n",
      "Iter 0920 | Time 24.1828(24.5558) | Bit/dim 1.3964(1.4156) | Steps 494(491.08) | Grad Norm 13.0228(13.3357) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0014 | Time 96.1241, Epoch Time 1730.6648, Bit/dim 1.3989\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0930 | Time 24.0279(24.5379) | Bit/dim 1.3919(1.4085) | Steps 488(491.04) | Grad Norm 5.3681(12.1757) | Total Time 10.00(10.00)\n",
      "Iter 0940 | Time 24.7374(24.6100) | Bit/dim 1.4183(1.4122) | Steps 494(493.23) | Grad Norm 22.9419(13.7841) | Total Time 10.00(10.00)\n",
      "Iter 0950 | Time 25.1139(24.6842) | Bit/dim 1.4105(1.4137) | Steps 512(495.18) | Grad Norm 14.3021(14.3657) | Total Time 10.00(10.00)\n",
      "Iter 0960 | Time 25.2156(24.7655) | Bit/dim 1.3699(1.4098) | Steps 494(496.50) | Grad Norm 9.9878(14.2022) | Total Time 10.00(10.00)\n",
      "Iter 0970 | Time 24.7931(24.7054) | Bit/dim 1.3694(1.4026) | Steps 494(496.79) | Grad Norm 4.8139(13.2304) | Total Time 10.00(10.00)\n",
      "Iter 0980 | Time 24.7622(24.6453) | Bit/dim 1.3734(1.3952) | Steps 494(496.20) | Grad Norm 1.4403(11.8520) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0015 | Time 99.6520, Epoch Time 1744.4337, Bit/dim 1.3699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0990 | Time 24.8693(24.6128) | Bit/dim 1.3739(1.3897) | Steps 512(496.62) | Grad Norm 12.7929(11.6874) | Total Time 10.00(10.00)\n",
      "Iter 1000 | Time 24.1403(24.6086) | Bit/dim 1.3773(1.3849) | Steps 494(497.47) | Grad Norm 16.1406(11.8579) | Total Time 10.00(10.00)\n",
      "Iter 1010 | Time 24.7892(24.6556) | Bit/dim 1.4451(1.3831) | Steps 512(498.61) | Grad Norm 29.5723(12.7137) | Total Time 10.00(10.00)\n",
      "Iter 1020 | Time 24.8933(24.7807) | Bit/dim 1.3575(1.3826) | Steps 512(500.41) | Grad Norm 2.2115(13.0895) | Total Time 10.00(10.00)\n",
      "Iter 1030 | Time 24.6051(24.8337) | Bit/dim 1.3732(1.3778) | Steps 494(500.81) | Grad Norm 9.6990(12.0482) | Total Time 10.00(10.00)\n",
      "Iter 1040 | Time 25.4158(24.9016) | Bit/dim 1.4029(1.3730) | Steps 512(501.52) | Grad Norm 24.1944(11.9439) | Total Time 10.00(10.00)\n",
      "Iter 1050 | Time 25.7104(25.0153) | Bit/dim 1.3807(1.3715) | Steps 512(502.70) | Grad Norm 22.8778(12.5245) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0016 | Time 98.8022, Epoch Time 1765.9901, Bit/dim 1.3361\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1060 | Time 25.1531(25.0409) | Bit/dim 1.3658(1.3691) | Steps 512(504.19) | Grad Norm 14.9709(12.6352) | Total Time 10.00(10.00)\n",
      "Iter 1070 | Time 24.4435(24.9437) | Bit/dim 1.3544(1.3659) | Steps 512(503.62) | Grad Norm 12.3476(12.4795) | Total Time 10.00(10.00)\n",
      "Iter 1080 | Time 25.6500(24.9473) | Bit/dim 1.3705(1.3624) | Steps 518(504.27) | Grad Norm 20.1907(12.4639) | Total Time 10.00(10.00)\n",
      "Iter 1090 | Time 25.3720(24.9992) | Bit/dim 1.3545(1.3599) | Steps 512(504.42) | Grad Norm 7.3932(12.5477) | Total Time 10.00(10.00)\n",
      "Iter 1100 | Time 25.3682(25.0383) | Bit/dim 1.3337(1.3553) | Steps 512(505.10) | Grad Norm 2.8272(12.3503) | Total Time 10.00(10.00)\n",
      "Iter 1110 | Time 25.4803(25.0281) | Bit/dim 1.3455(1.3528) | Steps 518(505.54) | Grad Norm 2.5811(12.1842) | Total Time 10.00(10.00)\n",
      "Iter 1120 | Time 24.7344(25.0248) | Bit/dim 1.3546(1.3517) | Steps 512(506.12) | Grad Norm 11.5114(12.4368) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0017 | Time 98.7404, Epoch Time 1763.8449, Bit/dim 1.3225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1130 | Time 24.5892(25.0626) | Bit/dim 1.3437(1.3496) | Steps 500(506.37) | Grad Norm 11.8232(12.1491) | Total Time 10.00(10.00)\n",
      "Iter 1140 | Time 24.9337(25.0363) | Bit/dim 1.3188(1.3443) | Steps 500(506.40) | Grad Norm 11.9585(11.8004) | Total Time 10.00(10.00)\n",
      "Iter 1150 | Time 25.2860(25.0456) | Bit/dim 1.3240(1.3420) | Steps 518(507.14) | Grad Norm 7.2282(11.9225) | Total Time 10.00(10.00)\n",
      "Iter 1160 | Time 25.0029(25.0964) | Bit/dim 1.3190(1.3404) | Steps 512(507.83) | Grad Norm 13.2047(12.3484) | Total Time 10.00(10.00)\n",
      "Iter 1170 | Time 25.4931(25.1801) | Bit/dim 1.3136(1.3362) | Steps 512(507.90) | Grad Norm 10.9123(12.3393) | Total Time 10.00(10.00)\n",
      "Iter 1180 | Time 25.2621(25.1833) | Bit/dim 1.3225(1.3329) | Steps 512(509.33) | Grad Norm 10.0000(11.7807) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0018 | Time 98.9704, Epoch Time 1775.8695, Bit/dim 1.3125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1190 | Time 24.5435(25.1810) | Bit/dim 1.3157(1.3312) | Steps 494(508.54) | Grad Norm 15.5382(12.1418) | Total Time 10.00(10.00)\n",
      "Iter 1200 | Time 25.7094(25.2057) | Bit/dim 1.3152(1.3282) | Steps 512(509.48) | Grad Norm 5.2528(12.0705) | Total Time 10.00(10.00)\n",
      "Iter 1210 | Time 24.6267(25.1874) | Bit/dim 1.3149(1.3243) | Steps 494(509.47) | Grad Norm 13.2167(11.5376) | Total Time 10.00(10.00)\n",
      "Iter 1220 | Time 26.1263(25.2626) | Bit/dim 1.3561(1.3232) | Steps 518(510.04) | Grad Norm 26.4163(11.9391) | Total Time 10.00(10.00)\n",
      "Iter 1230 | Time 25.3665(25.3261) | Bit/dim 1.2911(1.3234) | Steps 512(510.16) | Grad Norm 4.6408(12.3181) | Total Time 10.00(10.00)\n",
      "Iter 1240 | Time 25.9108(25.3757) | Bit/dim 1.2951(1.3201) | Steps 512(510.91) | Grad Norm 2.2490(11.6321) | Total Time 10.00(10.00)\n",
      "Iter 1250 | Time 25.6746(25.3814) | Bit/dim 1.2991(1.3157) | Steps 500(509.72) | Grad Norm 6.9496(11.3100) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0019 | Time 98.5148, Epoch Time 1790.1739, Bit/dim 1.2930\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do _generatedo _generate\n",
      "\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1260 | Time 25.1586(25.3375) | Bit/dim 1.3248(1.3156) | Steps 494(508.85) | Grad Norm 19.4006(12.0061) | Total Time 10.00(10.00)\n",
      "Iter 1270 | Time 25.4543(25.3515) | Bit/dim 1.2988(1.3137) | Steps 518(508.91) | Grad Norm 2.3381(12.0207) | Total Time 10.00(10.00)\n",
      "Iter 1280 | Time 24.8986(25.3760) | Bit/dim 1.2921(1.3098) | Steps 512(509.90) | Grad Norm 15.7363(11.5482) | Total Time 10.00(10.00)\n",
      "Iter 1290 | Time 26.1505(25.3986) | Bit/dim 1.2995(1.3087) | Steps 512(509.60) | Grad Norm 13.8273(12.0688) | Total Time 10.00(10.00)\n",
      "Iter 1300 | Time 26.1444(25.4021) | Bit/dim 1.2888(1.3048) | Steps 518(511.03) | Grad Norm 14.5837(11.0814) | Total Time 10.00(10.00)\n",
      "Iter 1310 | Time 25.6395(25.4105) | Bit/dim 1.3418(1.3067) | Steps 512(511.01) | Grad Norm 18.8994(12.0238) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0020 | Time 99.2303, Epoch Time 1790.5855, Bit/dim 1.2962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generatedo _generate\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1320 | Time 25.1627(25.3810) | Bit/dim 1.3103(1.3064) | Steps 512(510.94) | Grad Norm 14.2835(13.0008) | Total Time 10.00(10.00)\n",
      "Iter 1330 | Time 25.2856(25.4192) | Bit/dim 1.2760(1.3014) | Steps 518(511.84) | Grad Norm 10.5814(12.1192) | Total Time 10.00(10.00)\n",
      "Iter 1340 | Time 25.5034(25.3948) | Bit/dim 1.2751(1.2981) | Steps 518(511.89) | Grad Norm 3.5542(11.7374) | Total Time 10.00(10.00)\n",
      "Iter 1350 | Time 25.1790(25.4002) | Bit/dim 1.3115(1.2961) | Steps 512(511.90) | Grad Norm 19.3799(11.9054) | Total Time 10.00(10.00)\n",
      "Iter 1360 | Time 24.9196(25.3653) | Bit/dim 1.3085(1.2937) | Steps 512(512.07) | Grad Norm 17.0409(11.7108) | Total Time 10.00(10.00)\n",
      "Iter 1370 | Time 24.7275(25.3254) | Bit/dim 1.3037(1.2928) | Steps 506(511.88) | Grad Norm 15.5220(11.8343) | Total Time 10.00(10.00)\n",
      "Iter 1380 | Time 24.5000(25.3094) | Bit/dim 1.2934(1.2942) | Steps 506(511.77) | Grad Norm 15.6110(12.8796) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0021 | Time 98.1552, Epoch Time 1786.8578, Bit/dim 1.2649\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do _generate\n",
      "do _generatedo _generate\n",
      "\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1390 | Time 25.0127(25.3569) | Bit/dim 1.2828(1.2905) | Steps 506(511.68) | Grad Norm 13.7579(12.2223) | Total Time 10.00(10.00)\n",
      "Iter 1400 | Time 26.5812(25.3381) | Bit/dim 1.3103(1.2883) | Steps 518(512.39) | Grad Norm 22.6886(12.0376) | Total Time 10.00(10.00)\n",
      "Iter 1410 | Time 25.6375(25.3410) | Bit/dim 1.3113(1.2888) | Steps 518(512.28) | Grad Norm 23.4215(12.6092) | Total Time 10.00(10.00)\n",
      "Iter 1420 | Time 26.0674(25.3508) | Bit/dim 1.2706(1.2886) | Steps 512(512.03) | Grad Norm 6.4763(12.2585) | Total Time 10.00(10.00)\n",
      "Iter 1430 | Time 25.2377(25.3887) | Bit/dim 1.2726(1.2849) | Steps 512(512.35) | Grad Norm 8.3647(11.9153) | Total Time 10.00(10.00)\n",
      "Iter 1440 | Time 25.6243(25.3841) | Bit/dim 1.2895(1.2825) | Steps 512(512.56) | Grad Norm 16.2358(11.8078) | Total Time 10.00(10.00)\n",
      "Iter 1450 | Time 25.7843(25.3942) | Bit/dim 1.2727(1.2795) | Steps 512(512.72) | Grad Norm 7.9561(11.9225) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0022 | Time 98.8704, Epoch Time 1790.4329, Bit/dim 1.2744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1460 | Time 25.3715(25.4058) | Bit/dim 1.2845(1.2791) | Steps 512(512.37) | Grad Norm 13.5159(12.5749) | Total Time 10.00(10.00)\n",
      "Iter 1470 | Time 25.1167(25.3853) | Bit/dim 1.2581(1.2776) | Steps 512(511.94) | Grad Norm 7.1798(12.5781) | Total Time 10.00(10.00)\n",
      "Iter 1480 | Time 25.0327(25.3431) | Bit/dim 1.2640(1.2767) | Steps 512(511.80) | Grad Norm 10.4004(12.1172) | Total Time 10.00(10.00)\n",
      "Iter 1490 | Time 25.6204(25.3533) | Bit/dim 1.2425(1.2755) | Steps 512(511.68) | Grad Norm 2.7758(12.0740) | Total Time 10.00(10.00)\n",
      "Iter 1500 | Time 26.2118(25.4112) | Bit/dim 1.2518(1.2732) | Steps 512(511.61) | Grad Norm 8.5546(11.8570) | Total Time 10.00(10.00)\n",
      "Iter 1510 | Time 25.6387(25.4483) | Bit/dim 1.2706(1.2725) | Steps 512(511.17) | Grad Norm 3.1004(12.0926) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0023 | Time 97.6973, Epoch Time 1790.3980, Bit/dim 1.2454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1520 | Time 26.1454(25.4728) | Bit/dim 1.2662(1.2726) | Steps 512(511.23) | Grad Norm 14.2184(11.9440) | Total Time 10.00(10.00)\n",
      "Iter 1530 | Time 25.3434(25.5050) | Bit/dim 1.2537(1.2704) | Steps 512(511.27) | Grad Norm 8.7817(11.7872) | Total Time 10.00(10.00)\n",
      "Iter 1540 | Time 24.8554(25.5407) | Bit/dim 1.3109(1.2701) | Steps 506(511.28) | Grad Norm 23.9625(12.0440) | Total Time 10.00(10.00)\n",
      "Iter 1550 | Time 26.0272(25.4997) | Bit/dim 1.2589(1.2690) | Steps 512(510.63) | Grad Norm 4.1100(12.2812) | Total Time 10.00(10.00)\n",
      "Iter 1560 | Time 25.3888(25.4683) | Bit/dim 1.2303(1.2662) | Steps 512(510.67) | Grad Norm 1.6921(11.6536) | Total Time 10.00(10.00)\n",
      "Iter 1570 | Time 25.8735(25.4975) | Bit/dim 1.2587(1.2647) | Steps 512(510.86) | Grad Norm 14.8882(11.8348) | Total Time 10.00(10.00)\n",
      "Iter 1580 | Time 25.4873(25.5150) | Bit/dim 1.2526(1.2630) | Steps 512(510.99) | Grad Norm 13.3037(11.9359) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0024 | Time 97.7294, Epoch Time 1800.2218, Bit/dim 1.2451\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1590 | Time 25.0258(25.4922) | Bit/dim 1.2807(1.2635) | Steps 512(511.09) | Grad Norm 18.3186(12.5547) | Total Time 10.00(10.00)\n",
      "Iter 1600 | Time 25.1838(25.4972) | Bit/dim 1.2565(1.2638) | Steps 512(511.29) | Grad Norm 10.7732(12.6463) | Total Time 10.00(10.00)\n",
      "Iter 1610 | Time 25.4314(25.4803) | Bit/dim 1.2365(1.2619) | Steps 512(511.16) | Grad Norm 4.8341(12.7166) | Total Time 10.00(10.00)\n",
      "Iter 1620 | Time 24.9367(25.4661) | Bit/dim 1.2812(1.2613) | Steps 506(511.55) | Grad Norm 20.8133(12.8612) | Total Time 10.00(10.00)\n",
      "Iter 1630 | Time 26.1688(25.5255) | Bit/dim 1.2629(1.2613) | Steps 512(511.38) | Grad Norm 16.0255(12.8616) | Total Time 10.00(10.00)\n",
      "Iter 1640 | Time 25.1408(25.5251) | Bit/dim 1.2526(1.2593) | Steps 512(511.37) | Grad Norm 6.5293(11.9659) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0025 | Time 99.6733, Epoch Time 1798.5096, Bit/dim 1.2482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1650 | Time 26.5281(25.5472) | Bit/dim 1.2610(1.2547) | Steps 518(512.01) | Grad Norm 17.4474(11.2247) | Total Time 10.00(10.00)\n",
      "Iter 1660 | Time 25.5871(25.5422) | Bit/dim 1.2702(1.2552) | Steps 512(511.58) | Grad Norm 20.2161(11.8627) | Total Time 10.00(10.00)\n",
      "Iter 1670 | Time 25.8883(25.5440) | Bit/dim 1.2516(1.2539) | Steps 512(511.85) | Grad Norm 16.1237(12.0447) | Total Time 10.00(10.00)\n",
      "Iter 1680 | Time 26.1183(25.5547) | Bit/dim 1.2704(1.2512) | Steps 518(512.36) | Grad Norm 18.7993(11.6930) | Total Time 10.00(10.00)\n",
      "Iter 1690 | Time 26.2828(25.6178) | Bit/dim 1.2487(1.2504) | Steps 518(512.60) | Grad Norm 17.2261(11.9291) | Total Time 10.00(10.00)\n",
      "Iter 1700 | Time 26.7721(25.6564) | Bit/dim 1.2405(1.2478) | Steps 518(513.08) | Grad Norm 18.8959(11.6927) | Total Time 10.00(10.00)\n",
      "Iter 1710 | Time 26.2446(25.6440) | Bit/dim 1.2481(1.2482) | Steps 518(512.97) | Grad Norm 15.3085(11.9741) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0026 | Time 97.8891, Epoch Time 1806.8542, Bit/dim 1.2471\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1720 | Time 25.9455(25.6652) | Bit/dim 1.2337(1.2451) | Steps 518(513.37) | Grad Norm 11.5753(11.3779) | Total Time 10.00(10.00)\n",
      "Iter 1730 | Time 25.0856(25.6612) | Bit/dim 1.2283(1.2480) | Steps 512(513.45) | Grad Norm 9.9615(12.2765) | Total Time 10.00(10.00)\n",
      "Iter 1740 | Time 24.6304(25.6587) | Bit/dim 1.2634(1.2470) | Steps 512(513.83) | Grad Norm 18.9058(12.3535) | Total Time 10.00(10.00)\n",
      "Iter 1750 | Time 25.0110(25.6712) | Bit/dim 1.2574(1.2448) | Steps 512(513.82) | Grad Norm 15.4821(11.7456) | Total Time 10.00(10.00)\n",
      "Iter 1760 | Time 24.8842(25.6537) | Bit/dim 1.2437(1.2424) | Steps 512(514.70) | Grad Norm 9.7636(11.5028) | Total Time 10.00(10.00)\n",
      "Iter 1770 | Time 25.6050(25.7118) | Bit/dim 1.2516(1.2417) | Steps 512(514.81) | Grad Norm 14.0666(11.6991) | Total Time 10.00(10.00)\n",
      "Iter 1780 | Time 24.5853(25.7158) | Bit/dim 1.2506(1.2417) | Steps 512(515.93) | Grad Norm 16.8124(12.1617) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0027 | Time 99.6753, Epoch Time 1812.2769, Bit/dim 1.2219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1790 | Time 25.4561(25.7437) | Bit/dim 1.2333(1.2414) | Steps 512(516.77) | Grad Norm 4.4487(11.8923) | Total Time 10.00(10.00)\n",
      "Iter 1800 | Time 25.9922(25.8342) | Bit/dim 1.2272(1.2409) | Steps 518(517.28) | Grad Norm 4.7917(11.8923) | Total Time 10.00(10.00)\n",
      "Iter 1810 | Time 26.2373(25.8582) | Bit/dim 1.2285(1.2388) | Steps 530(517.49) | Grad Norm 10.0188(11.4068) | Total Time 10.00(10.00)\n",
      "Iter 1820 | Time 24.8762(25.9119) | Bit/dim 1.3239(1.2381) | Steps 494(517.72) | Grad Norm 26.5827(11.4692) | Total Time 10.00(10.00)\n",
      "Iter 1830 | Time 26.0069(25.9736) | Bit/dim 1.2426(1.2423) | Steps 512(519.10) | Grad Norm 14.4550(12.6454) | Total Time 10.00(10.00)\n",
      "Iter 1840 | Time 27.0995(26.0046) | Bit/dim 1.2149(1.2433) | Steps 518(519.65) | Grad Norm 6.0876(12.3635) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0028 | Time 98.4184, Epoch Time 1832.9705, Bit/dim 1.2313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1850 | Time 26.1568(26.0249) | Bit/dim 1.2334(1.2417) | Steps 518(519.67) | Grad Norm 1.6462(11.6429) | Total Time 10.00(10.00)\n",
      "Iter 1860 | Time 25.7854(26.0072) | Bit/dim 1.2173(1.2383) | Steps 530(520.56) | Grad Norm 1.4979(10.5769) | Total Time 10.00(10.00)\n",
      "Iter 1870 | Time 27.3657(26.0411) | Bit/dim 1.2478(1.2375) | Steps 548(521.57) | Grad Norm 20.8421(11.4118) | Total Time 10.00(10.00)\n",
      "Iter 1880 | Time 26.1952(26.0427) | Bit/dim 1.2294(1.2413) | Steps 548(522.02) | Grad Norm 14.7117(12.3277) | Total Time 10.00(10.00)\n",
      "Iter 1890 | Time 26.1975(26.0550) | Bit/dim 1.2343(1.2442) | Steps 548(523.33) | Grad Norm 10.2308(12.6172) | Total Time 10.00(10.00)\n",
      "Iter 1900 | Time 25.9358(26.0306) | Bit/dim 1.2364(1.2422) | Steps 530(522.95) | Grad Norm 7.5899(12.4044) | Total Time 10.00(10.00)\n",
      "Iter 1910 | Time 26.0382(26.0096) | Bit/dim 1.2154(1.2407) | Steps 512(522.62) | Grad Norm 4.1562(12.3104) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0029 | Time 98.6885, Epoch Time 1832.0221, Bit/dim 1.2384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1920 | Time 25.6338(26.0301) | Bit/dim 1.2198(1.2396) | Steps 518(522.88) | Grad Norm 5.6722(12.2833) | Total Time 10.00(10.00)\n",
      "Iter 1930 | Time 25.6541(26.0438) | Bit/dim 1.2286(1.2347) | Steps 518(523.21) | Grad Norm 10.0029(11.5173) | Total Time 10.00(10.00)\n",
      "Iter 1940 | Time 24.8745(26.0738) | Bit/dim 1.2276(1.2292) | Steps 512(524.32) | Grad Norm 12.8635(10.4202) | Total Time 10.00(10.00)\n",
      "Iter 1950 | Time 25.2384(26.0621) | Bit/dim 1.2501(1.2257) | Steps 512(525.45) | Grad Norm 19.1953(9.9689) | Total Time 10.00(10.00)\n",
      "Iter 1960 | Time 26.0750(26.1124) | Bit/dim 1.2321(1.2246) | Steps 524(525.66) | Grad Norm 15.6617(10.5098) | Total Time 10.00(10.00)\n",
      "Iter 1970 | Time 25.6746(26.1013) | Bit/dim 1.2149(1.2227) | Steps 530(526.21) | Grad Norm 3.1644(9.9624) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0030 | Time 102.3087, Epoch Time 1842.4169, Bit/dim 1.2115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1980 | Time 26.3850(26.1126) | Bit/dim 1.1945(1.2213) | Steps 536(527.38) | Grad Norm 11.8026(10.3970) | Total Time 10.00(10.00)\n",
      "Iter 1990 | Time 25.6083(26.1543) | Bit/dim 1.2297(1.2205) | Steps 524(528.49) | Grad Norm 16.6158(10.4211) | Total Time 10.00(10.00)\n",
      "Iter 2000 | Time 25.5729(26.1581) | Bit/dim 1.2150(1.2206) | Steps 530(529.53) | Grad Norm 11.2109(10.3127) | Total Time 10.00(10.00)\n",
      "Iter 2010 | Time 27.0414(26.2299) | Bit/dim 1.2174(1.2197) | Steps 536(530.46) | Grad Norm 13.6140(10.6062) | Total Time 10.00(10.00)\n",
      "Iter 2020 | Time 26.3753(26.2152) | Bit/dim 1.2205(1.2192) | Steps 530(530.67) | Grad Norm 7.4666(10.5349) | Total Time 10.00(10.00)\n",
      "Iter 2030 | Time 26.5921(26.2465) | Bit/dim 1.2119(1.2185) | Steps 530(530.63) | Grad Norm 8.8486(10.4926) | Total Time 10.00(10.00)\n",
      "Iter 2040 | Time 26.4806(26.1950) | Bit/dim 1.2184(1.2179) | Steps 542(529.99) | Grad Norm 12.0089(10.7511) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0031 | Time 106.2539, Epoch Time 1853.8421, Bit/dim 1.2306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2050 | Time 25.6833(26.2152) | Bit/dim 1.2324(1.2238) | Steps 518(530.49) | Grad Norm 13.9938(12.2574) | Total Time 10.00(10.00)\n",
      "Iter 2060 | Time 26.3216(26.2446) | Bit/dim 1.2301(1.2273) | Steps 536(530.44) | Grad Norm 5.0711(12.5196) | Total Time 10.00(10.00)\n",
      "Iter 2070 | Time 26.9279(26.3283) | Bit/dim 1.2140(1.2261) | Steps 536(531.32) | Grad Norm 4.3683(12.2145) | Total Time 10.00(10.00)\n",
      "Iter 2080 | Time 26.3372(26.3657) | Bit/dim 1.2194(1.2233) | Steps 530(531.42) | Grad Norm 8.1586(11.1445) | Total Time 10.00(10.00)\n",
      "Iter 2090 | Time 26.6166(26.4054) | Bit/dim 1.1979(1.2181) | Steps 536(532.31) | Grad Norm 5.9802(9.8579) | Total Time 10.00(10.00)\n",
      "Iter 2100 | Time 27.0217(26.4221) | Bit/dim 1.1823(1.2128) | Steps 536(533.28) | Grad Norm 7.2857(9.0874) | Total Time 10.00(10.00)\n",
      "Iter 2110 | Time 26.7673(26.4093) | Bit/dim 1.2105(1.2124) | Steps 530(532.88) | Grad Norm 12.3159(9.7179) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0032 | Time 103.6350, Epoch Time 1865.1114, Bit/dim 1.2259\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2120 | Time 25.6449(26.3604) | Bit/dim 1.2274(1.2127) | Steps 524(532.75) | Grad Norm 18.5014(10.3805) | Total Time 10.00(10.00)\n",
      "Iter 2130 | Time 26.7436(26.3695) | Bit/dim 1.1956(1.2111) | Steps 536(533.31) | Grad Norm 5.1277(10.5570) | Total Time 10.00(10.00)\n",
      "Iter 2140 | Time 26.9342(26.3950) | Bit/dim 1.2065(1.2102) | Steps 536(532.79) | Grad Norm 14.2512(10.3891) | Total Time 10.00(10.00)\n",
      "Iter 2220 | Time 26.5270(26.5736) | Bit/dim 1.2114(1.2165) | Steps 530(534.52) | Grad Norm 12.1040(12.2309) | Total Time 10.00(10.00)\n",
      "Iter 2230 | Time 26.7886(26.5784) | Bit/dim 1.1937(1.2133) | Steps 536(533.97) | Grad Norm 7.2921(10.9182) | Total Time 10.00(10.00)\n",
      "Iter 2240 | Time 26.2309(26.5105) | Bit/dim 1.1969(1.2091) | Steps 530(533.88) | Grad Norm 8.3876(9.9202) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0034 | Time 104.4203, Epoch Time 1872.3768, Bit/dim 1.1819\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2250 | Time 25.6807(26.4629) | Bit/dim 1.2233(1.2083) | Steps 530(533.65) | Grad Norm 17.4084(10.3875) | Total Time 10.00(10.00)\n",
      "Iter 2260 | Time 27.0939(26.5404) | Bit/dim 1.2523(1.2142) | Steps 530(535.04) | Grad Norm 17.5836(11.5252) | Total Time 10.00(10.00)\n",
      "Iter 2270 | Time 25.7256(26.6069) | Bit/dim 1.2073(1.2169) | Steps 530(535.90) | Grad Norm 11.8654(11.9337) | Total Time 10.00(10.00)\n",
      "Iter 2280 | Time 26.2801(26.5650) | Bit/dim 1.1946(1.2140) | Steps 530(535.58) | Grad Norm 11.1016(11.0945) | Total Time 10.00(10.00)\n",
      "Iter 2290 | Time 26.4087(26.5534) | Bit/dim 1.2002(1.2122) | Steps 530(534.93) | Grad Norm 8.9433(10.9883) | Total Time 10.00(10.00)\n",
      "Iter 2300 | Time 25.8464(26.5207) | Bit/dim 1.1844(1.2109) | Steps 536(535.01) | Grad Norm 3.3050(10.9145) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0035 | Time 103.8420, Epoch Time 1874.3236, Bit/dim 1.1953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2310 | Time 26.3991(26.5646) | Bit/dim 1.1988(1.2130) | Steps 536(535.26) | Grad Norm 8.6900(11.4989) | Total Time 10.00(10.00)\n",
      "Iter 2320 | Time 26.4312(26.5988) | Bit/dim 1.1873(1.2098) | Steps 536(535.42) | Grad Norm 3.3874(10.8338) | Total Time 10.00(10.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p train_cnf.py --data mnist --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 900 --save experiments/cnf_nocond_bs900 --conditional False --log_freq 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
