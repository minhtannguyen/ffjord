{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_rl.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "GATES = [\"cnn1\", \"cnn2\", \"rnn\"]\n",
      "\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--gate', type=str, default='cnn1', choices=GATES)\n",
      "parser.add_argument('--scale', type=float, default=1.0)\n",
      "parser.add_argument('--scale_fac', type=float, default=1.0)\n",
      "parser.add_argument('--scale_std', type=float, default=1.0)\n",
      "parser.add_argument('--eta', default=0.1, type=float,\n",
      "                        help='tuning parameter that allows us to trade-off the competing goals of' \n",
      "                                'minimizing the prediction loss and maximizing the gate rewards ')\n",
      "parser.add_argument('--rl-weight', default=0.01, type=float,\n",
      "                        help='rl weight')\n",
      "\n",
      "parser.add_argument('--gamma', default=0.99, type=float,\n",
      "                        help='discount factor, default: (0.99)')\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "import lib.odenvp_conditional_rl as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol, \"scale\": args.scale, \"scale_fac\": args.scale_fac, \"scale_std\": args.scale_std, \"gate\": args.gate},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            # re-weight the gate rewards\n",
      "            normalized_eta = args.eta / len(logp_actions)\n",
      "            \n",
      "            # collect cumulative future rewards\n",
      "            R = - loss\n",
      "            cum_rewards = []\n",
      "            for r in nfe[::-1]:\n",
      "                R = -normalized_eta * r.view(-1,1) + args.gamma * R\n",
      "                cum_rewards.insert(0,R)\n",
      "            \n",
      "            # apply REINFORCE\n",
      "            rl_loss = 0\n",
      "            for lpa, r in zip(logp_actions, cum_rewards):\n",
      "                rl_loss = rl_loss - lpa.view(-1,1) * args.rl_weight * r\n",
      "                \n",
      "            loss = loss + rl_loss.mean()\n",
      "            \n",
      "            loss.backward()\n",
      "            \n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "            \n",
      "            for idx in range(len(model.module.transforms)):\n",
      "                for layer in model.module.transforms[idx].chain:\n",
      "                    if hasattr(layer, 'atol'):\n",
      "                        layer.odefunc.after_odeint()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'train': atol[tol_indx].mean()}, itr)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'train': rtol[tol_indx].mean()}, itr)\n",
      "                    \n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples, atol, rtol, logp_actions, nfe = model(fixed_z, reverse=True)\n",
      "            generated_samples = generated_samples.view(-1, *data_shape)\n",
      "            for tol_indx in range(len(atol)):\n",
      "                writer.add_scalars('atol_gen_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                writer.add_scalars('rtol_gen_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=0.0001, autoencode=False, batch_norm=False, batch_size=8000, batch_size_schedule='', begin_epoch=1, condition_ratio=0.5, conditional=True, controlled_tol=True, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, eta=0.1, gamma=0.99, gate='cnn1', imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=1, lr=0.01, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume=None, rl_weight=0.01, rtol=0.0001, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_rl_stdscale_1_run1', scale=1.0, scale_fac=1.0, scale_std=1.0, seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=5000, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000.0, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1450732\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc_rl.py:291: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Iter 0001 | Time 109.4222(109.4222) | Bit/dim 8.9616(8.9616) | Xent 2.3026(2.3026) | Loss 24.3089(24.3089) | Error 0.8982(0.8982) Steps 0(0.00) | Grad Norm 32.1866(32.1866) | Total Time 0.00(0.00)\n",
      "Iter 0002 | Time 46.4699(107.5336) | Bit/dim 8.8753(8.9590) | Xent 2.2924(2.3023) | Loss 24.0486(24.3011) | Error 0.7749(0.8945) Steps 0(0.00) | Grad Norm 29.0579(32.0927) | Total Time 0.00(0.00)\n",
      "Iter 0003 | Time 45.4683(105.6717) | Bit/dim 8.7787(8.9536) | Xent 2.2788(2.3016) | Loss 23.8803(24.2885) | Error 0.7664(0.8907) Steps 0(0.00) | Grad Norm 24.4114(31.8623) | Total Time 0.00(0.00)\n",
      "Iter 0004 | Time 47.5585(103.9283) | Bit/dim 8.7008(8.9460) | Xent 2.2612(2.3004) | Loss 23.1753(24.2551) | Error 0.7584(0.8867) Steps 0(0.00) | Grad Norm 18.3393(31.4566) | Total Time 0.00(0.00)\n",
      "Iter 0005 | Time 45.5345(102.1764) | Bit/dim 8.6246(8.9364) | Xent 2.2404(2.2986) | Loss 23.2662(24.2254) | Error 0.7541(0.8828) Steps 0(0.00) | Grad Norm 13.1259(30.9067) | Total Time 0.00(0.00)\n",
      "Iter 0006 | Time 45.5995(100.4791) | Bit/dim 8.5362(8.9244) | Xent 2.2224(2.2963) | Loss 23.0038(24.1888) | Error 0.7511(0.8788) Steps 0(0.00) | Grad Norm 9.8530(30.2751) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0001 | Time 39.7851, Epoch Time 396.5323(396.5323), Bit/dim 8.4975(best: inf), Xent 2.1969, Loss 9.5959, Error 0.7389(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0007 | Time 54.3336(99.0948) | Bit/dim 8.5288(8.9125) | Xent 2.1993(2.2934) | Loss 27.2230(24.2798) | Error 0.7540(0.8751) Steps 0(0.00) | Grad Norm 9.6925(29.6576) | Total Time 0.00(0.00)\n",
      "Iter 0008 | Time 48.6737(97.5821) | Bit/dim 8.4838(8.8996) | Xent 2.1766(2.2899) | Loss 23.0089(24.2417) | Error 0.7489(0.8713) Steps 0(0.00) | Grad Norm 12.6395(29.1471) | Total Time 0.00(0.00)\n",
      "Iter 0009 | Time 45.5854(96.0222) | Bit/dim 8.4525(8.8862) | Xent 2.1703(2.2863) | Loss 22.5274(24.1902) | Error 0.7634(0.8680) Steps 0(0.00) | Grad Norm 15.3396(28.7328) | Total Time 0.00(0.00)\n",
      "Iter 0010 | Time 50.1473(94.6460) | Bit/dim 8.4076(8.8719) | Xent 2.1583(2.2824) | Loss 22.6790(24.1449) | Error 0.7638(0.8649) Steps 0(0.00) | Grad Norm 17.5023(28.3959) | Total Time 0.00(0.00)\n",
      "Iter 0011 | Time 46.3488(93.1971) | Bit/dim 8.3450(8.8561) | Xent 2.1510(2.2785) | Loss 22.5536(24.0972) | Error 0.7536(0.8616) Steps 0(0.00) | Grad Norm 17.4954(28.0689) | Total Time 0.00(0.00)\n",
      "Iter 0012 | Time 46.0755(91.7834) | Bit/dim 8.2956(8.8393) | Xent 2.1450(2.2745) | Loss 22.5315(24.0502) | Error 0.7499(0.8582) Steps 0(0.00) | Grad Norm 16.2320(27.7138) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0002 | Time 23.1086, Epoch Time 330.3787(394.5477), Bit/dim 8.2158(best: 8.4975), Xent 2.1141, Loss 9.2729, Error 0.7341(best: 0.7389)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0013 | Time 44.7381(90.3721) | Bit/dim 8.1959(8.8200) | Xent 2.1187(2.2698) | Loss 25.9513(24.1072) | Error 0.7411(0.8547) Steps 0(0.00) | Grad Norm 12.7854(27.2660) | Total Time 0.00(0.00)\n",
      "Iter 0014 | Time 44.5987(88.9989) | Bit/dim 8.1814(8.8008) | Xent 2.1054(2.2649) | Loss 21.9790(24.0434) | Error 0.7166(0.8506) Steps 0(0.00) | Grad Norm 10.3976(26.7599) | Total Time 0.00(0.00)\n",
      "Iter 0015 | Time 47.8875(87.7655) | Bit/dim 8.0842(8.7793) | Xent 2.0851(2.2595) | Loss 21.3438(23.9624) | Error 0.7249(0.8468) Steps 0(0.00) | Grad Norm 7.7609(26.1899) | Total Time 0.00(0.00)\n",
      "Iter 0016 | Time 44.3013(86.4616) | Bit/dim 8.0410(8.7571) | Xent 2.0774(2.2540) | Loss 21.9189(23.9011) | Error 0.7255(0.8432) Steps 0(0.00) | Grad Norm 9.4628(25.6881) | Total Time 0.00(0.00)\n",
      "Iter 0017 | Time 45.9288(85.2456) | Bit/dim 7.9856(8.7340) | Xent 2.0797(2.2488) | Loss 21.5667(23.8311) | Error 0.7310(0.8398) Steps 0(0.00) | Grad Norm 12.2253(25.2842) | Total Time 0.00(0.00)\n",
      "Iter 0018 | Time 44.8520(84.0338) | Bit/dim 7.9055(8.7091) | Xent 2.0754(2.2436) | Loss 21.7266(23.7679) | Error 0.7346(0.8366) Steps 0(0.00) | Grad Norm 13.2858(24.9243) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0003 | Time 22.9196, Epoch Time 311.1762(392.0466), Bit/dim 7.8072(best: 8.2158), Xent 2.0453, Loss 8.8298, Error 0.7135(best: 0.7341)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0019 | Time 45.5206(82.8784) | Bit/dim 7.8174(8.6824) | Xent 2.0539(2.2379) | Loss 25.6602(23.8247) | Error 0.7218(0.8332) Steps 0(0.00) | Grad Norm 12.6399(24.5557) | Total Time 0.00(0.00)\n",
      "Iter 0020 | Time 48.7448(81.8544) | Bit/dim 7.7158(8.6534) | Xent 2.0344(2.2318) | Loss 21.0976(23.7429) | Error 0.6964(0.8291) Steps 0(0.00) | Grad Norm 9.4453(24.1024) | Total Time 0.00(0.00)\n",
      "Iter 0021 | Time 47.1584(80.8135) | Bit/dim 7.6228(8.6225) | Xent 2.0424(2.2261) | Loss 20.3811(23.6420) | Error 0.6935(0.8250) Steps 0(0.00) | Grad Norm 5.9155(23.5568) | Total Time 0.00(0.00)\n",
      "Iter 0022 | Time 45.5062(79.7543) | Bit/dim 7.5403(8.5900) | Xent 2.0291(2.2202) | Loss 20.7106(23.5541) | Error 0.6911(0.8210) Steps 0(0.00) | Grad Norm 6.7968(23.0540) | Total Time 0.00(0.00)\n",
      "Iter 0023 | Time 44.9301(78.7096) | Bit/dim 7.4684(8.5564) | Xent 2.0349(2.2146) | Loss 20.0065(23.4477) | Error 0.7015(0.8174) Steps 0(0.00) | Grad Norm 9.6423(22.6517) | Total Time 0.00(0.00)\n",
      "Iter 0024 | Time 44.4620(77.6821) | Bit/dim 7.4084(8.5219) | Xent 2.0343(2.2092) | Loss 20.4586(23.3580) | Error 0.6983(0.8138) Steps 0(0.00) | Grad Norm 11.0119(22.3025) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0004 | Time 23.0901, Epoch Time 315.7833(389.7587), Bit/dim 7.3429(best: 7.8072), Xent 2.0257, Loss 8.3557, Error 0.6921(best: 0.7135)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0025 | Time 44.6590(76.6914) | Bit/dim 7.3441(8.4866) | Xent 2.0332(2.2040) | Loss 24.5949(23.3951) | Error 0.6986(0.8104) Steps 0(0.00) | Grad Norm 9.7750(21.9267) | Total Time 0.00(0.00)\n",
      "Iter 0026 | Time 46.6947(75.7915) | Bit/dim 7.2790(8.4504) | Xent 2.0287(2.1987) | Loss 20.1989(23.2992) | Error 0.6957(0.8069) Steps 0(0.00) | Grad Norm 5.7412(21.4411) | Total Time 0.00(0.00)\n",
      "Iter 0027 | Time 45.5123(74.8832) | Bit/dim 7.2263(8.4136) | Xent 2.0325(2.1937) | Loss 20.1100(23.2035) | Error 0.6776(0.8031) Steps 0(0.00) | Grad Norm 6.0883(20.9805) | Total Time 0.00(0.00)\n",
      "Iter 0028 | Time 47.2730(74.0549) | Bit/dim 7.1846(8.3768) | Xent 2.0304(2.1888) | Loss 19.8845(23.1040) | Error 0.6765(0.7993) Steps 0(0.00) | Grad Norm 8.6238(20.6098) | Total Time 0.00(0.00)\n",
      "Iter 0029 | Time 45.2792(73.1916) | Bit/dim 7.1469(8.3399) | Xent 2.0380(2.1843) | Loss 20.0255(23.0116) | Error 0.6846(0.7958) Steps 0(0.00) | Grad Norm 7.6953(20.2224) | Total Time 0.00(0.00)\n",
      "Iter 0030 | Time 47.4060(72.4180) | Bit/dim 7.1215(8.3033) | Xent 2.0328(2.1797) | Loss 19.7185(22.9128) | Error 0.6935(0.7928) Steps 0(0.00) | Grad Norm 4.8444(19.7610) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0005 | Time 22.8451, Epoch Time 315.7479(387.5383), Bit/dim 7.0977(best: 7.3429), Xent 2.0322, Loss 8.1138, Error 0.6807(best: 0.6921)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0031 | Time 46.9078(71.6527) | Bit/dim 7.0921(8.2670) | Xent 2.0416(2.1756) | Loss 23.7191(22.9370) | Error 0.6999(0.7900) Steps 0(0.00) | Grad Norm 6.0486(19.3497) | Total Time 0.00(0.00)\n",
      "Iter 0032 | Time 46.8050(70.9073) | Bit/dim 7.0769(8.2313) | Xent 2.0433(2.1716) | Loss 19.6301(22.8378) | Error 0.7115(0.7876) Steps 0(0.00) | Grad Norm 6.7078(18.9704) | Total Time 0.00(0.00)\n",
      "Iter 0033 | Time 51.0292(70.3109) | Bit/dim 7.0565(8.1960) | Xent 2.0509(2.1680) | Loss 19.3367(22.7328) | Error 0.7250(0.7857) Steps 0(0.00) | Grad Norm 3.9868(18.5209) | Total Time 0.00(0.00)\n",
      "Iter 0034 | Time 47.1131(69.6150) | Bit/dim 7.0569(8.1619) | Xent 2.0542(2.1646) | Loss 19.5087(22.6360) | Error 0.7163(0.7837) Steps 0(0.00) | Grad Norm 6.4920(18.1600) | Total Time 0.00(0.00)\n",
      "Iter 0035 | Time 49.2820(69.0050) | Bit/dim 7.0414(8.1283) | Xent 2.0498(2.1612) | Loss 19.7021(22.5480) | Error 0.7114(0.7815) Steps 0(0.00) | Grad Norm 4.8739(17.7614) | Total Time 0.00(0.00)\n",
      "Iter 0036 | Time 50.0431(68.4362) | Bit/dim 7.0308(8.0953) | Xent 2.0377(2.1575) | Loss 19.4952(22.4564) | Error 0.7101(0.7793) Steps 0(0.00) | Grad Norm 5.2036(17.3847) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0006 | Time 23.8803, Epoch Time 330.9789(385.8416), Bit/dim 7.0258(best: 7.0977), Xent 2.0389, Loss 8.0453, Error 0.7041(best: 0.6807)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0037 | Time 52.9384(67.9712) | Bit/dim 7.0200(8.0631) | Xent 2.0406(2.1539) | Loss 22.6730(22.4629) | Error 0.7092(0.7772) Steps 0(0.00) | Grad Norm 6.1845(17.0487) | Total Time 0.00(0.00)\n",
      "Iter 0038 | Time 55.5333(67.5981) | Bit/dim 7.0150(8.0316) | Xent 2.0496(2.1508) | Loss 19.3836(22.3705) | Error 0.7006(0.7749) Steps 0(0.00) | Grad Norm 5.6176(16.7058) | Total Time 0.00(0.00)\n",
      "Iter 0039 | Time 60.6949(67.3910) | Bit/dim 7.0036(8.0008) | Xent 2.0464(2.1477) | Loss 18.9703(22.2685) | Error 0.7046(0.7728) Steps 0(0.00) | Grad Norm 5.4845(16.3691) | Total Time 0.00(0.00)\n",
      "Iter 0040 | Time 55.4256(67.0320) | Bit/dim 7.0030(7.9709) | Xent 2.0408(2.1445) | Loss 19.3937(22.1823) | Error 0.7141(0.7711) Steps 0(0.00) | Grad Norm 5.9374(16.0562) | Total Time 0.00(0.00)\n",
      "Iter 0041 | Time 55.3658(66.6820) | Bit/dim 7.0013(7.9418) | Xent 2.0232(2.1408) | Loss 19.6272(22.1056) | Error 0.7091(0.7692) Steps 0(0.00) | Grad Norm 5.6964(15.7454) | Total Time 0.00(0.00)\n",
      "Iter 0042 | Time 58.6388(66.4407) | Bit/dim 7.0032(7.9136) | Xent 2.0465(2.1380) | Loss 19.2843(22.0210) | Error 0.7064(0.7673) Steps 0(0.00) | Grad Norm 3.5392(15.3792) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0007 | Time 24.2616, Epoch Time 380.6252(385.6851), Bit/dim 6.9929(best: 7.0258), Xent 2.0154, Loss 8.0006, Error 0.6837(best: 0.6807)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0043 | Time 60.5295(66.2634) | Bit/dim 6.9855(7.8858) | Xent 2.0206(2.1345) | Loss 23.5568(22.0671) | Error 0.6957(0.7652) Steps 0(0.00) | Grad Norm 4.0535(15.0394) | Total Time 0.00(0.00)\n",
      "Iter 0044 | Time 60.3617(66.0864) | Bit/dim 6.9819(7.8586) | Xent 2.0139(2.1309) | Loss 19.2912(21.9838) | Error 0.6934(0.7630) Steps 0(0.00) | Grad Norm 4.1292(14.7121) | Total Time 0.00(0.00)\n",
      "Iter 0045 | Time 59.0177(65.8743) | Bit/dim 6.9803(7.8323) | Xent 2.0193(2.1275) | Loss 19.6407(21.9135) | Error 0.7025(0.7612) Steps 0(0.00) | Grad Norm 11.1686(14.6058) | Total Time 0.00(0.00)\n",
      "Iter 0046 | Time 58.5602(65.6549) | Bit/dim 6.9723(7.8065) | Xent 2.0566(2.1254) | Loss 19.2488(21.8336) | Error 0.7258(0.7601) Steps 0(0.00) | Grad Norm 27.4889(14.9923) | Total Time 0.00(0.00)\n",
      "Iter 0047 | Time 59.5701(65.4723) | Bit/dim 6.9981(7.7822) | Xent 2.1591(2.1264) | Loss 19.5920(21.7663) | Error 0.7714(0.7605) Steps 0(0.00) | Grad Norm 55.8594(16.2183) | Total Time 0.00(0.00)\n",
      "Iter 0048 | Time 61.8164(65.3627) | Bit/dim 7.0082(7.7590) | Xent 2.2759(2.1309) | Loss 19.8969(21.7102) | Error 0.7970(0.7616) Steps 0(0.00) | Grad Norm 69.8549(17.8274) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0008 | Time 24.6458, Epoch Time 400.8632(386.1404), Bit/dim 6.9581(best: 6.9929), Xent 2.0235, Loss 7.9699, Error 0.7110(best: 0.6807)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0049 | Time 60.7646(65.2247) | Bit/dim 6.9530(7.7348) | Xent 2.0401(2.1282) | Loss 23.1456(21.7533) | Error 0.7235(0.7604) Steps 0(0.00) | Grad Norm 25.9094(18.0699) | Total Time 0.00(0.00)\n",
      "Iter 0050 | Time 61.3936(65.1098) | Bit/dim 6.9695(7.7119) | Xent 2.0875(2.1269) | Loss 19.4815(21.6851) | Error 0.7403(0.7598) Steps 0(0.00) | Grad Norm 40.1473(18.7322) | Total Time 0.00(0.00)\n",
      "Iter 0051 | Time 62.0754(65.0187) | Bit/dim 6.9508(7.6891) | Xent 2.0612(2.1250) | Loss 19.3073(21.6138) | Error 0.7369(0.7591) Steps 0(0.00) | Grad Norm 34.7677(19.2133) | Total Time 0.00(0.00)\n",
      "Iter 0052 | Time 61.9672(64.9272) | Bit/dim 6.9314(7.6663) | Xent 2.0249(2.1220) | Loss 18.4714(21.5195) | Error 0.7091(0.7576) Steps 0(0.00) | Grad Norm 22.5784(19.3142) | Total Time 0.00(0.00)\n",
      "Iter 0053 | Time 58.9182(64.7469) | Bit/dim 6.9351(7.6444) | Xent 2.0494(2.1198) | Loss 19.2934(21.4528) | Error 0.7300(0.7568) Steps 0(0.00) | Grad Norm 27.8434(19.5701) | Total Time 0.00(0.00)\n",
      "Iter 0054 | Time 61.8708(64.6606) | Bit/dim 6.9381(7.6232) | Xent 2.0031(2.1163) | Loss 19.1444(21.3835) | Error 0.7024(0.7552) Steps 0(0.00) | Grad Norm 19.0970(19.5559) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0009 | Time 25.0694, Epoch Time 407.8101(386.7905), Bit/dim 6.9102(best: 6.9581), Xent 1.9931, Loss 7.9067, Error 0.6780(best: 0.6807)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0055 | Time 60.9589(64.5496) | Bit/dim 6.9128(7.6019) | Xent 2.0142(2.1132) | Loss 23.5286(21.4479) | Error 0.6885(0.7532) Steps 0(0.00) | Grad Norm 15.4621(19.4331) | Total Time 0.00(0.00)\n",
      "Iter 0056 | Time 57.8485(64.3486) | Bit/dim 6.9196(7.5814) | Xent 2.0060(2.1100) | Loss 19.3382(21.3846) | Error 0.6834(0.7511) Steps 0(0.00) | Grad Norm 19.4329(19.4331) | Total Time 0.00(0.00)\n",
      "Iter 0057 | Time 58.2643(64.1660) | Bit/dim 6.8788(7.5603) | Xent 1.9869(2.1063) | Loss 18.7700(21.3061) | Error 0.6765(0.7488) Steps 0(0.00) | Grad Norm 4.8651(18.9960) | Total Time 0.00(0.00)\n",
      "Iter 0058 | Time 60.1927(64.0468) | Bit/dim 6.8980(7.5405) | Xent 1.9989(2.1031) | Loss 19.1201(21.2405) | Error 0.7030(0.7475) Steps 0(0.00) | Grad Norm 17.0112(18.9365) | Total Time 0.00(0.00)\n",
      "Iter 0059 | Time 60.2196(63.9320) | Bit/dim 6.8671(7.5203) | Xent 1.9904(2.0997) | Loss 19.1259(21.1771) | Error 0.6815(0.7455) Steps 0(0.00) | Grad Norm 7.7267(18.6002) | Total Time 0.00(0.00)\n",
      "Iter 0060 | Time 58.2300(63.7610) | Bit/dim 6.8607(7.5005) | Xent 1.9708(2.0958) | Loss 19.0477(21.1132) | Error 0.6604(0.7429) Steps 0(0.00) | Grad Norm 9.2040(18.3183) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0010 | Time 24.3230, Epoch Time 399.2500(387.1643), Bit/dim 6.8563(best: 6.9102), Xent 1.9735, Loss 7.8430, Error 0.6536(best: 0.6780)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0061 | Time 54.8643(63.4941) | Bit/dim 6.8473(7.4809) | Xent 1.9866(2.0926) | Loss 23.1937(21.1756) | Error 0.6585(0.7404) Steps 0(0.00) | Grad Norm 12.5305(18.1447) | Total Time 0.00(0.00)\n",
      "Iter 0062 | Time 56.7639(63.2922) | Bit/dim 6.8258(7.4612) | Xent 1.9720(2.0890) | Loss 18.7240(21.1021) | Error 0.6716(0.7383) Steps 0(0.00) | Grad Norm 3.8476(17.7158) | Total Time 0.00(0.00)\n",
      "Iter 0063 | Time 50.7674(62.9164) | Bit/dim 6.8218(7.4421) | Xent 1.9765(2.0856) | Loss 18.4624(21.0229) | Error 0.6799(0.7366) Steps 0(0.00) | Grad Norm 11.2239(17.5210) | Total Time 0.00(0.00)\n",
      "Iter 0064 | Time 54.4055(62.6611) | Bit/dim 6.8086(7.4230) | Xent 1.9703(2.0821) | Loss 19.0609(20.9640) | Error 0.6725(0.7347) Steps 0(0.00) | Grad Norm 9.5983(17.2833) | Total Time 0.00(0.00)\n",
      "Iter 0065 | Time 58.0793(62.5236) | Bit/dim 6.7901(7.4041) | Xent 1.9656(2.0786) | Loss 18.9232(20.9028) | Error 0.6650(0.7326) Steps 0(0.00) | Grad Norm 7.6182(16.9934) | Total Time 0.00(0.00)\n",
      "Iter 0066 | Time 52.9039(62.2350) | Bit/dim 6.7885(7.3856) | Xent 1.9565(2.0750) | Loss 18.9108(20.8431) | Error 0.6490(0.7301) Steps 0(0.00) | Grad Norm 6.7542(16.6862) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0011 | Time 23.9327, Epoch Time 375.2181(386.8059), Bit/dim 6.7517(best: 6.8563), Xent 1.9406, Loss 7.7220, Error 0.6471(best: 0.6536)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0067 | Time 57.6958(62.0989) | Bit/dim 6.7437(7.3663) | Xent 1.9489(2.0712) | Loss 22.6992(20.8987) | Error 0.6574(0.7279) Steps 0(0.00) | Grad Norm 4.4585(16.3194) | Total Time 0.00(0.00)\n",
      "Iter 0068 | Time 49.8871(61.7325) | Bit/dim 6.7321(7.3473) | Xent 1.9622(2.0679) | Loss 18.9969(20.8417) | Error 0.6754(0.7263) Steps 0(0.00) | Grad Norm 12.1427(16.1941) | Total Time 0.00(0.00)\n",
      "Iter 0069 | Time 52.5225(61.4562) | Bit/dim 6.7242(7.3286) | Xent 1.9978(2.0658) | Loss 18.8826(20.7829) | Error 0.7012(0.7256) Steps 0(0.00) | Grad Norm 22.3582(16.3790) | Total Time 0.00(0.00)\n",
      "Iter 0070 | Time 50.3918(61.1243) | Bit/dim 6.7331(7.3108) | Xent 2.1678(2.0689) | Loss 19.2303(20.7363) | Error 0.7761(0.7271) Steps 0(0.00) | Grad Norm 52.7843(17.4712) | Total Time 0.00(0.00)\n",
      "Iter 0071 | Time 56.5024(60.9856) | Bit/dim 6.7372(7.2935) | Xent 2.4824(2.0813) | Loss 19.2997(20.6932) | Error 0.7586(0.7280) Steps 0(0.00) | Grad Norm 52.8858(18.5336) | Total Time 0.00(0.00)\n",
      "Iter 0072 | Time 48.2447(60.6034) | Bit/dim 6.6617(7.2746) | Xent 2.0173(2.0794) | Loss 18.7341(20.6345) | Error 0.6997(0.7272) Steps 0(0.00) | Grad Norm 19.4088(18.5599) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0012 | Time 24.0318, Epoch Time 355.5426(385.8680), Bit/dim 6.6803(best: 6.7517), Xent 2.3643, Loss 7.8624, Error 0.8150(best: 0.6471)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0073 | Time 50.8530(60.3109) | Bit/dim 6.6743(7.2566) | Xent 2.3658(2.0879) | Loss 22.4303(20.6883) | Error 0.8241(0.7301) Steps 0(0.00) | Grad Norm 54.6383(19.6422) | Total Time 0.00(0.00)\n",
      "Iter 0074 | Time 54.3365(60.1316) | Bit/dim 6.6178(7.2374) | Xent 2.0374(2.0864) | Loss 18.6825(20.6282) | Error 0.7130(0.7296) Steps 0(0.00) | Grad Norm 37.4895(20.1776) | Total Time 0.00(0.00)\n",
      "Iter 0075 | Time 52.4348(59.9007) | Bit/dim 6.6330(7.2193) | Xent 2.0179(2.0844) | Loss 18.0771(20.5516) | Error 0.7468(0.7301) Steps 0(0.00) | Grad Norm 30.6595(20.4921) | Total Time 0.00(0.00)\n",
      "Iter 0076 | Time 50.1424(59.6080) | Bit/dim 6.5929(7.2005) | Xent 2.0144(2.0823) | Loss 18.5736(20.4923) | Error 0.7284(0.7300) Steps 0(0.00) | Grad Norm 14.7047(20.3185) | Total Time 0.00(0.00)\n",
      "Iter 0077 | Time 49.5391(59.3059) | Bit/dim 6.5480(7.1809) | Xent 2.0539(2.0814) | Loss 18.3347(20.4276) | Error 0.7283(0.7300) Steps 0(0.00) | Grad Norm 35.9626(20.7878) | Total Time 0.00(0.00)\n",
      "Iter 0078 | Time 55.8701(59.2028) | Bit/dim 6.5309(7.1614) | Xent 2.1422(2.0832) | Loss 18.5787(20.3721) | Error 0.7594(0.7309) Steps 0(0.00) | Grad Norm 60.7881(21.9878) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0013 | Time 23.2733, Epoch Time 352.7918(384.8757), Bit/dim 6.5308(best: 6.6803), Xent 2.2100, Loss 7.6358, Error 0.8051(best: 0.6471)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0079 | Time 48.0899(58.8695) | Bit/dim 6.5279(7.1424) | Xent 2.2489(2.0882) | Loss 22.8302(20.4458) | Error 0.8155(0.7334) Steps 0(0.00) | Grad Norm 83.0585(23.8199) | Total Time 0.00(0.00)\n",
      "Iter 0080 | Time 50.6013(58.6214) | Bit/dim 6.5341(7.1242) | Xent 2.2721(2.0937) | Loss 18.8539(20.3981) | Error 0.8011(0.7354) Steps 0(0.00) | Grad Norm 87.0457(25.7167) | Total Time 0.00(0.00)\n",
      "Iter 0081 | Time 50.1964(58.3687) | Bit/dim 6.4449(7.1038) | Xent 2.0770(2.0932) | Loss 18.2208(20.3328) | Error 0.7392(0.7355) Steps 0(0.00) | Grad Norm 42.4247(26.2179) | Total Time 0.00(0.00)\n",
      "Iter 0082 | Time 50.8236(58.1423) | Bit/dim 6.3922(7.0824) | Xent 2.0676(2.0925) | Loss 17.9804(20.2622) | Error 0.7364(0.7356) Steps 0(0.00) | Grad Norm 34.1074(26.4546) | Total Time 0.00(0.00)\n",
      "Iter 0083 | Time 49.4332(57.8810) | Bit/dim 6.3503(7.0605) | Xent 2.0989(2.0927) | Loss 17.9332(20.1923) | Error 0.7516(0.7361) Steps 0(0.00) | Grad Norm 39.5399(26.8472) | Total Time 0.00(0.00)\n",
      "Iter 0084 | Time 53.0329(57.7356) | Bit/dim 6.2607(7.0365) | Xent 2.0378(2.0910) | Loss 17.7395(20.1187) | Error 0.7037(0.7351) Steps 0(0.00) | Grad Norm 23.4570(26.7455) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0014 | Time 23.4528, Epoch Time 341.4577(383.5732), Bit/dim 6.2238(best: 6.5308), Xent 2.0774, Loss 7.2625, Error 0.7476(best: 0.6471)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0085 | Time 49.8944(57.5004) | Bit/dim 6.2253(7.0121) | Xent 2.0859(2.0909) | Loss 21.5362(20.1613) | Error 0.7436(0.7353) Steps 0(0.00) | Grad Norm 30.9075(26.8703) | Total Time 0.00(0.00)\n",
      "Iter 0086 | Time 48.7913(57.2391) | Bit/dim 6.1556(6.9864) | Xent 2.0602(2.0899) | Loss 17.4649(20.0804) | Error 0.7284(0.7351) Steps 0(0.00) | Grad Norm 24.5637(26.8011) | Total Time 0.00(0.00)\n",
      "Iter 0087 | Time 47.6619(56.9518) | Bit/dim 6.0855(6.9594) | Xent 2.0695(2.0893) | Loss 17.6211(20.0066) | Error 0.7025(0.7342) Steps 0(0.00) | Grad Norm 25.6996(26.7681) | Total Time 0.00(0.00)\n",
      "Iter 0088 | Time 49.3421(56.7235) | Bit/dim 6.0154(6.9311) | Xent 2.0657(2.0886) | Loss 17.2110(19.9227) | Error 0.7389(0.7343) Steps 0(0.00) | Grad Norm 17.0685(26.4771) | Total Time 0.00(0.00)\n",
      "Iter 0089 | Time 50.2083(56.5280) | Bit/dim 5.9955(6.9030) | Xent 2.1022(2.0890) | Loss 17.0973(19.8380) | Error 0.7711(0.7354) Steps 0(0.00) | Grad Norm 30.9567(26.6115) | Total Time 0.00(0.00)\n",
      "Iter 0090 | Time 52.0819(56.3946) | Bit/dim 5.9308(6.8739) | Xent 2.0634(2.0883) | Loss 17.0064(19.7530) | Error 0.7278(0.7352) Steps 0(0.00) | Grad Norm 13.9350(26.2312) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0015 | Time 23.7337, Epoch Time 343.4673(382.3700), Bit/dim 5.9142(best: 6.2238), Xent 2.0339, Loss 6.9311, Error 0.6959(best: 0.6471)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0091 | Time 56.0609(56.3846) | Bit/dim 5.9151(6.8451) | Xent 2.0653(2.0876) | Loss 20.6146(19.7789) | Error 0.7152(0.7346) Steps 0(0.00) | Grad Norm 17.1884(25.9599) | Total Time 0.00(0.00)\n",
      "Iter 0092 | Time 50.1412(56.1973) | Bit/dim 5.9421(6.8180) | Xent 2.0554(2.0866) | Loss 16.6890(19.6862) | Error 0.7091(0.7338) Steps 0(0.00) | Grad Norm 51.6970(26.7320) | Total Time 0.00(0.00)\n",
      "Iter 0093 | Time 50.5269(56.0272) | Bit/dim 5.9717(6.7926) | Xent 2.1040(2.0871) | Loss 17.2203(19.6122) | Error 0.7680(0.7348) Steps 0(0.00) | Grad Norm 87.8631(28.5660) | Total Time 0.00(0.00)\n",
      "Iter 0094 | Time 53.9844(55.9659) | Bit/dim 6.1431(6.7731) | Xent 2.0723(2.0867) | Loss 17.4491(19.5473) | Error 0.7355(0.7349) Steps 0(0.00) | Grad Norm 83.9916(30.2287) | Total Time 0.00(0.00)\n",
      "Iter 0095 | Time 53.3613(55.8878) | Bit/dim 6.1630(6.7548) | Xent 2.0415(2.0853) | Loss 17.5340(19.4869) | Error 0.7185(0.7344) Steps 0(0.00) | Grad Norm 36.6938(30.4227) | Total Time 0.00(0.00)\n",
      "Iter 0096 | Time 49.3774(55.6925) | Bit/dim 5.8557(6.7279) | Xent 2.0145(2.0832) | Loss 16.7742(19.4055) | Error 0.6915(0.7331) Steps 0(0.00) | Grad Norm 34.9791(30.5594) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0016 | Time 22.8031, Epoch Time 359.0477(381.6703), Bit/dim 5.9138(best: 5.9142), Xent 2.0035, Loss 6.9156, Error 0.6743(best: 0.6471)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0097 | Time 47.7169(55.4532) | Bit/dim 5.9087(6.7033) | Xent 2.0200(2.0813) | Loss 21.0676(19.4554) | Error 0.6846(0.7316) Steps 0(0.00) | Grad Norm 18.3301(30.1925) | Total Time 0.00(0.00)\n",
      "Iter 0098 | Time 46.2388(55.1768) | Bit/dim 5.8832(6.6787) | Xent 2.0449(2.0802) | Loss 16.6839(19.3722) | Error 0.7246(0.7314) Steps 0(0.00) | Grad Norm 37.6881(30.4174) | Total Time 0.00(0.00)\n",
      "Iter 0099 | Time 48.3202(54.9711) | Bit/dim 5.7509(6.6508) | Xent 1.9947(2.0776) | Loss 16.5601(19.2879) | Error 0.6787(0.7298) Steps 0(0.00) | Grad Norm 12.2255(29.8716) | Total Time 0.00(0.00)\n",
      "Iter 0100 | Time 45.9264(54.6997) | Bit/dim 5.7956(6.6252) | Xent 1.9912(2.0750) | Loss 16.6316(19.2082) | Error 0.6823(0.7284) Steps 0(0.00) | Grad Norm 19.8100(29.5698) | Total Time 0.00(0.00)\n",
      "Iter 0101 | Time 47.6518(54.4883) | Bit/dim 5.7740(6.5997) | Xent 2.0070(2.0730) | Loss 16.7671(19.1350) | Error 0.6901(0.7273) Steps 0(0.00) | Grad Norm 16.9264(29.1905) | Total Time 0.00(0.00)\n",
      "Iter 0102 | Time 49.3890(54.3353) | Bit/dim 5.7303(6.5736) | Xent 2.0126(2.0712) | Loss 16.2082(19.0472) | Error 0.6890(0.7261) Steps 0(0.00) | Grad Norm 11.9252(28.6725) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0017 | Time 23.3372, Epoch Time 324.6904(379.9609), Bit/dim 5.6931(best: 5.9138), Xent 1.9821, Loss 6.6841, Error 0.6682(best: 0.6471)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0103 | Time 50.3810(54.2167) | Bit/dim 5.6973(6.5473) | Xent 2.0031(2.0691) | Loss 20.3818(19.0872) | Error 0.6886(0.7250) Steps 0(0.00) | Grad Norm 13.5667(28.2193) | Total Time 0.00(0.00)\n",
      "Iter 0104 | Time 47.5119(54.0155) | Bit/dim 5.6942(6.5217) | Xent 2.0037(2.0672) | Loss 16.2514(19.0021) | Error 0.7003(0.7242) Steps 0(0.00) | Grad Norm 12.8425(27.7580) | Total Time 0.00(0.00)\n",
      "Iter 0105 | Time 50.0271(53.8959) | Bit/dim 5.6542(6.4957) | Xent 1.9785(2.0645) | Loss 16.3425(18.9223) | Error 0.6825(0.7230) Steps 0(0.00) | Grad Norm 12.4717(27.2994) | Total Time 0.00(0.00)\n",
      "Iter 0106 | Time 51.9588(53.8378) | Bit/dim 5.6326(6.4698) | Xent 1.9923(2.0624) | Loss 16.2667(18.8427) | Error 0.6854(0.7219) Steps 0(0.00) | Grad Norm 10.2331(26.7874) | Total Time 0.00(0.00)\n",
      "Iter 0107 | Time 52.8498(53.8081) | Bit/dim 5.6307(6.4446) | Xent 1.9745(2.0597) | Loss 16.3629(18.7683) | Error 0.6863(0.7208) Steps 0(0.00) | Grad Norm 13.9498(26.4023) | Total Time 0.00(0.00)\n",
      "Iter 0108 | Time 47.6666(53.6239) | Bit/dim 5.6072(6.4195) | Xent 1.9693(2.0570) | Loss 16.3699(18.6963) | Error 0.6784(0.7195) Steps 0(0.00) | Grad Norm 8.4724(25.8644) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0018 | Time 23.6155, Epoch Time 340.3489(378.7726), Bit/dim 5.5978(best: 5.6931), Xent 1.9505, Loss 6.5731, Error 0.6711(best: 0.6471)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0109 | Time 50.7215(53.5368) | Bit/dim 5.5826(6.3944) | Xent 1.9690(2.0544) | Loss 20.9550(18.7641) | Error 0.6754(0.7182) Steps 0(0.00) | Grad Norm 11.4317(25.4314) | Total Time 0.00(0.00)\n",
      "Iter 0110 | Time 45.6320(53.2997) | Bit/dim 5.5775(6.3699) | Xent 1.9615(2.0516) | Loss 15.6303(18.6701) | Error 0.6714(0.7168) Steps 0(0.00) | Grad Norm 10.4465(24.9819) | Total Time 0.00(0.00)\n",
      "Iter 0111 | Time 49.2585(53.1784) | Bit/dim 5.5724(6.3459) | Xent 1.9512(2.0486) | Loss 16.2716(18.5981) | Error 0.6827(0.7158) Steps 0(0.00) | Grad Norm 17.1680(24.7475) | Total Time 0.00(0.00)\n",
      "Iter 0112 | Time 48.5511(53.0396) | Bit/dim 5.5699(6.3227) | Xent 1.9747(2.0464) | Loss 16.3729(18.5314) | Error 0.6927(0.7151) Steps 0(0.00) | Grad Norm 24.3058(24.7342) | Total Time 0.00(0.00)\n",
      "Iter 0113 | Time 50.2868(52.9570) | Bit/dim 5.5920(6.3007) | Xent 2.0776(2.0473) | Loss 16.2147(18.4619) | Error 0.7425(0.7159) Steps 0(0.00) | Grad Norm 60.7581(25.8149) | Total Time 0.00(0.00)\n",
      "Iter 0114 | Time 49.5574(52.8550) | Bit/dim 5.8358(6.2868) | Xent 2.5355(2.0619) | Loss 17.5523(18.4346) | Error 0.7948(0.7183) Steps 0(0.00) | Grad Norm 70.3328(27.1505) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0019 | Time 23.6846, Epoch Time 333.8529(377.4250), Bit/dim 5.5759(best: 5.5978), Xent 1.9904, Loss 6.5711, Error 0.6862(best: 0.6471)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0115 | Time 49.7046(52.7605) | Bit/dim 5.5699(6.2653) | Xent 2.0038(2.0602) | Loss 20.7636(18.5044) | Error 0.6990(0.7177) Steps 0(0.00) | Grad Norm 24.0637(27.0579) | Total Time 0.00(0.00)\n",
      "Iter 0116 | Time 49.8755(52.6740) | Bit/dim 5.5860(6.2449) | Xent 2.2363(2.0655) | Loss 16.3992(18.4413) | Error 0.7981(0.7201) Steps 0(0.00) | Grad Norm 47.8720(27.6823) | Total Time 0.00(0.00)\n",
      "Iter 0117 | Time 51.2494(52.6312) | Bit/dim 5.5516(6.2241) | Xent 2.0638(2.0654) | Loss 16.1829(18.3735) | Error 0.7206(0.7201) Steps 0(0.00) | Grad Norm 24.0119(27.5722) | Total Time 0.00(0.00)\n",
      "Iter 0118 | Time 50.1008(52.5553) | Bit/dim 5.4876(6.2020) | Xent 2.0147(2.0639) | Loss 15.9391(18.3005) | Error 0.7103(0.7198) Steps 0(0.00) | Grad Norm 6.8154(26.9495) | Total Time 0.00(0.00)\n",
      "Iter 0119 | Time 53.3719(52.5798) | Bit/dim 5.4904(6.1807) | Xent 2.0769(2.0643) | Loss 16.2428(18.2388) | Error 0.7394(0.7204) Steps 0(0.00) | Grad Norm 21.4441(26.7843) | Total Time 0.00(0.00)\n",
      "Iter 0120 | Time 49.2342(52.4795) | Bit/dim 5.4880(6.1599) | Xent 2.0705(2.0645) | Loss 16.2501(18.1791) | Error 0.7524(0.7214) Steps 0(0.00) | Grad Norm 6.8292(26.1857) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0020 | Time 24.3959, Epoch Time 343.6825(376.4127), Bit/dim 5.4606(best: 5.5759), Xent 2.0432, Loss 6.4822, Error 0.7384(best: 0.6471)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0121 | Time 51.5139(52.4505) | Bit/dim 5.4498(6.1386) | Xent 2.0574(2.0643) | Loss 20.4236(18.2464) | Error 0.7411(0.7220) Steps 0(0.00) | Grad Norm 17.8392(25.9353) | Total Time 0.00(0.00)\n",
      "Iter 0122 | Time 53.8347(52.4920) | Bit/dim 5.4278(6.1173) | Xent 2.0482(2.0638) | Loss 16.1253(18.1828) | Error 0.7126(0.7217) Steps 0(0.00) | Grad Norm 8.0316(25.3982) | Total Time 0.00(0.00)\n",
      "Iter 0123 | Time 48.8036(52.3814) | Bit/dim 5.4408(6.0970) | Xent 2.0376(2.0630) | Loss 16.0505(18.1188) | Error 0.7140(0.7215) Steps 0(0.00) | Grad Norm 14.6608(25.0760) | Total Time 0.00(0.00)\n",
      "Iter 0124 | Time 50.6393(52.3291) | Bit/dim 5.4157(6.0765) | Xent 2.0266(2.0619) | Loss 15.7694(18.0484) | Error 0.6979(0.7207) Steps 0(0.00) | Grad Norm 11.4087(24.6660) | Total Time 0.00(0.00)\n",
      "Iter 0125 | Time 50.3021(52.2683) | Bit/dim 5.3857(6.0558) | Xent 2.0399(2.0612) | Loss 15.2727(17.9651) | Error 0.7131(0.7205) Steps 0(0.00) | Grad Norm 10.6429(24.2453) | Total Time 0.00(0.00)\n",
      "Iter 0126 | Time 49.6024(52.1883) | Bit/dim 5.3909(6.0359) | Xent 2.0385(2.0606) | Loss 15.8101(17.9004) | Error 0.7124(0.7203) Steps 0(0.00) | Grad Norm 9.0255(23.7887) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0021 | Time 22.6977, Epoch Time 343.4974(375.4252), Bit/dim 5.3588(best: 5.4606), Xent 2.0071, Loss 6.3623, Error 0.6924(best: 0.6471)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0127 | Time 51.0513(52.1542) | Bit/dim 5.3509(6.0153) | Xent 2.0412(2.0600) | Loss 19.9649(17.9624) | Error 0.7176(0.7202) Steps 0(0.00) | Grad Norm 8.8309(23.3400) | Total Time 0.00(0.00)\n",
      "Iter 0128 | Time 50.9818(52.1190) | Bit/dim 5.3392(5.9950) | Xent 2.0186(2.0587) | Loss 15.6457(17.8929) | Error 0.6999(0.7196) Steps 0(0.00) | Grad Norm 5.3689(22.8009) | Total Time 0.00(0.00)\n",
      "Iter 0129 | Time 52.6027(52.1335) | Bit/dim 5.3384(5.9753) | Xent 2.0211(2.0576) | Loss 15.6379(17.8252) | Error 0.7012(0.7190) Steps 0(0.00) | Grad Norm 7.4446(22.3402) | Total Time 0.00(0.00)\n",
      "Iter 0130 | Time 49.5978(52.0575) | Bit/dim 5.3092(5.9553) | Xent 2.0136(2.0563) | Loss 15.3108(17.7498) | Error 0.7014(0.7185) Steps 0(0.00) | Grad Norm 6.8232(21.8747) | Total Time 0.00(0.00)\n",
      "Iter 0131 | Time 50.8276(52.0206) | Bit/dim 5.2880(5.9353) | Xent 2.0143(2.0550) | Loss 15.6325(17.6863) | Error 0.7037(0.7181) Steps 0(0.00) | Grad Norm 9.0672(21.4904) | Total Time 0.00(0.00)\n",
      "Iter 0132 | Time 51.5094(52.0052) | Bit/dim 5.3011(5.9163) | Xent 1.9870(2.0530) | Loss 15.5991(17.6237) | Error 0.6937(0.7173) Steps 0(0.00) | Grad Norm 10.3942(21.1576) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0022 | Time 22.8745, Epoch Time 345.9207(374.5401), Bit/dim 5.2651(best: 5.3588), Xent 1.9687, Loss 6.2495, Error 0.6782(best: 0.6471)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0133 | Time 51.6082(51.9933) | Bit/dim 5.2635(5.8967) | Xent 1.9907(2.0511) | Loss 19.4379(17.6781) | Error 0.6929(0.7166) Steps 0(0.00) | Grad Norm 4.4306(20.6557) | Total Time 0.00(0.00)\n",
      "Iter 0134 | Time 50.8841(51.9601) | Bit/dim 5.2503(5.8773) | Xent 1.9827(2.0491) | Loss 15.3251(17.6075) | Error 0.6955(0.7160) Steps 0(0.00) | Grad Norm 8.8811(20.3025) | Total Time 0.00(0.00)\n",
      "Iter 0135 | Time 52.9445(51.9896) | Bit/dim 5.2340(5.8580) | Xent 1.9995(2.0476) | Loss 15.3761(17.5406) | Error 0.7063(0.7157) Steps 0(0.00) | Grad Norm 11.0595(20.0252) | Total Time 0.00(0.00)\n",
      "Iter 0136 | Time 48.8079(51.8941) | Bit/dim 5.2243(5.8390) | Xent 1.9677(2.0452) | Loss 15.3463(17.4747) | Error 0.6753(0.7145) Steps 0(0.00) | Grad Norm 7.2778(19.6428) | Total Time 0.00(0.00)\n",
      "Iter 0137 | Time 51.4646(51.8813) | Bit/dim 5.1996(5.8198) | Xent 1.9532(2.0424) | Loss 15.1242(17.4042) | Error 0.6758(0.7133) Steps 0(0.00) | Grad Norm 5.2664(19.2115) | Total Time 0.00(0.00)\n",
      "Iter 0138 | Time 48.4209(51.7774) | Bit/dim 5.2058(5.8014) | Xent 1.9415(2.0394) | Loss 15.2636(17.3400) | Error 0.6716(0.7120) Steps 0(0.00) | Grad Norm 5.9998(18.8152) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0023 | Time 22.7450, Epoch Time 343.0118(373.5943), Bit/dim 5.1794(best: 5.2651), Xent 1.9202, Loss 6.1395, Error 0.6559(best: 0.6471)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0139 | Time 49.8867(51.7207) | Bit/dim 5.1944(5.7832) | Xent 1.9370(2.0363) | Loss 19.5085(17.4051) | Error 0.6754(0.7109) Steps 0(0.00) | Grad Norm 5.9267(18.4285) | Total Time 0.00(0.00)\n",
      "Iter 0140 | Time 51.7092(51.7204) | Bit/dim 5.1700(5.7648) | Xent 1.9540(2.0339) | Loss 15.2708(17.3410) | Error 0.6783(0.7100) Steps 0(0.00) | Grad Norm 6.0567(18.0573) | Total Time 0.00(0.00)\n",
      "Iter 0141 | Time 50.7837(51.6923) | Bit/dim 5.1523(5.7464) | Xent 1.9317(2.0308) | Loss 14.8986(17.2678) | Error 0.6663(0.7087) Steps 0(0.00) | Grad Norm 4.5968(17.6535) | Total Time 0.00(0.00)\n",
      "Iter 0142 | Time 49.5160(51.6270) | Bit/dim 5.1504(5.7285) | Xent 1.9316(2.0278) | Loss 14.6002(17.1877) | Error 0.6746(0.7076) Steps 0(0.00) | Grad Norm 3.9428(17.2422) | Total Time 0.00(0.00)\n",
      "Iter 0143 | Time 52.3862(51.6498) | Bit/dim 5.1327(5.7107) | Xent 1.8984(2.0239) | Loss 15.1710(17.1272) | Error 0.6569(0.7061) Steps 0(0.00) | Grad Norm 13.9780(17.1443) | Total Time 0.00(0.00)\n",
      "Iter 0144 | Time 52.5573(51.6770) | Bit/dim 5.1279(5.6932) | Xent 1.9770(2.0225) | Loss 15.0015(17.0634) | Error 0.6897(0.7056) Steps 0(0.00) | Grad Norm 32.4620(17.6038) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0024 | Time 23.2781, Epoch Time 345.7787(372.7598), Bit/dim 5.1959(best: 5.1794), Xent 2.1603, Loss 6.2760, Error 0.7667(best: 0.6471)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0145 | Time 52.0472(51.6881) | Bit/dim 5.1946(5.6782) | Xent 2.1879(2.0275) | Loss 19.2931(17.1303) | Error 0.7654(0.7074) Steps 0(0.00) | Grad Norm 71.8449(19.2310) | Total Time 0.00(0.00)\n",
      "Iter 0146 | Time 51.8914(51.6942) | Bit/dim 5.4074(5.6701) | Xent 2.6762(2.0470) | Loss 16.1900(17.1021) | Error 0.8125(0.7106) Steps 0(0.00) | Grad Norm 119.3247(22.2339) | Total Time 0.00(0.00)\n",
      "Iter 0147 | Time 50.6332(51.6624) | Bit/dim 5.5245(5.6657) | Xent 2.0178(2.0461) | Loss 15.8582(17.0648) | Error 0.7116(0.7106) Steps 0(0.00) | Grad Norm 29.9603(22.4656) | Total Time 0.00(0.00)\n",
      "Iter 0148 | Time 48.2765(51.5608) | Bit/dim 5.7179(5.6673) | Xent 2.0364(2.0458) | Loss 16.6380(17.0520) | Error 0.7234(0.7110) Steps 0(0.00) | Grad Norm 32.5889(22.7693) | Total Time 0.00(0.00)\n",
      "Iter 0149 | Time 47.6118(51.4423) | Bit/dim 5.5774(5.6646) | Xent 2.0080(2.0447) | Loss 16.1417(17.0247) | Error 0.6986(0.7106) Steps 0(0.00) | Grad Norm 24.1948(22.8121) | Total Time 0.00(0.00)\n",
      "Iter 0150 | Time 48.5904(51.3568) | Bit/dim 5.3181(5.6542) | Xent 2.0476(2.0447) | Loss 15.4273(16.9768) | Error 0.7170(0.7108) Steps 0(0.00) | Grad Norm 16.0889(22.6104) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0025 | Time 23.5278, Epoch Time 339.0469(371.7484), Bit/dim 5.3093(best: 5.1794), Xent 2.1162, Loss 6.3674, Error 0.7373(best: 0.6471)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0151 | Time 51.2023(51.3521) | Bit/dim 5.3311(5.6445) | Xent 2.1625(2.0483) | Loss 20.1282(17.0713) | Error 0.7364(0.7116) Steps 0(0.00) | Grad Norm 21.8855(22.5887) | Total Time 0.00(0.00)\n",
      "Iter 0152 | Time 54.5253(51.4473) | Bit/dim 5.2326(5.6322) | Xent 2.1631(2.0517) | Loss 15.2324(17.0161) | Error 0.7241(0.7119) Steps 0(0.00) | Grad Norm 14.8569(22.3567) | Total Time 0.00(0.00)\n",
      "Iter 0153 | Time 47.0094(51.3142) | Bit/dim 5.2459(5.6206) | Xent 2.0720(2.0523) | Loss 15.2795(16.9640) | Error 0.7201(0.7122) Steps 0(0.00) | Grad Norm 12.5963(22.0639) | Total Time 0.00(0.00)\n",
      "Iter 0154 | Time 52.1881(51.3404) | Bit/dim 5.2253(5.6087) | Xent 2.0436(2.0521) | Loss 15.1012(16.9082) | Error 0.7136(0.7122) Steps 0(0.00) | Grad Norm 9.4387(21.6851) | Total Time 0.00(0.00)\n",
      "Iter 0155 | Time 52.4869(51.3748) | Bit/dim 5.2110(5.5968) | Xent 2.1204(2.0541) | Loss 14.9191(16.8485) | Error 0.7465(0.7133) Steps 0(0.00) | Grad Norm 21.1196(21.6682) | Total Time 0.00(0.00)\n",
      "Iter 0156 | Time 52.2907(51.4023) | Bit/dim 5.1779(5.5842) | Xent 2.2520(2.0600) | Loss 15.4096(16.8053) | Error 0.7806(0.7153) Steps 0(0.00) | Grad Norm 31.7022(21.9692) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0026 | Time 21.9685, Epoch Time 347.9525(371.0345), Bit/dim 5.1850(best: 5.1794), Xent 2.2953, Loss 6.3326, Error 0.7641(best: 0.6471)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0157 | Time 51.4976(51.4051) | Bit/dim 5.1915(5.5724) | Xent 2.3358(2.0683) | Loss 19.4039(16.8833) | Error 0.7639(0.7167) Steps 0(0.00) | Grad Norm 38.9617(22.4790) | Total Time 0.00(0.00)\n",
      "Iter 0158 | Time 51.6945(51.4138) | Bit/dim 5.1486(5.5597) | Xent 2.0845(2.0688) | Loss 15.2448(16.8341) | Error 0.7465(0.7176) Steps 0(0.00) | Grad Norm 10.9989(22.1346) | Total Time 0.00(0.00)\n",
      "Iter 0159 | Time 52.2203(51.4380) | Bit/dim 5.1375(5.5470) | Xent 2.1617(2.0716) | Loss 15.3209(16.7887) | Error 0.7732(0.7193) Steps 0(0.00) | Grad Norm 16.3507(21.9611) | Total Time 0.00(0.00)\n",
      "Iter 0160 | Time 51.4130(51.4373) | Bit/dim 5.1151(5.5341) | Xent 2.1160(2.0729) | Loss 14.8772(16.7314) | Error 0.7515(0.7203) Steps 0(0.00) | Grad Norm 8.5521(21.5588) | Total Time 0.00(0.00)\n",
      "Iter 0161 | Time 54.8895(51.5408) | Bit/dim 5.1143(5.5215) | Xent 2.1905(2.0764) | Loss 15.5092(16.6947) | Error 0.7612(0.7215) Steps 0(0.00) | Grad Norm 14.8581(21.3578) | Total Time 0.00(0.00)\n",
      "Iter 0162 | Time 53.3064(51.5938) | Bit/dim 5.0896(5.5085) | Xent 2.0365(2.0753) | Loss 14.6523(16.6334) | Error 0.7230(0.7215) Steps 0(0.00) | Grad Norm 5.0065(20.8672) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0027 | Time 22.8595, Epoch Time 354.0386(370.5247), Bit/dim 5.0873(best: 5.1794), Xent 2.0140, Loss 6.0944, Error 0.7131(best: 0.6471)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0163 | Time 48.8404(51.5112) | Bit/dim 5.0788(5.4956) | Xent 2.0527(2.0746) | Loss 18.9133(16.7018) | Error 0.7330(0.7219) Steps 0(0.00) | Grad Norm 8.4213(20.4938) | Total Time 0.00(0.00)\n",
      "Iter 0164 | Time 50.5707(51.4830) | Bit/dim 5.0749(5.4830) | Xent 2.0266(2.0731) | Loss 14.9364(16.6489) | Error 0.7191(0.7218) Steps 0(0.00) | Grad Norm 6.4001(20.0710) | Total Time 0.00(0.00)\n",
      "Iter 0165 | Time 52.3384(51.5086) | Bit/dim 5.0445(5.4699) | Xent 1.9722(2.0701) | Loss 14.6199(16.5880) | Error 0.6966(0.7210) Steps 0(0.00) | Grad Norm 5.4542(19.6325) | Total Time 0.00(0.00)\n",
      "Iter 0166 | Time 50.1308(51.4673) | Bit/dim 5.0559(5.4574) | Xent 2.0010(2.0680) | Loss 14.6484(16.5298) | Error 0.7006(0.7204) Steps 0(0.00) | Grad Norm 11.4532(19.3872) | Total Time 0.00(0.00)\n",
      "Iter 0167 | Time 57.7731(51.6565) | Bit/dim 5.0214(5.4444) | Xent 1.9898(2.0657) | Loss 14.8991(16.4809) | Error 0.6987(0.7198) Steps 0(0.00) | Grad Norm 4.9178(18.9531) | Total Time 0.00(0.00)\n",
      "Iter 0168 | Time 52.8231(51.6915) | Bit/dim 5.0414(5.4323) | Xent 1.9832(2.0632) | Loss 14.7315(16.4284) | Error 0.6976(0.7191) Steps 0(0.00) | Grad Norm 8.9205(18.6521) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0028 | Time 21.9329, Epoch Time 350.9278(369.9367), Bit/dim 5.0034(best: 5.0873), Xent 1.9400, Loss 5.9734, Error 0.6720(best: 0.6471)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0169 | Time 52.7761(51.7240) | Bit/dim 5.0156(5.4198) | Xent 1.9782(2.0607) | Loss 18.5291(16.4914) | Error 0.6871(0.7182) Steps 0(0.00) | Grad Norm 4.2856(18.2211) | Total Time 0.00(0.00)\n",
      "Iter 0170 | Time 52.0705(51.7344) | Bit/dim 5.0173(5.4077) | Xent 1.9758(2.0581) | Loss 14.6794(16.4371) | Error 0.6914(0.7174) Steps 0(0.00) | Grad Norm 8.8127(17.9388) | Total Time 0.00(0.00)\n",
      "Iter 0171 | Time 54.2904(51.8111) | Bit/dim 4.9722(5.3946) | Xent 1.9586(2.0551) | Loss 14.5065(16.3792) | Error 0.6861(0.7164) Steps 0(0.00) | Grad Norm 3.5623(17.5076) | Total Time 0.00(0.00)\n",
      "Iter 0172 | Time 54.6386(51.8959) | Bit/dim 4.9541(5.3814) | Xent 1.9675(2.0525) | Loss 14.7703(16.3309) | Error 0.6924(0.7157) Steps 0(0.00) | Grad Norm 6.1074(17.1655) | Total Time 0.00(0.00)\n",
      "Iter 0173 | Time 56.8945(52.0459) | Bit/dim 4.9445(5.3683) | Xent 1.9780(2.0503) | Loss 14.3967(16.2729) | Error 0.6956(0.7151) Steps 0(0.00) | Grad Norm 3.3365(16.7507) | Total Time 0.00(0.00)\n",
      "Iter 0174 | Time 55.9641(52.1634) | Bit/dim 4.9478(5.3557) | Xent 1.9773(2.0481) | Loss 14.6047(16.2228) | Error 0.6914(0.7144) Steps 0(0.00) | Grad Norm 6.1736(16.4334) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0029 | Time 24.3657, Epoch Time 367.0917(369.8514), Bit/dim 4.9341(best: 5.0034), Xent 1.9180, Loss 5.8931, Error 0.6665(best: 0.6471)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0175 | Time 57.4236(52.3212) | Bit/dim 4.9319(5.3430) | Xent 1.9524(2.0452) | Loss 18.4424(16.2894) | Error 0.6847(0.7135) Steps 0(0.00) | Grad Norm 2.3837(16.0119) | Total Time 0.00(0.00)\n",
      "Iter 0176 | Time 54.7712(52.3947) | Bit/dim 4.9336(5.3307) | Xent 1.9400(2.0420) | Loss 14.6116(16.2391) | Error 0.6723(0.7123) Steps 0(0.00) | Grad Norm 4.5920(15.6693) | Total Time 0.00(0.00)\n",
      "Iter 0177 | Time 57.2526(52.5405) | Bit/dim 4.9209(5.3184) | Xent 1.9279(2.0386) | Loss 14.5461(16.1883) | Error 0.6747(0.7111) Steps 0(0.00) | Grad Norm 3.2325(15.2962) | Total Time 0.00(0.00)\n",
      "Iter 0178 | Time 56.9377(52.6724) | Bit/dim 4.9090(5.3061) | Xent 1.9295(2.0354) | Loss 14.3280(16.1325) | Error 0.6783(0.7101) Steps 0(0.00) | Grad Norm 4.0460(14.9587) | Total Time 0.00(0.00)\n",
      "Iter 0179 | Time 57.9102(52.8295) | Bit/dim 4.8995(5.2939) | Xent 1.9225(2.0320) | Loss 14.2790(16.0769) | Error 0.6724(0.7090) Steps 0(0.00) | Grad Norm 3.8545(14.6255) | Total Time 0.00(0.00)\n",
      "Iter 0180 | Time 57.3556(52.9653) | Bit/dim 4.8961(5.2820) | Xent 1.9153(2.0285) | Loss 14.3018(16.0236) | Error 0.6723(0.7079) Steps 0(0.00) | Grad Norm 2.8023(14.2708) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0030 | Time 23.8137, Epoch Time 381.1774(370.1912), Bit/dim 4.8858(best: 4.9341), Xent 1.8773, Loss 5.8245, Error 0.6505(best: 0.6471)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0181 | Time 57.1613(53.0912) | Bit/dim 4.8837(5.2700) | Xent 1.9025(2.0247) | Loss 18.6124(16.1013) | Error 0.6750(0.7069) Steps 0(0.00) | Grad Norm 4.8205(13.9873) | Total Time 0.00(0.00)\n",
      "Iter 0182 | Time 53.1588(53.0932) | Bit/dim 4.8810(5.2584) | Xent 1.9134(2.0213) | Loss 14.6595(16.0580) | Error 0.6683(0.7058) Steps 0(0.00) | Grad Norm 5.4926(13.7325) | Total Time 0.00(0.00)\n",
      "Iter 0183 | Time 56.3926(53.1922) | Bit/dim 4.8741(5.2468) | Xent 1.9041(2.0178) | Loss 14.2868(16.0049) | Error 0.6695(0.7047) Steps 0(0.00) | Grad Norm 6.6450(13.5199) | Total Time 0.00(0.00)\n",
      "Iter 0184 | Time 58.4561(53.3501) | Bit/dim 4.8528(5.2350) | Xent 1.9108(2.0146) | Loss 14.5379(15.9609) | Error 0.6736(0.7037) Steps 0(0.00) | Grad Norm 7.0033(13.3244) | Total Time 0.00(0.00)\n",
      "Iter 0185 | Time 59.6471(53.5390) | Bit/dim 4.8541(5.2236) | Xent 1.8965(2.0111) | Loss 14.5188(15.9176) | Error 0.6671(0.7026) Steps 0(0.00) | Grad Norm 9.9018(13.2217) | Total Time 0.00(0.00)\n",
      "Iter 0186 | Time 57.1276(53.6467) | Bit/dim 4.8487(5.2124) | Xent 1.9124(2.0081) | Loss 14.4946(15.8749) | Error 0.6687(0.7016) Steps 0(0.00) | Grad Norm 13.4599(13.2288) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0031 | Time 23.7331, Epoch Time 381.9899(370.5451), Bit/dim 4.8432(best: 4.8858), Xent 1.9073, Loss 5.7969, Error 0.6776(best: 0.6471)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0187 | Time 58.3195(53.7869) | Bit/dim 4.8567(5.2017) | Xent 1.9318(2.0058) | Loss 18.2913(15.9474) | Error 0.6870(0.7012) Steps 0(0.00) | Grad Norm 17.7575(13.3647) | Total Time 0.00(0.00)\n",
      "Iter 0188 | Time 58.3805(53.9247) | Bit/dim 4.8383(5.1908) | Xent 1.9730(2.0048) | Loss 14.4850(15.9035) | Error 0.6972(0.7011) Steps 0(0.00) | Grad Norm 20.8247(13.5885) | Total Time 0.00(0.00)\n",
      "Iter 0189 | Time 53.9867(53.9265) | Bit/dim 4.8421(5.1803) | Xent 1.9733(2.0039) | Loss 14.6228(15.8651) | Error 0.7015(0.7011) Steps 0(0.00) | Grad Norm 18.6297(13.7397) | Total Time 0.00(0.00)\n",
      "Iter 0190 | Time 57.4906(54.0334) | Bit/dim 4.8149(5.1694) | Xent 1.8909(2.0005) | Loss 14.0327(15.8101) | Error 0.6620(0.6999) Steps 0(0.00) | Grad Norm 3.8329(13.4425) | Total Time 0.00(0.00)\n",
      "Iter 0191 | Time 56.0433(54.0937) | Bit/dim 4.8143(5.1587) | Xent 1.9634(1.9994) | Loss 14.2297(15.7627) | Error 0.6954(0.6998) Steps 0(0.00) | Grad Norm 16.5574(13.5360) | Total Time 0.00(0.00)\n",
      "Iter 0192 | Time 53.7403(54.0831) | Bit/dim 4.8437(5.1493) | Xent 1.9397(1.9976) | Loss 14.5376(15.7260) | Error 0.6917(0.6995) Steps 0(0.00) | Grad Norm 13.2345(13.5269) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0032 | Time 24.8180, Epoch Time 379.1660(370.8038), Bit/dim 4.8059(best: 4.8432), Xent 1.8880, Loss 5.7499, Error 0.6700(best: 0.6471)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0193 | Time 55.5794(54.1280) | Bit/dim 4.8054(5.1389) | Xent 1.9199(1.9953) | Loss 18.5210(15.8098) | Error 0.6831(0.6990) Steps 0(0.00) | Grad Norm 13.2621(13.5190) | Total Time 0.00(0.00)\n",
      "Iter 0194 | Time 58.6968(54.2651) | Bit/dim 4.8155(5.1292) | Xent 1.9536(1.9940) | Loss 14.5732(15.7727) | Error 0.6947(0.6989) Steps 0(0.00) | Grad Norm 19.9437(13.7117) | Total Time 0.00(0.00)\n",
      "Iter 0195 | Time 52.6804(54.2176) | Bit/dim 4.7855(5.1189) | Xent 1.8909(1.9909) | Loss 14.3365(15.7296) | Error 0.6583(0.6977) Steps 0(0.00) | Grad Norm 8.2409(13.5476) | Total Time 0.00(0.00)\n",
      "Iter 0196 | Time 56.6778(54.2914) | Bit/dim 4.8161(5.1098) | Xent 1.9620(1.9901) | Loss 14.4273(15.6906) | Error 0.6996(0.6978) Steps 0(0.00) | Grad Norm 24.0232(13.8619) | Total Time 0.00(0.00)\n",
      "Iter 0197 | Time 54.5068(54.2978) | Bit/dim 4.8635(5.1025) | Xent 1.8882(1.9870) | Loss 14.2564(15.6475) | Error 0.6630(0.6967) Steps 0(0.00) | Grad Norm 16.5455(13.9424) | Total Time 0.00(0.00)\n",
      "Iter 0198 | Time 55.1204(54.3225) | Bit/dim 4.7777(5.0927) | Xent 1.8868(1.9840) | Loss 14.2304(15.6050) | Error 0.6649(0.6958) Steps 0(0.00) | Grad Norm 5.9586(13.7029) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0033 | Time 25.3571, Epoch Time 375.0906(370.9324), Bit/dim 4.7859(best: 4.8059), Xent 1.8454, Loss 5.7086, Error 0.6459(best: 0.6471)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0199 | Time 54.6323(54.3318) | Bit/dim 4.7787(5.0833) | Xent 1.8839(1.9810) | Loss 18.5753(15.6941) | Error 0.6615(0.6947) Steps 0(0.00) | Grad Norm 9.1751(13.5670) | Total Time 0.00(0.00)\n",
      "Iter 0200 | Time 55.4100(54.3641) | Bit/dim 4.8425(5.0761) | Xent 1.8665(1.9776) | Loss 14.2366(15.6504) | Error 0.6532(0.6935) Steps 0(0.00) | Grad Norm 13.3882(13.5617) | Total Time 0.00(0.00)\n",
      "Iter 0201 | Time 57.3288(54.4531) | Bit/dim 4.7617(5.0666) | Xent 1.8532(1.9738) | Loss 14.2081(15.6071) | Error 0.6605(0.6925) Steps 0(0.00) | Grad Norm 5.7850(13.3284) | Total Time 0.00(0.00)\n",
      "Iter 0202 | Time 57.8605(54.5553) | Bit/dim 4.8015(5.0587) | Xent 1.9070(1.9718) | Loss 14.5303(15.5748) | Error 0.6747(0.6920) Steps 0(0.00) | Grad Norm 19.0467(13.4999) | Total Time 0.00(0.00)\n",
      "Iter 0203 | Time 57.8798(54.6550) | Bit/dim 4.8302(5.0518) | Xent 1.8851(1.9692) | Loss 14.2165(15.5341) | Error 0.6640(0.6911) Steps 0(0.00) | Grad Norm 16.7483(13.5974) | Total Time 0.00(0.00)\n",
      "Iter 0204 | Time 55.4789(54.6798) | Bit/dim 4.7547(5.0429) | Xent 1.8995(1.9671) | Loss 14.1663(15.4931) | Error 0.6745(0.6906) Steps 0(0.00) | Grad Norm 10.7644(13.5124) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0034 | Time 24.6750, Epoch Time 379.8972(371.2013), Bit/dim 4.8444(best: 4.7859), Xent 1.8532, Loss 5.7710, Error 0.6520(best: 0.6459)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0205 | Time 59.0694(54.8114) | Bit/dim 4.8436(5.0369) | Xent 1.8959(1.9650) | Loss 18.8764(15.5946) | Error 0.6764(0.6902) Steps 0(0.00) | Grad Norm 24.3108(13.8363) | Total Time 0.00(0.00)\n",
      "Iter 0206 | Time 51.7918(54.7209) | Bit/dim 4.8239(5.0305) | Xent 1.8709(1.9622) | Loss 14.3728(15.5579) | Error 0.6561(0.6892) Steps 0(0.00) | Grad Norm 12.5777(13.7986) | Total Time 0.00(0.00)\n",
      "Iter 0207 | Time 57.7409(54.8115) | Bit/dim 4.8444(5.0250) | Xent 1.8813(1.9597) | Loss 14.3497(15.5217) | Error 0.6660(0.6885) Steps 0(0.00) | Grad Norm 11.3485(13.7251) | Total Time 0.00(0.00)\n",
      "Iter 0208 | Time 56.1812(54.8525) | Bit/dim 4.7661(5.0172) | Xent 1.9346(1.9590) | Loss 14.2076(15.4822) | Error 0.6959(0.6887) Steps 0(0.00) | Grad Norm 14.1270(13.7371) | Total Time 0.00(0.00)\n",
      "Iter 0209 | Time 57.0915(54.9197) | Bit/dim 4.7595(5.0095) | Xent 1.8706(1.9563) | Loss 13.9832(15.4373) | Error 0.6561(0.6877) Steps 0(0.00) | Grad Norm 10.0624(13.6269) | Total Time 0.00(0.00)\n",
      "Iter 0210 | Time 59.8527(55.0677) | Bit/dim 4.7344(5.0012) | Xent 1.8993(1.9546) | Loss 14.1261(15.3979) | Error 0.6740(0.6873) Steps 0(0.00) | Grad Norm 9.8207(13.5127) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0035 | Time 24.6428, Epoch Time 382.2620(371.5331), Bit/dim 4.7537(best: 4.7859), Xent 1.8838, Loss 5.6957, Error 0.6642(best: 0.6459)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0211 | Time 57.1384(55.1298) | Bit/dim 4.7583(4.9939) | Xent 1.9253(1.9537) | Loss 18.9042(15.5031) | Error 0.6879(0.6873) Steps 0(0.00) | Grad Norm 12.2782(13.4757) | Total Time 0.00(0.00)\n",
      "Iter 0212 | Time 59.7696(55.2690) | Bit/dim 4.7224(4.9858) | Xent 1.8824(1.9516) | Loss 14.0490(15.4595) | Error 0.6680(0.6867) Steps 0(0.00) | Grad Norm 4.8424(13.2167) | Total Time 0.00(0.00)\n",
      "Iter 0213 | Time 59.9689(55.4100) | Bit/dim 4.7222(4.9779) | Xent 1.8662(1.9490) | Loss 13.9904(15.4154) | Error 0.6564(0.6858) Steps 0(0.00) | Grad Norm 14.4442(13.2535) | Total Time 0.00(0.00)\n",
      "Iter 0214 | Time 59.5106(55.5330) | Bit/dim 4.7031(4.9696) | Xent 1.8203(1.9452) | Loss 13.9972(15.3729) | Error 0.6311(0.6842) Steps 0(0.00) | Grad Norm 6.0271(13.0367) | Total Time 0.00(0.00)\n",
      "Iter 0215 | Time 61.1007(55.7001) | Bit/dim 4.7099(4.9618) | Xent 1.8599(1.9426) | Loss 14.0541(15.3333) | Error 0.6452(0.6830) Steps 0(0.00) | Grad Norm 8.8704(12.9117) | Total Time 0.00(0.00)\n",
      "Iter 0216 | Time 57.9007(55.7661) | Bit/dim 4.6867(4.9536) | Xent 1.8227(1.9390) | Loss 13.9451(15.2917) | Error 0.6485(0.6820) Steps 0(0.00) | Grad Norm 9.1476(12.7988) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0036 | Time 24.1515, Epoch Time 395.8912(372.2639), Bit/dim 4.6738(best: 4.7537), Xent 1.8095, Loss 5.5786, Error 0.6311(best: 0.6459)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0217 | Time 61.2492(55.9306) | Bit/dim 4.6762(4.9453) | Xent 1.8284(1.9357) | Loss 17.9410(15.3711) | Error 0.6472(0.6809) Steps 0(0.00) | Grad Norm 6.3293(12.6047) | Total Time 0.00(0.00)\n",
      "Iter 0218 | Time 57.3836(55.9742) | Bit/dim 4.6752(4.9372) | Xent 1.8361(1.9327) | Loss 13.7346(15.3220) | Error 0.6398(0.6797) Steps 0(0.00) | Grad Norm 6.1654(12.4115) | Total Time 0.00(0.00)\n",
      "Iter 0219 | Time 54.6956(55.9358) | Bit/dim 4.6536(4.9287) | Xent 1.8252(1.9295) | Loss 14.0002(15.2824) | Error 0.6467(0.6787) Steps 0(0.00) | Grad Norm 5.6127(12.2076) | Total Time 0.00(0.00)\n",
      "Iter 0220 | Time 58.3268(56.0075) | Bit/dim 4.6515(4.9203) | Xent 1.8297(1.9265) | Loss 14.0118(15.2443) | Error 0.6520(0.6779) Steps 0(0.00) | Grad Norm 10.1224(12.1450) | Total Time 0.00(0.00)\n",
      "Iter 0221 | Time 55.6897(55.9980) | Bit/dim 4.6482(4.9122) | Xent 1.8204(1.9233) | Loss 13.9990(15.2069) | Error 0.6409(0.6768) Steps 0(0.00) | Grad Norm 5.8690(11.9567) | Total Time 0.00(0.00)\n",
      "Iter 0222 | Time 60.4835(56.1326) | Bit/dim 4.6467(4.9042) | Xent 1.8087(1.9199) | Loss 13.6544(15.1603) | Error 0.6338(0.6755) Steps 0(0.00) | Grad Norm 7.3491(11.8185) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0037 | Time 24.4084, Epoch Time 388.0660(372.7379), Bit/dim 4.6421(best: 4.6738), Xent 1.7831, Loss 5.5336, Error 0.6242(best: 0.6311)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0223 | Time 58.8025(56.2127) | Bit/dim 4.6505(4.8966) | Xent 1.8004(1.9163) | Loss 17.9351(15.2436) | Error 0.6381(0.6744) Steps 0(0.00) | Grad Norm 10.3051(11.7731) | Total Time 0.00(0.00)\n",
      "Iter 0224 | Time 57.2604(56.2441) | Bit/dim 4.6381(4.8888) | Xent 1.8032(1.9129) | Loss 13.6806(15.1967) | Error 0.6385(0.6733) Steps 0(0.00) | Grad Norm 8.8735(11.6861) | Total Time 0.00(0.00)\n",
      "Iter 0225 | Time 59.4134(56.3392) | Bit/dim 4.6283(4.8810) | Xent 1.8208(1.9101) | Loss 13.5727(15.1480) | Error 0.6418(0.6724) Steps 0(0.00) | Grad Norm 12.4760(11.7098) | Total Time 0.00(0.00)\n",
      "Iter 0226 | Time 59.3153(56.4285) | Bit/dim 4.6593(4.8744) | Xent 1.8506(1.9083) | Loss 14.0107(15.1139) | Error 0.6562(0.6719) Steps 0(0.00) | Grad Norm 21.4463(12.0019) | Total Time 0.00(0.00)\n",
      "Iter 0227 | Time 56.1981(56.4216) | Bit/dim 4.7420(4.8704) | Xent 1.8388(1.9063) | Loss 14.1348(15.0845) | Error 0.6571(0.6714) Steps 0(0.00) | Grad Norm 26.0936(12.4247) | Total Time 0.00(0.00)\n",
      "Iter 0228 | Time 60.4545(56.5425) | Bit/dim 4.6202(4.8629) | Xent 1.8200(1.9037) | Loss 13.8224(15.0466) | Error 0.6384(0.6705) Steps 0(0.00) | Grad Norm 10.9996(12.3819) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0038 | Time 25.1938, Epoch Time 392.8774(373.3421), Bit/dim 4.6865(best: 4.6421), Xent 1.8125, Loss 5.5927, Error 0.6426(best: 0.6242)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0229 | Time 61.9875(56.7059) | Bit/dim 4.6916(4.8578) | Xent 1.8599(1.9024) | Loss 18.4168(15.1477) | Error 0.6506(0.6699) Steps 0(0.00) | Grad Norm 26.1099(12.7937) | Total Time 0.00(0.00)\n",
      "Iter 0230 | Time 57.1837(56.7202) | Bit/dim 4.6759(4.8523) | Xent 1.9113(1.9026) | Loss 13.9781(15.1126) | Error 0.6905(0.6705) Steps 0(0.00) | Grad Norm 23.2816(13.1084) | Total Time 0.00(0.00)\n",
      "Iter 0231 | Time 55.1068(56.6718) | Bit/dim 4.7053(4.8479) | Xent 1.8991(1.9025) | Loss 14.1492(15.0837) | Error 0.6840(0.6709) Steps 0(0.00) | Grad Norm 27.2840(13.5336) | Total Time 0.00(0.00)\n",
      "Iter 0232 | Time 60.1939(56.7775) | Bit/dim 4.6247(4.8412) | Xent 1.8057(1.8996) | Loss 13.8309(15.0462) | Error 0.6392(0.6699) Steps 0(0.00) | Grad Norm 9.8275(13.4225) | Total Time 0.00(0.00)\n",
      "Iter 0233 | Time 59.9332(56.8722) | Bit/dim 4.6407(4.8352) | Xent 1.8795(1.8990) | Loss 14.0080(15.0150) | Error 0.6656(0.6698) Steps 0(0.00) | Grad Norm 21.7601(13.6726) | Total Time 0.00(0.00)\n",
      "Iter 0234 | Time 55.1470(56.8204) | Bit/dim 4.6605(4.8299) | Xent 1.8161(1.8965) | Loss 13.8541(14.9802) | Error 0.6426(0.6690) Steps 0(0.00) | Grad Norm 12.6982(13.6434) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0039 | Time 23.8947, Epoch Time 389.8273(373.8367), Bit/dim 4.6348(best: 4.6421), Xent 1.8208, Loss 5.5453, Error 0.6516(best: 0.6242)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0235 | Time 60.3725(56.9270) | Bit/dim 4.6309(4.8240) | Xent 1.8391(1.8948) | Loss 18.0362(15.0719) | Error 0.6593(0.6687) Steps 0(0.00) | Grad Norm 10.9506(13.5626) | Total Time 0.00(0.00)\n",
      "Iter 0236 | Time 53.8353(56.8342) | Bit/dim 4.6233(4.8179) | Xent 1.8684(1.8940) | Loss 13.9093(15.0370) | Error 0.6620(0.6685) Steps 0(0.00) | Grad Norm 12.9536(13.5443) | Total Time 0.00(0.00)\n",
      "Iter 0237 | Time 55.0071(56.7794) | Bit/dim 4.5859(4.8110) | Xent 1.8394(1.8924) | Loss 13.8377(15.0010) | Error 0.6546(0.6681) Steps 0(0.00) | Grad Norm 6.9018(13.3450) | Total Time 0.00(0.00)\n",
      "Iter 0238 | Time 54.1147(56.6995) | Bit/dim 4.5873(4.8043) | Xent 1.8501(1.8911) | Loss 13.9077(14.9682) | Error 0.6569(0.6677) Steps 0(0.00) | Grad Norm 13.3138(13.3441) | Total Time 0.00(0.00)\n",
      "Iter 0239 | Time 58.4592(56.7522) | Bit/dim 4.6079(4.7984) | Xent 1.8014(1.8884) | Loss 13.3425(14.9194) | Error 0.6394(0.6669) Steps 0(0.00) | Grad Norm 7.7603(13.1766) | Total Time 0.00(0.00)\n",
      "Iter 0240 | Time 66.2146(57.0361) | Bit/dim 4.6022(4.7925) | Xent 1.8203(1.8864) | Loss 13.4202(14.8745) | Error 0.6367(0.6660) Steps 0(0.00) | Grad Norm 8.5555(13.0379) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0040 | Time 24.0738, Epoch Time 388.3134(374.2710), Bit/dim 4.5621(best: 4.6348), Xent 1.7594, Loss 5.4418, Error 0.6177(best: 0.6242)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0241 | Time 60.8243(57.1498) | Bit/dim 4.5666(4.7857) | Xent 1.8007(1.8838) | Loss 17.4932(14.9530) | Error 0.6309(0.6649) Steps 0(0.00) | Grad Norm 4.9370(12.7949) | Total Time 0.00(0.00)\n",
      "Iter 0242 | Time 57.0072(57.1455) | Bit/dim 4.5758(4.7794) | Xent 1.7846(1.8808) | Loss 13.5047(14.9096) | Error 0.6259(0.6638) Steps 0(0.00) | Grad Norm 5.9785(12.5904) | Total Time 0.00(0.00)\n",
      "Iter 0243 | Time 59.7802(57.2245) | Bit/dim 4.5536(4.7727) | Xent 1.7788(1.8778) | Loss 13.6172(14.8708) | Error 0.6256(0.6626) Steps 0(0.00) | Grad Norm 4.9928(12.3625) | Total Time 0.00(0.00)\n",
      "Iter 0244 | Time 58.3775(57.2591) | Bit/dim 4.5410(4.7657) | Xent 1.7906(1.8752) | Loss 13.3293(14.8246) | Error 0.6274(0.6616) Steps 0(0.00) | Grad Norm 6.8548(12.1973) | Total Time 0.00(0.00)\n",
      "Iter 0245 | Time 55.8967(57.2182) | Bit/dim 4.5328(4.7587) | Xent 1.7619(1.8718) | Loss 13.5883(14.7875) | Error 0.6216(0.6604) Steps 0(0.00) | Grad Norm 5.4727(11.9955) | Total Time 0.00(0.00)\n",
      "Iter 0246 | Time 61.1034(57.3348) | Bit/dim 4.5229(4.7516) | Xent 1.7961(1.8695) | Loss 13.4723(14.7480) | Error 0.6407(0.6598) Steps 0(0.00) | Grad Norm 8.9269(11.9035) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0041 | Time 24.5640, Epoch Time 393.3469(374.8433), Bit/dim 4.5218(best: 4.5621), Xent 1.7381, Loss 5.3909, Error 0.6107(best: 0.6177)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0247 | Time 63.7466(57.5272) | Bit/dim 4.5303(4.7450) | Xent 1.7688(1.8665) | Loss 17.8997(14.8426) | Error 0.6282(0.6588) Steps 0(0.00) | Grad Norm 9.4849(11.8309) | Total Time 0.00(0.00)\n",
      "Iter 0248 | Time 62.2540(57.6690) | Bit/dim 4.5236(4.7384) | Xent 1.7752(1.8637) | Loss 13.5707(14.8044) | Error 0.6282(0.6579) Steps 0(0.00) | Grad Norm 9.4593(11.7598) | Total Time 0.00(0.00)\n",
      "Iter 0249 | Time 67.0705(57.9510) | Bit/dim 4.5216(4.7319) | Xent 1.7473(1.8602) | Loss 13.4016(14.7623) | Error 0.6200(0.6568) Steps 0(0.00) | Grad Norm 8.8090(11.6712) | Total Time 0.00(0.00)\n",
      "Iter 0250 | Time 69.4882(58.2971) | Bit/dim 4.5734(4.7271) | Xent 1.7877(1.8581) | Loss 13.4731(14.7236) | Error 0.6322(0.6560) Steps 0(0.00) | Grad Norm 19.7112(11.9124) | Total Time 0.00(0.00)\n",
      "Iter 0251 | Time 67.7293(58.5801) | Bit/dim 4.7017(4.7263) | Xent 1.8517(1.8579) | Loss 13.9281(14.6998) | Error 0.6551(0.6560) Steps 0(0.00) | Grad Norm 26.4022(12.3471) | Total Time 0.00(0.00)\n",
      "Iter 0252 | Time 64.1527(58.7473) | Bit/dim 4.5253(4.7203) | Xent 1.8305(1.8570) | Loss 13.3635(14.6597) | Error 0.6534(0.6559) Steps 0(0.00) | Grad Norm 21.3524(12.6173) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0042 | Time 25.1689, Epoch Time 436.0765(376.6803), Bit/dim 4.6423(best: 4.5218), Xent 1.7405, Loss 5.5126, Error 0.6207(best: 0.6107)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0253 | Time 59.2951(58.7637) | Bit/dim 4.6498(4.7182) | Xent 1.7930(1.8551) | Loss 17.7566(14.7526) | Error 0.6365(0.6554) Steps 0(0.00) | Grad Norm 26.9282(13.0466) | Total Time 0.00(0.00)\n",
      "Iter 0254 | Time 65.9349(58.9788) | Bit/dim 4.6242(4.7154) | Xent 1.8291(1.8543) | Loss 13.6322(14.7190) | Error 0.6555(0.6554) Steps 0(0.00) | Grad Norm 26.5011(13.4503) | Total Time 0.00(0.00)\n",
      "Iter 0255 | Time 64.7563(59.1522) | Bit/dim 4.6241(4.7126) | Xent 1.7819(1.8522) | Loss 13.5280(14.6833) | Error 0.6294(0.6546) Steps 0(0.00) | Grad Norm 18.5981(13.6047) | Total Time 0.00(0.00)\n",
      "Iter 0256 | Time 59.5902(59.1653) | Bit/dim 4.5446(4.7076) | Xent 1.7922(1.8504) | Loss 13.4024(14.6448) | Error 0.6387(0.6541) Steps 0(0.00) | Grad Norm 10.2512(13.5041) | Total Time 0.00(0.00)\n",
      "Iter 0257 | Time 66.4931(59.3851) | Bit/dim 4.5822(4.7038) | Xent 1.8278(1.8497) | Loss 13.4193(14.6081) | Error 0.6482(0.6539) Steps 0(0.00) | Grad Norm 16.0564(13.5807) | Total Time 0.00(0.00)\n",
      "Iter 0258 | Time 66.0139(59.5840) | Bit/dim 4.5438(4.6990) | Xent 1.8219(1.8489) | Loss 13.8041(14.5839) | Error 0.6436(0.6536) Steps 0(0.00) | Grad Norm 19.1430(13.7475) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0043 | Time 24.2971, Epoch Time 422.5031(378.0549), Bit/dim 4.5316(best: 4.5218), Xent 1.8087, Loss 5.4359, Error 0.6580(best: 0.6107)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0259 | Time 65.2620(59.7543) | Bit/dim 4.5277(4.6939) | Xent 1.8427(1.8487) | Loss 17.7569(14.6791) | Error 0.6583(0.6538) Steps 0(0.00) | Grad Norm 15.2580(13.7928) | Total Time 0.00(0.00)\n",
      "Iter 0260 | Time 67.7226(59.9934) | Bit/dim 4.5317(4.6890) | Xent 1.8379(1.8484) | Loss 13.5617(14.6456) | Error 0.6646(0.6541) Steps 0(0.00) | Grad Norm 15.7209(13.8507) | Total Time 0.00(0.00)\n",
      "Iter 0261 | Time 60.6849(60.0141) | Bit/dim 4.4959(4.6832) | Xent 1.8178(1.8474) | Loss 13.4496(14.6097) | Error 0.6461(0.6538) Steps 0(0.00) | Grad Norm 13.1052(13.8283) | Total Time 0.00(0.00)\n",
      "Iter 0262 | Time 58.7774(59.9770) | Bit/dim 4.5132(4.6781) | Xent 1.7861(1.8456) | Loss 13.6454(14.5808) | Error 0.6384(0.6534) Steps 0(0.00) | Grad Norm 13.5328(13.8194) | Total Time 0.00(0.00)\n",
      "Iter 0263 | Time 66.9116(60.1851) | Bit/dim 4.4852(4.6723) | Xent 1.7430(1.8425) | Loss 13.5000(14.5484) | Error 0.6051(0.6519) Steps 0(0.00) | Grad Norm 5.8271(13.5797) | Total Time 0.00(0.00)\n",
      "Iter 0264 | Time 63.2200(60.2761) | Bit/dim 4.4836(4.6667) | Xent 1.7721(1.8404) | Loss 13.2960(14.5108) | Error 0.6302(0.6513) Steps 0(0.00) | Grad Norm 11.2610(13.5101) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0044 | Time 23.9743, Epoch Time 422.9206(379.4009), Bit/dim 4.4888(best: 4.5218), Xent 1.7139, Loss 5.3457, Error 0.6032(best: 0.6107)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0265 | Time 65.5805(60.4352) | Bit/dim 4.4885(4.6613) | Xent 1.7419(1.8374) | Loss 17.4638(14.5994) | Error 0.6144(0.6502) Steps 0(0.00) | Grad Norm 10.1232(13.4085) | Total Time 0.00(0.00)\n",
      "Iter 0266 | Time 64.2839(60.5507) | Bit/dim 4.4853(4.6561) | Xent 1.7734(1.8355) | Loss 13.6934(14.5722) | Error 0.6302(0.6496) Steps 0(0.00) | Grad Norm 14.8136(13.4507) | Total Time 0.00(0.00)\n",
      "Iter 0267 | Time 61.8708(60.5903) | Bit/dim 4.4567(4.6501) | Xent 1.7460(1.8328) | Loss 13.3664(14.5360) | Error 0.6155(0.6486) Steps 0(0.00) | Grad Norm 12.7595(13.4299) | Total Time 0.00(0.00)\n",
      "Iter 0268 | Time 64.0005(60.6926) | Bit/dim 4.4591(4.6443) | Xent 1.7649(1.8308) | Loss 13.3831(14.5015) | Error 0.6280(0.6479) Steps 0(0.00) | Grad Norm 14.9586(13.4758) | Total Time 0.00(0.00)\n",
      "Iter 0269 | Time 60.1399(60.6760) | Bit/dim 4.4850(4.6396) | Xent 1.7796(1.8293) | Loss 13.5042(14.4715) | Error 0.6336(0.6475) Steps 0(0.00) | Grad Norm 19.8663(13.6675) | Total Time 0.00(0.00)\n",
      "Iter 0270 | Time 61.8275(60.7106) | Bit/dim 4.4464(4.6338) | Xent 1.8060(1.8286) | Loss 13.3838(14.4389) | Error 0.6440(0.6474) Steps 0(0.00) | Grad Norm 24.5387(13.9936) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0045 | Time 23.5417, Epoch Time 417.2970(380.5378), Bit/dim 4.4422(best: 4.4888), Xent 1.7147, Loss 5.2996, Error 0.6096(best: 0.6032)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0271 | Time 69.7855(60.9828) | Bit/dim 4.4393(4.6279) | Xent 1.7491(1.8262) | Loss 17.5412(14.5320) | Error 0.6266(0.6468) Steps 0(0.00) | Grad Norm 16.5051(14.0690) | Total Time 0.00(0.00)\n",
      "Iter 0272 | Time 65.4612(61.1172) | Bit/dim 4.4170(4.6216) | Xent 1.7404(1.8236) | Loss 13.3497(14.4965) | Error 0.6222(0.6460) Steps 0(0.00) | Grad Norm 5.3382(13.8071) | Total Time 0.00(0.00)\n",
      "Iter 0273 | Time 63.9720(61.2028) | Bit/dim 4.4272(4.6158) | Xent 1.7248(1.8206) | Loss 13.0066(14.4518) | Error 0.6128(0.6450) Steps 0(0.00) | Grad Norm 10.7524(13.7154) | Total Time 0.00(0.00)\n",
      "Iter 0274 | Time 66.7238(61.3684) | Bit/dim 4.4117(4.6096) | Xent 1.7485(1.8185) | Loss 13.1824(14.4137) | Error 0.6180(0.6442) Steps 0(0.00) | Grad Norm 9.0064(13.5741) | Total Time 0.00(0.00)\n",
      "Iter 0275 | Time 66.5202(61.5230) | Bit/dim 4.4140(4.6038) | Xent 1.7218(1.8156) | Loss 13.3266(14.3811) | Error 0.6104(0.6432) Steps 0(0.00) | Grad Norm 7.1320(13.3809) | Total Time 0.00(0.00)\n",
      "Iter 0276 | Time 65.8904(61.6540) | Bit/dim 4.3969(4.5976) | Xent 1.7242(1.8128) | Loss 13.2569(14.3474) | Error 0.6141(0.6423) Steps 0(0.00) | Grad Norm 12.7897(13.3632) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0046 | Time 23.8084, Epoch Time 438.0462(382.2630), Bit/dim 4.3972(best: 4.4422), Xent 1.7096, Loss 5.2520, Error 0.6130(best: 0.6032)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0277 | Time 59.4499(61.5879) | Bit/dim 4.3989(4.5916) | Xent 1.7504(1.8110) | Loss 17.4072(14.4392) | Error 0.6231(0.6418) Steps 0(0.00) | Grad Norm 16.1278(13.4461) | Total Time 0.00(0.00)\n",
      "Iter 0278 | Time 62.2691(61.6083) | Bit/dim 4.4076(4.5861) | Xent 1.7406(1.8089) | Loss 13.0535(14.3976) | Error 0.6148(0.6410) Steps 0(0.00) | Grad Norm 11.2525(13.3803) | Total Time 0.00(0.00)\n",
      "Iter 0279 | Time 67.3592(61.7809) | Bit/dim 4.4116(4.5809) | Xent 1.7205(1.8062) | Loss 13.3119(14.3650) | Error 0.6138(0.6401) Steps 0(0.00) | Grad Norm 11.3928(13.3207) | Total Time 0.00(0.00)\n",
      "Iter 0280 | Time 57.1433(61.6417) | Bit/dim 4.5282(4.5793) | Xent 1.6981(1.8030) | Loss 13.3356(14.3342) | Error 0.5936(0.6387) Steps 0(0.00) | Grad Norm 19.4921(13.5058) | Total Time 0.00(0.00)\n",
      "Iter 0281 | Time 65.7004(61.7635) | Bit/dim 4.4787(4.5763) | Xent 1.8598(1.8047) | Loss 13.4916(14.3089) | Error 0.6487(0.6390) Steps 0(0.00) | Grad Norm 32.0855(14.0632) | Total Time 0.00(0.00)\n",
      "Iter 0282 | Time 68.9183(61.9781) | Bit/dim 4.4382(4.5721) | Xent 1.8330(1.8055) | Loss 13.4076(14.2818) | Error 0.6529(0.6395) Steps 0(0.00) | Grad Norm 21.4853(14.2859) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0047 | Time 23.2958, Epoch Time 420.1285(383.3990), Bit/dim 4.4325(best: 4.3972), Xent 1.8503, Loss 5.3577, Error 0.6640(best: 0.6032)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0283 | Time 60.9614(61.9476) | Bit/dim 4.4274(4.5678) | Xent 1.8941(1.8082) | Loss 17.3862(14.3750) | Error 0.6686(0.6403) Steps 0(0.00) | Grad Norm 34.4348(14.8903) | Total Time 0.00(0.00)\n",
      "Iter 0284 | Time 60.2278(61.8960) | Bit/dim 4.4380(4.5639) | Xent 1.8661(1.8099) | Loss 13.3221(14.3434) | Error 0.6655(0.6411) Steps 0(0.00) | Grad Norm 31.7457(15.3960) | Total Time 0.00(0.00)\n",
      "Iter 0285 | Time 66.9983(62.0491) | Bit/dim 4.4912(4.5617) | Xent 2.0164(1.8161) | Loss 13.7224(14.3248) | Error 0.7029(0.6429) Steps 0(0.00) | Grad Norm 43.6292(16.2430) | Total Time 0.00(0.00)\n",
      "Iter 0286 | Time 64.3679(62.1187) | Bit/dim 4.5855(4.5624) | Xent 1.9998(1.8216) | Loss 13.7926(14.3088) | Error 0.7026(0.6447) Steps 0(0.00) | Grad Norm 32.9080(16.7429) | Total Time 0.00(0.00)\n",
      "Iter 0287 | Time 63.4033(62.1572) | Bit/dim 4.5157(4.5610) | Xent 1.8696(1.8231) | Loss 13.5689(14.2866) | Error 0.6678(0.6454) Steps 0(0.00) | Grad Norm 12.7279(16.6225) | Total Time 0.00(0.00)\n",
      "Iter 0288 | Time 62.7301(62.1744) | Bit/dim 4.5481(4.5606) | Xent 1.9030(1.8255) | Loss 13.3820(14.2595) | Error 0.6747(0.6463) Steps 0(0.00) | Grad Norm 19.3713(16.7049) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0048 | Time 23.3087, Epoch Time 418.3864(384.4486), Bit/dim 4.4653(best: 4.3972), Xent 1.8553, Loss 5.3930, Error 0.6544(best: 0.6032)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0289 | Time 59.8675(62.1052) | Bit/dim 4.4754(4.5581) | Xent 1.8818(1.8271) | Loss 17.5488(14.3581) | Error 0.6625(0.6468) Steps 0(0.00) | Grad Norm 12.0642(16.5657) | Total Time 0.00(0.00)\n",
      "Iter 0290 | Time 61.6634(62.0919) | Bit/dim 4.4864(4.5559) | Xent 1.8776(1.8287) | Loss 13.6373(14.3365) | Error 0.6666(0.6474) Steps 0(0.00) | Grad Norm 7.7116(16.3001) | Total Time 0.00(0.00)\n",
      "Iter 0291 | Time 63.0116(62.1195) | Bit/dim 4.4802(4.5536) | Xent 1.8714(1.8299) | Loss 13.4226(14.3091) | Error 0.6591(0.6477) Steps 0(0.00) | Grad Norm 10.6078(16.1293) | Total Time 0.00(0.00)\n",
      "Iter 0292 | Time 59.7890(62.0496) | Bit/dim 4.4332(4.5500) | Xent 1.8594(1.8308) | Loss 13.2617(14.2777) | Error 0.6561(0.6480) Steps 0(0.00) | Grad Norm 9.2953(15.9243) | Total Time 0.00(0.00)\n",
      "Iter 0293 | Time 60.0799(61.9905) | Bit/dim 4.4217(4.5462) | Xent 1.8348(1.8309) | Loss 13.2526(14.2469) | Error 0.6532(0.6481) Steps 0(0.00) | Grad Norm 5.8833(15.6231) | Total Time 0.00(0.00)\n",
      "Iter 0294 | Time 70.2208(62.2374) | Bit/dim 4.4245(4.5425) | Xent 1.8431(1.8313) | Loss 13.1763(14.2148) | Error 0.6566(0.6484) Steps 0(0.00) | Grad Norm 8.1540(15.3990) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0049 | Time 23.2272, Epoch Time 414.2728(385.3434), Bit/dim 4.3979(best: 4.3972), Xent 1.7822, Loss 5.2890, Error 0.6219(best: 0.6032)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0295 | Time 63.3992(62.2723) | Bit/dim 4.3924(4.5380) | Xent 1.8095(1.8307) | Loss 16.9398(14.2966) | Error 0.6419(0.6482) Steps 0(0.00) | Grad Norm 4.8589(15.0828) | Total Time 0.00(0.00)\n",
      "Iter 0296 | Time 61.2401(62.2413) | Bit/dim 4.3953(4.5337) | Xent 1.8202(1.8303) | Loss 13.2962(14.2665) | Error 0.6522(0.6483) Steps 0(0.00) | Grad Norm 6.1468(14.8147) | Total Time 0.00(0.00)\n",
      "Iter 0297 | Time 60.5358(62.1901) | Bit/dim 4.3839(4.5292) | Xent 1.8159(1.8299) | Loss 13.3177(14.2381) | Error 0.6478(0.6483) Steps 0(0.00) | Grad Norm 5.3833(14.5318) | Total Time 0.00(0.00)\n",
      "Iter 0298 | Time 63.9380(62.2426) | Bit/dim 4.3780(4.5247) | Xent 1.7880(1.8286) | Loss 12.9649(14.1999) | Error 0.6352(0.6479) Steps 0(0.00) | Grad Norm 7.2261(14.3126) | Total Time 0.00(0.00)\n",
      "Iter 0299 | Time 65.0078(62.3255) | Bit/dim 4.3727(4.5202) | Xent 1.7857(1.8274) | Loss 12.9282(14.1617) | Error 0.6329(0.6475) Steps 0(0.00) | Grad Norm 6.2033(14.0693) | Total Time 0.00(0.00)\n",
      "Iter 0300 | Time 61.7797(62.3092) | Bit/dim 4.3538(4.5152) | Xent 1.7751(1.8258) | Loss 13.1323(14.1308) | Error 0.6367(0.6471) Steps 0(0.00) | Grad Norm 6.9502(13.8558) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0050 | Time 23.6762, Epoch Time 415.6447(386.2524), Bit/dim 4.3479(best: 4.3972), Xent 1.7095, Loss 5.2026, Error 0.6006(best: 0.6032)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0301 | Time 64.7924(62.3837) | Bit/dim 4.3470(4.5101) | Xent 1.7426(1.8233) | Loss 16.3274(14.1967) | Error 0.6206(0.6463) Steps 0(0.00) | Grad Norm 3.7818(13.5535) | Total Time 0.00(0.00)\n",
      "Iter 0302 | Time 63.9031(62.4292) | Bit/dim 4.3441(4.5051) | Xent 1.7521(1.8212) | Loss 13.0670(14.1629) | Error 0.6261(0.6457) Steps 0(0.00) | Grad Norm 6.9014(13.3540) | Total Time 0.00(0.00)\n",
      "Iter 0303 | Time 67.3712(62.5775) | Bit/dim 4.3385(4.5001) | Xent 1.7120(1.8179) | Loss 13.0753(14.1302) | Error 0.6054(0.6445) Steps 0(0.00) | Grad Norm 5.0599(13.1051) | Total Time 0.00(0.00)\n",
      "Iter 0304 | Time 61.9541(62.5588) | Bit/dim 4.3216(4.4948) | Xent 1.7277(1.8152) | Loss 12.9523(14.0949) | Error 0.6174(0.6437) Steps 0(0.00) | Grad Norm 4.8061(12.8562) | Total Time 0.00(0.00)\n",
      "Iter 0305 | Time 65.8221(62.6567) | Bit/dim 4.3239(4.4897) | Xent 1.7598(1.8135) | Loss 13.1837(14.0675) | Error 0.6216(0.6431) Steps 0(0.00) | Grad Norm 5.9079(12.6477) | Total Time 0.00(0.00)\n",
      "Iter 0306 | Time 64.5923(62.7148) | Bit/dim 4.3603(4.4858) | Xent 1.7000(1.8101) | Loss 12.9521(14.0341) | Error 0.6028(0.6418) Steps 0(0.00) | Grad Norm 10.8136(12.5927) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0051 | Time 24.2144, Epoch Time 428.5123(387.5202), Bit/dim 4.3968(best: 4.3479), Xent 1.7444, Loss 5.2690, Error 0.6155(best: 0.6006)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0307 | Time 68.4925(62.8881) | Bit/dim 4.3950(4.4830) | Xent 1.7959(1.8097) | Loss 17.2058(14.1292) | Error 0.6332(0.6416) Steps 0(0.00) | Grad Norm 26.3526(13.0055) | Total Time 0.00(0.00)\n",
      "Iter 0308 | Time 63.9070(62.9187) | Bit/dim 4.6239(4.4873) | Xent 1.8441(1.8107) | Loss 13.8509(14.1209) | Error 0.6535(0.6419) Steps 0(0.00) | Grad Norm 27.6893(13.4460) | Total Time 0.00(0.00)\n",
      "Iter 0309 | Time 66.1718(63.0163) | Bit/dim 4.3162(4.4821) | Xent 1.7088(1.8077) | Loss 12.8669(14.0833) | Error 0.6094(0.6410) Steps 0(0.00) | Grad Norm 8.9493(13.3111) | Total Time 0.00(0.00)\n",
      "Iter 0310 | Time 73.5702(63.3329) | Bit/dim 4.4475(4.4811) | Xent 1.9950(1.8133) | Loss 13.6634(14.0707) | Error 0.6840(0.6423) Steps 0(0.00) | Grad Norm 34.8508(13.9573) | Total Time 0.00(0.00)\n",
      "Iter 0311 | Time 58.0795(63.1753) | Bit/dim 4.4040(4.4788) | Xent 1.9285(1.8167) | Loss 13.3416(14.0488) | Error 0.6774(0.6433) Steps 0(0.00) | Grad Norm 28.2575(14.3863) | Total Time 0.00(0.00)\n",
      "Iter 0312 | Time 61.2609(63.1178) | Bit/dim 4.5410(4.4807) | Xent 1.8455(1.8176) | Loss 13.5465(14.0337) | Error 0.6589(0.6438) Steps 0(0.00) | Grad Norm 12.3050(14.3239) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0052 | Time 24.1179, Epoch Time 431.7251(388.8463), Bit/dim 4.4094(best: 4.3479), Xent 1.8071, Loss 5.3129, Error 0.6497(best: 0.6006)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0313 | Time 61.0378(63.0554) | Bit/dim 4.4227(4.4789) | Xent 1.8487(1.8185) | Loss 17.5800(14.1401) | Error 0.6558(0.6441) Steps 0(0.00) | Grad Norm 11.5903(14.2419) | Total Time 0.00(0.00)\n",
      "Iter 0314 | Time 67.1013(63.1768) | Bit/dim 4.4278(4.4774) | Xent 1.8930(1.8208) | Loss 13.1316(14.1099) | Error 0.6739(0.6450) Steps 0(0.00) | Grad Norm 22.2675(14.4826) | Total Time 0.00(0.00)\n",
      "Iter 0315 | Time 72.5538(63.4581) | Bit/dim 4.3727(4.4742) | Xent 1.8745(1.8224) | Loss 13.3530(14.0872) | Error 0.6727(0.6459) Steps 0(0.00) | Grad Norm 18.0393(14.5893) | Total Time 0.00(0.00)\n",
      "Iter 0316 | Time 63.0750(63.4466) | Bit/dim 4.3966(4.4719) | Xent 1.8095(1.8220) | Loss 13.3822(14.0660) | Error 0.6394(0.6457) Steps 0(0.00) | Grad Norm 18.2677(14.6997) | Total Time 0.00(0.00)\n",
      "Iter 0317 | Time 60.6269(63.3620) | Bit/dim 4.3532(4.4684) | Xent 1.8233(1.8220) | Loss 13.2820(14.0425) | Error 0.6472(0.6457) Steps 0(0.00) | Grad Norm 18.0757(14.8010) | Total Time 0.00(0.00)\n",
      "Iter 0318 | Time 59.1627(63.2361) | Bit/dim 4.3390(4.4645) | Xent 1.7653(1.8203) | Loss 12.7967(14.0051) | Error 0.6224(0.6450) Steps 0(0.00) | Grad Norm 11.1788(14.6923) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0053 | Time 22.8990, Epoch Time 422.5379(389.8571), Bit/dim 4.3322(best: 4.3479), Xent 1.7664, Loss 5.2153, Error 0.6394(best: 0.6006)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0319 | Time 63.3928(63.2408) | Bit/dim 4.3347(4.4606) | Xent 1.8266(1.8205) | Loss 17.1777(14.1003) | Error 0.6547(0.6453) Steps 0(0.00) | Grad Norm 14.9739(14.7007) | Total Time 0.00(0.00)\n",
      "Iter 0320 | Time 63.2346(63.2406) | Bit/dim 4.3421(4.4570) | Xent 1.7581(1.8186) | Loss 13.0597(14.0691) | Error 0.6276(0.6448) Steps 0(0.00) | Grad Norm 8.6181(14.5183) | Total Time 0.00(0.00)\n",
      "Iter 0321 | Time 62.5321(63.2193) | Bit/dim 4.3292(4.4532) | Xent 1.7329(1.8161) | Loss 13.0575(14.0387) | Error 0.6114(0.6438) Steps 0(0.00) | Grad Norm 8.0309(14.3236) | Total Time 0.00(0.00)\n",
      "Iter 0322 | Time 67.1589(63.3375) | Bit/dim 4.3149(4.4490) | Xent 1.7450(1.8139) | Loss 13.0487(14.0090) | Error 0.6088(0.6427) Steps 0(0.00) | Grad Norm 8.5415(14.1502) | Total Time 0.00(0.00)\n",
      "Iter 0323 | Time 64.9076(63.3846) | Bit/dim 4.2985(4.4445) | Xent 1.7235(1.8112) | Loss 12.7095(13.9700) | Error 0.6109(0.6418) Steps 0(0.00) | Grad Norm 6.1252(13.9094) | Total Time 0.00(0.00)\n",
      "Iter 0324 | Time 62.3717(63.3542) | Bit/dim 4.2721(4.4394) | Xent 1.7138(1.8083) | Loss 12.7476(13.9334) | Error 0.6121(0.6409) Steps 0(0.00) | Grad Norm 6.6119(13.6905) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0054 | Time 23.2389, Epoch Time 422.4547(390.8350), Bit/dim 4.2765(best: 4.3322), Xent 1.6553, Loss 5.1041, Error 0.5851(best: 0.6006)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0325 | Time 62.6572(63.3333) | Bit/dim 4.2707(4.4343) | Xent 1.7230(1.8057) | Loss 17.1309(14.0293) | Error 0.6084(0.6399) Steps 0(0.00) | Grad Norm 9.5021(13.5649) | Total Time 0.00(0.00)\n",
      "Iter 0326 | Time 62.5105(63.3086) | Bit/dim 4.2895(4.4299) | Xent 1.6940(1.8024) | Loss 12.3906(13.9801) | Error 0.6030(0.6388) Steps 0(0.00) | Grad Norm 7.9050(13.3951) | Total Time 0.00(0.00)\n",
      "Iter 0327 | Time 65.7786(63.3827) | Bit/dim 4.2824(4.4255) | Xent 1.6864(1.7989) | Loss 12.7262(13.9425) | Error 0.5980(0.6376) Steps 0(0.00) | Grad Norm 11.3531(13.3338) | Total Time 0.00(0.00)\n",
      "Iter 0328 | Time 60.8427(63.3065) | Bit/dim 4.2710(4.4209) | Xent 1.6846(1.7955) | Loss 12.7497(13.9067) | Error 0.6002(0.6365) Steps 0(0.00) | Grad Norm 9.8403(13.2290) | Total Time 0.00(0.00)\n",
      "Iter 0329 | Time 58.2892(63.1560) | Bit/dim 4.2494(4.4157) | Xent 1.6822(1.7921) | Loss 12.7334(13.8715) | Error 0.5994(0.6353) Steps 0(0.00) | Grad Norm 10.1680(13.1372) | Total Time 0.00(0.00)\n",
      "Iter 0330 | Time 73.9105(63.4786) | Bit/dim 4.2472(4.4107) | Xent 1.6602(1.7881) | Loss 12.5586(13.8321) | Error 0.5930(0.6341) Steps 0(0.00) | Grad Norm 6.4534(12.9367) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0055 | Time 24.4390, Epoch Time 424.1547(391.8346), Bit/dim 4.2454(best: 4.2765), Xent 1.6171, Loss 5.0539, Error 0.5680(best: 0.5851)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0331 | Time 66.4187(63.5668) | Bit/dim 4.2437(4.4057) | Xent 1.6457(1.7839) | Loss 16.8427(13.9225) | Error 0.5984(0.6330) Steps 0(0.00) | Grad Norm 7.6023(12.7766) | Total Time 0.00(0.00)\n",
      "Iter 0332 | Time 65.5384(63.6260) | Bit/dim 4.2549(4.4012) | Xent 1.6793(1.7807) | Loss 12.8438(13.8901) | Error 0.5956(0.6319) Steps 0(0.00) | Grad Norm 13.4352(12.7964) | Total Time 0.00(0.00)\n",
      "Iter 0333 | Time 66.3573(63.7079) | Bit/dim 4.2508(4.3966) | Xent 1.6980(1.7782) | Loss 12.6613(13.8532) | Error 0.5955(0.6308) Steps 0(0.00) | Grad Norm 15.2582(12.8702) | Total Time 0.00(0.00)\n",
      "Iter 0334 | Time 63.8060(63.7109) | Bit/dim 4.2532(4.3923) | Xent 1.6812(1.7753) | Loss 12.6023(13.8157) | Error 0.5981(0.6298) Steps 0(0.00) | Grad Norm 17.1734(12.9993) | Total Time 0.00(0.00)\n",
      "Iter 0335 | Time 73.1600(63.9944) | Bit/dim 4.2689(4.3886) | Xent 1.7019(1.7731) | Loss 12.8464(13.7866) | Error 0.6081(0.6292) Steps 0(0.00) | Grad Norm 17.8952(13.1462) | Total Time 0.00(0.00)\n",
      "Iter 0336 | Time 69.1628(64.1494) | Bit/dim 4.2367(4.3841) | Xent 1.6764(1.7702) | Loss 12.6068(13.7512) | Error 0.5990(0.6283) Steps 0(0.00) | Grad Norm 16.8763(13.2581) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0056 | Time 23.6376, Epoch Time 444.4753(393.4138), Bit/dim 4.2470(best: 4.2454), Xent 1.6977, Loss 5.0958, Error 0.6017(best: 0.5680)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0337 | Time 67.2693(64.2430) | Bit/dim 4.2413(4.3798) | Xent 1.7460(1.7695) | Loss 15.9163(13.8162) | Error 0.6099(0.6277) Steps 0(0.00) | Grad Norm 26.6265(13.6592) | Total Time 0.00(0.00)\n",
      "Iter 0338 | Time 69.1506(64.3902) | Bit/dim 4.3141(4.3778) | Xent 1.7249(1.7682) | Loss 12.8910(13.7884) | Error 0.6090(0.6271) Steps 0(0.00) | Grad Norm 18.8764(13.8157) | Total Time 0.00(0.00)\n",
      "Iter 0339 | Time 66.2286(64.4454) | Bit/dim 4.2176(4.3730) | Xent 1.6732(1.7653) | Loss 12.5204(13.7504) | Error 0.5959(0.6262) Steps 0(0.00) | Grad Norm 9.2559(13.6789) | Total Time 0.00(0.00)\n",
      "Iter 0340 | Time 73.6341(64.7210) | Bit/dim 4.2722(4.3700) | Xent 1.7211(1.7640) | Loss 13.0221(13.7285) | Error 0.6098(0.6257) Steps 0(0.00) | Grad Norm 17.1547(13.7832) | Total Time 0.00(0.00)\n",
      "Iter 0341 | Time 63.2894(64.6781) | Bit/dim 4.2430(4.3662) | Xent 1.6932(1.7619) | Loss 12.9532(13.7053) | Error 0.6070(0.6251) Steps 0(0.00) | Grad Norm 16.1600(13.8545) | Total Time 0.00(0.00)\n",
      "Iter 0342 | Time 65.2398(64.6949) | Bit/dim 4.2693(4.3633) | Xent 1.6901(1.7597) | Loss 12.1298(13.6580) | Error 0.5953(0.6242) Steps 0(0.00) | Grad Norm 15.0910(13.8916) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0057 | Time 24.5646, Epoch Time 445.2479(394.9689), Bit/dim 4.2049(best: 4.2454), Xent 1.6008, Loss 5.0053, Error 0.5710(best: 0.5680)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0343 | Time 62.6864(64.6347) | Bit/dim 4.1974(4.3583) | Xent 1.6435(1.7562) | Loss 16.7025(13.7493) | Error 0.5837(0.6230) Steps 0(0.00) | Grad Norm 5.7610(13.6476) | Total Time 0.00(0.00)\n",
      "Iter 0344 | Time 72.1075(64.8589) | Bit/dim 4.2320(4.3545) | Xent 1.6779(1.7539) | Loss 12.6731(13.7171) | Error 0.5931(0.6221) Steps 0(0.00) | Grad Norm 11.7521(13.5908) | Total Time 0.00(0.00)\n",
      "Iter 0345 | Time 68.8515(64.9786) | Bit/dim 4.1869(4.3495) | Xent 1.6250(1.7500) | Loss 12.5765(13.6828) | Error 0.5815(0.6209) Steps 0(0.00) | Grad Norm 6.6682(13.3831) | Total Time 0.00(0.00)\n",
      "Iter 0346 | Time 63.2557(64.9270) | Bit/dim 4.2410(4.3462) | Xent 1.6504(1.7470) | Loss 12.7618(13.6552) | Error 0.5894(0.6200) Steps 0(0.00) | Grad Norm 9.6927(13.2724) | Total Time 0.00(0.00)\n",
      "Iter 0347 | Time 69.3039(65.0583) | Bit/dim 4.1938(4.3417) | Xent 1.6444(1.7439) | Loss 12.5346(13.6216) | Error 0.5845(0.6189) Steps 0(0.00) | Grad Norm 6.2613(13.0621) | Total Time 0.00(0.00)\n",
      "Iter 0348 | Time 69.9738(65.2057) | Bit/dim 4.2019(4.3375) | Xent 1.6462(1.7410) | Loss 12.7736(13.5962) | Error 0.5933(0.6181) Steps 0(0.00) | Grad Norm 9.4824(12.9547) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0058 | Time 23.7131, Epoch Time 445.9536(396.4984), Bit/dim 4.1926(best: 4.2049), Xent 1.5882, Loss 4.9867, Error 0.5702(best: 0.5680)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0349 | Time 61.5154(65.0950) | Bit/dim 4.2023(4.3334) | Xent 1.6436(1.7381) | Loss 16.9735(13.6975) | Error 0.5874(0.6172) Steps 0(0.00) | Grad Norm 10.0005(12.8660) | Total Time 0.00(0.00)\n",
      "Iter 0350 | Time 69.8163(65.2367) | Bit/dim 4.1753(4.3287) | Xent 1.6046(1.7341) | Loss 12.4498(13.6600) | Error 0.5751(0.6160) Steps 0(0.00) | Grad Norm 10.4723(12.7942) | Total Time 0.00(0.00)\n",
      "Iter 0351 | Time 68.2527(65.3271) | Bit/dim 4.2070(4.3250) | Xent 1.6359(1.7311) | Loss 12.5840(13.6278) | Error 0.5870(0.6151) Steps 0(0.00) | Grad Norm 10.6102(12.7287) | Total Time 0.00(0.00)\n",
      "Iter 0352 | Time 67.9754(65.4066) | Bit/dim 4.1543(4.3199) | Xent 1.6250(1.7279) | Loss 12.4675(13.5930) | Error 0.5813(0.6141) Steps 0(0.00) | Grad Norm 5.7194(12.5184) | Total Time 0.00(0.00)\n",
      "Iter 0353 | Time 67.4047(65.4665) | Bit/dim 4.1746(4.3155) | Xent 1.6082(1.7244) | Loss 12.2998(13.5542) | Error 0.5726(0.6128) Steps 0(0.00) | Grad Norm 14.6756(12.5831) | Total Time 0.00(0.00)\n",
      "Iter 0354 | Time 67.1915(65.5183) | Bit/dim 4.1709(4.3112) | Xent 1.6383(1.7218) | Loss 12.3971(13.5195) | Error 0.5871(0.6121) Steps 0(0.00) | Grad Norm 14.9123(12.6530) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0059 | Time 24.6892, Epoch Time 442.3748(397.8747), Bit/dim 4.1635(best: 4.1926), Xent 1.5752, Loss 4.9511, Error 0.5659(best: 0.5680)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0355 | Time 70.5218(65.6684) | Bit/dim 4.1658(4.3068) | Xent 1.6430(1.7194) | Loss 17.0928(13.6267) | Error 0.5834(0.6112) Steps 0(0.00) | Grad Norm 13.5983(12.6814) | Total Time 0.00(0.00)\n",
      "Iter 0356 | Time 64.9718(65.6475) | Bit/dim 4.2050(4.3038) | Xent 1.6233(1.7165) | Loss 12.7828(13.6013) | Error 0.5751(0.6101) Steps 0(0.00) | Grad Norm 16.6227(12.7996) | Total Time 0.00(0.00)\n",
      "Iter 0357 | Time 68.3242(65.7278) | Bit/dim 4.1447(4.2990) | Xent 1.6121(1.7134) | Loss 12.2766(13.5616) | Error 0.5840(0.6093) Steps 0(0.00) | Grad Norm 12.7438(12.7979) | Total Time 0.00(0.00)\n",
      "Iter 0358 | Time 71.0077(65.8862) | Bit/dim 4.1568(4.2947) | Xent 1.6116(1.7103) | Loss 12.2869(13.5234) | Error 0.5777(0.6084) Steps 0(0.00) | Grad Norm 14.5716(12.8512) | Total Time 0.00(0.00)\n",
      "Iter 0359 | Time 62.2444(65.7769) | Bit/dim 4.1586(4.2907) | Xent 1.6079(1.7073) | Loss 12.2407(13.4849) | Error 0.5740(0.6074) Steps 0(0.00) | Grad Norm 13.6998(12.8766) | Total Time 0.00(0.00)\n",
      "Iter 0360 | Time 65.6714(65.7738) | Bit/dim 4.1577(4.2867) | Xent 1.5994(1.7040) | Loss 12.3801(13.4517) | Error 0.5707(0.6063) Steps 0(0.00) | Grad Norm 9.3164(12.7698) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0060 | Time 23.6178, Epoch Time 442.3639(399.2094), Bit/dim 4.1477(best: 4.1635), Xent 1.5636, Loss 4.9295, Error 0.5594(best: 0.5659)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0361 | Time 68.7994(65.8645) | Bit/dim 4.1474(4.2825) | Xent 1.6249(1.7017) | Loss 16.7824(13.5516) | Error 0.5864(0.6057) Steps 0(0.00) | Grad Norm 19.0149(12.9572) | Total Time 0.00(0.00)\n",
      "Iter 0362 | Time 62.6239(65.7673) | Bit/dim 4.1588(4.2788) | Xent 1.8080(1.7048) | Loss 12.5462(13.5215) | Error 0.6386(0.6066) Steps 0(0.00) | Grad Norm 32.9100(13.5557) | Total Time 0.00(0.00)\n",
      "Iter 0363 | Time 63.3566(65.6950) | Bit/dim 4.1660(4.2754) | Xent 1.8462(1.7091) | Loss 12.6082(13.4941) | Error 0.6501(0.6079) Steps 0(0.00) | Grad Norm 32.8317(14.1340) | Total Time 0.00(0.00)\n",
      "Iter 0364 | Time 66.9937(65.7340) | Bit/dim 4.1906(4.2728) | Xent 1.7151(1.7093) | Loss 12.1616(13.4541) | Error 0.6282(0.6086) Steps 0(0.00) | Grad Norm 18.7701(14.2731) | Total Time 0.00(0.00)\n",
      "Iter 0365 | Time 69.4834(65.8464) | Bit/dim 4.1585(4.2694) | Xent 1.7188(1.7096) | Loss 12.6197(13.4291) | Error 0.6085(0.6086) Steps 0(0.00) | Grad Norm 16.8937(14.3517) | Total Time 0.00(0.00)\n",
      "Iter 0366 | Time 68.1375(65.9152) | Bit/dim 4.1779(4.2667) | Xent 1.6946(1.7091) | Loss 12.7588(13.4090) | Error 0.6058(0.6085) Steps 0(0.00) | Grad Norm 11.5032(14.2663) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0061 | Time 24.5468, Epoch Time 440.4112(400.4454), Bit/dim 4.1481(best: 4.1477), Xent 1.6178, Loss 4.9569, Error 0.5794(best: 0.5594)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0367 | Time 69.3994(66.0197) | Bit/dim 4.1395(4.2629) | Xent 1.6765(1.7081) | Loss 16.5537(13.5033) | Error 0.6019(0.6083) Steps 0(0.00) | Grad Norm 12.6856(14.2188) | Total Time 0.00(0.00)\n",
      "Iter 0368 | Time 64.3261(65.9689) | Bit/dim 4.2065(4.2612) | Xent 1.6487(1.7063) | Loss 12.5909(13.4759) | Error 0.5813(0.6075) Steps 0(0.00) | Grad Norm 13.4789(14.1967) | Total Time 0.00(0.00)\n",
      "Iter 0369 | Time 65.2173(65.9463) | Bit/dim 4.1837(4.2588) | Xent 1.6274(1.7040) | Loss 12.5982(13.4496) | Error 0.5794(0.6066) Steps 0(0.00) | Grad Norm 12.4067(14.1430) | Total Time 0.00(0.00)\n",
      "Iter 0370 | Time 67.1175(65.9815) | Bit/dim 4.1226(4.2548) | Xent 1.6128(1.7012) | Loss 12.5139(13.4215) | Error 0.5746(0.6057) Steps 0(0.00) | Grad Norm 4.4858(13.8532) | Total Time 0.00(0.00)\n",
      "Iter 0371 | Time 69.3318(66.0820) | Bit/dim 4.1413(4.2514) | Xent 1.6155(1.6987) | Loss 12.4602(13.3927) | Error 0.5811(0.6049) Steps 0(0.00) | Grad Norm 9.7437(13.7300) | Total Time 0.00(0.00)\n",
      "Iter 0372 | Time 72.7292(66.2814) | Bit/dim 4.1470(4.2482) | Xent 1.6320(1.6967) | Loss 12.5501(13.3674) | Error 0.5910(0.6045) Steps 0(0.00) | Grad Norm 10.0969(13.6210) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0062 | Time 25.3489, Epoch Time 449.7868(401.9257), Bit/dim 4.1440(best: 4.1477), Xent 1.5639, Loss 4.9260, Error 0.5546(best: 0.5594)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0373 | Time 68.6010(66.3510) | Bit/dim 4.1329(4.2448) | Xent 1.5885(1.6934) | Loss 16.3026(13.4555) | Error 0.5709(0.6035) Steps 0(0.00) | Grad Norm 8.2796(13.4607) | Total Time 0.00(0.00)\n",
      "Iter 0374 | Time 65.1398(66.3147) | Bit/dim 4.1339(4.2414) | Xent 1.5984(1.6906) | Loss 12.4848(13.4264) | Error 0.5774(0.6027) Steps 0(0.00) | Grad Norm 5.3032(13.2160) | Total Time 0.00(0.00)\n",
      "Iter 0375 | Time 68.5292(66.3811) | Bit/dim 4.1183(4.2377) | Xent 1.6033(1.6880) | Loss 12.4817(13.3980) | Error 0.5741(0.6019) Steps 0(0.00) | Grad Norm 6.8523(13.0251) | Total Time 0.00(0.00)\n",
      "Iter 0376 | Time 68.9427(66.4579) | Bit/dim 4.1228(4.2343) | Xent 1.5998(1.6853) | Loss 12.2786(13.3644) | Error 0.5797(0.6012) Steps 0(0.00) | Grad Norm 7.3658(12.8553) | Total Time 0.00(0.00)\n",
      "Iter 0377 | Time 69.4621(66.5481) | Bit/dim 4.1085(4.2305) | Xent 1.5798(1.6821) | Loss 12.2552(13.3312) | Error 0.5581(0.5999) Steps 0(0.00) | Grad Norm 6.9083(12.6769) | Total Time 0.00(0.00)\n",
      "Iter 0378 | Time 66.5802(66.5490) | Bit/dim 4.1107(4.2269) | Xent 1.5670(1.6787) | Loss 12.3358(13.3013) | Error 0.5656(0.5989) Steps 0(0.00) | Grad Norm 10.3605(12.6074) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0063 | Time 24.5241, Epoch Time 448.1871(403.3135), Bit/dim 4.0986(best: 4.1440), Xent 1.5242, Loss 4.8607, Error 0.5443(best: 0.5546)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0379 | Time 68.9028(66.6196) | Bit/dim 4.0881(4.2228) | Xent 1.5768(1.6756) | Loss 16.4790(13.3966) | Error 0.5662(0.5979) Steps 0(0.00) | Grad Norm 8.6325(12.4882) | Total Time 0.00(0.00)\n",
      "Iter 0380 | Time 70.4504(66.7346) | Bit/dim 4.0941(4.2189) | Xent 1.5680(1.6724) | Loss 12.3971(13.3666) | Error 0.5577(0.5967) Steps 0(0.00) | Grad Norm 8.5911(12.3712) | Total Time 0.00(0.00)\n",
      "Iter 0381 | Time 66.9509(66.7411) | Bit/dim 4.0853(4.2149) | Xent 1.5699(1.6693) | Loss 11.9373(13.3238) | Error 0.5644(0.5957) Steps 0(0.00) | Grad Norm 11.9648(12.3591) | Total Time 0.00(0.00)\n",
      "Iter 0382 | Time 65.8365(66.7139) | Bit/dim 4.1033(4.2115) | Xent 1.6044(1.6674) | Loss 12.1487(13.2885) | Error 0.5765(0.5951) Steps 0(0.00) | Grad Norm 15.5430(12.4546) | Total Time 0.00(0.00)\n",
      "Iter 0383 | Time 67.5861(66.7401) | Bit/dim 4.1500(4.2097) | Xent 1.6503(1.6669) | Loss 12.4908(13.2646) | Error 0.5856(0.5949) Steps 0(0.00) | Grad Norm 22.2929(12.7497) | Total Time 0.00(0.00)\n",
      "Iter 0384 | Time 76.3459(67.0283) | Bit/dim 4.1164(4.2069) | Xent 1.6298(1.6658) | Loss 12.2700(13.2347) | Error 0.5856(0.5946) Steps 0(0.00) | Grad Norm 21.9020(13.0243) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0064 | Time 24.9099, Epoch Time 457.2461(404.9315), Bit/dim 4.1094(best: 4.0986), Xent 1.6042, Loss 4.9115, Error 0.5802(best: 0.5443)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0385 | Time 67.5527(67.0440) | Bit/dim 4.1030(4.2038) | Xent 1.6515(1.6653) | Loss 16.4681(13.3317) | Error 0.5929(0.5945) Steps 0(0.00) | Grad Norm 16.9131(13.1409) | Total Time 0.00(0.00)\n",
      "Iter 0386 | Time 66.6730(67.0329) | Bit/dim 4.0932(4.2005) | Xent 1.5705(1.6625) | Loss 12.3115(13.3011) | Error 0.5717(0.5938) Steps 0(0.00) | Grad Norm 8.9513(13.0153) | Total Time 0.00(0.00)\n",
      "Iter 0387 | Time 64.5637(66.9588) | Bit/dim 4.0970(4.1974) | Xent 1.6054(1.6608) | Loss 12.3377(13.2722) | Error 0.5742(0.5933) Steps 0(0.00) | Grad Norm 15.2170(13.0813) | Total Time 0.00(0.00)\n",
      "Iter 0388 | Time 69.6661(67.0400) | Bit/dim 4.0986(4.1944) | Xent 1.6036(1.6591) | Loss 12.0346(13.2351) | Error 0.5817(0.5929) Steps 0(0.00) | Grad Norm 12.7366(13.0710) | Total Time 0.00(0.00)\n",
      "Iter 0389 | Time 62.4309(66.9017) | Bit/dim 4.0861(4.1911) | Xent 1.5669(1.6563) | Loss 12.3786(13.2094) | Error 0.5576(0.5919) Steps 0(0.00) | Grad Norm 7.9730(12.9180) | Total Time 0.00(0.00)\n",
      "Iter 0390 | Time 70.4806(67.0091) | Bit/dim 4.0785(4.1878) | Xent 1.5812(1.6540) | Loss 12.3094(13.1824) | Error 0.5721(0.5913) Steps 0(0.00) | Grad Norm 13.1584(12.9252) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0065 | Time 24.0131, Epoch Time 441.7326(406.0355), Bit/dim 4.0782(best: 4.0986), Xent 1.5067, Loss 4.8316, Error 0.5445(best: 0.5443)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0391 | Time 65.8654(66.9748) | Bit/dim 4.0750(4.1844) | Xent 1.5383(1.6506) | Loss 16.2736(13.2751) | Error 0.5591(0.5903) Steps 0(0.00) | Grad Norm 9.2365(12.8146) | Total Time 0.00(0.00)\n",
      "Iter 0392 | Time 68.8230(67.0302) | Bit/dim 4.0913(4.1816) | Xent 1.5689(1.6481) | Loss 12.2336(13.2439) | Error 0.5642(0.5895) Steps 0(0.00) | Grad Norm 15.5382(12.8963) | Total Time 0.00(0.00)\n",
      "Iter 0393 | Time 69.8852(67.1159) | Bit/dim 4.1042(4.1793) | Xent 1.6104(1.6470) | Loss 12.2977(13.2155) | Error 0.5800(0.5892) Steps 0(0.00) | Grad Norm 22.1303(13.1733) | Total Time 0.00(0.00)\n",
      "Iter 0394 | Time 66.1785(67.0878) | Bit/dim 4.1437(4.1782) | Xent 1.6040(1.6457) | Loss 12.3796(13.1904) | Error 0.5754(0.5888) Steps 0(0.00) | Grad Norm 18.7178(13.3396) | Total Time 0.00(0.00)\n",
      "Iter 0395 | Time 74.3480(67.3056) | Bit/dim 4.1017(4.1759) | Xent 1.5749(1.6436) | Loss 12.3422(13.1650) | Error 0.5646(0.5881) Steps 0(0.00) | Grad Norm 15.5188(13.4050) | Total Time 0.00(0.00)\n",
      "Iter 0396 | Time 71.9564(67.4451) | Bit/dim 4.0880(4.1733) | Xent 1.7684(1.6473) | Loss 12.4703(13.1441) | Error 0.6318(0.5894) Steps 0(0.00) | Grad Norm 28.6693(13.8629) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0066 | Time 24.2835, Epoch Time 457.1948(407.5703), Bit/dim 4.0870(best: 4.0782), Xent 1.7306, Loss 4.9523, Error 0.6283(best: 0.5443)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0397 | Time 69.9315(67.5197) | Bit/dim 4.0841(4.1706) | Xent 1.7907(1.6516) | Loss 16.7201(13.2514) | Error 0.6359(0.5908) Steps 0(0.00) | Grad Norm 26.2675(14.2351) | Total Time 0.00(0.00)\n",
      "Iter 0398 | Time 68.8578(67.5598) | Bit/dim 4.1001(4.1685) | Xent 1.6214(1.6507) | Loss 12.2735(13.2221) | Error 0.5831(0.5906) Steps 0(0.00) | Grad Norm 6.5512(14.0046) | Total Time 0.00(0.00)\n",
      "Iter 0399 | Time 69.4091(67.6153) | Bit/dim 4.0722(4.1656) | Xent 1.6555(1.6509) | Loss 12.4568(13.1991) | Error 0.5941(0.5907) Steps 0(0.00) | Grad Norm 10.8287(13.9093) | Total Time 0.00(0.00)\n",
      "Iter 0400 | Time 68.7619(67.6497) | Bit/dim 4.0815(4.1631) | Xent 1.6275(1.6502) | Loss 12.2005(13.1692) | Error 0.5877(0.5906) Steps 0(0.00) | Grad Norm 8.5156(13.7475) | Total Time 0.00(0.00)\n",
      "Iter 0401 | Time 73.9327(67.8382) | Bit/dim 4.0830(4.1607) | Xent 1.5577(1.6474) | Loss 12.1511(13.1386) | Error 0.5548(0.5895) Steps 0(0.00) | Grad Norm 7.3458(13.5554) | Total Time 0.00(0.00)\n",
      "Iter 0402 | Time 59.7289(67.5949) | Bit/dim 4.0833(4.1583) | Xent 1.5468(1.6444) | Loss 12.1416(13.1087) | Error 0.5525(0.5884) Steps 0(0.00) | Grad Norm 8.9433(13.4171) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0067 | Time 24.6102, Epoch Time 451.1300(408.8771), Bit/dim 4.0764(best: 4.0782), Xent 1.4829, Loss 4.8179, Error 0.5375(best: 0.5443)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0403 | Time 68.9409(67.6353) | Bit/dim 4.0732(4.1558) | Xent 1.5227(1.6407) | Loss 16.3330(13.2054) | Error 0.5533(0.5873) Steps 0(0.00) | Grad Norm 7.0107(13.2249) | Total Time 0.00(0.00)\n",
      "Iter 0404 | Time 70.5591(67.7230) | Bit/dim 4.0704(4.1532) | Xent 1.5401(1.6377) | Loss 12.1515(13.1738) | Error 0.5584(0.5865) Steps 0(0.00) | Grad Norm 9.0055(13.0983) | Total Time 0.00(0.00)\n",
      "Iter 0405 | Time 69.1730(67.7665) | Bit/dim 4.0712(4.1508) | Xent 1.5467(1.6350) | Loss 12.2872(13.1472) | Error 0.5581(0.5856) Steps 0(0.00) | Grad Norm 11.7901(13.0590) | Total Time 0.00(0.00)\n",
      "Iter 0406 | Time 68.0485(67.7750) | Bit/dim 4.0703(4.1484) | Xent 1.5221(1.6316) | Loss 12.2520(13.1204) | Error 0.5443(0.5844) Steps 0(0.00) | Grad Norm 6.1528(12.8519) | Total Time 0.00(0.00)\n",
      "Iter 0407 | Time 74.1712(67.9669) | Bit/dim 4.0521(4.1455) | Xent 1.5673(1.6297) | Loss 12.2755(13.0950) | Error 0.5642(0.5838) Steps 0(0.00) | Grad Norm 11.8820(12.8228) | Total Time 0.00(0.00)\n",
      "Iter 0408 | Time 63.7738(67.8411) | Bit/dim 4.0810(4.1435) | Xent 1.6576(1.6305) | Loss 12.4644(13.0761) | Error 0.6009(0.5843) Steps 0(0.00) | Grad Norm 20.5570(13.0548) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0068 | Time 24.3718, Epoch Time 454.7875(410.2544), Bit/dim 4.1021(best: 4.0764), Xent 1.5887, Loss 4.8965, Error 0.5777(best: 0.5375)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0409 | Time 64.8495(67.7513) | Bit/dim 4.1113(4.1426) | Xent 1.6558(1.6312) | Loss 16.3647(13.1748) | Error 0.5949(0.5846) Steps 0(0.00) | Grad Norm 23.3248(13.3629) | Total Time 0.00(0.00)\n",
      "Iter 0410 | Time 65.0475(67.6702) | Bit/dim 4.0897(4.1410) | Xent 1.7871(1.6359) | Loss 12.2760(13.1478) | Error 0.6265(0.5859) Steps 0(0.00) | Grad Norm 24.8283(13.7069) | Total Time 0.00(0.00)\n",
      "Iter 0411 | Time 61.8493(67.4956) | Bit/dim 4.0883(4.1394) | Xent 1.6366(1.6359) | Loss 12.3390(13.1235) | Error 0.5871(0.5859) Steps 0(0.00) | Grad Norm 18.8590(13.8614) | Total Time 0.00(0.00)\n",
      "Iter 0412 | Time 63.7393(67.3829) | Bit/dim 4.0904(4.1379) | Xent 1.6506(1.6364) | Loss 12.3870(13.1014) | Error 0.5978(0.5863) Steps 0(0.00) | Grad Norm 12.7720(13.8287) | Total Time 0.00(0.00)\n",
      "Iter 0413 | Time 67.3341(67.3814) | Bit/dim 4.1101(4.1371) | Xent 1.5948(1.6351) | Loss 12.4608(13.0822) | Error 0.5820(0.5861) Steps 0(0.00) | Grad Norm 13.5075(13.8191) | Total Time 0.00(0.00)\n",
      "Iter 0414 | Time 63.2841(67.2585) | Bit/dim 4.0662(4.1350) | Xent 1.5652(1.6330) | Loss 12.1030(13.0528) | Error 0.5685(0.5856) Steps 0(0.00) | Grad Norm 6.2563(13.5922) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0069 | Time 25.0447, Epoch Time 427.3407(410.7670), Bit/dim 4.0708(best: 4.0764), Xent 1.5707, Loss 4.8562, Error 0.5632(best: 0.5375)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0415 | Time 68.6384(67.2999) | Bit/dim 4.0727(4.1331) | Xent 1.5959(1.6319) | Loss 16.4637(13.1552) | Error 0.5784(0.5854) Steps 0(0.00) | Grad Norm 14.7822(13.6279) | Total Time 0.00(0.00)\n",
      "Iter 0416 | Time 68.4262(67.3337) | Bit/dim 4.0684(4.1312) | Xent 1.6581(1.6327) | Loss 12.3614(13.1313) | Error 0.5931(0.5856) Steps 0(0.00) | Grad Norm 17.4208(13.7417) | Total Time 0.00(0.00)\n",
      "Iter 0417 | Time 67.4392(67.3369) | Bit/dim 4.0764(4.1295) | Xent 1.6350(1.6328) | Loss 12.5326(13.1134) | Error 0.5851(0.5856) Steps 0(0.00) | Grad Norm 20.1220(13.9331) | Total Time 0.00(0.00)\n",
      "Iter 0418 | Time 69.5592(67.4035) | Bit/dim 4.0947(4.1285) | Xent 1.5672(1.6308) | Loss 12.0454(13.0814) | Error 0.5647(0.5850) Steps 0(0.00) | Grad Norm 10.5957(13.8330) | Total Time 0.00(0.00)\n",
      "Iter 0419 | Time 65.6387(67.3506) | Bit/dim 4.0585(4.1264) | Xent 1.5599(1.6287) | Loss 12.1112(13.0522) | Error 0.5639(0.5843) Steps 0(0.00) | Grad Norm 7.6946(13.6488) | Total Time 0.00(0.00)\n",
      "Iter 0420 | Time 68.5703(67.3872) | Bit/dim 4.0643(4.1245) | Xent 1.6215(1.6285) | Loss 12.2927(13.0295) | Error 0.5904(0.5845) Steps 0(0.00) | Grad Norm 18.8008(13.8034) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0070 | Time 24.3986, Epoch Time 449.0781(411.9163), Bit/dim 4.0437(best: 4.0708), Xent 1.5437, Loss 4.8156, Error 0.5548(best: 0.5375)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0421 | Time 67.5224(67.3912) | Bit/dim 4.0408(4.1220) | Xent 1.5769(1.6269) | Loss 15.9508(13.1171) | Error 0.5691(0.5841) Steps 0(0.00) | Grad Norm 12.4724(13.7635) | Total Time 0.00(0.00)\n",
      "Iter 0422 | Time 66.9145(67.3769) | Bit/dim 4.0568(4.1200) | Xent 1.5468(1.6245) | Loss 12.1954(13.0895) | Error 0.5536(0.5832) Steps 0(0.00) | Grad Norm 8.7339(13.6126) | Total Time 0.00(0.00)\n",
      "Iter 0423 | Time 67.7361(67.3877) | Bit/dim 4.0484(4.1179) | Xent 1.5723(1.6229) | Loss 12.2643(13.0647) | Error 0.5735(0.5829) Steps 0(0.00) | Grad Norm 12.0003(13.5642) | Total Time 0.00(0.00)\n",
      "Iter 0424 | Time 73.8614(67.5819) | Bit/dim 4.0347(4.1154) | Xent 1.5279(1.6201) | Loss 12.0733(13.0350) | Error 0.5516(0.5819) Steps 0(0.00) | Grad Norm 5.8927(13.3341) | Total Time 0.00(0.00)\n",
      "Iter 0425 | Time 72.8044(67.7386) | Bit/dim 4.0342(4.1130) | Xent 1.5444(1.6178) | Loss 12.2085(13.0102) | Error 0.5548(0.5811) Steps 0(0.00) | Grad Norm 7.1637(13.1490) | Total Time 0.00(0.00)\n",
      "Iter 0426 | Time 72.9319(67.8944) | Bit/dim 4.0207(4.1102) | Xent 1.5302(1.6152) | Loss 12.0916(12.9826) | Error 0.5554(0.5803) Steps 0(0.00) | Grad Norm 8.3253(13.0042) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0071 | Time 24.4345, Epoch Time 462.2022(413.4249), Bit/dim 4.0176(best: 4.0437), Xent 1.4608, Loss 4.7480, Error 0.5250(best: 0.5375)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0427 | Time 68.6273(67.9164) | Bit/dim 4.0163(4.1074) | Xent 1.5057(1.6119) | Loss 15.8127(13.0675) | Error 0.5416(0.5792) Steps 0(0.00) | Grad Norm 4.0367(12.7352) | Total Time 0.00(0.00)\n",
      "Iter 0428 | Time 70.8472(68.0043) | Bit/dim 4.0046(4.1043) | Xent 1.5165(1.6090) | Loss 11.9605(13.0343) | Error 0.5484(0.5783) Steps 0(0.00) | Grad Norm 6.0092(12.5334) | Total Time 0.00(0.00)\n",
      "Iter 0429 | Time 75.7212(68.2358) | Bit/dim 4.0086(4.1014) | Xent 1.5045(1.6059) | Loss 12.1727(13.0084) | Error 0.5425(0.5772) Steps 0(0.00) | Grad Norm 4.4154(12.2899) | Total Time 0.00(0.00)\n",
      "Iter 0430 | Time 75.2127(68.4451) | Bit/dim 4.0148(4.0988) | Xent 1.4889(1.6024) | Loss 12.0079(12.9784) | Error 0.5395(0.5760) Steps 0(0.00) | Grad Norm 5.0300(12.0721) | Total Time 0.00(0.00)\n",
      "Iter 0431 | Time 65.7178(68.3633) | Bit/dim 3.9930(4.0957) | Xent 1.4743(1.5986) | Loss 11.8789(12.9454) | Error 0.5364(0.5749) Steps 0(0.00) | Grad Norm 8.1303(11.9538) | Total Time 0.00(0.00)\n",
      "Iter 0432 | Time 68.1724(68.3576) | Bit/dim 3.9956(4.0927) | Xent 1.4866(1.5952) | Loss 12.1662(12.9221) | Error 0.5365(0.5737) Steps 0(0.00) | Grad Norm 11.7315(11.9472) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0072 | Time 25.5259, Epoch Time 465.7418(414.9944), Bit/dim 3.9986(best: 4.0176), Xent 1.4835, Loss 4.7403, Error 0.5357(best: 0.5250)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0433 | Time 69.9959(68.4067) | Bit/dim 4.0032(4.0900) | Xent 1.5254(1.5931) | Loss 16.3429(13.0247) | Error 0.5493(0.5730) Steps 0(0.00) | Grad Norm 13.8960(12.0056) | Total Time 0.00(0.00)\n",
      "Iter 0434 | Time 72.0377(68.5156) | Bit/dim 3.9807(4.0867) | Xent 1.4992(1.5903) | Loss 12.1236(12.9977) | Error 0.5400(0.5720) Steps 0(0.00) | Grad Norm 11.2093(11.9817) | Total Time 0.00(0.00)\n",
      "Iter 0435 | Time 69.3037(68.5393) | Bit/dim 3.9967(4.0840) | Xent 1.4885(1.5872) | Loss 12.0216(12.9684) | Error 0.5443(0.5712) Steps 0(0.00) | Grad Norm 8.8562(11.8880) | Total Time 0.00(0.00)\n",
      "Iter 0436 | Time 69.6195(68.5717) | Bit/dim 3.9970(4.0814) | Xent 1.4900(1.5843) | Loss 12.2561(12.9470) | Error 0.5391(0.5702) Steps 0(0.00) | Grad Norm 12.9372(11.9195) | Total Time 0.00(0.00)\n",
      "Iter 0437 | Time 66.0685(68.4966) | Bit/dim 4.0040(4.0791) | Xent 1.5655(1.5838) | Loss 11.9766(12.9179) | Error 0.5651(0.5700) Steps 0(0.00) | Grad Norm 15.6017(12.0299) | Total Time 0.00(0.00)\n",
      "Iter 0438 | Time 70.6693(68.5618) | Bit/dim 3.9934(4.0765) | Xent 1.5152(1.5817) | Loss 11.9102(12.8877) | Error 0.5521(0.5695) Steps 0(0.00) | Grad Norm 13.1752(12.0643) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0073 | Time 24.1628, Epoch Time 458.5052(416.2997), Bit/dim 3.9983(best: 3.9986), Xent 1.4897, Loss 4.7432, Error 0.5351(best: 0.5250)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0439 | Time 70.0338(68.6059) | Bit/dim 4.0010(4.0742) | Xent 1.5253(1.5800) | Loss 15.6486(12.9705) | Error 0.5460(0.5688) Steps 0(0.00) | Grad Norm 16.3228(12.1920) | Total Time 0.00(0.00)\n",
      "Iter 0440 | Time 67.0318(68.5587) | Bit/dim 3.9949(4.0718) | Xent 1.5358(1.5787) | Loss 12.0096(12.9417) | Error 0.5588(0.5685) Steps 0(0.00) | Grad Norm 18.8994(12.3933) | Total Time 0.00(0.00)\n",
      "Iter 0441 | Time 65.8211(68.4766) | Bit/dim 3.9855(4.0693) | Xent 1.5607(1.5781) | Loss 11.9803(12.9128) | Error 0.5611(0.5683) Steps 0(0.00) | Grad Norm 17.2763(12.5398) | Total Time 0.00(0.00)\n",
      "Iter 0442 | Time 72.5945(68.6001) | Bit/dim 3.9697(4.0663) | Xent 1.5219(1.5765) | Loss 11.9008(12.8825) | Error 0.5501(0.5677) Steps 0(0.00) | Grad Norm 13.1135(12.5570) | Total Time 0.00(0.00)\n",
      "Iter 0443 | Time 66.8645(68.5481) | Bit/dim 3.9763(4.0636) | Xent 1.4844(1.5737) | Loss 11.8971(12.8529) | Error 0.5428(0.5670) Steps 0(0.00) | Grad Norm 7.0313(12.3912) | Total Time 0.00(0.00)\n",
      "Iter 0444 | Time 69.0034(68.5617) | Bit/dim 3.9809(4.0611) | Xent 1.4876(1.5711) | Loss 11.8124(12.8217) | Error 0.5383(0.5661) Steps 0(0.00) | Grad Norm 7.7966(12.2534) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0074 | Time 24.5765, Epoch Time 452.0782(417.3731), Bit/dim 3.9941(best: 3.9983), Xent 1.4366, Loss 4.7124, Error 0.5217(best: 0.5250)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0445 | Time 66.1430(68.4892) | Bit/dim 4.0063(4.0594) | Xent 1.4748(1.5682) | Loss 16.1998(12.9230) | Error 0.5289(0.5650) Steps 0(0.00) | Grad Norm 12.8438(12.2711) | Total Time 0.00(0.00)\n",
      "Iter 0446 | Time 68.8891(68.5012) | Bit/dim 3.9894(4.0573) | Xent 1.4683(1.5652) | Loss 11.7224(12.8870) | Error 0.5324(0.5640) Steps 0(0.00) | Grad Norm 10.7219(12.2246) | Total Time 0.00(0.00)\n",
      "Iter 0447 | Time 68.1838(68.4916) | Bit/dim 3.9604(4.0544) | Xent 1.4733(1.5625) | Loss 11.9072(12.8576) | Error 0.5257(0.5629) Steps 0(0.00) | Grad Norm 9.1709(12.1330) | Total Time 0.00(0.00)\n",
      "Iter 0448 | Time 68.5941(68.4947) | Bit/dim 3.9755(4.0521) | Xent 1.4595(1.5594) | Loss 11.8830(12.8284) | Error 0.5331(0.5620) Steps 0(0.00) | Grad Norm 11.6863(12.1196) | Total Time 0.00(0.00)\n",
      "Iter 0449 | Time 74.4428(68.6732) | Bit/dim 3.9687(4.0496) | Xent 1.4624(1.5565) | Loss 11.9022(12.8006) | Error 0.5300(0.5610) Steps 0(0.00) | Grad Norm 9.7109(12.0473) | Total Time 0.00(0.00)\n",
      "Iter 0450 | Time 66.1784(68.5983) | Bit/dim 3.9550(4.0467) | Xent 1.4590(1.5535) | Loss 11.8451(12.7719) | Error 0.5209(0.5598) Steps 0(0.00) | Grad Norm 2.7982(11.7699) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0075 | Time 23.1860, Epoch Time 451.4914(418.3966), Bit/dim 3.9685(best: 3.9941), Xent 1.4036, Loss 4.6703, Error 0.5070(best: 0.5217)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0451 | Time 68.1393(68.5845) | Bit/dim 3.9649(4.0443) | Xent 1.4483(1.5504) | Loss 15.7696(12.8619) | Error 0.5206(0.5586) Steps 0(0.00) | Grad Norm 7.3284(11.6366) | Total Time 0.00(0.00)\n",
      "Iter 0452 | Time 66.1344(68.5110) | Bit/dim 3.9346(4.0410) | Xent 1.4518(1.5474) | Loss 11.7337(12.8280) | Error 0.5179(0.5574) Steps 0(0.00) | Grad Norm 5.6319(11.4565) | Total Time 0.00(0.00)\n",
      "Iter 0453 | Time 67.9548(68.4944) | Bit/dim 3.9470(4.0382) | Xent 1.4333(1.5440) | Loss 11.6666(12.7932) | Error 0.5136(0.5561) Steps 0(0.00) | Grad Norm 3.6641(11.2227) | Total Time 0.00(0.00)\n",
      "Iter 0454 | Time 70.1948(68.5454) | Bit/dim 3.9617(4.0359) | Xent 1.4651(1.5416) | Loss 11.8491(12.7649) | Error 0.5241(0.5551) Steps 0(0.00) | Grad Norm 10.6187(11.2046) | Total Time 0.00(0.00)\n",
      "Iter 0455 | Time 68.8795(68.5554) | Bit/dim 3.9707(4.0339) | Xent 1.4893(1.5401) | Loss 11.8786(12.7383) | Error 0.5351(0.5545) Steps 0(0.00) | Grad Norm 16.3568(11.3591) | Total Time 0.00(0.00)\n",
      "Iter 0456 | Time 69.1801(68.5741) | Bit/dim 3.9945(4.0327) | Xent 1.8505(1.5494) | Loss 12.3081(12.7254) | Error 0.6211(0.5565) Steps 0(0.00) | Grad Norm 42.7465(12.3008) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0076 | Time 24.6010, Epoch Time 451.3447(419.3851), Bit/dim 4.2025(best: 3.9685), Xent 2.2482, Loss 5.3267, Error 0.7423(best: 0.5070)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0457 | Time 70.6063(68.6351) | Bit/dim 4.2136(4.0382) | Xent 2.3175(1.5724) | Loss 16.3834(12.8351) | Error 0.7426(0.5621) Steps 0(0.00) | Grad Norm 33.6818(12.9422) | Total Time 0.00(0.00)\n",
      "Iter 0458 | Time 66.1726(68.5612) | Bit/dim 4.2654(4.0450) | Xent 3.5144(1.6307) | Loss 14.8194(12.8946) | Error 0.8346(0.5703) Steps 0(0.00) | Grad Norm 63.7829(14.4674) | Total Time 0.00(0.00)\n",
      "Iter 0459 | Time 65.4717(68.4685) | Bit/dim 5.3504(4.0841) | Xent 3.3811(1.6832) | Loss 17.4454(13.0312) | Error 0.8754(0.5795) Steps 0(0.00) | Grad Norm 35.1112(15.0867) | Total Time 0.00(0.00)\n",
      "Iter 0460 | Time 74.8788(68.6608) | Bit/dim 6.3815(4.1531) | Xent 4.8556(1.7784) | Loss 21.7128(13.2916) | Error 0.8455(0.5874) Steps 0(0.00) | Grad Norm 204.5493(20.7706) | Total Time 0.00(0.00)\n",
      "Iter 0461 | Time 62.8267(68.4858) | Bit/dim 5.9836(4.2080) | Xent 3.3717(1.8262) | Loss 18.6963(13.4537) | Error 0.8680(0.5959) Steps 0(0.00) | Grad Norm 39.3368(21.3276) | Total Time 0.00(0.00)\n",
      "Iter 0462 | Time 89.7475(69.1237) | Bit/dim 6.1196(4.2653) | Xent 5.0813(1.9238) | Loss 21.8704(13.7062) | Error 0.8820(0.6044) Steps 0(0.00) | Grad Norm 7773.1374(253.8819) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0077 | Time 38.6973, Epoch Time 484.2119(421.3299), Bit/dim 9.5193(best: 3.9685), Xent 16.4519, Loss 17.7452, Error 0.8970(best: 0.5070)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_rl.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 8000 --test_batch_size 5000 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_rl_stdscale_1_run1 --resume ../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_rl_stdscale_1_run1/epoch_72_checkpt.pth --seed 1 --conditional True --controlled_tol True --train_mode semisup --lr 0.01 --warmup_iters 1000 --atol 1e-4  --rtol 1e-4 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5 --scale_fac 1.0 --scale_std 1.0 --max_grad_norm 20.0\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
