{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='4,5,6,7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_rl.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import torch.utils.data as data\n",
      "from torch.utils.data import Dataset\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "plt.rcParams['figure.dpi'] = 300\n",
      "\n",
      "from PIL import Image\n",
      "import os.path\n",
      "import errno\n",
      "import codecs\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time, count_nfe_gate\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "GATES = [\"cnn1\", \"cnn2\", \"rnn\"]\n",
      "\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"colormnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--gate', type=str, default='cnn1', choices=GATES)\n",
      "parser.add_argument('--scale', type=float, default=1.0)\n",
      "parser.add_argument('--scale_fac', type=float, default=1.0)\n",
      "parser.add_argument('--scale_std', type=float, default=1.0)\n",
      "parser.add_argument('--eta', default=0.1, type=float,\n",
      "                        help='tuning parameter that allows us to trade-off the competing goals of' \n",
      "                                'minimizing the prediction loss and maximizing the gate rewards ')\n",
      "parser.add_argument('--rl-weight', default=0.01, type=float,\n",
      "                        help='rl weight')\n",
      "\n",
      "parser.add_argument('--gamma', default=0.99, type=float,\n",
      "                        help='discount factor, default: (0.99)')\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=500)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "parser.add_argument(\"--annealing_std\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--y_class\", type=int, default=10)\n",
      "parser.add_argument(\"--y_color\", type=int, default=10)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "import lib.odenvp_conditional_rl as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "class ColorMNIST(data.Dataset):\n",
      "    \"\"\"\n",
      "    ColorMNIST\n",
      "    \"\"\"\n",
      "    urls = [\n",
      "        'http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz',\n",
      "    ]\n",
      "    raw_folder = 'raw'\n",
      "    processed_folder = 'processed'\n",
      "    training_file = 'training.pt'\n",
      "    test_file = 'test.pt'\n",
      "\n",
      "    def __init__(self, root, train=True, transform=None, target_transform=None, download=False):\n",
      "        self.root = os.path.expanduser(root)\n",
      "        self.transform = transform\n",
      "        self.target_transform = target_transform\n",
      "        self.train = train  # training set or test set\n",
      "\n",
      "        if download:\n",
      "            self.download()\n",
      "\n",
      "        if not self._check_exists():\n",
      "            raise RuntimeError('Dataset not found.' +\n",
      "                               ' You can use download=True to download it')\n",
      "\n",
      "        if self.train:\n",
      "            self.train_data, self.train_labels = torch.load(\n",
      "                os.path.join(self.root, self.processed_folder, self.training_file))\n",
      "            \n",
      "            self.train_data = np.tile(self.train_data[:, :, :, np.newaxis], 3)\n",
      "        else:\n",
      "            self.test_data, self.test_labels = torch.load(\n",
      "                os.path.join(self.root, self.processed_folder, self.test_file))\n",
      "            \n",
      "            self.test_data = np.tile(self.test_data[:, :, :, np.newaxis], 3)\n",
      "        \n",
      "        self.pallette = [[31, 119, 180],\n",
      "                         [255, 127, 14],\n",
      "                         [44, 160, 44],\n",
      "                         [214, 39, 40],\n",
      "                         [148, 103, 189],\n",
      "                         [140, 86, 75],\n",
      "                         [227, 119, 194],\n",
      "                         [127, 127, 127],\n",
      "                         [188, 189, 34],\n",
      "                         [23, 190, 207]]\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        \"\"\"\n",
      "        Args:\n",
      "            index (int): Index\n",
      "\n",
      "        Returns:\n",
      "            tuple: (image, target) where target is index of the target class.\n",
      "        \"\"\"\n",
      "        if self.train:\n",
      "            img, target = self.train_data[index].copy(), self.train_labels[index]\n",
      "        else:\n",
      "            img, target = self.test_data[index].copy(), self.test_labels[index]\n",
      "        \n",
      "        # doing this so that it is consistent with all other datasets\n",
      "        # to return a PIL Image\n",
      "        y_color_digit = np.random.randint(0, args.y_color)\n",
      "        c_digit = self.pallette[y_color_digit]\n",
      "        \n",
      "        img[:, :, 0] = img[:, :, 0] / 255 * c_digit[0]\n",
      "        img[:, :, 1] = img[:, :, 1] / 255 * c_digit[1]\n",
      "        img[:, :, 2] = img[:, :, 2] / 255 * c_digit[2]\n",
      "        \n",
      "        img = Image.fromarray(img)\n",
      "\n",
      "        if self.transform is not None:\n",
      "            img = self.transform(img)\n",
      "\n",
      "        if self.target_transform is not None:\n",
      "            target = self.target_transform(target)\n",
      "\n",
      "        return img, [target,torch.from_numpy(np.array(y_color_digit))]\n",
      "\n",
      "    def __len__(self):\n",
      "        if self.train:\n",
      "            return len(self.train_data)\n",
      "        else:\n",
      "            return len(self.test_data)\n",
      "\n",
      "    def _check_exists(self):\n",
      "        return os.path.exists(os.path.join(self.root, self.processed_folder, self.training_file)) and \\\n",
      "            os.path.exists(os.path.join(self.root, self.processed_folder, self.test_file))\n",
      "\n",
      "    def download(self):\n",
      "        \"\"\"Download the MNIST data if it doesn't exist in processed_folder already.\"\"\"\n",
      "        from six.moves import urllib\n",
      "        import gzip\n",
      "\n",
      "        if self._check_exists():\n",
      "            return\n",
      "\n",
      "        # download files\n",
      "        try:\n",
      "            os.makedirs(os.path.join(self.root, self.raw_folder))\n",
      "            os.makedirs(os.path.join(self.root, self.processed_folder))\n",
      "        except OSError as e:\n",
      "            if e.errno == errno.EEXIST:\n",
      "                pass\n",
      "            else:\n",
      "                raise\n",
      "\n",
      "        for url in self.urls:\n",
      "            print('Downloading ' + url)\n",
      "            data = urllib.request.urlopen(url)\n",
      "            filename = url.rpartition('/')[2]\n",
      "            file_path = os.path.join(self.root, self.raw_folder, filename)\n",
      "            with open(file_path, 'wb') as f:\n",
      "                f.write(data.read())\n",
      "            with open(file_path.replace('.gz', ''), 'wb') as out_f, \\\n",
      "                    gzip.GzipFile(file_path) as zip_f:\n",
      "                out_f.write(zip_f.read())\n",
      "            os.unlink(file_path)\n",
      "\n",
      "        # process and save as torch files\n",
      "        print('Processing...')\n",
      "\n",
      "        training_set = (\n",
      "            read_image_file(os.path.join(self.root, self.raw_folder, 'train-images-idx3-ubyte')),\n",
      "            read_label_file(os.path.join(self.root, self.raw_folder, 'train-labels-idx1-ubyte'))\n",
      "        )\n",
      "        test_set = (\n",
      "            read_image_file(os.path.join(self.root, self.raw_folder, 't10k-images-idx3-ubyte')),\n",
      "            read_label_file(os.path.join(self.root, self.raw_folder, 't10k-labels-idx1-ubyte'))\n",
      "        )\n",
      "        with open(os.path.join(self.root, self.processed_folder, self.training_file), 'wb') as f:\n",
      "            torch.save(training_set, f)\n",
      "        with open(os.path.join(self.root, self.processed_folder, self.test_file), 'wb') as f:\n",
      "            torch.save(test_set, f)\n",
      "\n",
      "        print('Done!')\n",
      "\n",
      "    def __repr__(self):\n",
      "        fmt_str = 'Dataset ' + self.__class__.__name__ + '\\n'\n",
      "        fmt_str += '    Number of datapoints: {}\\n'.format(self.__len__())\n",
      "        tmp = 'train' if self.train is True else 'test'\n",
      "        fmt_str += '    Split: {}\\n'.format(tmp)\n",
      "        fmt_str += '    Root Location: {}\\n'.format(self.root)\n",
      "        tmp = '    Transforms (if any): '\n",
      "        fmt_str += '{0}{1}\\n'.format(tmp, self.transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
      "        tmp = '    Target Transforms (if any): '\n",
      "        fmt_str += '{0}{1}'.format(tmp, self.target_transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
      "        return fmt_str\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "        \n",
      "def update_scale_std(model, epoch):\n",
      "    epoch_frac = 1.0 - float(epoch - 1) / max(args.num_epochs + 1, 1)\n",
      "    scale_std = args.scale_std * epoch_frac\n",
      "    model.set_scale_std(scale_std)\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"colormnist\":\n",
      "        im_dim = 3\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = ColorMNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = ColorMNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol, \"scale\": args.scale, \"scale_fac\": args.scale_fac, \"scale_std\": args.scale_std, \"gate\": args.gate},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,\n",
      "            y_class = args.y_class)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        if args.annealing_std:\n",
      "            update_scale_std(model.module, epoch)\n",
      "            \n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            if args.data == \"colormnist\":\n",
      "                y = y[0]\n",
      "            \n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            # re-weight the gate rewards\n",
      "            normalized_eta = args.eta / len(logp_actions)\n",
      "            \n",
      "            # collect cumulative future rewards\n",
      "            R = - loss\n",
      "            cum_rewards = []\n",
      "            for r in nfe[::-1]:\n",
      "                R = -normalized_eta * r.view(-1,1) + args.gamma * R\n",
      "                cum_rewards.insert(0,R)\n",
      "            \n",
      "            # apply REINFORCE\n",
      "            rl_loss = 0\n",
      "            for lpa, r in zip(logp_actions, cum_rewards):\n",
      "                rl_loss = rl_loss - lpa.view(-1,1) * args.rl_weight * r\n",
      "                \n",
      "            loss = loss + rl_loss.mean()\n",
      "            \n",
      "            loss.backward()\n",
      "            \n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe_gate(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            for idx in range(len(model.module.transforms)):\n",
      "                for layer in model.module.transforms[idx].chain:\n",
      "                    if hasattr(layer, 'atol'):\n",
      "                        layer.odefunc.after_odeint()\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'train': atol[tol_indx].mean()}, itr)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'train': rtol[tol_indx].mean()}, itr)\n",
      "                    \n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        if args.data == \"colormnist\":\n",
      "            # print train images\n",
      "            xall = []\n",
      "            ximg = x[0:40].cpu().numpy().transpose((0,2,3,1))\n",
      "            for i in range(ximg.shape[0]):\n",
      "                xall.append(ximg[i])\n",
      "        \n",
      "            xall = np.hstack(xall)\n",
      "\n",
      "            plt.imshow(xall)\n",
      "            plt.axis('off')\n",
      "            plt.show()\n",
      "            \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if args.data == \"colormnist\":\n",
      "                        y = y[0]\n",
      "                        \n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                if args.data == \"colormnist\":\n",
      "                    # print test images\n",
      "                    xall = []\n",
      "                    ximg = x[0:40].cpu().numpy().transpose((0,2,3,1))\n",
      "                    for i in range(ximg.shape[0]):\n",
      "                        xall.append(ximg[i])\n",
      "\n",
      "                    xall = np.hstack(xall)\n",
      "\n",
      "                    plt.imshow(xall)\n",
      "                    plt.axis('off')\n",
      "                    plt.show()\n",
      "                    \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples, atol, rtol, logp_actions, nfe = model(fixed_z, reverse=True)\n",
      "            generated_samples = generated_samples.view(-1, *data_shape)\n",
      "            for tol_indx in range(len(atol)):\n",
      "                writer.add_scalars('atol_gen_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                writer.add_scalars('rtol_gen_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, annealing_std=False, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, condition_ratio=0.5, conditional=True, controlled_tol=False, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, eta=0.1, gamma=0.99, gate='cnn1', imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.0001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=500, parallel=False, rademacher=True, residual=False, resume='../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_6_run2/current_checkpt.pth', rl_weight=0.01, rtol=1e-05, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_6_run2', scale=1.0, scale_fac=1.0, scale_std=6.0, seed=2, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5, y_class=10, y_color=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1450732\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc_rl.py:291: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Iter 14470 | Time 24.4912(25.5482) | Bit/dim 3.5891(3.5751) | Xent 0.1095(0.1239) | Loss 9.2125(10.4980) | Error 0.0389(0.0422) Steps 958(987.04) | Grad Norm 2.5659(3.2134) | Total Time 0.00(0.00)\n",
      "Iter 14480 | Time 24.6114(25.2662) | Bit/dim 3.5444(3.5730) | Xent 0.1193(0.1221) | Loss 9.3105(10.1927) | Error 0.0356(0.0415) Steps 1018(985.66) | Grad Norm 2.4702(3.1401) | Total Time 0.00(0.00)\n",
      "Iter 14490 | Time 25.3150(25.1640) | Bit/dim 3.5722(3.5709) | Xent 0.1137(0.1208) | Loss 9.2584(9.9606) | Error 0.0422(0.0410) Steps 1018(985.17) | Grad Norm 3.7026(3.1017) | Total Time 0.00(0.00)\n",
      "Iter 14500 | Time 24.8908(25.0336) | Bit/dim 3.6182(3.5758) | Xent 0.1104(0.1213) | Loss 9.5590(9.8072) | Error 0.0411(0.0416) Steps 1042(983.24) | Grad Norm 2.7017(3.0934) | Total Time 0.00(0.00)\n",
      "Iter 14510 | Time 25.0037(24.9596) | Bit/dim 3.5671(3.5758) | Xent 0.1169(0.1208) | Loss 9.3798(9.6801) | Error 0.0400(0.0420) Steps 1006(985.62) | Grad Norm 2.8503(3.2164) | Total Time 0.00(0.00)\n",
      "Iter 14520 | Time 24.8314(24.9758) | Bit/dim 3.5412(3.5740) | Xent 0.1110(0.1203) | Loss 9.2151(9.5779) | Error 0.0333(0.0416) Steps 1024(987.74) | Grad Norm 2.8669(3.2962) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0264 | Time 116.5958, Epoch Time 1521.2573(1384.7911), Bit/dim 3.5858(best: inf), Xent 0.9451, Loss 4.0583, Error 0.2230(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14530 | Time 25.8171(24.9689) | Bit/dim 3.5542(3.5707) | Xent 0.1122(0.1190) | Loss 9.2135(10.3383) | Error 0.0356(0.0413) Steps 1048(989.48) | Grad Norm 4.3909(3.3130) | Total Time 0.00(0.00)\n",
      "Iter 14540 | Time 25.3717(25.0035) | Bit/dim 3.5942(3.5727) | Xent 0.1360(0.1191) | Loss 9.3866(10.0938) | Error 0.0467(0.0409) Steps 1024(990.89) | Grad Norm 3.7731(3.3689) | Total Time 0.00(0.00)\n",
      "Iter 14550 | Time 23.1759(24.8596) | Bit/dim 3.5637(3.5749) | Xent 0.0977(0.1194) | Loss 9.1401(9.8927) | Error 0.0278(0.0410) Steps 958(989.92) | Grad Norm 3.0048(3.5015) | Total Time 0.00(0.00)\n",
      "Iter 14560 | Time 25.1041(24.8429) | Bit/dim 3.5703(3.5744) | Xent 0.1243(0.1194) | Loss 9.3945(9.7473) | Error 0.0400(0.0404) Steps 1030(989.87) | Grad Norm 3.1270(3.3922) | Total Time 0.00(0.00)\n",
      "Iter 14570 | Time 24.9809(24.8426) | Bit/dim 3.5484(3.5727) | Xent 0.1265(0.1192) | Loss 9.2506(9.6415) | Error 0.0367(0.0402) Steps 988(990.60) | Grad Norm 3.7714(3.3030) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0265 | Time 114.1036, Epoch Time 1499.0795(1388.2197), Bit/dim 3.5794(best: 3.5858), Xent 0.9489, Loss 4.0538, Error 0.2219(best: 0.2230)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14580 | Time 25.6908(24.8273) | Bit/dim 3.5292(3.5726) | Xent 0.1382(0.1183) | Loss 9.2368(10.5542) | Error 0.0533(0.0406) Steps 958(988.79) | Grad Norm 3.0700(3.1975) | Total Time 0.00(0.00)\n",
      "Iter 14590 | Time 25.2698(24.8108) | Bit/dim 3.5516(3.5720) | Xent 0.1203(0.1188) | Loss 9.2815(10.2296) | Error 0.0444(0.0406) Steps 1036(989.73) | Grad Norm 2.9865(3.1709) | Total Time 0.00(0.00)\n",
      "Iter 14600 | Time 24.0484(24.9390) | Bit/dim 3.5713(3.5730) | Xent 0.1257(0.1179) | Loss 9.2564(9.9985) | Error 0.0467(0.0404) Steps 970(990.56) | Grad Norm 3.9349(3.2523) | Total Time 0.00(0.00)\n",
      "Iter 14610 | Time 25.1085(24.8552) | Bit/dim 3.5806(3.5721) | Xent 0.1142(0.1172) | Loss 9.5593(9.8198) | Error 0.0422(0.0405) Steps 964(987.60) | Grad Norm 2.6806(3.2945) | Total Time 0.00(0.00)\n",
      "Iter 14620 | Time 24.9437(24.8821) | Bit/dim 3.5716(3.5701) | Xent 0.1041(0.1159) | Loss 9.3371(9.6996) | Error 0.0378(0.0401) Steps 1012(991.24) | Grad Norm 3.0390(3.3315) | Total Time 0.00(0.00)\n",
      "Iter 14630 | Time 23.3277(24.8549) | Bit/dim 3.5529(3.5706) | Xent 0.1451(0.1197) | Loss 9.2657(9.6079) | Error 0.0533(0.0417) Steps 970(992.22) | Grad Norm 4.3086(3.3953) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0266 | Time 114.7475, Epoch Time 1500.7229(1391.5948), Bit/dim 3.5801(best: 3.5794), Xent 0.9397, Loss 4.0500, Error 0.2209(best: 0.2219)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14640 | Time 23.6515(24.8691) | Bit/dim 3.5449(3.5727) | Xent 0.1058(0.1182) | Loss 9.3169(10.3998) | Error 0.0367(0.0406) Steps 970(993.36) | Grad Norm 2.9187(3.4583) | Total Time 0.00(0.00)\n",
      "Iter 14650 | Time 25.4618(24.9851) | Bit/dim 3.5445(3.5718) | Xent 0.1170(0.1178) | Loss 9.4596(10.1225) | Error 0.0378(0.0405) Steps 976(992.95) | Grad Norm 3.0727(3.4850) | Total Time 0.00(0.00)\n",
      "Iter 14660 | Time 25.7933(24.9347) | Bit/dim 3.5965(3.5724) | Xent 0.0960(0.1159) | Loss 9.3745(9.9153) | Error 0.0356(0.0401) Steps 982(988.91) | Grad Norm 3.2437(3.4655) | Total Time 0.00(0.00)\n",
      "Iter 14670 | Time 24.6547(24.9860) | Bit/dim 3.5926(3.5712) | Xent 0.1349(0.1167) | Loss 9.4337(9.7642) | Error 0.0489(0.0401) Steps 976(991.69) | Grad Norm 4.3294(3.4134) | Total Time 0.00(0.00)\n",
      "Iter 14680 | Time 24.4310(24.9032) | Bit/dim 3.5898(3.5754) | Xent 0.1171(0.1185) | Loss 9.2869(9.6529) | Error 0.0478(0.0406) Steps 958(990.54) | Grad Norm 2.6822(3.3176) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0267 | Time 113.4022, Epoch Time 1505.7305(1395.0189), Bit/dim 3.5790(best: 3.5794), Xent 0.9404, Loss 4.0492, Error 0.2220(best: 0.2209)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14690 | Time 24.6873(24.9070) | Bit/dim 3.5642(3.5757) | Xent 0.1382(0.1201) | Loss 9.3571(10.5851) | Error 0.0511(0.0411) Steps 1006(991.96) | Grad Norm 3.5587(3.3032) | Total Time 0.00(0.00)\n",
      "Iter 14700 | Time 25.5218(24.9384) | Bit/dim 3.5643(3.5752) | Xent 0.0968(0.1179) | Loss 9.4220(10.2678) | Error 0.0300(0.0401) Steps 1006(991.61) | Grad Norm 4.3732(3.3801) | Total Time 0.00(0.00)\n",
      "Iter 14710 | Time 24.4000(24.9461) | Bit/dim 3.5803(3.5750) | Xent 0.1106(0.1168) | Loss 9.2231(10.0265) | Error 0.0422(0.0396) Steps 1012(994.05) | Grad Norm 2.6627(3.4003) | Total Time 0.00(0.00)\n",
      "Iter 14720 | Time 25.2874(24.9718) | Bit/dim 3.5605(3.5749) | Xent 0.1142(0.1181) | Loss 9.3892(9.8510) | Error 0.0389(0.0405) Steps 1012(993.42) | Grad Norm 3.3369(3.3814) | Total Time 0.00(0.00)\n",
      "Iter 14730 | Time 24.2157(24.9430) | Bit/dim 3.5495(3.5701) | Xent 0.0972(0.1142) | Loss 9.2762(9.6996) | Error 0.0356(0.0395) Steps 988(992.53) | Grad Norm 3.2095(3.2821) | Total Time 0.00(0.00)\n",
      "Iter 14740 | Time 24.3568(24.9786) | Bit/dim 3.5682(3.5712) | Xent 0.1013(0.1168) | Loss 9.4270(9.6118) | Error 0.0389(0.0406) Steps 1012(992.33) | Grad Norm 2.6831(3.2968) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0268 | Time 112.9338, Epoch Time 1504.5775(1398.3057), Bit/dim 3.5838(best: 3.5790), Xent 0.9623, Loss 4.0649, Error 0.2214(best: 0.2209)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14750 | Time 25.2399(24.9348) | Bit/dim 3.5932(3.5724) | Xent 0.1066(0.1144) | Loss 9.3827(10.3930) | Error 0.0333(0.0397) Steps 1012(990.57) | Grad Norm 2.9905(3.2484) | Total Time 0.00(0.00)\n",
      "Iter 14760 | Time 25.1292(24.9307) | Bit/dim 3.5749(3.5727) | Xent 0.1182(0.1144) | Loss 9.3104(10.1227) | Error 0.0367(0.0395) Steps 988(994.18) | Grad Norm 2.9040(3.2107) | Total Time 0.00(0.00)\n",
      "Iter 14770 | Time 24.4185(24.8690) | Bit/dim 3.5703(3.5715) | Xent 0.1026(0.1148) | Loss 9.2720(9.9198) | Error 0.0367(0.0396) Steps 1000(994.43) | Grad Norm 5.0382(3.2110) | Total Time 0.00(0.00)\n",
      "Iter 14780 | Time 25.4071(24.9176) | Bit/dim 3.5912(3.5724) | Xent 0.1315(0.1163) | Loss 9.3488(9.7695) | Error 0.0467(0.0402) Steps 976(994.58) | Grad Norm 3.2739(3.2560) | Total Time 0.00(0.00)\n",
      "Iter 14790 | Time 24.3526(24.8704) | Bit/dim 3.5389(3.5713) | Xent 0.1211(0.1151) | Loss 9.2427(9.6414) | Error 0.0444(0.0396) Steps 982(993.58) | Grad Norm 3.6398(3.2152) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0269 | Time 114.5862, Epoch Time 1501.4904(1401.4012), Bit/dim 3.5802(best: 3.5790), Xent 0.9696, Loss 4.0650, Error 0.2235(best: 0.2209)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14800 | Time 25.0337(24.9459) | Bit/dim 3.5314(3.5713) | Xent 0.1193(0.1133) | Loss 9.3937(10.5637) | Error 0.0456(0.0392) Steps 1018(994.78) | Grad Norm 3.9633(3.1723) | Total Time 0.00(0.00)\n",
      "Iter 14810 | Time 24.4915(24.9292) | Bit/dim 3.5792(3.5720) | Xent 0.1023(0.1131) | Loss 9.3688(10.2418) | Error 0.0367(0.0395) Steps 982(989.49) | Grad Norm 4.4057(3.1774) | Total Time 0.00(0.00)\n",
      "Iter 14820 | Time 24.9550(24.8433) | Bit/dim 3.5818(3.5725) | Xent 0.1478(0.1145) | Loss 9.3342(10.0039) | Error 0.0456(0.0397) Steps 994(986.26) | Grad Norm 2.6019(3.1765) | Total Time 0.00(0.00)\n",
      "Iter 14830 | Time 25.4363(24.9519) | Bit/dim 3.6013(3.5733) | Xent 0.1074(0.1138) | Loss 9.4045(9.8263) | Error 0.0344(0.0391) Steps 1042(988.23) | Grad Norm 3.4648(3.2382) | Total Time 0.00(0.00)\n",
      "Iter 14840 | Time 24.8194(24.9065) | Bit/dim 3.5379(3.5707) | Xent 0.0984(0.1134) | Loss 9.1440(9.6923) | Error 0.0344(0.0387) Steps 964(990.37) | Grad Norm 4.8881(3.4107) | Total Time 0.00(0.00)\n",
      "Iter 14850 | Time 25.0846(24.8609) | Bit/dim 3.5635(3.5697) | Xent 0.1568(0.1179) | Loss 9.3945(9.5977) | Error 0.0522(0.0404) Steps 982(988.22) | Grad Norm 5.6555(3.7774) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0270 | Time 114.0457, Epoch Time 1499.6879(1404.3498), Bit/dim 3.5780(best: 3.5790), Xent 0.9833, Loss 4.0697, Error 0.2239(best: 0.2209)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14860 | Time 24.2362(24.8093) | Bit/dim 3.5517(3.5705) | Xent 0.1097(0.1153) | Loss 9.1874(10.3747) | Error 0.0344(0.0398) Steps 1018(990.13) | Grad Norm 3.0683(3.7909) | Total Time 0.00(0.00)\n",
      "Iter 14870 | Time 25.2578(24.7310) | Bit/dim 3.5863(3.5728) | Xent 0.1251(0.1158) | Loss 9.3547(10.1077) | Error 0.0422(0.0401) Steps 976(992.01) | Grad Norm 2.6262(3.8581) | Total Time 0.00(0.00)\n",
      "Iter 14880 | Time 25.2875(24.8289) | Bit/dim 3.5531(3.5707) | Xent 0.1346(0.1168) | Loss 9.2460(9.9077) | Error 0.0500(0.0398) Steps 940(991.26) | Grad Norm 5.2494(3.8645) | Total Time 0.00(0.00)\n",
      "Iter 14890 | Time 24.5976(24.8031) | Bit/dim 3.5850(3.5714) | Xent 0.1202(0.1176) | Loss 9.3034(9.7635) | Error 0.0422(0.0409) Steps 964(989.01) | Grad Norm 4.9727(3.8569) | Total Time 0.00(0.00)\n",
      "Iter 14900 | Time 25.0398(24.7710) | Bit/dim 3.5716(3.5701) | Xent 0.0910(0.1163) | Loss 9.2919(9.6429) | Error 0.0322(0.0408) Steps 1000(992.04) | Grad Norm 4.1467(3.7388) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0271 | Time 115.3218, Epoch Time 1495.3988(1407.0813), Bit/dim 3.5816(best: 3.5780), Xent 0.9578, Loss 4.0605, Error 0.2210(best: 0.2209)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14910 | Time 24.3566(24.8460) | Bit/dim 3.6304(3.5704) | Xent 0.0884(0.1158) | Loss 9.4909(10.5414) | Error 0.0244(0.0403) Steps 958(993.04) | Grad Norm 2.6142(3.5618) | Total Time 0.00(0.00)\n",
      "Iter 14920 | Time 25.5900(24.9603) | Bit/dim 3.5779(3.5692) | Xent 0.1024(0.1140) | Loss 9.4130(10.2278) | Error 0.0333(0.0394) Steps 982(990.98) | Grad Norm 2.8696(3.4572) | Total Time 0.00(0.00)\n",
      "Iter 14930 | Time 25.4849(24.9860) | Bit/dim 3.5701(3.5706) | Xent 0.1307(0.1136) | Loss 9.4908(10.0024) | Error 0.0433(0.0395) Steps 994(992.79) | Grad Norm 4.1279(3.3999) | Total Time 0.00(0.00)\n",
      "Iter 14940 | Time 25.4286(24.9767) | Bit/dim 3.5599(3.5699) | Xent 0.1462(0.1155) | Loss 9.3996(9.8333) | Error 0.0489(0.0401) Steps 964(993.19) | Grad Norm 3.1267(3.4426) | Total Time 0.00(0.00)\n",
      "Iter 14950 | Time 24.4903(24.9275) | Bit/dim 3.5710(3.5710) | Xent 0.1072(0.1145) | Loss 9.3250(9.7016) | Error 0.0344(0.0397) Steps 1006(998.17) | Grad Norm 2.8275(3.3372) | Total Time 0.00(0.00)\n",
      "Iter 14960 | Time 25.0310(25.0008) | Bit/dim 3.5684(3.5703) | Xent 0.1466(0.1168) | Loss 9.3689(9.6095) | Error 0.0533(0.0404) Steps 1036(1000.89) | Grad Norm 3.3550(3.3322) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0272 | Time 112.7400, Epoch Time 1508.6559(1410.1285), Bit/dim 3.5791(best: 3.5780), Xent 0.9638, Loss 4.0611, Error 0.2255(best: 0.2209)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14970 | Time 25.1927(25.0202) | Bit/dim 3.5839(3.5716) | Xent 0.1106(0.1157) | Loss 9.4684(10.3994) | Error 0.0389(0.0396) Steps 994(996.75) | Grad Norm 2.8879(3.3767) | Total Time 0.00(0.00)\n",
      "Iter 14980 | Time 25.0144(24.9885) | Bit/dim 3.5483(3.5726) | Xent 0.1076(0.1145) | Loss 9.1468(10.1176) | Error 0.0333(0.0390) Steps 1006(996.65) | Grad Norm 3.0876(3.4093) | Total Time 0.00(0.00)\n",
      "Iter 14990 | Time 25.1396(24.9975) | Bit/dim 3.5620(3.5711) | Xent 0.0903(0.1131) | Loss 9.3857(9.9201) | Error 0.0311(0.0388) Steps 1048(998.91) | Grad Norm 2.6245(3.6088) | Total Time 0.00(0.00)\n",
      "Iter 15000 | Time 24.8759(24.9676) | Bit/dim 3.5692(3.5736) | Xent 0.1102(0.1122) | Loss 9.4306(9.7733) | Error 0.0400(0.0386) Steps 1000(996.04) | Grad Norm 5.3393(3.7730) | Total Time 0.00(0.00)\n",
      "Iter 15010 | Time 23.7744(24.9382) | Bit/dim 3.5564(3.5668) | Xent 0.1133(0.1125) | Loss 9.1494(9.6482) | Error 0.0422(0.0388) Steps 946(993.23) | Grad Norm 3.3173(3.6699) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0273 | Time 114.0922, Epoch Time 1503.1794(1412.9200), Bit/dim 3.5740(best: 3.5780), Xent 0.9888, Loss 4.0683, Error 0.2248(best: 0.2209)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15020 | Time 24.0225(24.8902) | Bit/dim 3.5458(3.5691) | Xent 0.1083(0.1141) | Loss 9.2856(10.5763) | Error 0.0444(0.0398) Steps 958(991.90) | Grad Norm 3.5036(3.6489) | Total Time 0.00(0.00)\n",
      "Iter 15030 | Time 26.1784(24.9237) | Bit/dim 3.5924(3.5693) | Xent 0.1518(0.1132) | Loss 9.3766(10.2503) | Error 0.0478(0.0392) Steps 1006(995.79) | Grad Norm 3.1360(3.5218) | Total Time 0.00(0.00)\n",
      "Iter 15040 | Time 24.2793(24.9026) | Bit/dim 3.5589(3.5685) | Xent 0.1166(0.1143) | Loss 9.3854(10.0161) | Error 0.0411(0.0401) Steps 970(995.11) | Grad Norm 4.1264(3.4226) | Total Time 0.00(0.00)\n",
      "Iter 15050 | Time 25.0657(24.8480) | Bit/dim 3.5617(3.5681) | Xent 0.1112(0.1157) | Loss 9.2715(9.8462) | Error 0.0400(0.0402) Steps 988(992.97) | Grad Norm 3.3330(3.3820) | Total Time 0.00(0.00)\n",
      "Iter 15060 | Time 25.5977(24.9009) | Bit/dim 3.5641(3.5697) | Xent 0.0958(0.1149) | Loss 9.3927(9.7125) | Error 0.0333(0.0397) Steps 1000(993.82) | Grad Norm 3.4899(3.5006) | Total Time 0.00(0.00)\n",
      "Iter 15070 | Time 24.1024(24.8945) | Bit/dim 3.5484(3.5690) | Xent 0.1085(0.1137) | Loss 9.4186(9.6142) | Error 0.0378(0.0395) Steps 1018(994.05) | Grad Norm 3.2665(3.4982) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0274 | Time 113.6979, Epoch Time 1500.9463(1415.5608), Bit/dim 3.5792(best: 3.5740), Xent 0.9837, Loss 4.0710, Error 0.2223(best: 0.2209)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15080 | Time 24.7548(24.7297) | Bit/dim 3.5868(3.5693) | Xent 0.1096(0.1138) | Loss 9.3525(10.3949) | Error 0.0444(0.0397) Steps 988(991.29) | Grad Norm 3.5718(3.5035) | Total Time 0.00(0.00)\n",
      "Iter 15090 | Time 24.9774(24.7695) | Bit/dim 3.5412(3.5654) | Xent 0.1014(0.1115) | Loss 9.2420(10.1009) | Error 0.0356(0.0389) Steps 1000(996.58) | Grad Norm 3.3245(3.5157) | Total Time 0.00(0.00)\n",
      "Iter 15100 | Time 24.9062(24.8537) | Bit/dim 3.5561(3.5675) | Xent 0.1170(0.1121) | Loss 9.3060(9.9149) | Error 0.0344(0.0389) Steps 982(995.93) | Grad Norm 3.1411(3.4665) | Total Time 0.00(0.00)\n",
      "Iter 15110 | Time 25.8537(24.8485) | Bit/dim 3.5929(3.5693) | Xent 0.0972(0.1112) | Loss 9.5163(9.7729) | Error 0.0300(0.0389) Steps 1042(999.27) | Grad Norm 3.3318(3.4314) | Total Time 0.00(0.00)\n",
      "Iter 15120 | Time 24.5840(24.8388) | Bit/dim 3.5744(3.5687) | Xent 0.1017(0.1102) | Loss 9.2057(9.6493) | Error 0.0367(0.0385) Steps 1006(998.38) | Grad Norm 3.7153(3.4569) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0275 | Time 114.4044, Epoch Time 1496.5745(1417.9912), Bit/dim 3.5767(best: 3.5740), Xent 0.9764, Loss 4.0649, Error 0.2231(best: 0.2209)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15130 | Time 24.4434(24.9121) | Bit/dim 3.5780(3.5721) | Xent 0.1085(0.1098) | Loss 9.3577(10.5996) | Error 0.0333(0.0380) Steps 982(996.17) | Grad Norm 3.0145(3.3326) | Total Time 0.00(0.00)\n",
      "Iter 15140 | Time 25.5944(24.9648) | Bit/dim 3.5577(3.5727) | Xent 0.1114(0.1116) | Loss 9.3614(10.2682) | Error 0.0400(0.0384) Steps 1012(991.78) | Grad Norm 4.1497(3.3725) | Total Time 0.00(0.00)\n",
      "Iter 15150 | Time 25.7393(25.0432) | Bit/dim 3.5798(3.5709) | Xent 0.1062(0.1096) | Loss 9.4049(10.0291) | Error 0.0333(0.0376) Steps 1054(994.32) | Grad Norm 3.3017(3.3462) | Total Time 0.00(0.00)\n",
      "Iter 15160 | Time 24.3409(25.0564) | Bit/dim 3.5886(3.5709) | Xent 0.1296(0.1098) | Loss 9.2798(9.8367) | Error 0.0422(0.0381) Steps 994(995.08) | Grad Norm 3.5100(3.4230) | Total Time 0.00(0.00)\n",
      "Iter 15170 | Time 25.9957(25.1265) | Bit/dim 3.5549(3.5682) | Xent 0.1126(0.1111) | Loss 9.3563(9.6959) | Error 0.0389(0.0384) Steps 1036(995.93) | Grad Norm 3.0803(3.5397) | Total Time 0.00(0.00)\n",
      "Iter 15180 | Time 24.7477(25.1498) | Bit/dim 3.5977(3.5674) | Xent 0.1083(0.1092) | Loss 9.3995(9.5985) | Error 0.0333(0.0373) Steps 946(996.46) | Grad Norm 3.2966(3.5282) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0276 | Time 115.1559, Epoch Time 1518.9538(1421.0201), Bit/dim 3.5801(best: 3.5740), Xent 1.0081, Loss 4.0841, Error 0.2250(best: 0.2209)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15190 | Time 24.4557(25.1526) | Bit/dim 3.6003(3.5681) | Xent 0.0851(0.1095) | Loss 9.3181(10.3728) | Error 0.0278(0.0375) Steps 964(999.29) | Grad Norm 3.1960(3.4783) | Total Time 0.00(0.00)\n",
      "Iter 15200 | Time 25.5415(25.1740) | Bit/dim 3.5697(3.5678) | Xent 0.1159(0.1086) | Loss 9.4541(10.1016) | Error 0.0389(0.0372) Steps 1006(995.78) | Grad Norm 3.5856(3.4300) | Total Time 0.00(0.00)\n",
      "Iter 15210 | Time 25.7219(25.2199) | Bit/dim 3.5570(3.5689) | Xent 0.1073(0.1081) | Loss 9.3496(9.8987) | Error 0.0400(0.0371) Steps 982(997.77) | Grad Norm 2.7070(3.3595) | Total Time 0.00(0.00)\n",
      "Iter 15220 | Time 25.4529(25.2224) | Bit/dim 3.5570(3.5671) | Xent 0.1097(0.1081) | Loss 9.1685(9.7494) | Error 0.0411(0.0364) Steps 1012(999.05) | Grad Norm 3.7549(3.5722) | Total Time 0.00(0.00)\n",
      "Iter 15230 | Time 23.8909(25.1533) | Bit/dim 3.5857(3.5692) | Xent 0.1160(0.1091) | Loss 9.3173(9.6374) | Error 0.0433(0.0371) Steps 970(998.28) | Grad Norm 2.8296(3.7110) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0277 | Time 114.3351, Epoch Time 1516.2036(1423.8756), Bit/dim 3.5809(best: 3.5740), Xent 1.0017, Loss 4.0817, Error 0.2236(best: 0.2209)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15240 | Time 24.4625(25.0388) | Bit/dim 3.5820(3.5697) | Xent 0.1088(0.1113) | Loss 9.3881(10.5535) | Error 0.0422(0.0384) Steps 994(999.93) | Grad Norm 4.8651(3.8052) | Total Time 0.00(0.00)\n",
      "Iter 15250 | Time 23.5029(25.0096) | Bit/dim 3.5857(3.5703) | Xent 0.0824(0.1104) | Loss 9.3035(10.2334) | Error 0.0256(0.0382) Steps 970(997.06) | Grad Norm 3.8576(3.6800) | Total Time 0.00(0.00)\n",
      "Iter 15260 | Time 25.6080(24.9970) | Bit/dim 3.5685(3.5690) | Xent 0.1029(0.1101) | Loss 9.4205(9.9873) | Error 0.0344(0.0377) Steps 976(996.04) | Grad Norm 3.2213(3.5711) | Total Time 0.00(0.00)\n",
      "Iter 15270 | Time 25.3583(25.1056) | Bit/dim 3.5689(3.5701) | Xent 0.0979(0.1101) | Loss 9.3748(9.8245) | Error 0.0378(0.0381) Steps 1000(998.21) | Grad Norm 2.5546(3.4426) | Total Time 0.00(0.00)\n",
      "Iter 15280 | Time 24.8900(25.1403) | Bit/dim 3.5425(3.5700) | Xent 0.1142(0.1107) | Loss 9.2989(9.7027) | Error 0.0433(0.0383) Steps 1006(1001.60) | Grad Norm 3.3166(3.4345) | Total Time 0.00(0.00)\n",
      "Iter 15290 | Time 25.3703(25.1476) | Bit/dim 3.5714(3.5693) | Xent 0.1345(0.1120) | Loss 9.4120(9.6081) | Error 0.0478(0.0384) Steps 1066(1000.89) | Grad Norm 4.1918(3.5189) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0278 | Time 115.8561, Epoch Time 1514.4358(1426.5924), Bit/dim 3.5814(best: 3.5740), Xent 0.9974, Loss 4.0801, Error 0.2265(best: 0.2209)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15300 | Time 24.7789(25.1210) | Bit/dim 3.5681(3.5718) | Xent 0.1232(0.1103) | Loss 9.2779(10.3836) | Error 0.0444(0.0381) Steps 952(998.38) | Grad Norm 3.6443(3.4641) | Total Time 0.00(0.00)\n",
      "Iter 15310 | Time 24.8981(25.0511) | Bit/dim 3.5786(3.5702) | Xent 0.1204(0.1110) | Loss 9.3256(10.1166) | Error 0.0378(0.0388) Steps 970(999.31) | Grad Norm 3.5308(3.3891) | Total Time 0.00(0.00)\n",
      "Iter 15320 | Time 24.3604(24.9104) | Bit/dim 3.5898(3.5668) | Xent 0.1018(0.1115) | Loss 9.4675(9.9198) | Error 0.0311(0.0391) Steps 1006(999.33) | Grad Norm 3.2655(3.3514) | Total Time 0.00(0.00)\n",
      "Iter 15330 | Time 25.1981(24.9903) | Bit/dim 3.5909(3.5707) | Xent 0.0889(0.1091) | Loss 9.3085(9.7680) | Error 0.0289(0.0377) Steps 964(999.12) | Grad Norm 3.2096(3.3508) | Total Time 0.00(0.00)\n",
      "Iter 15340 | Time 25.1615(24.9808) | Bit/dim 3.5772(3.5691) | Xent 0.1348(0.1099) | Loss 9.3943(9.6545) | Error 0.0522(0.0381) Steps 970(997.54) | Grad Norm 5.8578(3.4225) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0279 | Time 114.5319, Epoch Time 1503.1283(1428.8885), Bit/dim 3.5726(best: 3.5740), Xent 0.9857, Loss 4.0655, Error 0.2243(best: 0.2209)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15350 | Time 24.8310(24.9585) | Bit/dim 3.5889(3.5658) | Xent 0.1138(0.1104) | Loss 9.4593(10.5633) | Error 0.0400(0.0380) Steps 976(993.78) | Grad Norm 4.5560(3.4558) | Total Time 0.00(0.00)\n",
      "Iter 15360 | Time 24.5620(24.9196) | Bit/dim 3.5708(3.5671) | Xent 0.1192(0.1125) | Loss 9.3069(10.2513) | Error 0.0411(0.0388) Steps 1006(994.42) | Grad Norm 4.0673(3.6220) | Total Time 0.00(0.00)\n",
      "Iter 15370 | Time 23.8237(24.9140) | Bit/dim 3.5595(3.5677) | Xent 0.0869(0.1118) | Loss 9.0487(10.0101) | Error 0.0311(0.0381) Steps 976(996.39) | Grad Norm 4.0305(3.7012) | Total Time 0.00(0.00)\n",
      "Iter 15380 | Time 25.0570(24.9742) | Bit/dim 3.5468(3.5684) | Xent 0.1442(0.1135) | Loss 9.2832(9.8384) | Error 0.0500(0.0387) Steps 1036(996.91) | Grad Norm 5.2843(3.8574) | Total Time 0.00(0.00)\n",
      "Iter 15390 | Time 25.4467(25.0498) | Bit/dim 3.5271(3.5662) | Xent 0.1317(0.1125) | Loss 9.3177(9.6959) | Error 0.0478(0.0383) Steps 1012(996.61) | Grad Norm 3.0258(3.7448) | Total Time 0.00(0.00)\n",
      "Iter 15400 | Time 26.2629(25.1458) | Bit/dim 3.5617(3.5679) | Xent 0.1189(0.1127) | Loss 9.3219(9.6122) | Error 0.0378(0.0385) Steps 1006(996.12) | Grad Norm 3.9297(3.7730) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0280 | Time 113.8451, Epoch Time 1511.6210(1431.3705), Bit/dim 3.5757(best: 3.5726), Xent 1.0132, Loss 4.0823, Error 0.2248(best: 0.2209)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15410 | Time 26.0513(25.1478) | Bit/dim 3.5816(3.5683) | Xent 0.0845(0.1110) | Loss 9.3656(10.3788) | Error 0.0311(0.0386) Steps 1042(998.87) | Grad Norm 3.3367(3.7231) | Total Time 0.00(0.00)\n",
      "Iter 15420 | Time 25.2901(25.1301) | Bit/dim 3.5680(3.5675) | Xent 0.1078(0.1118) | Loss 9.4302(10.1055) | Error 0.0378(0.0386) Steps 970(1000.13) | Grad Norm 3.3802(3.7798) | Total Time 0.00(0.00)\n",
      "Iter 15430 | Time 24.9767(25.0394) | Bit/dim 3.5799(3.5662) | Xent 0.1268(0.1110) | Loss 9.3825(9.8971) | Error 0.0456(0.0385) Steps 1012(1000.66) | Grad Norm 3.3080(3.7002) | Total Time 0.00(0.00)\n",
      "Iter 15440 | Time 24.1655(24.9710) | Bit/dim 3.5759(3.5660) | Xent 0.1185(0.1107) | Loss 9.4746(9.7538) | Error 0.0367(0.0383) Steps 1018(1000.65) | Grad Norm 5.0275(3.9126) | Total Time 0.00(0.00)\n",
      "Iter 15450 | Time 25.6727(25.0047) | Bit/dim 3.5852(3.5691) | Xent 0.0840(0.1113) | Loss 9.3577(9.6469) | Error 0.0278(0.0382) Steps 982(997.60) | Grad Norm 2.7987(4.1167) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0281 | Time 114.3161, Epoch Time 1506.2845(1433.6179), Bit/dim 3.5730(best: 3.5726), Xent 1.0182, Loss 4.0821, Error 0.2317(best: 0.2209)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15460 | Time 24.3688(24.9803) | Bit/dim 3.5208(3.5666) | Xent 0.1103(0.1118) | Loss 9.1510(10.5954) | Error 0.0444(0.0388) Steps 1000(997.03) | Grad Norm 4.6311(4.2518) | Total Time 0.00(0.00)\n",
      "Iter 15470 | Time 25.7295(25.0099) | Bit/dim 3.5590(3.5664) | Xent 0.0903(0.1101) | Loss 9.2690(10.2662) | Error 0.0222(0.0382) Steps 952(999.29) | Grad Norm 3.1535(4.1651) | Total Time 0.00(0.00)\n",
      "Iter 15480 | Time 24.2855(24.9862) | Bit/dim 3.5560(3.5682) | Xent 0.1036(0.1102) | Loss 9.2304(10.0217) | Error 0.0389(0.0381) Steps 1006(1000.35) | Grad Norm 3.0513(4.2791) | Total Time 0.00(0.00)\n",
      "Iter 15490 | Time 25.4284(24.9793) | Bit/dim 3.5604(3.5659) | Xent 0.1372(0.1101) | Loss 9.3735(9.8424) | Error 0.0400(0.0380) Steps 1024(1003.97) | Grad Norm 5.4798(4.1755) | Total Time 0.00(0.00)\n",
      "Iter 15500 | Time 25.0007(24.9488) | Bit/dim 3.5658(3.5661) | Xent 0.0940(0.1106) | Loss 9.3106(9.7066) | Error 0.0333(0.0384) Steps 988(1003.82) | Grad Norm 3.3216(4.0363) | Total Time 0.00(0.00)\n",
      "Iter 15510 | Time 25.9193(25.0078) | Bit/dim 3.5821(3.5679) | Xent 0.1263(0.1112) | Loss 9.4333(9.6125) | Error 0.0478(0.0388) Steps 1000(1001.71) | Grad Norm 3.6381(3.8617) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0282 | Time 114.0401, Epoch Time 1506.9480(1435.8178), Bit/dim 3.5775(best: 3.5726), Xent 1.0128, Loss 4.0839, Error 0.2240(best: 0.2209)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15520 | Time 25.3652(25.0448) | Bit/dim 3.5965(3.5706) | Xent 0.1075(0.1088) | Loss 9.5184(10.4135) | Error 0.0311(0.0377) Steps 1036(1005.31) | Grad Norm 3.2212(3.7505) | Total Time 0.00(0.00)\n",
      "Iter 15530 | Time 24.7252(25.0045) | Bit/dim 3.5765(3.5704) | Xent 0.1085(0.1098) | Loss 9.2544(10.1245) | Error 0.0422(0.0386) Steps 1024(1002.04) | Grad Norm 4.6916(3.8445) | Total Time 0.00(0.00)\n",
      "Iter 15540 | Time 24.5387(25.0483) | Bit/dim 3.5437(3.5673) | Xent 0.1110(0.1116) | Loss 9.3605(9.9203) | Error 0.0300(0.0384) Steps 994(1005.72) | Grad Norm 3.1799(3.9605) | Total Time 0.00(0.00)\n",
      "Iter 15550 | Time 24.8355(25.0142) | Bit/dim 3.5937(3.5672) | Xent 0.1095(0.1125) | Loss 9.4237(9.7787) | Error 0.0422(0.0392) Steps 970(1004.70) | Grad Norm 5.1648(4.0454) | Total Time 0.00(0.00)\n",
      "Iter 15560 | Time 26.3897(25.1463) | Bit/dim 3.5571(3.5657) | Xent 0.0852(0.1121) | Loss 9.4078(9.6685) | Error 0.0278(0.0387) Steps 982(1003.65) | Grad Norm 3.5046(3.9139) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0283 | Time 115.7270, Epoch Time 1516.3028(1438.2324), Bit/dim 3.5781(best: 3.5726), Xent 1.0168, Loss 4.0865, Error 0.2266(best: 0.2209)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15570 | Time 24.9311(25.1834) | Bit/dim 3.6040(3.5684) | Xent 0.0994(0.1100) | Loss 9.2647(10.5916) | Error 0.0333(0.0380) Steps 988(1003.64) | Grad Norm 3.3845(3.9389) | Total Time 0.00(0.00)\n",
      "Iter 15580 | Time 25.4916(25.1407) | Bit/dim 3.5561(3.5663) | Xent 0.1152(0.1090) | Loss 9.3444(10.2696) | Error 0.0378(0.0376) Steps 994(1004.49) | Grad Norm 3.8828(3.8234) | Total Time 0.00(0.00)\n",
      "Iter 15590 | Time 24.6960(25.1458) | Bit/dim 3.5735(3.5684) | Xent 0.1032(0.1096) | Loss 9.3852(10.0478) | Error 0.0356(0.0379) Steps 1024(1003.87) | Grad Norm 3.4520(3.9514) | Total Time 0.00(0.00)\n",
      "Iter 15600 | Time 25.2260(25.1698) | Bit/dim 3.5874(3.5661) | Xent 0.0958(0.1093) | Loss 9.3532(9.8589) | Error 0.0344(0.0381) Steps 970(1005.47) | Grad Norm 4.9896(4.0609) | Total Time 0.00(0.00)\n",
      "Iter 15610 | Time 24.4237(25.1702) | Bit/dim 3.5921(3.5659) | Xent 0.0908(0.1085) | Loss 9.4487(9.7341) | Error 0.0311(0.0375) Steps 1000(1007.06) | Grad Norm 3.2558(4.0367) | Total Time 0.00(0.00)\n",
      "Iter 15620 | Time 24.9931(25.1234) | Bit/dim 3.5821(3.5654) | Xent 0.1142(0.1084) | Loss 9.4590(9.6369) | Error 0.0344(0.0371) Steps 1000(1008.00) | Grad Norm 3.2710(3.9866) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0284 | Time 115.2844, Epoch Time 1516.5064(1440.5806), Bit/dim 3.5742(best: 3.5726), Xent 1.0382, Loss 4.0933, Error 0.2253(best: 0.2209)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15630 | Time 25.7697(25.1556) | Bit/dim 3.5831(3.5681) | Xent 0.0984(0.1067) | Loss 9.3966(10.4324) | Error 0.0333(0.0369) Steps 1024(1005.36) | Grad Norm 2.8777(4.0269) | Total Time 0.00(0.00)\n",
      "Iter 15640 | Time 25.1700(25.1613) | Bit/dim 3.5135(3.5657) | Xent 0.0955(0.1079) | Loss 9.1624(10.1403) | Error 0.0378(0.0374) Steps 988(1006.67) | Grad Norm 4.7761(4.1911) | Total Time 0.00(0.00)\n",
      "Iter 15650 | Time 25.1317(25.2479) | Bit/dim 3.5621(3.5681) | Xent 0.1027(0.1076) | Loss 9.3661(9.9435) | Error 0.0356(0.0375) Steps 964(1004.78) | Grad Norm 3.1231(4.2042) | Total Time 0.00(0.00)\n",
      "Iter 15660 | Time 25.4287(25.2999) | Bit/dim 3.5459(3.5690) | Xent 0.1293(0.1085) | Loss 9.3018(9.7999) | Error 0.0467(0.0377) Steps 1006(1008.34) | Grad Norm 4.4966(4.1864) | Total Time 0.00(0.00)\n",
      "Iter 15670 | Time 25.2060(25.2691) | Bit/dim 3.5467(3.5654) | Xent 0.1126(0.1114) | Loss 9.3717(9.6755) | Error 0.0356(0.0388) Steps 1012(1008.05) | Grad Norm 4.3105(4.1125) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0285 | Time 115.7283, Epoch Time 1528.7129(1443.2245), Bit/dim 3.5752(best: 3.5726), Xent 1.0211, Loss 4.0858, Error 0.2248(best: 0.2209)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15680 | Time 25.5972(25.3699) | Bit/dim 3.5353(3.5636) | Xent 0.0988(0.1105) | Loss 9.4851(10.6079) | Error 0.0311(0.0389) Steps 1054(1007.72) | Grad Norm 2.4752(4.0130) | Total Time 0.00(0.00)\n",
      "Iter 15690 | Time 26.2978(25.4822) | Bit/dim 3.5597(3.5656) | Xent 0.0936(0.1091) | Loss 9.3783(10.2817) | Error 0.0278(0.0380) Steps 994(1007.63) | Grad Norm 3.5962(3.9635) | Total Time 0.00(0.00)\n",
      "Iter 15700 | Time 25.1511(25.3918) | Bit/dim 3.5889(3.5668) | Xent 0.0904(0.1073) | Loss 9.3166(10.0349) | Error 0.0333(0.0379) Steps 1030(1009.71) | Grad Norm 3.2115(3.8076) | Total Time 0.00(0.00)\n",
      "Iter 15710 | Time 24.6164(25.2957) | Bit/dim 3.5859(3.5660) | Xent 0.1184(0.1096) | Loss 9.3211(9.8515) | Error 0.0389(0.0389) Steps 1024(1009.61) | Grad Norm 3.4286(3.9067) | Total Time 0.00(0.00)\n",
      "Iter 15720 | Time 24.7181(25.3534) | Bit/dim 3.5726(3.5634) | Xent 0.0951(0.1091) | Loss 9.3958(9.7084) | Error 0.0344(0.0391) Steps 1006(1006.34) | Grad Norm 3.4963(3.8382) | Total Time 0.00(0.00)\n",
      "Iter 15730 | Time 25.3521(25.3317) | Bit/dim 3.5533(3.5652) | Xent 0.0991(0.1121) | Loss 9.2466(9.6230) | Error 0.0356(0.0399) Steps 1006(1007.47) | Grad Norm 3.0136(3.8519) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0286 | Time 116.4893, Epoch Time 1530.1638(1445.8327), Bit/dim 3.5751(best: 3.5726), Xent 1.0222, Loss 4.0862, Error 0.2274(best: 0.2209)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15740 | Time 24.3113(25.2896) | Bit/dim 3.5780(3.5667) | Xent 0.1012(0.1095) | Loss 9.3944(10.4303) | Error 0.0367(0.0385) Steps 1000(1007.45) | Grad Norm 5.0932(3.7608) | Total Time 0.00(0.00)\n",
      "Iter 15750 | Time 25.3608(25.3169) | Bit/dim 3.5623(3.5655) | Xent 0.1072(0.1081) | Loss 9.3014(10.1489) | Error 0.0356(0.0380) Steps 976(1007.26) | Grad Norm 2.5344(3.6045) | Total Time 0.00(0.00)\n",
      "Iter 15760 | Time 25.5461(25.2552) | Bit/dim 3.5640(3.5665) | Xent 0.1308(0.1089) | Loss 9.3300(9.9427) | Error 0.0433(0.0379) Steps 1012(1004.52) | Grad Norm 4.3275(3.6148) | Total Time 0.00(0.00)\n",
      "Iter 15770 | Time 24.9535(25.2518) | Bit/dim 3.5572(3.5653) | Xent 0.0893(0.1082) | Loss 9.1910(9.7965) | Error 0.0267(0.0376) Steps 1006(1006.31) | Grad Norm 2.7415(3.5894) | Total Time 0.00(0.00)\n",
      "Iter 15780 | Time 24.9315(25.2251) | Bit/dim 3.5662(3.5633) | Xent 0.1128(0.1081) | Loss 9.2283(9.6692) | Error 0.0411(0.0372) Steps 1006(1007.74) | Grad Norm 3.8073(3.6121) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0287 | Time 115.9835, Epoch Time 1521.1870(1448.0933), Bit/dim 3.5728(best: 3.5726), Xent 1.0385, Loss 4.0920, Error 0.2270(best: 0.2209)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15790 | Time 25.6990(25.2600) | Bit/dim 3.5529(3.5639) | Xent 0.0981(0.1078) | Loss 9.2745(10.5795) | Error 0.0311(0.0371) Steps 970(1006.34) | Grad Norm 4.3568(3.8460) | Total Time 0.00(0.00)\n",
      "Iter 15800 | Time 24.8050(25.2422) | Bit/dim 3.5791(3.5644) | Xent 0.0964(0.1064) | Loss 9.4314(10.2699) | Error 0.0333(0.0372) Steps 970(1007.95) | Grad Norm 3.2231(3.8208) | Total Time 0.00(0.00)\n",
      "Iter 15810 | Time 24.8415(25.3112) | Bit/dim 3.5458(3.5639) | Xent 0.1040(0.1055) | Loss 9.2624(10.0374) | Error 0.0322(0.0370) Steps 1018(1009.92) | Grad Norm 2.8753(3.7430) | Total Time 0.00(0.00)\n",
      "Iter 15820 | Time 24.9814(25.2863) | Bit/dim 3.5589(3.5632) | Xent 0.1129(0.1084) | Loss 9.4407(9.8653) | Error 0.0422(0.0381) Steps 994(1010.90) | Grad Norm 3.3702(3.7182) | Total Time 0.00(0.00)\n",
      "Iter 15830 | Time 25.5833(25.3295) | Bit/dim 3.5276(3.5639) | Xent 0.1084(0.1100) | Loss 9.2595(9.7317) | Error 0.0433(0.0392) Steps 1000(1010.52) | Grad Norm 4.0734(3.7847) | Total Time 0.00(0.00)\n",
      "Iter 15840 | Time 25.2508(25.3339) | Bit/dim 3.5481(3.5652) | Xent 0.1006(0.1086) | Loss 9.1903(9.6177) | Error 0.0433(0.0387) Steps 994(1009.17) | Grad Norm 3.3298(3.6039) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0288 | Time 115.9948, Epoch Time 1528.7660(1450.5135), Bit/dim 3.5756(best: 3.5726), Xent 1.0653, Loss 4.1082, Error 0.2291(best: 0.2209)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15850 | Time 25.2409(25.2324) | Bit/dim 3.5478(3.5641) | Xent 0.0977(0.1086) | Loss 9.2697(10.4077) | Error 0.0400(0.0391) Steps 1024(1008.48) | Grad Norm 3.8276(3.6333) | Total Time 0.00(0.00)\n",
      "Iter 15860 | Time 25.2118(25.3087) | Bit/dim 3.5360(3.5624) | Xent 0.0918(0.1082) | Loss 9.2385(10.1293) | Error 0.0300(0.0381) Steps 982(1009.40) | Grad Norm 2.9277(3.6550) | Total Time 0.00(0.00)\n",
      "Iter 15870 | Time 25.2811(25.1863) | Bit/dim 3.5678(3.5642) | Xent 0.0936(0.1082) | Loss 9.1604(9.9264) | Error 0.0300(0.0385) Steps 994(1006.58) | Grad Norm 3.9064(3.6742) | Total Time 0.00(0.00)\n",
      "Iter 15880 | Time 24.7913(25.1288) | Bit/dim 3.5414(3.5641) | Xent 0.1353(0.1067) | Loss 9.2864(9.7708) | Error 0.0489(0.0372) Steps 1012(1005.17) | Grad Norm 5.0456(3.8944) | Total Time 0.00(0.00)\n",
      "Iter 15890 | Time 24.6042(25.1998) | Bit/dim 3.5453(3.5668) | Xent 0.0856(0.1067) | Loss 9.2655(9.6675) | Error 0.0344(0.0371) Steps 1000(1007.14) | Grad Norm 3.7659(3.9062) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0289 | Time 115.6279, Epoch Time 1517.1612(1452.5130), Bit/dim 3.5756(best: 3.5726), Xent 1.0411, Loss 4.0961, Error 0.2272(best: 0.2209)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15900 | Time 25.3752(25.2232) | Bit/dim 3.5504(3.5686) | Xent 0.1124(0.1058) | Loss 9.3480(10.5599) | Error 0.0411(0.0375) Steps 994(1005.18) | Grad Norm 5.4493(4.0418) | Total Time 0.00(0.00)\n",
      "Iter 15910 | Time 25.5749(25.2489) | Bit/dim 3.5705(3.5653) | Xent 0.0784(0.1051) | Loss 9.3211(10.2354) | Error 0.0233(0.0372) Steps 970(1003.99) | Grad Norm 3.9661(4.1263) | Total Time 0.00(0.00)\n",
      "Iter 15920 | Time 25.7143(25.2984) | Bit/dim 3.5509(3.5653) | Xent 0.1486(0.1074) | Loss 9.3376(10.0038) | Error 0.0444(0.0373) Steps 1024(1005.07) | Grad Norm 9.5891(4.3726) | Total Time 0.00(0.00)\n",
      "Iter 15930 | Time 25.4513(25.2510) | Bit/dim 3.5943(3.5644) | Xent 0.0837(0.1079) | Loss 9.3832(9.8242) | Error 0.0278(0.0378) Steps 1006(1005.41) | Grad Norm 3.5602(4.3863) | Total Time 0.00(0.00)\n",
      "Iter 15940 | Time 26.2938(25.2272) | Bit/dim 3.5634(3.5649) | Xent 0.0800(0.1085) | Loss 9.3825(9.7025) | Error 0.0322(0.0378) Steps 1030(1007.36) | Grad Norm 4.1923(4.3262) | Total Time 0.00(0.00)\n",
      "Iter 15950 | Time 24.5194(25.1559) | Bit/dim 3.5703(3.5657) | Xent 0.1154(0.1091) | Loss 9.3676(9.6015) | Error 0.0433(0.0379) Steps 1000(1005.81) | Grad Norm 3.8641(4.3525) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0290 | Time 116.7073, Epoch Time 1520.7834(1454.5611), Bit/dim 3.5768(best: 3.5726), Xent 1.0472, Loss 4.1004, Error 0.2302(best: 0.2209)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15960 | Time 24.8723(25.1806) | Bit/dim 3.5588(3.5640) | Xent 0.0971(0.1082) | Loss 9.3283(10.4132) | Error 0.0311(0.0376) Steps 1042(1008.13) | Grad Norm 3.2834(4.1372) | Total Time 0.00(0.00)\n",
      "Iter 15970 | Time 24.3461(25.2567) | Bit/dim 3.5466(3.5648) | Xent 0.1026(0.1066) | Loss 9.2233(10.1270) | Error 0.0344(0.0371) Steps 1000(1007.46) | Grad Norm 3.0113(3.9779) | Total Time 0.00(0.00)\n",
      "Iter 15980 | Time 25.0949(25.2345) | Bit/dim 3.5585(3.5640) | Xent 0.1087(0.1070) | Loss 9.2972(9.9264) | Error 0.0378(0.0371) Steps 1006(1006.22) | Grad Norm 4.5548(3.9175) | Total Time 0.00(0.00)\n",
      "Iter 15990 | Time 24.9913(25.3013) | Bit/dim 3.5261(3.5638) | Xent 0.1205(0.1076) | Loss 9.4381(9.7796) | Error 0.0411(0.0369) Steps 1048(1007.86) | Grad Norm 3.6930(3.8854) | Total Time 0.00(0.00)\n",
      "Iter 16000 | Time 25.3567(25.2636) | Bit/dim 3.6104(3.5628) | Xent 0.0957(0.1084) | Loss 9.4663(9.6620) | Error 0.0344(0.0371) Steps 1042(1005.38) | Grad Norm 3.3476(3.8259) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0291 | Time 114.7269, Epoch Time 1526.3840(1456.7158), Bit/dim 3.5712(best: 3.5726), Xent 1.0610, Loss 4.1016, Error 0.2285(best: 0.2209)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16010 | Time 24.7439(25.2385) | Bit/dim 3.5511(3.5652) | Xent 0.0900(0.1061) | Loss 9.3019(10.6165) | Error 0.0322(0.0370) Steps 988(1008.14) | Grad Norm 4.0733(3.7658) | Total Time 0.00(0.00)\n",
      "Iter 16020 | Time 24.9850(25.2099) | Bit/dim 3.6023(3.5661) | Xent 0.1114(0.1052) | Loss 9.1814(10.2707) | Error 0.0411(0.0365) Steps 1006(1008.30) | Grad Norm 4.4620(4.0416) | Total Time 0.00(0.00)\n",
      "Iter 16030 | Time 25.3555(25.2256) | Bit/dim 3.5776(3.5673) | Xent 0.1138(0.1061) | Loss 9.3951(10.0416) | Error 0.0400(0.0369) Steps 982(1010.56) | Grad Norm 4.8895(4.1013) | Total Time 0.00(0.00)\n",
      "Iter 16040 | Time 24.7514(25.1822) | Bit/dim 3.5605(3.5669) | Xent 0.1295(0.1067) | Loss 9.3154(9.8566) | Error 0.0456(0.0366) Steps 994(1010.17) | Grad Norm 5.7234(4.2680) | Total Time 0.00(0.00)\n",
      "Iter 16050 | Time 26.4336(25.2325) | Bit/dim 3.5755(3.5643) | Xent 0.1088(0.1059) | Loss 9.4182(9.7249) | Error 0.0422(0.0366) Steps 988(1011.42) | Grad Norm 4.5862(4.1762) | Total Time 0.00(0.00)\n",
      "Iter 16060 | Time 24.8056(25.1766) | Bit/dim 3.5616(3.5652) | Xent 0.1184(0.1068) | Loss 9.2673(9.6239) | Error 0.0411(0.0370) Steps 1042(1008.89) | Grad Norm 4.2642(4.1136) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0292 | Time 116.4339, Epoch Time 1517.2237(1458.5310), Bit/dim 3.5710(best: 3.5712), Xent 1.0595, Loss 4.1007, Error 0.2254(best: 0.2209)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16070 | Time 25.0753(25.3724) | Bit/dim 3.5727(3.5631) | Xent 0.1080(0.1060) | Loss 9.3289(10.4126) | Error 0.0456(0.0369) Steps 1000(1007.85) | Grad Norm 3.6223(4.0137) | Total Time 0.00(0.00)\n",
      "Iter 16080 | Time 26.2594(25.3557) | Bit/dim 3.5715(3.5634) | Xent 0.0996(0.1056) | Loss 9.2516(10.1396) | Error 0.0356(0.0370) Steps 994(1009.74) | Grad Norm 3.1758(3.9029) | Total Time 0.00(0.00)\n",
      "Iter 16090 | Time 25.8066(25.4255) | Bit/dim 3.5924(3.5640) | Xent 0.1306(0.1073) | Loss 9.5077(9.9413) | Error 0.0422(0.0376) Steps 1030(1010.46) | Grad Norm 3.5369(3.9632) | Total Time 0.00(0.00)\n",
      "Iter 16100 | Time 26.1418(25.4754) | Bit/dim 3.5737(3.5650) | Xent 0.0948(0.1076) | Loss 9.2811(9.7983) | Error 0.0344(0.0378) Steps 1024(1012.78) | Grad Norm 2.7007(3.7826) | Total Time 0.00(0.00)\n",
      "Iter 16110 | Time 25.6544(25.5534) | Bit/dim 3.5672(3.5651) | Xent 0.1209(0.1066) | Loss 9.3134(9.6857) | Error 0.0422(0.0371) Steps 1012(1013.80) | Grad Norm 2.7902(3.6552) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0293 | Time 116.6823, Epoch Time 1543.6792(1461.0854), Bit/dim 3.5718(best: 3.5710), Xent 1.0602, Loss 4.1019, Error 0.2258(best: 0.2209)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16120 | Time 24.7040(25.4627) | Bit/dim 3.5675(3.5672) | Xent 0.1126(0.1058) | Loss 9.3854(10.6230) | Error 0.0400(0.0374) Steps 1024(1015.28) | Grad Norm 3.5470(3.6594) | Total Time 0.00(0.00)\n",
      "Iter 16130 | Time 25.2136(25.4708) | Bit/dim 3.5919(3.5680) | Xent 0.1064(0.1036) | Loss 9.4659(10.2928) | Error 0.0367(0.0366) Steps 1006(1012.97) | Grad Norm 3.6796(3.5776) | Total Time 0.00(0.00)\n",
      "Iter 16140 | Time 26.3172(25.4550) | Bit/dim 3.5190(3.5650) | Xent 0.1023(0.1029) | Loss 9.2894(10.0412) | Error 0.0378(0.0369) Steps 1066(1015.61) | Grad Norm 5.4706(3.6826) | Total Time 0.00(0.00)\n",
      "Iter 16150 | Time 25.1468(25.4815) | Bit/dim 3.5869(3.5655) | Xent 0.0931(0.1021) | Loss 9.4103(9.8683) | Error 0.0311(0.0365) Steps 982(1014.26) | Grad Norm 4.6268(3.8153) | Total Time 0.00(0.00)\n",
      "Iter 16160 | Time 25.6351(25.5198) | Bit/dim 3.5945(3.5650) | Xent 0.1101(0.1074) | Loss 9.4382(9.7435) | Error 0.0367(0.0381) Steps 994(1016.02) | Grad Norm 4.4508(3.9715) | Total Time 0.00(0.00)\n",
      "Iter 16170 | Time 25.0250(25.4653) | Bit/dim 3.5811(3.5642) | Xent 0.1220(0.1096) | Loss 9.5037(9.6533) | Error 0.0378(0.0387) Steps 1012(1017.52) | Grad Norm 5.4328(4.0883) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0294 | Time 116.0019, Epoch Time 1533.9093(1463.2702), Bit/dim 3.5720(best: 3.5710), Xent 1.0474, Loss 4.0956, Error 0.2276(best: 0.2209)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16180 | Time 25.8915(25.4199) | Bit/dim 3.5706(3.5640) | Xent 0.0952(0.1076) | Loss 9.4210(10.4505) | Error 0.0311(0.0378) Steps 1048(1016.60) | Grad Norm 3.4290(4.1922) | Total Time 0.00(0.00)\n",
      "Iter 16190 | Time 25.2180(25.4208) | Bit/dim 3.5624(3.5648) | Xent 0.1275(0.1064) | Loss 9.4967(10.1786) | Error 0.0478(0.0379) Steps 1012(1020.24) | Grad Norm 4.2449(4.0904) | Total Time 0.00(0.00)\n",
      "Iter 16200 | Time 26.3243(25.4186) | Bit/dim 3.5828(3.5647) | Xent 0.0929(0.1051) | Loss 9.3418(9.9773) | Error 0.0322(0.0374) Steps 982(1020.09) | Grad Norm 4.1799(4.0851) | Total Time 0.00(0.00)\n",
      "Iter 16210 | Time 24.6328(25.4254) | Bit/dim 3.5670(3.5666) | Xent 0.0991(0.1038) | Loss 9.2611(9.8139) | Error 0.0333(0.0366) Steps 982(1011.72) | Grad Norm 4.6169(4.0139) | Total Time 0.00(0.00)\n",
      "Iter 16220 | Time 24.9085(25.3709) | Bit/dim 3.5728(3.5645) | Xent 0.1144(0.1038) | Loss 9.2400(9.6915) | Error 0.0378(0.0366) Steps 1018(1013.73) | Grad Norm 3.6099(4.0275) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0295 | Time 115.3910, Epoch Time 1530.4987(1465.2870), Bit/dim 3.5710(best: 3.5710), Xent 1.0564, Loss 4.0991, Error 0.2272(best: 0.2209)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16230 | Time 24.7893(25.3935) | Bit/dim 3.5971(3.5644) | Xent 0.1012(0.1043) | Loss 9.3829(10.6235) | Error 0.0289(0.0368) Steps 1006(1013.73) | Grad Norm 3.3878(3.9337) | Total Time 0.00(0.00)\n",
      "Iter 16240 | Time 27.0692(25.4242) | Bit/dim 3.5983(3.5660) | Xent 0.1094(0.1032) | Loss 9.5210(10.2907) | Error 0.0344(0.0363) Steps 988(1013.67) | Grad Norm 5.4022(3.9989) | Total Time 0.00(0.00)\n",
      "Iter 16250 | Time 25.5342(25.3222) | Bit/dim 3.5666(3.5658) | Xent 0.1272(0.1041) | Loss 9.5140(10.0529) | Error 0.0422(0.0368) Steps 1072(1017.09) | Grad Norm 3.5258(3.9460) | Total Time 0.00(0.00)\n",
      "Iter 16260 | Time 24.9139(25.2962) | Bit/dim 3.5825(3.5674) | Xent 0.1065(0.1049) | Loss 9.2986(9.8638) | Error 0.0478(0.0374) Steps 1036(1015.04) | Grad Norm 4.7455(3.9904) | Total Time 0.00(0.00)\n",
      "Iter 16270 | Time 25.5271(25.2801) | Bit/dim 3.5384(3.5639) | Xent 0.1007(0.1043) | Loss 9.3096(9.7327) | Error 0.0333(0.0370) Steps 1012(1010.64) | Grad Norm 5.0402(3.9768) | Total Time 0.00(0.00)\n",
      "Iter 16280 | Time 26.2402(25.5300) | Bit/dim 3.5528(3.5598) | Xent 0.0967(0.1031) | Loss 9.1600(9.6270) | Error 0.0356(0.0366) Steps 1024(1013.49) | Grad Norm 3.6221(3.9173) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0296 | Time 116.1274, Epoch Time 1532.8076(1467.3126), Bit/dim 3.5752(best: 3.5710), Xent 1.0672, Loss 4.1088, Error 0.2299(best: 0.2209)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16290 | Time 24.2492(25.4703) | Bit/dim 3.5827(3.5600) | Xent 0.1011(0.1007) | Loss 9.2563(10.4190) | Error 0.0344(0.0355) Steps 988(1011.35) | Grad Norm 4.8981(3.9689) | Total Time 0.00(0.00)\n",
      "Iter 16300 | Time 25.9317(25.5774) | Bit/dim 3.5716(3.5622) | Xent 0.0940(0.1010) | Loss 9.3901(10.1452) | Error 0.0289(0.0352) Steps 1036(1014.59) | Grad Norm 3.1224(4.3038) | Total Time 0.00(0.00)\n",
      "Iter 16310 | Time 25.6245(25.5339) | Bit/dim 3.5541(3.5636) | Xent 0.0897(0.0998) | Loss 9.3911(9.9411) | Error 0.0267(0.0346) Steps 1006(1015.02) | Grad Norm 3.8233(4.3727) | Total Time 0.00(0.00)\n",
      "Iter 16320 | Time 25.9488(25.5210) | Bit/dim 3.5481(3.5636) | Xent 0.1037(0.1019) | Loss 9.2677(9.7880) | Error 0.0367(0.0357) Steps 1018(1016.35) | Grad Norm 3.9073(4.4143) | Total Time 0.00(0.00)\n",
      "Iter 16330 | Time 25.4659(25.3847) | Bit/dim 3.5785(3.5624) | Xent 0.1011(0.1041) | Loss 9.4101(9.6742) | Error 0.0289(0.0365) Steps 1042(1017.56) | Grad Norm 5.1672(4.5255) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0297 | Time 115.5653, Epoch Time 1533.4435(1469.2966), Bit/dim 3.5708(best: 3.5710), Xent 1.0648, Loss 4.1032, Error 0.2242(best: 0.2209)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16340 | Time 25.1736(25.3696) | Bit/dim 3.5901(3.5620) | Xent 0.1047(0.1051) | Loss 9.5267(10.6282) | Error 0.0356(0.0367) Steps 1012(1017.17) | Grad Norm 4.6218(4.5246) | Total Time 0.00(0.00)\n",
      "Iter 16350 | Time 25.1888(25.4587) | Bit/dim 3.5974(3.5644) | Xent 0.1225(0.1030) | Loss 9.5066(10.3127) | Error 0.0456(0.0360) Steps 1018(1018.89) | Grad Norm 4.3656(4.4004) | Total Time 0.00(0.00)\n",
      "Iter 16360 | Time 26.0161(25.4403) | Bit/dim 3.5867(3.5638) | Xent 0.0801(0.1032) | Loss 9.3261(10.0666) | Error 0.0222(0.0356) Steps 994(1012.65) | Grad Norm 3.5038(4.3380) | Total Time 0.00(0.00)\n",
      "Iter 16370 | Time 25.0156(25.3897) | Bit/dim 3.5454(3.5611) | Xent 0.1112(0.1035) | Loss 9.3442(9.8784) | Error 0.0433(0.0357) Steps 1012(1012.96) | Grad Norm 3.6241(4.2108) | Total Time 0.00(0.00)\n",
      "Iter 16380 | Time 24.4790(25.3694) | Bit/dim 3.5801(3.5658) | Xent 0.0984(0.1029) | Loss 9.6085(9.7482) | Error 0.0367(0.0355) Steps 1066(1015.95) | Grad Norm 4.0745(4.1048) | Total Time 0.00(0.00)\n",
      "Iter 16390 | Time 24.2459(25.4286) | Bit/dim 3.5613(3.5639) | Xent 0.1298(0.1043) | Loss 9.4025(9.6494) | Error 0.0456(0.0368) Steps 1006(1013.70) | Grad Norm 3.7388(4.0590) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0298 | Time 116.6081, Epoch Time 1533.3253(1471.2174), Bit/dim 3.5733(best: 3.5708), Xent 1.0712, Loss 4.1089, Error 0.2268(best: 0.2209)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16400 | Time 25.3838(25.4473) | Bit/dim 3.5717(3.5635) | Xent 0.1081(0.1043) | Loss 9.3790(10.4431) | Error 0.0356(0.0364) Steps 1030(1015.09) | Grad Norm 4.1038(3.9567) | Total Time 0.00(0.00)\n",
      "Iter 16410 | Time 26.5209(25.5162) | Bit/dim 3.5415(3.5625) | Xent 0.1136(0.1039) | Loss 9.3259(10.1593) | Error 0.0422(0.0359) Steps 1036(1017.11) | Grad Norm 4.6352(4.0560) | Total Time 0.00(0.00)\n",
      "Iter 16420 | Time 25.4394(25.4020) | Bit/dim 3.5367(3.5620) | Xent 0.1160(0.1041) | Loss 9.2907(9.9423) | Error 0.0433(0.0359) Steps 1000(1017.62) | Grad Norm 5.0026(4.1171) | Total Time 0.00(0.00)\n",
      "Iter 16430 | Time 25.9256(25.4508) | Bit/dim 3.5760(3.5648) | Xent 0.1042(0.1028) | Loss 9.4680(9.7961) | Error 0.0389(0.0358) Steps 1054(1018.95) | Grad Norm 6.6860(4.2323) | Total Time 0.00(0.00)\n",
      "Iter 16440 | Time 25.3145(25.5366) | Bit/dim 3.5717(3.5638) | Xent 0.1043(0.1041) | Loss 9.4185(9.6827) | Error 0.0389(0.0360) Steps 1000(1018.92) | Grad Norm 3.3267(4.5609) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0299 | Time 117.9221, Epoch Time 1541.0519(1473.3125), Bit/dim 3.5699(best: 3.5708), Xent 1.0641, Loss 4.1019, Error 0.2281(best: 0.2209)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16450 | Time 25.9980(25.6475) | Bit/dim 3.5801(3.5645) | Xent 0.0943(0.1028) | Loss 9.3659(10.6453) | Error 0.0367(0.0358) Steps 1006(1020.72) | Grad Norm 5.0955(4.6911) | Total Time 0.00(0.00)\n",
      "Iter 16460 | Time 25.1133(25.5536) | Bit/dim 3.5480(3.5615) | Xent 0.0871(0.1013) | Loss 9.3295(10.3034) | Error 0.0211(0.0353) Steps 1012(1019.73) | Grad Norm 3.6912(4.5466) | Total Time 0.00(0.00)\n",
      "Iter 16470 | Time 25.0352(25.5683) | Bit/dim 3.5344(3.5621) | Xent 0.0949(0.1024) | Loss 9.1404(10.0566) | Error 0.0322(0.0363) Steps 1024(1022.01) | Grad Norm 5.2304(4.6163) | Total Time 0.00(0.00)\n",
      "Iter 16480 | Time 24.8854(25.5499) | Bit/dim 3.6020(3.5614) | Xent 0.0934(0.1014) | Loss 9.4495(9.8764) | Error 0.0322(0.0355) Steps 1030(1022.97) | Grad Norm 3.6273(4.4318) | Total Time 0.00(0.00)\n",
      "Iter 16490 | Time 25.3338(25.5595) | Bit/dim 3.5599(3.5627) | Xent 0.1080(0.1003) | Loss 9.2917(9.7384) | Error 0.0389(0.0349) Steps 1012(1020.69) | Grad Norm 2.8043(4.1211) | Total Time 0.00(0.00)\n",
      "Iter 16500 | Time 24.6426(25.6126) | Bit/dim 3.5939(3.5634) | Xent 0.0891(0.1013) | Loss 9.4837(9.6499) | Error 0.0322(0.0355) Steps 1024(1022.50) | Grad Norm 4.3499(4.0444) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0300 | Time 115.8022, Epoch Time 1543.3655(1475.4140), Bit/dim 3.5760(best: 3.5699), Xent 1.0696, Loss 4.1108, Error 0.2270(best: 0.2209)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16510 | Time 26.0651(25.5869) | Bit/dim 3.5899(3.5653) | Xent 0.0885(0.0996) | Loss 9.5014(10.4227) | Error 0.0300(0.0343) Steps 1048(1021.79) | Grad Norm 3.9217(4.0181) | Total Time 0.00(0.00)\n",
      "Iter 16520 | Time 25.4252(25.5702) | Bit/dim 3.5725(3.5634) | Xent 0.0992(0.0996) | Loss 9.3107(10.1422) | Error 0.0300(0.0339) Steps 1030(1021.82) | Grad Norm 3.5217(3.9902) | Total Time 0.00(0.00)\n",
      "Iter 16530 | Time 25.8532(25.5542) | Bit/dim 3.5679(3.5613) | Xent 0.0964(0.0990) | Loss 9.4088(9.9370) | Error 0.0311(0.0341) Steps 1012(1020.84) | Grad Norm 3.1450(3.9266) | Total Time 0.00(0.00)\n",
      "Iter 16540 | Time 24.7765(25.5040) | Bit/dim 3.5873(3.5622) | Xent 0.1133(0.1010) | Loss 9.3418(9.7900) | Error 0.0311(0.0349) Steps 1054(1020.64) | Grad Norm 5.0149(4.0562) | Total Time 0.00(0.00)\n",
      "Iter 16550 | Time 25.9122(25.5979) | Bit/dim 3.5478(3.5604) | Xent 0.0983(0.1021) | Loss 9.2859(9.6694) | Error 0.0389(0.0352) Steps 1000(1019.69) | Grad Norm 4.0495(4.1627) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0301 | Time 117.7201, Epoch Time 1546.2182(1477.5382), Bit/dim 3.5721(best: 3.5699), Xent 1.0682, Loss 4.1062, Error 0.2256(best: 0.2209)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16560 | Time 25.4776(25.6797) | Bit/dim 3.5468(3.5598) | Xent 0.1125(0.1019) | Loss 9.2636(10.6176) | Error 0.0411(0.0350) Steps 1018(1021.26) | Grad Norm 4.0606(4.1608) | Total Time 0.00(0.00)\n",
      "Iter 16570 | Time 26.6178(25.6901) | Bit/dim 3.5583(3.5587) | Xent 0.0929(0.1016) | Loss 9.2785(10.2828) | Error 0.0344(0.0353) Steps 1000(1020.88) | Grad Norm 4.2528(4.1335) | Total Time 0.00(0.00)\n",
      "Iter 16580 | Time 25.6774(25.6844) | Bit/dim 3.5798(3.5621) | Xent 0.1192(0.1009) | Loss 9.4363(10.0519) | Error 0.0411(0.0351) Steps 1048(1020.95) | Grad Norm 4.0508(4.0572) | Total Time 0.00(0.00)\n",
      "Iter 16590 | Time 25.2915(25.7272) | Bit/dim 3.5857(3.5667) | Xent 0.0948(0.1003) | Loss 9.3841(9.8729) | Error 0.0389(0.0358) Steps 1018(1018.94) | Grad Norm 3.9183(3.9704) | Total Time 0.00(0.00)\n",
      "Iter 16600 | Time 26.1416(25.7947) | Bit/dim 3.5856(3.5647) | Xent 0.1013(0.0993) | Loss 9.1894(9.7361) | Error 0.0400(0.0356) Steps 1042(1023.87) | Grad Norm 6.1235(4.0560) | Total Time 0.00(0.00)\n",
      "Iter 16610 | Time 24.8285(25.7530) | Bit/dim 3.5673(3.5637) | Xent 0.0944(0.1016) | Loss 9.4866(9.6482) | Error 0.0322(0.0364) Steps 1024(1025.81) | Grad Norm 3.7504(4.4571) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0302 | Time 116.3652, Epoch Time 1550.1260(1479.7158), Bit/dim 3.5736(best: 3.5699), Xent 1.0997, Loss 4.1235, Error 0.2338(best: 0.2209)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16620 | Time 25.6967(25.7039) | Bit/dim 3.5723(3.5640) | Xent 0.0933(0.1003) | Loss 9.4715(10.4541) | Error 0.0311(0.0360) Steps 1018(1025.37) | Grad Norm 5.7511(4.6293) | Total Time 0.00(0.00)\n",
      "Iter 16630 | Time 27.0176(25.7853) | Bit/dim 3.5344(3.5645) | Xent 0.1044(0.1006) | Loss 9.4412(10.1764) | Error 0.0344(0.0357) Steps 1078(1027.83) | Grad Norm 4.7391(4.5505) | Total Time 0.00(0.00)\n",
      "Iter 16640 | Time 25.3887(25.6588) | Bit/dim 3.5453(3.5639) | Xent 0.0984(0.1014) | Loss 9.2769(9.9688) | Error 0.0356(0.0360) Steps 988(1025.18) | Grad Norm 3.3493(4.3355) | Total Time 0.00(0.00)\n",
      "Iter 16650 | Time 24.6166(25.6425) | Bit/dim 3.5659(3.5615) | Xent 0.0999(0.1006) | Loss 9.3697(9.8072) | Error 0.0322(0.0356) Steps 1024(1024.85) | Grad Norm 3.7635(4.1813) | Total Time 0.00(0.00)\n",
      "Iter 16660 | Time 25.5403(25.6719) | Bit/dim 3.5581(3.5615) | Xent 0.0920(0.1001) | Loss 9.3898(9.6903) | Error 0.0300(0.0352) Steps 1030(1023.82) | Grad Norm 4.2353(4.0256) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0303 | Time 117.6966, Epoch Time 1549.0028(1481.7944), Bit/dim 3.5696(best: 3.5699), Xent 1.0937, Loss 4.1164, Error 0.2285(best: 0.2209)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16670 | Time 25.4862(25.7716) | Bit/dim 3.5344(3.5600) | Xent 0.1011(0.0991) | Loss 9.1828(10.6106) | Error 0.0367(0.0349) Steps 1036(1028.29) | Grad Norm 5.0317(4.0548) | Total Time 0.00(0.00)\n",
      "Iter 16680 | Time 26.0287(25.7625) | Bit/dim 3.5750(3.5605) | Xent 0.0846(0.0980) | Loss 9.5014(10.2803) | Error 0.0267(0.0345) Steps 1030(1025.21) | Grad Norm 3.5537(4.1263) | Total Time 0.00(0.00)\n",
      "Iter 16690 | Time 25.7137(25.7941) | Bit/dim 3.5576(3.5609) | Xent 0.1245(0.0988) | Loss 9.3609(10.0486) | Error 0.0489(0.0349) Steps 982(1023.96) | Grad Norm 4.3319(4.0568) | Total Time 0.00(0.00)\n",
      "Iter 16700 | Time 26.5254(25.8383) | Bit/dim 3.5705(3.5600) | Xent 0.0699(0.0964) | Loss 9.4598(9.8659) | Error 0.0233(0.0336) Steps 1078(1025.65) | Grad Norm 3.4934(3.8883) | Total Time 0.00(0.00)\n",
      "Iter 16710 | Time 26.2764(25.7797) | Bit/dim 3.5891(3.5611) | Xent 0.1062(0.0964) | Loss 9.4565(9.7294) | Error 0.0378(0.0340) Steps 1042(1025.39) | Grad Norm 4.3829(3.9658) | Total Time 0.00(0.00)\n",
      "Iter 16720 | Time 26.5518(25.7341) | Bit/dim 3.5712(3.5614) | Xent 0.1166(0.0980) | Loss 9.5466(9.6508) | Error 0.0389(0.0345) Steps 1066(1026.40) | Grad Norm 3.9889(4.0244) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0304 | Time 117.2721, Epoch Time 1553.7481(1483.9530), Bit/dim 3.5717(best: 3.5696), Xent 1.1124, Loss 4.1279, Error 0.2286(best: 0.2209)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16730 | Time 25.5490(25.7468) | Bit/dim 3.5799(3.5590) | Xent 0.1102(0.0979) | Loss 9.1550(10.4505) | Error 0.0389(0.0345) Steps 1030(1027.40) | Grad Norm 5.3098(4.0505) | Total Time 0.00(0.00)\n",
      "Iter 16740 | Time 25.5013(25.7532) | Bit/dim 3.5685(3.5628) | Xent 0.1306(0.0979) | Loss 9.4019(10.1566) | Error 0.0422(0.0343) Steps 1030(1020.71) | Grad Norm 4.3283(3.9802) | Total Time 0.00(0.00)\n",
      "Iter 16750 | Time 25.5447(25.7888) | Bit/dim 3.5634(3.5635) | Xent 0.0972(0.0969) | Loss 9.3968(9.9490) | Error 0.0389(0.0345) Steps 1006(1021.64) | Grad Norm 4.6705(4.0285) | Total Time 0.00(0.00)\n",
      "Iter 16760 | Time 25.7259(25.6845) | Bit/dim 3.5128(3.5608) | Xent 0.1135(0.0991) | Loss 9.2356(9.8017) | Error 0.0433(0.0351) Steps 1012(1019.03) | Grad Norm 3.6074(4.1592) | Total Time 0.00(0.00)\n",
      "Iter 16770 | Time 25.3119(25.6824) | Bit/dim 3.5690(3.5610) | Xent 0.1031(0.0994) | Loss 9.3914(9.7019) | Error 0.0389(0.0352) Steps 994(1019.55) | Grad Norm 4.2000(4.0543) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0305 | Time 116.5761, Epoch Time 1549.6632(1485.9243), Bit/dim 3.5732(best: 3.5696), Xent 1.0936, Loss 4.1200, Error 0.2300(best: 0.2209)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16780 | Time 25.3327(25.7507) | Bit/dim 3.5745(3.5640) | Xent 0.0781(0.0997) | Loss 9.3638(10.6815) | Error 0.0256(0.0354) Steps 1030(1019.39) | Grad Norm 3.0377(4.0575) | Total Time 0.00(0.00)\n",
      "Iter 16790 | Time 25.7722(25.7550) | Bit/dim 3.5329(3.5598) | Xent 0.1023(0.1018) | Loss 9.2989(10.3336) | Error 0.0333(0.0360) Steps 1030(1020.71) | Grad Norm 4.6310(4.3155) | Total Time 0.00(0.00)\n",
      "Iter 16800 | Time 25.4914(25.6962) | Bit/dim 3.5219(3.5616) | Xent 0.1048(0.1002) | Loss 9.3312(10.0822) | Error 0.0389(0.0352) Steps 1024(1020.80) | Grad Norm 3.9708(4.1678) | Total Time 0.00(0.00)\n",
      "Iter 16810 | Time 25.8542(25.7203) | Bit/dim 3.6135(3.5608) | Xent 0.1034(0.1008) | Loss 9.5150(9.9105) | Error 0.0378(0.0352) Steps 1018(1021.55) | Grad Norm 4.3025(4.1240) | Total Time 0.00(0.00)\n",
      "Iter 16820 | Time 26.0100(25.7424) | Bit/dim 3.5432(3.5624) | Xent 0.0947(0.0989) | Loss 9.5121(9.7813) | Error 0.0333(0.0343) Steps 1054(1026.67) | Grad Norm 4.3665(4.2083) | Total Time 0.00(0.00)\n",
      "Iter 16830 | Time 25.5263(25.7433) | Bit/dim 3.5772(3.5629) | Xent 0.0883(0.0979) | Loss 9.3850(9.6759) | Error 0.0289(0.0338) Steps 1030(1025.33) | Grad Norm 3.2319(4.1238) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0306 | Time 117.8668, Epoch Time 1552.4718(1487.9208), Bit/dim 3.5714(best: 3.5696), Xent 1.0882, Loss 4.1155, Error 0.2271(best: 0.2209)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16840 | Time 25.2462(25.6980) | Bit/dim 3.5760(3.5620) | Xent 0.0843(0.0956) | Loss 9.2442(10.4853) | Error 0.0278(0.0329) Steps 1018(1025.21) | Grad Norm 3.2286(3.9579) | Total Time 0.00(0.00)\n",
      "Iter 16850 | Time 25.2594(25.5840) | Bit/dim 3.5547(3.5631) | Xent 0.0825(0.0955) | Loss 9.2495(10.1844) | Error 0.0333(0.0337) Steps 1012(1023.97) | Grad Norm 2.7965(4.1357) | Total Time 0.00(0.00)\n",
      "Iter 16860 | Time 25.0622(25.5452) | Bit/dim 3.5667(3.5622) | Xent 0.1142(0.0968) | Loss 9.3478(9.9751) | Error 0.0389(0.0338) Steps 1030(1023.73) | Grad Norm 4.4931(4.2023) | Total Time 0.00(0.00)\n",
      "Iter 16870 | Time 26.6270(25.6681) | Bit/dim 3.6039(3.5637) | Xent 0.0835(0.0974) | Loss 9.6209(9.8357) | Error 0.0300(0.0340) Steps 1042(1023.18) | Grad Norm 3.6385(4.1954) | Total Time 0.00(0.00)\n",
      "Iter 16880 | Time 26.5527(25.6624) | Bit/dim 3.5400(3.5617) | Xent 0.0982(0.0981) | Loss 9.2257(9.7182) | Error 0.0367(0.0345) Steps 1000(1026.77) | Grad Norm 4.1312(4.2290) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0307 | Time 117.8093, Epoch Time 1544.3935(1489.6149), Bit/dim 3.5734(best: 3.5696), Xent 1.1013, Loss 4.1240, Error 0.2263(best: 0.2209)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16890 | Time 26.3257(25.6914) | Bit/dim 3.5607(3.5589) | Xent 0.1071(0.0977) | Loss 9.3926(10.6387) | Error 0.0389(0.0343) Steps 1042(1025.51) | Grad Norm 4.9230(4.2476) | Total Time 0.00(0.00)\n",
      "Iter 16900 | Time 25.6766(25.6444) | Bit/dim 3.5436(3.5607) | Xent 0.1115(0.0978) | Loss 9.3339(10.3115) | Error 0.0411(0.0342) Steps 1042(1025.29) | Grad Norm 3.7172(4.2175) | Total Time 0.00(0.00)\n",
      "Iter 16910 | Time 25.4906(25.6763) | Bit/dim 3.5646(3.5624) | Xent 0.0891(0.0969) | Loss 9.2623(10.0579) | Error 0.0233(0.0335) Steps 1012(1022.12) | Grad Norm 3.9629(4.2278) | Total Time 0.00(0.00)\n",
      "Iter 16920 | Time 24.1476(25.7322) | Bit/dim 3.5832(3.5603) | Xent 0.1045(0.0992) | Loss 9.2065(9.8775) | Error 0.0322(0.0340) Steps 1018(1022.68) | Grad Norm 5.3842(4.4360) | Total Time 0.00(0.00)\n",
      "Iter 16930 | Time 25.9491(25.8510) | Bit/dim 3.5441(3.5604) | Xent 0.0838(0.1006) | Loss 9.2570(9.7371) | Error 0.0322(0.0345) Steps 1048(1024.65) | Grad Norm 3.5657(4.4755) | Total Time 0.00(0.00)\n",
      "Iter 16940 | Time 26.2894(25.7146) | Bit/dim 3.5796(3.5601) | Xent 0.1230(0.1029) | Loss 9.5588(9.6456) | Error 0.0444(0.0359) Steps 1030(1027.31) | Grad Norm 5.7604(4.5111) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0308 | Time 118.3613, Epoch Time 1552.9859(1491.5161), Bit/dim 3.5714(best: 3.5696), Xent 1.1273, Loss 4.1350, Error 0.2317(best: 0.2209)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16950 | Time 25.0286(25.6936) | Bit/dim 3.5677(3.5618) | Xent 0.1020(0.1018) | Loss 9.3665(10.4687) | Error 0.0378(0.0352) Steps 1024(1027.08) | Grad Norm 3.6486(4.3694) | Total Time 0.00(0.00)\n",
      "Iter 16960 | Time 26.3795(25.7321) | Bit/dim 3.5570(3.5635) | Xent 0.0807(0.1015) | Loss 9.3928(10.1962) | Error 0.0233(0.0351) Steps 1030(1026.55) | Grad Norm 3.0594(4.2707) | Total Time 0.00(0.00)\n",
      "Iter 16970 | Time 25.4528(25.6482) | Bit/dim 3.5581(3.5612) | Xent 0.0986(0.1000) | Loss 9.1997(9.9740) | Error 0.0367(0.0343) Steps 1036(1026.41) | Grad Norm 3.0930(4.0776) | Total Time 0.00(0.00)\n",
      "Iter 16980 | Time 25.9904(25.6639) | Bit/dim 3.5397(3.5613) | Xent 0.1349(0.1009) | Loss 9.4654(9.8287) | Error 0.0544(0.0347) Steps 1042(1030.23) | Grad Norm 11.7494(4.5495) | Total Time 0.00(0.00)\n",
      "Iter 16990 | Time 25.9420(25.6521) | Bit/dim 3.5657(3.5601) | Xent 0.1050(0.1028) | Loss 9.4647(9.7151) | Error 0.0389(0.0357) Steps 1060(1028.57) | Grad Norm 4.6840(4.8400) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0309 | Time 117.6128, Epoch Time 1547.5689(1493.1976), Bit/dim 3.5727(best: 3.5696), Xent 1.0884, Loss 4.1169, Error 0.2267(best: 0.2209)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17000 | Time 25.1664(25.6892) | Bit/dim 3.5557(3.5602) | Xent 0.0970(0.1017) | Loss 9.3856(10.6624) | Error 0.0322(0.0350) Steps 1036(1027.27) | Grad Norm 2.9581(4.7232) | Total Time 0.00(0.00)\n",
      "Iter 17010 | Time 27.1564(25.7878) | Bit/dim 3.5707(3.5629) | Xent 0.0744(0.1018) | Loss 9.4023(10.3293) | Error 0.0289(0.0351) Steps 1018(1025.55) | Grad Norm 3.2723(4.5026) | Total Time 0.00(0.00)\n",
      "Iter 17020 | Time 26.0610(25.8155) | Bit/dim 3.5988(3.5591) | Xent 0.0954(0.0992) | Loss 9.4596(10.0733) | Error 0.0300(0.0344) Steps 1042(1028.37) | Grad Norm 2.7690(4.2595) | Total Time 0.00(0.00)\n",
      "Iter 17030 | Time 26.4887(25.8206) | Bit/dim 3.5887(3.5628) | Xent 0.1059(0.0987) | Loss 9.5321(9.9019) | Error 0.0289(0.0345) Steps 1036(1028.58) | Grad Norm 5.3357(4.4721) | Total Time 0.00(0.00)\n",
      "Iter 17040 | Time 26.8933(25.9022) | Bit/dim 3.5441(3.5614) | Xent 0.1054(0.0975) | Loss 9.5087(9.7863) | Error 0.0356(0.0342) Steps 1000(1029.42) | Grad Norm 3.6956(4.2885) | Total Time 0.00(0.00)\n",
      "Iter 17050 | Time 25.7045(25.9181) | Bit/dim 3.5255(3.5613) | Xent 0.1235(0.0989) | Loss 9.4609(9.6952) | Error 0.0411(0.0344) Steps 1072(1028.98) | Grad Norm 5.6275(4.2324) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0310 | Time 118.1977, Epoch Time 1563.2889(1495.3004), Bit/dim 3.5699(best: 3.5696), Xent 1.0978, Loss 4.1188, Error 0.2294(best: 0.2209)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17060 | Time 25.9260(25.8894) | Bit/dim 3.5519(3.5602) | Xent 0.0937(0.0977) | Loss 9.5138(10.5138) | Error 0.0344(0.0340) Steps 1036(1032.08) | Grad Norm 4.5401(4.1911) | Total Time 0.00(0.00)\n",
      "Iter 17070 | Time 25.9918(25.8678) | Bit/dim 3.5459(3.5581) | Xent 0.1029(0.0978) | Loss 9.4032(10.2196) | Error 0.0411(0.0339) Steps 1042(1030.66) | Grad Norm 3.7237(4.2860) | Total Time 0.00(0.00)\n",
      "Iter 17080 | Time 26.0177(25.7646) | Bit/dim 3.5355(3.5582) | Xent 0.0804(0.0960) | Loss 9.2107(9.9988) | Error 0.0211(0.0328) Steps 988(1027.88) | Grad Norm 3.6639(4.2275) | Total Time 0.00(0.00)\n",
      "Iter 17090 | Time 25.1866(25.7557) | Bit/dim 3.5723(3.5596) | Xent 0.0695(0.0939) | Loss 9.2703(9.8465) | Error 0.0233(0.0325) Steps 1042(1032.88) | Grad Norm 3.6500(4.2404) | Total Time 0.00(0.00)\n",
      "Iter 17100 | Time 26.1317(25.7738) | Bit/dim 3.5880(3.5596) | Xent 0.1201(0.0980) | Loss 9.4875(9.7389) | Error 0.0422(0.0339) Steps 1006(1033.40) | Grad Norm 6.9853(4.5381) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0311 | Time 117.4574, Epoch Time 1551.5799(1496.9888), Bit/dim 3.5731(best: 3.5696), Xent 1.1178, Loss 4.1320, Error 0.2320(best: 0.2209)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17110 | Time 26.0275(25.8078) | Bit/dim 3.5553(3.5629) | Xent 0.0932(0.0967) | Loss 9.4876(10.7128) | Error 0.0344(0.0335) Steps 1030(1033.43) | Grad Norm 4.1064(4.5255) | Total Time 0.00(0.00)\n",
      "Iter 17120 | Time 26.1956(25.8393) | Bit/dim 3.5615(3.5623) | Xent 0.0835(0.0953) | Loss 9.4613(10.3670) | Error 0.0256(0.0329) Steps 1012(1035.18) | Grad Norm 5.6949(4.6262) | Total Time 0.00(0.00)\n",
      "Iter 17130 | Time 27.0306(25.9387) | Bit/dim 3.5791(3.5611) | Xent 0.1051(0.0991) | Loss 9.4080(10.1269) | Error 0.0378(0.0342) Steps 1048(1036.25) | Grad Norm 5.4601(4.7441) | Total Time 0.00(0.00)\n",
      "Iter 17140 | Time 26.4753(25.9213) | Bit/dim 3.5679(3.5595) | Xent 0.0937(0.0986) | Loss 9.3260(9.9174) | Error 0.0300(0.0338) Steps 1036(1034.61) | Grad Norm 4.5429(4.7275) | Total Time 0.00(0.00)\n",
      "Iter 17150 | Time 26.7713(25.8881) | Bit/dim 3.5547(3.5628) | Xent 0.1025(0.0997) | Loss 9.4423(9.7872) | Error 0.0367(0.0342) Steps 1024(1035.53) | Grad Norm 4.0969(4.5422) | Total Time 0.00(0.00)\n",
      "Iter 17160 | Time 26.2312(25.9122) | Bit/dim 3.5792(3.5618) | Xent 0.0938(0.0992) | Loss 9.4381(9.6987) | Error 0.0300(0.0341) Steps 1042(1036.37) | Grad Norm 3.5139(4.4507) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0312 | Time 118.9608, Epoch Time 1564.3035(1499.0082), Bit/dim 3.5688(best: 3.5696), Xent 1.0900, Loss 4.1137, Error 0.2257(best: 0.2209)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17170 | Time 25.8337(25.9081) | Bit/dim 3.5338(3.5630) | Xent 0.0997(0.0962) | Loss 9.2291(10.5080) | Error 0.0344(0.0331) Steps 1024(1035.64) | Grad Norm 5.1333(4.2690) | Total Time 0.00(0.00)\n",
      "Iter 17180 | Time 25.3051(25.8885) | Bit/dim 3.5597(3.5635) | Xent 0.0771(0.0944) | Loss 9.2712(10.2089) | Error 0.0222(0.0324) Steps 1036(1031.58) | Grad Norm 3.6856(4.0988) | Total Time 0.00(0.00)\n",
      "Iter 17190 | Time 26.1771(25.8287) | Bit/dim 3.5168(3.5584) | Xent 0.1004(0.0942) | Loss 9.2435(9.9749) | Error 0.0367(0.0328) Steps 1042(1032.65) | Grad Norm 4.3776(4.0536) | Total Time 0.00(0.00)\n",
      "Iter 17200 | Time 24.8082(25.8591) | Bit/dim 3.5896(3.5608) | Xent 0.1047(0.0958) | Loss 9.4054(9.8116) | Error 0.0389(0.0332) Steps 1048(1034.27) | Grad Norm 4.7659(4.0385) | Total Time 0.00(0.00)\n",
      "Iter 17210 | Time 26.2100(25.8782) | Bit/dim 3.5282(3.5595) | Xent 0.0918(0.0955) | Loss 9.3098(9.7017) | Error 0.0311(0.0337) Steps 1030(1033.29) | Grad Norm 7.0724(4.1895) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0313 | Time 117.5585, Epoch Time 1555.2568(1500.6957), Bit/dim 3.5733(best: 3.5688), Xent 1.0996, Loss 4.1231, Error 0.2291(best: 0.2209)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17220 | Time 25.4117(25.7916) | Bit/dim 3.5627(3.5596) | Xent 0.1124(0.0951) | Loss 9.2953(10.6473) | Error 0.0389(0.0339) Steps 1030(1030.81) | Grad Norm 3.4897(4.2797) | Total Time 0.00(0.00)\n",
      "Iter 17230 | Time 26.4844(25.8578) | Bit/dim 3.5743(3.5622) | Xent 0.0962(0.0953) | Loss 9.3504(10.3282) | Error 0.0311(0.0339) Steps 1024(1031.29) | Grad Norm 6.2462(4.6855) | Total Time 0.00(0.00)\n",
      "Iter 17240 | Time 26.9408(25.8834) | Bit/dim 3.5826(3.5603) | Xent 0.0867(0.0971) | Loss 9.5083(10.0920) | Error 0.0267(0.0339) Steps 1054(1033.72) | Grad Norm 3.1437(4.5809) | Total Time 0.00(0.00)\n",
      "Iter 17250 | Time 25.6841(25.8999) | Bit/dim 3.5779(3.5614) | Xent 0.0961(0.0971) | Loss 9.4384(9.9098) | Error 0.0333(0.0343) Steps 1054(1032.43) | Grad Norm 4.3937(4.4663) | Total Time 0.00(0.00)\n",
      "Iter 17260 | Time 26.1501(25.8524) | Bit/dim 3.5617(3.5620) | Xent 0.1072(0.0970) | Loss 9.3999(9.7672) | Error 0.0344(0.0343) Steps 1036(1031.82) | Grad Norm 5.8379(4.4125) | Total Time 0.00(0.00)\n",
      "Iter 17270 | Time 26.5668(25.8190) | Bit/dim 3.5861(3.5621) | Xent 0.0962(0.0978) | Loss 9.4315(9.6695) | Error 0.0344(0.0344) Steps 1030(1031.80) | Grad Norm 3.8066(4.3710) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0314 | Time 117.2398, Epoch Time 1557.2854(1502.3934), Bit/dim 3.5704(best: 3.5688), Xent 1.1198, Loss 4.1303, Error 0.2312(best: 0.2209)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17280 | Time 25.2414(25.7522) | Bit/dim 3.5128(3.5615) | Xent 0.0959(0.0970) | Loss 9.2970(10.5001) | Error 0.0311(0.0340) Steps 1024(1034.02) | Grad Norm 4.4962(4.2900) | Total Time 0.00(0.00)\n",
      "Iter 17290 | Time 25.9820(25.7466) | Bit/dim 3.5532(3.5585) | Xent 0.1124(0.0967) | Loss 9.4756(10.2034) | Error 0.0322(0.0335) Steps 1048(1034.85) | Grad Norm 5.3266(4.2981) | Total Time 0.00(0.00)\n",
      "Iter 17300 | Time 25.6410(25.7501) | Bit/dim 3.5391(3.5571) | Xent 0.0860(0.0961) | Loss 9.3908(9.9844) | Error 0.0344(0.0335) Steps 1000(1030.88) | Grad Norm 4.0568(4.2808) | Total Time 0.00(0.00)\n",
      "Iter 17310 | Time 25.5651(25.8074) | Bit/dim 3.5298(3.5592) | Xent 0.0782(0.0961) | Loss 9.2316(9.8336) | Error 0.0300(0.0338) Steps 1012(1031.48) | Grad Norm 3.7379(4.2832) | Total Time 0.00(0.00)\n",
      "Iter 17320 | Time 26.4812(25.8044) | Bit/dim 3.5365(3.5601) | Xent 0.1264(0.0968) | Loss 9.4137(9.7109) | Error 0.0422(0.0342) Steps 1030(1030.98) | Grad Norm 7.2755(4.7420) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0315 | Time 118.6573, Epoch Time 1554.5196(1503.9572), Bit/dim 3.5679(best: 3.5688), Xent 1.1012, Loss 4.1185, Error 0.2254(best: 0.2209)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17330 | Time 26.7420(25.8741) | Bit/dim 3.5263(3.5586) | Xent 0.0943(0.0973) | Loss 9.3904(10.6756) | Error 0.0333(0.0342) Steps 1048(1031.91) | Grad Norm 4.3921(4.5979) | Total Time 0.00(0.00)\n",
      "Iter 17340 | Time 24.8563(25.9891) | Bit/dim 3.5771(3.5585) | Xent 0.1090(0.0966) | Loss 9.4708(10.3481) | Error 0.0378(0.0338) Steps 1024(1034.27) | Grad Norm 5.4471(4.4250) | Total Time 0.00(0.00)\n",
      "Iter 17350 | Time 25.1719(25.9940) | Bit/dim 3.5742(3.5597) | Xent 0.1078(0.0966) | Loss 9.4116(10.0962) | Error 0.0389(0.0339) Steps 1072(1034.64) | Grad Norm 5.1947(4.5398) | Total Time 0.00(0.00)\n",
      "Iter 17360 | Time 25.7580(25.9889) | Bit/dim 3.5302(3.5593) | Xent 0.1013(0.0990) | Loss 9.3466(9.9201) | Error 0.0356(0.0346) Steps 1066(1036.54) | Grad Norm 4.4252(4.7758) | Total Time 0.00(0.00)\n",
      "Iter 17370 | Time 25.0342(25.9223) | Bit/dim 3.5679(3.5588) | Xent 0.0981(0.0977) | Loss 9.4134(9.7903) | Error 0.0311(0.0335) Steps 1042(1038.97) | Grad Norm 6.5251(4.9254) | Total Time 0.00(0.00)\n",
      "Iter 17380 | Time 25.7325(25.9288) | Bit/dim 3.5804(3.5605) | Xent 0.0732(0.0977) | Loss 9.4348(9.6891) | Error 0.0267(0.0335) Steps 1030(1038.00) | Grad Norm 3.4389(4.9774) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0316 | Time 118.1898, Epoch Time 1567.7818(1505.8719), Bit/dim 3.5688(best: 3.5679), Xent 1.1630, Loss 4.1503, Error 0.2302(best: 0.2209)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17390 | Time 26.2242(25.9924) | Bit/dim 3.5841(3.5602) | Xent 0.1280(0.1001) | Loss 9.4782(10.5164) | Error 0.0533(0.0346) Steps 1018(1036.41) | Grad Norm 6.2060(4.9698) | Total Time 0.00(0.00)\n",
      "Iter 17400 | Time 26.6324(25.9846) | Bit/dim 3.5444(3.5595) | Xent 0.1231(0.1000) | Loss 9.4882(10.2282) | Error 0.0422(0.0350) Steps 1096(1042.25) | Grad Norm 4.3349(4.9327) | Total Time 0.00(0.00)\n",
      "Iter 17410 | Time 26.9768(26.0053) | Bit/dim 3.6066(3.5621) | Xent 0.1006(0.0992) | Loss 9.4659(10.0070) | Error 0.0356(0.0342) Steps 1024(1040.69) | Grad Norm 5.2956(5.0318) | Total Time 0.00(0.00)\n",
      "Iter 17420 | Time 25.2601(26.0343) | Bit/dim 3.5931(3.5642) | Xent 0.0927(0.0979) | Loss 9.4044(9.8525) | Error 0.0367(0.0345) Steps 1048(1041.35) | Grad Norm 4.0095(4.9961) | Total Time 0.00(0.00)\n",
      "Iter 17430 | Time 25.8101(26.0440) | Bit/dim 3.5522(3.5618) | Xent 0.0907(0.0977) | Loss 9.5545(9.7317) | Error 0.0344(0.0344) Steps 1066(1038.63) | Grad Norm 4.6702(4.8088) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0317 | Time 118.9098, Epoch Time 1569.8844(1507.7923), Bit/dim 3.5700(best: 3.5679), Xent 1.1489, Loss 4.1444, Error 0.2269(best: 0.2209)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17440 | Time 26.3784(25.9974) | Bit/dim 3.5806(3.5623) | Xent 0.0790(0.0956) | Loss 9.3097(10.6665) | Error 0.0311(0.0339) Steps 1078(1039.82) | Grad Norm 2.9320(4.5801) | Total Time 0.00(0.00)\n",
      "Iter 17450 | Time 26.1703(25.9611) | Bit/dim 3.5636(3.5613) | Xent 0.0868(0.0948) | Loss 9.3976(10.3375) | Error 0.0300(0.0333) Steps 1054(1037.68) | Grad Norm 3.6786(4.5215) | Total Time 0.00(0.00)\n",
      "Iter 17460 | Time 25.4835(25.9079) | Bit/dim 3.5618(3.5639) | Xent 0.0899(0.0962) | Loss 9.3068(10.1161) | Error 0.0278(0.0330) Steps 1042(1039.46) | Grad Norm 5.2897(4.5715) | Total Time 0.00(0.00)\n",
      "Iter 17470 | Time 26.2528(25.9373) | Bit/dim 3.5352(3.5577) | Xent 0.0594(0.0942) | Loss 9.1503(9.9108) | Error 0.0167(0.0330) Steps 1060(1041.13) | Grad Norm 3.0112(4.5036) | Total Time 0.00(0.00)\n",
      "Iter 17480 | Time 26.1328(25.9557) | Bit/dim 3.5556(3.5556) | Xent 0.0968(0.0932) | Loss 9.3561(9.7726) | Error 0.0333(0.0328) Steps 1048(1043.96) | Grad Norm 3.3430(4.2478) | Total Time 0.00(0.00)\n",
      "Iter 17490 | Time 26.3833(25.9354) | Bit/dim 3.5167(3.5569) | Xent 0.0728(0.0920) | Loss 9.3461(9.6852) | Error 0.0222(0.0321) Steps 1042(1044.60) | Grad Norm 3.9107(4.0525) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0318 | Time 118.2985, Epoch Time 1561.3974(1509.4004), Bit/dim 3.5662(best: 3.5679), Xent 1.1041, Loss 4.1183, Error 0.2277(best: 0.2209)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17500 | Time 26.9820(26.0002) | Bit/dim 3.5779(3.5584) | Xent 0.1034(0.0926) | Loss 9.4151(10.5227) | Error 0.0356(0.0320) Steps 1024(1042.60) | Grad Norm 3.8517(4.0921) | Total Time 0.00(0.00)\n",
      "Iter 17510 | Time 26.2476(25.9905) | Bit/dim 3.5603(3.5591) | Xent 0.0739(0.0930) | Loss 9.4594(10.2391) | Error 0.0267(0.0321) Steps 1042(1041.50) | Grad Norm 3.6995(4.0297) | Total Time 0.00(0.00)\n",
      "Iter 17520 | Time 25.8674(25.9037) | Bit/dim 3.5774(3.5583) | Xent 0.0815(0.0905) | Loss 9.3712(10.0129) | Error 0.0278(0.0310) Steps 1048(1041.29) | Grad Norm 5.4289(3.9951) | Total Time 0.00(0.00)\n",
      "Iter 17530 | Time 26.4481(26.0340) | Bit/dim 3.5501(3.5587) | Xent 0.0952(0.0914) | Loss 9.4008(9.8475) | Error 0.0311(0.0315) Steps 1036(1042.06) | Grad Norm 6.2987(4.1261) | Total Time 0.00(0.00)\n",
      "Iter 17540 | Time 26.8778(26.0843) | Bit/dim 3.5718(3.5576) | Xent 0.0799(0.0931) | Loss 9.4516(9.7365) | Error 0.0267(0.0323) Steps 1078(1042.58) | Grad Norm 5.8792(4.3633) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0319 | Time 118.3968, Epoch Time 1572.6429(1511.2977), Bit/dim 3.5715(best: 3.5662), Xent 1.1184, Loss 4.1307, Error 0.2263(best: 0.2209)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17550 | Time 26.1330(26.1753) | Bit/dim 3.5944(3.5592) | Xent 0.0919(0.0935) | Loss 9.4606(10.7188) | Error 0.0311(0.0326) Steps 1090(1045.17) | Grad Norm 4.3258(4.2860) | Total Time 0.00(0.00)\n",
      "Iter 17560 | Time 25.3054(26.0735) | Bit/dim 3.5396(3.5591) | Xent 0.0846(0.0935) | Loss 9.3683(10.3676) | Error 0.0244(0.0324) Steps 1000(1042.79) | Grad Norm 3.4200(4.3583) | Total Time 0.00(0.00)\n",
      "Iter 17570 | Time 25.7380(26.1387) | Bit/dim 3.5497(3.5572) | Xent 0.0854(0.0928) | Loss 9.3286(10.1189) | Error 0.0311(0.0320) Steps 1036(1043.55) | Grad Norm 2.5891(4.3741) | Total Time 0.00(0.00)\n",
      "Iter 17580 | Time 25.7854(26.2255) | Bit/dim 3.5762(3.5591) | Xent 0.0876(0.0929) | Loss 9.4488(9.9308) | Error 0.0289(0.0322) Steps 1054(1043.32) | Grad Norm 3.6025(4.2273) | Total Time 0.00(0.00)\n",
      "Iter 17590 | Time 26.0029(26.2269) | Bit/dim 3.5712(3.5597) | Xent 0.0895(0.0930) | Loss 9.3880(9.7870) | Error 0.0300(0.0323) Steps 1024(1041.62) | Grad Norm 5.0987(4.2508) | Total Time 0.00(0.00)\n",
      "Iter 17600 | Time 26.4998(26.2802) | Bit/dim 3.5701(3.5588) | Xent 0.1050(0.0935) | Loss 9.5098(9.6909) | Error 0.0344(0.0324) Steps 1048(1042.22) | Grad Norm 5.2118(4.2180) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0320 | Time 119.7945, Epoch Time 1583.0534(1513.4504), Bit/dim 3.5685(best: 3.5662), Xent 1.1183, Loss 4.1276, Error 0.2294(best: 0.2209)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17610 | Time 25.8685(26.2790) | Bit/dim 3.5924(3.5590) | Xent 0.0652(0.0918) | Loss 9.5458(10.5074) | Error 0.0189(0.0320) Steps 1066(1043.15) | Grad Norm 3.3721(4.1420) | Total Time 0.00(0.00)\n",
      "Iter 17620 | Time 25.3460(26.2265) | Bit/dim 3.5218(3.5581) | Xent 0.0931(0.0938) | Loss 9.2305(10.2225) | Error 0.0367(0.0328) Steps 1030(1045.16) | Grad Norm 5.7108(4.1202) | Total Time 0.00(0.00)\n",
      "Iter 17630 | Time 25.9514(26.1377) | Bit/dim 3.5656(3.5581) | Xent 0.1116(0.0943) | Loss 9.2923(10.0065) | Error 0.0467(0.0330) Steps 1024(1043.00) | Grad Norm 6.9578(4.3829) | Total Time 0.00(0.00)\n",
      "Iter 17640 | Time 26.7407(26.0799) | Bit/dim 3.5154(3.5558) | Xent 0.0746(0.0914) | Loss 9.1900(9.8399) | Error 0.0267(0.0319) Steps 988(1040.32) | Grad Norm 2.6159(4.4653) | Total Time 0.00(0.00)\n",
      "Iter 17650 | Time 25.8309(26.0838) | Bit/dim 3.5763(3.5609) | Xent 0.1056(0.0913) | Loss 9.4952(9.7495) | Error 0.0389(0.0318) Steps 1060(1044.45) | Grad Norm 3.6791(4.2816) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0321 | Time 119.8668, Epoch Time 1572.9676(1515.2359), Bit/dim 3.5681(best: 3.5662), Xent 1.1892, Loss 4.1627, Error 0.2356(best: 0.2209)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17660 | Time 25.7833(26.1635) | Bit/dim 3.5713(3.5616) | Xent 0.1095(0.0932) | Loss 9.5272(10.7389) | Error 0.0378(0.0323) Steps 1054(1046.41) | Grad Norm 7.8991(4.4606) | Total Time 0.00(0.00)\n",
      "Iter 17670 | Time 27.6128(26.3407) | Bit/dim 3.5442(3.5607) | Xent 0.0975(0.0916) | Loss 9.4670(10.4008) | Error 0.0311(0.0317) Steps 1072(1051.45) | Grad Norm 4.7004(4.3766) | Total Time 0.00(0.00)\n",
      "Iter 17680 | Time 25.4974(26.3406) | Bit/dim 3.5748(3.5611) | Xent 0.0980(0.0920) | Loss 9.3728(10.1474) | Error 0.0422(0.0325) Steps 1018(1051.75) | Grad Norm 5.7428(4.5847) | Total Time 0.00(0.00)\n",
      "Iter 17690 | Time 25.4857(26.2727) | Bit/dim 3.5615(3.5625) | Xent 0.0566(0.0917) | Loss 9.3802(9.9617) | Error 0.0189(0.0326) Steps 1048(1049.42) | Grad Norm 3.4104(4.6746) | Total Time 0.00(0.00)\n",
      "Iter 17700 | Time 26.6761(26.2231) | Bit/dim 3.5307(3.5598) | Xent 0.0988(0.0915) | Loss 9.4203(9.8211) | Error 0.0356(0.0322) Steps 1060(1051.74) | Grad Norm 2.9921(4.4938) | Total Time 0.00(0.00)\n",
      "Iter 17710 | Time 26.6759(26.2267) | Bit/dim 3.5434(3.5592) | Xent 0.1296(0.0918) | Loss 9.4964(9.7178) | Error 0.0367(0.0317) Steps 1096(1051.60) | Grad Norm 5.8784(4.4020) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0322 | Time 119.1721, Epoch Time 1584.5905(1517.3165), Bit/dim 3.5706(best: 3.5662), Xent 1.1481, Loss 4.1447, Error 0.2333(best: 0.2209)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17720 | Time 26.8055(26.2247) | Bit/dim 3.5884(3.5578) | Xent 0.0907(0.0925) | Loss 9.5590(10.5579) | Error 0.0278(0.0323) Steps 1078(1051.54) | Grad Norm 4.2640(4.2689) | Total Time 0.00(0.00)\n",
      "Iter 17730 | Time 26.6846(26.2635) | Bit/dim 3.5555(3.5596) | Xent 0.0841(0.0913) | Loss 9.5414(10.2613) | Error 0.0211(0.0316) Steps 1078(1048.88) | Grad Norm 4.0581(4.2785) | Total Time 0.00(0.00)\n",
      "Iter 17740 | Time 26.4233(26.2354) | Bit/dim 3.5344(3.5576) | Xent 0.0925(0.0916) | Loss 9.3578(10.0326) | Error 0.0311(0.0315) Steps 1048(1046.93) | Grad Norm 4.2484(4.2657) | Total Time 0.00(0.00)\n",
      "Iter 17750 | Time 27.0735(26.3046) | Bit/dim 3.5808(3.5574) | Xent 0.0937(0.0916) | Loss 9.4766(9.8726) | Error 0.0333(0.0316) Steps 1048(1046.40) | Grad Norm 4.5664(4.2372) | Total Time 0.00(0.00)\n",
      "Iter 17760 | Time 25.4459(26.2846) | Bit/dim 3.5808(3.5584) | Xent 0.0894(0.0919) | Loss 9.2893(9.7375) | Error 0.0311(0.0318) Steps 1036(1043.70) | Grad Norm 3.3561(4.2741) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0323 | Time 120.3026, Epoch Time 1584.3924(1519.3288), Bit/dim 3.5694(best: 3.5662), Xent 1.1355, Loss 4.1371, Error 0.2316(best: 0.2209)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17770 | Time 25.5487(26.2094) | Bit/dim 3.5355(3.5591) | Xent 0.0859(0.0951) | Loss 9.3218(10.6687) | Error 0.0344(0.0329) Steps 1036(1042.06) | Grad Norm 3.2413(4.6525) | Total Time 0.00(0.00)\n",
      "Iter 17780 | Time 26.9046(26.2816) | Bit/dim 3.5427(3.5589) | Xent 0.0710(0.0937) | Loss 9.2689(10.3336) | Error 0.0244(0.0332) Steps 1024(1041.12) | Grad Norm 2.9998(4.6207) | Total Time 0.00(0.00)\n",
      "Iter 17790 | Time 26.1863(26.2038) | Bit/dim 3.5789(3.5595) | Xent 0.1205(0.0948) | Loss 9.4670(10.0924) | Error 0.0400(0.0335) Steps 1030(1039.64) | Grad Norm 6.1761(4.6797) | Total Time 0.00(0.00)\n",
      "Iter 17800 | Time 25.1267(26.1122) | Bit/dim 3.5661(3.5616) | Xent 0.0870(0.0935) | Loss 9.5308(9.9235) | Error 0.0311(0.0336) Steps 1012(1039.23) | Grad Norm 4.6557(4.8234) | Total Time 0.00(0.00)\n",
      "Iter 17810 | Time 27.2154(26.1186) | Bit/dim 3.5502(3.5578) | Xent 0.0856(0.0947) | Loss 9.4881(9.7991) | Error 0.0322(0.0343) Steps 1048(1039.90) | Grad Norm 5.0230(4.8797) | Total Time 0.00(0.00)\n",
      "Iter 17820 | Time 26.3144(26.0367) | Bit/dim 3.5740(3.5579) | Xent 0.1060(0.0971) | Loss 9.4054(9.6864) | Error 0.0367(0.0350) Steps 1048(1039.15) | Grad Norm 4.6958(4.9279) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0324 | Time 121.3907, Epoch Time 1571.7433(1520.9012), Bit/dim 3.5704(best: 3.5662), Xent 1.1490, Loss 4.1449, Error 0.2271(best: 0.2209)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17830 | Time 25.9783(26.0265) | Bit/dim 3.5661(3.5608) | Xent 0.0682(0.0956) | Loss 9.4536(10.5670) | Error 0.0211(0.0342) Steps 1036(1039.84) | Grad Norm 3.4219(4.8156) | Total Time 0.00(0.00)\n",
      "Iter 17840 | Time 26.0618(26.0930) | Bit/dim 3.5467(3.5597) | Xent 0.1080(0.0948) | Loss 9.4453(10.2648) | Error 0.0367(0.0335) Steps 1000(1038.74) | Grad Norm 4.2539(4.6421) | Total Time 0.00(0.00)\n",
      "Iter 17850 | Time 26.2287(26.1474) | Bit/dim 3.5549(3.5596) | Xent 0.0916(0.0936) | Loss 9.4349(10.0488) | Error 0.0300(0.0325) Steps 1036(1040.08) | Grad Norm 3.0545(4.4722) | Total Time 0.00(0.00)\n",
      "Iter 17860 | Time 26.3696(26.1633) | Bit/dim 3.5404(3.5574) | Xent 0.0849(0.0918) | Loss 9.5797(9.8940) | Error 0.0378(0.0322) Steps 1084(1045.74) | Grad Norm 4.9764(4.3178) | Total Time 0.00(0.00)\n",
      "Iter 17870 | Time 27.2474(26.2448) | Bit/dim 3.5640(3.5573) | Xent 0.0975(0.0912) | Loss 9.4195(9.7653) | Error 0.0400(0.0320) Steps 1048(1046.22) | Grad Norm 4.9595(4.3657) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0325 | Time 119.6500, Epoch Time 1581.3196(1522.7138), Bit/dim 3.5698(best: 3.5662), Xent 1.1650, Loss 4.1523, Error 0.2327(best: 0.2209)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17880 | Time 26.8963(26.2829) | Bit/dim 3.5262(3.5560) | Xent 0.0754(0.0906) | Loss 9.3631(10.7455) | Error 0.0267(0.0319) Steps 1078(1050.80) | Grad Norm 5.6587(4.3671) | Total Time 0.00(0.00)\n",
      "Iter 17890 | Time 25.8306(26.1486) | Bit/dim 3.5582(3.5563) | Xent 0.0923(0.0903) | Loss 9.3844(10.3936) | Error 0.0278(0.0315) Steps 1042(1049.38) | Grad Norm 4.4777(4.3757) | Total Time 0.00(0.00)\n",
      "Iter 17900 | Time 26.5251(26.1880) | Bit/dim 3.5417(3.5578) | Xent 0.0960(0.0915) | Loss 9.3567(10.1416) | Error 0.0411(0.0326) Steps 1036(1045.64) | Grad Norm 6.3562(4.7960) | Total Time 0.00(0.00)\n",
      "Iter 17910 | Time 26.3170(26.1799) | Bit/dim 3.5306(3.5575) | Xent 0.1060(0.0908) | Loss 9.3995(9.9563) | Error 0.0400(0.0322) Steps 1030(1046.83) | Grad Norm 3.8969(4.5801) | Total Time 0.00(0.00)\n",
      "Iter 17920 | Time 25.7905(26.2302) | Bit/dim 3.5551(3.5569) | Xent 0.0932(0.0901) | Loss 9.2122(9.8236) | Error 0.0433(0.0325) Steps 1036(1049.99) | Grad Norm 5.3709(4.4372) | Total Time 0.00(0.00)\n",
      "Iter 17930 | Time 26.2094(26.2198) | Bit/dim 3.5412(3.5567) | Xent 0.0671(0.0896) | Loss 9.4105(9.7207) | Error 0.0211(0.0319) Steps 1024(1050.33) | Grad Norm 2.6540(4.4878) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0326 | Time 120.1729, Epoch Time 1579.0317(1524.4033), Bit/dim 3.5688(best: 3.5662), Xent 1.1585, Loss 4.1480, Error 0.2268(best: 0.2209)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17940 | Time 26.0081(26.2742) | Bit/dim 3.5723(3.5572) | Xent 0.1176(0.0887) | Loss 9.4843(10.5595) | Error 0.0422(0.0316) Steps 1066(1052.60) | Grad Norm 4.5381(4.5034) | Total Time 0.00(0.00)\n",
      "Iter 17950 | Time 26.7242(26.2966) | Bit/dim 3.5400(3.5553) | Xent 0.1110(0.0901) | Loss 9.5068(10.2746) | Error 0.0433(0.0322) Steps 1060(1055.42) | Grad Norm 4.6051(4.3369) | Total Time 0.00(0.00)\n",
      "Iter 17960 | Time 25.3049(26.3016) | Bit/dim 3.5367(3.5553) | Xent 0.0901(0.0897) | Loss 9.4740(10.0633) | Error 0.0344(0.0329) Steps 1054(1056.61) | Grad Norm 4.8337(4.2562) | Total Time 0.00(0.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_rl.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_6_run2 --resume ../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_6_run2/current_checkpt.pth --seed 2 --lr 0.0001 --conditional True --controlled_tol False --train_mode semisup --log_freq 10 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5 --scale_fac 1.0 --scale_std 6.0\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
