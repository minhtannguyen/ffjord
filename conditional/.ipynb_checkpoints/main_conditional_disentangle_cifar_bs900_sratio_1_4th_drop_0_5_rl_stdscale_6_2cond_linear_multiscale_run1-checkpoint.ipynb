{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0,1,2,3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_rl_2cond_multiscale.py\n",
      "from __future__ import print_function\n",
      "\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import torch.utils.data as data\n",
      "from torch.utils.data import Dataset\n",
      "\n",
      "from PIL import Image\n",
      "import os.path\n",
      "import errno\n",
      "import codecs\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time, count_nfe_gate\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "GATES = [\"cnn1\", \"cnn2\", \"rnn\"]\n",
      "\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"colormnist\", \"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--gate', type=str, default='cnn1', choices=GATES)\n",
      "parser.add_argument('--scale', type=float, default=1.0)\n",
      "parser.add_argument('--scale_fac', type=float, default=1.0)\n",
      "parser.add_argument('--scale_std', type=float, default=1.0)\n",
      "parser.add_argument('--eta', default=0.1, type=float,\n",
      "                        help='tuning parameter that allows us to trade-off the competing goals of' \n",
      "                                'minimizing the prediction loss and maximizing the gate rewards ')\n",
      "parser.add_argument('--rl-weight', default=0.01, type=float,\n",
      "                        help='rl weight')\n",
      "\n",
      "parser.add_argument('--gamma', default=0.99, type=float,\n",
      "                        help='discount factor, default: (0.99)')\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=500)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "parser.add_argument(\"--annealing_std\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--y_class\", type=int, default=10)\n",
      "parser.add_argument(\"--y_color\", type=int, default=10)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "parser.add_argument(\"--cond_nn\", choices=[\"linear\", \"mlp\"], type=str, default=\"linear\")\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "import lib.odenvp_conditional_rl_2cond_multiscale as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "class ColorMNIST(data.Dataset):\n",
      "    \"\"\"\n",
      "    ColorMNIST\n",
      "    \"\"\"\n",
      "    urls = [\n",
      "        'http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz',\n",
      "    ]\n",
      "    raw_folder = 'raw'\n",
      "    processed_folder = 'processed'\n",
      "    training_file = 'training.pt'\n",
      "    test_file = 'test.pt'\n",
      "\n",
      "    def __init__(self, root, train=True, transform=None, target_transform=None, download=False):\n",
      "        self.root = os.path.expanduser(root)\n",
      "        self.transform = transform\n",
      "        self.target_transform = target_transform\n",
      "        self.train = train  # training set or test set\n",
      "\n",
      "        if download:\n",
      "            self.download()\n",
      "\n",
      "        if not self._check_exists():\n",
      "            raise RuntimeError('Dataset not found.' +\n",
      "                               ' You can use download=True to download it')\n",
      "\n",
      "        if self.train:\n",
      "            self.train_data, self.train_labels = torch.load(\n",
      "                os.path.join(self.root, self.processed_folder, self.training_file))\n",
      "            \n",
      "            self.train_data = np.tile(self.train_data[:, :, :, np.newaxis], 3)\n",
      "        else:\n",
      "            self.test_data, self.test_labels = torch.load(\n",
      "                os.path.join(self.root, self.processed_folder, self.test_file))\n",
      "            \n",
      "            self.test_data = np.tile(self.test_data[:, :, :, np.newaxis], 3)\n",
      "        \n",
      "        self.pallette = [[31, 119, 180],\n",
      "                         [255, 127, 14],\n",
      "                         [44, 160, 44],\n",
      "                         [214, 39, 40],\n",
      "                         [148, 103, 189],\n",
      "                         [140, 86, 75],\n",
      "                         [227, 119, 194],\n",
      "                         [127, 127, 127],\n",
      "                         [188, 189, 34],\n",
      "                         [23, 190, 207]]\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        \"\"\"\n",
      "        Args:\n",
      "            index (int): Index\n",
      "\n",
      "        Returns:\n",
      "            tuple: (image, target) where target is index of the target class.\n",
      "        \"\"\"\n",
      "        if self.train:\n",
      "            img, target = self.train_data[index].copy(), self.train_labels[index]\n",
      "        else:\n",
      "            img, target = self.test_data[index].copy(), self.test_labels[index]\n",
      "        \n",
      "        # doing this so that it is consistent with all other datasets\n",
      "        # to return a PIL Image\n",
      "        y_color_digit = np.random.randint(0, args.y_color)\n",
      "        c_digit = self.pallette[y_color_digit]\n",
      "        \n",
      "        img[:, :, 0] = img[:, :, 0] / 255 * c_digit[0]\n",
      "        img[:, :, 1] = img[:, :, 1] / 255 * c_digit[1]\n",
      "        img[:, :, 2] = img[:, :, 2] / 255 * c_digit[2]\n",
      "        \n",
      "        img = Image.fromarray(img)\n",
      "\n",
      "        if self.transform is not None:\n",
      "            img = self.transform(img)\n",
      "\n",
      "        if self.target_transform is not None:\n",
      "            target = self.target_transform(target)\n",
      "\n",
      "        return img, [target,torch.from_numpy(np.array(y_color_digit))]\n",
      "\n",
      "    def __len__(self):\n",
      "        if self.train:\n",
      "            return len(self.train_data)\n",
      "        else:\n",
      "            return len(self.test_data)\n",
      "\n",
      "    def _check_exists(self):\n",
      "        return os.path.exists(os.path.join(self.root, self.processed_folder, self.training_file)) and \\\n",
      "            os.path.exists(os.path.join(self.root, self.processed_folder, self.test_file))\n",
      "\n",
      "    def download(self):\n",
      "        \"\"\"Download the MNIST data if it doesn't exist in processed_folder already.\"\"\"\n",
      "        from six.moves import urllib\n",
      "        import gzip\n",
      "\n",
      "        if self._check_exists():\n",
      "            return\n",
      "\n",
      "        # download files\n",
      "        try:\n",
      "            os.makedirs(os.path.join(self.root, self.raw_folder))\n",
      "            os.makedirs(os.path.join(self.root, self.processed_folder))\n",
      "        except OSError as e:\n",
      "            if e.errno == errno.EEXIST:\n",
      "                pass\n",
      "            else:\n",
      "                raise\n",
      "\n",
      "        for url in self.urls:\n",
      "            print('Downloading ' + url)\n",
      "            data = urllib.request.urlopen(url)\n",
      "            filename = url.rpartition('/')[2]\n",
      "            file_path = os.path.join(self.root, self.raw_folder, filename)\n",
      "            with open(file_path, 'wb') as f:\n",
      "                f.write(data.read())\n",
      "            with open(file_path.replace('.gz', ''), 'wb') as out_f, \\\n",
      "                    gzip.GzipFile(file_path) as zip_f:\n",
      "                out_f.write(zip_f.read())\n",
      "            os.unlink(file_path)\n",
      "\n",
      "        # process and save as torch files\n",
      "        print('Processing...')\n",
      "\n",
      "        training_set = (\n",
      "            read_image_file(os.path.join(self.root, self.raw_folder, 'train-images-idx3-ubyte')),\n",
      "            read_label_file(os.path.join(self.root, self.raw_folder, 'train-labels-idx1-ubyte'))\n",
      "        )\n",
      "        test_set = (\n",
      "            read_image_file(os.path.join(self.root, self.raw_folder, 't10k-images-idx3-ubyte')),\n",
      "            read_label_file(os.path.join(self.root, self.raw_folder, 't10k-labels-idx1-ubyte'))\n",
      "        )\n",
      "        with open(os.path.join(self.root, self.processed_folder, self.training_file), 'wb') as f:\n",
      "            torch.save(training_set, f)\n",
      "        with open(os.path.join(self.root, self.processed_folder, self.test_file), 'wb') as f:\n",
      "            torch.save(test_set, f)\n",
      "\n",
      "        print('Done!')\n",
      "\n",
      "    def __repr__(self):\n",
      "        fmt_str = 'Dataset ' + self.__class__.__name__ + '\\n'\n",
      "        fmt_str += '    Number of datapoints: {}\\n'.format(self.__len__())\n",
      "        tmp = 'train' if self.train is True else 'test'\n",
      "        fmt_str += '    Split: {}\\n'.format(tmp)\n",
      "        fmt_str += '    Root Location: {}\\n'.format(self.root)\n",
      "        tmp = '    Transforms (if any): '\n",
      "        fmt_str += '{0}{1}\\n'.format(tmp, self.transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
      "        tmp = '    Target Transforms (if any): '\n",
      "        fmt_str += '{0}{1}'.format(tmp, self.target_transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
      "        return fmt_str\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "        \n",
      "def update_scale_std(model, epoch):\n",
      "    epoch_frac = 1.0 - float(epoch - 1) / max(args.num_epochs + 1, 1)\n",
      "    scale_std = args.scale_std * epoch_frac\n",
      "    model.set_scale_std(scale_std)\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    if args.data == \"colormnist\":\n",
      "        im_dim = 3\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = ColorMNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = ColorMNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, y_color, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "    y_onehot_color = thops.onehot(y_color, num_classes=model.module.y_color).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, z_unsup, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "    \n",
      "    z_unsup = torch.cat(z_unsup, 1)\n",
      "    \n",
      "    z_sup_class = [o[:,:int(np.prod(o.size()[1:])*0.5)] for o in z]\n",
      "    z_sup_class = torch.cat(z_sup_class,1)\n",
      "    \n",
      "    z_sup_color = [o[:,int(np.prod(o.size()[1:])*0.5):] for o in z]\n",
      "    z_sup_color = torch.cat(z_sup_color,1)\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "    mean_color, logs_color = model.module._prior_color(y_onehot_color)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z_sup_class).view(-1,1)  # logp(z)_sup\n",
      "    logpz_color_sup = modules.GaussianDiag.logp(mean_color, logs_color, z_sup_color).view(-1,1)  # logp(z)_color_sup\n",
      "    logpz_unsup = standard_normal_logprob(z_unsup).view(z_unsup.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_color_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        z_sup_class = model.module.dropout(z_sup_class)\n",
      "        z_sup_color = model.module.dropout_color(z_sup_color)\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(z_sup_class)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "    \n",
      "    y_logits_color = model.module.project_color(z_sup_color)\n",
      "    loss_xent_color = model.module.loss_class(y_logits_color, y_color.to(x.get_device()))\n",
      "    y_color_predicted = np.argmax(y_logits_color.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, loss_xent_color, y_predicted, y_color_predicted, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol, \"scale\": args.scale, \"scale_fac\": args.scale_fac, \"scale_std\": args.scale_std, \"gate\": args.gate},\n",
      "            condition_ratio=args.condition_ratio * 2.,\n",
      "            dropout_rate=args.dropout_rate,\n",
      "            cond_nn=args.cond_nn,\n",
      "            y_class = args.y_class,\n",
      "            y_color = args.y_color)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    xent_color_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    error_color_meter = utils.RunningAverageMeter(0.97)\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        xent_color_meter.set(checkpt['xent_train_color'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        error_color_meter.set(checkpt['error_train_color'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        \n",
      "        fixed_y_color = torch.from_numpy(np.arange(model.module.y_color)).repeat(model.module.y_color).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot_color = thops.onehot(fixed_y_color, num_classes=model.module.y_color)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            mean_color, logs_color = model.module._prior_color(fixed_y_onehot_color)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_color_sup = modules.GaussianDiag.sample(mean_color, logs_color)\n",
      "            dim_unsup = np.prod(data_shape) - np.prod(fixed_z_sup.shape[1:]) - np.prod(fixed_z_color_sup.shape[1:])\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            \n",
      "            a_sup = fixed_z_sup.shape[1] // (2**(model.module.n_scale - 1))\n",
      "            a_color_sup = fixed_z_color_sup.shape[1] // (2**(model.module.n_scale - 1))\n",
      "            a_unsup = fixed_z_unsup.shape[1] // (2**(model.module.n_scale - 1))\n",
      "            \n",
      "            fixed_z = []\n",
      "            start_sup = 0; start_color_sup = 0; start_unsup = 0\n",
      "            for ns in range(model.module.n_scale, 1, -1):\n",
      "                end_sup = start_sup + (2**(ns-2))*a_sup\n",
      "                end_color_sup = start_color_sup + (2**(ns-2))*a_color_sup\n",
      "                end_unsup = start_unsup + (2**(ns-2))*a_unsup\n",
      "                \n",
      "                fixed_z.append(fixed_z_sup[:,start_sup:end_sup])\n",
      "                fixed_z.append(fixed_z_color_sup[:,start_color_sup:end_color_sup])\n",
      "                fixed_z.append(fixed_z_unsup[:,start_unsup:end_unsup])\n",
      "                \n",
      "                start_sup = end_sup; start_color_sup = end_color_sup; start_unsup = end_unsup\n",
      "            \n",
      "            end_sup = start_sup + a_sup\n",
      "            end_color_sup = start_color_sup + a_color_sup\n",
      "            end_unsup = start_unsup + a_unsup\n",
      "            \n",
      "            fixed_z.append(fixed_z_sup[:,start_sup:end_sup])\n",
      "            fixed_z.append(fixed_z_color_sup[:,start_color_sup:end_color_sup])\n",
      "            fixed_z.append(fixed_z_unsup[:,start_unsup:end_unsup])\n",
      "            \n",
      "            # for i_z in range(len(fixed_z)): print(fixed_z[i_z].shape)\n",
      "            \n",
      "            fixed_z = torch.cat(fixed_z,1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    best_error_score_color = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        if args.annealing_std:\n",
      "            update_scale_std(model.module, epoch)\n",
      "            \n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y_all) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            \n",
      "            y = y_all[0]\n",
      "            y_color = y_all[1]\n",
      "            \n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, loss_xent_color, y_predicted, y_color_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, y_color, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * 0.5 * (loss_xent + loss_xent_color)\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  0.5 * (loss_xent + loss_xent_color)\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy()) \n",
      "                error_score_color = 1. - np.mean(y_color_predicted.astype(int) == y_color.numpy())\n",
      "                \n",
      "            else:\n",
      "                loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, loss_xent_color, error_score, error_score_color = loss, 0., 0., 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            # re-weight the gate rewards\n",
      "            normalized_eta = args.eta / len(logp_actions)\n",
      "            \n",
      "            # collect cumulative future rewards\n",
      "            R = - loss\n",
      "            cum_rewards = []\n",
      "            for r in nfe[::-1]:\n",
      "                R = -normalized_eta * r.view(-1,1) + args.gamma * R\n",
      "                cum_rewards.insert(0,R)\n",
      "            \n",
      "            # apply REINFORCE\n",
      "            rl_loss = 0\n",
      "            for lpa, r in zip(logp_actions, cum_rewards):\n",
      "                rl_loss = rl_loss - lpa.view(-1,1) * args.rl_weight * r\n",
      "                \n",
      "            loss = loss + rl_loss.mean()\n",
      "            \n",
      "            loss.backward()\n",
      "            \n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "                xent_color_meter.update(loss_xent_color.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "                xent_color_meter.update(loss_xent_color)\n",
      "            error_meter.update(error_score)\n",
      "            error_color_meter.update(error_score_color)\n",
      "            steps_meter.update(count_nfe_gate(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            for idx in range(len(model.module.transforms)):\n",
      "                for layer in model.module.transforms[idx].chain:\n",
      "                    if hasattr(layer, 'atol'):\n",
      "                        layer.odefunc.after_odeint()\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('xent_color', {'train_iter': xent_color_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('error_color', {'train_iter': error_color_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'train': atol[tol_indx].mean()}, itr)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'train': rtol[tol_indx].mean()}, itr)\n",
      "                    \n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Xent Color {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) | Error Color {:.4f}({:.4f}) |\"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, xent_color_meter.val, xent_color_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, error_color_meter.val, error_color_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent_color', {'train_epoch': xent_color_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('error_color', {'train_epoch': error_color_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses_xent_color = []; losses = []\n",
      "                total_correct = 0\n",
      "                total_correct_color = 0\n",
      "                \n",
      "                for (x, y_all) in test_loader:\n",
      "                    y = y_all[0]\n",
      "                    y_color = y_all[1]\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, loss_xent_color, y_predicted, y_color_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, y_color, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * 0.5 * (loss_xent + loss_xent_color)\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  0.5 * (loss_xent + loss_xent_color)\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                        total_correct_color += np.sum(y_color_predicted.astype(int) == y_color.numpy())\n",
      "                    else:\n",
      "                        loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent, loss_xent_color = loss, 0., 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                        losses_xent_color.append(loss_xent_color.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                        losses_xent_color.append(loss_xent_color)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss_xent_color = np.mean(losses_xent_color); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                error_score_color =  1. - total_correct_color / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('xent_color', {'validation': loss_xent_color}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                writer.add_scalars('error_color', {'validation': error_score_color}, epoch)\n",
      "                \n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Xent Color {:.4f}. Loss {:.4f}, Error {:.4f}(best: {:.4f}), Error Color {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss_xent_color, loss, error_score, best_error_score, error_score_color, best_error_score_color)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"error_color\": error_score_color,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"xent_color\": loss_xent_color,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"best_error_score_color\": best_error_score_color,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"error_train_color\": error_color_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"xent_train_color\": xent_color_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"error_color\": error_score_color,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"xent_color\": loss_xent_color,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"best_error_score_color\": best_error_score_color,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"error_train_color\": error_color_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"xent_train_color\": xent_color_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"error_color\": error_score_color,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"xent_color\": loss_xent_color,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"best_error_score_color\": best_error_score_color,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"error_train_color\": error_color_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"xent_train_color\": xent_color_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"error_color\": error_score_color,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"xent_color\": loss_xent_color,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"best_error_score_color\": best_error_score_color,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"error_train_color\": error_color_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"xent_train_color\": xent_color_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "                    if error_score_color < best_error_score_color:\n",
      "                        best_error_score_color = error_score_color\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"error_color\": error_score_color,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"xent_color\": loss_xent_color,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"best_error_score_color\": best_error_score_color,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"error_train_color\": error_color_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"xent_train_color\": xent_color_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_color_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples, atol, rtol, logp_actions, nfe = model(fixed_z, reverse=True)\n",
      "            generated_samples = generated_samples.view(-1, *data_shape)\n",
      "            for tol_indx in range(len(atol)):\n",
      "                writer.add_scalars('atol_gen_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                writer.add_scalars('rtol_gen_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, annealing_std=False, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, cond_nn='linear', condition_ratio=0.16667, conditional=True, controlled_tol=False, conv=True, data='colormnist', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, eta=0.1, gamma=0.99, gate='cnn1', imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=500, parallel=False, rademacher=True, residual=False, resume=None, rl_weight=0.01, rtol=1e-05, save='../experiments_published/cnf_conditional_disentangle_colormnist_bs900_sratio_1_6th_drop_0_5_rl_stdscale_6_2cond_linear_multiscale_run1', scale=1.0, scale_fac=1.0, scale_std=6.0, seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5, y_class=10, y_color=10)\n",
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=784, bias=True)\n",
      "  (project_ycond_color): LinearZeros(in_features=10, out_features=784, bias=True)\n",
      "  (project_class): LinearZeros(in_features=392, out_features=10, bias=True)\n",
      "  (project_color): LinearZeros(in_features=392, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      "  (dropout_color): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 936992\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc_rl.py:291: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Iter 0010 | Time 6.8131(21.8142) | Bit/dim 25.4582(27.3895) | Xent 2.2921(2.3014) | Xent Color 2.3009(2.3024) | Loss 48.6393(52.2152) | Error 0.8967(0.8937) | Error Color 0.9067(0.8900) |Steps 296(300.72) | Grad Norm 260.9923(277.4898) | Total Time 0.00(0.00)\n",
      "Iter 0020 | Time 7.0789(18.0091) | Bit/dim 20.1441(26.1201) | Xent 2.2660(2.2955) | Xent Color 2.2957(2.3011) | Loss 39.0890(49.9290) | Error 0.8778(0.8909) | Error Color 0.9056(0.8919) |Steps 314(301.57) | Grad Norm 216.4436(266.8441) | Total Time 0.00(0.00)\n",
      "Iter 0030 | Time 7.7626(15.2176) | Bit/dim 13.9102(23.6009) | Xent 2.2252(2.2825) | Xent Color 2.2835(2.2979) | Loss 28.1360(45.3539) | Error 0.7878(0.8804) | Error Color 0.8322(0.8839) |Steps 326(305.79) | Grad Norm 160.4539(245.0956) | Total Time 0.00(0.00)\n",
      "Iter 0040 | Time 8.7521(13.3516) | Bit/dim 8.9848(20.2615) | Xent 2.1734(2.2608) | Xent Color 2.2675(2.2923) | Loss 19.1395(39.3464) | Error 0.4233(0.8070) | Error Color 0.8189(0.8613) |Steps 362(314.71) | Grad Norm 96.6397(213.1935) | Total Time 0.00(0.00)\n",
      "Iter 0050 | Time 9.1158(12.1722) | Bit/dim 6.7105(16.9009) | Xent 2.1271(2.2303) | Xent Color 2.2620(2.2853) | Loss 14.8359(33.2974) | Error 0.3600(0.6902) | Error Color 0.8822(0.8650) |Steps 374(328.78) | Grad Norm 33.6366(172.6646) | Total Time 0.00(0.00)\n",
      "Iter 0060 | Time 9.5120(11.4461) | Bit/dim 6.1829(14.1257) | Xent 2.0614(2.1933) | Xent Color 2.2402(2.2761) | Loss 14.0014(28.2932) | Error 0.3600(0.6050) | Error Color 0.8111(0.8577) |Steps 398(345.97) | Grad Norm 25.4861(133.3968) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0001 | Time 56.4185, Epoch Time 637.4562(637.4562), Bit/dim 5.7450(best: inf), Xent 2.0162, Xent Color 2.2280. Loss 6.8060, Error 0.2817(best: inf), Error Color 0.7567(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0070 | Time 8.9102(10.8551) | Bit/dim 5.5752(11.9312) | Xent 2.0087(2.1513) | Xent Color 2.2229(2.2646) | Loss 12.6770(24.7821) | Error 0.3322(0.5351) | Error Color 0.7667(0.8378) |Steps 368(355.65) | Grad Norm 18.1616(104.3880) | Total Time 0.00(0.00)\n",
      "Iter 0080 | Time 8.6671(10.3222) | Bit/dim 4.9786(10.1766) | Xent 1.9531(2.1054) | Xent Color 2.1958(2.2508) | Loss 11.6287(21.4502) | Error 0.3222(0.4863) | Error Color 0.7389(0.8143) |Steps 356(358.88) | Grad Norm 14.4842(80.7295) | Total Time 0.00(0.00)\n",
      "Iter 0090 | Time 9.0614(9.9197) | Bit/dim 4.6074(8.7689) | Xent 1.9167(2.0599) | Xent Color 2.1809(2.2363) | Loss 10.9164(18.7810) | Error 0.3100(0.4454) | Error Color 0.7467(0.7984) |Steps 362(361.72) | Grad Norm 13.4443(63.4012) | Total Time 0.00(0.00)\n",
      "Iter 0100 | Time 8.7929(9.6558) | Bit/dim 4.1948(7.6160) | Xent 1.8950(2.0192) | Xent Color 2.1566(2.2200) | Loss 10.1738(16.6252) | Error 0.3178(0.4155) | Error Color 0.6433(0.7731) |Steps 386(367.38) | Grad Norm 9.0198(49.4216) | Total Time 0.00(0.00)\n",
      "Iter 0110 | Time 8.7487(9.4792) | Bit/dim 3.7176(6.6534) | Xent 1.8891(1.9845) | Xent Color 2.1569(2.2036) | Loss 9.2092(14.8224) | Error 0.3244(0.3910) | Error Color 0.6600(0.7469) |Steps 350(371.15) | Grad Norm 7.9718(38.7327) | Total Time 0.00(0.00)\n",
      "Iter 0120 | Time 8.7566(9.3596) | Bit/dim 3.3801(5.8344) | Xent 1.8857(1.9589) | Xent Color 2.1357(2.1872) | Loss 8.6636(13.2937) | Error 0.3511(0.3773) | Error Color 0.6978(0.7312) |Steps 380(375.27) | Grad Norm 8.3005(30.7528) | Total Time 0.00(0.00)\n",
      "Iter 0130 | Time 8.8704(9.2940) | Bit/dim 3.0875(5.1394) | Xent 1.9193(1.9469) | Xent Color 2.1161(2.1706) | Loss 8.1980(12.0050) | Error 0.3789(0.3752) | Error Color 0.7044(0.7220) |Steps 374(376.74) | Grad Norm 6.4289(24.5734) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0002 | Time 55.5650, Epoch Time 670.3204(638.4421), Bit/dim 2.9644(best: 5.7450), Xent 1.9192, Xent Color 2.1053. Loss 3.9705, Error 0.3146(best: 0.2817), Error Color 0.6533(best: 0.7567)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0140 | Time 9.4987(9.3039) | Bit/dim 2.7771(4.5464) | Xent 1.9458(1.9447) | Xent Color 2.0925(2.1547) | Loss 7.6592(11.3388) | Error 0.3978(0.3794) | Error Color 0.6878(0.7147) |Steps 392(379.70) | Grad Norm 5.5311(19.6535) | Total Time 0.00(0.00)\n",
      "Iter 0150 | Time 9.5889(9.3501) | Bit/dim 2.6301(4.0588) | Xent 1.9971(1.9525) | Xent Color 2.0720(2.1362) | Loss 7.4579(10.3445) | Error 0.4533(0.3914) | Error Color 0.7111(0.7108) |Steps 398(384.62) | Grad Norm 4.3007(15.7374) | Total Time 0.00(0.00)\n",
      "Iter 0160 | Time 9.4354(9.3834) | Bit/dim 2.4867(3.6580) | Xent 1.9983(1.9642) | Xent Color 2.0343(2.1148) | Loss 7.1562(9.5323) | Error 0.4344(0.4072) | Error Color 0.6544(0.7038) |Steps 410(388.56) | Grad Norm 3.2602(12.5690) | Total Time 0.00(0.00)\n",
      "Iter 0170 | Time 9.7575(9.3971) | Bit/dim 2.3964(3.3322) | Xent 2.0035(1.9756) | Xent Color 2.0057(2.0902) | Loss 7.0060(8.8742) | Error 0.4833(0.4246) | Error Color 0.6833(0.6978) |Steps 422(391.03) | Grad Norm 2.6324(10.0409) | Total Time 0.00(0.00)\n",
      "Iter 0180 | Time 9.5698(9.4185) | Bit/dim 2.3042(3.0706) | Xent 2.0179(1.9873) | Xent Color 1.9600(2.0610) | Loss 6.8668(8.3477) | Error 0.4844(0.4443) | Error Color 0.6389(0.6877) |Steps 416(394.74) | Grad Norm 2.8534(8.0712) | Total Time 0.00(0.00)\n",
      "Iter 0190 | Time 9.6377(9.5074) | Bit/dim 2.2427(2.8627) | Xent 2.0305(1.9969) | Xent Color 1.9280(2.0279) | Loss 6.6569(7.9307) | Error 0.5267(0.4606) | Error Color 0.6378(0.6755) |Steps 422(397.95) | Grad Norm 2.0524(6.5548) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0003 | Time 57.3001, Epoch Time 708.3002(640.5378), Bit/dim 2.2263(best: 2.9644), Xent 2.0113, Xent Color 1.8375. Loss 3.1885, Error 0.4179(best: 0.2817), Error Color 0.5979(best: 0.6533)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0200 | Time 9.4446(9.5368) | Bit/dim 2.2219(2.6969) | Xent 2.0355(2.0051) | Xent Color 1.8429(1.9881) | Loss 6.6826(8.1203) | Error 0.5511(0.4776) | Error Color 0.6011(0.6594) |Steps 422(398.88) | Grad Norm 1.6885(5.3302) | Total Time 0.00(0.00)\n",
      "Iter 0210 | Time 9.3184(9.5311) | Bit/dim 2.2014(2.5677) | Xent 1.9985(2.0080) | Xent Color 1.7877(1.9436) | Loss 6.3978(7.7089) | Error 0.5078(0.4885) | Error Color 0.6033(0.6461) |Steps 386(399.64) | Grad Norm 2.1225(4.5015) | Total Time 0.00(0.00)\n",
      "Iter 0220 | Time 9.9019(9.5606) | Bit/dim 2.1667(2.4641) | Xent 1.9917(2.0046) | Xent Color 1.7269(1.8914) | Loss 6.3685(7.3812) | Error 0.5167(0.4954) | Error Color 0.5967(0.6334) |Steps 422(399.71) | Grad Norm 2.8609(4.0897) | Total Time 0.00(0.00)\n",
      "Iter 0230 | Time 9.9716(9.5831) | Bit/dim 2.1393(2.3855) | Xent 1.9595(1.9971) | Xent Color 1.6654(1.8311) | Loss 6.3934(7.1270) | Error 0.4967(0.5002) | Error Color 0.6078(0.6175) |Steps 428(403.63) | Grad Norm 4.9944(3.8103) | Total Time 0.00(0.00)\n",
      "Iter 0240 | Time 9.3625(9.5036) | Bit/dim 2.1324(2.3258) | Xent 1.9350(1.9850) | Xent Color 1.5580(1.7666) | Loss 6.2810(6.9263) | Error 0.4933(0.5008) | Error Color 0.5333(0.6024) |Steps 386(401.25) | Grad Norm 6.7557(4.2858) | Total Time 0.00(0.00)\n",
      "Iter 0250 | Time 9.7333(9.4449) | Bit/dim 2.1645(2.2779) | Xent 1.8976(1.9641) | Xent Color 1.4957(1.7073) | Loss 6.3068(6.7453) | Error 0.4933(0.4946) | Error Color 0.5078(0.5834) |Steps 422(401.79) | Grad Norm 6.9715(6.6051) | Total Time 0.00(0.00)\n",
      "Iter 0260 | Time 9.6570(9.4483) | Bit/dim 2.1516(2.2426) | Xent 1.8553(1.9383) | Xent Color 1.5161(1.6538) | Loss 6.2217(6.6163) | Error 0.4389(0.4846) | Error Color 0.5422(0.5592) |Steps 404(403.30) | Grad Norm 30.1113(10.0107) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0004 | Time 56.6799, Epoch Time 704.7434(642.4640), Bit/dim 2.1525(best: 2.2263), Xent 1.7857, Xent Color 1.4862. Loss 2.9704, Error 0.3274(best: 0.2817), Error Color 0.5404(best: 0.5979)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0270 | Time 9.3010(9.4405) | Bit/dim 2.1189(2.2145) | Xent 1.8095(1.9071) | Xent Color 1.4391(1.6006) | Loss 6.1573(6.9676) | Error 0.4333(0.4736) | Error Color 0.4500(0.5319) |Steps 404(402.78) | Grad Norm 19.2212(12.9178) | Total Time 0.00(0.00)\n",
      "Iter 0280 | Time 9.1486(9.4171) | Bit/dim 2.1245(2.1946) | Xent 1.7375(1.8677) | Xent Color 1.3112(1.5354) | Loss 6.0454(6.7426) | Error 0.4189(0.4586) | Error Color 0.3822(0.5006) |Steps 398(402.48) | Grad Norm 22.0107(14.7221) | Total Time 0.00(0.00)\n",
      "Iter 0290 | Time 9.3926(9.3844) | Bit/dim 2.1430(2.1828) | Xent 1.6625(1.8195) | Xent Color 1.2592(1.4976) | Loss 6.0559(6.5790) | Error 0.4011(0.4440) | Error Color 0.3933(0.4990) |Steps 380(401.01) | Grad Norm 33.3988(22.8340) | Total Time 0.00(0.00)\n",
      "Iter 0300 | Time 9.2338(9.3509) | Bit/dim 2.1201(2.1719) | Xent 1.5687(1.7676) | Xent Color 1.2325(1.4438) | Loss 5.8827(6.4309) | Error 0.3511(0.4246) | Error Color 0.4256(0.4896) |Steps 386(398.95) | Grad Norm 42.8721(27.9190) | Total Time 0.00(0.00)\n",
      "Iter 0310 | Time 9.4800(9.3287) | Bit/dim 2.1252(2.1647) | Xent 1.5149(1.7092) | Xent Color 1.1228(1.3655) | Loss 5.9016(6.2970) | Error 0.3600(0.4081) | Error Color 0.3333(0.4549) |Steps 398(397.32) | Grad Norm 18.7542(27.7329) | Total Time 0.00(0.00)\n",
      "Iter 0320 | Time 9.3366(9.3140) | Bit/dim 2.1797(2.1614) | Xent 1.4183(1.6408) | Xent Color 1.0704(1.3029) | Loss 5.8805(6.1900) | Error 0.3456(0.3886) | Error Color 0.3700(0.4354) |Steps 398(398.65) | Grad Norm 59.1963(34.2501) | Total Time 0.00(0.00)\n",
      "Iter 0330 | Time 8.9839(9.2869) | Bit/dim 2.1547(2.1596) | Xent 1.2879(1.5660) | Xent Color 1.0963(1.2251) | Loss 5.7903(6.0906) | Error 0.3033(0.3728) | Error Color 0.4100(0.3999) |Steps 386(397.73) | Grad Norm 60.7037(33.8987) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0005 | Time 56.0099, Epoch Time 692.8006(643.9741), Bit/dim 2.1718(best: 2.1525), Xent 1.2403, Xent Color 1.1015. Loss 2.7573, Error 0.2240(best: 0.2817), Error Color 0.4668(best: 0.5404)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0340 | Time 9.2099(9.2789) | Bit/dim 2.1910(2.1611) | Xent 1.1800(1.4863) | Xent Color 1.1288(1.1812) | Loss 5.8304(6.4305) | Error 0.2678(0.3556) | Error Color 0.4522(0.3962) |Steps 398(398.29) | Grad Norm 68.5149(40.7744) | Total Time 0.00(0.00)\n",
      "Iter 0350 | Time 8.9202(9.2616) | Bit/dim 2.1343(2.1568) | Xent 1.1268(1.4066) | Xent Color 0.9804(1.1193) | Loss 5.6582(6.2425) | Error 0.2578(0.3365) | Error Color 0.3378(0.3666) |Steps 386(397.70) | Grad Norm 44.6978(39.5320) | Total Time 0.00(0.00)\n",
      "Iter 0360 | Time 8.8988(9.2125) | Bit/dim 2.1750(2.1615) | Xent 1.0767(1.3240) | Xent Color 1.1535(1.1379) | Loss 5.8691(6.1310) | Error 0.2722(0.3194) | Error Color 0.4922(0.3988) |Steps 398(395.78) | Grad Norm 77.4830(53.7289) | Total Time 0.00(0.00)\n",
      "Iter 0370 | Time 9.2543(9.2231) | Bit/dim 2.1355(2.1582) | Xent 0.9703(1.2458) | Xent Color 0.9508(1.0980) | Loss 5.5848(6.0034) | Error 0.2411(0.3018) | Error Color 0.3589(0.3845) |Steps 410(397.29) | Grad Norm 49.6946(52.8556) | Total Time 0.00(0.00)\n",
      "Iter 0380 | Time 9.2351(9.2298) | Bit/dim 2.1429(2.1506) | Xent 0.9128(1.1753) | Xent Color 0.8857(1.0386) | Loss 5.5892(5.8799) | Error 0.2233(0.2903) | Error Color 0.2589(0.3462) |Steps 410(397.58) | Grad Norm 35.2857(46.9640) | Total Time 0.00(0.00)\n",
      "Iter 0390 | Time 8.9105(9.2050) | Bit/dim 2.1388(2.1485) | Xent 0.9006(1.1072) | Xent Color 0.8022(0.9734) | Loss 5.4963(5.7783) | Error 0.2444(0.2784) | Error Color 0.2256(0.3071) |Steps 398(397.07) | Grad Norm 33.5985(40.9954) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0006 | Time 56.4943, Epoch Time 686.6991(645.2559), Bit/dim 2.1439(best: 2.1525), Xent 0.7426, Xent Color 0.6546. Loss 2.4933, Error 0.1706(best: 0.2240), Error Color 0.1002(best: 0.4668)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0400 | Time 9.3628(9.2008) | Bit/dim 2.1291(2.1436) | Xent 0.8492(1.0428) | Xent Color 0.6817(0.9084) | Loss 5.4444(6.1937) | Error 0.2367(0.2677) | Error Color 0.1356(0.2700) |Steps 410(398.28) | Grad Norm 15.8782(36.0699) | Total Time 0.00(0.00)\n",
      "Iter 0410 | Time 9.4936(9.1975) | Bit/dim 2.1289(2.1435) | Xent 0.8373(0.9883) | Xent Color 0.8362(0.9327) | Loss 5.3455(6.0285) | Error 0.2356(0.2577) | Error Color 0.2744(0.2974) |Steps 356(398.91) | Grad Norm 40.6594(46.0673) | Total Time 0.00(0.00)\n",
      "Iter 0420 | Time 9.1730(9.1747) | Bit/dim 2.1078(2.1374) | Xent 0.8628(0.9499) | Xent Color 0.8853(0.9038) | Loss 5.4608(5.8621) | Error 0.2367(0.2518) | Error Color 0.3189(0.2910) |Steps 380(396.85) | Grad Norm 84.7464(46.0748) | Total Time 0.00(0.00)\n",
      "Iter 0430 | Time 9.0651(9.1718) | Bit/dim 2.1364(2.1353) | Xent 0.7804(0.9159) | Xent Color 0.7704(0.9307) | Loss 5.3570(5.7770) | Error 0.1967(0.2465) | Error Color 0.2389(0.3102) |Steps 398(396.76) | Grad Norm 44.2233(56.0341) | Total Time 0.00(0.00)\n",
      "Iter 0440 | Time 9.0148(9.1560) | Bit/dim 2.0887(2.1256) | Xent 0.8475(0.8859) | Xent Color 0.7089(0.8747) | Loss 5.2529(5.6563) | Error 0.2489(0.2416) | Error Color 0.1611(0.2761) |Steps 386(396.63) | Grad Norm 25.2824(47.8990) | Total Time 0.00(0.00)\n",
      "Iter 0450 | Time 9.1819(9.1508) | Bit/dim 2.0944(2.1181) | Xent 0.7018(0.8479) | Xent Color 0.6112(0.8155) | Loss 5.2092(5.5547) | Error 0.1900(0.2346) | Error Color 0.1244(0.2396) |Steps 410(395.99) | Grad Norm 12.2685(38.5596) | Total Time 0.00(0.00)\n",
      "Iter 0460 | Time 8.8351(9.1286) | Bit/dim 2.0914(2.1109) | Xent 0.7245(0.8113) | Xent Color 0.5879(0.7601) | Loss 5.1813(5.4693) | Error 0.2233(0.2269) | Error Color 0.1444(0.2126) |Steps 374(394.29) | Grad Norm 25.4849(32.4825) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0007 | Time 55.9097, Epoch Time 682.5164(646.3737), Bit/dim 2.0968(best: 2.1439), Xent 0.5809, Xent Color 0.4927. Loss 2.3652, Error 0.1466(best: 0.1706), Error Color 0.0420(best: 0.1002)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0470 | Time 9.4880(9.1330) | Bit/dim 2.0875(2.1034) | Xent 0.6890(0.7830) | Xent Color 0.7094(0.7119) | Loss 5.1328(5.8458) | Error 0.2056(0.2226) | Error Color 0.2978(0.1945) |Steps 404(394.72) | Grad Norm 67.9033(30.1594) | Total Time 0.00(0.00)\n",
      "Iter 0480 | Time 9.1722(9.1878) | Bit/dim 2.1070(2.1155) | Xent 0.8023(0.7691) | Xent Color 1.7313(0.9999) | Loss 5.7834(5.8512) | Error 0.2456(0.2218) | Error Color 0.6711(0.3001) |Steps 386(393.97) | Grad Norm 103.1790(52.3703) | Total Time 0.00(0.00)\n",
      "Iter 0490 | Time 8.9821(9.1598) | Bit/dim 2.0599(2.1078) | Xent 0.7956(0.7756) | Xent Color 0.7895(1.0069) | Loss 5.2629(5.7291) | Error 0.2433(0.2250) | Error Color 0.2122(0.3263) |Steps 392(391.58) | Grad Norm 7.4869(51.4059) | Total Time 0.00(0.00)\n",
      "Iter 0500 | Time 9.0890(9.1406) | Bit/dim 2.0548(2.0967) | Xent 0.7227(0.7633) | Xent Color 0.6904(0.9402) | Loss 5.1518(5.5941) | Error 0.2156(0.2238) | Error Color 0.1811(0.2981) |Steps 386(389.87) | Grad Norm 6.3880(43.0296) | Total Time 0.00(0.00)\n",
      "Iter 0510 | Time 8.9227(9.0986) | Bit/dim 2.0667(2.0901) | Xent 0.6538(0.7454) | Xent Color 0.6195(0.8604) | Loss 5.1648(5.4789) | Error 0.2100(0.2220) | Error Color 0.1511(0.2615) |Steps 380(387.35) | Grad Norm 3.4603(34.3795) | Total Time 0.00(0.00)\n",
      "Iter 0520 | Time 9.1962(9.1074) | Bit/dim 2.0641(2.0822) | Xent 0.6584(0.7273) | Xent Color 0.5368(0.7826) | Loss 5.1735(5.3914) | Error 0.1889(0.2178) | Error Color 0.1156(0.2268) |Steps 392(391.45) | Grad Norm 9.3019(27.9807) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0008 | Time 56.4590, Epoch Time 681.4460(647.4258), Bit/dim 2.0581(best: 2.0968), Xent 0.5131, Xent Color 0.4085. Loss 2.2885, Error 0.1368(best: 0.1466), Error Color 0.0338(best: 0.0420)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0530 | Time 9.2265(9.1095) | Bit/dim 2.0572(2.0737) | Xent 0.6461(0.7111) | Xent Color 0.4729(0.7088) | Loss 5.0642(5.8287) | Error 0.2000(0.2130) | Error Color 0.0856(0.1945) |Steps 404(392.24) | Grad Norm 9.7219(22.4019) | Total Time 0.00(0.00)\n",
      "Iter 0540 | Time 9.5765(9.1243) | Bit/dim 2.0415(2.0661) | Xent 0.6707(0.7014) | Xent Color 0.4305(0.6421) | Loss 5.0970(5.6183) | Error 0.2233(0.2104) | Error Color 0.0789(0.1667) |Steps 416(394.26) | Grad Norm 11.5441(19.1729) | Total Time 0.00(0.00)\n",
      "Iter 0550 | Time 9.0034(9.1445) | Bit/dim 2.0285(2.0586) | Xent 0.6751(0.6846) | Xent Color 0.4289(0.5921) | Loss 4.9745(5.4624) | Error 0.2067(0.2058) | Error Color 0.0900(0.1493) |Steps 392(394.90) | Grad Norm 7.6013(21.2755) | Total Time 0.00(0.00)\n",
      "Iter 0560 | Time 9.4836(9.1907) | Bit/dim 2.2026(2.0574) | Xent 0.6639(0.6790) | Xent Color 6.0008(0.7565) | Loss 7.8854(5.4492) | Error 0.2078(0.2046) | Error Color 0.7633(0.1741) |Steps 416(397.78) | Grad Norm 330.4476(38.8057) | Total Time 0.00(0.00)\n",
      "Iter 0570 | Time 8.9826(9.1782) | Bit/dim 2.0573(2.0698) | Xent 0.7971(0.6986) | Xent Color 1.0454(0.8927) | Loss 5.4149(5.4742) | Error 0.2489(0.2127) | Error Color 0.4444(0.2474) |Steps 392(395.73) | Grad Norm 31.1374(47.1617) | Total Time 0.00(0.00)\n",
      "Iter 0580 | Time 9.0954(9.1297) | Bit/dim 2.0462(2.0650) | Xent 0.6633(0.6948) | Xent Color 0.7001(0.8699) | Loss 5.1811(5.4008) | Error 0.2122(0.2134) | Error Color 0.1889(0.2575) |Steps 398(394.11) | Grad Norm 5.2639(38.2393) | Total Time 0.00(0.00)\n",
      "Iter 0590 | Time 9.1309(9.1414) | Bit/dim 2.0063(2.0547) | Xent 0.6510(0.6831) | Xent Color 0.6478(0.8234) | Loss 5.0093(5.3300) | Error 0.1978(0.2109) | Error Color 0.1956(0.2452) |Steps 356(393.27) | Grad Norm 5.1169(30.0797) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0009 | Time 56.0781, Epoch Time 683.7211(648.5147), Bit/dim 2.0264(best: 2.0581), Xent 0.4623, Xent Color 0.5062. Loss 2.2685, Error 0.1285(best: 0.1368), Error Color 0.0833(best: 0.0338)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0600 | Time 9.2252(9.1464) | Bit/dim 2.0175(2.0437) | Xent 0.7068(0.6717) | Xent Color 0.5481(0.7622) | Loss 5.0658(5.7084) | Error 0.2111(0.2077) | Error Color 0.1344(0.2206) |Steps 416(394.72) | Grad Norm 3.4001(23.1880) | Total Time 0.00(0.00)\n",
      "Iter 0610 | Time 9.7817(9.1798) | Bit/dim 2.0157(2.0338) | Xent 0.6201(0.6611) | Xent Color 0.4995(0.6967) | Loss 5.0628(5.5146) | Error 0.2033(0.2048) | Error Color 0.1044(0.1956) |Steps 404(393.48) | Grad Norm 3.3789(17.9992) | Total Time 0.00(0.00)\n",
      "Iter 0620 | Time 9.3364(9.2276) | Bit/dim 2.0013(2.0265) | Xent 0.5650(0.6417) | Xent Color 0.3919(0.6298) | Loss 4.8904(5.3686) | Error 0.1778(0.1987) | Error Color 0.0956(0.1726) |Steps 374(394.85) | Grad Norm 4.7889(14.2344) | Total Time 0.00(0.00)\n",
      "Iter 0630 | Time 9.2769(9.2586) | Bit/dim 1.9896(2.0179) | Xent 0.6609(0.6342) | Xent Color 0.3764(0.5657) | Loss 4.9166(5.2452) | Error 0.2056(0.1970) | Error Color 0.0844(0.1485) |Steps 410(396.01) | Grad Norm 5.8037(11.6421) | Total Time 0.00(0.00)\n",
      "Iter 0640 | Time 9.6085(9.2908) | Bit/dim 1.9831(2.0092) | Xent 0.5818(0.6277) | Xent Color 0.3170(0.5043) | Loss 4.8291(5.1461) | Error 0.1800(0.1945) | Error Color 0.0622(0.1264) |Steps 416(398.58) | Grad Norm 4.8021(9.6201) | Total Time 0.00(0.00)\n",
      "Iter 0650 | Time 9.3195(9.3239) | Bit/dim 2.0033(2.0020) | Xent 0.5989(0.6201) | Xent Color 0.2675(0.4477) | Loss 4.8866(5.0621) | Error 0.1767(0.1924) | Error Color 0.0522(0.1071) |Steps 410(400.03) | Grad Norm 2.5816(7.9435) | Total Time 0.00(0.00)\n",
      "Iter 0660 | Time 9.3201(9.3291) | Bit/dim 1.9749(1.9964) | Xent 0.6188(0.6080) | Xent Color 0.3116(0.4019) | Loss 4.7702(5.0022) | Error 0.2000(0.1887) | Error Color 0.0911(0.0944) |Steps 410(400.86) | Grad Norm 28.6363(9.3689) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0010 | Time 57.8855, Epoch Time 697.5543(649.9859), Bit/dim 1.9866(best: 2.0264), Xent 0.4242, Xent Color 0.2885. Loss 2.1647, Error 0.1204(best: 0.1285), Error Color 0.0883(best: 0.0338)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0670 | Time 9.3749(9.3275) | Bit/dim 1.9603(1.9917) | Xent 0.5907(0.6001) | Xent Color 0.3611(0.4299) | Loss 4.8730(5.3956) | Error 0.1756(0.1860) | Error Color 0.1211(0.1171) |Steps 404(400.33) | Grad Norm 23.6982(20.1022) | Total Time 0.00(0.00)\n",
      "Iter 0680 | Time 9.3221(9.3007) | Bit/dim 1.9611(1.9839) | Xent 0.6084(0.6006) | Xent Color 0.2584(0.3960) | Loss 4.8332(5.2380) | Error 0.1900(0.1883) | Error Color 0.0478(0.1062) |Steps 410(399.95) | Grad Norm 7.0376(19.3537) | Total Time 0.00(0.00)\n",
      "Iter 0690 | Time 8.8429(9.2946) | Bit/dim 1.9712(1.9800) | Xent 0.5764(0.5969) | Xent Color 0.2051(0.3539) | Loss 4.6671(5.1135) | Error 0.1744(0.1868) | Error Color 0.0322(0.0915) |Steps 374(398.98) | Grad Norm 11.9004(17.9858) | Total Time 0.00(0.00)\n",
      "Iter 0700 | Time 9.6331(9.3000) | Bit/dim 1.9566(1.9744) | Xent 0.5467(0.5887) | Xent Color 0.2192(0.3172) | Loss 4.7495(5.0158) | Error 0.1667(0.1841) | Error Color 0.0433(0.0791) |Steps 422(400.60) | Grad Norm 17.1268(16.9737) | Total Time 0.00(0.00)\n",
      "Iter 0710 | Time 9.3500(9.2758) | Bit/dim 1.9457(1.9668) | Xent 0.5940(0.5835) | Xent Color 0.1685(0.2840) | Loss 4.6565(4.9300) | Error 0.1778(0.1817) | Error Color 0.0278(0.0682) |Steps 410(400.45) | Grad Norm 5.7526(15.3243) | Total Time 0.00(0.00)\n",
      "Iter 0720 | Time 9.3761(9.2939) | Bit/dim 1.9332(1.9594) | Xent 0.5359(0.5766) | Xent Color 0.1737(0.2542) | Loss 4.6474(4.8638) | Error 0.1667(0.1807) | Error Color 0.0378(0.0589) |Steps 410(401.28) | Grad Norm 13.2595(13.4383) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0011 | Time 58.2423, Epoch Time 694.9928(651.3361), Bit/dim 1.9434(best: 1.9866), Xent 0.4045, Xent Color 0.1036. Loss 2.0705, Error 0.1154(best: 0.1204), Error Color 0.0089(best: 0.0338)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0730 | Time 9.2911(9.3317) | Bit/dim 1.9444(1.9528) | Xent 0.5138(0.5646) | Xent Color 0.1409(0.2274) | Loss 4.6342(5.3199) | Error 0.1544(0.1768) | Error Color 0.0278(0.0505) |Steps 410(404.01) | Grad Norm 6.1565(11.8848) | Total Time 0.00(0.00)\n",
      "Iter 0740 | Time 9.0659(9.3537) | Bit/dim 1.9164(1.9465) | Xent 0.5973(0.5600) | Xent Color 0.1479(0.2041) | Loss 4.6469(5.1439) | Error 0.1756(0.1746) | Error Color 0.0311(0.0435) |Steps 410(404.65) | Grad Norm 15.2380(10.8143) | Total Time 0.00(0.00)\n",
      "Iter 0750 | Time 9.3762(9.4112) | Bit/dim 1.9417(1.9391) | Xent 0.5648(0.5582) | Xent Color 0.1288(0.1869) | Loss 4.6875(5.0206) | Error 0.1556(0.1749) | Error Color 0.0267(0.0391) |Steps 404(406.72) | Grad Norm 12.0165(12.0538) | Total Time 0.00(0.00)\n",
      "Iter 0760 | Time 9.6643(9.4259) | Bit/dim 2.0629(1.9814) | Xent 0.9432(0.5775) | Xent Color 1.0948(0.8097) | Loss 5.5161(5.3042) | Error 0.3022(0.1812) | Error Color 0.4178(0.1492) |Steps 422(408.00) | Grad Norm 27.8853(42.9580) | Total Time 0.00(0.00)\n",
      "Iter 0770 | Time 9.0575(9.4128) | Bit/dim 2.0953(2.0183) | Xent 0.6089(0.6233) | Xent Color 0.9595(0.9185) | Loss 5.3573(5.3737) | Error 0.2033(0.1975) | Error Color 0.3778(0.2319) |Steps 386(405.21) | Grad Norm 13.2112(37.9500) | Total Time 0.00(0.00)\n",
      "Iter 0780 | Time 9.3501(9.4186) | Bit/dim 2.0292(2.0224) | Xent 0.6996(0.6369) | Xent Color 0.6355(0.8883) | Loss 5.1377(5.3149) | Error 0.2267(0.2037) | Error Color 0.2289(0.2518) |Steps 410(403.12) | Grad Norm 5.2565(30.2309) | Total Time 0.00(0.00)\n",
      "Iter 0790 | Time 9.7427(9.3634) | Bit/dim 1.9989(2.0168) | Xent 0.5325(0.6198) | Xent Color 0.4429(0.7944) | Loss 4.9828(5.2122) | Error 0.1733(0.1980) | Error Color 0.1411(0.2319) |Steps 416(397.66) | Grad Norm 4.1370(23.6316) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0012 | Time 57.7823, Epoch Time 703.0529(652.8876), Bit/dim 1.9889(best: 1.9434), Xent 0.4115, Xent Color 0.3483. Loss 2.1788, Error 0.1180(best: 0.1154), Error Color 0.0624(best: 0.0089)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0800 | Time 9.6018(9.3707) | Bit/dim 1.9828(2.0057) | Xent 0.5774(0.6099) | Xent Color 0.3652(0.6950) | Loss 4.8736(5.5511) | Error 0.1889(0.1955) | Error Color 0.1111(0.2037) |Steps 380(395.55) | Grad Norm 4.8265(18.4742) | Total Time 0.00(0.00)\n",
      "Iter 0810 | Time 9.8956(9.3984) | Bit/dim 1.9811(1.9963) | Xent 0.5107(0.5966) | Xent Color 0.2799(0.5959) | Loss 4.8518(5.3551) | Error 0.1767(0.1912) | Error Color 0.0767(0.1724) |Steps 440(397.29) | Grad Norm 4.0034(14.5066) | Total Time 0.00(0.00)\n",
      "Iter 0820 | Time 9.5457(9.4231) | Bit/dim 1.9734(1.9867) | Xent 0.5156(0.5823) | Xent Color 0.2419(0.5057) | Loss 4.7497(5.1914) | Error 0.1667(0.1850) | Error Color 0.0600(0.1438) |Steps 404(398.03) | Grad Norm 2.3918(11.3750) | Total Time 0.00(0.00)\n",
      "Iter 0830 | Time 10.0425(9.5004) | Bit/dim 1.9191(1.9743) | Xent 0.6044(0.5741) | Xent Color 0.1971(0.4264) | Loss 4.7284(5.0635) | Error 0.1867(0.1819) | Error Color 0.0444(0.1179) |Steps 422(400.00) | Grad Norm 2.5474(9.1073) | Total Time 0.00(0.00)\n",
      "Iter 0840 | Time 10.0072(9.5484) | Bit/dim 1.9216(1.9632) | Xent 0.5683(0.5657) | Xent Color 0.1677(0.3595) | Loss 4.6940(4.9662) | Error 0.1744(0.1791) | Error Color 0.0389(0.0975) |Steps 416(403.90) | Grad Norm 2.4899(7.3349) | Total Time 0.00(0.00)\n",
      "Iter 0850 | Time 9.2787(9.5684) | Bit/dim 1.9137(1.9547) | Xent 0.5421(0.5601) | Xent Color 0.1363(0.3021) | Loss 4.5965(4.8829) | Error 0.1744(0.1771) | Error Color 0.0289(0.0799) |Steps 404(405.03) | Grad Norm 2.0109(5.9644) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0013 | Time 56.4987, Epoch Time 711.6946(654.6518), Bit/dim 1.9258(best: 1.9434), Xent 0.3813, Xent Color 0.0730. Loss 2.0394, Error 0.1091(best: 0.1154), Error Color 0.0073(best: 0.0089)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0860 | Time 9.5619(9.5872) | Bit/dim 1.9166(1.9456) | Xent 0.6179(0.5502) | Xent Color 0.1145(0.2536) | Loss 4.6053(5.3212) | Error 0.1833(0.1744) | Error Color 0.0256(0.0651) |Steps 428(406.76) | Grad Norm 3.9055(5.2886) | Total Time 0.00(0.00)\n",
      "Iter 0870 | Time 10.0889(9.5714) | Bit/dim 1.9058(1.9378) | Xent 0.5209(0.5449) | Xent Color 0.1041(0.2178) | Loss 4.6038(5.1341) | Error 0.1633(0.1722) | Error Color 0.0189(0.0544) |Steps 434(406.63) | Grad Norm 3.9058(4.7378) | Total Time 0.00(0.00)\n",
      "Iter 0880 | Time 10.0307(9.5493) | Bit/dim 1.8936(1.9288) | Xent 0.5489(0.5367) | Xent Color 0.0935(0.1869) | Loss 4.5759(4.9831) | Error 0.1622(0.1681) | Error Color 0.0144(0.0452) |Steps 398(405.35) | Grad Norm 3.6105(4.2511) | Total Time 0.00(0.00)\n",
      "Iter 0890 | Time 9.8017(9.5741) | Bit/dim 1.9132(1.9212) | Xent 0.4723(0.5314) | Xent Color 0.0849(0.1626) | Loss 4.5862(4.8733) | Error 0.1522(0.1664) | Error Color 0.0178(0.0382) |Steps 428(406.08) | Grad Norm 5.9882(4.6339) | Total Time 0.00(0.00)\n",
      "Iter 0900 | Time 9.7182(9.5367) | Bit/dim 1.8914(1.9127) | Xent 0.5359(0.5258) | Xent Color 0.0797(0.1421) | Loss 4.5786(4.7798) | Error 0.1733(0.1649) | Error Color 0.0133(0.0328) |Steps 422(404.17) | Grad Norm 7.8983(5.0713) | Total Time 0.00(0.00)\n",
      "Iter 0910 | Time 9.6878(9.5407) | Bit/dim 1.8607(1.9026) | Xent 0.5364(0.5243) | Xent Color 0.0706(0.1263) | Loss 4.5473(4.7112) | Error 0.1722(0.1641) | Error Color 0.0133(0.0286) |Steps 416(403.88) | Grad Norm 8.7347(5.4571) | Total Time 0.00(0.00)\n",
      "Iter 0920 | Time 9.4124(9.5656) | Bit/dim 1.9028(1.8968) | Xent 0.4862(0.5156) | Xent Color 0.0898(0.1153) | Loss 4.5487(4.6631) | Error 0.1544(0.1620) | Error Color 0.0256(0.0262) |Steps 410(406.09) | Grad Norm 13.7433(6.5170) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0014 | Time 59.9789, Epoch Time 713.8698(656.4284), Bit/dim 1.8771(best: 1.9258), Xent 0.3591, Xent Color 0.0417. Loss 1.9773, Error 0.1055(best: 0.1091), Error Color 0.0035(best: 0.0073)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0930 | Time 9.5499(9.5681) | Bit/dim 1.8638(1.8906) | Xent 0.4999(0.5142) | Xent Color 0.0653(0.1041) | Loss 4.4714(5.0949) | Error 0.1567(0.1603) | Error Color 0.0133(0.0235) |Steps 410(407.34) | Grad Norm 6.0249(6.8715) | Total Time 0.00(0.00)\n",
      "Iter 0940 | Time 9.5786(9.5852) | Bit/dim 1.8688(1.8826) | Xent 0.4500(0.5121) | Xent Color 0.0522(0.0930) | Loss 4.4993(4.9373) | Error 0.1422(0.1605) | Error Color 0.0067(0.0203) |Steps 410(409.70) | Grad Norm 4.3704(6.2660) | Total Time 0.00(0.00)\n",
      "Iter 0950 | Time 9.5024(9.6170) | Bit/dim 1.8407(1.8742) | Xent 0.4859(0.5066) | Xent Color 0.0550(0.0844) | Loss 4.4793(4.8134) | Error 0.1578(0.1581) | Error Color 0.0067(0.0181) |Steps 416(410.36) | Grad Norm 4.0111(5.8745) | Total Time 0.00(0.00)\n",
      "Iter 0960 | Time 10.0934(9.6285) | Bit/dim 1.8495(1.8667) | Xent 0.5275(0.5067) | Xent Color 0.0627(0.0771) | Loss 4.5643(4.7226) | Error 0.1589(0.1586) | Error Color 0.0100(0.0162) |Steps 446(413.44) | Grad Norm 12.2483(6.0832) | Total Time 0.00(0.00)\n",
      "Iter 0970 | Time 9.7869(9.6575) | Bit/dim 2.1138(1.9279) | Xent 0.7092(0.5333) | Xent Color 0.9912(0.6638) | Loss 5.5073(5.0493) | Error 0.2244(0.1679) | Error Color 0.4078(0.1271) |Steps 428(413.23) | Grad Norm 25.8101(26.1465) | Total Time 0.00(0.00)\n",
      "Iter 0980 | Time 10.2526(9.8006) | Bit/dim 1.9634(1.9460) | Xent 0.6372(0.5803) | Xent Color 0.4760(0.6577) | Loss 4.9000(5.0671) | Error 0.2089(0.1855) | Error Color 0.1822(0.1581) |Steps 446(419.30) | Grad Norm 7.9537(24.7010) | Total Time 0.00(0.00)\n",
      "Iter 0990 | Time 8.9586(9.8067) | Bit/dim 1.9269(1.9434) | Xent 0.5481(0.5799) | Xent Color 0.2560(0.5758) | Loss 4.7618(5.0045) | Error 0.1689(0.1857) | Error Color 0.0656(0.1455) |Steps 404(420.32) | Grad Norm 4.2488(20.5873) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0015 | Time 60.9749, Epoch Time 728.6644(658.5954), Bit/dim 1.9164(best: 1.8771), Xent 0.3841, Xent Color 0.2147. Loss 2.0661, Error 0.1108(best: 0.1055), Error Color 0.0666(best: 0.0035)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1000 | Time 9.6708(9.8137) | Bit/dim 1.8958(1.9308) | Xent 0.5458(0.5718) | Xent Color 0.1732(0.4816) | Loss 4.5894(5.3566) | Error 0.1656(0.1828) | Error Color 0.0411(0.1243) |Steps 428(421.36) | Grad Norm 5.5543(16.7043) | Total Time 0.00(0.00)\n",
      "Iter 1010 | Time 9.7592(9.8426) | Bit/dim 1.8752(1.9168) | Xent 0.5698(0.5642) | Xent Color 0.1004(0.3924) | Loss 4.5811(5.1697) | Error 0.1867(0.1795) | Error Color 0.0133(0.1000) |Steps 422(422.91) | Grad Norm 2.0934(13.4908) | Total Time 0.00(0.00)\n",
      "Iter 1020 | Time 9.6231(9.8747) | Bit/dim 1.8211(1.9009) | Xent 0.4996(0.5525) | Xent Color 0.0956(0.3198) | Loss 4.4467(5.0056) | Error 0.1333(0.1764) | Error Color 0.0122(0.0804) |Steps 416(422.08) | Grad Norm 1.5090(10.7602) | Total Time 0.00(0.00)\n",
      "Iter 1030 | Time 10.0635(9.8655) | Bit/dim 1.8248(1.8859) | Xent 0.5471(0.5452) | Xent Color 0.1106(0.2638) | Loss 4.5434(4.8811) | Error 0.1733(0.1731) | Error Color 0.0211(0.0652) |Steps 452(423.38) | Grad Norm 2.8804(8.5998) | Total Time 0.00(0.00)\n",
      "Iter 1040 | Time 10.1490(9.8641) | Bit/dim 1.8220(1.8724) | Xent 0.5260(0.5391) | Xent Color 0.0736(0.2182) | Loss 4.5209(4.7875) | Error 0.1744(0.1702) | Error Color 0.0122(0.0530) |Steps 428(422.63) | Grad Norm 4.7022(7.2396) | Total Time 0.00(0.00)\n",
      "Iter 1050 | Time 9.9524(9.8973) | Bit/dim 1.8080(1.8598) | Xent 0.4945(0.5321) | Xent Color 0.0771(0.1822) | Loss 4.4732(4.7121) | Error 0.1611(0.1670) | Error Color 0.0178(0.0435) |Steps 416(424.03) | Grad Norm 2.4096(6.5536) | Total Time 0.00(0.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_rl_2cond_multiscale.py --data colormnist --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/cnf_conditional_disentangle_colormnist_bs900_sratio_1_4th_drop_0_5_rl_stdscale_6_2cond_linear_multiscale_run1 --seed 1 --lr 0.001 --conditional True --controlled_tol False --train_mode semisup --log_freq 10 --weight_y 0.5 --condition_ratio 0.25 --dropout_rate 0.5 --scale_fac 1.0 --scale_std 6.0 --cond_nn linear --y_color 10 --y_class 10\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
