{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0,1,2,3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_rl.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "GATES = [\"cnn1\", \"cnn2\", \"rnn\"]\n",
      "\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--gate', type=str, default='cnn1', choices=GATES)\n",
      "parser.add_argument('--scale', type=float, default=1.0)\n",
      "parser.add_argument('--scale_fac', type=float, default=1.0)\n",
      "parser.add_argument('--scale_std', type=float, default=1.0)\n",
      "parser.add_argument('--eta', default=0.1, type=float,\n",
      "                        help='tuning parameter that allows us to trade-off the competing goals of' \n",
      "                                'minimizing the prediction loss and maximizing the gate rewards ')\n",
      "parser.add_argument('--rl-weight', default=0.01, type=float,\n",
      "                        help='rl weight')\n",
      "\n",
      "parser.add_argument('--gamma', default=0.99, type=float,\n",
      "                        help='discount factor, default: (0.99)')\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "import lib.odenvp_conditional_rl as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol, \"scale\": args.scale, \"scale_fac\": args.scale_fac, \"scale_std\": args.scale_std, \"gate\": args.gate},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            # re-weight the gate rewards\n",
      "            normalized_eta = args.eta / len(logp_actions)\n",
      "            \n",
      "            # collect cumulative future rewards\n",
      "            R = - loss\n",
      "            cum_rewards = []\n",
      "            for r in nfe[::-1]:\n",
      "                R = -normalized_eta * r.view(-1,1) + args.gamma * R\n",
      "                cum_rewards.insert(0,R)\n",
      "            \n",
      "            # apply REINFORCE\n",
      "            rl_loss = 0\n",
      "            for lpa, r in zip(logp_actions, cum_rewards):\n",
      "                rl_loss = rl_loss - lpa.view(-1,1) * args.rl_weight * r\n",
      "                \n",
      "            loss = loss + rl_loss.mean()\n",
      "            \n",
      "            loss.backward()\n",
      "            \n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "            \n",
      "            for idx in range(len(model.module.transforms)):\n",
      "                for layer in model.module.transforms[idx].chain:\n",
      "                    if hasattr(layer, 'atol'):\n",
      "                        layer.odefunc.after_odeint()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'train': atol[tol_indx].mean()}, itr)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'train': rtol[tol_indx].mean()}, itr)\n",
      "                    \n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples, atol, rtol, logp_actions, nfe = model(fixed_z, reverse=True)\n",
      "            generated_samples = generated_samples.view(-1, *data_shape)\n",
      "            for tol_indx in range(len(atol)):\n",
      "                writer.add_scalars('atol_gen_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                writer.add_scalars('rtol_gen_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, condition_ratio=0.5, conditional=True, controlled_tol=False, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, eta=0.1, gamma=0.99, gate='cnn1', imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume=None, rl_weight=0.01, rtol=1e-05, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_6_run1', scale=1.0, scale_fac=1.0, scale_std=6.0, seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1450732\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc_rl.py:291: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Iter 0010 | Time 12.6032(31.6459) | Bit/dim 8.6893(8.9517) | Xent 2.2805(2.3001) | Loss 20.9267(21.3272) | Error 0.7956(0.8599) Steps 0(0.00) | Grad Norm 22.1360(28.2106) | Total Time 0.00(0.00)\n",
      "Iter 0020 | Time 12.2381(26.6184) | Bit/dim 8.4867(8.8627) | Xent 2.2264(2.2874) | Loss 20.4590(21.1201) | Error 0.7256(0.8326) Steps 0(0.00) | Grad Norm 9.2144(24.4184) | Total Time 0.00(0.00)\n",
      "Iter 0030 | Time 13.1344(22.9334) | Bit/dim 8.3931(8.7512) | Xent 2.1744(2.2634) | Loss 20.5105(20.8808) | Error 0.7611(0.8094) Steps 0(0.00) | Grad Norm 9.2366(20.0577) | Total Time 0.00(0.00)\n",
      "Iter 0040 | Time 12.5126(20.1774) | Bit/dim 8.1916(8.6232) | Xent 2.1135(2.2346) | Loss 19.7188(20.6084) | Error 0.7233(0.7908) Steps 0(0.00) | Grad Norm 5.6644(16.4652) | Total Time 0.00(0.00)\n",
      "Iter 0050 | Time 12.0463(18.1735) | Bit/dim 7.9731(8.4736) | Xent 2.1025(2.2021) | Loss 19.0683(20.2797) | Error 0.7067(0.7754) Steps 0(0.00) | Grad Norm 5.4972(13.6767) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0001 | Time 78.1457, Epoch Time 814.4753(814.4753), Bit/dim 7.7698(best: inf), Xent 2.0786, Loss 8.8091, Error 0.6995(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0060 | Time 12.8664(16.7490) | Bit/dim 7.6771(8.2913) | Xent 2.0782(2.1722) | Loss 18.4202(20.4314) | Error 0.6989(0.7584) Steps 0(0.00) | Grad Norm 5.7133(11.5457) | Total Time 0.00(0.00)\n",
      "Iter 0070 | Time 13.0330(15.7391) | Bit/dim 7.3711(8.0805) | Xent 2.0566(2.1464) | Loss 18.0753(19.8520) | Error 0.6689(0.7416) Steps 0(0.00) | Grad Norm 4.1264(9.7589) | Total Time 0.00(0.00)\n",
      "Iter 0080 | Time 12.4842(14.9855) | Bit/dim 7.1861(7.8637) | Xent 2.0711(2.1259) | Loss 17.6611(19.2818) | Error 0.6722(0.7267) Steps 0(0.00) | Grad Norm 3.3772(8.1560) | Total Time 0.00(0.00)\n",
      "Iter 0090 | Time 13.0065(14.4902) | Bit/dim 7.0822(7.6689) | Xent 2.0847(2.1130) | Loss 17.5182(18.7934) | Error 0.7256(0.7182) Steps 0(0.00) | Grad Norm 2.1297(6.7156) | Total Time 0.00(0.00)\n",
      "Iter 0100 | Time 14.0904(14.1758) | Bit/dim 7.0012(7.5061) | Xent 2.0627(2.1034) | Loss 17.3955(18.4037) | Error 0.6978(0.7125) Steps 0(0.00) | Grad Norm 3.4471(5.7543) | Total Time 0.00(0.00)\n",
      "Iter 0110 | Time 14.1275(14.0440) | Bit/dim 7.0034(7.3753) | Xent 2.0595(2.0947) | Loss 17.4917(18.1056) | Error 0.7333(0.7123) Steps 0(0.00) | Grad Norm 2.3546(4.8612) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0002 | Time 76.8820, Epoch Time 815.6311(814.5100), Bit/dim 6.9921(best: 7.7698), Xent 2.0585, Loss 8.0214, Error 0.6958(best: 0.6995)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0120 | Time 14.1792(14.1141) | Bit/dim 6.9410(7.2695) | Xent 2.0562(2.0853) | Loss 17.0054(18.4016) | Error 0.7100(0.7110) Steps 0(0.00) | Grad Norm 1.7444(4.2546) | Total Time 0.00(0.00)\n",
      "Iter 0130 | Time 14.2986(14.1965) | Bit/dim 6.9197(7.1828) | Xent 2.0420(2.0744) | Loss 17.1426(18.0462) | Error 0.6822(0.7054) Steps 0(0.00) | Grad Norm 3.0597(3.7942) | Total Time 0.00(0.00)\n",
      "Iter 0140 | Time 16.2846(14.2886) | Bit/dim 6.8948(7.1070) | Xent 2.0451(2.0645) | Loss 17.0127(17.7578) | Error 0.6856(0.7019) Steps 0(0.00) | Grad Norm 8.0943(3.9281) | Total Time 0.00(0.00)\n",
      "Iter 0150 | Time 15.3406(14.3489) | Bit/dim 6.7862(7.0357) | Xent 2.0151(2.0565) | Loss 16.5020(17.4948) | Error 0.7044(0.6996) Steps 0(0.00) | Grad Norm 4.3381(4.9958) | Total Time 0.00(0.00)\n",
      "Iter 0160 | Time 14.7437(14.4151) | Bit/dim 6.7054(6.9610) | Xent 2.0383(2.0518) | Loss 16.7720(17.2980) | Error 0.7200(0.6983) Steps 0(0.00) | Grad Norm 22.7965(7.6123) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0003 | Time 79.7560, Epoch Time 894.3948(816.9065), Bit/dim 6.6374(best: 6.9921), Xent 2.0296, Loss 7.6522, Error 0.6954(best: 0.6958)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0170 | Time 14.6698(14.4966) | Bit/dim 6.6080(6.8784) | Xent 2.0471(2.0527) | Loss 16.6465(17.7281) | Error 0.7178(0.7025) Steps 0(0.00) | Grad Norm 40.6791(14.5756) | Total Time 0.00(0.00)\n",
      "Iter 0180 | Time 15.8085(14.6627) | Bit/dim 6.4440(6.7776) | Xent 2.2082(2.0565) | Loss 16.2043(17.3374) | Error 0.8144(0.7126) Steps 0(0.00) | Grad Norm 108.4480(23.8156) | Total Time 0.00(0.00)\n",
      "Iter 0190 | Time 13.3015(14.5941) | Bit/dim 6.2509(6.6605) | Xent 2.0734(2.0597) | Loss 15.7461(16.9768) | Error 0.7222(0.7184) Steps 0(0.00) | Grad Norm 73.5106(32.7465) | Total Time 0.00(0.00)\n",
      "Iter 0200 | Time 13.7249(14.5749) | Bit/dim 6.0696(6.5272) | Xent 2.0369(2.0584) | Loss 15.3149(16.6057) | Error 0.6878(0.7186) Steps 0(0.00) | Grad Norm 31.4530(36.3012) | Total Time 0.00(0.00)\n",
      "Iter 0210 | Time 15.2471(14.6920) | Bit/dim 5.9999(6.4149) | Xent 2.0784(2.0794) | Loss 15.3317(16.3039) | Error 0.7256(0.7289) Steps 0(0.00) | Grad Norm 21.6208(53.4145) | Total Time 0.00(0.00)\n",
      "Iter 0220 | Time 14.5822(14.5819) | Bit/dim 5.8553(6.2869) | Xent 2.0384(2.0712) | Loss 15.0740(15.9770) | Error 0.6622(0.7202) Steps 0(0.00) | Grad Norm 22.1479(47.6541) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0004 | Time 77.6988, Epoch Time 902.8426(819.4846), Bit/dim 5.8657(best: 6.6374), Xent 2.0339, Loss 6.8827, Error 0.6859(best: 0.6954)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0230 | Time 13.4563(14.5347) | Bit/dim 5.8211(6.1652) | Xent 2.0581(2.0609) | Loss 14.7362(16.1886) | Error 0.7111(0.7157) Steps 0(0.00) | Grad Norm 23.0054(41.5691) | Total Time 0.00(0.00)\n",
      "Iter 0240 | Time 14.2889(14.3950) | Bit/dim 5.7710(6.0579) | Xent 2.0555(2.0584) | Loss 14.8896(15.7711) | Error 0.7678(0.7187) Steps 0(0.00) | Grad Norm 50.6934(42.6629) | Total Time 0.00(0.00)\n",
      "Iter 0250 | Time 13.3298(14.2789) | Bit/dim 5.6876(5.9672) | Xent 1.9928(2.0463) | Loss 14.4301(15.4582) | Error 0.6844(0.7094) Steps 0(0.00) | Grad Norm 27.3235(37.2999) | Total Time 0.00(0.00)\n",
      "Iter 0260 | Time 13.5689(14.1803) | Bit/dim 5.6387(5.8859) | Xent 1.9743(2.0306) | Loss 14.3386(15.2068) | Error 0.6933(0.6999) Steps 0(0.00) | Grad Norm 23.8638(33.7374) | Total Time 0.00(0.00)\n",
      "Iter 0270 | Time 13.6628(14.1514) | Bit/dim 5.6171(5.8225) | Xent 1.9961(2.0202) | Loss 13.8434(15.0054) | Error 0.6900(0.6953) Steps 0(0.00) | Grad Norm 58.3580(36.4423) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0005 | Time 80.4644, Epoch Time 871.5734(821.0472), Bit/dim 5.6199(best: 5.8657), Xent 1.9482, Loss 6.5940, Error 0.6483(best: 0.6859)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0280 | Time 14.4038(14.1876) | Bit/dim 5.5661(5.7638) | Xent 1.9962(2.0086) | Loss 14.4543(15.4100) | Error 0.7022(0.6930) Steps 0(0.00) | Grad Norm 48.1712(35.9411) | Total Time 0.00(0.00)\n",
      "Iter 0290 | Time 14.0826(14.0749) | Bit/dim 5.6112(5.7214) | Xent 1.9371(1.9966) | Loss 14.4275(15.1207) | Error 0.6544(0.6877) Steps 0(0.00) | Grad Norm 51.4497(35.7866) | Total Time 0.00(0.00)\n",
      "Iter 0300 | Time 13.6567(14.0333) | Bit/dim 5.5262(5.6829) | Xent 1.9018(1.9817) | Loss 14.0546(14.8983) | Error 0.6522(0.6812) Steps 0(0.00) | Grad Norm 12.2158(33.1980) | Total Time 0.00(0.00)\n",
      "Iter 0310 | Time 13.7385(13.9915) | Bit/dim 5.5169(5.6491) | Xent 1.9902(1.9840) | Loss 13.7718(14.7316) | Error 0.6922(0.6858) Steps 0(0.00) | Grad Norm 37.8967(36.8987) | Total Time 0.00(0.00)\n",
      "Iter 0320 | Time 13.2603(13.9887) | Bit/dim 5.5026(5.6155) | Xent 1.9756(1.9872) | Loss 14.0948(14.5808) | Error 0.6578(0.6901) Steps 0(0.00) | Grad Norm 30.7909(38.2498) | Total Time 0.00(0.00)\n",
      "Iter 0330 | Time 14.8194(14.0537) | Bit/dim 5.5338(5.5887) | Xent 1.9664(1.9871) | Loss 14.2420(14.4663) | Error 0.6978(0.6912) Steps 0(0.00) | Grad Norm 43.2103(39.4368) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0006 | Time 78.9630, Epoch Time 866.6327(822.4148), Bit/dim 5.4763(best: 5.6199), Xent 1.9248, Loss 6.4387, Error 0.6377(best: 0.6483)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0340 | Time 14.4442(14.0836) | Bit/dim 5.4726(5.5597) | Xent 1.9508(1.9748) | Loss 14.1779(14.8937) | Error 0.6644(0.6844) Steps 0(0.00) | Grad Norm 20.2961(34.3260) | Total Time 0.00(0.00)\n",
      "Iter 0350 | Time 13.8568(14.1459) | Bit/dim 5.3762(5.5190) | Xent 1.8845(1.9559) | Loss 13.8838(14.6359) | Error 0.6411(0.6749) Steps 0(0.00) | Grad Norm 11.4191(29.1672) | Total Time 0.00(0.00)\n",
      "Iter 0360 | Time 13.8920(14.1769) | Bit/dim 5.3349(5.4838) | Xent 1.9547(1.9477) | Loss 13.6656(14.4188) | Error 0.6911(0.6737) Steps 0(0.00) | Grad Norm 23.6723(29.0543) | Total Time 0.00(0.00)\n",
      "Iter 0370 | Time 14.5851(14.2897) | Bit/dim 5.3762(5.4517) | Xent 1.9475(1.9443) | Loss 13.7674(14.2616) | Error 0.6900(0.6731) Steps 0(0.00) | Grad Norm 41.5824(32.6297) | Total Time 0.00(0.00)\n",
      "Iter 0380 | Time 14.4404(14.4063) | Bit/dim 5.3093(5.4159) | Xent 1.9070(1.9419) | Loss 13.5403(14.1247) | Error 0.6778(0.6738) Steps 0(0.00) | Grad Norm 14.7691(32.3042) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0007 | Time 82.0895, Epoch Time 893.0131(824.5328), Bit/dim 5.2700(best: 5.4763), Xent 1.8652, Loss 6.2026, Error 0.6267(best: 0.6377)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0390 | Time 15.0412(14.5032) | Bit/dim 5.2316(5.3859) | Xent 1.8562(1.9344) | Loss 13.6040(14.6231) | Error 0.6622(0.6723) Steps 0(0.00) | Grad Norm 21.1513(29.3507) | Total Time 0.00(0.00)\n",
      "Iter 0400 | Time 15.4652(14.5640) | Bit/dim 5.2963(5.3493) | Xent 1.8883(1.9183) | Loss 13.9130(14.3454) | Error 0.6622(0.6654) Steps 0(0.00) | Grad Norm 27.2642(26.1014) | Total Time 0.00(0.00)\n",
      "Iter 0410 | Time 15.3025(14.6316) | Bit/dim 5.3938(5.3278) | Xent 2.2181(1.9419) | Loss 14.0556(14.1925) | Error 0.7778(0.6771) Steps 0(0.00) | Grad Norm 101.4521(32.3191) | Total Time 0.00(0.00)\n",
      "Iter 0420 | Time 17.2199(14.7725) | Bit/dim 5.2251(5.3034) | Xent 1.9142(1.9593) | Loss 13.6679(14.0557) | Error 0.6756(0.6856) Steps 0(0.00) | Grad Norm 18.8687(32.4954) | Total Time 0.00(0.00)\n",
      "Iter 0430 | Time 14.6161(14.7786) | Bit/dim 5.2125(5.2698) | Xent 1.9417(1.9571) | Loss 13.6121(13.8884) | Error 0.6889(0.6865) Steps 0(0.00) | Grad Norm 28.4824(30.1931) | Total Time 0.00(0.00)\n",
      "Iter 0440 | Time 15.5774(14.8745) | Bit/dim 5.2100(5.2380) | Xent 1.9061(1.9473) | Loss 13.4532(13.7521) | Error 0.6600(0.6807) Steps 0(0.00) | Grad Norm 13.5494(26.5433) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0008 | Time 81.5238, Epoch Time 920.1571(827.4015), Bit/dim 5.1266(best: 5.2700), Xent 1.8887, Loss 6.0710, Error 0.6450(best: 0.6267)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0450 | Time 15.9819(14.9053) | Bit/dim 5.1204(5.2079) | Xent 1.9143(1.9361) | Loss 13.0324(14.1885) | Error 0.6544(0.6750) Steps 0(0.00) | Grad Norm 9.9468(24.5875) | Total Time 0.00(0.00)\n",
      "Iter 0460 | Time 17.0758(15.1528) | Bit/dim 5.0851(5.1734) | Xent 1.8835(1.9183) | Loss 13.3300(13.9405) | Error 0.6589(0.6685) Steps 0(0.00) | Grad Norm 16.5140(23.2580) | Total Time 0.00(0.00)\n",
      "Iter 0470 | Time 14.7033(15.2356) | Bit/dim 5.0594(5.1460) | Xent 1.8672(1.9073) | Loss 13.0071(13.7480) | Error 0.6644(0.6650) Steps 0(0.00) | Grad Norm 22.9714(24.6683) | Total Time 0.00(0.00)\n",
      "Iter 0480 | Time 16.2898(15.3283) | Bit/dim 5.0136(5.1135) | Xent 1.8524(1.8995) | Loss 13.1526(13.5710) | Error 0.6367(0.6627) Steps 0(0.00) | Grad Norm 22.5223(24.4381) | Total Time 0.00(0.00)\n",
      "Iter 0490 | Time 15.3238(15.4067) | Bit/dim 5.0203(5.0861) | Xent 1.8485(1.8916) | Loss 13.1954(13.4465) | Error 0.6400(0.6615) Steps 0(0.00) | Grad Norm 14.6513(26.3920) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0009 | Time 83.6453, Epoch Time 954.9131(831.2268), Bit/dim 4.9557(best: 5.1266), Xent 1.8668, Loss 5.8891, Error 0.6615(best: 0.6267)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0500 | Time 14.7060(15.3777) | Bit/dim 4.9806(5.0602) | Xent 1.8639(1.8943) | Loss 12.7802(13.9034) | Error 0.6611(0.6635) Steps 0(0.00) | Grad Norm 19.8586(31.3058) | Total Time 0.00(0.00)\n",
      "Iter 0510 | Time 15.0794(15.3797) | Bit/dim 5.0191(5.0500) | Xent 1.9220(1.9204) | Loss 13.2110(13.7204) | Error 0.6589(0.6752) Steps 0(0.00) | Grad Norm 24.8070(35.2611) | Total Time 0.00(0.00)\n",
      "Iter 0520 | Time 16.3764(15.5356) | Bit/dim 4.9278(5.0230) | Xent 1.8875(1.9218) | Loss 13.1106(13.5432) | Error 0.6411(0.6756) Steps 0(0.00) | Grad Norm 12.6716(30.3118) | Total Time 0.00(0.00)\n",
      "Iter 0530 | Time 15.2749(15.7000) | Bit/dim 4.8849(4.9896) | Xent 1.9612(1.9223) | Loss 12.6471(13.3833) | Error 0.6911(0.6755) Steps 0(0.00) | Grad Norm 17.7246(25.2641) | Total Time 0.00(0.00)\n",
      "Iter 0540 | Time 16.3435(15.8178) | Bit/dim 4.8819(4.9654) | Xent 1.8810(1.9218) | Loss 12.9580(13.2445) | Error 0.6733(0.6766) Steps 0(0.00) | Grad Norm 25.6004(25.7563) | Total Time 0.00(0.00)\n",
      "Iter 0550 | Time 17.5695(15.9721) | Bit/dim 4.8066(4.9378) | Xent 1.9509(1.9099) | Loss 12.7936(13.1226) | Error 0.6833(0.6714) Steps 0(0.00) | Grad Norm 19.8695(24.0968) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0010 | Time 84.7379, Epoch Time 977.5682(835.6171), Bit/dim 4.8375(best: 4.9557), Xent 1.8357, Loss 5.7554, Error 0.6374(best: 0.6267)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0560 | Time 16.1069(15.9746) | Bit/dim 4.8202(4.9116) | Xent 1.8382(1.8910) | Loss 12.6259(13.5656) | Error 0.6467(0.6640) Steps 0(0.00) | Grad Norm 33.7391(25.8118) | Total Time 0.00(0.00)\n",
      "Iter 0570 | Time 15.1110(16.0116) | Bit/dim 4.8301(4.8923) | Xent 1.8370(1.8716) | Loss 12.5494(13.3137) | Error 0.6367(0.6587) Steps 0(0.00) | Grad Norm 22.0854(26.3699) | Total Time 0.00(0.00)\n",
      "Iter 0580 | Time 16.6600(16.0679) | Bit/dim 4.8319(4.8713) | Xent 1.8199(1.8606) | Loss 12.6488(13.1649) | Error 0.6278(0.6535) Steps 0(0.00) | Grad Norm 15.9614(26.9047) | Total Time 0.00(0.00)\n",
      "Iter 0590 | Time 16.3371(16.1484) | Bit/dim 4.8112(4.8516) | Xent 1.8626(1.8468) | Loss 12.3465(13.0098) | Error 0.6667(0.6503) Steps 0(0.00) | Grad Norm 57.4655(27.9070) | Total Time 0.00(0.00)\n",
      "Iter 0600 | Time 16.2814(16.0580) | Bit/dim 4.8458(4.8462) | Xent 1.9552(1.8683) | Loss 12.7168(12.9432) | Error 0.6800(0.6576) Steps 0(0.00) | Grad Norm 45.5450(35.2525) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0011 | Time 85.8080, Epoch Time 992.6263(840.3274), Bit/dim 4.7674(best: 4.8375), Xent 1.8301, Loss 5.6824, Error 0.6544(best: 0.6267)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0610 | Time 15.5727(16.1879) | Bit/dim 4.7711(4.8272) | Xent 1.8657(1.8708) | Loss 12.1977(13.4929) | Error 0.6656(0.6610) Steps 0(0.00) | Grad Norm 12.5496(30.8838) | Total Time 0.00(0.00)\n",
      "Iter 0620 | Time 15.4789(16.3371) | Bit/dim 4.7747(4.8044) | Xent 1.7689(1.8550) | Loss 12.6118(13.2349) | Error 0.6078(0.6550) Steps 0(0.00) | Grad Norm 49.6352(27.2437) | Total Time 0.00(0.00)\n",
      "Iter 0630 | Time 16.5715(16.2372) | Bit/dim 4.7840(4.8230) | Xent 1.7961(1.8620) | Loss 12.3620(13.1074) | Error 0.6311(0.6568) Steps 0(0.00) | Grad Norm 13.0013(30.9057) | Total Time 0.00(0.00)\n",
      "Iter 0640 | Time 16.0252(16.2356) | Bit/dim 4.7404(4.8075) | Xent 1.8249(1.8581) | Loss 12.6214(12.9661) | Error 0.6322(0.6531) Steps 0(0.00) | Grad Norm 18.6083(27.4860) | Total Time 0.00(0.00)\n",
      "Iter 0650 | Time 15.5608(16.0660) | Bit/dim 4.6938(4.7818) | Xent 1.8021(1.8385) | Loss 12.2860(12.8113) | Error 0.6389(0.6472) Steps 0(0.00) | Grad Norm 11.8984(23.2955) | Total Time 0.00(0.00)\n",
      "Iter 0660 | Time 15.8651(16.0425) | Bit/dim 4.6854(4.7578) | Xent 1.8242(1.8217) | Loss 12.5209(12.6987) | Error 0.6378(0.6422) Steps 0(0.00) | Grad Norm 29.3353(21.2962) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0012 | Time 85.3080, Epoch Time 989.1962(844.7934), Bit/dim 4.6704(best: 4.7674), Xent 1.6626, Loss 5.5017, Error 0.5792(best: 0.6267)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0670 | Time 15.4718(16.0532) | Bit/dim 4.6700(4.7386) | Xent 1.7253(1.8029) | Loss 12.3090(13.1285) | Error 0.6311(0.6368) Steps 0(0.00) | Grad Norm 31.5332(24.5281) | Total Time 0.00(0.00)\n",
      "Iter 0680 | Time 14.8834(16.1363) | Bit/dim 4.6236(4.7178) | Xent 1.7233(1.7817) | Loss 12.2102(12.8948) | Error 0.6100(0.6299) Steps 0(0.00) | Grad Norm 26.8981(25.0265) | Total Time 0.00(0.00)\n",
      "Iter 0690 | Time 15.9992(16.2108) | Bit/dim 4.6950(4.7171) | Xent 1.8852(1.7986) | Loss 12.2093(12.7793) | Error 0.6789(0.6355) Steps 0(0.00) | Grad Norm 27.4458(29.4903) | Total Time 0.00(0.00)\n",
      "Iter 0700 | Time 16.5705(16.1317) | Bit/dim 4.6753(4.7027) | Xent 1.7591(1.7966) | Loss 12.2801(12.6511) | Error 0.6556(0.6390) Steps 0(0.00) | Grad Norm 6.6893(26.0204) | Total Time 0.00(0.00)\n",
      "Iter 0710 | Time 16.1426(16.1854) | Bit/dim 4.6332(4.6860) | Xent 1.6713(1.7815) | Loss 12.3191(12.5607) | Error 0.5789(0.6340) Steps 0(0.00) | Grad Norm 19.3462(24.3636) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0013 | Time 84.8336, Epoch Time 992.2000(849.2156), Bit/dim 4.6135(best: 4.6704), Xent 1.6480, Loss 5.4375, Error 0.5763(best: 0.5792)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0720 | Time 16.7004(16.2247) | Bit/dim 4.6500(4.6693) | Xent 1.7259(1.7689) | Loss 12.1965(13.1090) | Error 0.6211(0.6289) Steps 0(0.00) | Grad Norm 51.1687(23.1599) | Total Time 0.00(0.00)\n",
      "Iter 0730 | Time 15.3211(16.1462) | Bit/dim 4.5918(4.6668) | Xent 1.7571(1.7683) | Loss 11.7942(12.8750) | Error 0.6267(0.6285) Steps 0(0.00) | Grad Norm 16.9909(26.3920) | Total Time 0.00(0.00)\n",
      "Iter 0740 | Time 15.3475(16.4318) | Bit/dim 4.5458(4.6473) | Xent 1.6881(1.7556) | Loss 12.1128(12.6871) | Error 0.6167(0.6247) Steps 0(0.00) | Grad Norm 15.5650(24.0790) | Total Time 0.00(0.00)\n",
      "Iter 0750 | Time 16.3168(16.7002) | Bit/dim 4.6025(4.6326) | Xent 1.7002(1.7407) | Loss 12.0826(12.5180) | Error 0.6089(0.6194) Steps 0(0.00) | Grad Norm 32.7339(23.9572) | Total Time 0.00(0.00)\n",
      "Iter 0760 | Time 16.0332(16.6283) | Bit/dim 4.6366(4.6303) | Xent 1.7307(1.7439) | Loss 12.4207(12.4466) | Error 0.5922(0.6200) Steps 0(0.00) | Grad Norm 27.8275(27.0901) | Total Time 0.00(0.00)\n",
      "Iter 0770 | Time 16.6483(16.6629) | Bit/dim 4.5837(4.6169) | Xent 1.6932(1.7517) | Loss 12.1271(12.3602) | Error 0.6067(0.6229) Steps 0(0.00) | Grad Norm 15.2384(26.4460) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0014 | Time 85.2907, Epoch Time 1024.1755(854.4644), Bit/dim 4.5504(best: 4.6135), Xent 1.6429, Loss 5.3719, Error 0.5789(best: 0.5763)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0780 | Time 17.5462(16.7147) | Bit/dim 4.5494(4.6019) | Xent 1.6389(1.7314) | Loss 11.6994(12.8370) | Error 0.5811(0.6162) Steps 0(0.00) | Grad Norm 10.5097(23.2859) | Total Time 0.00(0.00)\n",
      "Iter 0790 | Time 15.4567(16.6166) | Bit/dim 4.5239(4.5859) | Xent 1.6365(1.7104) | Loss 12.0507(12.6136) | Error 0.6244(0.6103) Steps 0(0.00) | Grad Norm 6.5572(20.6959) | Total Time 0.00(0.00)\n",
      "Iter 0800 | Time 17.2677(16.6189) | Bit/dim 4.5395(4.5712) | Xent 1.7014(1.6955) | Loss 11.9329(12.4361) | Error 0.5844(0.6031) Steps 0(0.00) | Grad Norm 25.4020(19.9771) | Total Time 0.00(0.00)\n",
      "Iter 0810 | Time 17.2558(16.6951) | Bit/dim 4.4926(4.5589) | Xent 1.7331(1.6998) | Loss 11.9148(12.3235) | Error 0.6278(0.6043) Steps 0(0.00) | Grad Norm 18.1569(23.6601) | Total Time 0.00(0.00)\n",
      "Iter 0820 | Time 19.4776(16.7403) | Bit/dim 4.4984(4.5435) | Xent 1.7502(1.7026) | Loss 11.8427(12.2061) | Error 0.6089(0.6044) Steps 0(0.00) | Grad Norm 28.9792(23.3202) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0015 | Time 84.6091, Epoch Time 1021.3334(859.4705), Bit/dim 4.5131(best: 4.5504), Xent 1.5909, Loss 5.3086, Error 0.5620(best: 0.5763)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0830 | Time 16.8422(16.7897) | Bit/dim 4.4616(4.5312) | Xent 1.6411(1.6887) | Loss 11.6929(12.7998) | Error 0.5878(0.5996) Steps 0(0.00) | Grad Norm 13.4768(22.7453) | Total Time 0.00(0.00)\n",
      "Iter 0840 | Time 16.1415(16.8511) | Bit/dim 4.5105(4.5169) | Xent 1.6711(1.6763) | Loss 11.8086(12.5362) | Error 0.6056(0.5959) Steps 0(0.00) | Grad Norm 26.9967(21.0117) | Total Time 0.00(0.00)\n",
      "Iter 0850 | Time 17.0343(16.7909) | Bit/dim 4.4612(4.5080) | Xent 1.6495(1.6773) | Loss 11.7976(12.3641) | Error 0.6056(0.5991) Steps 0(0.00) | Grad Norm 18.3164(21.7266) | Total Time 0.00(0.00)\n",
      "Iter 0860 | Time 15.2811(16.7502) | Bit/dim 4.4498(4.5002) | Xent 1.6164(1.6639) | Loss 11.5223(12.2041) | Error 0.5944(0.5961) Steps 0(0.00) | Grad Norm 7.1333(20.8271) | Total Time 0.00(0.00)\n",
      "Iter 0870 | Time 16.4639(16.7963) | Bit/dim 4.4743(4.4887) | Xent 1.6486(1.6573) | Loss 11.8629(12.1257) | Error 0.5767(0.5953) Steps 0(0.00) | Grad Norm 29.3945(21.0177) | Total Time 0.00(0.00)\n",
      "Iter 0880 | Time 18.5674(16.9497) | Bit/dim 4.4210(4.4732) | Xent 1.6067(1.6487) | Loss 11.7314(12.0351) | Error 0.5778(0.5922) Steps 0(0.00) | Grad Norm 13.8191(19.7348) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0016 | Time 85.0834, Epoch Time 1030.7617(864.6092), Bit/dim 4.4432(best: 4.5131), Xent 1.5693, Loss 5.2279, Error 0.5603(best: 0.5620)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0890 | Time 15.8924(16.8747) | Bit/dim 4.4237(4.4655) | Xent 1.6110(1.6367) | Loss 11.5998(12.5190) | Error 0.5767(0.5881) Steps 0(0.00) | Grad Norm 17.5574(19.4504) | Total Time 0.00(0.00)\n",
      "Iter 0900 | Time 17.5596(16.9685) | Bit/dim 4.3881(4.4481) | Xent 1.5030(1.6207) | Loss 11.3192(12.2775) | Error 0.5444(0.5830) Steps 0(0.00) | Grad Norm 9.1249(17.3285) | Total Time 0.00(0.00)\n",
      "Iter 0910 | Time 18.3894(17.1018) | Bit/dim 4.7393(4.5218) | Xent 1.8090(1.6606) | Loss 12.6356(12.3593) | Error 0.6178(0.5935) Steps 0(0.00) | Grad Norm 33.2421(23.1021) | Total Time 0.00(0.00)\n",
      "Iter 0920 | Time 16.2448(16.9945) | Bit/dim 4.5235(4.5498) | Xent 1.7290(1.6879) | Loss 11.6530(12.3079) | Error 0.6267(0.6025) Steps 0(0.00) | Grad Norm 8.1111(21.1850) | Total Time 0.00(0.00)\n",
      "Iter 0930 | Time 16.5833(16.8416) | Bit/dim 4.4629(4.5363) | Xent 1.7120(1.6854) | Loss 11.7731(12.1878) | Error 0.6189(0.6028) Steps 0(0.00) | Grad Norm 20.7719(19.3101) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0017 | Time 81.5784, Epoch Time 1020.9519(869.2995), Bit/dim 4.4279(best: 4.4432), Xent 1.5500, Loss 5.2029, Error 0.5561(best: 0.5603)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0940 | Time 17.1092(16.6289) | Bit/dim 4.4346(4.5068) | Xent 1.5312(1.6674) | Loss 11.6252(12.6438) | Error 0.5578(0.5989) Steps 0(0.00) | Grad Norm 6.8841(16.7748) | Total Time 0.00(0.00)\n",
      "Iter 0950 | Time 16.6218(16.4916) | Bit/dim 4.3630(4.4756) | Xent 1.5144(1.6403) | Loss 11.6104(12.3415) | Error 0.5344(0.5903) Steps 0(0.00) | Grad Norm 19.0920(15.8525) | Total Time 0.00(0.00)\n",
      "Iter 0960 | Time 16.4183(16.5498) | Bit/dim 4.3813(4.4465) | Xent 1.6023(1.6260) | Loss 11.2721(12.0988) | Error 0.5878(0.5867) Steps 0(0.00) | Grad Norm 12.4919(15.0247) | Total Time 0.00(0.00)\n",
      "Iter 0970 | Time 15.4541(16.7226) | Bit/dim 4.3206(4.4214) | Xent 1.6334(1.6127) | Loss 11.5310(11.9559) | Error 0.5756(0.5819) Steps 0(0.00) | Grad Norm 18.2944(13.7495) | Total Time 0.00(0.00)\n",
      "Iter 0980 | Time 16.8693(16.6947) | Bit/dim 4.3560(4.4010) | Xent 1.5978(1.6052) | Loss 11.5130(11.8441) | Error 0.5689(0.5806) Steps 0(0.00) | Grad Norm 15.2406(14.5797) | Total Time 0.00(0.00)\n",
      "Iter 0990 | Time 20.5237(16.8586) | Bit/dim 4.2991(4.3816) | Xent 1.6081(1.5979) | Loss 11.5454(11.7456) | Error 0.5944(0.5788) Steps 0(0.00) | Grad Norm 12.8731(13.3002) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0018 | Time 88.7826, Epoch Time 1024.2807(873.9489), Bit/dim 4.3145(best: 4.4279), Xent 1.4819, Loss 5.0554, Error 0.5325(best: 0.5561)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1000 | Time 17.2741(16.9494) | Bit/dim 4.2950(4.3606) | Xent 1.4991(1.5815) | Loss 11.5490(12.2524) | Error 0.5544(0.5741) Steps 0(0.00) | Grad Norm 20.0856(12.8834) | Total Time 0.00(0.00)\n",
      "Iter 1010 | Time 16.8853(16.9463) | Bit/dim 4.3129(4.3480) | Xent 1.5656(1.5784) | Loss 11.3231(12.0291) | Error 0.5800(0.5720) Steps 0(0.00) | Grad Norm 7.3135(14.3742) | Total Time 0.00(0.00)\n",
      "Iter 1020 | Time 18.4864(16.9919) | Bit/dim 4.3965(4.3540) | Xent 1.6688(1.6000) | Loss 11.8210(11.9426) | Error 0.6167(0.5767) Steps 0(0.00) | Grad Norm 23.9494(19.2066) | Total Time 0.00(0.00)\n",
      "Iter 1030 | Time 16.7418(16.9074) | Bit/dim 4.2884(4.3489) | Xent 1.5397(1.5944) | Loss 11.2801(11.8318) | Error 0.5711(0.5748) Steps 0(0.00) | Grad Norm 9.2232(18.6736) | Total Time 0.00(0.00)\n",
      "Iter 1040 | Time 16.5690(16.9084) | Bit/dim 4.2923(4.3343) | Xent 1.5852(1.5807) | Loss 11.2085(11.6919) | Error 0.5711(0.5709) Steps 0(0.00) | Grad Norm 11.5283(16.1708) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0019 | Time 87.1339, Epoch Time 1044.4240(879.0632), Bit/dim 4.2683(best: 4.3145), Xent 1.4473, Loss 4.9920, Error 0.5262(best: 0.5325)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1050 | Time 17.1954(17.1664) | Bit/dim 4.2379(4.3179) | Xent 1.5200(1.5643) | Loss 11.1904(12.2048) | Error 0.5511(0.5667) Steps 0(0.00) | Grad Norm 12.3764(15.2723) | Total Time 0.00(0.00)\n",
      "Iter 1060 | Time 17.2389(17.3473) | Bit/dim 4.2368(4.3018) | Xent 1.4749(1.5517) | Loss 11.3800(11.9747) | Error 0.5144(0.5621) Steps 0(0.00) | Grad Norm 15.4520(15.4966) | Total Time 0.00(0.00)\n",
      "Iter 1070 | Time 17.0659(17.4119) | Bit/dim 4.2245(4.2824) | Xent 1.4794(1.5369) | Loss 11.1627(11.7764) | Error 0.5300(0.5567) Steps 0(0.00) | Grad Norm 14.0206(13.9441) | Total Time 0.00(0.00)\n",
      "Iter 1080 | Time 18.5228(17.3723) | Bit/dim 4.2209(4.2697) | Xent 1.5694(1.5293) | Loss 11.3484(11.6345) | Error 0.5511(0.5545) Steps 0(0.00) | Grad Norm 17.4606(14.4382) | Total Time 0.00(0.00)\n",
      "Iter 1090 | Time 17.2550(17.2494) | Bit/dim 4.2260(4.2602) | Xent 1.5509(1.5312) | Loss 11.3706(11.5199) | Error 0.5678(0.5557) Steps 0(0.00) | Grad Norm 20.5700(15.9812) | Total Time 0.00(0.00)\n",
      "Iter 1100 | Time 16.4652(17.2788) | Bit/dim 4.2258(4.2536) | Xent 1.5515(1.5338) | Loss 11.1992(11.4416) | Error 0.5756(0.5577) Steps 0(0.00) | Grad Norm 8.0211(16.4923) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0020 | Time 86.0996, Epoch Time 1058.2226(884.4380), Bit/dim 4.2091(best: 4.2683), Xent 1.4291, Loss 4.9236, Error 0.5180(best: 0.5262)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1110 | Time 16.8173(17.3451) | Bit/dim 4.2311(4.2433) | Xent 1.4857(1.5239) | Loss 11.1252(11.9337) | Error 0.5444(0.5536) Steps 0(0.00) | Grad Norm 4.9660(16.2501) | Total Time 0.00(0.00)\n",
      "Iter 1120 | Time 18.3663(17.2048) | Bit/dim 4.1989(4.2293) | Xent 1.3668(1.5057) | Loss 11.0101(11.7054) | Error 0.4922(0.5450) Steps 0(0.00) | Grad Norm 15.6401(14.8932) | Total Time 0.00(0.00)\n",
      "Iter 1130 | Time 19.3480(17.2860) | Bit/dim 4.2089(4.2264) | Xent 1.4543(1.4981) | Loss 11.0093(11.5557) | Error 0.5478(0.5448) Steps 0(0.00) | Grad Norm 8.6433(16.3093) | Total Time 0.00(0.00)\n",
      "Iter 1140 | Time 16.9788(17.1946) | Bit/dim 4.2076(4.2217) | Xent 1.4480(1.5000) | Loss 10.8976(11.4519) | Error 0.5278(0.5441) Steps 0(0.00) | Grad Norm 9.2449(16.2970) | Total Time 0.00(0.00)\n",
      "Iter 1150 | Time 16.5798(17.1100) | Bit/dim 4.1482(4.2095) | Xent 1.5008(1.4947) | Loss 11.0472(11.3168) | Error 0.5289(0.5414) Steps 0(0.00) | Grad Norm 19.8995(15.7394) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0021 | Time 86.9670, Epoch Time 1047.6719(889.3350), Bit/dim 4.1694(best: 4.2091), Xent 1.3616, Loss 4.8502, Error 0.4942(best: 0.5180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1160 | Time 16.7454(17.2589) | Bit/dim 4.1594(4.2009) | Xent 1.4717(1.4816) | Loss 11.0151(11.9151) | Error 0.5322(0.5367) Steps 0(0.00) | Grad Norm 23.0492(15.5207) | Total Time 0.00(0.00)\n",
      "Iter 1170 | Time 17.6689(17.1812) | Bit/dim 4.1516(4.1940) | Xent 1.4298(1.4710) | Loss 11.0435(11.6801) | Error 0.5200(0.5316) Steps 0(0.00) | Grad Norm 18.5772(15.6654) | Total Time 0.00(0.00)\n",
      "Iter 1180 | Time 15.9251(17.2168) | Bit/dim 4.1669(4.1923) | Xent 1.4507(1.4693) | Loss 10.7006(11.5088) | Error 0.5311(0.5330) Steps 0(0.00) | Grad Norm 22.1066(17.1913) | Total Time 0.00(0.00)\n",
      "Iter 1190 | Time 19.0601(17.2029) | Bit/dim 4.1499(4.1804) | Xent 1.5659(1.4619) | Loss 11.0553(11.3559) | Error 0.5667(0.5300) Steps 0(0.00) | Grad Norm 15.7549(16.8141) | Total Time 0.00(0.00)\n",
      "Iter 1200 | Time 16.8084(17.1509) | Bit/dim 4.1534(4.1681) | Xent 1.4546(1.4553) | Loss 11.0482(11.2567) | Error 0.5267(0.5268) Steps 0(0.00) | Grad Norm 7.5273(15.0615) | Total Time 0.00(0.00)\n",
      "Iter 1210 | Time 17.7138(17.1084) | Bit/dim 4.1203(4.1576) | Xent 1.3548(1.4475) | Loss 10.8204(11.1483) | Error 0.5067(0.5256) Steps 0(0.00) | Grad Norm 5.2395(13.9815) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0022 | Time 84.8095, Epoch Time 1041.7338(893.9070), Bit/dim 4.1268(best: 4.1694), Xent 1.3660, Loss 4.8098, Error 0.5029(best: 0.4942)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1220 | Time 15.8292(17.0627) | Bit/dim 4.1149(4.1500) | Xent 1.5583(1.4499) | Loss 11.1486(11.6623) | Error 0.5456(0.5264) Steps 0(0.00) | Grad Norm 13.4141(15.7472) | Total Time 0.00(0.00)\n",
      "Iter 1230 | Time 16.6256(16.8671) | Bit/dim 4.1534(4.1481) | Xent 1.4363(1.4535) | Loss 10.7645(11.4608) | Error 0.5033(0.5278) Steps 0(0.00) | Grad Norm 17.1574(15.5148) | Total Time 0.00(0.00)\n",
      "Iter 1240 | Time 17.1610(16.8716) | Bit/dim 4.1266(4.1407) | Xent 1.3547(1.4391) | Loss 10.8232(11.2732) | Error 0.4911(0.5231) Steps 0(0.00) | Grad Norm 11.3788(14.7192) | Total Time 0.00(0.00)\n",
      "Iter 1250 | Time 18.5085(16.8823) | Bit/dim 4.1171(4.1370) | Xent 1.3547(1.4282) | Loss 10.9591(11.1789) | Error 0.4667(0.5182) Steps 0(0.00) | Grad Norm 13.3010(14.5441) | Total Time 0.00(0.00)\n",
      "Iter 1260 | Time 15.2435(16.8392) | Bit/dim 4.1173(4.1300) | Xent 1.3522(1.4168) | Loss 10.6098(11.0703) | Error 0.5011(0.5162) Steps 0(0.00) | Grad Norm 9.2320(14.3568) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0023 | Time 84.3990, Epoch Time 1024.9326(897.8377), Bit/dim 4.1038(best: 4.1268), Xent 1.3034, Loss 4.7556, Error 0.4714(best: 0.4942)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1270 | Time 18.2261(17.0046) | Bit/dim 4.0743(4.1215) | Xent 1.3656(1.4027) | Loss 10.9041(11.6631) | Error 0.5044(0.5103) Steps 0(0.00) | Grad Norm 13.1566(13.6771) | Total Time 0.00(0.00)\n",
      "Iter 1280 | Time 15.7181(16.9610) | Bit/dim 4.1246(4.1148) | Xent 1.3631(1.3901) | Loss 10.6008(11.4268) | Error 0.4989(0.5052) Steps 0(0.00) | Grad Norm 17.0766(13.3892) | Total Time 0.00(0.00)\n",
      "Iter 1290 | Time 18.3965(16.8832) | Bit/dim 4.0854(4.1101) | Xent 1.3667(1.3863) | Loss 10.8818(11.2777) | Error 0.5000(0.5039) Steps 0(0.00) | Grad Norm 6.7034(14.3386) | Total Time 0.00(0.00)\n",
      "Iter 1300 | Time 16.3196(16.8135) | Bit/dim 4.1157(4.1051) | Xent 1.3759(1.3903) | Loss 10.7537(11.1447) | Error 0.4822(0.5049) Steps 0(0.00) | Grad Norm 18.7492(14.8745) | Total Time 0.00(0.00)\n",
      "Iter 1310 | Time 17.7581(17.2789) | Bit/dim 4.0907(4.1009) | Xent 1.3496(1.3796) | Loss 11.0068(11.0467) | Error 0.4856(0.5021) Steps 0(0.00) | Grad Norm 9.0050(14.6417) | Total Time 0.00(0.00)\n",
      "Iter 1320 | Time 16.1266(17.1500) | Bit/dim 4.0725(4.0913) | Xent 1.3141(1.3707) | Loss 10.7522(10.9380) | Error 0.4667(0.4981) Steps 0(0.00) | Grad Norm 12.2173(13.3919) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0024 | Time 85.4754, Epoch Time 1042.7147(902.1840), Bit/dim 4.0649(best: 4.1038), Xent 1.2653, Loss 4.6975, Error 0.4540(best: 0.4714)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1330 | Time 16.4347(17.0454) | Bit/dim 4.0899(4.0855) | Xent 1.3275(1.3563) | Loss 10.8266(11.4553) | Error 0.4733(0.4925) Steps 0(0.00) | Grad Norm 23.3289(12.8049) | Total Time 0.00(0.00)\n",
      "Iter 1340 | Time 15.8703(17.0479) | Bit/dim 4.0693(4.0809) | Xent 1.3297(1.3509) | Loss 10.4400(11.2455) | Error 0.4811(0.4901) Steps 0(0.00) | Grad Norm 16.8939(13.7146) | Total Time 0.00(0.00)\n",
      "Iter 1350 | Time 16.8888(16.8954) | Bit/dim 4.0791(4.0764) | Xent 1.3286(1.3467) | Loss 10.6229(11.1034) | Error 0.4622(0.4881) Steps 0(0.00) | Grad Norm 11.7041(13.7650) | Total Time 0.00(0.00)\n",
      "Iter 1360 | Time 16.0326(16.9054) | Bit/dim 4.0765(4.0710) | Xent 1.3740(1.3411) | Loss 10.7874(10.9903) | Error 0.4900(0.4859) Steps 0(0.00) | Grad Norm 16.7831(12.8673) | Total Time 0.00(0.00)\n",
      "Iter 1370 | Time 17.2082(16.8315) | Bit/dim 4.0163(4.0610) | Xent 1.3107(1.3368) | Loss 10.5851(10.8944) | Error 0.4856(0.4846) Steps 0(0.00) | Grad Norm 8.4231(11.5640) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0025 | Time 83.4362, Epoch Time 1021.1311(905.7524), Bit/dim 4.0359(best: 4.0649), Xent 1.2295, Loss 4.6506, Error 0.4488(best: 0.4540)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1380 | Time 17.4043(16.8113) | Bit/dim 4.0048(4.0536) | Xent 1.3104(1.3298) | Loss 10.6530(11.4478) | Error 0.4811(0.4813) Steps 0(0.00) | Grad Norm 6.4031(11.4733) | Total Time 0.00(0.00)\n",
      "Iter 1390 | Time 16.5270(16.8134) | Bit/dim 4.0645(4.0502) | Xent 1.4155(1.3396) | Loss 10.9099(11.2401) | Error 0.5222(0.4843) Steps 0(0.00) | Grad Norm 22.3395(14.3279) | Total Time 0.00(0.00)\n",
      "Iter 1400 | Time 17.0828(16.8334) | Bit/dim 4.0560(4.0485) | Xent 1.2350(1.3321) | Loss 10.5670(11.0742) | Error 0.4644(0.4809) Steps 0(0.00) | Grad Norm 7.9662(13.4834) | Total Time 0.00(0.00)\n",
      "Iter 1410 | Time 16.5429(16.8353) | Bit/dim 4.0452(4.0463) | Xent 1.2392(1.3278) | Loss 10.5211(10.9475) | Error 0.4500(0.4801) Steps 0(0.00) | Grad Norm 8.9413(12.6968) | Total Time 0.00(0.00)\n",
      "Iter 1420 | Time 16.2679(17.0080) | Bit/dim 4.0351(4.0417) | Xent 1.2600(1.3148) | Loss 10.6449(10.8515) | Error 0.4489(0.4758) Steps 0(0.00) | Grad Norm 8.9523(12.0460) | Total Time 0.00(0.00)\n",
      "Iter 1430 | Time 17.8050(17.0227) | Bit/dim 3.9956(4.0380) | Xent 1.2514(1.3046) | Loss 10.5492(10.7764) | Error 0.4489(0.4725) Steps 0(0.00) | Grad Norm 5.8813(11.1854) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0026 | Time 82.3429, Epoch Time 1032.7213(909.5615), Bit/dim 4.0196(best: 4.0359), Xent 1.2182, Loss 4.6287, Error 0.4355(best: 0.4488)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1440 | Time 15.1589(17.0016) | Bit/dim 4.0426(4.0303) | Xent 1.1831(1.2915) | Loss 10.4061(11.2331) | Error 0.4311(0.4667) Steps 0(0.00) | Grad Norm 15.3055(10.3732) | Total Time 0.00(0.00)\n",
      "Iter 1450 | Time 16.0602(16.8684) | Bit/dim 4.0052(4.0255) | Xent 1.2442(1.2813) | Loss 10.2528(11.0224) | Error 0.4678(0.4637) Steps 0(0.00) | Grad Norm 10.1089(10.6169) | Total Time 0.00(0.00)\n",
      "Iter 1460 | Time 16.6401(16.9961) | Bit/dim 3.9552(4.0183) | Xent 1.3297(1.2810) | Loss 10.3258(10.8827) | Error 0.4700(0.4634) Steps 0(0.00) | Grad Norm 26.9927(11.9062) | Total Time 0.00(0.00)\n",
      "Iter 1470 | Time 17.7217(16.9990) | Bit/dim 4.0019(4.0162) | Xent 1.2676(1.2792) | Loss 10.7675(10.8079) | Error 0.4422(0.4636) Steps 0(0.00) | Grad Norm 18.7005(12.9685) | Total Time 0.00(0.00)\n",
      "Iter 1480 | Time 15.3892(16.7771) | Bit/dim 3.9892(4.0128) | Xent 1.3157(1.2790) | Loss 10.1223(10.7094) | Error 0.4667(0.4632) Steps 0(0.00) | Grad Norm 13.4087(12.7451) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0027 | Time 82.6454, Epoch Time 1021.3722(912.9158), Bit/dim 3.9995(best: 4.0196), Xent 1.1874, Loss 4.5932, Error 0.4295(best: 0.4355)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1490 | Time 17.4355(16.7359) | Bit/dim 3.9663(4.0068) | Xent 1.2542(1.2719) | Loss 10.4664(11.2811) | Error 0.4111(0.4588) Steps 0(0.00) | Grad Norm 15.4018(12.7504) | Total Time 0.00(0.00)\n",
      "Iter 1500 | Time 16.4904(16.7128) | Bit/dim 3.9742(4.0021) | Xent 1.2384(1.2622) | Loss 10.2900(11.0511) | Error 0.4578(0.4553) Steps 0(0.00) | Grad Norm 9.8557(12.1600) | Total Time 0.00(0.00)\n",
      "Iter 1510 | Time 16.2349(16.8834) | Bit/dim 3.9667(3.9946) | Xent 1.2043(1.2482) | Loss 10.3634(10.8819) | Error 0.4178(0.4496) Steps 0(0.00) | Grad Norm 11.7919(11.2243) | Total Time 0.00(0.00)\n",
      "Iter 1520 | Time 17.2013(16.9005) | Bit/dim 3.9934(3.9915) | Xent 1.2134(1.2460) | Loss 10.3721(10.7535) | Error 0.4311(0.4498) Steps 0(0.00) | Grad Norm 13.0801(11.6600) | Total Time 0.00(0.00)\n",
      "Iter 1530 | Time 17.2493(16.8782) | Bit/dim 3.9947(3.9874) | Xent 1.1778(1.2379) | Loss 10.5277(10.6520) | Error 0.4144(0.4463) Steps 0(0.00) | Grad Norm 6.5081(11.6787) | Total Time 0.00(0.00)\n",
      "Iter 1540 | Time 15.7084(16.8997) | Bit/dim 3.9956(3.9855) | Xent 1.2253(1.2301) | Loss 10.1223(10.5562) | Error 0.4233(0.4422) Steps 0(0.00) | Grad Norm 14.5955(11.3259) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0028 | Time 83.9376, Epoch Time 1030.8406(916.4536), Bit/dim 3.9783(best: 3.9995), Xent 1.1196, Loss 4.5381, Error 0.4020(best: 0.4295)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1550 | Time 17.8926(16.9508) | Bit/dim 3.9784(3.9835) | Xent 1.1979(1.2228) | Loss 10.4538(11.1003) | Error 0.4400(0.4398) Steps 0(0.00) | Grad Norm 8.9518(11.6417) | Total Time 0.00(0.00)\n",
      "Iter 1560 | Time 16.1903(16.9012) | Bit/dim 3.9556(3.9805) | Xent 1.2094(1.2379) | Loss 10.0857(10.9058) | Error 0.4433(0.4445) Steps 0(0.00) | Grad Norm 11.7124(13.1365) | Total Time 0.00(0.00)\n",
      "Iter 1570 | Time 16.6144(16.7630) | Bit/dim 3.9769(3.9791) | Xent 1.1324(1.2252) | Loss 10.3478(10.7663) | Error 0.4133(0.4394) Steps 0(0.00) | Grad Norm 15.1143(12.8806) | Total Time 0.00(0.00)\n",
      "Iter 1580 | Time 17.3597(16.7868) | Bit/dim 3.9706(3.9757) | Xent 1.2643(1.2223) | Loss 10.3952(10.6645) | Error 0.4256(0.4383) Steps 0(0.00) | Grad Norm 20.7029(13.2711) | Total Time 0.00(0.00)\n",
      "Iter 1590 | Time 16.1234(16.6371) | Bit/dim 3.9913(3.9734) | Xent 1.2175(1.2205) | Loss 10.3018(10.5681) | Error 0.4300(0.4388) Steps 0(0.00) | Grad Norm 14.7968(13.1872) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0029 | Time 82.8118, Epoch Time 1016.6520(919.4595), Bit/dim 3.9570(best: 3.9783), Xent 1.1528, Loss 4.5334, Error 0.4158(best: 0.4020)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1600 | Time 15.7946(16.6247) | Bit/dim 3.9655(3.9702) | Xent 1.1943(1.2196) | Loss 10.1705(11.1447) | Error 0.4356(0.4391) Steps 0(0.00) | Grad Norm 10.3137(13.0393) | Total Time 0.00(0.00)\n",
      "Iter 1610 | Time 15.2285(16.5009) | Bit/dim 3.9482(3.9645) | Xent 1.1559(1.2045) | Loss 9.9269(10.8834) | Error 0.4000(0.4332) Steps 0(0.00) | Grad Norm 11.6784(12.1797) | Total Time 0.00(0.00)\n",
      "Iter 1620 | Time 17.1641(16.5226) | Bit/dim 3.9527(3.9619) | Xent 1.2374(1.2046) | Loss 10.3101(10.7315) | Error 0.4289(0.4335) Steps 0(0.00) | Grad Norm 8.2535(11.9936) | Total Time 0.00(0.00)\n",
      "Iter 1630 | Time 16.1866(16.5618) | Bit/dim 3.9335(3.9592) | Xent 1.1844(1.1999) | Loss 10.0961(10.6120) | Error 0.4089(0.4317) Steps 0(0.00) | Grad Norm 18.3422(12.0361) | Total Time 0.00(0.00)\n",
      "Iter 1640 | Time 16.0505(16.4305) | Bit/dim 3.9676(3.9580) | Xent 1.1483(1.2000) | Loss 10.1319(10.5175) | Error 0.4156(0.4320) Steps 0(0.00) | Grad Norm 9.1029(12.6930) | Total Time 0.00(0.00)\n",
      "Iter 1650 | Time 16.7893(16.5307) | Bit/dim 3.9349(3.9560) | Xent 1.2217(1.2170) | Loss 10.2951(10.4737) | Error 0.4567(0.4380) Steps 0(0.00) | Grad Norm 9.8899(13.8538) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0030 | Time 83.3141, Epoch Time 1002.6247(921.9545), Bit/dim 3.9539(best: 3.9570), Xent 1.1275, Loss 4.5176, Error 0.4082(best: 0.4020)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1660 | Time 16.5873(16.4553) | Bit/dim 3.9301(3.9514) | Xent 1.1941(1.2123) | Loss 10.1874(10.9393) | Error 0.4167(0.4363) Steps 0(0.00) | Grad Norm 8.5112(13.3005) | Total Time 0.00(0.00)\n",
      "Iter 1670 | Time 16.2152(16.4374) | Bit/dim 3.9273(3.9506) | Xent 1.1134(1.1982) | Loss 10.2098(10.7326) | Error 0.3933(0.4309) Steps 0(0.00) | Grad Norm 9.2182(12.9087) | Total Time 0.00(0.00)\n",
      "Iter 1680 | Time 20.0157(16.6372) | Bit/dim 3.9315(3.9476) | Xent 1.1680(1.1846) | Loss 10.0938(10.6001) | Error 0.4389(0.4269) Steps 0(0.00) | Grad Norm 8.1576(11.7563) | Total Time 0.00(0.00)\n",
      "Iter 1690 | Time 15.5495(16.5081) | Bit/dim 3.9696(3.9471) | Xent 1.0917(1.1715) | Loss 10.2651(10.4933) | Error 0.4033(0.4222) Steps 0(0.00) | Grad Norm 5.3786(11.3227) | Total Time 0.00(0.00)\n",
      "Iter 1700 | Time 16.0741(16.3764) | Bit/dim 3.9165(3.9408) | Xent 1.1477(1.1655) | Loss 9.9945(10.3897) | Error 0.3989(0.4196) Steps 0(0.00) | Grad Norm 8.5319(10.3834) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0031 | Time 81.6643, Epoch Time 1001.3822(924.3373), Bit/dim 3.9392(best: 3.9539), Xent 1.1038, Loss 4.4911, Error 0.3912(best: 0.4020)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1710 | Time 16.1604(16.3537) | Bit/dim 3.9320(3.9355) | Xent 1.1855(1.1609) | Loss 10.3063(10.9729) | Error 0.4200(0.4171) Steps 0(0.00) | Grad Norm 17.0272(11.1967) | Total Time 0.00(0.00)\n",
      "Iter 1720 | Time 15.6078(16.3283) | Bit/dim 3.9464(3.9352) | Xent 1.1386(1.1506) | Loss 10.1776(10.7576) | Error 0.4144(0.4137) Steps 0(0.00) | Grad Norm 7.8721(11.2422) | Total Time 0.00(0.00)\n",
      "Iter 1730 | Time 16.6404(16.3424) | Bit/dim 3.9539(3.9347) | Xent 1.1037(1.1573) | Loss 10.3136(10.6240) | Error 0.3822(0.4144) Steps 0(0.00) | Grad Norm 16.2382(12.8656) | Total Time 0.00(0.00)\n",
      "Iter 1740 | Time 15.6721(16.3459) | Bit/dim 3.9586(3.9321) | Xent 1.0624(1.1516) | Loss 10.0252(10.5089) | Error 0.3956(0.4125) Steps 0(0.00) | Grad Norm 7.1448(12.2038) | Total Time 0.00(0.00)\n",
      "Iter 1750 | Time 16.1022(16.4596) | Bit/dim 3.9536(3.9277) | Xent 1.0883(1.1494) | Loss 10.2443(10.4249) | Error 0.3833(0.4112) Steps 0(0.00) | Grad Norm 7.0963(11.7218) | Total Time 0.00(0.00)\n",
      "Iter 1760 | Time 15.1442(16.5301) | Bit/dim 3.8915(3.9216) | Xent 1.1099(1.1481) | Loss 9.8785(10.3554) | Error 0.4111(0.4119) Steps 0(0.00) | Grad Norm 8.7352(11.7882) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0032 | Time 82.6386, Epoch Time 1002.8245(926.6919), Bit/dim 3.9054(best: 3.9392), Xent 1.0633, Loss 4.4370, Error 0.3724(best: 0.3912)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1770 | Time 16.8995(16.6670) | Bit/dim 3.9032(3.9193) | Xent 1.0435(1.1367) | Loss 10.0193(10.8544) | Error 0.3711(0.4073) Steps 0(0.00) | Grad Norm 9.6400(11.5230) | Total Time 0.00(0.00)\n",
      "Iter 1780 | Time 15.0625(16.5587) | Bit/dim 3.9345(3.9181) | Xent 1.1274(1.1304) | Loss 10.1048(10.6639) | Error 0.3733(0.4032) Steps 0(0.00) | Grad Norm 12.0266(11.1363) | Total Time 0.00(0.00)\n",
      "Iter 1790 | Time 15.5864(16.6151) | Bit/dim 3.9090(3.9136) | Xent 1.1754(1.1339) | Loss 10.0172(10.5103) | Error 0.4200(0.4053) Steps 0(0.00) | Grad Norm 8.7996(11.6537) | Total Time 0.00(0.00)\n",
      "Iter 1800 | Time 15.3398(16.5388) | Bit/dim 3.9144(3.9155) | Xent 1.1357(1.1414) | Loss 10.0409(10.4293) | Error 0.4222(0.4069) Steps 0(0.00) | Grad Norm 13.4072(12.9177) | Total Time 0.00(0.00)\n",
      "Iter 1810 | Time 16.6887(16.5010) | Bit/dim 3.9123(3.9136) | Xent 1.0841(1.1412) | Loss 10.2003(10.3263) | Error 0.3778(0.4083) Steps 0(0.00) | Grad Norm 10.4240(12.8531) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0033 | Time 83.6857, Epoch Time 1016.9476(929.3996), Bit/dim 3.9030(best: 3.9054), Xent 1.0507, Loss 4.4284, Error 0.3691(best: 0.3724)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1820 | Time 15.2939(16.6491) | Bit/dim 3.9133(3.9121) | Xent 1.0734(1.1303) | Loss 10.2194(10.9065) | Error 0.3967(0.4050) Steps 0(0.00) | Grad Norm 6.3510(11.6171) | Total Time 0.00(0.00)\n",
      "Iter 1830 | Time 16.4564(16.5996) | Bit/dim 3.8591(3.9082) | Xent 1.0645(1.1178) | Loss 9.9174(10.6827) | Error 0.3978(0.4012) Steps 0(0.00) | Grad Norm 10.7058(11.3307) | Total Time 0.00(0.00)\n",
      "Iter 1840 | Time 16.4040(16.6073) | Bit/dim 3.8570(3.9032) | Xent 1.2080(1.1150) | Loss 9.6045(10.5059) | Error 0.4300(0.3998) Steps 0(0.00) | Grad Norm 19.0492(11.9389) | Total Time 0.00(0.00)\n",
      "Iter 1850 | Time 16.5272(16.4886) | Bit/dim 3.8804(3.9041) | Xent 1.1662(1.1178) | Loss 10.2638(10.4069) | Error 0.4222(0.4013) Steps 0(0.00) | Grad Norm 13.4537(12.0027) | Total Time 0.00(0.00)\n",
      "Iter 1860 | Time 15.8452(16.3897) | Bit/dim 3.8664(3.8979) | Xent 1.0163(1.1092) | Loss 9.7953(10.2924) | Error 0.3578(0.3970) Steps 0(0.00) | Grad Norm 6.7297(11.4377) | Total Time 0.00(0.00)\n",
      "Iter 1870 | Time 16.0215(16.3738) | Bit/dim 3.8944(3.8965) | Xent 1.1175(1.1094) | Loss 9.9609(10.2501) | Error 0.3978(0.3973) Steps 0(0.00) | Grad Norm 10.3480(10.6021) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0034 | Time 82.7141, Epoch Time 997.6431(931.4469), Bit/dim 3.8830(best: 3.9030), Xent 1.0496, Loss 4.4078, Error 0.3769(best: 0.3691)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1880 | Time 16.3626(16.3956) | Bit/dim 3.9094(3.8951) | Xent 1.0295(1.1009) | Loss 10.0695(10.7439) | Error 0.3711(0.3931) Steps 0(0.00) | Grad Norm 13.0725(10.1765) | Total Time 0.00(0.00)\n",
      "Iter 1890 | Time 15.5845(16.4340) | Bit/dim 3.8811(3.8932) | Xent 1.0403(1.0932) | Loss 9.8653(10.5517) | Error 0.3544(0.3903) Steps 0(0.00) | Grad Norm 14.8411(10.2571) | Total Time 0.00(0.00)\n",
      "Iter 1900 | Time 16.0174(16.5048) | Bit/dim 3.8707(3.8934) | Xent 1.1222(1.0955) | Loss 9.7936(10.4250) | Error 0.4100(0.3918) Steps 0(0.00) | Grad Norm 12.8287(11.7368) | Total Time 0.00(0.00)\n",
      "Iter 1910 | Time 16.5540(16.3688) | Bit/dim 3.8719(3.8913) | Xent 1.0925(1.1001) | Loss 10.1390(10.3348) | Error 0.3989(0.3948) Steps 0(0.00) | Grad Norm 5.8914(11.6487) | Total Time 0.00(0.00)\n",
      "Iter 1920 | Time 16.9330(16.4042) | Bit/dim 3.8873(3.8856) | Xent 1.1754(1.1051) | Loss 10.1296(10.2616) | Error 0.4267(0.3966) Steps 0(0.00) | Grad Norm 18.3607(12.0604) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0035 | Time 82.3110, Epoch Time 1002.4986(933.5785), Bit/dim 3.8894(best: 3.8830), Xent 1.0689, Loss 4.4238, Error 0.3846(best: 0.3691)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1930 | Time 16.8972(16.4856) | Bit/dim 3.8900(3.8883) | Xent 1.2078(1.1207) | Loss 10.2441(10.8851) | Error 0.4333(0.4000) Steps 0(0.00) | Grad Norm 15.2089(12.9339) | Total Time 0.00(0.00)\n",
      "Iter 1940 | Time 17.7284(16.4768) | Bit/dim 3.8672(3.8826) | Xent 1.0469(1.1129) | Loss 9.7770(10.6462) | Error 0.3622(0.3979) Steps 0(0.00) | Grad Norm 7.4388(11.9433) | Total Time 0.00(0.00)\n",
      "Iter 1950 | Time 18.1220(16.5603) | Bit/dim 3.9068(3.8836) | Xent 1.0711(1.0993) | Loss 10.2438(10.4746) | Error 0.3744(0.3928) Steps 0(0.00) | Grad Norm 13.7156(11.0766) | Total Time 0.00(0.00)\n",
      "Iter 1960 | Time 17.8081(16.6798) | Bit/dim 3.8684(3.8806) | Xent 1.1206(1.0938) | Loss 10.0072(10.3443) | Error 0.4078(0.3908) Steps 0(0.00) | Grad Norm 13.2225(10.6646) | Total Time 0.00(0.00)\n",
      "Iter 1970 | Time 16.9040(16.6293) | Bit/dim 3.8691(3.8763) | Xent 1.0481(1.0916) | Loss 9.8897(10.2492) | Error 0.3889(0.3886) Steps 0(0.00) | Grad Norm 10.9296(11.4080) | Total Time 0.00(0.00)\n",
      "Iter 1980 | Time 16.5858(16.5418) | Bit/dim 3.8837(3.8793) | Xent 1.0563(1.0873) | Loss 10.0994(10.1698) | Error 0.3600(0.3861) Steps 0(0.00) | Grad Norm 9.8931(11.5235) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0036 | Time 82.6597, Epoch Time 1012.0047(935.9312), Bit/dim 3.8668(best: 3.8830), Xent 1.0376, Loss 4.3856, Error 0.3676(best: 0.3691)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1990 | Time 16.0172(16.4460) | Bit/dim 3.8757(3.8768) | Xent 1.0409(1.0842) | Loss 10.2347(10.6771) | Error 0.3767(0.3849) Steps 0(0.00) | Grad Norm 5.7031(11.5491) | Total Time 0.00(0.00)\n",
      "Iter 2000 | Time 18.3323(16.5853) | Bit/dim 3.8640(3.8768) | Xent 1.0736(1.0703) | Loss 10.1203(10.5088) | Error 0.3911(0.3800) Steps 0(0.00) | Grad Norm 7.6709(11.1660) | Total Time 0.00(0.00)\n",
      "Iter 2010 | Time 16.7742(16.5297) | Bit/dim 3.8783(3.8743) | Xent 1.0446(1.0634) | Loss 9.9551(10.3645) | Error 0.3867(0.3782) Steps 0(0.00) | Grad Norm 13.1987(10.4383) | Total Time 0.00(0.00)\n",
      "Iter 2020 | Time 15.6482(16.5665) | Bit/dim 3.8789(3.8699) | Xent 1.0832(1.0596) | Loss 10.0374(10.2498) | Error 0.3978(0.3776) Steps 0(0.00) | Grad Norm 11.5761(11.4855) | Total Time 0.00(0.00)\n",
      "Iter 2030 | Time 16.7553(16.6869) | Bit/dim 3.9469(3.8699) | Xent 1.1511(1.0733) | Loss 9.9545(10.1875) | Error 0.3911(0.3823) Steps 0(0.00) | Grad Norm 20.5872(12.9142) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0037 | Time 83.9358, Epoch Time 1017.7042(938.3844), Bit/dim 3.8905(best: 3.8668), Xent 1.1157, Loss 4.4484, Error 0.4007(best: 0.3676)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2040 | Time 17.5257(16.7663) | Bit/dim 3.8889(3.8716) | Xent 1.0708(1.0843) | Loss 10.0402(10.8040) | Error 0.4044(0.3877) Steps 0(0.00) | Grad Norm 12.9502(13.3170) | Total Time 0.00(0.00)\n",
      "Iter 2050 | Time 16.7552(16.7693) | Bit/dim 3.8728(3.8709) | Xent 0.9975(1.0800) | Loss 10.0076(10.5839) | Error 0.3422(0.3867) Steps 0(0.00) | Grad Norm 7.9021(12.6775) | Total Time 0.00(0.00)\n",
      "Iter 2060 | Time 15.6883(16.7882) | Bit/dim 3.8515(3.8688) | Xent 1.0608(1.0735) | Loss 9.9156(10.4394) | Error 0.3744(0.3844) Steps 0(0.00) | Grad Norm 6.1205(12.1905) | Total Time 0.00(0.00)\n",
      "Iter 2070 | Time 16.7548(16.8129) | Bit/dim 3.8417(3.8653) | Xent 1.0446(1.0599) | Loss 9.8050(10.3007) | Error 0.3611(0.3787) Steps 0(0.00) | Grad Norm 6.6301(11.0426) | Total Time 0.00(0.00)\n",
      "Iter 2080 | Time 17.1814(16.8763) | Bit/dim 3.8649(3.8652) | Xent 1.0184(1.0541) | Loss 9.8765(10.2171) | Error 0.3489(0.3770) Steps 0(0.00) | Grad Norm 8.7614(10.4639) | Total Time 0.00(0.00)\n",
      "Iter 2090 | Time 17.2726(16.8616) | Bit/dim 3.8063(3.8591) | Xent 1.1243(1.0603) | Loss 10.0457(10.1641) | Error 0.4033(0.3779) Steps 0(0.00) | Grad Norm 16.2282(10.8185) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0038 | Time 84.8604, Epoch Time 1029.4080(941.1151), Bit/dim 3.8563(best: 3.8668), Xent 1.0755, Loss 4.3940, Error 0.3862(best: 0.3676)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2100 | Time 16.7339(16.8135) | Bit/dim 3.8665(3.8578) | Xent 1.0379(1.0567) | Loss 10.0050(10.6803) | Error 0.3756(0.3764) Steps 0(0.00) | Grad Norm 14.6103(11.3750) | Total Time 0.00(0.00)\n",
      "Iter 2110 | Time 17.9156(16.8269) | Bit/dim 3.8434(3.8582) | Xent 1.0771(1.0533) | Loss 9.9021(10.4653) | Error 0.3956(0.3754) Steps 0(0.00) | Grad Norm 11.7309(11.5604) | Total Time 0.00(0.00)\n",
      "Iter 2120 | Time 16.6496(16.7482) | Bit/dim 3.8344(3.8538) | Xent 1.0505(1.0497) | Loss 9.9822(10.3267) | Error 0.3600(0.3750) Steps 0(0.00) | Grad Norm 7.4201(11.0107) | Total Time 0.00(0.00)\n",
      "Iter 2130 | Time 16.4649(16.6072) | Bit/dim 3.8290(3.8516) | Xent 1.0112(1.0509) | Loss 9.8183(10.2346) | Error 0.3733(0.3760) Steps 0(0.00) | Grad Norm 8.7176(10.7845) | Total Time 0.00(0.00)\n",
      "Iter 2140 | Time 15.8928(16.5883) | Bit/dim 3.8653(3.8502) | Xent 1.0706(1.0507) | Loss 9.7853(10.1514) | Error 0.3967(0.3750) Steps 0(0.00) | Grad Norm 6.9521(10.3983) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0039 | Time 83.0370, Epoch Time 1016.6386(943.3808), Bit/dim 3.8429(best: 3.8563), Xent 0.9798, Loss 4.3328, Error 0.3485(best: 0.3676)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2150 | Time 16.9951(16.7682) | Bit/dim 3.8382(3.8482) | Xent 1.0635(1.0460) | Loss 9.9303(10.7463) | Error 0.3767(0.3739) Steps 0(0.00) | Grad Norm 12.7383(10.1424) | Total Time 0.00(0.00)\n",
      "Iter 2160 | Time 17.5090(16.7722) | Bit/dim 3.8545(3.8487) | Xent 0.9893(1.0346) | Loss 9.9362(10.5448) | Error 0.3433(0.3694) Steps 0(0.00) | Grad Norm 7.6506(9.6552) | Total Time 0.00(0.00)\n",
      "Iter 2170 | Time 16.2430(16.7824) | Bit/dim 3.8367(3.8529) | Xent 1.0786(1.0388) | Loss 9.9891(10.3915) | Error 0.3722(0.3688) Steps 0(0.00) | Grad Norm 9.1425(10.7792) | Total Time 0.00(0.00)\n",
      "Iter 2180 | Time 15.7176(16.7206) | Bit/dim 3.8202(3.8525) | Xent 1.0176(1.0359) | Loss 9.8949(10.2664) | Error 0.3711(0.3692) Steps 0(0.00) | Grad Norm 10.2386(10.9902) | Total Time 0.00(0.00)\n",
      "Iter 2190 | Time 17.2754(16.8415) | Bit/dim 3.8081(3.8475) | Xent 1.0243(1.0275) | Loss 10.0259(10.1820) | Error 0.3678(0.3666) Steps 0(0.00) | Grad Norm 4.9973(10.3532) | Total Time 0.00(0.00)\n",
      "Iter 2200 | Time 16.5244(16.8057) | Bit/dim 3.8213(3.8431) | Xent 0.9947(1.0218) | Loss 9.7808(10.0975) | Error 0.3378(0.3640) Steps 0(0.00) | Grad Norm 15.0385(9.7552) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0040 | Time 83.9148, Epoch Time 1023.5242(945.7851), Bit/dim 3.8445(best: 3.8429), Xent 1.0199, Loss 4.3545, Error 0.3592(best: 0.3485)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2210 | Time 15.9544(16.6538) | Bit/dim 3.8333(3.8386) | Xent 0.9745(1.0163) | Loss 9.7369(10.5600) | Error 0.3289(0.3620) Steps 0(0.00) | Grad Norm 8.0459(10.3035) | Total Time 0.00(0.00)\n",
      "Iter 2220 | Time 15.7701(16.7440) | Bit/dim 3.8246(3.8349) | Xent 1.0394(1.0133) | Loss 9.6782(10.3659) | Error 0.3733(0.3602) Steps 0(0.00) | Grad Norm 9.9915(9.9844) | Total Time 0.00(0.00)\n",
      "Iter 2230 | Time 16.7538(16.7261) | Bit/dim 3.8341(3.8343) | Xent 1.0301(1.0076) | Loss 9.9072(10.2424) | Error 0.3567(0.3579) Steps 0(0.00) | Grad Norm 7.4223(9.8115) | Total Time 0.00(0.00)\n",
      "Iter 2240 | Time 16.6249(16.6069) | Bit/dim 3.8136(3.8327) | Xent 1.0224(1.0131) | Loss 9.5449(10.1168) | Error 0.3678(0.3611) Steps 0(0.00) | Grad Norm 9.6542(9.6379) | Total Time 0.00(0.00)\n",
      "Iter 2250 | Time 15.9040(16.6143) | Bit/dim 3.8292(3.8338) | Xent 1.1622(1.0227) | Loss 10.0452(10.0872) | Error 0.4300(0.3651) Steps 0(0.00) | Grad Norm 12.8428(10.9338) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0041 | Time 84.5907, Epoch Time 1013.9325(947.8296), Bit/dim 3.8381(best: 3.8429), Xent 1.0074, Loss 4.3419, Error 0.3613(best: 0.3485)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2260 | Time 15.6779(16.6078) | Bit/dim 3.8285(3.8349) | Xent 0.9404(1.0202) | Loss 9.6627(10.6816) | Error 0.3278(0.3640) Steps 0(0.00) | Grad Norm 7.6545(11.2117) | Total Time 0.00(0.00)\n",
      "Iter 2270 | Time 16.6222(16.7390) | Bit/dim 3.8248(3.8348) | Xent 1.0477(1.0169) | Loss 9.7532(10.4683) | Error 0.3689(0.3616) Steps 0(0.00) | Grad Norm 7.5287(11.2707) | Total Time 0.00(0.00)\n",
      "Iter 2280 | Time 16.9319(16.7179) | Bit/dim 3.8315(3.8338) | Xent 0.9731(1.0126) | Loss 9.8619(10.3007) | Error 0.3489(0.3607) Steps 0(0.00) | Grad Norm 12.7018(11.0732) | Total Time 0.00(0.00)\n",
      "Iter 2290 | Time 15.9471(16.8315) | Bit/dim 3.7859(3.8298) | Xent 1.0927(1.0169) | Loss 9.7875(10.1901) | Error 0.4000(0.3634) Steps 0(0.00) | Grad Norm 9.3575(11.0721) | Total Time 0.00(0.00)\n",
      "Iter 2300 | Time 18.4607(16.8552) | Bit/dim 3.8349(3.8269) | Xent 1.0514(1.0251) | Loss 9.8233(10.1103) | Error 0.3722(0.3654) Steps 0(0.00) | Grad Norm 9.2699(10.9064) | Total Time 0.00(0.00)\n",
      "Iter 2310 | Time 18.4526(16.9784) | Bit/dim 3.8239(3.8251) | Xent 1.0019(1.0152) | Loss 9.9495(10.0550) | Error 0.3511(0.3625) Steps 0(0.00) | Grad Norm 19.6522(10.6584) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0042 | Time 83.8410, Epoch Time 1032.9984(950.3846), Bit/dim 3.8241(best: 3.8381), Xent 0.9888, Loss 4.3185, Error 0.3540(best: 0.3485)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2320 | Time 16.6969(17.0088) | Bit/dim 3.8236(3.8239) | Xent 0.9690(1.0139) | Loss 9.8031(10.5586) | Error 0.3489(0.3612) Steps 0(0.00) | Grad Norm 7.9863(10.2436) | Total Time 0.00(0.00)\n",
      "Iter 2330 | Time 17.6633(16.9541) | Bit/dim 3.8103(3.8230) | Xent 0.9879(1.0055) | Loss 10.0820(10.3627) | Error 0.3433(0.3592) Steps 0(0.00) | Grad Norm 12.7754(10.4913) | Total Time 0.00(0.00)\n",
      "Iter 2340 | Time 17.3366(16.9257) | Bit/dim 3.8128(3.8215) | Xent 0.9546(0.9993) | Loss 9.9096(10.2308) | Error 0.3456(0.3586) Steps 0(0.00) | Grad Norm 8.6914(10.3096) | Total Time 0.00(0.00)\n",
      "Iter 2350 | Time 16.3975(16.7719) | Bit/dim 3.8155(3.8197) | Xent 1.0010(0.9950) | Loss 10.0141(10.1279) | Error 0.3589(0.3562) Steps 0(0.00) | Grad Norm 8.9228(10.0754) | Total Time 0.00(0.00)\n",
      "Iter 2360 | Time 16.4143(16.8309) | Bit/dim 3.7849(3.8188) | Xent 1.0553(0.9992) | Loss 9.8097(10.0712) | Error 0.3633(0.3570) Steps 0(0.00) | Grad Norm 15.5396(11.1573) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0043 | Time 83.8297, Epoch Time 1024.3018(952.6021), Bit/dim 3.8144(best: 3.8241), Xent 0.9454, Loss 4.2871, Error 0.3327(best: 0.3485)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2370 | Time 16.8799(16.8001) | Bit/dim 3.8239(3.8192) | Xent 0.9214(0.9994) | Loss 9.9369(10.6619) | Error 0.3267(0.3571) Steps 0(0.00) | Grad Norm 11.0244(11.5764) | Total Time 0.00(0.00)\n",
      "Iter 2380 | Time 16.6849(16.6548) | Bit/dim 3.8491(3.8216) | Xent 0.9749(0.9917) | Loss 10.0297(10.4414) | Error 0.3411(0.3539) Steps 0(0.00) | Grad Norm 7.8160(11.0714) | Total Time 0.00(0.00)\n",
      "Iter 2390 | Time 16.3984(16.7532) | Bit/dim 3.7912(3.8189) | Xent 1.0031(0.9937) | Loss 9.7509(10.2967) | Error 0.3556(0.3534) Steps 0(0.00) | Grad Norm 14.2847(11.0035) | Total Time 0.00(0.00)\n",
      "Iter 2400 | Time 16.5960(16.7244) | Bit/dim 3.8045(3.8174) | Xent 1.0016(0.9957) | Loss 9.9067(10.1762) | Error 0.3644(0.3543) Steps 0(0.00) | Grad Norm 9.6936(10.7456) | Total Time 0.00(0.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_rl.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_6_run1 --resume ../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_6_run1/epoch_40_checkpt.pth --seed 1 --lr 0.001 --conditional True --controlled_tol False --train_mode semisup --log_freq 10 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5 --scale_fac 1.0 --scale_std 6.0\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
