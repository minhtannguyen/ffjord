{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0,1,2,3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_rl.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import torch.utils.data as data\n",
      "from torch.utils.data import Dataset\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "plt.rcParams['figure.dpi'] = 300\n",
      "\n",
      "from PIL import Image\n",
      "import os.path\n",
      "import errno\n",
      "import codecs\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time, count_nfe_gate\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "GATES = [\"cnn1\", \"cnn2\", \"rnn\"]\n",
      "\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"colormnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--gate', type=str, default='cnn1', choices=GATES)\n",
      "parser.add_argument('--scale', type=float, default=1.0)\n",
      "parser.add_argument('--scale_fac', type=float, default=1.0)\n",
      "parser.add_argument('--scale_std', type=float, default=1.0)\n",
      "parser.add_argument('--eta', default=0.1, type=float,\n",
      "                        help='tuning parameter that allows us to trade-off the competing goals of' \n",
      "                                'minimizing the prediction loss and maximizing the gate rewards ')\n",
      "parser.add_argument('--rl-weight', default=0.01, type=float,\n",
      "                        help='rl weight')\n",
      "\n",
      "parser.add_argument('--gamma', default=0.99, type=float,\n",
      "                        help='discount factor, default: (0.99)')\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=500)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "parser.add_argument(\"--annealing_std\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--y_class\", type=int, default=10)\n",
      "parser.add_argument(\"--y_color\", type=int, default=10)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "import lib.odenvp_conditional_rl as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "class ColorMNIST(data.Dataset):\n",
      "    \"\"\"\n",
      "    ColorMNIST\n",
      "    \"\"\"\n",
      "    urls = [\n",
      "        'http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz',\n",
      "    ]\n",
      "    raw_folder = 'raw'\n",
      "    processed_folder = 'processed'\n",
      "    training_file = 'training.pt'\n",
      "    test_file = 'test.pt'\n",
      "\n",
      "    def __init__(self, root, train=True, transform=None, target_transform=None, download=False):\n",
      "        self.root = os.path.expanduser(root)\n",
      "        self.transform = transform\n",
      "        self.target_transform = target_transform\n",
      "        self.train = train  # training set or test set\n",
      "\n",
      "        if download:\n",
      "            self.download()\n",
      "\n",
      "        if not self._check_exists():\n",
      "            raise RuntimeError('Dataset not found.' +\n",
      "                               ' You can use download=True to download it')\n",
      "\n",
      "        if self.train:\n",
      "            self.train_data, self.train_labels = torch.load(\n",
      "                os.path.join(self.root, self.processed_folder, self.training_file))\n",
      "            \n",
      "            self.train_data = np.tile(self.train_data[:, :, :, np.newaxis], 3)\n",
      "        else:\n",
      "            self.test_data, self.test_labels = torch.load(\n",
      "                os.path.join(self.root, self.processed_folder, self.test_file))\n",
      "            \n",
      "            self.test_data = np.tile(self.test_data[:, :, :, np.newaxis], 3)\n",
      "        \n",
      "        self.pallette = [[31, 119, 180],\n",
      "                         [255, 127, 14],\n",
      "                         [44, 160, 44],\n",
      "                         [214, 39, 40],\n",
      "                         [148, 103, 189],\n",
      "                         [140, 86, 75],\n",
      "                         [227, 119, 194],\n",
      "                         [127, 127, 127],\n",
      "                         [188, 189, 34],\n",
      "                         [23, 190, 207]]\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        \"\"\"\n",
      "        Args:\n",
      "            index (int): Index\n",
      "\n",
      "        Returns:\n",
      "            tuple: (image, target) where target is index of the target class.\n",
      "        \"\"\"\n",
      "        if self.train:\n",
      "            img, target = self.train_data[index].copy(), self.train_labels[index]\n",
      "        else:\n",
      "            img, target = self.test_data[index].copy(), self.test_labels[index]\n",
      "        \n",
      "        # doing this so that it is consistent with all other datasets\n",
      "        # to return a PIL Image\n",
      "        y_color_digit = np.random.randint(0, args.y_color)\n",
      "        c_digit = self.pallette[y_color_digit]\n",
      "        \n",
      "        img[:, :, 0] = img[:, :, 0] / 255 * c_digit[0]\n",
      "        img[:, :, 1] = img[:, :, 1] / 255 * c_digit[1]\n",
      "        img[:, :, 2] = img[:, :, 2] / 255 * c_digit[2]\n",
      "        \n",
      "        img = Image.fromarray(img)\n",
      "\n",
      "        if self.transform is not None:\n",
      "            img = self.transform(img)\n",
      "\n",
      "        if self.target_transform is not None:\n",
      "            target = self.target_transform(target)\n",
      "\n",
      "        return img, [target,torch.from_numpy(np.array(y_color_digit))]\n",
      "\n",
      "    def __len__(self):\n",
      "        if self.train:\n",
      "            return len(self.train_data)\n",
      "        else:\n",
      "            return len(self.test_data)\n",
      "\n",
      "    def _check_exists(self):\n",
      "        return os.path.exists(os.path.join(self.root, self.processed_folder, self.training_file)) and \\\n",
      "            os.path.exists(os.path.join(self.root, self.processed_folder, self.test_file))\n",
      "\n",
      "    def download(self):\n",
      "        \"\"\"Download the MNIST data if it doesn't exist in processed_folder already.\"\"\"\n",
      "        from six.moves import urllib\n",
      "        import gzip\n",
      "\n",
      "        if self._check_exists():\n",
      "            return\n",
      "\n",
      "        # download files\n",
      "        try:\n",
      "            os.makedirs(os.path.join(self.root, self.raw_folder))\n",
      "            os.makedirs(os.path.join(self.root, self.processed_folder))\n",
      "        except OSError as e:\n",
      "            if e.errno == errno.EEXIST:\n",
      "                pass\n",
      "            else:\n",
      "                raise\n",
      "\n",
      "        for url in self.urls:\n",
      "            print('Downloading ' + url)\n",
      "            data = urllib.request.urlopen(url)\n",
      "            filename = url.rpartition('/')[2]\n",
      "            file_path = os.path.join(self.root, self.raw_folder, filename)\n",
      "            with open(file_path, 'wb') as f:\n",
      "                f.write(data.read())\n",
      "            with open(file_path.replace('.gz', ''), 'wb') as out_f, \\\n",
      "                    gzip.GzipFile(file_path) as zip_f:\n",
      "                out_f.write(zip_f.read())\n",
      "            os.unlink(file_path)\n",
      "\n",
      "        # process and save as torch files\n",
      "        print('Processing...')\n",
      "\n",
      "        training_set = (\n",
      "            read_image_file(os.path.join(self.root, self.raw_folder, 'train-images-idx3-ubyte')),\n",
      "            read_label_file(os.path.join(self.root, self.raw_folder, 'train-labels-idx1-ubyte'))\n",
      "        )\n",
      "        test_set = (\n",
      "            read_image_file(os.path.join(self.root, self.raw_folder, 't10k-images-idx3-ubyte')),\n",
      "            read_label_file(os.path.join(self.root, self.raw_folder, 't10k-labels-idx1-ubyte'))\n",
      "        )\n",
      "        with open(os.path.join(self.root, self.processed_folder, self.training_file), 'wb') as f:\n",
      "            torch.save(training_set, f)\n",
      "        with open(os.path.join(self.root, self.processed_folder, self.test_file), 'wb') as f:\n",
      "            torch.save(test_set, f)\n",
      "\n",
      "        print('Done!')\n",
      "\n",
      "    def __repr__(self):\n",
      "        fmt_str = 'Dataset ' + self.__class__.__name__ + '\\n'\n",
      "        fmt_str += '    Number of datapoints: {}\\n'.format(self.__len__())\n",
      "        tmp = 'train' if self.train is True else 'test'\n",
      "        fmt_str += '    Split: {}\\n'.format(tmp)\n",
      "        fmt_str += '    Root Location: {}\\n'.format(self.root)\n",
      "        tmp = '    Transforms (if any): '\n",
      "        fmt_str += '{0}{1}\\n'.format(tmp, self.transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
      "        tmp = '    Target Transforms (if any): '\n",
      "        fmt_str += '{0}{1}'.format(tmp, self.target_transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
      "        return fmt_str\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "        \n",
      "def update_scale_std(model, epoch):\n",
      "    epoch_frac = 1.0 - float(epoch - 1) / max(args.num_epochs + 1, 1)\n",
      "    scale_std = args.scale_std * epoch_frac\n",
      "    model.set_scale_std(scale_std)\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"colormnist\":\n",
      "        im_dim = 3\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = ColorMNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = ColorMNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol, \"scale\": args.scale, \"scale_fac\": args.scale_fac, \"scale_std\": args.scale_std, \"gate\": args.gate},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,\n",
      "            y_class = args.y_class)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        if args.annealing_std:\n",
      "            update_scale_std(model.module, epoch)\n",
      "            \n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            if args.data == \"colormnist\":\n",
      "                y = y[0]\n",
      "            \n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            # re-weight the gate rewards\n",
      "            normalized_eta = args.eta / len(logp_actions)\n",
      "            \n",
      "            # collect cumulative future rewards\n",
      "            R = - loss\n",
      "            cum_rewards = []\n",
      "            for r in nfe[::-1]:\n",
      "                R = -normalized_eta * r.view(-1,1) + args.gamma * R\n",
      "                cum_rewards.insert(0,R)\n",
      "            \n",
      "            # apply REINFORCE\n",
      "            rl_loss = 0\n",
      "            for lpa, r in zip(logp_actions, cum_rewards):\n",
      "                rl_loss = rl_loss - lpa.view(-1,1) * args.rl_weight * r\n",
      "                \n",
      "            loss = loss + rl_loss.mean()\n",
      "            \n",
      "            loss.backward()\n",
      "            \n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe_gate(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            for idx in range(len(model.module.transforms)):\n",
      "                for layer in model.module.transforms[idx].chain:\n",
      "                    if hasattr(layer, 'atol'):\n",
      "                        layer.odefunc.after_odeint()\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'train': atol[tol_indx].mean()}, itr)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'train': rtol[tol_indx].mean()}, itr)\n",
      "                    \n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        if args.data == \"colormnist\":\n",
      "            # print train images\n",
      "            xall = []\n",
      "            ximg = x[0:40].cpu().numpy().transpose((0,2,3,1))\n",
      "            for i in range(ximg.shape[0]):\n",
      "                xall.append(ximg[i])\n",
      "        \n",
      "            xall = np.hstack(xall)\n",
      "\n",
      "            plt.imshow(xall)\n",
      "            plt.axis('off')\n",
      "            plt.show()\n",
      "            \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if args.data == \"colormnist\":\n",
      "                        y = y[0]\n",
      "                        \n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                if args.data == \"colormnist\":\n",
      "                    # print test images\n",
      "                    xall = []\n",
      "                    ximg = x[0:40].cpu().numpy().transpose((0,2,3,1))\n",
      "                    for i in range(ximg.shape[0]):\n",
      "                        xall.append(ximg[i])\n",
      "\n",
      "                    xall = np.hstack(xall)\n",
      "\n",
      "                    plt.imshow(xall)\n",
      "                    plt.axis('off')\n",
      "                    plt.show()\n",
      "                    \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples, atol, rtol, logp_actions, nfe = model(fixed_z, reverse=True)\n",
      "            generated_samples = generated_samples.view(-1, *data_shape)\n",
      "            for tol_indx in range(len(atol)):\n",
      "                writer.add_scalars('atol_gen_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                writer.add_scalars('rtol_gen_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, annealing_std=False, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, condition_ratio=0.5, conditional=True, controlled_tol=False, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, eta=0.1, gamma=0.99, gate='cnn1', imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.0001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=500, parallel=False, rademacher=True, residual=False, resume='../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_6_run1/current_checkpt.pth', rl_weight=0.01, rtol=1e-05, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_6_run1', scale=1.0, scale_fac=1.0, scale_std=6.0, seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5, y_class=10, y_color=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1450732\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc_rl.py:291: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Iter 14580 | Time 24.7175(25.6431) | Bit/dim 3.5309(3.5671) | Xent 0.1095(0.1247) | Loss 9.1681(10.5190) | Error 0.0411(0.0422) Steps 1012(1020.12) | Grad Norm 3.0085(3.2741) | Total Time 0.00(0.00)\n",
      "Iter 14590 | Time 24.6620(25.4819) | Bit/dim 3.5631(3.5669) | Xent 0.1086(0.1229) | Loss 9.2865(10.2093) | Error 0.0344(0.0415) Steps 1006(1020.76) | Grad Norm 2.5407(3.0763) | Total Time 0.00(0.00)\n",
      "Iter 14600 | Time 25.8099(25.4424) | Bit/dim 3.5391(3.5677) | Xent 0.1528(0.1229) | Loss 9.3937(9.9902) | Error 0.0511(0.0419) Steps 1024(1019.52) | Grad Norm 3.0764(3.0395) | Total Time 0.00(0.00)\n",
      "Iter 14610 | Time 25.4525(25.3833) | Bit/dim 3.5702(3.5670) | Xent 0.0855(0.1211) | Loss 9.3704(9.8199) | Error 0.0311(0.0413) Steps 988(1019.83) | Grad Norm 2.2900(3.0226) | Total Time 0.00(0.00)\n",
      "Iter 14620 | Time 25.2290(25.4202) | Bit/dim 3.5290(3.5653) | Xent 0.1122(0.1212) | Loss 9.3294(9.7148) | Error 0.0322(0.0410) Steps 1072(1021.73) | Grad Norm 2.7660(2.9969) | Total Time 0.00(0.00)\n",
      "Iter 14630 | Time 24.9593(25.3863) | Bit/dim 3.5560(3.5663) | Xent 0.1322(0.1215) | Loss 9.3201(9.6198) | Error 0.0422(0.0410) Steps 1012(1019.56) | Grad Norm 2.7994(2.9763) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0266 | Time 123.1077, Epoch Time 1549.8464(1455.8428), Bit/dim 3.5741(best: inf), Xent 0.9126, Loss 4.0304, Error 0.2204(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14640 | Time 26.1228(25.4309) | Bit/dim 3.5620(3.5652) | Xent 0.0963(0.1198) | Loss 9.2440(10.3935) | Error 0.0333(0.0403) Steps 1078(1023.52) | Grad Norm 2.0661(2.9002) | Total Time 0.00(0.00)\n",
      "Iter 14650 | Time 25.0747(25.4516) | Bit/dim 3.5670(3.5646) | Xent 0.1245(0.1194) | Loss 9.4614(10.1286) | Error 0.0444(0.0408) Steps 1012(1023.07) | Grad Norm 2.8418(2.9154) | Total Time 0.00(0.00)\n",
      "Iter 14660 | Time 25.6045(25.3920) | Bit/dim 3.5300(3.5621) | Xent 0.1174(0.1191) | Loss 9.2108(9.9228) | Error 0.0389(0.0409) Steps 1018(1025.06) | Grad Norm 2.4611(2.8373) | Total Time 0.00(0.00)\n",
      "Iter 14670 | Time 24.8866(25.4644) | Bit/dim 3.5637(3.5629) | Xent 0.1260(0.1203) | Loss 9.4154(9.7803) | Error 0.0433(0.0414) Steps 1012(1025.77) | Grad Norm 2.1608(2.7699) | Total Time 0.00(0.00)\n",
      "Iter 14680 | Time 25.1139(25.5123) | Bit/dim 3.5787(3.5652) | Xent 0.1306(0.1202) | Loss 9.4388(9.6780) | Error 0.0533(0.0414) Steps 1054(1025.11) | Grad Norm 3.7089(2.8491) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0267 | Time 119.0913, Epoch Time 1540.1919(1458.3733), Bit/dim 3.5728(best: 3.5741), Xent 0.9299, Loss 4.0377, Error 0.2221(best: 0.2204)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14690 | Time 25.4475(25.5099) | Bit/dim 3.5541(3.5646) | Xent 0.1359(0.1203) | Loss 9.2687(10.5781) | Error 0.0467(0.0414) Steps 1036(1028.38) | Grad Norm 4.1717(2.9394) | Total Time 0.00(0.00)\n",
      "Iter 14700 | Time 26.2299(25.5457) | Bit/dim 3.5540(3.5669) | Xent 0.1461(0.1201) | Loss 9.5262(10.2693) | Error 0.0400(0.0412) Steps 1072(1031.30) | Grad Norm 5.0234(3.1347) | Total Time 0.00(0.00)\n",
      "Iter 14710 | Time 25.1705(25.4265) | Bit/dim 3.5995(3.5671) | Xent 0.1481(0.1203) | Loss 9.4639(10.0307) | Error 0.0544(0.0412) Steps 1024(1029.75) | Grad Norm 4.3507(3.1077) | Total Time 0.00(0.00)\n",
      "Iter 14720 | Time 24.6975(25.2935) | Bit/dim 3.5820(3.5656) | Xent 0.1107(0.1195) | Loss 9.3345(9.8536) | Error 0.0400(0.0407) Steps 1018(1027.71) | Grad Norm 3.2913(3.0616) | Total Time 0.00(0.00)\n",
      "Iter 14730 | Time 25.6700(25.2409) | Bit/dim 3.5356(3.5656) | Xent 0.1340(0.1195) | Loss 9.4072(9.7250) | Error 0.0478(0.0408) Steps 988(1025.49) | Grad Norm 2.5961(3.0747) | Total Time 0.00(0.00)\n",
      "Iter 14740 | Time 25.4995(25.3414) | Bit/dim 3.5457(3.5657) | Xent 0.0969(0.1184) | Loss 9.1999(9.6256) | Error 0.0367(0.0405) Steps 976(1025.16) | Grad Norm 2.3311(3.0301) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0268 | Time 118.6253, Epoch Time 1528.3492(1460.4726), Bit/dim 3.5730(best: 3.5728), Xent 0.9267, Loss 4.0363, Error 0.2215(best: 0.2204)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14750 | Time 24.7185(25.3251) | Bit/dim 3.5532(3.5658) | Xent 0.1057(0.1167) | Loss 9.2627(10.3932) | Error 0.0300(0.0395) Steps 1018(1025.94) | Grad Norm 3.3305(2.9882) | Total Time 0.00(0.00)\n",
      "Iter 14760 | Time 26.4427(25.3676) | Bit/dim 3.5735(3.5655) | Xent 0.1196(0.1175) | Loss 9.5277(10.1399) | Error 0.0422(0.0396) Steps 1054(1027.87) | Grad Norm 3.9853(3.0480) | Total Time 0.00(0.00)\n",
      "Iter 14770 | Time 25.3016(25.4728) | Bit/dim 3.5909(3.5619) | Xent 0.1141(0.1190) | Loss 9.4121(9.9357) | Error 0.0411(0.0398) Steps 1030(1029.42) | Grad Norm 2.3356(3.0772) | Total Time 0.00(0.00)\n",
      "Iter 14780 | Time 25.5583(25.4545) | Bit/dim 3.5399(3.5627) | Xent 0.1270(0.1185) | Loss 9.3503(9.7838) | Error 0.0456(0.0402) Steps 976(1028.47) | Grad Norm 4.0576(3.1297) | Total Time 0.00(0.00)\n",
      "Iter 14790 | Time 25.5271(25.4502) | Bit/dim 3.5538(3.5647) | Xent 0.1047(0.1188) | Loss 9.4102(9.6649) | Error 0.0444(0.0403) Steps 1012(1026.46) | Grad Norm 3.4886(3.1813) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0269 | Time 119.1377, Epoch Time 1540.3227(1462.8681), Bit/dim 3.5728(best: 3.5728), Xent 0.9470, Loss 4.0463, Error 0.2227(best: 0.2204)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14800 | Time 25.6972(25.4942) | Bit/dim 3.5661(3.5640) | Xent 0.1212(0.1194) | Loss 9.4436(10.5780) | Error 0.0411(0.0403) Steps 1024(1027.82) | Grad Norm 2.3970(3.1226) | Total Time 0.00(0.00)\n",
      "Iter 14810 | Time 25.6740(25.5360) | Bit/dim 3.5744(3.5672) | Xent 0.1226(0.1188) | Loss 9.3477(10.2697) | Error 0.0367(0.0396) Steps 1018(1027.86) | Grad Norm 3.5639(3.0938) | Total Time 0.00(0.00)\n",
      "Iter 14820 | Time 26.3152(25.6054) | Bit/dim 3.5608(3.5693) | Xent 0.1100(0.1173) | Loss 9.2779(10.0371) | Error 0.0356(0.0393) Steps 1042(1027.06) | Grad Norm 3.3256(3.1070) | Total Time 0.00(0.00)\n",
      "Iter 14830 | Time 26.0000(25.5938) | Bit/dim 3.5260(3.5670) | Xent 0.1013(0.1168) | Loss 9.3070(9.8496) | Error 0.0300(0.0395) Steps 1024(1026.81) | Grad Norm 2.5264(3.0897) | Total Time 0.00(0.00)\n",
      "Iter 14840 | Time 24.6743(25.6081) | Bit/dim 3.5964(3.5665) | Xent 0.1451(0.1187) | Loss 9.3214(9.7273) | Error 0.0500(0.0404) Steps 1018(1029.91) | Grad Norm 2.3720(3.1521) | Total Time 0.00(0.00)\n",
      "Iter 14850 | Time 25.3530(25.5819) | Bit/dim 3.5377(3.5627) | Xent 0.1453(0.1204) | Loss 9.3522(9.6288) | Error 0.0522(0.0408) Steps 1018(1029.21) | Grad Norm 3.2352(3.1275) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0270 | Time 118.6366, Epoch Time 1545.5616(1465.3489), Bit/dim 3.5743(best: 3.5728), Xent 0.9511, Loss 4.0499, Error 0.2224(best: 0.2204)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14860 | Time 24.9313(25.5295) | Bit/dim 3.5512(3.5622) | Xent 0.1043(0.1198) | Loss 9.1970(10.3985) | Error 0.0322(0.0405) Steps 1018(1028.11) | Grad Norm 1.8141(3.0358) | Total Time 0.00(0.00)\n",
      "Iter 14870 | Time 26.1723(25.5667) | Bit/dim 3.5830(3.5611) | Xent 0.1101(0.1171) | Loss 9.4238(10.1268) | Error 0.0322(0.0399) Steps 1042(1028.21) | Grad Norm 2.4224(2.9061) | Total Time 0.00(0.00)\n",
      "Iter 14880 | Time 25.3581(25.6024) | Bit/dim 3.5847(3.5638) | Xent 0.1091(0.1177) | Loss 9.5636(9.9348) | Error 0.0444(0.0399) Steps 1024(1028.38) | Grad Norm 3.0309(2.9090) | Total Time 0.00(0.00)\n",
      "Iter 14890 | Time 26.6126(25.6063) | Bit/dim 3.5472(3.5627) | Xent 0.1039(0.1177) | Loss 9.1603(9.7744) | Error 0.0400(0.0400) Steps 988(1029.43) | Grad Norm 3.6449(2.8948) | Total Time 0.00(0.00)\n",
      "Iter 14900 | Time 24.7074(25.5702) | Bit/dim 3.5739(3.5643) | Xent 0.1013(0.1172) | Loss 9.3958(9.6713) | Error 0.0300(0.0399) Steps 1042(1030.38) | Grad Norm 2.3007(2.8710) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0271 | Time 120.2037, Epoch Time 1544.9456(1467.7368), Bit/dim 3.5730(best: 3.5728), Xent 0.9507, Loss 4.0483, Error 0.2222(best: 0.2204)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14910 | Time 25.7016(25.5280) | Bit/dim 3.5592(3.5640) | Xent 0.1064(0.1155) | Loss 9.2498(10.5752) | Error 0.0367(0.0389) Steps 1000(1030.76) | Grad Norm 2.1803(2.8843) | Total Time 0.00(0.00)\n",
      "Iter 14920 | Time 24.9190(25.4818) | Bit/dim 3.5086(3.5642) | Xent 0.1064(0.1150) | Loss 9.1164(10.2522) | Error 0.0389(0.0389) Steps 1018(1029.65) | Grad Norm 2.8916(2.8937) | Total Time 0.00(0.00)\n",
      "Iter 14930 | Time 25.2509(25.5467) | Bit/dim 3.5512(3.5646) | Xent 0.1183(0.1146) | Loss 9.3186(10.0201) | Error 0.0444(0.0388) Steps 1024(1031.74) | Grad Norm 2.5649(2.8255) | Total Time 0.00(0.00)\n",
      "Iter 14940 | Time 24.8245(25.5164) | Bit/dim 3.5622(3.5622) | Xent 0.1308(0.1184) | Loss 9.4625(9.8524) | Error 0.0467(0.0405) Steps 1024(1028.12) | Grad Norm 2.9905(2.8513) | Total Time 0.00(0.00)\n",
      "Iter 14950 | Time 26.1800(25.5532) | Bit/dim 3.5493(3.5663) | Xent 0.1388(0.1180) | Loss 9.4365(9.7342) | Error 0.0411(0.0399) Steps 1042(1030.08) | Grad Norm 2.8418(2.9291) | Total Time 0.00(0.00)\n",
      "Iter 14960 | Time 26.6626(25.6864) | Bit/dim 3.5648(3.5669) | Xent 0.1286(0.1197) | Loss 9.3953(9.6535) | Error 0.0478(0.0411) Steps 1054(1031.66) | Grad Norm 4.9999(3.0746) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0272 | Time 120.4270, Epoch Time 1547.0397(1470.1159), Bit/dim 3.5729(best: 3.5728), Xent 0.9467, Loss 4.0462, Error 0.2219(best: 0.2204)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14970 | Time 25.2223(25.5669) | Bit/dim 3.5411(3.5640) | Xent 0.1056(0.1171) | Loss 9.3035(10.4304) | Error 0.0389(0.0401) Steps 1030(1029.18) | Grad Norm 2.7725(3.0931) | Total Time 0.00(0.00)\n",
      "Iter 14980 | Time 24.6325(25.5066) | Bit/dim 3.5689(3.5668) | Xent 0.1156(0.1166) | Loss 9.3399(10.1494) | Error 0.0356(0.0399) Steps 1012(1029.20) | Grad Norm 3.4451(3.0852) | Total Time 0.00(0.00)\n",
      "Iter 14990 | Time 25.3685(25.5146) | Bit/dim 3.5566(3.5648) | Xent 0.1177(0.1168) | Loss 9.3393(9.9401) | Error 0.0389(0.0398) Steps 1030(1032.79) | Grad Norm 3.4729(3.1519) | Total Time 0.00(0.00)\n",
      "Iter 15000 | Time 24.7374(25.5804) | Bit/dim 3.5584(3.5636) | Xent 0.1126(0.1169) | Loss 9.3949(9.7859) | Error 0.0422(0.0397) Steps 1024(1034.58) | Grad Norm 2.7662(3.0795) | Total Time 0.00(0.00)\n",
      "Iter 15010 | Time 24.9473(25.5091) | Bit/dim 3.5575(3.5646) | Xent 0.0995(0.1181) | Loss 9.2364(9.6787) | Error 0.0322(0.0400) Steps 994(1032.57) | Grad Norm 2.6773(3.0990) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0273 | Time 119.2280, Epoch Time 1535.6264(1472.0812), Bit/dim 3.5709(best: 3.5728), Xent 0.9361, Loss 4.0389, Error 0.2206(best: 0.2204)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15020 | Time 26.2769(25.4650) | Bit/dim 3.5617(3.5645) | Xent 0.1189(0.1171) | Loss 9.4503(10.5716) | Error 0.0411(0.0398) Steps 1066(1032.71) | Grad Norm 2.9233(3.0749) | Total Time 0.00(0.00)\n",
      "Iter 15030 | Time 26.1849(25.5467) | Bit/dim 3.5512(3.5625) | Xent 0.1315(0.1156) | Loss 9.3477(10.2559) | Error 0.0456(0.0390) Steps 1018(1034.43) | Grad Norm 4.0405(3.0494) | Total Time 0.00(0.00)\n",
      "Iter 15040 | Time 25.5798(25.5812) | Bit/dim 3.5691(3.5632) | Xent 0.0951(0.1132) | Loss 9.3931(10.0321) | Error 0.0311(0.0381) Steps 1048(1036.79) | Grad Norm 2.6181(2.9444) | Total Time 0.00(0.00)\n",
      "Iter 15050 | Time 25.9992(25.5760) | Bit/dim 3.5692(3.5644) | Xent 0.1352(0.1139) | Loss 9.3289(9.8482) | Error 0.0456(0.0382) Steps 1006(1036.29) | Grad Norm 3.6497(2.9165) | Total Time 0.00(0.00)\n",
      "Iter 15060 | Time 26.0186(25.6744) | Bit/dim 3.5769(3.5654) | Xent 0.1045(0.1137) | Loss 9.3890(9.7240) | Error 0.0422(0.0386) Steps 1030(1035.10) | Grad Norm 2.2541(2.9399) | Total Time 0.00(0.00)\n",
      "Iter 15070 | Time 24.8933(25.6104) | Bit/dim 3.5677(3.5668) | Xent 0.1143(0.1152) | Loss 9.3667(9.6428) | Error 0.0467(0.0392) Steps 1048(1032.34) | Grad Norm 3.5150(2.9306) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0274 | Time 119.6167, Epoch Time 1549.5309(1474.4047), Bit/dim 3.5735(best: 3.5709), Xent 0.9718, Loss 4.0593, Error 0.2242(best: 0.2204)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15080 | Time 25.0414(25.5352) | Bit/dim 3.5767(3.5651) | Xent 0.0956(0.1154) | Loss 9.3613(10.4058) | Error 0.0300(0.0395) Steps 1042(1033.49) | Grad Norm 2.4258(2.9543) | Total Time 0.00(0.00)\n",
      "Iter 15090 | Time 25.6187(25.5671) | Bit/dim 3.6006(3.5656) | Xent 0.1363(0.1174) | Loss 9.3001(10.1261) | Error 0.0478(0.0409) Steps 994(1029.94) | Grad Norm 3.0279(3.0500) | Total Time 0.00(0.00)\n",
      "Iter 15100 | Time 25.3895(25.5018) | Bit/dim 3.5655(3.5645) | Xent 0.1199(0.1172) | Loss 9.2912(9.9301) | Error 0.0411(0.0410) Steps 1000(1029.73) | Grad Norm 3.1055(3.0385) | Total Time 0.00(0.00)\n",
      "Iter 15110 | Time 24.6363(25.4934) | Bit/dim 3.5750(3.5657) | Xent 0.1075(0.1164) | Loss 9.2683(9.7765) | Error 0.0322(0.0398) Steps 1018(1030.30) | Grad Norm 4.0067(3.0256) | Total Time 0.00(0.00)\n",
      "Iter 15120 | Time 25.4573(25.5347) | Bit/dim 3.5858(3.5651) | Xent 0.1052(0.1162) | Loss 9.4221(9.6714) | Error 0.0378(0.0396) Steps 1036(1029.48) | Grad Norm 2.9544(3.0566) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0275 | Time 118.9758, Epoch Time 1539.2296(1476.3494), Bit/dim 3.5739(best: 3.5709), Xent 0.9639, Loss 4.0559, Error 0.2211(best: 0.2204)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15130 | Time 25.4941(25.5384) | Bit/dim 3.5436(3.5643) | Xent 0.1356(0.1160) | Loss 9.2623(10.5827) | Error 0.0556(0.0401) Steps 1000(1030.57) | Grad Norm 3.8516(3.0639) | Total Time 0.00(0.00)\n",
      "Iter 15140 | Time 25.6515(25.4988) | Bit/dim 3.5208(3.5648) | Xent 0.1220(0.1148) | Loss 9.2824(10.2657) | Error 0.0411(0.0396) Steps 1060(1032.19) | Grad Norm 4.7138(3.0942) | Total Time 0.00(0.00)\n",
      "Iter 15150 | Time 25.4244(25.5136) | Bit/dim 3.5249(3.5639) | Xent 0.1018(0.1153) | Loss 9.3283(10.0309) | Error 0.0378(0.0397) Steps 1000(1031.60) | Grad Norm 2.6011(3.0854) | Total Time 0.00(0.00)\n",
      "Iter 15160 | Time 25.9363(25.5495) | Bit/dim 3.5485(3.5661) | Xent 0.1318(0.1145) | Loss 9.3547(9.8576) | Error 0.0422(0.0390) Steps 1024(1035.45) | Grad Norm 3.1274(3.1422) | Total Time 0.00(0.00)\n",
      "Iter 15170 | Time 26.3727(25.6897) | Bit/dim 3.5627(3.5642) | Xent 0.0804(0.1142) | Loss 9.3735(9.7256) | Error 0.0344(0.0389) Steps 1060(1037.48) | Grad Norm 2.6469(3.0946) | Total Time 0.00(0.00)\n",
      "Iter 15180 | Time 26.2453(25.7112) | Bit/dim 3.5209(3.5617) | Xent 0.1170(0.1144) | Loss 9.3353(9.6333) | Error 0.0356(0.0390) Steps 1000(1035.09) | Grad Norm 2.9239(3.0499) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0276 | Time 120.3624, Epoch Time 1551.1074(1478.5922), Bit/dim 3.5709(best: 3.5709), Xent 0.9566, Loss 4.0493, Error 0.2207(best: 0.2204)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15190 | Time 25.9131(25.7183) | Bit/dim 3.5251(3.5624) | Xent 0.1286(0.1140) | Loss 9.3932(10.4176) | Error 0.0400(0.0386) Steps 1006(1036.35) | Grad Norm 2.9473(3.0603) | Total Time 0.00(0.00)\n",
      "Iter 15200 | Time 27.6924(25.8420) | Bit/dim 3.5768(3.5613) | Xent 0.1063(0.1149) | Loss 9.3626(10.1458) | Error 0.0367(0.0390) Steps 1108(1036.67) | Grad Norm 2.6867(3.1397) | Total Time 0.00(0.00)\n",
      "Iter 15210 | Time 26.4458(25.8922) | Bit/dim 3.5662(3.5636) | Xent 0.1251(0.1144) | Loss 9.4234(9.9535) | Error 0.0433(0.0392) Steps 1072(1036.95) | Grad Norm 3.1061(3.3024) | Total Time 0.00(0.00)\n",
      "Iter 15220 | Time 26.0189(25.9737) | Bit/dim 3.5416(3.5663) | Xent 0.1128(0.1147) | Loss 9.4386(9.8110) | Error 0.0389(0.0392) Steps 1090(1042.27) | Grad Norm 3.2754(3.3561) | Total Time 0.00(0.00)\n",
      "Iter 15230 | Time 26.2060(26.0449) | Bit/dim 3.5765(3.5665) | Xent 0.1637(0.1146) | Loss 9.5189(9.7031) | Error 0.0544(0.0388) Steps 1072(1047.11) | Grad Norm 4.7094(3.4246) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0277 | Time 119.6799, Epoch Time 1570.9053(1481.3616), Bit/dim 3.5772(best: 3.5709), Xent 0.9831, Loss 4.0688, Error 0.2222(best: 0.2204)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15240 | Time 26.1026(26.0148) | Bit/dim 3.5738(3.5637) | Xent 0.1289(0.1146) | Loss 9.4249(10.5957) | Error 0.0433(0.0389) Steps 1048(1045.77) | Grad Norm 3.7705(3.3844) | Total Time 0.00(0.00)\n",
      "Iter 15250 | Time 26.5200(26.0528) | Bit/dim 3.5651(3.5645) | Xent 0.1290(0.1124) | Loss 9.3174(10.2724) | Error 0.0411(0.0381) Steps 1054(1043.70) | Grad Norm 4.6103(3.4347) | Total Time 0.00(0.00)\n",
      "Iter 15260 | Time 25.5516(26.0746) | Bit/dim 3.5751(3.5656) | Xent 0.1310(0.1143) | Loss 9.5292(10.0362) | Error 0.0422(0.0390) Steps 1018(1044.61) | Grad Norm 4.8298(3.4076) | Total Time 0.00(0.00)\n",
      "Iter 15270 | Time 25.6542(26.1295) | Bit/dim 3.5734(3.5672) | Xent 0.1273(0.1162) | Loss 9.3974(9.8743) | Error 0.0522(0.0401) Steps 1012(1046.56) | Grad Norm 4.1249(3.4746) | Total Time 0.00(0.00)\n",
      "Iter 15280 | Time 25.3496(26.0955) | Bit/dim 3.5953(3.5701) | Xent 0.1076(0.1181) | Loss 9.2893(9.7382) | Error 0.0389(0.0409) Steps 1024(1043.97) | Grad Norm 2.9906(3.5509) | Total Time 0.00(0.00)\n",
      "Iter 15290 | Time 25.7534(25.9834) | Bit/dim 3.5651(3.5687) | Xent 0.1316(0.1175) | Loss 9.3589(9.6375) | Error 0.0467(0.0405) Steps 1012(1043.49) | Grad Norm 5.3860(3.5206) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0278 | Time 120.2132, Epoch Time 1571.2369(1484.0578), Bit/dim 3.5815(best: 3.5709), Xent 0.9774, Loss 4.0702, Error 0.2224(best: 0.2204)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15300 | Time 26.1249(26.0746) | Bit/dim 3.5571(3.5687) | Xent 0.1283(0.1167) | Loss 9.4270(10.4310) | Error 0.0478(0.0401) Steps 1072(1044.88) | Grad Norm 4.7597(3.6269) | Total Time 0.00(0.00)\n",
      "Iter 15310 | Time 26.2400(26.1507) | Bit/dim 3.5531(3.5686) | Xent 0.1233(0.1161) | Loss 9.2209(10.1515) | Error 0.0467(0.0397) Steps 1042(1044.41) | Grad Norm 3.7677(3.6601) | Total Time 0.00(0.00)\n",
      "Iter 15320 | Time 26.7195(26.2885) | Bit/dim 3.5667(3.5729) | Xent 0.1276(0.1179) | Loss 9.4844(9.9648) | Error 0.0378(0.0397) Steps 1090(1048.27) | Grad Norm 3.7215(3.6865) | Total Time 0.00(0.00)\n",
      "Iter 15330 | Time 26.7465(26.4859) | Bit/dim 3.5988(3.5745) | Xent 0.1239(0.1188) | Loss 9.5418(9.8265) | Error 0.0433(0.0404) Steps 1072(1053.28) | Grad Norm 13.5715(4.1195) | Total Time 0.00(0.00)\n",
      "Iter 15340 | Time 29.1241(26.8938) | Bit/dim 3.6617(3.5835) | Xent 0.1739(0.1255) | Loss 9.6555(9.7498) | Error 0.0589(0.0422) Steps 1132(1061.81) | Grad Norm 13.9586(4.8988) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0279 | Time 124.3470, Epoch Time 1632.0664(1488.4981), Bit/dim 3.6149(best: 3.5709), Xent 0.9598, Loss 4.0948, Error 0.2274(best: 0.2204)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15350 | Time 27.4599(27.3780) | Bit/dim 3.6088(3.5923) | Xent 0.1263(0.1354) | Loss 9.4018(10.7351) | Error 0.0344(0.0458) Steps 1048(1070.02) | Grad Norm 7.0207(6.3846) | Total Time 0.00(0.00)\n",
      "Iter 15360 | Time 30.2774(27.6898) | Bit/dim 3.6075(3.5982) | Xent 0.1673(0.1424) | Loss 9.6336(10.4190) | Error 0.0522(0.0483) Steps 1150(1080.42) | Grad Norm 7.4026(7.0381) | Total Time 0.00(0.00)\n",
      "Iter 15370 | Time 28.3064(28.2380) | Bit/dim 3.6075(3.6034) | Xent 0.1390(0.1431) | Loss 9.5169(10.1895) | Error 0.0500(0.0488) Steps 1066(1086.17) | Grad Norm 14.4102(7.0880) | Total Time 0.00(0.00)\n",
      "Iter 15380 | Time 29.3289(28.2950) | Bit/dim 3.6200(3.6040) | Xent 0.1089(0.1409) | Loss 9.5396(10.0120) | Error 0.0367(0.0481) Steps 1096(1089.82) | Grad Norm 3.3288(6.7821) | Total Time 0.00(0.00)\n",
      "Iter 15390 | Time 29.4840(28.3920) | Bit/dim 3.5870(3.5993) | Xent 0.1612(0.1413) | Loss 9.3783(9.8664) | Error 0.0622(0.0488) Steps 1084(1090.92) | Grad Norm 4.4428(6.5641) | Total Time 0.00(0.00)\n",
      "Iter 15400 | Time 28.4010(28.2931) | Bit/dim 3.5716(3.5983) | Xent 0.1162(0.1365) | Loss 9.4704(9.7616) | Error 0.0433(0.0469) Steps 1078(1088.90) | Grad Norm 5.7638(6.5816) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0280 | Time 122.2000, Epoch Time 1717.6720(1495.3733), Bit/dim 3.5931(best: 3.5709), Xent 0.9667, Loss 4.0765, Error 0.2283(best: 0.2204)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15410 | Time 28.7514(28.2802) | Bit/dim 3.5681(3.5951) | Xent 0.1337(0.1328) | Loss 9.2869(10.5507) | Error 0.0444(0.0457) Steps 1030(1086.70) | Grad Norm 3.6724(6.1092) | Total Time 0.00(0.00)\n",
      "Iter 15420 | Time 27.5427(28.1897) | Bit/dim 3.5988(3.5910) | Xent 0.1181(0.1304) | Loss 9.4622(10.2548) | Error 0.0467(0.0452) Steps 1102(1086.53) | Grad Norm 9.8451(6.0715) | Total Time 0.00(0.00)\n",
      "Iter 15430 | Time 28.2166(28.0912) | Bit/dim 3.6017(3.5861) | Xent 0.0918(0.1299) | Loss 9.3899(10.0322) | Error 0.0311(0.0455) Steps 1018(1083.19) | Grad Norm 3.2185(6.0409) | Total Time 0.00(0.00)\n",
      "Iter 15440 | Time 26.2016(27.8441) | Bit/dim 3.5611(3.5827) | Xent 0.1097(0.1263) | Loss 9.3890(9.8588) | Error 0.0278(0.0438) Steps 1060(1074.72) | Grad Norm 5.1622(5.7528) | Total Time 0.00(0.00)\n",
      "Iter 15450 | Time 27.4834(27.7165) | Bit/dim 3.5831(3.5806) | Xent 0.1340(0.1252) | Loss 9.3931(9.7423) | Error 0.0456(0.0432) Steps 1048(1074.26) | Grad Norm 3.4610(5.6516) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0281 | Time 121.0564, Epoch Time 1663.7628(1500.4250), Bit/dim 3.5847(best: 3.5709), Xent 0.9758, Loss 4.0726, Error 0.2255(best: 0.2204)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15460 | Time 28.3165(27.7676) | Bit/dim 3.5354(3.5768) | Xent 0.1326(0.1238) | Loss 9.4570(10.6510) | Error 0.0533(0.0430) Steps 1090(1070.05) | Grad Norm 4.5450(5.3819) | Total Time 0.00(0.00)\n",
      "Iter 15470 | Time 27.6052(27.6722) | Bit/dim 3.5276(3.5752) | Xent 0.1307(0.1224) | Loss 9.3752(10.3244) | Error 0.0600(0.0431) Steps 1036(1069.29) | Grad Norm 4.8046(5.0162) | Total Time 0.00(0.00)\n",
      "Iter 15480 | Time 26.6448(27.5290) | Bit/dim 3.5564(3.5723) | Xent 0.1071(0.1224) | Loss 9.4957(10.0862) | Error 0.0378(0.0429) Steps 1078(1066.17) | Grad Norm 3.5796(4.9239) | Total Time 0.00(0.00)\n",
      "Iter 15490 | Time 26.8894(27.4152) | Bit/dim 3.5940(3.5710) | Xent 0.1150(0.1216) | Loss 9.5290(9.8985) | Error 0.0444(0.0427) Steps 1042(1064.59) | Grad Norm 3.8771(4.6966) | Total Time 0.00(0.00)\n",
      "Iter 15500 | Time 27.7146(27.3507) | Bit/dim 3.5720(3.5725) | Xent 0.1003(0.1183) | Loss 9.4699(9.7683) | Error 0.0311(0.0411) Steps 1036(1060.47) | Grad Norm 8.8966(4.6397) | Total Time 0.00(0.00)\n",
      "Iter 15510 | Time 27.2681(27.3394) | Bit/dim 3.5755(3.5738) | Xent 0.1267(0.1184) | Loss 9.3327(9.6743) | Error 0.0411(0.0411) Steps 1060(1059.12) | Grad Norm 4.5532(4.7340) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0282 | Time 119.9099, Epoch Time 1639.4097(1504.5945), Bit/dim 3.5792(best: 3.5709), Xent 0.9986, Loss 4.0785, Error 0.2260(best: 0.2204)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15520 | Time 27.5774(27.2728) | Bit/dim 3.5741(3.5733) | Xent 0.1052(0.1170) | Loss 9.4998(10.4593) | Error 0.0322(0.0405) Steps 1078(1057.87) | Grad Norm 5.8135(4.8411) | Total Time 0.00(0.00)\n",
      "Iter 15530 | Time 27.2537(27.2223) | Bit/dim 3.5449(3.5715) | Xent 0.1235(0.1182) | Loss 9.3876(10.1799) | Error 0.0489(0.0410) Steps 1006(1058.02) | Grad Norm 9.2322(4.9181) | Total Time 0.00(0.00)\n",
      "Iter 15540 | Time 27.1768(27.1206) | Bit/dim 3.5526(3.5696) | Xent 0.1225(0.1171) | Loss 9.4170(9.9741) | Error 0.0367(0.0404) Steps 1054(1059.46) | Grad Norm 3.8703(4.6830) | Total Time 0.00(0.00)\n",
      "Iter 15550 | Time 26.9567(26.9909) | Bit/dim 3.5573(3.5672) | Xent 0.1131(0.1162) | Loss 9.4221(9.8215) | Error 0.0378(0.0403) Steps 1084(1058.93) | Grad Norm 3.0408(4.7239) | Total Time 0.00(0.00)\n",
      "Iter 15560 | Time 27.4175(27.0230) | Bit/dim 3.5257(3.5701) | Xent 0.1309(0.1152) | Loss 9.2976(9.7064) | Error 0.0422(0.0399) Steps 1102(1060.08) | Grad Norm 7.5876(4.6713) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0283 | Time 122.2100, Epoch Time 1621.9750(1508.1159), Bit/dim 3.5798(best: 3.5709), Xent 0.9834, Loss 4.0715, Error 0.2259(best: 0.2204)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15570 | Time 27.5872(27.0440) | Bit/dim 3.5371(3.5706) | Xent 0.1251(0.1150) | Loss 9.3760(10.6122) | Error 0.0400(0.0395) Steps 1048(1059.23) | Grad Norm 6.9084(4.7400) | Total Time 0.00(0.00)\n",
      "Iter 15580 | Time 25.9806(26.9648) | Bit/dim 3.5568(3.5710) | Xent 0.1110(0.1168) | Loss 9.3720(10.2944) | Error 0.0467(0.0406) Steps 1084(1060.80) | Grad Norm 9.5685(5.7520) | Total Time 0.00(0.00)\n",
      "Iter 15590 | Time 29.6436(27.3420) | Bit/dim 3.7740(3.5940) | Xent 0.3566(0.1402) | Loss 10.2104(10.1492) | Error 0.1000(0.0469) Steps 1138(1069.43) | Grad Norm 16.0817(10.2865) | Total Time 0.00(0.00)\n",
      "Iter 15600 | Time 29.7922(28.1798) | Bit/dim 3.7826(3.6513) | Xent 0.3720(0.2116) | Loss 10.0898(10.1747) | Error 0.1189(0.0681) Steps 1102(1084.02) | Grad Norm 70.4874(23.4501) | Total Time 0.00(0.00)\n",
      "Iter 15610 | Time 27.5137(28.1580) | Bit/dim 3.6696(3.6661) | Xent 0.3186(0.2327) | Loss 9.7436(10.0722) | Error 0.1000(0.0760) Steps 1084(1077.36) | Grad Norm 11.9159(21.4033) | Total Time 0.00(0.00)\n",
      "Iter 15620 | Time 27.2827(27.9496) | Bit/dim 3.6145(3.6619) | Xent 0.1820(0.2257) | Loss 9.3537(9.9471) | Error 0.0700(0.0753) Steps 1084(1072.92) | Grad Norm 10.4664(18.1634) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0284 | Time 117.5142, Epoch Time 1683.9771(1513.3918), Bit/dim 3.6457(best: 3.5709), Xent 0.9130, Loss 4.1022, Error 0.2246(best: 0.2204)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15630 | Time 26.8333(27.8962) | Bit/dim 3.6077(3.6533) | Xent 0.1175(0.2087) | Loss 9.5181(10.6746) | Error 0.0467(0.0710) Steps 1090(1071.21) | Grad Norm 4.4734(14.6051) | Total Time 0.00(0.00)\n",
      "Iter 15640 | Time 27.8079(27.7786) | Bit/dim 3.5844(3.6368) | Xent 0.1300(0.1891) | Loss 9.4523(10.3463) | Error 0.0433(0.0647) Steps 1072(1067.55) | Grad Norm 3.7138(12.1521) | Total Time 0.00(0.00)\n",
      "Iter 15650 | Time 26.8657(27.5424) | Bit/dim 3.6000(3.6238) | Xent 0.1456(0.1738) | Loss 9.3911(10.0990) | Error 0.0422(0.0593) Steps 1066(1065.87) | Grad Norm 6.5996(10.1250) | Total Time 0.00(0.00)\n",
      "Iter 15660 | Time 27.0758(27.4028) | Bit/dim 3.5971(3.6129) | Xent 0.1079(0.1595) | Loss 9.4746(9.9204) | Error 0.0367(0.0547) Steps 1084(1065.86) | Grad Norm 3.6765(8.4194) | Total Time 0.00(0.00)\n",
      "Iter 15670 | Time 27.7988(27.3512) | Bit/dim 3.5958(3.6048) | Xent 0.1158(0.1500) | Loss 9.5123(9.7952) | Error 0.0378(0.0513) Steps 1048(1066.16) | Grad Norm 4.3834(7.6197) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0285 | Time 119.7786, Epoch Time 1637.2162(1517.1065), Bit/dim 3.5824(best: 3.5709), Xent 0.9373, Loss 4.0511, Error 0.2180(best: 0.2204)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15680 | Time 27.0051(27.3277) | Bit/dim 3.5698(3.5950) | Xent 0.1295(0.1429) | Loss 9.3554(10.6675) | Error 0.0411(0.0495) Steps 1042(1061.13) | Grad Norm 7.6847(6.7397) | Total Time 0.00(0.00)\n",
      "Iter 15690 | Time 28.5689(27.3529) | Bit/dim 3.5937(3.5903) | Xent 0.1448(0.1383) | Loss 9.4918(10.3313) | Error 0.0467(0.0482) Steps 1084(1061.77) | Grad Norm 5.5431(5.9973) | Total Time 0.00(0.00)\n",
      "Iter 15700 | Time 26.9744(27.3595) | Bit/dim 3.5679(3.5858) | Xent 0.0992(0.1298) | Loss 9.3308(10.0864) | Error 0.0300(0.0449) Steps 1048(1061.98) | Grad Norm 3.3873(5.7242) | Total Time 0.00(0.00)\n",
      "Iter 15710 | Time 27.6920(27.3445) | Bit/dim 3.5332(3.5799) | Xent 0.1228(0.1257) | Loss 9.3100(9.9075) | Error 0.0433(0.0436) Steps 1036(1060.44) | Grad Norm 4.6229(5.4832) | Total Time 0.00(0.00)\n",
      "Iter 15720 | Time 27.8804(27.2014) | Bit/dim 3.5634(3.5764) | Xent 0.1440(0.1238) | Loss 9.3704(9.7652) | Error 0.0500(0.0430) Steps 1066(1058.22) | Grad Norm 3.6314(5.1187) | Total Time 0.00(0.00)\n",
      "Iter 15730 | Time 26.5958(27.1999) | Bit/dim 3.5658(3.5751) | Xent 0.1402(0.1209) | Loss 9.3572(9.6579) | Error 0.0500(0.0423) Steps 1054(1059.51) | Grad Norm 20.9334(6.2828) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0286 | Time 118.8710, Epoch Time 1634.6160(1520.6318), Bit/dim 3.5817(best: 3.5709), Xent 0.9649, Loss 4.0642, Error 0.2227(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15740 | Time 27.4726(27.2109) | Bit/dim 3.5712(3.5769) | Xent 0.1120(0.1177) | Loss 9.3275(10.4348) | Error 0.0378(0.0409) Steps 1042(1057.58) | Grad Norm 5.0462(5.7697) | Total Time 0.00(0.00)\n",
      "Iter 15750 | Time 27.2323(27.1302) | Bit/dim 3.5637(3.5758) | Xent 0.1166(0.1191) | Loss 9.3838(10.1614) | Error 0.0433(0.0419) Steps 1036(1054.38) | Grad Norm 3.8274(5.4304) | Total Time 0.00(0.00)\n",
      "Iter 15760 | Time 27.1389(27.1617) | Bit/dim 3.5314(3.5721) | Xent 0.1183(0.1192) | Loss 9.3889(9.9565) | Error 0.0444(0.0418) Steps 1054(1053.62) | Grad Norm 9.5482(5.1649) | Total Time 0.00(0.00)\n",
      "Iter 15770 | Time 27.5000(27.3089) | Bit/dim 3.5700(3.5697) | Xent 0.1213(0.1190) | Loss 9.4106(9.8055) | Error 0.0400(0.0414) Steps 1054(1059.26) | Grad Norm 6.4719(5.8056) | Total Time 0.00(0.00)\n",
      "Iter 15780 | Time 27.5450(27.2634) | Bit/dim 3.5375(3.5681) | Xent 0.1030(0.1174) | Loss 9.2975(9.6842) | Error 0.0344(0.0408) Steps 1048(1055.16) | Grad Norm 3.6160(5.6264) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0287 | Time 118.6818, Epoch Time 1636.2477(1524.1003), Bit/dim 3.5798(best: 3.5709), Xent 0.9621, Loss 4.0609, Error 0.2241(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15790 | Time 27.2509(27.1795) | Bit/dim 3.5769(3.5691) | Xent 0.1251(0.1175) | Loss 9.4864(10.5580) | Error 0.0400(0.0407) Steps 1084(1055.01) | Grad Norm 5.9000(5.9529) | Total Time 0.00(0.00)\n",
      "Iter 15800 | Time 26.6610(27.0850) | Bit/dim 3.5275(3.5690) | Xent 0.1120(0.1160) | Loss 9.2381(10.2456) | Error 0.0389(0.0399) Steps 1042(1055.74) | Grad Norm 5.9662(5.8498) | Total Time 0.00(0.00)\n",
      "Iter 15810 | Time 26.6450(27.0538) | Bit/dim 3.5382(3.5683) | Xent 0.1116(0.1185) | Loss 9.1845(10.0123) | Error 0.0344(0.0409) Steps 1072(1056.69) | Grad Norm 3.1109(5.8624) | Total Time 0.00(0.00)\n",
      "Iter 15820 | Time 26.9390(26.9674) | Bit/dim 3.5809(3.5682) | Xent 0.1077(0.1186) | Loss 9.4428(9.8489) | Error 0.0411(0.0410) Steps 1048(1052.10) | Grad Norm 4.6673(5.5094) | Total Time 0.00(0.00)\n",
      "Iter 15830 | Time 26.5773(27.0090) | Bit/dim 3.5461(3.5706) | Xent 0.1179(0.1181) | Loss 9.2492(9.7367) | Error 0.0422(0.0413) Steps 988(1052.65) | Grad Norm 8.2883(5.4814) | Total Time 0.00(0.00)\n",
      "Iter 15840 | Time 26.5323(26.9683) | Bit/dim 3.5379(3.5714) | Xent 0.1123(0.1166) | Loss 9.3387(9.6588) | Error 0.0322(0.0404) Steps 1066(1053.45) | Grad Norm 6.5574(5.7493) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0288 | Time 120.4219, Epoch Time 1616.5604(1526.8741), Bit/dim 3.5858(best: 3.5709), Xent 0.9770, Loss 4.0743, Error 0.2203(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15850 | Time 27.5713(26.9164) | Bit/dim 3.6167(3.5733) | Xent 0.1286(0.1165) | Loss 9.5889(10.4687) | Error 0.0489(0.0406) Steps 1030(1054.48) | Grad Norm 4.7022(5.9657) | Total Time 0.00(0.00)\n",
      "Iter 15860 | Time 26.8864(27.0762) | Bit/dim 3.5949(3.5741) | Xent 0.1306(0.1172) | Loss 9.4478(10.1922) | Error 0.0489(0.0403) Steps 1012(1056.83) | Grad Norm 3.3614(6.5729) | Total Time 0.00(0.00)\n",
      "Iter 15870 | Time 26.9567(26.9542) | Bit/dim 3.5866(3.5727) | Xent 0.1317(0.1173) | Loss 9.4773(9.9814) | Error 0.0478(0.0403) Steps 1042(1056.45) | Grad Norm 6.8597(8.9820) | Total Time 0.00(0.00)\n",
      "Iter 15880 | Time 28.8680(26.9729) | Bit/dim 3.5439(3.5738) | Xent 0.1221(0.1195) | Loss 9.4009(9.8306) | Error 0.0467(0.0421) Steps 1120(1058.57) | Grad Norm 7.1450(9.9099) | Total Time 0.00(0.00)\n",
      "Iter 15890 | Time 26.8493(26.9959) | Bit/dim 3.6022(3.5733) | Xent 0.1234(0.1202) | Loss 9.5654(9.7221) | Error 0.0444(0.0425) Steps 1060(1057.29) | Grad Norm 8.0630(9.9641) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0289 | Time 120.6523, Epoch Time 1625.9153(1529.8453), Bit/dim 3.5810(best: 3.5709), Xent 0.9506, Loss 4.0563, Error 0.2215(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15900 | Time 26.7986(27.0063) | Bit/dim 3.5418(3.5736) | Xent 0.0991(0.1185) | Loss 9.2978(10.6334) | Error 0.0333(0.0414) Steps 1030(1058.95) | Grad Norm 6.2418(8.6397) | Total Time 0.00(0.00)\n",
      "Iter 15910 | Time 27.0781(27.0723) | Bit/dim 3.5687(3.5702) | Xent 0.1178(0.1167) | Loss 9.3851(10.2928) | Error 0.0378(0.0408) Steps 1072(1056.55) | Grad Norm 14.3693(8.0431) | Total Time 0.00(0.00)\n",
      "Iter 15920 | Time 26.5927(27.0570) | Bit/dim 3.6116(3.5727) | Xent 0.1343(0.1169) | Loss 9.4806(10.0555) | Error 0.0478(0.0410) Steps 1078(1057.03) | Grad Norm 3.4923(7.7267) | Total Time 0.00(0.00)\n",
      "Iter 15930 | Time 26.5292(27.0799) | Bit/dim 3.5858(3.5752) | Xent 0.1019(0.1160) | Loss 9.4328(9.8956) | Error 0.0367(0.0405) Steps 1060(1057.55) | Grad Norm 3.5014(6.9270) | Total Time 0.00(0.00)\n",
      "Iter 15940 | Time 28.2562(27.1847) | Bit/dim 3.6090(3.5732) | Xent 0.1094(0.1167) | Loss 9.3883(9.7597) | Error 0.0356(0.0401) Steps 1060(1055.02) | Grad Norm 22.7588(7.1707) | Total Time 0.00(0.00)\n",
      "Iter 15950 | Time 27.6375(27.1689) | Bit/dim 3.6340(3.5787) | Xent 0.2114(0.1231) | Loss 9.6620(9.6914) | Error 0.0689(0.0418) Steps 1090(1058.53) | Grad Norm 43.7516(10.7835) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0290 | Time 124.4332, Epoch Time 1636.3092(1533.0392), Bit/dim 3.6665(best: 3.5709), Xent 1.0188, Loss 4.1759, Error 0.2313(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15960 | Time 28.3341(27.2917) | Bit/dim 3.6180(3.5985) | Xent 0.1767(0.1421) | Loss 9.5972(10.5467) | Error 0.0600(0.0487) Steps 1090(1065.85) | Grad Norm 39.3229(19.5066) | Total Time 0.00(0.00)\n",
      "Iter 15970 | Time 27.0729(27.3772) | Bit/dim 3.5729(3.6036) | Xent 0.1693(0.1503) | Loss 9.5398(10.2842) | Error 0.0567(0.0519) Steps 1048(1065.35) | Grad Norm 11.9042(19.1679) | Total Time 0.00(0.00)\n",
      "Iter 15980 | Time 26.8108(27.2510) | Bit/dim 3.5695(3.6045) | Xent 0.1134(0.1481) | Loss 9.3447(10.0676) | Error 0.0444(0.0518) Steps 1042(1063.96) | Grad Norm 4.7955(15.8879) | Total Time 0.00(0.00)\n",
      "Iter 15990 | Time 27.1323(27.2131) | Bit/dim 3.5971(3.6021) | Xent 0.1144(0.1412) | Loss 9.3994(9.8985) | Error 0.0389(0.0490) Steps 1078(1066.29) | Grad Norm 3.9064(12.9694) | Total Time 0.00(0.00)\n",
      "Iter 16000 | Time 26.2878(27.2205) | Bit/dim 3.5252(3.5936) | Xent 0.1345(0.1368) | Loss 9.4164(9.7650) | Error 0.0489(0.0478) Steps 1078(1066.32) | Grad Norm 8.5851(12.0678) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0291 | Time 122.5903, Epoch Time 1642.4894(1536.3227), Bit/dim 3.5893(best: 3.5709), Xent 0.9929, Loss 4.0857, Error 0.2231(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16010 | Time 26.7908(27.2048) | Bit/dim 3.5475(3.5882) | Xent 0.1393(0.1326) | Loss 9.2978(10.6528) | Error 0.0544(0.0463) Steps 1072(1068.09) | Grad Norm 6.2116(10.3206) | Total Time 0.00(0.00)\n",
      "Iter 16020 | Time 27.4833(27.2372) | Bit/dim 3.5582(3.5854) | Xent 0.1252(0.1293) | Loss 9.3378(10.3398) | Error 0.0400(0.0447) Steps 1060(1069.13) | Grad Norm 11.4364(9.6466) | Total Time 0.00(0.00)\n",
      "Iter 16030 | Time 27.4128(27.2233) | Bit/dim 3.5627(3.5820) | Xent 0.0977(0.1233) | Loss 9.4334(10.0957) | Error 0.0333(0.0425) Steps 1066(1065.57) | Grad Norm 3.2719(8.3324) | Total Time 0.00(0.00)\n",
      "Iter 16040 | Time 26.5339(27.2571) | Bit/dim 3.5247(3.5792) | Xent 0.1294(0.1193) | Loss 9.3503(9.9047) | Error 0.0433(0.0409) Steps 1048(1062.63) | Grad Norm 3.6810(7.4082) | Total Time 0.00(0.00)\n",
      "Iter 16050 | Time 27.4457(27.1810) | Bit/dim 3.5743(3.5780) | Xent 0.1034(0.1181) | Loss 9.3359(9.7685) | Error 0.0411(0.0404) Steps 1054(1061.38) | Grad Norm 4.8159(6.7132) | Total Time 0.00(0.00)\n",
      "Iter 16060 | Time 27.8026(27.1839) | Bit/dim 3.5804(3.5794) | Xent 0.1149(0.1161) | Loss 9.4913(9.6750) | Error 0.0422(0.0402) Steps 1042(1060.63) | Grad Norm 4.9974(6.8138) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0292 | Time 122.7657, Epoch Time 1637.1603(1539.3478), Bit/dim 3.5802(best: 3.5709), Xent 0.9856, Loss 4.0730, Error 0.2271(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16070 | Time 26.4758(27.1307) | Bit/dim 3.5519(3.5775) | Xent 0.1126(0.1158) | Loss 9.2475(10.4643) | Error 0.0367(0.0401) Steps 1048(1059.35) | Grad Norm 4.2124(6.8763) | Total Time 0.00(0.00)\n",
      "Iter 16080 | Time 26.4192(27.1237) | Bit/dim 3.5681(3.5739) | Xent 0.1107(0.1142) | Loss 9.3102(10.1777) | Error 0.0400(0.0394) Steps 1060(1060.45) | Grad Norm 3.5186(6.2934) | Total Time 0.00(0.00)\n",
      "Iter 16090 | Time 26.4284(27.0711) | Bit/dim 3.5836(3.5733) | Xent 0.1133(0.1122) | Loss 9.5310(9.9817) | Error 0.0411(0.0382) Steps 1078(1062.36) | Grad Norm 5.3098(5.6224) | Total Time 0.00(0.00)\n",
      "Iter 16100 | Time 26.4210(26.9946) | Bit/dim 3.5705(3.5729) | Xent 0.1223(0.1121) | Loss 9.3577(9.8239) | Error 0.0356(0.0383) Steps 1060(1059.03) | Grad Norm 9.4886(5.2040) | Total Time 0.00(0.00)\n",
      "Iter 16110 | Time 26.0606(26.9836) | Bit/dim 3.5459(3.5696) | Xent 0.1041(0.1105) | Loss 9.2807(9.7018) | Error 0.0389(0.0377) Steps 1018(1059.79) | Grad Norm 2.7875(4.8540) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0293 | Time 121.7221, Epoch Time 1621.8227(1541.8221), Bit/dim 3.5733(best: 3.5709), Xent 0.9680, Loss 4.0572, Error 0.2213(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16120 | Time 26.6385(26.9697) | Bit/dim 3.5847(3.5696) | Xent 0.1271(0.1138) | Loss 9.5453(10.6232) | Error 0.0411(0.0390) Steps 1066(1059.99) | Grad Norm 3.8392(4.8086) | Total Time 0.00(0.00)\n",
      "Iter 16130 | Time 28.0186(26.9473) | Bit/dim 3.6049(3.5682) | Xent 0.0819(0.1124) | Loss 9.4942(10.3005) | Error 0.0278(0.0386) Steps 1048(1058.72) | Grad Norm 2.7386(5.0475) | Total Time 0.00(0.00)\n",
      "Iter 16140 | Time 27.3988(26.9491) | Bit/dim 3.5738(3.5696) | Xent 0.0978(0.1106) | Loss 9.4316(10.0597) | Error 0.0300(0.0378) Steps 1048(1059.59) | Grad Norm 12.3248(5.2427) | Total Time 0.00(0.00)\n",
      "Iter 16150 | Time 26.2693(26.8609) | Bit/dim 3.5663(3.5668) | Xent 0.1160(0.1089) | Loss 9.3951(9.8801) | Error 0.0433(0.0377) Steps 1054(1060.03) | Grad Norm 2.9104(4.9150) | Total Time 0.00(0.00)\n",
      "Iter 16160 | Time 27.4175(26.8341) | Bit/dim 3.5763(3.5677) | Xent 0.0948(0.1091) | Loss 9.3597(9.7599) | Error 0.0300(0.0375) Steps 1060(1061.62) | Grad Norm 3.1458(4.7715) | Total Time 0.00(0.00)\n",
      "Iter 16170 | Time 26.4002(26.7765) | Bit/dim 3.5540(3.5663) | Xent 0.1047(0.1094) | Loss 9.3560(9.6532) | Error 0.0367(0.0379) Steps 1072(1059.25) | Grad Norm 3.7997(4.6213) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0294 | Time 120.7783, Epoch Time 1612.4637(1543.9413), Bit/dim 3.5742(best: 3.5709), Xent 0.9820, Loss 4.0651, Error 0.2232(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16180 | Time 24.9087(26.6781) | Bit/dim 3.5654(3.5657) | Xent 0.1149(0.1079) | Loss 9.3450(10.4312) | Error 0.0400(0.0374) Steps 1030(1056.21) | Grad Norm 6.6804(4.7960) | Total Time 0.00(0.00)\n",
      "Iter 16190 | Time 25.7131(26.6363) | Bit/dim 3.5695(3.5648) | Xent 0.1104(0.1065) | Loss 9.4780(10.1508) | Error 0.0333(0.0367) Steps 1078(1055.23) | Grad Norm 3.2211(4.5983) | Total Time 0.00(0.00)\n",
      "Iter 16200 | Time 26.6836(26.5724) | Bit/dim 3.5872(3.5641) | Xent 0.1194(0.1075) | Loss 9.5179(9.9600) | Error 0.0433(0.0372) Steps 1084(1056.05) | Grad Norm 3.0966(4.2241) | Total Time 0.00(0.00)\n",
      "Iter 16210 | Time 26.5694(26.4180) | Bit/dim 3.5704(3.5653) | Xent 0.1074(0.1080) | Loss 9.4219(9.8046) | Error 0.0367(0.0376) Steps 1066(1049.64) | Grad Norm 3.6160(4.1500) | Total Time 0.00(0.00)\n",
      "Iter 16220 | Time 26.6212(26.4360) | Bit/dim 3.5851(3.5655) | Xent 0.0913(0.1088) | Loss 9.3460(9.6892) | Error 0.0378(0.0376) Steps 1030(1050.47) | Grad Norm 3.0142(3.9414) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0295 | Time 121.2172, Epoch Time 1588.9375(1545.2912), Bit/dim 3.5775(best: 3.5709), Xent 1.0099, Loss 4.0824, Error 0.2223(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16230 | Time 27.1708(26.4478) | Bit/dim 3.6079(3.5665) | Xent 0.1157(0.1094) | Loss 9.4260(10.5757) | Error 0.0422(0.0380) Steps 1054(1050.21) | Grad Norm 5.0503(3.8655) | Total Time 0.00(0.00)\n",
      "Iter 16240 | Time 25.6505(26.4268) | Bit/dim 3.5640(3.5648) | Xent 0.1052(0.1079) | Loss 9.4466(10.2494) | Error 0.0356(0.0373) Steps 1054(1048.65) | Grad Norm 2.8202(3.7537) | Total Time 0.00(0.00)\n",
      "Iter 16250 | Time 26.3311(26.3770) | Bit/dim 3.5558(3.5676) | Xent 0.1179(0.1080) | Loss 9.3850(10.0142) | Error 0.0422(0.0375) Steps 1042(1046.05) | Grad Norm 4.4876(5.0320) | Total Time 0.00(0.00)\n",
      "Iter 16260 | Time 28.1003(26.5995) | Bit/dim 3.5445(3.5675) | Xent 0.1065(0.1100) | Loss 9.3613(9.8487) | Error 0.0444(0.0383) Steps 1048(1049.20) | Grad Norm 5.7033(5.3686) | Total Time 0.00(0.00)\n",
      "Iter 16270 | Time 28.0472(26.6902) | Bit/dim 3.5981(3.5672) | Xent 0.0944(0.1102) | Loss 9.4445(9.7212) | Error 0.0333(0.0380) Steps 1048(1048.19) | Grad Norm 3.1933(5.5659) | Total Time 0.00(0.00)\n",
      "Iter 16280 | Time 26.0776(26.7758) | Bit/dim 3.5651(3.5641) | Xent 0.1007(0.1108) | Loss 9.2739(9.6215) | Error 0.0356(0.0385) Steps 1030(1050.55) | Grad Norm 3.5010(5.1605) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0296 | Time 121.2843, Epoch Time 1610.9881(1547.2621), Bit/dim 3.5815(best: 3.5709), Xent 1.0313, Loss 4.0971, Error 0.2242(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16290 | Time 26.4898(26.8111) | Bit/dim 3.5521(3.5670) | Xent 0.1053(0.1108) | Loss 9.4128(10.4361) | Error 0.0367(0.0384) Steps 1048(1052.38) | Grad Norm 3.6298(4.9650) | Total Time 0.00(0.00)\n",
      "Iter 16300 | Time 25.6490(26.7258) | Bit/dim 3.5785(3.5677) | Xent 0.1515(0.1130) | Loss 9.4722(10.1623) | Error 0.0511(0.0388) Steps 1036(1052.32) | Grad Norm 14.8563(5.2014) | Total Time 0.00(0.00)\n",
      "Iter 16310 | Time 27.0865(26.7501) | Bit/dim 3.5662(3.5648) | Xent 0.1155(0.1123) | Loss 9.3344(9.9686) | Error 0.0400(0.0390) Steps 1060(1053.80) | Grad Norm 3.8672(5.0511) | Total Time 0.00(0.00)\n",
      "Iter 16320 | Time 26.9678(26.7337) | Bit/dim 3.5659(3.5633) | Xent 0.1186(0.1132) | Loss 9.4922(9.8216) | Error 0.0411(0.0393) Steps 1060(1053.19) | Grad Norm 5.3913(4.8424) | Total Time 0.00(0.00)\n",
      "Iter 16330 | Time 26.7402(26.6930) | Bit/dim 3.5631(3.5649) | Xent 0.1144(0.1130) | Loss 9.3671(9.7048) | Error 0.0400(0.0394) Steps 1030(1053.41) | Grad Norm 4.7023(4.7301) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0297 | Time 121.2995, Epoch Time 1609.3062(1549.1235), Bit/dim 3.5737(best: 3.5709), Xent 1.0107, Loss 4.0791, Error 0.2256(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16340 | Time 26.7090(26.7078) | Bit/dim 3.5607(3.5673) | Xent 0.0771(0.1103) | Loss 9.3931(10.6357) | Error 0.0211(0.0385) Steps 1078(1051.06) | Grad Norm 4.1998(5.3881) | Total Time 0.00(0.00)\n",
      "Iter 16350 | Time 26.6716(26.7851) | Bit/dim 3.5244(3.5690) | Xent 0.1481(0.1129) | Loss 9.3278(10.3116) | Error 0.0522(0.0393) Steps 1060(1051.21) | Grad Norm 4.4755(5.2136) | Total Time 0.00(0.00)\n",
      "Iter 16360 | Time 28.4648(26.8006) | Bit/dim 3.5457(3.5662) | Xent 0.0916(0.1103) | Loss 9.3442(10.0653) | Error 0.0300(0.0385) Steps 1066(1052.62) | Grad Norm 6.0979(5.2795) | Total Time 0.00(0.00)\n",
      "Iter 16370 | Time 27.1315(26.9638) | Bit/dim 3.5759(3.5664) | Xent 0.0923(0.1104) | Loss 9.3518(9.9006) | Error 0.0356(0.0387) Steps 1066(1059.49) | Grad Norm 7.7061(5.1160) | Total Time 0.00(0.00)\n",
      "Iter 16380 | Time 27.4842(26.9766) | Bit/dim 3.5560(3.5659) | Xent 0.0995(0.1097) | Loss 9.3129(9.7605) | Error 0.0411(0.0383) Steps 1066(1056.93) | Grad Norm 3.1116(5.2140) | Total Time 0.00(0.00)\n",
      "Iter 16390 | Time 27.2042(26.9878) | Bit/dim 3.5514(3.5685) | Xent 0.1047(0.1102) | Loss 9.4293(9.6675) | Error 0.0400(0.0380) Steps 1024(1056.72) | Grad Norm 3.6152(5.0377) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0298 | Time 120.7825, Epoch Time 1624.6449(1551.3891), Bit/dim 3.5748(best: 3.5709), Xent 1.0110, Loss 4.0803, Error 0.2241(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16400 | Time 27.0494(26.9838) | Bit/dim 3.5867(3.5704) | Xent 0.1025(0.1091) | Loss 9.4601(10.4371) | Error 0.0367(0.0378) Steps 1042(1057.94) | Grad Norm 3.8108(4.8417) | Total Time 0.00(0.00)\n",
      "Iter 16410 | Time 27.9964(27.0208) | Bit/dim 3.5860(3.5712) | Xent 0.1126(0.1103) | Loss 9.4097(10.1731) | Error 0.0444(0.0386) Steps 1108(1058.66) | Grad Norm 4.1891(5.0264) | Total Time 0.00(0.00)\n",
      "Iter 16420 | Time 26.8737(27.0555) | Bit/dim 3.5407(3.5696) | Xent 0.1036(0.1108) | Loss 9.1740(9.9673) | Error 0.0322(0.0386) Steps 1072(1062.06) | Grad Norm 3.9784(4.8763) | Total Time 0.00(0.00)\n",
      "Iter 16430 | Time 27.1553(27.0503) | Bit/dim 3.5725(3.5674) | Xent 0.1075(0.1101) | Loss 9.3644(9.8058) | Error 0.0311(0.0378) Steps 1066(1057.16) | Grad Norm 2.9585(4.9167) | Total Time 0.00(0.00)\n",
      "Iter 16440 | Time 26.5947(26.9824) | Bit/dim 3.5488(3.5670) | Xent 0.1005(0.1118) | Loss 9.2587(9.7036) | Error 0.0322(0.0382) Steps 1072(1057.08) | Grad Norm 7.7909(5.0259) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0299 | Time 122.4456, Epoch Time 1624.3653(1553.5784), Bit/dim 3.5781(best: 3.5709), Xent 1.0299, Loss 4.0931, Error 0.2276(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16450 | Time 26.6406(26.9398) | Bit/dim 3.5456(3.5666) | Xent 0.1266(0.1123) | Loss 9.2742(10.6108) | Error 0.0467(0.0385) Steps 1054(1058.20) | Grad Norm 7.2858(5.6013) | Total Time 0.00(0.00)\n",
      "Iter 16460 | Time 26.8359(26.9946) | Bit/dim 3.5759(3.5692) | Xent 0.1041(0.1106) | Loss 9.3486(10.2922) | Error 0.0367(0.0380) Steps 1078(1060.82) | Grad Norm 9.5011(6.1069) | Total Time 0.00(0.00)\n",
      "Iter 16470 | Time 26.1273(26.9759) | Bit/dim 3.6168(3.5683) | Xent 0.1255(0.1143) | Loss 9.4525(10.0550) | Error 0.0378(0.0390) Steps 1042(1060.18) | Grad Norm 6.8165(8.0810) | Total Time 0.00(0.00)\n",
      "Iter 16480 | Time 25.9944(26.9140) | Bit/dim 3.5592(3.5683) | Xent 0.0986(0.1166) | Loss 9.4096(9.8820) | Error 0.0378(0.0403) Steps 1078(1058.75) | Grad Norm 4.6848(8.1394) | Total Time 0.00(0.00)\n",
      "Iter 16490 | Time 26.0159(26.7668) | Bit/dim 3.5616(3.5680) | Xent 0.1246(0.1160) | Loss 9.3753(9.7510) | Error 0.0444(0.0402) Steps 1072(1055.48) | Grad Norm 5.9865(7.6670) | Total Time 0.00(0.00)\n",
      "Iter 16500 | Time 27.0191(26.8166) | Bit/dim 3.5704(3.5728) | Xent 0.1115(0.1129) | Loss 9.2920(9.6560) | Error 0.0433(0.0397) Steps 1084(1054.43) | Grad Norm 5.4082(7.5739) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0300 | Time 125.7631, Epoch Time 1620.7276(1555.5929), Bit/dim 3.5917(best: 3.5709), Xent 1.0295, Loss 4.1065, Error 0.2225(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16510 | Time 26.5844(27.0079) | Bit/dim 3.5628(3.5742) | Xent 0.0981(0.1123) | Loss 9.3329(10.4682) | Error 0.0367(0.0398) Steps 1036(1066.10) | Grad Norm 6.9663(7.0331) | Total Time 0.00(0.00)\n",
      "Iter 16520 | Time 28.2592(27.2757) | Bit/dim 3.5929(3.5770) | Xent 0.1324(0.1137) | Loss 9.4951(10.2088) | Error 0.0467(0.0403) Steps 1102(1072.86) | Grad Norm 12.1773(7.1871) | Total Time 0.00(0.00)\n",
      "Iter 16530 | Time 28.1475(27.4723) | Bit/dim 3.5882(3.5802) | Xent 0.1073(0.1130) | Loss 9.4341(10.0117) | Error 0.0411(0.0403) Steps 1066(1075.82) | Grad Norm 10.6292(7.7349) | Total Time 0.00(0.00)\n",
      "Iter 16540 | Time 27.0252(27.5687) | Bit/dim 3.5724(3.5798) | Xent 0.1124(0.1125) | Loss 9.5003(9.8736) | Error 0.0367(0.0393) Steps 1108(1083.78) | Grad Norm 3.0317(7.3077) | Total Time 0.00(0.00)\n",
      "Iter 16550 | Time 27.6769(27.7228) | Bit/dim 3.5737(3.5810) | Xent 0.1163(0.1124) | Loss 9.3433(9.7689) | Error 0.0433(0.0391) Steps 1084(1086.49) | Grad Norm 8.3748(7.0909) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0301 | Time 131.8423, Epoch Time 1686.5496(1559.5216), Bit/dim 3.6090(best: 3.5709), Xent 1.0393, Loss 4.1287, Error 0.2246(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16560 | Time 28.4648(27.8729) | Bit/dim 3.6004(3.5818) | Xent 0.1295(0.1157) | Loss 9.4454(10.7278) | Error 0.0444(0.0403) Steps 1162(1091.39) | Grad Norm 6.7618(8.0298) | Total Time 0.00(0.00)\n",
      "Iter 16570 | Time 27.2279(27.8694) | Bit/dim 3.5642(3.5812) | Xent 0.1086(0.1156) | Loss 9.3649(10.4048) | Error 0.0422(0.0410) Steps 1078(1088.14) | Grad Norm 8.6151(7.6962) | Total Time 0.00(0.00)\n",
      "Iter 16580 | Time 28.5592(28.0026) | Bit/dim 3.6248(3.5837) | Xent 0.1405(0.1195) | Loss 9.6071(10.1738) | Error 0.0556(0.0432) Steps 1132(1088.13) | Grad Norm 31.1790(9.4000) | Total Time 0.00(0.00)\n",
      "Iter 16590 | Time 29.8212(28.3833) | Bit/dim 3.6284(3.5918) | Xent 0.1618(0.1286) | Loss 9.6670(10.0190) | Error 0.0644(0.0462) Steps 1120(1094.54) | Grad Norm 15.0237(9.8271) | Total Time 0.00(0.00)\n",
      "Iter 16600 | Time 29.3165(28.6210) | Bit/dim 3.6124(3.5956) | Xent 0.1463(0.1326) | Loss 9.4756(9.8871) | Error 0.0489(0.0479) Steps 1096(1099.12) | Grad Norm 12.4036(11.0129) | Total Time 0.00(0.00)\n",
      "Iter 16610 | Time 29.6223(28.8114) | Bit/dim 3.6443(3.6015) | Xent 0.1028(0.1363) | Loss 9.5383(9.8139) | Error 0.0333(0.0493) Steps 1084(1105.64) | Grad Norm 14.9775(13.7169) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0302 | Time 131.6880, Epoch Time 1735.3057(1564.7951), Bit/dim 3.6148(best: 3.5709), Xent 1.0403, Loss 4.1349, Error 0.2226(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16620 | Time 28.4928(28.8118) | Bit/dim 3.5931(3.5993) | Xent 0.1292(0.1318) | Loss 9.5453(10.6268) | Error 0.0500(0.0477) Steps 1048(1104.22) | Grad Norm 8.3858(13.5301) | Total Time 0.00(0.00)\n",
      "Iter 16630 | Time 30.1233(28.9141) | Bit/dim 3.6105(3.6010) | Xent 0.1556(0.1296) | Loss 9.4951(10.3433) | Error 0.0500(0.0464) Steps 1114(1108.26) | Grad Norm 25.3053(13.5090) | Total Time 0.00(0.00)\n",
      "Iter 16640 | Time 28.9561(29.1408) | Bit/dim 3.6405(3.6068) | Xent 0.1481(0.1335) | Loss 9.5886(10.1623) | Error 0.0511(0.0475) Steps 1132(1114.40) | Grad Norm 9.6644(13.3785) | Total Time 0.00(0.00)\n",
      "Iter 16650 | Time 32.0307(29.6566) | Bit/dim 3.8606(3.6293) | Xent 0.4423(0.1643) | Loss 10.3869(10.0778) | Error 0.1244(0.0557) Steps 1222(1128.44) | Grad Norm 41.5628(37.7222) | Total Time 0.00(0.00)\n",
      "Iter 16660 | Time 30.8194(30.3335) | Bit/dim 3.7924(3.6744) | Xent 0.2802(0.2039) | Loss 10.1326(10.0966) | Error 0.0922(0.0672) Steps 1174(1142.72) | Grad Norm 162.9287(72.9476) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0303 | Time 122.3257, Epoch Time 1800.7536(1571.8738), Bit/dim 3.9116(best: 3.5709), Xent 1.0653, Loss 4.4442, Error 0.2628(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16670 | Time 25.8405(29.8878) | Bit/dim 3.9247(3.7268) | Xent 0.5172(0.2690) | Loss 10.6296(11.1591) | Error 0.1467(0.0874) Steps 1060(1127.67) | Grad Norm 10.2446(84.5618) | Total Time 0.00(0.00)\n",
      "Iter 16680 | Time 26.1229(28.9063) | Bit/dim 3.8470(3.7702) | Xent 0.4017(0.3099) | Loss 10.1720(10.9445) | Error 0.1222(0.1006) Steps 1060(1101.52) | Grad Norm 7.6070(64.8410) | Total Time 0.00(0.00)\n",
      "Iter 16690 | Time 25.3290(28.3298) | Bit/dim 3.7977(3.7831) | Xent 0.3176(0.3211) | Loss 10.0054(10.7225) | Error 0.1133(0.1055) Steps 1042(1088.67) | Grad Norm 5.5928(49.4989) | Total Time 0.00(0.00)\n",
      "Iter 16700 | Time 26.7536(27.7592) | Bit/dim 3.7880(3.7812) | Xent 0.2785(0.3051) | Loss 9.9665(10.5048) | Error 0.0933(0.1018) Steps 1060(1075.70) | Grad Norm 5.2996(37.9204) | Total Time 0.00(0.00)\n",
      "Iter 16710 | Time 25.9157(27.3364) | Bit/dim 3.7583(3.7697) | Xent 0.2145(0.2848) | Loss 9.7779(10.3042) | Error 0.0800(0.0966) Steps 1036(1064.54) | Grad Norm 6.7412(29.3302) | Total Time 0.00(0.00)\n",
      "Iter 16720 | Time 27.0947(27.1117) | Bit/dim 3.7118(3.7568) | Xent 0.1919(0.2645) | Loss 9.5897(10.1444) | Error 0.0678(0.0905) Steps 1012(1055.10) | Grad Norm 4.6286(22.8837) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0304 | Time 116.6640, Epoch Time 1589.6510(1572.4072), Bit/dim 3.7176(best: 3.5709), Xent 0.9167, Loss 4.1759, Error 0.2297(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16730 | Time 26.8095(27.0137) | Bit/dim 3.7214(3.7441) | Xent 0.1584(0.2426) | Loss 9.6731(10.8116) | Error 0.0556(0.0829) Steps 1018(1051.16) | Grad Norm 5.1069(18.1805) | Total Time 0.00(0.00)\n",
      "Iter 16740 | Time 26.5431(26.9685) | Bit/dim 3.6578(3.7309) | Xent 0.1850(0.2259) | Loss 9.6285(10.4953) | Error 0.0667(0.0780) Steps 1030(1046.65) | Grad Norm 4.1927(14.5919) | Total Time 0.00(0.00)\n",
      "Iter 16750 | Time 26.5643(26.9622) | Bit/dim 3.6595(3.7167) | Xent 0.1670(0.2100) | Loss 9.5366(10.2575) | Error 0.0600(0.0727) Steps 1012(1041.48) | Grad Norm 3.3656(11.8879) | Total Time 0.00(0.00)\n",
      "Iter 16760 | Time 27.7599(27.1220) | Bit/dim 3.6581(3.7055) | Xent 0.2282(0.2019) | Loss 9.5628(10.0835) | Error 0.0856(0.0700) Steps 1060(1041.51) | Grad Norm 6.8886(10.2630) | Total Time 0.00(0.00)\n",
      "Iter 16770 | Time 27.3915(27.1000) | Bit/dim 3.6410(3.6905) | Xent 0.1897(0.1926) | Loss 9.5354(9.9393) | Error 0.0611(0.0670) Steps 1042(1043.91) | Grad Norm 5.5952(8.9799) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0305 | Time 118.3385, Epoch Time 1625.6961(1574.0058), Bit/dim 3.6580(best: 3.5709), Xent 0.9313, Loss 4.1237, Error 0.2250(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16780 | Time 27.9796(27.2219) | Bit/dim 3.6426(3.6791) | Xent 0.1457(0.1826) | Loss 9.5681(10.7885) | Error 0.0533(0.0639) Steps 1048(1046.09) | Grad Norm 4.3299(7.8164) | Total Time 0.00(0.00)\n",
      "Iter 16790 | Time 26.0702(27.1558) | Bit/dim 3.6699(3.6700) | Xent 0.1661(0.1714) | Loss 9.4297(10.4401) | Error 0.0544(0.0602) Steps 1036(1046.51) | Grad Norm 5.4195(7.0651) | Total Time 0.00(0.00)\n",
      "Iter 16800 | Time 27.7437(27.3534) | Bit/dim 3.6701(3.6622) | Xent 0.1373(0.1641) | Loss 9.6002(10.1956) | Error 0.0544(0.0583) Steps 1072(1049.61) | Grad Norm 4.2860(6.4957) | Total Time 0.00(0.00)\n",
      "Iter 16810 | Time 27.0671(27.4447) | Bit/dim 3.6187(3.6558) | Xent 0.1514(0.1589) | Loss 9.4108(10.0147) | Error 0.0522(0.0566) Steps 1072(1052.42) | Grad Norm 6.0018(6.1814) | Total Time 0.00(0.00)\n",
      "Iter 16820 | Time 28.7152(27.5516) | Bit/dim 3.6089(3.6470) | Xent 0.1452(0.1561) | Loss 9.4637(9.8693) | Error 0.0478(0.0552) Steps 1036(1052.06) | Grad Norm 6.7610(6.1349) | Total Time 0.00(0.00)\n",
      "Iter 16830 | Time 28.4982(27.7340) | Bit/dim 3.6251(3.6420) | Xent 0.1355(0.1533) | Loss 9.4017(9.7829) | Error 0.0456(0.0544) Steps 1114(1061.87) | Grad Norm 5.1023(6.2215) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0306 | Time 123.4640, Epoch Time 1666.4003(1576.7777), Bit/dim 3.6424(best: 3.5709), Xent 0.9666, Loss 4.1257, Error 0.2236(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16840 | Time 28.1885(27.8839) | Bit/dim 3.5891(3.6368) | Xent 0.1251(0.1487) | Loss 9.2668(10.5405) | Error 0.0433(0.0524) Steps 1096(1068.33) | Grad Norm 3.3826(5.8848) | Total Time 0.00(0.00)\n",
      "Iter 16850 | Time 29.3036(28.0975) | Bit/dim 3.6063(3.6360) | Xent 0.1888(0.1508) | Loss 9.6290(10.2781) | Error 0.0633(0.0535) Steps 1114(1076.66) | Grad Norm 6.8696(6.2086) | Total Time 0.00(0.00)\n",
      "Iter 16860 | Time 28.2899(28.3499) | Bit/dim 3.6481(3.6355) | Xent 0.1162(0.1501) | Loss 9.3606(10.0902) | Error 0.0444(0.0531) Steps 1120(1084.79) | Grad Norm 4.0854(6.1364) | Total Time 0.00(0.00)\n",
      "Iter 16870 | Time 29.0432(28.6611) | Bit/dim 3.6072(3.6330) | Xent 0.1411(0.1477) | Loss 9.5043(9.9502) | Error 0.0589(0.0528) Steps 1132(1094.30) | Grad Norm 4.1298(6.1470) | Total Time 0.00(0.00)\n",
      "Iter 16880 | Time 28.2853(28.6913) | Bit/dim 3.6468(3.6305) | Xent 0.1850(0.1453) | Loss 9.6108(9.8447) | Error 0.0622(0.0514) Steps 1084(1094.71) | Grad Norm 5.3731(5.9597) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0307 | Time 126.0361, Epoch Time 1731.1082(1581.4076), Bit/dim 3.6251(best: 3.5709), Xent 0.9577, Loss 4.1039, Error 0.2230(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16890 | Time 28.5315(28.7135) | Bit/dim 3.6232(3.6268) | Xent 0.1242(0.1402) | Loss 9.4650(10.7295) | Error 0.0400(0.0495) Steps 1084(1095.12) | Grad Norm 15.6856(6.4366) | Total Time 0.00(0.00)\n",
      "Iter 16900 | Time 30.0105(28.8724) | Bit/dim 3.6358(3.6268) | Xent 0.1167(0.1351) | Loss 9.5867(10.4158) | Error 0.0344(0.0468) Steps 1102(1095.95) | Grad Norm 9.3979(6.3498) | Total Time 0.00(0.00)\n",
      "Iter 16910 | Time 28.8388(28.8576) | Bit/dim 3.6321(3.6247) | Xent 0.1244(0.1326) | Loss 9.5412(10.1755) | Error 0.0422(0.0457) Steps 1090(1099.88) | Grad Norm 23.8291(6.8874) | Total Time 0.00(0.00)\n",
      "Iter 16920 | Time 29.0317(28.8254) | Bit/dim 3.5991(3.6217) | Xent 0.1357(0.1341) | Loss 9.4785(10.0058) | Error 0.0444(0.0463) Steps 1102(1097.67) | Grad Norm 6.9660(6.8617) | Total Time 0.00(0.00)\n",
      "Iter 16930 | Time 28.7653(28.8300) | Bit/dim 3.6001(3.6195) | Xent 0.1844(0.1356) | Loss 9.5152(9.8661) | Error 0.0711(0.0474) Steps 1096(1096.75) | Grad Norm 12.4760(8.0724) | Total Time 0.00(0.00)\n",
      "Iter 16940 | Time 28.4802(28.7800) | Bit/dim 3.6422(3.6168) | Xent 0.1146(0.1354) | Loss 9.4710(9.7648) | Error 0.0444(0.0476) Steps 1048(1093.98) | Grad Norm 4.6220(7.8362) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0308 | Time 121.8489, Epoch Time 1728.0418(1585.8066), Bit/dim 3.6108(best: 3.5709), Xent 0.9596, Loss 4.0905, Error 0.2237(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16950 | Time 28.3665(28.6702) | Bit/dim 3.6003(3.6100) | Xent 0.1088(0.1310) | Loss 9.4167(10.5252) | Error 0.0378(0.0459) Steps 1102(1090.99) | Grad Norm 3.7412(7.0965) | Total Time 0.00(0.00)\n",
      "Iter 16960 | Time 27.9610(28.5267) | Bit/dim 3.5909(3.6066) | Xent 0.1266(0.1314) | Loss 9.5388(10.2474) | Error 0.0456(0.0464) Steps 1036(1083.52) | Grad Norm 6.1919(6.9088) | Total Time 0.00(0.00)\n",
      "Iter 16970 | Time 27.1807(28.3364) | Bit/dim 3.6158(3.6061) | Xent 0.0890(0.1285) | Loss 9.5736(10.0488) | Error 0.0244(0.0456) Steps 1066(1080.44) | Grad Norm 2.2555(6.3463) | Total Time 0.00(0.00)\n",
      "Iter 16980 | Time 28.8835(28.2461) | Bit/dim 3.5896(3.6021) | Xent 0.1268(0.1272) | Loss 9.5600(9.8845) | Error 0.0478(0.0453) Steps 1072(1075.37) | Grad Norm 4.0401(6.0356) | Total Time 0.00(0.00)\n",
      "Iter 16990 | Time 28.5551(28.1879) | Bit/dim 3.6147(3.5975) | Xent 0.1074(0.1269) | Loss 9.5790(9.7774) | Error 0.0378(0.0449) Steps 1078(1071.27) | Grad Norm 3.1314(5.5110) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0309 | Time 121.3167, Epoch Time 1681.9779(1588.6917), Bit/dim 3.5980(best: 3.5709), Xent 0.9878, Loss 4.0920, Error 0.2241(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17000 | Time 27.1512(28.0009) | Bit/dim 3.6056(3.5967) | Xent 0.1417(0.1251) | Loss 9.4633(10.6616) | Error 0.0522(0.0443) Steps 1090(1069.30) | Grad Norm 5.8207(5.3186) | Total Time 0.00(0.00)\n",
      "Iter 17010 | Time 28.5548(28.0069) | Bit/dim 3.5686(3.5954) | Xent 0.0995(0.1237) | Loss 9.3546(10.3461) | Error 0.0356(0.0435) Steps 1048(1070.54) | Grad Norm 3.3565(5.1240) | Total Time 0.00(0.00)\n",
      "Iter 17020 | Time 27.4044(27.8594) | Bit/dim 3.5621(3.5948) | Xent 0.1218(0.1206) | Loss 9.3889(10.1099) | Error 0.0400(0.0421) Steps 1072(1071.39) | Grad Norm 3.6914(4.8306) | Total Time 0.00(0.00)\n",
      "Iter 17030 | Time 28.0346(27.8790) | Bit/dim 3.5694(3.5895) | Xent 0.0935(0.1195) | Loss 9.2452(9.9120) | Error 0.0300(0.0415) Steps 1090(1069.86) | Grad Norm 3.3144(4.6909) | Total Time 0.00(0.00)\n",
      "Iter 17040 | Time 28.2156(27.7960) | Bit/dim 3.5843(3.5892) | Xent 0.1131(0.1203) | Loss 9.4223(9.7776) | Error 0.0389(0.0423) Steps 1066(1065.55) | Grad Norm 5.6291(4.9971) | Total Time 0.00(0.00)\n",
      "Iter 17050 | Time 27.8811(27.7576) | Bit/dim 3.5585(3.5879) | Xent 0.1317(0.1181) | Loss 9.3837(9.6824) | Error 0.0433(0.0421) Steps 1054(1065.14) | Grad Norm 3.1451(4.6984) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0310 | Time 119.0134, Epoch Time 1658.7711(1590.7941), Bit/dim 3.5967(best: 3.5709), Xent 0.9814, Loss 4.0874, Error 0.2219(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17060 | Time 28.0769(27.6316) | Bit/dim 3.5659(3.5852) | Xent 0.1000(0.1153) | Loss 9.4484(10.4952) | Error 0.0344(0.0405) Steps 1030(1064.99) | Grad Norm 3.2258(4.3755) | Total Time 0.00(0.00)\n",
      "Iter 17070 | Time 28.2383(27.5571) | Bit/dim 3.6179(3.5884) | Xent 0.1147(0.1146) | Loss 9.4349(10.2257) | Error 0.0456(0.0403) Steps 1054(1063.64) | Grad Norm 3.0701(4.3022) | Total Time 0.00(0.00)\n",
      "Iter 17080 | Time 28.0820(27.5785) | Bit/dim 3.5970(3.5874) | Xent 0.1233(0.1145) | Loss 9.3705(10.0092) | Error 0.0389(0.0400) Steps 1072(1063.01) | Grad Norm 3.9521(4.6702) | Total Time 0.00(0.00)\n",
      "Iter 17090 | Time 28.5204(27.6723) | Bit/dim 3.5707(3.5856) | Xent 0.1350(0.1151) | Loss 9.4032(9.8553) | Error 0.0444(0.0404) Steps 1084(1063.21) | Grad Norm 4.3334(4.8203) | Total Time 0.00(0.00)\n",
      "Iter 17100 | Time 27.7877(27.7132) | Bit/dim 3.6000(3.5849) | Xent 0.1220(0.1150) | Loss 9.4481(9.7376) | Error 0.0356(0.0403) Steps 1060(1065.12) | Grad Norm 4.6146(5.1886) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0311 | Time 120.2305, Epoch Time 1656.9488(1592.7788), Bit/dim 3.5944(best: 3.5709), Xent 0.9916, Loss 4.0902, Error 0.2226(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17110 | Time 28.9275(27.7340) | Bit/dim 3.5463(3.5842) | Xent 0.0970(0.1125) | Loss 9.2526(10.6236) | Error 0.0289(0.0391) Steps 1072(1065.60) | Grad Norm 4.8208(5.0145) | Total Time 0.00(0.00)\n",
      "Iter 17120 | Time 26.9305(27.6527) | Bit/dim 3.5748(3.5840) | Xent 0.0967(0.1142) | Loss 9.3706(10.3015) | Error 0.0367(0.0395) Steps 1066(1062.18) | Grad Norm 4.9940(4.9485) | Total Time 0.00(0.00)\n",
      "Iter 17130 | Time 27.7268(27.6042) | Bit/dim 3.6043(3.5851) | Xent 0.0903(0.1122) | Loss 9.5200(10.0617) | Error 0.0289(0.0391) Steps 1096(1065.18) | Grad Norm 3.0075(4.6096) | Total Time 0.00(0.00)\n",
      "Iter 17140 | Time 27.0346(27.6418) | Bit/dim 3.6149(3.5846) | Xent 0.1141(0.1128) | Loss 9.4696(9.8931) | Error 0.0433(0.0392) Steps 1054(1066.98) | Grad Norm 11.6402(4.7441) | Total Time 0.00(0.00)\n",
      "Iter 17150 | Time 27.3256(27.6736) | Bit/dim 3.5891(3.5829) | Xent 0.0992(0.1118) | Loss 9.3713(9.7586) | Error 0.0333(0.0393) Steps 1072(1067.64) | Grad Norm 3.3368(4.5232) | Total Time 0.00(0.00)\n",
      "Iter 17160 | Time 26.6889(27.5634) | Bit/dim 3.5428(3.5806) | Xent 0.1347(0.1120) | Loss 9.3547(9.6598) | Error 0.0456(0.0388) Steps 1072(1068.89) | Grad Norm 4.7881(4.9587) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0312 | Time 121.8591, Epoch Time 1656.7273(1594.6972), Bit/dim 3.5911(best: 3.5709), Xent 0.9982, Loss 4.0901, Error 0.2242(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17170 | Time 28.1740(27.5794) | Bit/dim 3.5991(3.5813) | Xent 0.1096(0.1090) | Loss 9.3053(10.4478) | Error 0.0400(0.0378) Steps 1054(1066.10) | Grad Norm 3.0779(4.7691) | Total Time 0.00(0.00)\n",
      "Iter 17180 | Time 28.1972(27.5697) | Bit/dim 3.5620(3.5804) | Xent 0.1113(0.1086) | Loss 9.4342(10.1667) | Error 0.0378(0.0374) Steps 1066(1063.97) | Grad Norm 2.9847(4.4906) | Total Time 0.00(0.00)\n",
      "Iter 17190 | Time 26.0329(27.5470) | Bit/dim 3.5779(3.5777) | Xent 0.1423(0.1104) | Loss 9.4346(9.9645) | Error 0.0433(0.0378) Steps 1042(1063.66) | Grad Norm 5.6875(4.3237) | Total Time 0.00(0.00)\n",
      "Iter 17200 | Time 26.2382(27.4758) | Bit/dim 3.5844(3.5787) | Xent 0.1276(0.1117) | Loss 9.3424(9.8168) | Error 0.0378(0.0378) Steps 1048(1064.45) | Grad Norm 3.9330(4.2058) | Total Time 0.00(0.00)\n",
      "Iter 17210 | Time 27.0031(27.4380) | Bit/dim 3.5840(3.5816) | Xent 0.1057(0.1113) | Loss 9.3220(9.7101) | Error 0.0367(0.0372) Steps 1066(1065.22) | Grad Norm 6.0030(4.3820) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0313 | Time 121.3461, Epoch Time 1650.3288(1596.3662), Bit/dim 3.5888(best: 3.5709), Xent 1.0034, Loss 4.0905, Error 0.2241(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17220 | Time 27.1760(27.4607) | Bit/dim 3.5642(3.5799) | Xent 0.1061(0.1138) | Loss 9.3973(10.6579) | Error 0.0367(0.0383) Steps 1054(1060.57) | Grad Norm 3.8671(4.7297) | Total Time 0.00(0.00)\n",
      "Iter 17230 | Time 27.3263(27.3862) | Bit/dim 3.5683(3.5793) | Xent 0.0982(0.1121) | Loss 9.5162(10.3372) | Error 0.0311(0.0375) Steps 1102(1061.11) | Grad Norm 6.2032(4.7000) | Total Time 0.00(0.00)\n",
      "Iter 17240 | Time 27.2876(27.3366) | Bit/dim 3.6051(3.5801) | Xent 0.0938(0.1108) | Loss 9.4799(10.0984) | Error 0.0322(0.0372) Steps 1090(1062.54) | Grad Norm 2.8282(6.1793) | Total Time 0.00(0.00)\n",
      "Iter 17250 | Time 27.8058(27.5166) | Bit/dim 3.5719(3.5816) | Xent 0.1055(0.1094) | Loss 9.4408(9.9373) | Error 0.0289(0.0370) Steps 1066(1068.85) | Grad Norm 5.3246(5.7517) | Total Time 0.00(0.00)\n",
      "Iter 17260 | Time 27.7161(27.6330) | Bit/dim 3.5761(3.5816) | Xent 0.1247(0.1094) | Loss 9.5455(9.8085) | Error 0.0400(0.0374) Steps 1060(1066.47) | Grad Norm 4.9759(5.4722) | Total Time 0.00(0.00)\n",
      "Iter 17270 | Time 28.7709(27.7205) | Bit/dim 3.5856(3.5817) | Xent 0.1288(0.1108) | Loss 9.5075(9.7183) | Error 0.0500(0.0386) Steps 1024(1069.11) | Grad Norm 3.8772(5.4362) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0314 | Time 122.4845, Epoch Time 1661.4411(1598.3184), Bit/dim 3.5922(best: 3.5709), Xent 1.0084, Loss 4.0964, Error 0.2254(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17280 | Time 27.2076(27.7004) | Bit/dim 3.5795(3.5817) | Xent 0.1325(0.1127) | Loss 9.4779(10.4962) | Error 0.0522(0.0391) Steps 1048(1071.26) | Grad Norm 4.2489(5.2005) | Total Time 0.00(0.00)\n",
      "Iter 17290 | Time 28.1945(27.7510) | Bit/dim 3.5669(3.5814) | Xent 0.1093(0.1134) | Loss 9.4649(10.2265) | Error 0.0400(0.0399) Steps 1078(1071.19) | Grad Norm 4.3650(5.1958) | Total Time 0.00(0.00)\n",
      "Iter 17300 | Time 28.1581(27.8757) | Bit/dim 3.5799(3.5829) | Xent 0.1294(0.1139) | Loss 9.4778(10.0242) | Error 0.0456(0.0400) Steps 1072(1071.42) | Grad Norm 3.5396(5.1946) | Total Time 0.00(0.00)\n",
      "Iter 17310 | Time 27.6284(27.8978) | Bit/dim 3.5909(3.5866) | Xent 0.1195(0.1144) | Loss 9.3199(9.8712) | Error 0.0467(0.0403) Steps 1090(1071.42) | Grad Norm 3.2531(5.6688) | Total Time 0.00(0.00)\n",
      "Iter 17320 | Time 28.9221(28.0292) | Bit/dim 3.5738(3.5840) | Xent 0.1293(0.1156) | Loss 9.6123(9.7694) | Error 0.0500(0.0409) Steps 1108(1075.62) | Grad Norm 6.7918(5.5591) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0315 | Time 126.1321, Epoch Time 1687.4528(1600.9924), Bit/dim 3.6012(best: 3.5709), Xent 1.0240, Loss 4.1132, Error 0.2259(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17330 | Time 28.6177(28.0746) | Bit/dim 3.5888(3.5874) | Xent 0.1237(0.1153) | Loss 9.5621(10.7328) | Error 0.0500(0.0403) Steps 1126(1079.52) | Grad Norm 7.4997(5.7243) | Total Time 0.00(0.00)\n",
      "Iter 17340 | Time 28.8366(28.1626) | Bit/dim 3.5829(3.5866) | Xent 0.1124(0.1167) | Loss 9.3957(10.4043) | Error 0.0367(0.0402) Steps 1108(1082.89) | Grad Norm 4.4119(5.8610) | Total Time 0.00(0.00)\n",
      "Iter 17350 | Time 29.6797(28.2529) | Bit/dim 3.6172(3.5898) | Xent 0.0948(0.1188) | Loss 9.4165(10.1606) | Error 0.0322(0.0415) Steps 1084(1084.58) | Grad Norm 7.1833(6.4830) | Total Time 0.00(0.00)\n",
      "Iter 17360 | Time 29.3543(28.3882) | Bit/dim 3.6332(3.5940) | Xent 0.1257(0.1187) | Loss 9.6603(9.9900) | Error 0.0422(0.0418) Steps 1066(1088.85) | Grad Norm 10.4320(6.8140) | Total Time 0.00(0.00)\n",
      "Iter 17370 | Time 28.8431(28.5754) | Bit/dim 3.6388(3.5992) | Xent 0.1065(0.1216) | Loss 9.6692(9.8831) | Error 0.0367(0.0428) Steps 1144(1096.26) | Grad Norm 20.6216(8.3892) | Total Time 0.00(0.00)\n",
      "Iter 17380 | Time 29.5822(28.7060) | Bit/dim 3.5913(3.6020) | Xent 0.1336(0.1222) | Loss 9.6085(9.7945) | Error 0.0456(0.0430) Steps 1108(1098.56) | Grad Norm 4.7917(9.1484) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0316 | Time 129.0760, Epoch Time 1726.1270(1604.7465), Bit/dim 3.6173(best: 3.5709), Xent 1.0329, Loss 4.1337, Error 0.2295(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17390 | Time 28.3341(28.7600) | Bit/dim 3.6048(3.6031) | Xent 0.0914(0.1211) | Loss 9.4916(10.6110) | Error 0.0267(0.0422) Steps 1102(1102.56) | Grad Norm 5.6121(9.5873) | Total Time 0.00(0.00)\n",
      "Iter 17400 | Time 29.0084(28.8537) | Bit/dim 3.6303(3.6056) | Xent 0.1707(0.1246) | Loss 9.6754(10.3319) | Error 0.0556(0.0431) Steps 1120(1105.83) | Grad Norm 14.4331(10.1698) | Total Time 0.00(0.00)\n",
      "Iter 17410 | Time 29.9831(29.0449) | Bit/dim 3.6628(3.6105) | Xent 0.1065(0.1271) | Loss 9.6709(10.1417) | Error 0.0422(0.0441) Steps 1108(1111.50) | Grad Norm 6.8683(9.8966) | Total Time 0.00(0.00)\n",
      "Iter 17420 | Time 30.6552(29.3553) | Bit/dim 3.6683(3.6149) | Xent 0.1375(0.1305) | Loss 9.7494(10.0033) | Error 0.0433(0.0454) Steps 1180(1116.41) | Grad Norm 5.7135(11.2043) | Total Time 0.00(0.00)\n",
      "Iter 17430 | Time 30.7609(29.6215) | Bit/dim 3.6413(3.6175) | Xent 0.1283(0.1336) | Loss 9.7858(9.8975) | Error 0.0456(0.0465) Steps 1144(1122.25) | Grad Norm 9.1710(13.2248) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0317 | Time 130.8327, Epoch Time 1782.3734(1610.0753), Bit/dim 3.6461(best: 3.5709), Xent 1.0174, Loss 4.1548, Error 0.2301(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17440 | Time 29.6544(29.9090) | Bit/dim 3.6417(3.6208) | Xent 0.1621(0.1339) | Loss 9.6556(10.8326) | Error 0.0511(0.0464) Steps 1114(1127.97) | Grad Norm 28.0790(17.0535) | Total Time 0.00(0.00)\n",
      "Iter 17450 | Time 33.8656(30.6206) | Bit/dim 3.8692(3.6584) | Xent 0.5122(0.1722) | Loss 10.5273(10.6278) | Error 0.1322(0.0573) Steps 1222(1139.29) | Grad Norm 188.1957(31.4195) | Total Time 0.00(0.00)\n",
      "Iter 17460 | Time 34.8948(32.2042) | Bit/dim 4.0599(3.8080) | Xent 0.7111(0.3626) | Loss 11.1693(10.9194) | Error 0.2044(0.1018) Steps 1216(1175.65) | Grad Norm 122.6564(83.0964) | Total Time 0.00(0.00)\n",
      "Iter 17470 | Time 33.8959(32.7351) | Bit/dim 3.8645(3.8454) | Xent 0.4624(0.4114) | Loss 10.4053(10.8744) | Error 0.1544(0.1195) Steps 1192(1184.82) | Grad Norm 76.4061(85.0079) | Total Time 0.00(0.00)\n",
      "Iter 17480 | Time 36.9334(33.3143) | Bit/dim 4.1342(3.8727) | Xent 0.7217(0.4403) | Loss 11.4485(10.8240) | Error 0.1878(0.1284) Steps 1288(1188.27) | Grad Norm 57.9324(93.1239) | Total Time 0.00(0.00)\n",
      "Iter 17490 | Time 36.2241(34.0908) | Bit/dim 4.0114(3.9874) | Xent 0.5959(0.5812) | Loss 10.9454(11.1562) | Error 0.1656(0.1492) Steps 1210(1208.43) | Grad Norm 64.3845(137.3021) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0318 | Time 137.4630, Epoch Time 2056.7653(1623.4760), Bit/dim 4.0217(best: 3.5709), Xent 1.0461, Loss 4.5448, Error 0.2749(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17500 | Time 31.4810(33.9041) | Bit/dim 3.7427(3.9559) | Xent 0.4029(0.5411) | Loss 10.0085(11.8602) | Error 0.1289(0.1449) Steps 1096(1200.22) | Grad Norm 10.6423(140.8367) | Total Time 0.00(0.00)\n",
      "Iter 17510 | Time 27.3239(32.4598) | Bit/dim 3.7321(3.8967) | Xent 0.2422(0.4802) | Loss 9.8265(11.3336) | Error 0.0967(0.1355) Steps 1054(1169.92) | Grad Norm 5.1279(105.5500) | Total Time 0.00(0.00)\n",
      "Iter 17520 | Time 25.8969(30.8402) | Bit/dim 3.6590(3.8433) | Xent 0.2306(0.4216) | Loss 9.5726(10.9105) | Error 0.0744(0.1230) Steps 1060(1135.62) | Grad Norm 4.4964(79.2637) | Total Time 0.00(0.00)\n",
      "Iter 17530 | Time 26.8933(29.8647) | Bit/dim 3.6799(3.7968) | Xent 0.1853(0.3655) | Loss 9.6309(10.5745) | Error 0.0633(0.1103) Steps 1048(1110.39) | Grad Norm 3.6401(59.6428) | Total Time 0.00(0.00)\n",
      "Iter 17540 | Time 26.7475(29.1860) | Bit/dim 3.6519(3.7595) | Xent 0.2050(0.3212) | Loss 9.5649(10.3272) | Error 0.0778(0.1002) Steps 1054(1097.32) | Grad Norm 6.2673(45.4804) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0319 | Time 118.3688, Epoch Time 1697.7042(1625.7028), Bit/dim 3.6451(best: 3.5709), Xent 0.9077, Loss 4.0989, Error 0.2273(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17550 | Time 26.7198(28.5516) | Bit/dim 3.6258(3.7275) | Xent 0.1571(0.2828) | Loss 9.5275(11.0927) | Error 0.0589(0.0897) Steps 1066(1085.94) | Grad Norm 3.7884(35.1593) | Total Time 0.00(0.00)\n",
      "Iter 17560 | Time 26.8024(28.1191) | Bit/dim 3.6439(3.7033) | Xent 0.1295(0.2510) | Loss 9.4235(10.6797) | Error 0.0489(0.0810) Steps 1060(1079.48) | Grad Norm 4.4832(27.1597) | Total Time 0.00(0.00)\n",
      "Iter 17570 | Time 27.2425(27.7746) | Bit/dim 3.6016(3.6802) | Xent 0.1297(0.2246) | Loss 9.4709(10.3560) | Error 0.0489(0.0737) Steps 1054(1072.44) | Grad Norm 3.5273(21.6191) | Total Time 0.00(0.00)\n",
      "Iter 17580 | Time 27.2278(27.6758) | Bit/dim 3.6098(3.6649) | Xent 0.1391(0.2052) | Loss 9.4666(10.1383) | Error 0.0478(0.0690) Steps 1066(1070.75) | Grad Norm 3.1528(17.2081) | Total Time 0.00(0.00)\n",
      "Iter 17590 | Time 26.5374(27.5305) | Bit/dim 3.5873(3.6475) | Xent 0.1651(0.1889) | Loss 9.4809(9.9520) | Error 0.0567(0.0640) Steps 1072(1067.61) | Grad Norm 2.8687(13.6655) | Total Time 0.00(0.00)\n",
      "Iter 17600 | Time 27.8122(27.6032) | Bit/dim 3.6180(3.6387) | Xent 0.1357(0.1765) | Loss 9.5331(9.8270) | Error 0.0489(0.0601) Steps 1090(1068.97) | Grad Norm 3.6058(11.1729) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0320 | Time 120.2135, Epoch Time 1632.3926(1625.9035), Bit/dim 3.6163(best: 3.5709), Xent 0.9346, Loss 4.0837, Error 0.2230(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17610 | Time 27.0744(27.6993) | Bit/dim 3.5995(3.6325) | Xent 0.1094(0.1637) | Loss 9.3512(10.5809) | Error 0.0444(0.0559) Steps 1072(1067.19) | Grad Norm 3.4775(9.3366) | Total Time 0.00(0.00)\n",
      "Iter 17620 | Time 27.5047(27.7082) | Bit/dim 3.6082(3.6248) | Xent 0.1155(0.1551) | Loss 9.5010(10.2977) | Error 0.0411(0.0539) Steps 1072(1067.27) | Grad Norm 5.2127(8.1581) | Total Time 0.00(0.00)\n",
      "Iter 17630 | Time 27.3846(27.8056) | Bit/dim 3.5978(3.6208) | Xent 0.1068(0.1497) | Loss 9.1966(10.0754) | Error 0.0300(0.0519) Steps 1048(1067.04) | Grad Norm 11.3279(7.4472) | Total Time 0.00(0.00)\n",
      "Iter 17640 | Time 28.2882(27.9151) | Bit/dim 3.6282(3.6154) | Xent 0.1113(0.1435) | Loss 9.4074(9.9077) | Error 0.0456(0.0499) Steps 1120(1070.46) | Grad Norm 2.8595(6.5218) | Total Time 0.00(0.00)\n",
      "Iter 17650 | Time 27.1614(28.0224) | Bit/dim 3.5626(3.6122) | Xent 0.1307(0.1403) | Loss 9.2287(9.7834) | Error 0.0456(0.0489) Steps 1066(1067.23) | Grad Norm 3.3140(5.9148) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0321 | Time 121.7377, Epoch Time 1688.5146(1627.7819), Bit/dim 3.6077(best: 3.5709), Xent 0.9572, Loss 4.0863, Error 0.2223(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17660 | Time 29.7016(28.1568) | Bit/dim 3.5933(3.6084) | Xent 0.1286(0.1367) | Loss 9.4494(10.6793) | Error 0.0456(0.0476) Steps 1090(1070.70) | Grad Norm 3.3959(5.5926) | Total Time 0.00(0.00)\n",
      "Iter 17670 | Time 27.5737(28.1136) | Bit/dim 3.6036(3.6079) | Xent 0.1056(0.1317) | Loss 9.4264(10.3555) | Error 0.0344(0.0455) Steps 1078(1073.44) | Grad Norm 4.2813(5.2503) | Total Time 0.00(0.00)\n",
      "Iter 17680 | Time 28.7345(28.2336) | Bit/dim 3.5977(3.6051) | Xent 0.1392(0.1324) | Loss 9.4292(10.1094) | Error 0.0511(0.0465) Steps 1060(1074.94) | Grad Norm 7.2107(5.1885) | Total Time 0.00(0.00)\n",
      "Iter 17690 | Time 28.6217(28.3876) | Bit/dim 3.5948(3.6023) | Xent 0.1380(0.1325) | Loss 9.4777(9.9397) | Error 0.0533(0.0466) Steps 1078(1076.94) | Grad Norm 6.0208(5.1845) | Total Time 0.00(0.00)\n",
      "Iter 17700 | Time 28.1478(28.4041) | Bit/dim 3.5876(3.5983) | Xent 0.1416(0.1312) | Loss 9.4915(9.7968) | Error 0.0422(0.0458) Steps 1090(1078.29) | Grad Norm 5.1950(5.3107) | Total Time 0.00(0.00)\n",
      "Iter 17710 | Time 28.5829(28.5146) | Bit/dim 3.6015(3.5965) | Xent 0.0921(0.1286) | Loss 9.3737(9.7121) | Error 0.0333(0.0445) Steps 1036(1078.67) | Grad Norm 3.8318(5.2032) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0322 | Time 121.6604, Epoch Time 1708.5103(1630.2037), Bit/dim 3.6062(best: 3.5709), Xent 0.9652, Loss 4.0888, Error 0.2233(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17720 | Time 28.8403(28.5738) | Bit/dim 3.5660(3.5967) | Xent 0.1044(0.1267) | Loss 9.3973(10.4955) | Error 0.0289(0.0443) Steps 1108(1082.79) | Grad Norm 3.5157(4.8596) | Total Time 0.00(0.00)\n",
      "Iter 17730 | Time 29.6456(28.6373) | Bit/dim 3.5892(3.5966) | Xent 0.0991(0.1243) | Loss 9.4913(10.2262) | Error 0.0333(0.0438) Steps 1096(1082.10) | Grad Norm 4.5708(4.6750) | Total Time 0.00(0.00)\n",
      "Iter 17740 | Time 28.5619(28.7646) | Bit/dim 3.5707(3.5953) | Xent 0.1212(0.1223) | Loss 9.3904(10.0279) | Error 0.0456(0.0436) Steps 1090(1085.97) | Grad Norm 5.6918(4.4763) | Total Time 0.00(0.00)\n",
      "Iter 17750 | Time 28.7909(28.8262) | Bit/dim 3.5843(3.5954) | Xent 0.1145(0.1219) | Loss 9.3382(9.8748) | Error 0.0344(0.0432) Steps 1102(1087.04) | Grad Norm 3.4859(4.6884) | Total Time 0.00(0.00)\n",
      "Iter 17760 | Time 28.7219(28.8004) | Bit/dim 3.5901(3.5938) | Xent 0.1388(0.1217) | Loss 9.5510(9.7493) | Error 0.0500(0.0432) Steps 1090(1087.56) | Grad Norm 3.3317(4.6915) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0323 | Time 120.4798, Epoch Time 1725.8969(1633.0745), Bit/dim 3.6022(best: 3.5709), Xent 0.9974, Loss 4.1009, Error 0.2243(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17770 | Time 29.6821(28.7973) | Bit/dim 3.5764(3.5923) | Xent 0.0938(0.1179) | Loss 9.4187(10.6828) | Error 0.0300(0.0414) Steps 1114(1089.79) | Grad Norm 3.2080(4.6088) | Total Time 0.00(0.00)\n",
      "Iter 17780 | Time 29.0809(28.8288) | Bit/dim 3.5785(3.5910) | Xent 0.1444(0.1168) | Loss 9.2858(10.3460) | Error 0.0522(0.0407) Steps 1054(1085.31) | Grad Norm 3.8991(4.3836) | Total Time 0.00(0.00)\n",
      "Iter 17790 | Time 27.8260(28.7706) | Bit/dim 3.5739(3.5887) | Xent 0.0981(0.1166) | Loss 9.2956(10.1026) | Error 0.0333(0.0410) Steps 1078(1085.33) | Grad Norm 3.2495(4.5996) | Total Time 0.00(0.00)\n",
      "Iter 17800 | Time 28.6681(28.7110) | Bit/dim 3.5806(3.5884) | Xent 0.1427(0.1163) | Loss 9.5453(9.9282) | Error 0.0544(0.0411) Steps 1084(1084.32) | Grad Norm 3.9599(4.5418) | Total Time 0.00(0.00)\n",
      "Iter 17810 | Time 27.5658(28.7837) | Bit/dim 3.6263(3.5891) | Xent 0.1102(0.1144) | Loss 9.4925(9.8010) | Error 0.0344(0.0402) Steps 1066(1084.46) | Grad Norm 3.3763(5.0098) | Total Time 0.00(0.00)\n",
      "Iter 17820 | Time 28.4385(28.8008) | Bit/dim 3.5889(3.5904) | Xent 0.1037(0.1159) | Loss 9.5392(9.7133) | Error 0.0367(0.0410) Steps 1120(1085.74) | Grad Norm 3.1103(5.0307) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0324 | Time 122.3046, Epoch Time 1724.9133(1635.8297), Bit/dim 3.5958(best: 3.5709), Xent 0.9894, Loss 4.0905, Error 0.2260(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17830 | Time 29.5296(28.8375) | Bit/dim 3.6403(3.5926) | Xent 0.1015(0.1141) | Loss 9.5812(10.4918) | Error 0.0311(0.0400) Steps 1108(1085.93) | Grad Norm 3.3956(4.9362) | Total Time 0.00(0.00)\n",
      "Iter 17840 | Time 27.6463(28.7368) | Bit/dim 3.5610(3.5926) | Xent 0.1174(0.1136) | Loss 9.3637(10.2099) | Error 0.0367(0.0399) Steps 1066(1085.64) | Grad Norm 4.8770(5.0875) | Total Time 0.00(0.00)\n",
      "Iter 17850 | Time 29.1219(28.8031) | Bit/dim 3.6093(3.5935) | Xent 0.1082(0.1145) | Loss 9.3518(10.0038) | Error 0.0322(0.0396) Steps 1054(1085.11) | Grad Norm 5.8883(6.3681) | Total Time 0.00(0.00)\n",
      "Iter 17860 | Time 28.2423(28.7795) | Bit/dim 3.5609(3.5897) | Xent 0.1045(0.1133) | Loss 9.3351(9.8500) | Error 0.0389(0.0388) Steps 1090(1086.42) | Grad Norm 4.7230(5.7593) | Total Time 0.00(0.00)\n",
      "Iter 17870 | Time 29.3972(28.7384) | Bit/dim 3.5616(3.5847) | Xent 0.1013(0.1114) | Loss 9.4166(9.7233) | Error 0.0289(0.0379) Steps 1078(1083.03) | Grad Norm 10.2478(5.5548) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0325 | Time 123.8667, Epoch Time 1726.8206(1638.5594), Bit/dim 3.5940(best: 3.5709), Xent 1.0265, Loss 4.1072, Error 0.2274(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17880 | Time 28.8340(28.9223) | Bit/dim 3.5839(3.5844) | Xent 0.1135(0.1133) | Loss 9.3810(10.6586) | Error 0.0411(0.0388) Steps 1120(1086.23) | Grad Norm 4.3614(5.6737) | Total Time 0.00(0.00)\n",
      "Iter 17890 | Time 29.2118(28.9953) | Bit/dim 3.6066(3.5840) | Xent 0.0975(0.1135) | Loss 9.5372(10.3422) | Error 0.0289(0.0388) Steps 1090(1087.23) | Grad Norm 3.1958(5.7334) | Total Time 0.00(0.00)\n",
      "Iter 17900 | Time 30.0072(29.1625) | Bit/dim 3.6033(3.5866) | Xent 0.1100(0.1128) | Loss 9.4195(10.1092) | Error 0.0322(0.0382) Steps 1048(1087.01) | Grad Norm 5.1632(5.7001) | Total Time 0.00(0.00)\n",
      "Iter 17910 | Time 29.2022(29.2055) | Bit/dim 3.6147(3.5910) | Xent 0.1150(0.1158) | Loss 9.4900(9.9487) | Error 0.0400(0.0396) Steps 1108(1090.04) | Grad Norm 7.5406(9.1509) | Total Time 0.00(0.00)\n",
      "Iter 17920 | Time 30.2979(29.3978) | Bit/dim 3.6180(3.5954) | Xent 0.1237(0.1180) | Loss 9.5465(9.8314) | Error 0.0444(0.0406) Steps 1126(1090.59) | Grad Norm 16.9597(9.7187) | Total Time 0.00(0.00)\n",
      "Iter 17930 | Time 31.8614(29.5639) | Bit/dim 3.5895(3.5971) | Xent 0.1506(0.1222) | Loss 9.6026(9.7478) | Error 0.0511(0.0421) Steps 1132(1093.72) | Grad Norm 5.6288(11.0426) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0326 | Time 123.7614, Epoch Time 1770.4371(1642.5157), Bit/dim 3.6134(best: 3.5709), Xent 0.9955, Loss 4.1111, Error 0.2250(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17940 | Time 30.0198(29.6079) | Bit/dim 3.5980(3.5985) | Xent 0.1202(0.1210) | Loss 9.5290(10.5388) | Error 0.0389(0.0420) Steps 1126(1097.20) | Grad Norm 7.3013(10.5325) | Total Time 0.00(0.00)\n",
      "Iter 17950 | Time 29.5991(29.6522) | Bit/dim 3.6119(3.6001) | Xent 0.1435(0.1225) | Loss 9.5776(10.2707) | Error 0.0544(0.0433) Steps 1102(1101.62) | Grad Norm 7.2558(10.0732) | Total Time 0.00(0.00)\n",
      "Iter 17960 | Time 30.2298(29.6385) | Bit/dim 3.5949(3.5993) | Xent 0.1324(0.1232) | Loss 9.5733(10.0784) | Error 0.0433(0.0431) Steps 1138(1102.38) | Grad Norm 6.0189(9.7926) | Total Time 0.00(0.00)\n",
      "Iter 17970 | Time 28.8627(29.3851) | Bit/dim 3.6014(3.5970) | Xent 0.1226(0.1224) | Loss 9.5362(9.9168) | Error 0.0378(0.0423) Steps 1120(1101.22) | Grad Norm 5.0063(9.4426) | Total Time 0.00(0.00)\n",
      "Iter 17980 | Time 28.0178(29.4577) | Bit/dim 3.5730(3.5956) | Xent 0.0813(0.1208) | Loss 9.4057(9.8138) | Error 0.0311(0.0418) Steps 1072(1098.59) | Grad Norm 35.1468(9.7296) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0327 | Time 121.4107, Epoch Time 1760.0572(1646.0420), Bit/dim 3.6022(best: 3.5709), Xent 0.9902, Loss 4.0973, Error 0.2221(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17990 | Time 29.1673(29.3387) | Bit/dim 3.5898(3.5958) | Xent 0.1312(0.1200) | Loss 9.5609(10.7309) | Error 0.0444(0.0416) Steps 1090(1098.02) | Grad Norm 7.4115(8.6122) | Total Time 0.00(0.00)\n",
      "Iter 18000 | Time 29.4736(29.4169) | Bit/dim 3.5767(3.5945) | Xent 0.1059(0.1171) | Loss 9.4429(10.3951) | Error 0.0400(0.0408) Steps 1066(1095.77) | Grad Norm 4.8423(10.0830) | Total Time 0.00(0.00)\n",
      "Iter 18010 | Time 30.3335(29.5597) | Bit/dim 3.5641(3.5956) | Xent 0.1333(0.1169) | Loss 9.4897(10.1653) | Error 0.0422(0.0404) Steps 1132(1096.75) | Grad Norm 12.7103(10.5597) | Total Time 0.00(0.00)\n",
      "Iter 18020 | Time 28.8185(29.5101) | Bit/dim 3.5619(3.5940) | Xent 0.1081(0.1161) | Loss 9.3060(9.9739) | Error 0.0400(0.0405) Steps 1096(1101.92) | Grad Norm 18.3133(9.4944) | Total Time 0.00(0.00)\n",
      "Iter 18030 | Time 29.6994(29.4416) | Bit/dim 3.5937(3.5899) | Xent 0.0997(0.1139) | Loss 9.5680(9.8307) | Error 0.0378(0.0397) Steps 1090(1099.53) | Grad Norm 4.0232(8.4457) | Total Time 0.00(0.00)\n",
      "Iter 18040 | Time 28.4060(29.3932) | Bit/dim 3.5719(3.5924) | Xent 0.1033(0.1137) | Loss 9.4188(9.7385) | Error 0.0400(0.0397) Steps 1084(1101.56) | Grad Norm 96.7218(11.2295) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0328 | Time 123.3582, Epoch Time 1761.6432(1649.5100), Bit/dim 3.5980(best: 3.5709), Xent 0.9900, Loss 4.0930, Error 0.2241(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18050 | Time 27.8828(29.3391) | Bit/dim 3.6139(3.5951) | Xent 0.1091(0.1152) | Loss 9.2893(10.5307) | Error 0.0411(0.0405) Steps 1090(1101.45) | Grad Norm 8.0483(11.2008) | Total Time 0.00(0.00)\n",
      "Iter 18060 | Time 29.9376(29.4095) | Bit/dim 3.5787(3.5959) | Xent 0.1432(0.1159) | Loss 9.5504(10.2587) | Error 0.0556(0.0404) Steps 1072(1102.17) | Grad Norm 81.1633(12.7258) | Total Time 0.00(0.00)\n",
      "Iter 18070 | Time 30.3691(29.4308) | Bit/dim 3.6105(3.5979) | Xent 0.0998(0.1179) | Loss 9.4473(10.0479) | Error 0.0389(0.0408) Steps 1072(1101.62) | Grad Norm 6.6728(12.7074) | Total Time 0.00(0.00)\n",
      "Iter 18080 | Time 29.4771(29.5787) | Bit/dim 3.5816(3.6003) | Xent 0.1426(0.1214) | Loss 9.4805(9.9004) | Error 0.0489(0.0416) Steps 1102(1107.05) | Grad Norm 10.1402(12.3072) | Total Time 0.00(0.00)\n",
      "Iter 18090 | Time 29.4269(29.6459) | Bit/dim 3.6026(3.6040) | Xent 0.1189(0.1236) | Loss 9.4639(9.8079) | Error 0.0378(0.0428) Steps 1090(1109.13) | Grad Norm 7.8427(11.2533) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0329 | Time 123.6429, Epoch Time 1771.3940(1653.1665), Bit/dim 3.6160(best: 3.5709), Xent 1.0431, Loss 4.1376, Error 0.2286(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18100 | Time 30.2409(29.7323) | Bit/dim 3.5982(3.6045) | Xent 0.1198(0.1198) | Loss 9.6284(10.7371) | Error 0.0378(0.0416) Steps 1102(1113.06) | Grad Norm 19.1826(12.5794) | Total Time 0.00(0.00)\n",
      "Iter 18110 | Time 31.0783(29.9581) | Bit/dim 3.5937(3.6032) | Xent 0.1259(0.1227) | Loss 9.5485(10.4185) | Error 0.0444(0.0425) Steps 1114(1113.29) | Grad Norm 16.6809(11.6889) | Total Time 0.00(0.00)\n",
      "Iter 18120 | Time 30.6250(30.0421) | Bit/dim 3.6066(3.6089) | Xent 0.1627(0.1259) | Loss 9.6885(10.2034) | Error 0.0533(0.0434) Steps 1132(1116.58) | Grad Norm 32.4939(15.9772) | Total Time 0.00(0.00)\n",
      "Iter 18130 | Time 31.5353(30.3265) | Bit/dim 3.6355(3.6153) | Xent 0.1624(0.1315) | Loss 9.7354(10.0511) | Error 0.0656(0.0458) Steps 1132(1123.46) | Grad Norm 73.9329(24.3972) | Total Time 0.00(0.00)\n",
      "Iter 18140 | Time 31.1440(30.6289) | Bit/dim 3.6383(3.6227) | Xent 0.1807(0.1427) | Loss 9.8270(9.9483) | Error 0.0600(0.0489) Steps 1204(1130.24) | Grad Norm 25.6437(27.4279) | Total Time 0.00(0.00)\n",
      "Iter 18150 | Time 31.3211(30.8510) | Bit/dim 3.6566(3.6282) | Xent 0.1739(0.1451) | Loss 9.6141(9.8685) | Error 0.0556(0.0495) Steps 1144(1137.38) | Grad Norm 153.5069(34.8154) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0330 | Time 126.9982, Epoch Time 1845.1923(1658.9273), Bit/dim 3.6543(best: 3.5709), Xent 1.0051, Loss 4.1569, Error 0.2277(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18160 | Time 34.7587(31.3662) | Bit/dim 3.7789(3.6460) | Xent 0.2866(0.1612) | Loss 10.1960(10.7230) | Error 0.0844(0.0538) Steps 1174(1148.34) | Grad Norm 80.4820(53.9388) | Total Time 0.00(0.00)\n",
      "Iter 18170 | Time 40.2527(32.9590) | Bit/dim 7.2034(3.9489) | Xent 5.4801(0.5778) | Loss 23.4472(11.5981) | Error 0.5856(0.1130) Steps 1444(1194.54) | Grad Norm 4140.5207(251.6646) | Total Time 0.00(0.00)\n",
      "Iter 18180 | Time 31.2234(33.2905) | Bit/dim 4.0085(4.1174) | Xent 0.8559(0.8353) | Loss 10.8781(12.0116) | Error 0.2867(0.1763) Steps 1198(1208.07) | Grad Norm 24.8236(263.5090) | Total Time 0.00(0.00)\n",
      "Iter 18190 | Time 29.3304(32.5607) | Bit/dim 3.8013(4.0532) | Xent 0.4301(0.7611) | Loss 10.0641(11.5847) | Error 0.1500(0.1781) Steps 1060(1186.49) | Grad Norm 11.7235(199.2893) | Total Time 0.00(0.00)\n",
      "Iter 18200 | Time 27.8501(31.5374) | Bit/dim 3.7492(3.9730) | Xent 0.3878(0.6686) | Loss 9.7739(11.1486) | Error 0.1433(0.1698) Steps 1072(1154.64) | Grad Norm 7.4238(149.4747) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0331 | Time 118.9751, Epoch Time 1915.6480(1666.6289), Bit/dim 3.7047(best: 3.5709), Xent 0.8560, Loss 4.1327, Error 0.2328(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18210 | Time 28.3891(30.6578) | Bit/dim 3.6777(3.8991) | Xent 0.2327(0.5689) | Loss 9.6162(11.7226) | Error 0.0811(0.1514) Steps 1108(1133.00) | Grad Norm 8.1785(111.9491) | Total Time 0.00(0.00)\n",
      "Iter 18220 | Time 29.1420(30.1830) | Bit/dim 3.6568(3.8404) | Xent 0.2147(0.4827) | Loss 9.4920(11.1766) | Error 0.0811(0.1345) Steps 1078(1117.42) | Grad Norm 5.7907(84.1858) | Total Time 0.00(0.00)\n",
      "Iter 18230 | Time 28.6375(29.7687) | Bit/dim 3.6741(3.7929) | Xent 0.2439(0.4122) | Loss 9.6301(10.7654) | Error 0.0756(0.1182) Steps 1072(1108.67) | Grad Norm 5.9675(64.1919) | Total Time 0.00(0.00)\n",
      "Iter 18240 | Time 28.9636(29.5256) | Bit/dim 3.6635(3.7546) | Xent 0.1621(0.3537) | Loss 9.5597(10.4435) | Error 0.0578(0.1048) Steps 1102(1101.56) | Grad Norm 4.1778(49.3238) | Total Time 0.00(0.00)\n",
      "Iter 18250 | Time 29.9429(29.4086) | Bit/dim 3.6099(3.7242) | Xent 0.2058(0.3069) | Loss 9.5654(10.2176) | Error 0.0678(0.0926) Steps 1060(1095.63) | Grad Norm 5.1026(37.7311) | Total Time 0.00(0.00)\n",
      "Iter 18260 | Time 29.3613(29.1879) | Bit/dim 3.6414(3.7052) | Xent 0.1611(0.2722) | Loss 9.6001(10.0543) | Error 0.0622(0.0843) Steps 1102(1092.42) | Grad Norm 36.0034(30.3990) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0332 | Time 120.0986, Epoch Time 1718.2442(1668.1774), Bit/dim 3.6432(best: 3.5709), Xent 0.9129, Loss 4.0997, Error 0.2250(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18270 | Time 28.7009(28.9995) | Bit/dim 3.6532(3.6848) | Xent 0.1613(0.2414) | Loss 9.6439(10.7779) | Error 0.0600(0.0765) Steps 1078(1090.50) | Grad Norm 9.2296(24.2099) | Total Time 0.00(0.00)\n",
      "Iter 18280 | Time 28.8239(28.9206) | Bit/dim 3.6534(3.6707) | Xent 0.1672(0.2207) | Loss 9.6980(10.4563) | Error 0.0556(0.0716) Steps 1102(1089.64) | Grad Norm 4.7175(19.2477) | Total Time 0.00(0.00)\n",
      "Iter 18290 | Time 27.7463(28.8806) | Bit/dim 3.6120(3.6585) | Xent 0.1261(0.2008) | Loss 9.2871(10.1942) | Error 0.0389(0.0660) Steps 1090(1088.02) | Grad Norm 6.4927(18.1925) | Total Time 0.00(0.00)\n",
      "Iter 18300 | Time 29.4556(28.9279) | Bit/dim 3.6133(3.6508) | Xent 0.1420(0.1880) | Loss 9.5439(10.0315) | Error 0.0522(0.0616) Steps 1126(1091.79) | Grad Norm 3.3069(15.6102) | Total Time 0.00(0.00)\n",
      "Iter 18310 | Time 30.0520(28.9985) | Bit/dim 3.6555(3.6440) | Xent 0.1443(0.1791) | Loss 9.4883(9.8823) | Error 0.0511(0.0596) Steps 1084(1092.54) | Grad Norm 6.7148(14.4098) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0333 | Time 123.3308, Epoch Time 1729.8437(1670.0274), Bit/dim 3.6247(best: 3.5709), Xent 0.9444, Loss 4.0969, Error 0.2219(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18320 | Time 29.6430(29.0249) | Bit/dim 3.5920(3.6373) | Xent 0.1607(0.1735) | Loss 9.5334(10.7557) | Error 0.0444(0.0584) Steps 1060(1091.56) | Grad Norm 98.0736(17.4722) | Total Time 0.00(0.00)\n",
      "Iter 18330 | Time 29.8649(29.0182) | Bit/dim 3.6317(3.6326) | Xent 0.1441(0.1660) | Loss 9.4885(10.4180) | Error 0.0556(0.0565) Steps 1138(1094.75) | Grad Norm 42.4582(18.1150) | Total Time 0.00(0.00)\n",
      "Iter 18340 | Time 29.0231(29.0684) | Bit/dim 3.6001(3.6284) | Xent 0.1442(0.1623) | Loss 9.4324(10.1844) | Error 0.0489(0.0551) Steps 1078(1092.91) | Grad Norm 4.5482(16.5866) | Total Time 0.00(0.00)\n",
      "Iter 18350 | Time 28.6857(28.9982) | Bit/dim 3.6141(3.6251) | Xent 0.1640(0.1580) | Loss 9.4835(10.0022) | Error 0.0622(0.0538) Steps 1072(1094.65) | Grad Norm 6.2400(17.1934) | Total Time 0.00(0.00)\n",
      "Iter 18360 | Time 29.9924(29.0115) | Bit/dim 3.5720(3.6171) | Xent 0.1427(0.1520) | Loss 9.4690(9.8606) | Error 0.0522(0.0524) Steps 1084(1089.73) | Grad Norm 7.1090(14.2733) | Total Time 0.00(0.00)\n",
      "Iter 18370 | Time 29.1118(28.9830) | Bit/dim 3.5865(3.6157) | Xent 0.1364(0.1499) | Loss 9.4475(9.7686) | Error 0.0422(0.0520) Steps 1120(1088.68) | Grad Norm 5.0487(14.5041) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0334 | Time 123.7759, Epoch Time 1736.0631(1672.0084), Bit/dim 3.6184(best: 3.5709), Xent 0.9543, Loss 4.0956, Error 0.2248(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18380 | Time 30.9068(29.0396) | Bit/dim 3.6010(3.6152) | Xent 0.1406(0.1454) | Loss 9.5315(10.5840) | Error 0.0533(0.0502) Steps 1096(1089.99) | Grad Norm 6.4462(13.7859) | Total Time 0.00(0.00)\n",
      "Iter 18390 | Time 28.0949(28.9855) | Bit/dim 3.5975(3.6123) | Xent 0.1273(0.1407) | Loss 9.5122(10.2988) | Error 0.0433(0.0482) Steps 1084(1089.78) | Grad Norm 4.4492(12.3882) | Total Time 0.00(0.00)\n",
      "Iter 18400 | Time 29.9570(28.9467) | Bit/dim 3.6106(3.6113) | Xent 0.1319(0.1384) | Loss 9.5339(10.0856) | Error 0.0456(0.0479) Steps 1078(1090.27) | Grad Norm 4.3066(11.5286) | Total Time 0.00(0.00)\n",
      "Iter 18410 | Time 29.1753(28.9596) | Bit/dim 3.5863(3.6076) | Xent 0.1349(0.1375) | Loss 9.4324(9.9309) | Error 0.0511(0.0475) Steps 1066(1091.72) | Grad Norm 5.4285(9.8660) | Total Time 0.00(0.00)\n",
      "Iter 18420 | Time 28.4074(28.8396) | Bit/dim 3.6556(3.6081) | Xent 0.1487(0.1350) | Loss 9.5841(9.8211) | Error 0.0511(0.0467) Steps 1048(1092.20) | Grad Norm 8.1591(10.0982) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0335 | Time 121.8862, Epoch Time 1729.6270(1673.7370), Bit/dim 3.6146(best: 3.5709), Xent 1.0073, Loss 4.1182, Error 0.2258(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18430 | Time 28.3918(28.9170) | Bit/dim 3.6112(3.6075) | Xent 0.1512(0.1341) | Loss 9.4631(10.7299) | Error 0.0500(0.0459) Steps 1072(1093.58) | Grad Norm 4.3046(10.3877) | Total Time 0.00(0.00)\n",
      "Iter 18440 | Time 27.8810(28.8812) | Bit/dim 3.5754(3.6039) | Xent 0.1500(0.1311) | Loss 9.3869(10.3818) | Error 0.0578(0.0453) Steps 1060(1094.84) | Grad Norm 12.6049(9.2268) | Total Time 0.00(0.00)\n",
      "Iter 18450 | Time 28.0766(28.6750) | Bit/dim 3.6200(3.6034) | Xent 0.1224(0.1289) | Loss 9.4531(10.1445) | Error 0.0489(0.0449) Steps 1084(1091.48) | Grad Norm 7.3423(8.5124) | Total Time 0.00(0.00)\n",
      "Iter 18460 | Time 27.9408(28.5550) | Bit/dim 3.5981(3.6010) | Xent 0.1450(0.1298) | Loss 9.4984(9.9581) | Error 0.0478(0.0447) Steps 1042(1088.15) | Grad Norm 11.0171(8.5769) | Total Time 0.00(0.00)\n",
      "Iter 18470 | Time 28.5456(28.4717) | Bit/dim 3.5479(3.5994) | Xent 0.1547(0.1309) | Loss 9.4508(9.8268) | Error 0.0578(0.0457) Steps 1078(1085.09) | Grad Norm 4.5292(8.3594) | Total Time 0.00(0.00)\n",
      "Iter 18480 | Time 27.6788(28.4252) | Bit/dim 3.5430(3.5964) | Xent 0.1283(0.1310) | Loss 9.2676(9.7330) | Error 0.0444(0.0458) Steps 1066(1088.27) | Grad Norm 5.7357(8.4765) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0336 | Time 121.0004, Epoch Time 1701.3709(1674.5660), Bit/dim 3.6060(best: 3.5709), Xent 0.9989, Loss 4.1055, Error 0.2247(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18490 | Time 28.1834(28.3712) | Bit/dim 3.5897(3.5948) | Xent 0.1228(0.1264) | Loss 9.4520(10.5203) | Error 0.0444(0.0439) Steps 1054(1084.41) | Grad Norm 4.5132(7.6227) | Total Time 0.00(0.00)\n",
      "Iter 18500 | Time 27.9065(28.3022) | Bit/dim 3.6218(3.5917) | Xent 0.1222(0.1226) | Loss 9.6044(10.2352) | Error 0.0478(0.0425) Steps 1108(1085.43) | Grad Norm 10.4552(7.5156) | Total Time 0.00(0.00)\n",
      "Iter 18510 | Time 27.4444(28.2788) | Bit/dim 3.6241(3.5973) | Xent 0.1192(0.1224) | Loss 9.5012(10.0410) | Error 0.0389(0.0423) Steps 1090(1082.86) | Grad Norm 53.8178(8.7117) | Total Time 0.00(0.00)\n",
      "Iter 18520 | Time 28.2482(28.1064) | Bit/dim 3.5878(3.5965) | Xent 0.1132(0.1231) | Loss 9.4072(9.8803) | Error 0.0411(0.0426) Steps 1054(1077.90) | Grad Norm 5.4928(8.0265) | Total Time 0.00(0.00)\n",
      "Iter 18530 | Time 26.9353(27.9420) | Bit/dim 3.5588(3.5935) | Xent 0.1292(0.1239) | Loss 9.4027(9.7561) | Error 0.0444(0.0432) Steps 1048(1072.90) | Grad Norm 17.9433(7.7933) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0337 | Time 120.3030, Epoch Time 1674.3024(1674.5581), Bit/dim 3.6002(best: 3.5709), Xent 0.9942, Loss 4.0973, Error 0.2254(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18540 | Time 27.2374(27.9298) | Bit/dim 3.6207(3.5920) | Xent 0.0969(0.1232) | Loss 9.5494(10.6840) | Error 0.0333(0.0427) Steps 1072(1072.34) | Grad Norm 4.2783(7.0909) | Total Time 0.00(0.00)\n",
      "Iter 18550 | Time 26.6836(27.7445) | Bit/dim 3.5943(3.5923) | Xent 0.1071(0.1231) | Loss 9.4898(10.3697) | Error 0.0444(0.0430) Steps 1078(1073.43) | Grad Norm 4.6508(6.6371) | Total Time 0.00(0.00)\n",
      "Iter 18560 | Time 27.4574(27.6545) | Bit/dim 3.6102(3.5924) | Xent 0.1177(0.1218) | Loss 9.4542(10.1217) | Error 0.0478(0.0428) Steps 1066(1069.27) | Grad Norm 3.6100(6.1196) | Total Time 0.00(0.00)\n",
      "Iter 18570 | Time 26.4490(27.6682) | Bit/dim 3.5918(3.5906) | Xent 0.1262(0.1201) | Loss 9.3595(9.9530) | Error 0.0467(0.0421) Steps 1048(1069.33) | Grad Norm 6.3050(5.7600) | Total Time 0.00(0.00)\n",
      "Iter 18580 | Time 26.8060(27.6480) | Bit/dim 3.5798(3.5858) | Xent 0.0947(0.1175) | Loss 9.4110(9.7986) | Error 0.0278(0.0405) Steps 1096(1072.03) | Grad Norm 4.0932(5.3912) | Total Time 0.00(0.00)\n",
      "Iter 18590 | Time 27.5560(27.6598) | Bit/dim 3.5932(3.5880) | Xent 0.1312(0.1185) | Loss 9.4488(9.7049) | Error 0.0467(0.0411) Steps 1102(1074.48) | Grad Norm 7.5822(5.6151) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0338 | Time 120.1590, Epoch Time 1655.0467(1673.9728), Bit/dim 3.5928(best: 3.5709), Xent 1.0089, Loss 4.0973, Error 0.2254(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18600 | Time 26.8635(27.5462) | Bit/dim 3.5619(3.5862) | Xent 0.1085(0.1170) | Loss 9.4276(10.4618) | Error 0.0344(0.0403) Steps 1066(1071.01) | Grad Norm 3.9503(5.7151) | Total Time 0.00(0.00)\n",
      "Iter 18610 | Time 27.4421(27.5767) | Bit/dim 3.5767(3.5854) | Xent 0.1028(0.1147) | Loss 9.4628(10.1955) | Error 0.0367(0.0397) Steps 1054(1068.25) | Grad Norm 4.9390(6.7992) | Total Time 0.00(0.00)\n",
      "Iter 18620 | Time 27.5429(27.6221) | Bit/dim 3.6120(3.5857) | Xent 0.1137(0.1137) | Loss 9.6045(10.0070) | Error 0.0344(0.0394) Steps 1066(1069.78) | Grad Norm 12.0672(7.1308) | Total Time 0.00(0.00)\n",
      "Iter 18630 | Time 27.5042(27.5915) | Bit/dim 3.6226(3.5848) | Xent 0.1252(0.1150) | Loss 9.4738(9.8524) | Error 0.0344(0.0398) Steps 1078(1073.07) | Grad Norm 13.6075(7.9481) | Total Time 0.00(0.00)\n",
      "Iter 18640 | Time 26.9329(27.6014) | Bit/dim 3.5956(3.5854) | Xent 0.0951(0.1148) | Loss 9.4720(9.7331) | Error 0.0378(0.0399) Steps 1024(1071.18) | Grad Norm 3.2258(8.7660) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0339 | Time 118.7276, Epoch Time 1653.6793(1673.3640), Bit/dim 3.5958(best: 3.5709), Xent 1.0007, Loss 4.0962, Error 0.2241(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18650 | Time 27.0518(27.5687) | Bit/dim 3.5752(3.5822) | Xent 0.0957(0.1151) | Loss 9.4089(10.6343) | Error 0.0311(0.0398) Steps 1084(1071.84) | Grad Norm 5.3415(8.2612) | Total Time 0.00(0.00)\n",
      "Iter 18660 | Time 27.6010(27.5694) | Bit/dim 3.5793(3.5835) | Xent 0.1151(0.1151) | Loss 9.4532(10.3298) | Error 0.0400(0.0397) Steps 1090(1071.76) | Grad Norm 4.1366(7.5425) | Total Time 0.00(0.00)\n",
      "Iter 18670 | Time 27.3182(27.5085) | Bit/dim 3.5861(3.5847) | Xent 0.0839(0.1151) | Loss 9.4743(10.0938) | Error 0.0344(0.0407) Steps 1060(1068.01) | Grad Norm 5.9629(7.8631) | Total Time 0.00(0.00)\n",
      "Iter 18680 | Time 27.5036(27.5144) | Bit/dim 3.6123(3.5859) | Xent 0.0973(0.1144) | Loss 9.5089(9.9182) | Error 0.0389(0.0403) Steps 1102(1068.93) | Grad Norm 5.5744(7.5667) | Total Time 0.00(0.00)\n",
      "Iter 18690 | Time 26.4340(27.4767) | Bit/dim 3.5728(3.5840) | Xent 0.0916(0.1138) | Loss 9.2180(9.7780) | Error 0.0300(0.0400) Steps 1060(1072.22) | Grad Norm 5.9543(9.0290) | Total Time 0.00(0.00)\n",
      "Iter 18700 | Time 26.5688(27.5405) | Bit/dim 3.5899(3.5839) | Xent 0.1113(0.1134) | Loss 9.3502(9.6891) | Error 0.0389(0.0401) Steps 1054(1077.59) | Grad Norm 28.7812(10.6321) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0340 | Time 120.6831, Epoch Time 1650.8548(1672.6887), Bit/dim 3.5918(best: 3.5709), Xent 0.9992, Loss 4.0914, Error 0.2233(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18710 | Time 28.7635(27.4883) | Bit/dim 3.6305(3.5866) | Xent 0.1136(0.1117) | Loss 9.5423(10.4873) | Error 0.0389(0.0392) Steps 1054(1074.93) | Grad Norm 23.6472(14.0752) | Total Time 0.00(0.00)\n",
      "Iter 18720 | Time 28.2039(27.5039) | Bit/dim 3.5691(3.5814) | Xent 0.1021(0.1109) | Loss 9.4507(10.1962) | Error 0.0400(0.0396) Steps 1072(1074.93) | Grad Norm 5.7024(12.3679) | Total Time 0.00(0.00)\n",
      "Iter 18730 | Time 27.9542(27.4995) | Bit/dim 3.5803(3.5809) | Xent 0.1401(0.1121) | Loss 9.5081(10.0016) | Error 0.0467(0.0393) Steps 1084(1074.05) | Grad Norm 5.3949(11.5322) | Total Time 0.00(0.00)\n",
      "Iter 18740 | Time 27.1526(27.3691) | Bit/dim 3.5638(3.5810) | Xent 0.0854(0.1106) | Loss 9.4397(9.8383) | Error 0.0333(0.0391) Steps 1066(1070.48) | Grad Norm 3.5430(11.3465) | Total Time 0.00(0.00)\n",
      "Iter 18750 | Time 27.9233(27.3104) | Bit/dim 3.5994(3.5813) | Xent 0.1208(0.1111) | Loss 9.4979(9.7309) | Error 0.0367(0.0389) Steps 1060(1071.06) | Grad Norm 5.8265(10.0623) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0341 | Time 121.9038, Epoch Time 1641.4858(1671.7526), Bit/dim 3.5928(best: 3.5709), Xent 1.0156, Loss 4.1006, Error 0.2265(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18760 | Time 28.2480(27.3231) | Bit/dim 3.5920(3.5823) | Xent 0.1134(0.1102) | Loss 9.4365(10.6521) | Error 0.0422(0.0384) Steps 1072(1070.65) | Grad Norm 12.8351(9.7846) | Total Time 0.00(0.00)\n",
      "Iter 18770 | Time 27.8431(27.3625) | Bit/dim 3.5791(3.5805) | Xent 0.0982(0.1120) | Loss 9.4500(10.3244) | Error 0.0322(0.0390) Steps 1054(1068.70) | Grad Norm 5.7015(10.0522) | Total Time 0.00(0.00)\n",
      "Iter 18780 | Time 27.6344(27.4450) | Bit/dim 3.5859(3.5814) | Xent 0.1293(0.1101) | Loss 9.5110(10.0820) | Error 0.0567(0.0391) Steps 1078(1068.15) | Grad Norm 6.7831(11.0725) | Total Time 0.00(0.00)\n",
      "Iter 18790 | Time 27.2711(27.5292) | Bit/dim 3.6613(3.5852) | Xent 0.1135(0.1109) | Loss 9.5659(9.9182) | Error 0.0444(0.0392) Steps 1078(1069.68) | Grad Norm 5.7132(10.2569) | Total Time 0.00(0.00)\n",
      "Iter 18800 | Time 29.2295(27.6033) | Bit/dim 3.5814(3.5830) | Xent 0.1386(0.1141) | Loss 9.3525(9.7958) | Error 0.0478(0.0400) Steps 1078(1071.20) | Grad Norm 17.6151(9.7659) | Total Time 0.00(0.00)\n",
      "Iter 18810 | Time 28.1801(27.6814) | Bit/dim 3.5802(3.5845) | Xent 0.1251(0.1140) | Loss 9.5682(9.7050) | Error 0.0467(0.0407) Steps 1078(1071.74) | Grad Norm 9.2963(8.9328) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0342 | Time 121.8709, Epoch Time 1663.5045(1671.5052), Bit/dim 3.5928(best: 3.5709), Xent 1.0580, Loss 4.1217, Error 0.2321(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18820 | Time 27.6120(27.6559) | Bit/dim 3.5923(3.5823) | Xent 0.1211(0.1135) | Loss 9.5439(10.4931) | Error 0.0444(0.0405) Steps 1084(1076.67) | Grad Norm 4.0782(8.2996) | Total Time 0.00(0.00)\n",
      "Iter 18830 | Time 27.7208(27.6322) | Bit/dim 3.5734(3.5818) | Xent 0.0996(0.1111) | Loss 9.3199(10.1986) | Error 0.0344(0.0392) Steps 1066(1075.21) | Grad Norm 6.7912(7.7366) | Total Time 0.00(0.00)\n",
      "Iter 18840 | Time 28.5310(27.6727) | Bit/dim 3.5924(3.5826) | Xent 0.1142(0.1109) | Loss 9.3469(10.0003) | Error 0.0444(0.0394) Steps 1036(1076.95) | Grad Norm 8.1261(7.2298) | Total Time 0.00(0.00)\n",
      "Iter 18850 | Time 27.4883(27.6771) | Bit/dim 3.5510(3.5817) | Xent 0.1302(0.1107) | Loss 9.3107(9.8489) | Error 0.0400(0.0393) Steps 1066(1075.60) | Grad Norm 4.6827(7.4460) | Total Time 0.00(0.00)\n",
      "Iter 18860 | Time 27.2232(27.5704) | Bit/dim 3.5911(3.5842) | Xent 0.1492(0.1119) | Loss 9.5264(9.7536) | Error 0.0544(0.0395) Steps 1072(1075.48) | Grad Norm 5.3197(8.0922) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0343 | Time 120.5259, Epoch Time 1653.9574(1670.9787), Bit/dim 3.5920(best: 3.5709), Xent 1.0264, Loss 4.1052, Error 0.2246(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18870 | Time 28.0070(27.5588) | Bit/dim 3.5863(3.5855) | Xent 0.1196(0.1129) | Loss 9.3385(10.6535) | Error 0.0411(0.0395) Steps 1054(1071.34) | Grad Norm 6.0761(7.3297) | Total Time 0.00(0.00)\n",
      "Iter 18880 | Time 27.6468(27.5205) | Bit/dim 3.5944(3.5845) | Xent 0.1003(0.1109) | Loss 9.5227(10.3198) | Error 0.0300(0.0389) Steps 1078(1069.29) | Grad Norm 4.3953(7.4798) | Total Time 0.00(0.00)\n",
      "Iter 18890 | Time 27.0612(27.4672) | Bit/dim 3.5805(3.5831) | Xent 0.1589(0.1136) | Loss 9.5273(10.0776) | Error 0.0589(0.0401) Steps 1048(1067.56) | Grad Norm 9.8368(7.5202) | Total Time 0.00(0.00)\n",
      "Iter 18900 | Time 26.6156(27.4737) | Bit/dim 3.5866(3.5816) | Xent 0.1117(0.1129) | Loss 9.4704(9.8963) | Error 0.0400(0.0396) Steps 1078(1066.36) | Grad Norm 6.4340(8.8221) | Total Time 0.00(0.00)\n",
      "Iter 18910 | Time 27.0789(27.4109) | Bit/dim 3.5967(3.5811) | Xent 0.0800(0.1119) | Loss 9.3705(9.7638) | Error 0.0278(0.0389) Steps 1096(1069.05) | Grad Norm 19.0629(8.7880) | Total Time 0.00(0.00)\n",
      "Iter 18920 | Time 28.4405(27.5502) | Bit/dim 3.5618(3.5796) | Xent 0.1043(0.1102) | Loss 9.5420(9.6730) | Error 0.0378(0.0382) Steps 1096(1070.21) | Grad Norm 15.5226(10.6772) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0344 | Time 120.7913, Epoch Time 1652.4765(1670.4237), Bit/dim 3.5883(best: 3.5709), Xent 1.0224, Loss 4.0995, Error 0.2257(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18930 | Time 27.6039(27.7058) | Bit/dim 3.5819(3.5791) | Xent 0.1036(0.1070) | Loss 9.4539(10.4815) | Error 0.0389(0.0370) Steps 1114(1073.67) | Grad Norm 6.0224(10.8341) | Total Time 0.00(0.00)\n",
      "Iter 18940 | Time 26.9451(27.6673) | Bit/dim 3.5724(3.5775) | Xent 0.1158(0.1073) | Loss 9.3496(10.2043) | Error 0.0356(0.0371) Steps 1066(1073.78) | Grad Norm 5.1812(11.6304) | Total Time 0.00(0.00)\n",
      "Iter 18950 | Time 27.7620(27.7178) | Bit/dim 3.5661(3.5798) | Xent 0.1124(0.1075) | Loss 9.2789(9.9890) | Error 0.0456(0.0373) Steps 1042(1069.95) | Grad Norm 5.9483(10.9495) | Total Time 0.00(0.00)\n",
      "Iter 18960 | Time 26.8620(27.6528) | Bit/dim 3.5674(3.5783) | Xent 0.1188(0.1073) | Loss 9.4512(9.8386) | Error 0.0378(0.0369) Steps 1084(1070.52) | Grad Norm 7.3216(10.4218) | Total Time 0.00(0.00)\n",
      "Iter 18970 | Time 25.9083(27.5859) | Bit/dim 3.5586(3.5774) | Xent 0.0856(0.1079) | Loss 9.2269(9.7219) | Error 0.0267(0.0374) Steps 1072(1070.08) | Grad Norm 3.0305(9.6509) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0345 | Time 121.3870, Epoch Time 1660.0965(1670.1138), Bit/dim 3.5843(best: 3.5709), Xent 1.0355, Loss 4.1021, Error 0.2249(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18980 | Time 27.4480(27.5327) | Bit/dim 3.5979(3.5762) | Xent 0.1239(0.1086) | Loss 9.3417(10.6546) | Error 0.0433(0.0375) Steps 1078(1073.39) | Grad Norm 6.1003(8.6160) | Total Time 0.00(0.00)\n",
      "Iter 18990 | Time 27.6998(27.5277) | Bit/dim 3.5826(3.5757) | Xent 0.1332(0.1080) | Loss 9.4944(10.3361) | Error 0.0489(0.0374) Steps 1108(1071.49) | Grad Norm 12.1745(8.2204) | Total Time 0.00(0.00)\n",
      "Iter 19000 | Time 26.7455(27.6233) | Bit/dim 3.5860(3.5772) | Xent 0.0866(0.1070) | Loss 9.4046(10.0913) | Error 0.0222(0.0373) Steps 1108(1073.21) | Grad Norm 6.4434(7.8966) | Total Time 0.00(0.00)\n",
      "Iter 19010 | Time 29.3233(27.7039) | Bit/dim 3.5800(3.5782) | Xent 0.1046(0.1066) | Loss 9.4100(9.9135) | Error 0.0300(0.0370) Steps 1066(1076.80) | Grad Norm 21.7514(8.3349) | Total Time 0.00(0.00)\n",
      "Iter 19020 | Time 27.8344(27.6634) | Bit/dim 3.5923(3.5748) | Xent 0.0907(0.1074) | Loss 9.4287(9.7650) | Error 0.0244(0.0369) Steps 1060(1075.51) | Grad Norm 7.3528(8.6633) | Total Time 0.00(0.00)\n",
      "Iter 19030 | Time 27.8736(27.7001) | Bit/dim 3.5727(3.5743) | Xent 0.0674(0.1063) | Loss 9.2105(9.6686) | Error 0.0211(0.0366) Steps 1042(1076.83) | Grad Norm 3.8113(8.4203) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0346 | Time 120.0614, Epoch Time 1662.4199(1669.8830), Bit/dim 3.5872(best: 3.5709), Xent 1.0638, Loss 4.1191, Error 0.2261(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 19040 | Time 28.5324(27.7621) | Bit/dim 3.5917(3.5744) | Xent 0.0909(0.1072) | Loss 9.5094(10.4680) | Error 0.0289(0.0366) Steps 1102(1078.62) | Grad Norm 5.0342(9.8783) | Total Time 0.00(0.00)\n",
      "Iter 19050 | Time 26.8941(27.7581) | Bit/dim 3.5730(3.5775) | Xent 0.1357(0.1077) | Loss 9.3674(10.1934) | Error 0.0478(0.0372) Steps 1084(1080.22) | Grad Norm 7.6225(9.1725) | Total Time 0.00(0.00)\n",
      "Iter 19060 | Time 26.9639(27.7195) | Bit/dim 3.5754(3.5750) | Xent 0.1062(0.1055) | Loss 9.3753(9.9812) | Error 0.0367(0.0364) Steps 1066(1079.67) | Grad Norm 10.4000(8.7533) | Total Time 0.00(0.00)\n",
      "Iter 19070 | Time 27.2719(27.6875) | Bit/dim 3.5354(3.5750) | Xent 0.1362(0.1076) | Loss 9.3010(9.8374) | Error 0.0500(0.0370) Steps 1084(1079.63) | Grad Norm 4.4504(9.0331) | Total Time 0.00(0.00)\n",
      "Iter 19080 | Time 27.9088(27.7047) | Bit/dim 3.5639(3.5735) | Xent 0.1087(0.1062) | Loss 9.4230(9.7343) | Error 0.0389(0.0367) Steps 1048(1076.86) | Grad Norm 4.5538(8.1204) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0347 | Time 119.6759, Epoch Time 1661.9477(1669.6450), Bit/dim 3.5828(best: 3.5709), Xent 1.0477, Loss 4.1067, Error 0.2264(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 19090 | Time 27.8207(27.6963) | Bit/dim 3.5558(3.5736) | Xent 0.1112(0.1024) | Loss 9.3159(10.6437) | Error 0.0411(0.0355) Steps 1090(1076.80) | Grad Norm 6.6694(8.4505) | Total Time 0.00(0.00)\n",
      "Iter 19100 | Time 28.0397(27.6565) | Bit/dim 3.5653(3.5716) | Xent 0.0777(0.1013) | Loss 9.4480(10.3206) | Error 0.0311(0.0354) Steps 1102(1079.93) | Grad Norm 5.5419(8.1752) | Total Time 0.00(0.00)\n",
      "Iter 19110 | Time 27.3106(27.4393) | Bit/dim 3.6064(3.5757) | Xent 0.0967(0.1024) | Loss 9.5449(10.0878) | Error 0.0267(0.0360) Steps 1102(1077.93) | Grad Norm 4.7368(7.2571) | Total Time 0.00(0.00)\n",
      "Iter 19120 | Time 26.4526(27.3207) | Bit/dim 3.6098(3.5762) | Xent 0.1234(0.1040) | Loss 9.4575(9.9074) | Error 0.0467(0.0365) Steps 1084(1075.13) | Grad Norm 5.4910(7.0025) | Total Time 0.00(0.00)\n",
      "Iter 19130 | Time 27.1056(27.3921) | Bit/dim 3.5700(3.5781) | Xent 0.1430(0.1067) | Loss 9.4275(9.7793) | Error 0.0456(0.0373) Steps 1078(1071.52) | Grad Norm 3.7805(6.3922) | Total Time 0.00(0.00)\n",
      "Iter 19140 | Time 27.1541(27.3715) | Bit/dim 3.5795(3.5776) | Xent 0.1110(0.1067) | Loss 9.3681(9.6724) | Error 0.0378(0.0370) Steps 1060(1069.29) | Grad Norm 5.4976(6.2087) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0348 | Time 122.3783, Epoch Time 1641.8812(1668.8120), Bit/dim 3.5835(best: 3.5709), Xent 1.0312, Loss 4.0991, Error 0.2283(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 19150 | Time 27.2724(27.3158) | Bit/dim 3.5947(3.5793) | Xent 0.0998(0.1041) | Loss 9.3046(10.4513) | Error 0.0322(0.0361) Steps 1054(1068.18) | Grad Norm 3.4662(5.8929) | Total Time 0.00(0.00)\n",
      "Iter 19160 | Time 28.3853(27.4172) | Bit/dim 3.5781(3.5780) | Xent 0.0933(0.1030) | Loss 9.3008(10.1773) | Error 0.0356(0.0355) Steps 1066(1069.96) | Grad Norm 10.3885(5.8013) | Total Time 0.00(0.00)\n",
      "Iter 19170 | Time 26.6012(27.4827) | Bit/dim 3.5560(3.5750) | Xent 0.1289(0.1037) | Loss 9.4862(9.9747) | Error 0.0467(0.0359) Steps 1054(1072.64) | Grad Norm 3.5692(5.5128) | Total Time 0.00(0.00)\n",
      "Iter 19180 | Time 27.6122(27.4365) | Bit/dim 3.5871(3.5745) | Xent 0.0783(0.1026) | Loss 9.4470(9.8204) | Error 0.0289(0.0355) Steps 1090(1073.53) | Grad Norm 3.8019(5.4377) | Total Time 0.00(0.00)\n",
      "Iter 19190 | Time 27.2513(27.4343) | Bit/dim 3.5601(3.5739) | Xent 0.1154(0.1023) | Loss 9.5223(9.7124) | Error 0.0478(0.0355) Steps 1084(1074.82) | Grad Norm 4.0127(5.1515) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0349 | Time 121.8741, Epoch Time 1650.3963(1668.2596), Bit/dim 3.5840(best: 3.5709), Xent 1.0625, Loss 4.1152, Error 0.2248(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 19200 | Time 27.4436(27.4457) | Bit/dim 3.5575(3.5728) | Xent 0.1119(0.1038) | Loss 9.4532(10.6270) | Error 0.0389(0.0361) Steps 1096(1071.85) | Grad Norm 12.0263(5.2495) | Total Time 0.00(0.00)\n",
      "Iter 19210 | Time 26.5933(27.4467) | Bit/dim 3.5924(3.5735) | Xent 0.1250(0.1062) | Loss 9.5192(10.3178) | Error 0.0378(0.0369) Steps 1066(1073.47) | Grad Norm 5.1676(5.1652) | Total Time 0.00(0.00)\n",
      "Iter 19220 | Time 26.7524(27.4273) | Bit/dim 3.5674(3.5730) | Xent 0.1178(0.1056) | Loss 9.3286(10.0678) | Error 0.0411(0.0367) Steps 1072(1073.70) | Grad Norm 13.5115(5.8379) | Total Time 0.00(0.00)\n",
      "Iter 19230 | Time 27.0193(27.4081) | Bit/dim 3.5958(3.5737) | Xent 0.0769(0.1040) | Loss 9.2459(9.8877) | Error 0.0300(0.0358) Steps 1048(1075.09) | Grad Norm 4.5048(5.9263) | Total Time 0.00(0.00)\n",
      "Iter 19240 | Time 28.2121(27.3785) | Bit/dim 3.5810(3.5747) | Xent 0.1002(0.1050) | Loss 9.3413(9.7596) | Error 0.0344(0.0366) Steps 1030(1071.76) | Grad Norm 4.0147(6.0494) | Total Time 0.00(0.00)\n",
      "Iter 19250 | Time 27.1700(27.4721) | Bit/dim 3.5773(3.5749) | Xent 0.1039(0.1067) | Loss 9.2256(9.6660) | Error 0.0433(0.0375) Steps 1060(1074.16) | Grad Norm 5.3615(5.7523) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0350 | Time 120.6710, Epoch Time 1648.2930(1667.6606), Bit/dim 3.5906(best: 3.5709), Xent 1.0677, Loss 4.1244, Error 0.2300(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 19260 | Time 28.5217(27.4925) | Bit/dim 3.6098(3.5742) | Xent 0.1093(0.1078) | Loss 9.6159(10.4668) | Error 0.0444(0.0377) Steps 1084(1069.57) | Grad Norm 6.1790(5.8091) | Total Time 0.00(0.00)\n",
      "Iter 19270 | Time 26.5589(27.3908) | Bit/dim 3.5744(3.5752) | Xent 0.0910(0.1044) | Loss 9.3020(10.1798) | Error 0.0322(0.0363) Steps 1042(1068.27) | Grad Norm 2.9595(5.4784) | Total Time 0.00(0.00)\n",
      "Iter 19280 | Time 27.7965(27.4508) | Bit/dim 3.5823(3.5738) | Xent 0.1123(0.1065) | Loss 9.4665(9.9754) | Error 0.0367(0.0373) Steps 1096(1074.63) | Grad Norm 4.9066(5.6907) | Total Time 0.00(0.00)\n",
      "Iter 19290 | Time 26.9999(27.5274) | Bit/dim 3.5578(3.5760) | Xent 0.1051(0.1063) | Loss 9.3984(9.8366) | Error 0.0378(0.0371) Steps 1054(1078.53) | Grad Norm 4.0169(5.3932) | Total Time 0.00(0.00)\n",
      "Iter 19300 | Time 27.4721(27.5681) | Bit/dim 3.5847(3.5771) | Xent 0.1250(0.1072) | Loss 9.5683(9.7255) | Error 0.0411(0.0371) Steps 1084(1076.95) | Grad Norm 8.0027(5.5222) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0351 | Time 121.3307, Epoch Time 1654.2137(1667.2572), Bit/dim 3.5871(best: 3.5709), Xent 1.0719, Loss 4.1230, Error 0.2310(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 19310 | Time 27.9827(27.5692) | Bit/dim 3.5668(3.5765) | Xent 0.0987(0.1037) | Loss 9.4443(10.6424) | Error 0.0333(0.0356) Steps 1078(1076.98) | Grad Norm 6.3116(5.3387) | Total Time 0.00(0.00)\n",
      "Iter 19320 | Time 28.1132(27.5481) | Bit/dim 3.6147(3.5775) | Xent 0.0872(0.1029) | Loss 9.5258(10.3211) | Error 0.0333(0.0356) Steps 1066(1075.23) | Grad Norm 3.5247(4.9508) | Total Time 0.00(0.00)\n",
      "Iter 19330 | Time 26.8621(27.5976) | Bit/dim 3.5693(3.5753) | Xent 0.1048(0.1024) | Loss 9.3542(10.0808) | Error 0.0378(0.0357) Steps 1078(1077.68) | Grad Norm 3.7127(4.9344) | Total Time 0.00(0.00)\n",
      "Iter 19340 | Time 28.1927(27.7836) | Bit/dim 3.5250(3.5748) | Xent 0.1113(0.1036) | Loss 9.2269(9.9103) | Error 0.0400(0.0357) Steps 1066(1077.74) | Grad Norm 3.8928(4.6816) | Total Time 0.00(0.00)\n",
      "Iter 19350 | Time 28.6372(27.8389) | Bit/dim 3.5728(3.5710) | Xent 0.1295(0.1042) | Loss 9.3486(9.7684) | Error 0.0400(0.0364) Steps 1030(1076.51) | Grad Norm 4.5178(4.7621) | Total Time 0.00(0.00)\n",
      "Iter 19360 | Time 28.0950(27.7708) | Bit/dim 3.5966(3.5713) | Xent 0.0830(0.1038) | Loss 9.5092(9.6738) | Error 0.0267(0.0363) Steps 1096(1074.75) | Grad Norm 7.3270(4.9394) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0352 | Time 121.8711, Epoch Time 1668.8658(1667.3054), Bit/dim 3.5824(best: 3.5709), Xent 1.0550, Loss 4.1099, Error 0.2247(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 19370 | Time 27.7256(27.7307) | Bit/dim 3.5931(3.5717) | Xent 0.0791(0.1003) | Loss 9.4963(10.4490) | Error 0.0289(0.0352) Steps 1096(1074.34) | Grad Norm 3.2953(4.8460) | Total Time 0.00(0.00)\n",
      "Iter 19380 | Time 27.1071(27.7689) | Bit/dim 3.5591(3.5710) | Xent 0.0978(0.0989) | Loss 9.4611(10.1727) | Error 0.0311(0.0345) Steps 1114(1075.75) | Grad Norm 9.6828(4.6515) | Total Time 0.00(0.00)\n",
      "Iter 19390 | Time 27.3444(27.7420) | Bit/dim 3.5682(3.5759) | Xent 0.1007(0.0998) | Loss 9.3805(9.9832) | Error 0.0344(0.0351) Steps 1030(1074.18) | Grad Norm 4.3212(4.6126) | Total Time 0.00(0.00)\n",
      "Iter 19400 | Time 27.4466(27.7205) | Bit/dim 3.5859(3.5751) | Xent 0.1195(0.1014) | Loss 9.4610(9.8378) | Error 0.0422(0.0358) Steps 1084(1074.69) | Grad Norm 4.6467(4.6184) | Total Time 0.00(0.00)\n",
      "Iter 19410 | Time 28.0550(27.7200) | Bit/dim 3.5889(3.5730) | Xent 0.1073(0.1031) | Loss 9.5598(9.7283) | Error 0.0389(0.0358) Steps 1126(1077.55) | Grad Norm 5.0340(4.7350) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0353 | Time 121.6970, Epoch Time 1665.6269(1667.2551), Bit/dim 3.5819(best: 3.5709), Xent 1.0656, Loss 4.1147, Error 0.2275(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 19420 | Time 28.0749(27.7889) | Bit/dim 3.5570(3.5723) | Xent 0.1202(0.1025) | Loss 9.4199(10.6404) | Error 0.0356(0.0354) Steps 1108(1076.25) | Grad Norm 4.0553(4.8098) | Total Time 0.00(0.00)\n",
      "Iter 19430 | Time 27.0759(27.8031) | Bit/dim 3.5491(3.5714) | Xent 0.0800(0.1014) | Loss 9.1479(10.3122) | Error 0.0267(0.0355) Steps 1048(1079.01) | Grad Norm 4.9744(4.7177) | Total Time 0.00(0.00)\n",
      "Iter 19440 | Time 28.8808(27.9231) | Bit/dim 3.5352(3.5724) | Xent 0.0927(0.1009) | Loss 9.3783(10.0817) | Error 0.0378(0.0355) Steps 1084(1081.52) | Grad Norm 8.2245(4.9359) | Total Time 0.00(0.00)\n",
      "Iter 19450 | Time 29.3795(28.0570) | Bit/dim 3.5665(3.5719) | Xent 0.1006(0.1001) | Loss 9.3891(9.9028) | Error 0.0400(0.0353) Steps 1102(1083.55) | Grad Norm 4.7290(5.0055) | Total Time 0.00(0.00)\n",
      "Iter 19460 | Time 27.7958(28.1134) | Bit/dim 3.5710(3.5706) | Xent 0.0772(0.0996) | Loss 9.4781(9.7783) | Error 0.0189(0.0349) Steps 1114(1085.95) | Grad Norm 7.8870(5.7010) | Total Time 0.00(0.00)\n",
      "Iter 19470 | Time 29.4332(28.2474) | Bit/dim 3.5562(3.5717) | Xent 0.0993(0.1015) | Loss 9.4660(9.6889) | Error 0.0389(0.0356) Steps 1150(1087.10) | Grad Norm 5.6017(6.2238) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0354 | Time 122.7569, Epoch Time 1695.0199(1668.0880), Bit/dim 3.5830(best: 3.5709), Xent 1.0556, Loss 4.1108, Error 0.2316(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 19480 | Time 28.9840(28.3138) | Bit/dim 3.5790(3.5696) | Xent 0.1020(0.1008) | Loss 9.4122(10.4820) | Error 0.0333(0.0351) Steps 1108(1091.84) | Grad Norm 7.9997(6.4658) | Total Time 0.00(0.00)\n",
      "Iter 19490 | Time 29.1131(28.3291) | Bit/dim 3.5656(3.5734) | Xent 0.1503(0.1020) | Loss 9.6375(10.2063) | Error 0.0556(0.0353) Steps 1138(1097.32) | Grad Norm 33.7723(8.7319) | Total Time 0.00(0.00)\n",
      "Iter 19500 | Time 27.1921(28.4255) | Bit/dim 3.5632(3.5731) | Xent 0.1004(0.1031) | Loss 9.3851(10.0124) | Error 0.0378(0.0362) Steps 1108(1098.85) | Grad Norm 7.3165(10.6833) | Total Time 0.00(0.00)\n",
      "Iter 19510 | Time 28.6253(28.4623) | Bit/dim 3.6030(3.5731) | Xent 0.1041(0.1052) | Loss 9.4952(9.8573) | Error 0.0344(0.0370) Steps 1114(1098.37) | Grad Norm 7.4290(11.0666) | Total Time 0.00(0.00)\n",
      "Iter 19520 | Time 29.2807(28.6712) | Bit/dim 3.6279(3.5774) | Xent 0.1177(0.1097) | Loss 9.5289(9.7609) | Error 0.0400(0.0387) Steps 1114(1101.26) | Grad Norm 9.9207(18.0174) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0355 | Time 126.7547, Epoch Time 1725.3436(1669.8057), Bit/dim 3.6781(best: 3.5709), Xent 1.0862, Loss 4.2213, Error 0.2355(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 19530 | Time 30.1262(28.9331) | Bit/dim 3.6733(3.5949) | Xent 0.2019(0.1327) | Loss 9.8236(10.7849) | Error 0.0711(0.0452) Steps 1156(1109.37) | Grad Norm 19.0925(46.1389) | Total Time 0.00(0.00)\n",
      "Iter 19540 | Time 28.7827(29.0690) | Bit/dim 3.6399(3.6071) | Xent 0.1837(0.1454) | Loss 9.6423(10.4858) | Error 0.0600(0.0501) Steps 1138(1112.68) | Grad Norm 17.6763(41.8346) | Total Time 0.00(0.00)\n",
      "Iter 19550 | Time 26.6673(28.7691) | Bit/dim 3.5979(3.6094) | Xent 0.1455(0.1485) | Loss 9.4281(10.2431) | Error 0.0489(0.0520) Steps 1066(1110.06) | Grad Norm 16.1630(33.8969) | Total Time 0.00(0.00)\n",
      "Iter 19560 | Time 28.0694(28.4584) | Bit/dim 3.6262(3.6059) | Xent 0.1174(0.1410) | Loss 9.6141(10.0339) | Error 0.0444(0.0499) Steps 1096(1103.19) | Grad Norm 4.2954(27.5620) | Total Time 0.00(0.00)\n",
      "Iter 19570 | Time 27.8547(28.1952) | Bit/dim 3.6213(3.5994) | Xent 0.1067(0.1317) | Loss 9.4272(9.8747) | Error 0.0322(0.0460) Steps 1072(1095.44) | Grad Norm 4.1099(22.1896) | Total Time 0.00(0.00)\n",
      "Iter 19580 | Time 27.8479(28.0225) | Bit/dim 3.5916(3.5978) | Xent 0.1077(0.1253) | Loss 9.3919(9.7537) | Error 0.0322(0.0434) Steps 1048(1090.74) | Grad Norm 5.3519(17.8884) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0356 | Time 121.0607, Epoch Time 1688.7898(1670.3752), Bit/dim 3.5932(best: 3.5709), Xent 1.0687, Loss 4.1276, Error 0.2296(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 19590 | Time 26.3654(27.8154) | Bit/dim 3.5752(3.5943) | Xent 0.0901(0.1184) | Loss 9.3915(10.5387) | Error 0.0289(0.0411) Steps 1060(1089.13) | Grad Norm 4.5160(14.4707) | Total Time 0.00(0.00)\n",
      "Iter 19600 | Time 27.0150(27.8154) | Bit/dim 3.6214(3.5886) | Xent 0.0833(0.1131) | Loss 9.3838(10.2461) | Error 0.0289(0.0392) Steps 1048(1089.52) | Grad Norm 9.8893(12.2457) | Total Time 0.00(0.00)\n",
      "Iter 19610 | Time 27.8363(27.7233) | Bit/dim 3.5488(3.5838) | Xent 0.1417(0.1125) | Loss 9.4289(10.0330) | Error 0.0511(0.0390) Steps 1120(1089.25) | Grad Norm 7.3664(10.2888) | Total Time 0.00(0.00)\n",
      "Iter 19620 | Time 28.1091(27.6838) | Bit/dim 3.5914(3.5839) | Xent 0.1218(0.1092) | Loss 9.5345(9.8738) | Error 0.0489(0.0374) Steps 1084(1085.41) | Grad Norm 4.3122(9.5790) | Total Time 0.00(0.00)\n",
      "Iter 19630 | Time 27.3881(27.6288) | Bit/dim 3.5406(3.5800) | Xent 0.1099(0.1086) | Loss 9.4185(9.7571) | Error 0.0356(0.0378) Steps 1072(1084.22) | Grad Norm 12.2145(9.3480) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0357 | Time 121.7648, Epoch Time 1654.9452(1669.9123), Bit/dim 3.5892(best: 3.5709), Xent 1.0628, Loss 4.1206, Error 0.2249(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 19640 | Time 28.7228(27.6951) | Bit/dim 3.5608(3.5795) | Xent 0.0919(0.1077) | Loss 9.3623(10.7005) | Error 0.0356(0.0377) Steps 1114(1084.36) | Grad Norm 4.9102(8.6882) | Total Time 0.00(0.00)\n",
      "Iter 19650 | Time 28.0262(27.7800) | Bit/dim 3.5777(3.5794) | Xent 0.0949(0.1043) | Loss 9.3648(10.3535) | Error 0.0322(0.0365) Steps 1090(1084.22) | Grad Norm 12.2114(7.8727) | Total Time 0.00(0.00)\n",
      "Iter 19660 | Time 27.4604(27.7996) | Bit/dim 3.6055(3.5801) | Xent 0.0957(0.1033) | Loss 9.4226(10.1160) | Error 0.0333(0.0358) Steps 1072(1083.97) | Grad Norm 3.5092(9.1389) | Total Time 0.00(0.00)\n",
      "Iter 19670 | Time 27.9075(27.8998) | Bit/dim 3.5882(3.5791) | Xent 0.1212(0.1036) | Loss 9.4384(9.9234) | Error 0.0433(0.0362) Steps 1084(1080.95) | Grad Norm 13.2005(12.1143) | Total Time 0.00(0.00)\n",
      "Iter 19680 | Time 28.9086(27.9482) | Bit/dim 3.5695(3.5781) | Xent 0.0656(0.1006) | Loss 9.4119(9.7827) | Error 0.0256(0.0349) Steps 1138(1086.16) | Grad Norm 4.5048(10.7322) | Total Time 0.00(0.00)\n",
      "Iter 19690 | Time 29.7809(28.0245) | Bit/dim 3.5855(3.5788) | Xent 0.1062(0.1020) | Loss 9.5064(9.6834) | Error 0.0311(0.0356) Steps 1108(1085.43) | Grad Norm 24.5161(12.6011) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0358 | Time 122.8131, Epoch Time 1683.8044(1670.3291), Bit/dim 3.5989(best: 3.5709), Xent 1.0776, Loss 4.1377, Error 0.2266(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 19700 | Time 29.0790(28.1707) | Bit/dim 3.5660(3.5828) | Xent 0.1143(0.1052) | Loss 9.4320(10.5193) | Error 0.0400(0.0369) Steps 1090(1087.38) | Grad Norm 6.7772(21.9164) | Total Time 0.00(0.00)\n",
      "Iter 19710 | Time 29.3698(28.4511) | Bit/dim 3.6563(3.5966) | Xent 0.2446(0.1206) | Loss 9.6125(10.2701) | Error 0.0744(0.0419) Steps 1120(1093.00) | Grad Norm 189.0087(54.1923) | Total Time 0.00(0.00)\n",
      "Iter 19720 | Time 31.0542(28.9256) | Bit/dim 3.6671(3.6188) | Xent 0.2164(0.1434) | Loss 9.7428(10.1373) | Error 0.0756(0.0494) Steps 1150(1102.25) | Grad Norm 27.4021(66.4510) | Total Time 0.00(0.00)\n",
      "Iter 19730 | Time 29.7410(29.2600) | Bit/dim 3.6538(3.6276) | Xent 0.1210(0.1519) | Loss 9.6012(10.0096) | Error 0.0400(0.0525) Steps 1084(1109.22) | Grad Norm 44.0430(58.6503) | Total Time 0.00(0.00)\n",
      "Iter 19740 | Time 29.6070(29.2923) | Bit/dim 3.6777(3.6333) | Xent 0.1755(0.1553) | Loss 9.8390(9.9063) | Error 0.0656(0.0538) Steps 1138(1110.31) | Grad Norm 30.1315(67.5215) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0359 | Time 125.5184, Epoch Time 1772.8919(1673.4060), Bit/dim 3.7017(best: 3.5709), Xent 1.0675, Loss 4.2355, Error 0.2320(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 19750 | Time 30.3702(29.5610) | Bit/dim 3.6669(3.6457) | Xent 0.1577(0.1670) | Loss 9.6689(10.8798) | Error 0.0489(0.0570) Steps 1114(1115.51) | Grad Norm 16.8923(67.0190) | Total Time 0.00(0.00)\n",
      "Iter 19760 | Time 32.5321(29.8489) | Bit/dim 3.8123(3.6700) | Xent 0.3043(0.1914) | Loss 10.2628(10.6339) | Error 0.0922(0.0629) Steps 1138(1122.83) | Grad Norm 518.5195(139.3023) | Total Time 0.00(0.00)\n",
      "Iter 19770 | Time 33.9607(30.6262) | Bit/dim 3.9805(3.7337) | Xent 0.5235(0.2671) | Loss 10.6939(10.6208) | Error 0.1478(0.0826) Steps 1192(1144.43) | Grad Norm 369.4503(195.7076) | Total Time 0.00(0.00)\n",
      "Iter 19780 | Time 32.4088(31.4728) | Bit/dim 3.9887(3.8083) | Xent 0.5314(0.3531) | Loss 10.6148(10.6965) | Error 0.1600(0.1058) Steps 1174(1166.96) | Grad Norm 786.1267(298.3046) | Total Time 0.00(0.00)\n",
      "Iter 19790 | Time 34.8031(32.4743) | Bit/dim 4.4313(3.9702) | Xent 1.1168(0.5338) | Loss 12.2831(11.1354) | Error 0.2556(0.1448) Steps 1270(1199.37) | Grad Norm 416.7337(437.9385) | Total Time 0.00(0.00)\n",
      "Iter 19800 | Time 33.5775(33.0456) | Bit/dim 4.0196(4.0335) | Xent 0.6175(0.5960) | Loss 10.8990(11.2584) | Error 0.1822(0.1605) Steps 1210(1210.74) | Grad Norm 310.3415(404.2287) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0360 | Time 136.7550, Epoch Time 1978.0886(1682.5464), Bit/dim 4.0263(best: 3.5709), Xent 0.9495, Loss 4.5011, Error 0.2705(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 19810 | Time 31.7051(33.0061) | Bit/dim 3.8744(4.0022) | Xent 0.3396(0.5585) | Loss 10.2429(11.9543) | Error 0.1156(0.1572) Steps 1168(1205.57) | Grad Norm 785.6984(354.6332) | Total Time 0.00(0.00)\n",
      "Iter 19820 | Time 34.2053(33.1069) | Bit/dim 3.8729(3.9643) | Xent 0.4210(0.5061) | Loss 10.3321(11.5134) | Error 0.1233(0.1468) Steps 1186(1199.48) | Grad Norm 378.4642(317.6513) | Total Time 0.00(0.00)\n",
      "Iter 19830 | Time 36.3210(33.5349) | Bit/dim 4.2259(3.9617) | Xent 0.6517(0.4822) | Loss 11.4518(11.2699) | Error 0.1778(0.1423) Steps 1276(1210.98) | Grad Norm 674.5070(389.6777) | Total Time 0.00(0.00)\n",
      "Iter 19840 | Time 39.4708(34.6956) | Bit/dim 12.8430(4.4596) | Xent 7.5937(0.9028) | Loss 37.3347(12.6747) | Error 0.5656(0.1857) Steps 1432(1246.79) | Grad Norm 9643.6954(841.2780) | Total Time 0.00(0.00)\n",
      "Iter 19850 | Time 35.4476(35.4588) | Bit/dim 5.2420(5.0712) | Xent 1.9564(1.6533) | Loss 15.0913(14.6872) | Error 0.5500(0.3016) Steps 1324(1287.06) | Grad Norm 144.5440(948.5087) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0361 | Time 142.9563, Epoch Time 2100.6549(1695.0897), Bit/dim 5.0236(best: 3.5709), Xent 1.3791, Loss 5.7131, Error 0.4669(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 19860 | Time 32.5204(35.0747) | Bit/dim 4.8750(5.0581) | Xent 1.5846(1.6731) | Loss 13.5528(15.6563) | Error 0.5056(0.3620) Steps 1198(1275.38) | Grad Norm 62.2518(719.8924) | Total Time 0.00(0.00)\n",
      "Iter 19870 | Time 32.2637(34.4939) | Bit/dim 4.6957(4.9865) | Xent 1.3095(1.5828) | Loss 12.9200(14.9978) | Error 0.4256(0.3829) Steps 1180(1257.35) | Grad Norm 29.5842(542.1795) | Total Time 0.00(0.00)\n",
      "Iter 19880 | Time 32.6711(33.7776) | Bit/dim 4.5645(4.8890) | Xent 1.0835(1.4723) | Loss 12.4202(14.3797) | Error 0.3767(0.3854) Steps 1150(1229.71) | Grad Norm 24.1741(407.4667) | Total Time 0.00(0.00)\n",
      "Iter 19890 | Time 30.7836(33.3070) | Bit/dim 4.4401(4.7832) | Xent 1.0180(1.3548) | Loss 12.0530(13.8084) | Error 0.3289(0.3740) Steps 1120(1203.60) | Grad Norm 21.2002(308.1225) | Total Time 0.00(0.00)\n",
      "Iter 19900 | Time 31.2644(32.7043) | Bit/dim 4.3792(4.6796) | Xent 0.9949(1.2374) | Loss 11.9886(13.2929) | Error 0.3233(0.3560) Steps 1144(1188.57) | Grad Norm 22.6592(233.6825) | Total Time 0.00(0.00)\n",
      "Iter 19910 | Time 31.1891(32.3270) | Bit/dim 4.3113(4.5813) | Xent 0.8116(1.1342) | Loss 11.6539(12.8490) | Error 0.2811(0.3366) Steps 1162(1177.57) | Grad Norm 18.4202(178.4634) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0362 | Time 129.2976, Epoch Time 1903.9258(1701.3548), Bit/dim 4.2873(best: 3.5709), Xent 0.9003, Loss 4.7375, Error 0.2929(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 19920 | Time 30.6325(31.8136) | Bit/dim 4.1897(4.4917) | Xent 0.7676(1.0404) | Loss 11.1718(13.2819) | Error 0.2700(0.3176) Steps 1102(1161.01) | Grad Norm 115.1154(156.8909) | Total Time 0.00(0.00)\n",
      "Iter 19930 | Time 29.6929(31.4615) | Bit/dim 4.1350(4.4081) | Xent 0.7547(0.9595) | Loss 11.1457(12.7383) | Error 0.2544(0.2985) Steps 1126(1154.00) | Grad Norm 79.3896(136.1395) | Total Time 0.00(0.00)\n",
      "Iter 19940 | Time 30.9960(31.0700) | Bit/dim 4.0949(4.3298) | Xent 0.6333(0.8812) | Loss 10.9768(12.2801) | Error 0.2089(0.2790) Steps 1114(1144.17) | Grad Norm 14.9299(133.2247) | Total Time 0.00(0.00)\n",
      "Iter 19950 | Time 29.6828(30.7872) | Bit/dim 4.0258(4.2565) | Xent 0.5410(0.8098) | Loss 10.6210(11.8926) | Error 0.1822(0.2608) Steps 1138(1138.30) | Grad Norm 19.2405(109.4111) | Total Time 0.00(0.00)\n",
      "Iter 19960 | Time 29.7627(30.6368) | Bit/dim 3.9946(4.1937) | Xent 0.5516(0.7474) | Loss 10.8228(11.5837) | Error 0.1911(0.2437) Steps 1114(1131.78) | Grad Norm 60.3716(94.1037) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0363 | Time 124.8787, Epoch Time 1801.2062(1704.3503), Bit/dim 3.9692(best: 3.5709), Xent 0.8136, Loss 4.3761, Error 0.2482(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 19970 | Time 31.6845(30.4050) | Bit/dim 3.9316(4.1322) | Xent 0.4758(0.6982) | Loss 10.4317(12.2536) | Error 0.1656(0.2293) Steps 1120(1127.25) | Grad Norm 63.2642(119.8132) | Total Time 0.00(0.00)\n",
      "Iter 19980 | Time 29.9929(30.1129) | Bit/dim 3.8420(4.0689) | Xent 0.3932(0.6338) | Loss 10.1680(11.7446) | Error 0.1322(0.2100) Steps 1108(1120.00) | Grad Norm 154.6893(113.4191) | Total Time 0.00(0.00)\n",
      "Iter 19990 | Time 30.0353(30.0001) | Bit/dim 3.8376(4.0129) | Xent 0.4471(0.5812) | Loss 10.1092(11.3471) | Error 0.1544(0.1944) Steps 1096(1113.60) | Grad Norm 80.5223(101.3100) | Total Time 0.00(0.00)\n",
      "Iter 20000 | Time 29.4107(29.8748) | Bit/dim 3.8151(3.9660) | Xent 0.3777(0.5342) | Loss 10.1057(11.0282) | Error 0.1233(0.1803) Steps 1114(1115.23) | Grad Norm 44.0956(90.1583) | Total Time 0.00(0.00)\n",
      "Iter 20010 | Time 28.7978(29.9437) | Bit/dim 3.8154(3.9289) | Xent 0.3495(0.4970) | Loss 10.1358(10.8028) | Error 0.1256(0.1684) Steps 1114(1114.89) | Grad Norm 18.3216(92.5230) | Total Time 0.00(0.00)\n",
      "Iter 20020 | Time 31.5828(30.2097) | Bit/dim 3.8324(3.9036) | Xent 0.4377(0.4728) | Loss 10.2495(10.6430) | Error 0.1633(0.1609) Steps 1120(1121.83) | Grad Norm 53.5876(97.3097) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0364 | Time 129.2489, Epoch Time 1793.0368(1707.0109), Bit/dim 3.8558(best: 3.5709), Xent 0.8262, Loss 4.2689, Error 0.2347(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 20030 | Time 33.2219(30.5815) | Bit/dim 3.9296(3.9021) | Xent 0.4995(0.4667) | Loss 10.4326(11.3854) | Error 0.1678(0.1585) Steps 1180(1128.80) | Grad Norm 163.6962(134.2261) | Total Time 0.00(0.00)\n",
      "Iter 20040 | Time 33.6640(31.2126) | Bit/dim 4.0076(3.9282) | Xent 0.5873(0.4838) | Loss 10.8638(11.2206) | Error 0.1933(0.1638) Steps 1180(1146.87) | Grad Norm 198.1048(130.8265) | Total Time 0.00(0.00)\n",
      "Iter 20050 | Time 34.9372(32.0679) | Bit/dim 4.1723(3.9756) | Xent 0.7108(0.5221) | Loss 11.3872(11.1877) | Error 0.2278(0.1754) Steps 1258(1167.98) | Grad Norm 860.2032(230.6122) | Total Time 0.00(0.00)\n",
      "Iter 20060 | Time 41.9319(33.5564) | Bit/dim 13.1657(4.5279) | Xent 10.3960(1.1842) | Loss 40.7838(12.9736) | Error 0.7778(0.2453) Steps 1510(1216.42) | Grad Norm 4392.1974(1143.2879) | Total Time 0.00(0.00)\n",
      "Iter 20070 | Time 36.9976(35.4736) | Bit/dim 6.4719(7.0750) | Xent 3.9157(3.5241) | Loss 19.7105(20.8252) | Error 0.7800(0.3985) Steps 1384(1304.19) | Grad Norm 458.5825(2648.5466) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0365 | Time 155.9796, Epoch Time 2136.6312(1719.8995), Bit/dim 5.5147(best: 3.5709), Xent 2.2735, Loss 6.6514, Error 0.7070(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 20080 | Time 35.2649(35.8041) | Bit/dim 5.2569(6.6928) | Xent 2.2453(3.3027) | Loss 15.0271(20.8897) | Error 0.6733(0.4865) Steps 1312(1319.98) | Grad Norm 198.2124(2046.8724) | Total Time 0.00(0.00)\n",
      "Iter 20090 | Time 32.5850(35.5873) | Bit/dim 4.9580(6.2634) | Xent 1.6263(2.9356) | Loss 13.9960(19.1923) | Error 0.5522(0.5220) Steps 1210(1304.81) | Grad Norm 324.2597(1649.6343) | Total Time 0.00(0.00)\n",
      "Iter 20100 | Time 32.5337(34.9575) | Bit/dim 4.7770(5.8915) | Xent 1.3811(2.5630) | Loss 13.2397(17.6904) | Error 0.4711(0.5213) Steps 1168(1271.91) | Grad Norm 135.5557(1400.7511) | Total Time 0.00(0.00)\n",
      "Iter 20110 | Time 32.4277(34.3007) | Bit/dim 4.6857(5.5836) | Xent 1.2249(2.2340) | Loss 12.7264(16.4438) | Error 0.4256(0.5034) Steps 1144(1243.12) | Grad Norm 999.2898(1144.5868) | Total Time 0.00(0.00)\n",
      "Iter 20120 | Time 33.3372(33.8197) | Bit/dim 4.6619(5.3448) | Xent 1.2150(1.9792) | Loss 12.6997(15.4960) | Error 0.4322(0.4849) Steps 1126(1218.78) | Grad Norm 553.2803(1017.9617) | Total Time 0.00(0.00)\n",
      "Iter 20130 | Time 32.5619(33.6994) | Bit/dim 4.6843(5.1704) | Xent 1.1793(1.7865) | Loss 12.6292(14.8106) | Error 0.3911(0.4674) Steps 1192(1209.04) | Grad Norm 973.0076(918.9446) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0366 | Time 134.8110, Epoch Time 1999.2227(1728.2792), Bit/dim 4.7049(best: 3.5709), Xent 1.1075, Loss 5.2587, Error 0.3846(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 20140 | Time 33.6509(33.6804) | Bit/dim 4.7345(5.0467) | Xent 1.2436(1.6477) | Loss 13.1668(15.1606) | Error 0.4267(0.4550) Steps 1216(1204.68) | Grad Norm 941.0244(846.7244) | Total Time 0.00(0.00)\n",
      "Iter 20150 | Time 35.5314(33.8444) | Bit/dim 4.8447(4.9699) | Xent 1.3124(1.5408) | Loss 13.3730(14.5997) | Error 0.4456(0.4448) Steps 1216(1208.22) | Grad Norm 142.9707(800.8216) | Total Time 0.00(0.00)\n",
      "Iter 20160 | Time 35.0484(34.1183) | Bit/dim 4.8280(4.9245) | Xent 1.3547(1.4758) | Loss 13.2144(14.2430) | Error 0.4489(0.4390) Steps 1198(1216.75) | Grad Norm 1131.9914(946.1565) | Total Time 0.00(0.00)\n",
      "Iter 20170 | Time 36.3252(34.4511) | Bit/dim 4.8148(4.9004) | Xent 1.3144(1.4139) | Loss 13.1667(13.9886) | Error 0.4311(0.4335) Steps 1246(1228.36) | Grad Norm 528.2752(785.6217) | Total Time 0.00(0.00)\n",
      "Iter 20180 | Time 35.7519(34.5541) | Bit/dim 4.7936(4.8716) | Xent 1.1624(1.3539) | Loss 13.1836(13.7439) | Error 0.3767(0.4222) Steps 1234(1233.33) | Grad Norm 494.1331(771.8299) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0367 | Time 143.9260, Epoch Time 2070.4510(1738.5444), Bit/dim 4.8361(best: 3.5709), Xent 1.1148, Loss 5.3935, Error 0.3792(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 20190 | Time 36.6802(34.8654) | Bit/dim 4.8170(4.8583) | Xent 1.2480(1.3203) | Loss 13.2327(14.6456) | Error 0.4044(0.4178) Steps 1234(1239.89) | Grad Norm 550.7766(768.4624) | Total Time 0.00(0.00)\n",
      "Iter 20200 | Time 36.4164(35.3988) | Bit/dim 4.9416(4.8642) | Xent 1.2952(1.3014) | Loss 13.4476(14.3122) | Error 0.4111(0.4146) Steps 1318(1252.17) | Grad Norm 1576.0327(1679.7308) | Total Time 0.00(0.00)\n",
      "Iter 20210 | Time 36.5228(35.8727) | Bit/dim 4.7424(4.8897) | Xent 1.2881(1.3043) | Loss 13.1968(14.1404) | Error 0.3911(0.4172) Steps 1282(1269.40) | Grad Norm 2354.7120(1878.0408) | Total Time 0.00(0.00)\n",
      "Iter 20220 | Time 34.8225(35.5887) | Bit/dim 4.6837(4.8449) | Xent 1.1645(1.2607) | Loss 12.8626(13.7926) | Error 0.3878(0.4046) Steps 1216(1257.25) | Grad Norm 52.6873(1519.6934) | Total Time 0.00(0.00)\n",
      "Iter 20230 | Time 33.7245(35.1846) | Bit/dim 4.5358(4.7761) | Xent 1.0181(1.2012) | Loss 12.1681(13.4266) | Error 0.3367(0.3888) Steps 1210(1245.22) | Grad Norm 1312.9967(1627.1657) | Total Time 0.00(0.00)\n",
      "Iter 20240 | Time 34.2549(34.8986) | Bit/dim 4.5819(4.7107) | Xent 0.9447(1.1370) | Loss 12.3065(13.1194) | Error 0.3311(0.3735) Steps 1210(1234.91) | Grad Norm 324.0083(1470.2334) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0368 | Time 138.1879, Epoch Time 2105.8800(1749.5644), Bit/dim 4.5765(best: 3.5709), Xent 0.9486, Loss 5.0508, Error 0.3283(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 20250 | Time 36.4569(35.0004) | Bit/dim 4.8905(4.7123) | Xent 1.1794(1.1222) | Loss 13.3850(13.8906) | Error 0.4067(0.3710) Steps 1282(1240.29) | Grad Norm 1913.3055(1481.5401) | Total Time 0.00(0.00)\n",
      "Iter 20260 | Time 37.5458(35.7605) | Bit/dim 4.6525(4.7346) | Xent 1.0829(1.1292) | Loss 12.6568(13.6891) | Error 0.3478(0.3716) Steps 1264(1249.54) | Grad Norm 1945.5629(1925.7725) | Total Time 0.00(0.00)\n",
      "Iter 20270 | Time 34.2564(35.8421) | Bit/dim 4.4305(4.6809) | Xent 0.9253(1.0827) | Loss 11.9826(13.3114) | Error 0.3122(0.3581) Steps 1210(1247.25) | Grad Norm 887.9187(2379.4816) | Total Time 0.00(0.00)\n",
      "Iter 20280 | Time 34.9390(35.4680) | Bit/dim 4.5458(4.6212) | Xent 0.9370(1.0333) | Loss 12.3944(12.9674) | Error 0.3100(0.3447) Steps 1222(1236.84) | Grad Norm 6405.0634(3209.3274) | Total Time 0.00(0.00)\n",
      "Iter 20290 | Time 40.8549(36.1342) | Bit/dim 5.5957(4.7576) | Xent 2.0895(1.1662) | Loss 16.2312(13.3218) | Error 0.5744(0.3775) Steps 1486(1262.21) | Grad Norm 9694.8645(3542.5568) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0369 | Time 173.1547, Epoch Time 2209.4129(1763.3599), Bit/dim 6.9232(best: 3.5709), Xent 2.4182, Loss 8.1323, Error 0.6464(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 20300 | Time 40.9580(37.2853) | Bit/dim 7.8220(5.3117) | Xent 3.5066(1.6600) | Loss 22.3833(16.2445) | Error 0.7244(0.4563) Steps 1468(1318.24) | Grad Norm 1853.8221(3935.9154) | Total Time 0.00(0.00)\n",
      "Iter 20310 | Time 41.2120(38.2343) | Bit/dim 6.3561(5.6520) | Xent 2.3151(1.9154) | Loss 17.6262(16.8773) | Error 0.5956(0.5043) Steps 1456(1351.96) | Grad Norm 6731.1488(3991.7597) | Total Time 0.00(0.00)\n",
      "Iter 20320 | Time 42.8435(39.4693) | Bit/dim 9.4613(6.0228) | Xent 4.1888(2.1887) | Loss 26.6387(17.7216) | Error 0.7089(0.5451) Steps 1528(1393.57) | Grad Norm 2070.5215(4975.0829) | Total Time 0.00(0.00)\n",
      "Iter 20330 | Time 42.6463(40.9846) | Bit/dim 10.0671(12.9753) | Xent 5.9226(4.4728) | Loss 29.9204(34.6482) | Error 0.7822(0.6152) Steps 1558(1462.97) | Grad Norm 3403.4216(9903.4554) | Total Time 0.00(0.00)\n",
      "Iter 20340 | Time 44.3981(41.8185) | Bit/dim 5.9819(11.3162) | Xent 2.0669(4.0226) | Loss 16.9049(30.5695) | Error 0.6389(0.6320) Steps 1504(1481.37) | Grad Norm 26113.9603(11911.0053) | Total Time 0.00(0.00)\n",
      "Iter 20350 | Time 41.3355(42.4851) | Bit/dim 5.9864(10.0521) | Xent 2.0396(3.5820) | Loss 16.7251(27.3280) | Error 0.6311(0.6406) Steps 1450(1493.59) | Grad Norm 8247.3591(14367.9593) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0370 | Time 167.4053, Epoch Time 2565.2604(1787.4169), Bit/dim 6.0018(best: 3.5709), Xent 1.7165, Loss 6.8600, Error 0.5872(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 20360 | Time 43.5050(42.4170) | Bit/dim 7.7650(9.1393) | Xent 3.4532(3.2975) | Loss 22.0751(25.9988) | Error 0.7211(0.6469) Steps 1414(1485.66) | Grad Norm 9535.3331(15118.7381) | Total Time 0.00(0.00)\n",
      "Iter 20370 | Time 39.0539(43.2775) | Bit/dim 5.0869(8.5570) | Xent 1.7233(3.1613) | Loss 14.2342(24.3061) | Error 0.5667(0.6559) Steps 1330(1491.71) | Grad Norm 2988.7122(29860.7807) | Total Time 0.00(0.00)\n",
      "Iter 20380 | Time 31.7959(40.6976) | Bit/dim 4.4553(7.5236) | Xent 1.1482(2.6784) | Loss 12.2096(21.2842) | Error 0.3911(0.6022) Steps 1108(1411.31) | Grad Norm 89.0335(22174.9035) | Total Time 0.00(0.00)\n",
      "Iter 20390 | Time 28.6032(37.7764) | Bit/dim 4.2978(6.6903) | Xent 0.9923(2.2590) | Loss 11.5501(18.8159) | Error 0.3567(0.5428) Steps 1066(1332.13) | Grad Norm 13.3980(16368.3636) | Total Time 0.00(0.00)\n",
      "Iter 20400 | Time 27.2444(35.3360) | Bit/dim 4.2275(6.0476) | Xent 0.9351(1.9205) | Loss 11.5307(16.9098) | Error 0.3300(0.4878) Steps 1054(1261.21) | Grad Norm 12.6400(12079.8775) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0371 | Time 121.9337, Epoch Time 2077.6940(1796.1252), Bit/dim 4.1909(best: 3.5709), Xent 0.9103, Loss 4.6461, Error 0.3157(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 20410 | Time 28.8701(33.3622) | Bit/dim 4.2040(5.5594) | Xent 0.9497(1.6456) | Loss 11.4860(16.3582) | Error 0.3256(0.4406) Steps 1042(1203.97) | Grad Norm 14.5512(8911.6157) | Total Time 0.00(0.00)\n",
      "Iter 20420 | Time 28.6359(31.8409) | Bit/dim 4.1357(5.1943) | Xent 0.8846(1.4312) | Loss 11.4449(15.0233) | Error 0.3044(0.4007) Steps 1054(1161.75) | Grad Norm 11.9602(6574.9952) | Total Time 0.00(0.00)\n",
      "Iter 20430 | Time 27.9873(30.6943) | Bit/dim 4.1430(4.9139) | Xent 0.7102(1.2576) | Loss 11.0103(13.9933) | Error 0.2544(0.3658) Steps 1048(1132.62) | Grad Norm 13.6083(4851.9277) | Total Time 0.00(0.00)\n",
      "Iter 20440 | Time 27.2548(29.7908) | Bit/dim 4.0885(4.7019) | Xent 0.7746(1.1270) | Loss 10.8945(13.2193) | Error 0.2844(0.3387) Steps 1078(1111.14) | Grad Norm 12.1706(3581.5302) | Total Time 0.00(0.00)\n",
      "Iter 20450 | Time 28.7131(29.2026) | Bit/dim 4.1014(4.5417) | Xent 0.7853(1.0269) | Loss 11.1467(12.6401) | Error 0.2511(0.3175) Steps 1036(1091.74) | Grad Norm 11.0020(2644.2660) | Total Time 0.00(0.00)\n",
      "Iter 20460 | Time 27.7660(28.7530) | Bit/dim 4.0789(4.4174) | Xent 0.7195(0.9490) | Loss 10.9932(12.1969) | Error 0.2500(0.3011) Steps 1024(1080.45) | Grad Norm 12.4332(1953.2071) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0372 | Time 120.9945, Epoch Time 1650.2831(1791.7500), Bit/dim 4.0739(best: 3.5709), Xent 0.8113, Loss 4.4796, Error 0.2765(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 20470 | Time 27.6942(28.4521) | Bit/dim 4.0609(4.3253) | Xent 0.6764(0.8770) | Loss 10.9440(12.6256) | Error 0.2311(0.2837) Steps 1042(1070.52) | Grad Norm 11.6075(1443.4005) | Total Time 0.00(0.00)\n",
      "Iter 20480 | Time 27.4235(28.2076) | Bit/dim 4.0475(4.2510) | Xent 0.6502(0.8202) | Loss 10.7841(12.1485) | Error 0.2322(0.2699) Steps 1054(1062.70) | Grad Norm 10.1146(1067.4807) | Total Time 0.00(0.00)\n",
      "Iter 20490 | Time 27.9047(28.0713) | Bit/dim 4.0072(4.1946) | Xent 0.6525(0.7798) | Loss 10.7854(11.7912) | Error 0.2400(0.2601) Steps 1090(1060.84) | Grad Norm 12.1434(790.8939) | Total Time 0.00(0.00)\n",
      "Iter 20500 | Time 27.5607(27.9673) | Bit/dim 4.0052(4.1502) | Xent 0.5526(0.7392) | Loss 10.6601(11.5137) | Error 0.1956(0.2496) Steps 1018(1057.47) | Grad Norm 10.9605(586.2553) | Total Time 0.00(0.00)\n",
      "Iter 20510 | Time 27.2734(27.9667) | Bit/dim 4.0073(4.1154) | Xent 0.5983(0.7085) | Loss 10.7445(11.3127) | Error 0.2111(0.2408) Steps 1066(1057.85) | Grad Norm 9.5031(435.1885) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0373 | Time 121.9140, Epoch Time 1663.6123(1787.9058), Bit/dim 4.0163(best: 3.5709), Xent 0.7853, Loss 4.4089, Error 0.2579(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 20520 | Time 27.2981(27.9121) | Bit/dim 4.0351(4.0870) | Xent 0.5585(0.6787) | Loss 10.7443(12.0914) | Error 0.1933(0.2331) Steps 1060(1056.31) | Grad Norm 12.2438(324.0208) | Total Time 0.00(0.00)\n",
      "Iter 20530 | Time 27.7696(27.9416) | Bit/dim 3.9935(4.0605) | Xent 0.5788(0.6560) | Loss 10.6373(11.7057) | Error 0.2144(0.2262) Steps 1060(1057.25) | Grad Norm 11.1698(242.5295) | Total Time 0.00(0.00)\n",
      "Iter 20540 | Time 27.3488(27.9173) | Bit/dim 3.9951(4.0413) | Xent 0.5915(0.6357) | Loss 10.6652(11.4062) | Error 0.2067(0.2205) Steps 1036(1058.68) | Grad Norm 9.0199(181.6783) | Total Time 0.00(0.00)\n",
      "Iter 20550 | Time 28.1619(27.8808) | Bit/dim 3.9870(4.0274) | Xent 0.5873(0.6306) | Loss 10.7541(11.2122) | Error 0.1989(0.2175) Steps 1060(1055.38) | Grad Norm 8.8657(138.3732) | Total Time 0.00(0.00)\n",
      "Iter 20560 | Time 27.8505(27.9582) | Bit/dim 4.0073(4.0175) | Xent 0.5670(0.6140) | Loss 10.7596(11.0601) | Error 0.1956(0.2121) Steps 1096(1059.61) | Grad Norm 10.5546(104.8900) | Total Time 0.00(0.00)\n",
      "Iter 20570 | Time 28.7774(28.0131) | Bit/dim 3.9969(4.0070) | Xent 0.5509(0.6005) | Loss 10.6710(10.9270) | Error 0.2044(0.2094) Steps 1072(1062.12) | Grad Norm 9.4241(80.1401) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0374 | Time 122.3366, Epoch Time 1679.7785(1784.6620), Bit/dim 3.9750(best: 3.5709), Xent 0.7649, Loss 4.3574, Error 0.2475(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 20580 | Time 27.5355(28.0229) | Bit/dim 3.9560(3.9975) | Xent 0.5402(0.5840) | Loss 10.4842(11.6098) | Error 0.1844(0.2040) Steps 1072(1059.05) | Grad Norm 9.3644(63.8278) | Total Time 0.00(0.00)\n",
      "Iter 20590 | Time 27.2090(28.1106) | Bit/dim 3.9493(3.9853) | Xent 0.5471(0.5684) | Loss 10.5529(11.3242) | Error 0.1889(0.1983) Steps 1066(1063.45) | Grad Norm 11.3798(50.2224) | Total Time 0.00(0.00)\n",
      "Iter 20600 | Time 27.5717(28.1030) | Bit/dim 3.9385(3.9772) | Xent 0.5327(0.5601) | Loss 10.5503(11.1222) | Error 0.1756(0.1951) Steps 1096(1063.54) | Grad Norm 10.2114(40.5901) | Total Time 0.00(0.00)\n",
      "Iter 20610 | Time 29.2160(28.1779) | Bit/dim 3.9195(3.9692) | Xent 0.5400(0.5507) | Loss 10.4454(10.9494) | Error 0.1856(0.1917) Steps 1078(1063.40) | Grad Norm 14.2294(32.9881) | Total Time 0.00(0.00)\n",
      "Iter 20620 | Time 28.9067(28.4052) | Bit/dim 3.9481(3.9584) | Xent 0.4559(0.5371) | Loss 10.4006(10.8092) | Error 0.1633(0.1865) Steps 1078(1068.74) | Grad Norm 10.6184(27.3167) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0375 | Time 123.7233, Epoch Time 1704.3325(1782.2521), Bit/dim 3.9432(best: 3.5709), Xent 0.7608, Loss 4.3236, Error 0.2439(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 20630 | Time 29.6661(28.5178) | Bit/dim 3.9763(3.9542) | Xent 0.5009(0.5303) | Loss 10.5827(11.6477) | Error 0.1756(0.1848) Steps 1090(1070.35) | Grad Norm 9.9957(23.0508) | Total Time 0.00(0.00)\n",
      "Iter 20640 | Time 28.4695(28.5363) | Bit/dim 3.9373(3.9481) | Xent 0.4893(0.5180) | Loss 10.5284(11.3254) | Error 0.1689(0.1810) Steps 1084(1072.06) | Grad Norm 39.7846(24.6878) | Total Time 0.00(0.00)\n",
      "Iter 20650 | Time 27.8928(28.5452) | Bit/dim 3.9176(3.9425) | Xent 0.5401(0.5153) | Loss 10.5409(11.0882) | Error 0.1856(0.1799) Steps 1078(1073.70) | Grad Norm 48.7140(24.2345) | Total Time 0.00(0.00)\n",
      "Iter 20660 | Time 29.4607(28.6855) | Bit/dim 3.9021(3.9345) | Xent 0.4948(0.5127) | Loss 10.3484(10.9151) | Error 0.1767(0.1782) Steps 1096(1074.22) | Grad Norm 10.0968(20.4325) | Total Time 0.00(0.00)\n",
      "Iter 20670 | Time 28.8572(28.7506) | Bit/dim 3.9443(3.9316) | Xent 0.4539(0.5038) | Loss 10.4029(10.7860) | Error 0.1511(0.1756) Steps 1102(1078.14) | Grad Norm 7.5138(25.4127) | Total Time 0.00(0.00)\n",
      "Iter 20680 | Time 29.5431(28.8610) | Bit/dim 3.9009(3.9229) | Xent 0.4763(0.4955) | Loss 10.3567(10.6568) | Error 0.1744(0.1728) Steps 1048(1078.73) | Grad Norm 10.6107(25.7114) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0376 | Time 122.8574, Epoch Time 1729.2406(1780.6618), Bit/dim 3.9181(best: 3.5709), Xent 0.7545, Loss 4.2954, Error 0.2379(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 20690 | Time 29.0395(28.9135) | Bit/dim 3.9196(3.9172) | Xent 0.5210(0.4889) | Loss 10.5564(11.4023) | Error 0.1878(0.1715) Steps 1108(1083.86) | Grad Norm 11.0267(25.4838) | Total Time 0.00(0.00)\n",
      "Iter 20700 | Time 28.1018(28.9255) | Bit/dim 3.9259(3.9120) | Xent 0.4611(0.4831) | Loss 10.3521(11.1289) | Error 0.1622(0.1690) Steps 1084(1086.48) | Grad Norm 9.6683(23.8499) | Total Time 0.00(0.00)\n",
      "Iter 20710 | Time 29.7659(28.8945) | Bit/dim 3.8813(3.9089) | Xent 0.4504(0.4798) | Loss 10.2704(10.9107) | Error 0.1578(0.1673) Steps 1120(1085.87) | Grad Norm 9.8266(25.2414) | Total Time 0.00(0.00)\n",
      "Iter 20720 | Time 28.3068(28.7781) | Bit/dim 3.9097(3.9059) | Xent 0.4317(0.4756) | Loss 10.3870(10.7546) | Error 0.1489(0.1661) Steps 1090(1086.31) | Grad Norm 8.4676(21.2049) | Total Time 0.00(0.00)\n",
      "Iter 20730 | Time 30.1746(28.8418) | Bit/dim 3.9309(3.9036) | Xent 0.4609(0.4720) | Loss 10.4304(10.6492) | Error 0.1656(0.1639) Steps 1102(1088.54) | Grad Norm 8.5120(17.9725) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0377 | Time 123.6473, Epoch Time 1733.1685(1779.2370), Bit/dim 3.8952(best: 3.5709), Xent 0.7531, Loss 4.2718, Error 0.2364(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 20740 | Time 28.3435(28.8713) | Bit/dim 3.8980(3.8975) | Xent 0.5631(0.4671) | Loss 10.4075(11.5245) | Error 0.2033(0.1623) Steps 1090(1092.69) | Grad Norm 10.7657(15.9010) | Total Time 0.00(0.00)\n",
      "Iter 20750 | Time 29.8453(28.8198) | Bit/dim 3.8788(3.8931) | Xent 0.4445(0.4576) | Loss 10.3181(11.1904) | Error 0.1511(0.1592) Steps 1126(1091.58) | Grad Norm 8.3228(14.2872) | Total Time 0.00(0.00)\n",
      "Iter 20760 | Time 28.2740(28.8478) | Bit/dim 3.8633(3.8898) | Xent 0.4275(0.4551) | Loss 10.1446(10.9632) | Error 0.1489(0.1583) Steps 1084(1092.58) | Grad Norm 11.2680(13.3842) | Total Time 0.00(0.00)\n",
      "Iter 20770 | Time 29.1027(28.9155) | Bit/dim 3.8772(3.8866) | Xent 0.3815(0.4471) | Loss 10.3366(10.7849) | Error 0.1444(0.1554) Steps 1096(1091.16) | Grad Norm 9.8284(17.2242) | Total Time 0.00(0.00)\n",
      "Iter 20780 | Time 30.4285(28.9725) | Bit/dim 3.8662(3.8836) | Xent 0.4646(0.4415) | Loss 10.2768(10.6451) | Error 0.1678(0.1531) Steps 1144(1096.61) | Grad Norm 10.1169(15.4689) | Total Time 0.00(0.00)\n",
      "Iter 20790 | Time 28.8848(28.9653) | Bit/dim 3.8510(3.8803) | Xent 0.4428(0.4386) | Loss 10.1903(10.5398) | Error 0.1556(0.1523) Steps 1090(1095.94) | Grad Norm 9.0783(14.1283) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0378 | Time 123.3164, Epoch Time 1731.1201(1777.7935), Bit/dim 3.8743(best: 3.5709), Xent 0.7497, Loss 4.2491, Error 0.2328(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 20800 | Time 29.7149(29.0338) | Bit/dim 3.8517(3.8782) | Xent 0.3820(0.4307) | Loss 10.0646(11.2656) | Error 0.1378(0.1491) Steps 1102(1096.87) | Grad Norm 8.3513(13.6103) | Total Time 0.00(0.00)\n",
      "Iter 20810 | Time 30.4242(29.1463) | Bit/dim 3.8588(3.8729) | Xent 0.4006(0.4266) | Loss 10.1345(10.9905) | Error 0.1433(0.1485) Steps 1114(1095.74) | Grad Norm 10.5348(13.2925) | Total Time 0.00(0.00)\n",
      "Iter 20820 | Time 29.9469(29.2582) | Bit/dim 3.8688(3.8698) | Xent 0.4377(0.4234) | Loss 10.3880(10.7906) | Error 0.1567(0.1475) Steps 1102(1098.71) | Grad Norm 11.2451(12.7452) | Total Time 0.00(0.00)\n",
      "Iter 20830 | Time 29.6860(29.2767) | Bit/dim 3.8366(3.8652) | Xent 0.4532(0.4220) | Loss 10.3559(10.6523) | Error 0.1489(0.1470) Steps 1126(1101.32) | Grad Norm 8.1792(11.8910) | Total Time 0.00(0.00)\n",
      "Iter 20840 | Time 28.1177(29.2523) | Bit/dim 3.8568(3.8632) | Xent 0.4131(0.4208) | Loss 10.2243(10.5387) | Error 0.1400(0.1469) Steps 1084(1102.06) | Grad Norm 8.7492(11.8754) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0379 | Time 124.4946, Epoch Time 1754.1670(1777.0847), Bit/dim 3.8601(best: 3.5709), Xent 0.7466, Loss 4.2334, Error 0.2307(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 20850 | Time 29.8070(29.2505) | Bit/dim 3.8337(3.8609) | Xent 0.4241(0.4152) | Loss 10.2132(11.4169) | Error 0.1478(0.1442) Steps 1072(1101.07) | Grad Norm 9.1826(10.9796) | Total Time 0.00(0.00)\n",
      "Iter 20860 | Time 29.6349(29.4441) | Bit/dim 3.8583(3.8562) | Xent 0.3760(0.4114) | Loss 10.1842(11.0934) | Error 0.1233(0.1426) Steps 1120(1103.79) | Grad Norm 9.4585(10.6518) | Total Time 0.00(0.00)\n",
      "Iter 20870 | Time 29.1964(29.4295) | Bit/dim 3.8341(3.8543) | Xent 0.3776(0.4082) | Loss 10.0485(10.8450) | Error 0.1222(0.1412) Steps 1084(1100.85) | Grad Norm 9.1667(10.3446) | Total Time 0.00(0.00)\n",
      "Iter 20880 | Time 28.4731(29.3655) | Bit/dim 3.8420(3.8513) | Xent 0.4126(0.4078) | Loss 10.2661(10.6705) | Error 0.1467(0.1408) Steps 1108(1103.25) | Grad Norm 10.5120(11.6982) | Total Time 0.00(0.00)\n",
      "Iter 20890 | Time 29.0169(29.3946) | Bit/dim 3.8457(3.8491) | Xent 0.3970(0.4069) | Loss 10.2197(10.5381) | Error 0.1444(0.1410) Steps 1114(1106.02) | Grad Norm 9.4872(25.8535) | Total Time 0.00(0.00)\n",
      "Iter 20900 | Time 29.7156(29.4910) | Bit/dim 3.8406(3.8460) | Xent 0.3891(0.4049) | Loss 10.1770(10.4602) | Error 0.1389(0.1405) Steps 1102(1110.54) | Grad Norm 21.7510(25.9724) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0380 | Time 126.2636, Epoch Time 1770.2295(1776.8790), Bit/dim 3.8429(best: 3.5709), Xent 0.7537, Loss 4.2197, Error 0.2297(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 20910 | Time 30.5188(29.5443) | Bit/dim 3.8420(3.8433) | Xent 0.3779(0.4014) | Loss 10.2019(11.2337) | Error 0.1400(0.1407) Steps 1162(1114.57) | Grad Norm 10.1729(24.1201) | Total Time 0.00(0.00)\n",
      "Iter 20920 | Time 29.6869(29.4483) | Bit/dim 3.8592(3.8430) | Xent 0.3719(0.3984) | Loss 10.1800(10.9478) | Error 0.1256(0.1391) Steps 1084(1110.11) | Grad Norm 9.1507(33.4902) | Total Time 0.00(0.00)\n",
      "Iter 20930 | Time 30.2109(29.3799) | Bit/dim 3.8230(3.8378) | Xent 0.3608(0.3937) | Loss 10.0760(10.7218) | Error 0.1356(0.1380) Steps 1126(1109.88) | Grad Norm 130.9155(34.0859) | Total Time 0.00(0.00)\n",
      "Iter 20940 | Time 29.6452(29.4541) | Bit/dim 3.8389(3.8384) | Xent 0.4566(0.3923) | Loss 10.3109(10.5806) | Error 0.1600(0.1367) Steps 1132(1109.73) | Grad Norm 18.0844(29.0299) | Total Time 0.00(0.00)\n",
      "Iter 20950 | Time 30.0117(29.5611) | Bit/dim 3.8237(3.8344) | Xent 0.4250(0.3877) | Loss 10.2038(10.4609) | Error 0.1467(0.1355) Steps 1132(1113.59) | Grad Norm 13.3933(31.2895) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0381 | Time 124.4616, Epoch Time 1766.8924(1776.5794), Bit/dim 3.8322(best: 3.5709), Xent 0.7533, Loss 4.2089, Error 0.2292(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 20960 | Time 29.8545(29.6370) | Bit/dim 3.8106(3.8326) | Xent 0.3701(0.3833) | Loss 9.9036(11.3084) | Error 0.1233(0.1342) Steps 1090(1113.65) | Grad Norm 34.3737(28.0291) | Total Time 0.00(0.00)\n",
      "Iter 20970 | Time 29.4903(29.7654) | Bit/dim 3.8349(3.8293) | Xent 0.3777(0.3781) | Loss 10.1252(10.9957) | Error 0.1411(0.1332) Steps 1090(1116.03) | Grad Norm 9.6977(34.5981) | Total Time 0.00(0.00)\n",
      "Iter 20980 | Time 29.3851(29.7550) | Bit/dim 3.8214(3.8290) | Xent 0.3418(0.3735) | Loss 10.0927(10.7636) | Error 0.1244(0.1312) Steps 1072(1112.24) | Grad Norm 8.6742(32.9707) | Total Time 0.00(0.00)\n",
      "Iter 20990 | Time 29.8444(29.8085) | Bit/dim 3.8196(3.8282) | Xent 0.4072(0.3741) | Loss 10.1163(10.5888) | Error 0.1322(0.1303) Steps 1120(1110.51) | Grad Norm 15.7363(37.6523) | Total Time 0.00(0.00)\n",
      "Iter 21000 | Time 30.3374(29.8367) | Bit/dim 3.8218(3.8247) | Xent 0.4301(0.3723) | Loss 10.1633(10.4621) | Error 0.1567(0.1298) Steps 1102(1115.13) | Grad Norm 76.2266(40.5595) | Total Time 0.00(0.00)\n",
      "Iter 21010 | Time 29.9941(29.9120) | Bit/dim 3.8272(3.8215) | Xent 0.3640(0.3703) | Loss 10.2026(10.3753) | Error 0.1300(0.1302) Steps 1114(1116.73) | Grad Norm 25.1178(40.5401) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0382 | Time 124.8590, Epoch Time 1791.2138(1777.0185), Bit/dim 3.8218(best: 3.5709), Xent 0.7632, Loss 4.2033, Error 0.2310(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 21020 | Time 30.5402(29.9387) | Bit/dim 3.7998(3.8199) | Xent 0.3880(0.3677) | Loss 10.1245(11.1354) | Error 0.1411(0.1290) Steps 1138(1115.90) | Grad Norm 31.0327(34.8805) | Total Time 0.00(0.00)\n",
      "Iter 21030 | Time 31.7901(30.0219) | Bit/dim 3.8181(3.8168) | Xent 0.3913(0.3646) | Loss 10.2074(10.8510) | Error 0.1344(0.1285) Steps 1204(1119.79) | Grad Norm 12.7856(33.7747) | Total Time 0.00(0.00)\n",
      "Iter 21040 | Time 29.7535(30.0908) | Bit/dim 3.8287(3.8157) | Xent 0.3469(0.3616) | Loss 10.2449(10.6544) | Error 0.1256(0.1269) Steps 1150(1124.15) | Grad Norm 10.2830(57.1554) | Total Time 0.00(0.00)\n",
      "Iter 21050 | Time 31.5933(30.2063) | Bit/dim 3.7674(3.8128) | Xent 0.3773(0.3579) | Loss 9.9793(10.5010) | Error 0.1267(0.1247) Steps 1096(1122.58) | Grad Norm 81.8512(126.5861) | Total Time 0.00(0.00)\n",
      "Iter 21060 | Time 30.0211(30.0973) | Bit/dim 3.8053(3.8127) | Xent 0.3856(0.3563) | Loss 10.0772(10.3854) | Error 0.1378(0.1242) Steps 1120(1125.29) | Grad Norm 116.3690(110.9541) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0383 | Time 126.1983, Epoch Time 1806.1209(1777.8915), Bit/dim 3.8124(best: 3.5709), Xent 0.7547, Loss 4.1898, Error 0.2262(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 21070 | Time 29.2427(30.2029) | Bit/dim 3.8050(3.8106) | Xent 0.3487(0.3536) | Loss 9.9315(11.2655) | Error 0.1256(0.1234) Steps 1114(1127.14) | Grad Norm 9.3179(101.9064) | Total Time 0.00(0.00)\n",
      "Iter 21080 | Time 30.1830(30.1490) | Bit/dim 3.8069(3.8094) | Xent 0.3279(0.3520) | Loss 9.9769(10.9383) | Error 0.1167(0.1235) Steps 1156(1125.60) | Grad Norm 56.6027(88.6488) | Total Time 0.00(0.00)\n",
      "Iter 21090 | Time 30.3015(30.2492) | Bit/dim 3.7818(3.8072) | Xent 0.3823(0.3523) | Loss 10.1565(10.7109) | Error 0.1411(0.1241) Steps 1114(1127.14) | Grad Norm 12.5757(75.0284) | Total Time 0.00(0.00)\n",
      "Iter 21100 | Time 30.0411(30.2219) | Bit/dim 3.7962(3.8036) | Xent 0.3691(0.3517) | Loss 9.9580(10.5346) | Error 0.1322(0.1237) Steps 1090(1125.88) | Grad Norm 24.0523(63.5877) | Total Time 0.00(0.00)\n",
      "Iter 21110 | Time 30.0410(30.2410) | Bit/dim 3.8024(3.8037) | Xent 0.3256(0.3496) | Loss 10.0727(10.4174) | Error 0.1111(0.1221) Steps 1120(1125.41) | Grad Norm 123.9452(61.2820) | Total Time 0.00(0.00)\n",
      "Iter 21120 | Time 30.4333(30.2843) | Bit/dim 3.7847(3.8026) | Xent 0.3792(0.3486) | Loss 9.8949(10.3165) | Error 0.1222(0.1216) Steps 1144(1128.35) | Grad Norm 8.3235(53.3792) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0384 | Time 125.7198, Epoch Time 1809.3184(1778.8343), Bit/dim 3.8054(best: 3.5709), Xent 0.7662, Loss 4.1885, Error 0.2287(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 21130 | Time 30.0660(30.3695) | Bit/dim 3.8255(3.8023) | Xent 0.3200(0.3441) | Loss 10.0879(11.0818) | Error 0.1122(0.1192) Steps 1150(1132.92) | Grad Norm 26.7538(56.0480) | Total Time 0.00(0.00)\n",
      "Iter 21140 | Time 31.0733(30.3592) | Bit/dim 3.7932(3.8012) | Xent 0.3176(0.3406) | Loss 10.1599(10.8164) | Error 0.1056(0.1181) Steps 1108(1132.74) | Grad Norm 15.1421(62.1563) | Total Time 0.00(0.00)\n",
      "Iter 21150 | Time 30.3412(30.3999) | Bit/dim 3.7988(3.7985) | Xent 0.3840(0.3436) | Loss 10.0701(10.6082) | Error 0.1300(0.1182) Steps 1138(1132.43) | Grad Norm 19.5522(62.3636) | Total Time 0.00(0.00)\n",
      "Iter 21160 | Time 30.4652(30.4303) | Bit/dim 3.7911(3.7964) | Xent 0.2983(0.3383) | Loss 9.9953(10.4435) | Error 0.0978(0.1171) Steps 1120(1130.32) | Grad Norm 13.2072(53.3784) | Total Time 0.00(0.00)\n",
      "Iter 21170 | Time 30.0361(30.4860) | Bit/dim 3.7961(3.7941) | Xent 0.3253(0.3346) | Loss 9.9969(10.3289) | Error 0.1200(0.1166) Steps 1066(1129.54) | Grad Norm 32.2251(76.0685) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0385 | Time 125.7337, Epoch Time 1821.6696(1780.1194), Bit/dim 3.7956(best: 3.5709), Xent 0.7759, Loss 4.1836, Error 0.2290(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 21180 | Time 30.8544(30.4778) | Bit/dim 3.8253(3.7921) | Xent 0.3388(0.3314) | Loss 10.1021(11.2245) | Error 0.1156(0.1155) Steps 1108(1127.91) | Grad Norm 19.4365(65.3227) | Total Time 0.00(0.00)\n",
      "Iter 21190 | Time 30.8680(30.5736) | Bit/dim 3.7961(3.7901) | Xent 0.3411(0.3306) | Loss 10.0989(10.9131) | Error 0.1244(0.1155) Steps 1126(1130.05) | Grad Norm 63.6561(74.3291) | Total Time 0.00(0.00)\n",
      "Iter 21200 | Time 31.5928(30.5722) | Bit/dim 3.7725(3.7861) | Xent 0.3192(0.3297) | Loss 10.0454(10.6691) | Error 0.1111(0.1156) Steps 1096(1127.64) | Grad Norm 64.3407(73.6714) | Total Time 0.00(0.00)\n",
      "Iter 21210 | Time 30.8353(30.6846) | Bit/dim 3.7632(3.7844) | Xent 0.3255(0.3274) | Loss 9.9746(10.4855) | Error 0.1167(0.1146) Steps 1096(1128.20) | Grad Norm 24.6983(66.4139) | Total Time 0.00(0.00)\n",
      "Iter 21220 | Time 30.4083(30.6618) | Bit/dim 3.7794(3.7847) | Xent 0.3025(0.3210) | Loss 9.8929(10.3548) | Error 0.1022(0.1122) Steps 1114(1128.16) | Grad Norm 906.2384(107.7928) | Total Time 0.00(0.00)\n",
      "Iter 21230 | Time 30.2397(30.6680) | Bit/dim 3.7882(3.7843) | Xent 0.3199(0.3236) | Loss 9.9665(10.2744) | Error 0.1078(0.1125) Steps 1114(1129.35) | Grad Norm 114.7439(94.6582) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0386 | Time 127.0808, Epoch Time 1834.6471(1781.7552), Bit/dim 3.7849(best: 3.5709), Xent 0.7693, Loss 4.1696, Error 0.2299(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 21240 | Time 31.5003(30.7421) | Bit/dim 3.7617(3.7829) | Xent 0.3417(0.3219) | Loss 10.0465(11.0648) | Error 0.1178(0.1115) Steps 1144(1132.03) | Grad Norm 32.3831(85.2297) | Total Time 0.00(0.00)\n",
      "Iter 21250 | Time 31.6681(30.8011) | Bit/dim 3.7617(3.7778) | Xent 0.3147(0.3188) | Loss 10.0376(10.7685) | Error 0.1022(0.1108) Steps 1180(1134.53) | Grad Norm 113.6424(82.5697) | Total Time 0.00(0.00)\n",
      "Iter 21260 | Time 31.5797(30.7801) | Bit/dim 3.7387(3.7750) | Xent 0.2981(0.3157) | Loss 9.9055(10.5515) | Error 0.1033(0.1102) Steps 1126(1133.37) | Grad Norm 20.2939(87.0182) | Total Time 0.00(0.00)\n",
      "Iter 21270 | Time 33.0082(30.9893) | Bit/dim 3.7658(3.7735) | Xent 0.3303(0.3161) | Loss 9.9577(10.3999) | Error 0.1178(0.1107) Steps 1138(1134.06) | Grad Norm 40.9875(74.6731) | Total Time 0.00(0.00)\n",
      "Iter 21280 | Time 30.7534(30.9134) | Bit/dim 3.7687(3.7740) | Xent 0.3130(0.3127) | Loss 9.9716(10.2801) | Error 0.1022(0.1092) Steps 1126(1132.62) | Grad Norm 22.6400(69.8646) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0387 | Time 127.3333, Epoch Time 1848.4502(1783.7561), Bit/dim 3.7687(best: 3.5709), Xent 0.7747, Loss 4.1560, Error 0.2246(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 21290 | Time 31.4057(30.9183) | Bit/dim 3.7798(3.7724) | Xent 0.3189(0.3139) | Loss 9.8978(11.2072) | Error 0.1156(0.1113) Steps 1114(1133.68) | Grad Norm 14.0108(63.3569) | Total Time 0.00(0.00)\n",
      "Iter 21300 | Time 31.3104(30.9549) | Bit/dim 3.7681(3.7704) | Xent 0.2515(0.3065) | Loss 9.9787(10.8716) | Error 0.0922(0.1085) Steps 1138(1135.00) | Grad Norm 8.8395(64.5194) | Total Time 0.00(0.00)\n",
      "Iter 21310 | Time 31.6616(31.0073) | Bit/dim 3.7702(3.7666) | Xent 0.2947(0.3034) | Loss 9.9879(10.6245) | Error 0.1011(0.1073) Steps 1120(1136.17) | Grad Norm 9.2649(65.1795) | Total Time 0.00(0.00)\n",
      "Iter 21320 | Time 31.5508(31.0465) | Bit/dim 3.7309(3.7612) | Xent 0.3109(0.3041) | Loss 10.0094(10.4461) | Error 0.0989(0.1080) Steps 1150(1136.86) | Grad Norm 73.0821(61.5787) | Total Time 0.00(0.00)\n",
      "Iter 21330 | Time 31.5442(31.0766) | Bit/dim 3.7576(3.7609) | Xent 0.2942(0.3027) | Loss 10.0718(10.3226) | Error 0.0911(0.1071) Steps 1156(1140.22) | Grad Norm 25.1066(66.4431) | Total Time 0.00(0.00)\n",
      "Iter 21340 | Time 31.1436(31.1496) | Bit/dim 3.7834(3.7584) | Xent 0.3322(0.3033) | Loss 9.9603(10.2328) | Error 0.1133(0.1065) Steps 1168(1141.18) | Grad Norm 44.5705(61.8639) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0388 | Time 126.0920, Epoch Time 1857.5330(1785.9694), Bit/dim 3.7688(best: 3.5709), Xent 0.7819, Loss 4.1598, Error 0.2273(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 21350 | Time 30.1959(31.1861) | Bit/dim 3.7870(3.7604) | Xent 0.2811(0.3003) | Loss 9.9513(11.0154) | Error 0.1000(0.1056) Steps 1126(1142.39) | Grad Norm 112.0671(76.9866) | Total Time 0.00(0.00)\n",
      "Iter 21360 | Time 31.8701(31.2152) | Bit/dim 3.7596(3.7594) | Xent 0.2962(0.2971) | Loss 9.9780(10.7254) | Error 0.1000(0.1041) Steps 1168(1144.00) | Grad Norm 26.2028(94.1278) | Total Time 0.00(0.00)\n",
      "Iter 21370 | Time 31.1517(31.3577) | Bit/dim 3.7268(3.7601) | Xent 0.2845(0.2949) | Loss 9.6945(10.5144) | Error 0.0911(0.1036) Steps 1132(1145.46) | Grad Norm 160.8578(91.3629) | Total Time 0.00(0.00)\n",
      "Iter 21380 | Time 31.3698(31.4447) | Bit/dim 3.7769(3.7627) | Xent 0.3040(0.2916) | Loss 10.0774(10.3684) | Error 0.1100(0.1024) Steps 1174(1145.72) | Grad Norm 390.9681(103.0991) | Total Time 0.00(0.00)\n",
      "Iter 21390 | Time 31.0657(31.5632) | Bit/dim 3.7751(3.7608) | Xent 0.3082(0.2930) | Loss 10.0224(10.2557) | Error 0.1089(0.1033) Steps 1156(1151.55) | Grad Norm 83.3712(96.6318) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0389 | Time 127.7593, Epoch Time 1883.3218(1788.8900), Bit/dim 3.7695(best: 3.5709), Xent 0.7843, Loss 4.1616, Error 0.2270(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 21400 | Time 31.5499(31.5375) | Bit/dim 3.7456(3.7594) | Xent 0.3111(0.2928) | Loss 9.8254(11.1448) | Error 0.1011(0.1028) Steps 1162(1151.13) | Grad Norm 57.5192(95.4810) | Total Time 0.00(0.00)\n",
      "Iter 21410 | Time 32.4290(31.7166) | Bit/dim 3.7501(3.7586) | Xent 0.2985(0.2896) | Loss 9.8738(10.8311) | Error 0.1200(0.1025) Steps 1210(1154.28) | Grad Norm 61.9101(88.2391) | Total Time 0.00(0.00)\n",
      "Iter 21420 | Time 31.0388(31.7091) | Bit/dim 3.7516(3.7592) | Xent 0.2794(0.2878) | Loss 9.8234(10.6051) | Error 0.1022(0.1019) Steps 1162(1156.97) | Grad Norm 75.0313(90.7608) | Total Time 0.00(0.00)\n",
      "Iter 21430 | Time 32.1696(31.6963) | Bit/dim 3.7463(3.7571) | Xent 0.2922(0.2856) | Loss 9.9420(10.4331) | Error 0.0956(0.1004) Steps 1144(1157.85) | Grad Norm 29.4622(104.6690) | Total Time 0.00(0.00)\n",
      "Iter 21440 | Time 32.9147(31.6883) | Bit/dim 3.7660(3.7602) | Xent 0.3120(0.2901) | Loss 9.9127(10.3127) | Error 0.1111(0.1012) Steps 1120(1154.39) | Grad Norm 134.3891(96.2315) | Total Time 0.00(0.00)\n",
      "Iter 21450 | Time 31.7232(31.7052) | Bit/dim 3.7538(3.7600) | Xent 0.3124(0.2909) | Loss 10.0156(10.2212) | Error 0.1178(0.1014) Steps 1156(1156.24) | Grad Norm 131.3732(91.8565) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0390 | Time 127.7806, Epoch Time 1892.6189(1792.0018), Bit/dim 3.7695(best: 3.5709), Xent 0.7859, Loss 4.1625, Error 0.2262(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 21460 | Time 32.1693(31.7394) | Bit/dim 3.7551(3.7579) | Xent 0.2783(0.2860) | Loss 9.8576(10.9971) | Error 0.0878(0.1002) Steps 1108(1154.10) | Grad Norm 51.9228(86.6132) | Total Time 0.00(0.00)\n",
      "Iter 21470 | Time 32.7371(31.7447) | Bit/dim 3.7737(3.7607) | Xent 0.2707(0.2874) | Loss 9.9331(10.7294) | Error 0.0933(0.1004) Steps 1162(1156.20) | Grad Norm 39.0171(90.1037) | Total Time 0.00(0.00)\n",
      "Iter 21480 | Time 31.5213(31.7081) | Bit/dim 3.7574(3.7609) | Xent 0.2831(0.2860) | Loss 9.8954(10.5235) | Error 0.1033(0.0996) Steps 1144(1154.65) | Grad Norm 202.3866(94.9001) | Total Time 0.00(0.00)\n",
      "Iter 21490 | Time 32.1693(31.7520) | Bit/dim 3.7844(3.7600) | Xent 0.2921(0.2862) | Loss 9.9501(10.3575) | Error 0.1089(0.1003) Steps 1156(1153.06) | Grad Norm 343.3315(96.8616) | Total Time 0.00(0.00)\n",
      "Iter 21500 | Time 32.3074(31.7441) | Bit/dim 3.7494(3.7592) | Xent 0.2515(0.2872) | Loss 9.9415(10.2517) | Error 0.0922(0.1003) Steps 1156(1153.16) | Grad Norm 10.7161(107.9204) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0391 | Time 127.9551, Epoch Time 1894.5335(1795.0778), Bit/dim 3.7672(best: 3.5709), Xent 0.7907, Loss 4.1626, Error 0.2268(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 21510 | Time 31.6412(31.7853) | Bit/dim 3.7578(3.7603) | Xent 0.3207(0.2839) | Loss 9.9184(11.1841) | Error 0.1189(0.1003) Steps 1156(1157.03) | Grad Norm 68.2138(94.6049) | Total Time 0.00(0.00)\n",
      "Iter 21520 | Time 31.6147(31.8172) | Bit/dim 3.7736(3.7617) | Xent 0.2306(0.2827) | Loss 9.7943(10.8660) | Error 0.0800(0.0987) Steps 1168(1159.81) | Grad Norm 52.7712(86.6193) | Total Time 0.00(0.00)\n",
      "Iter 21530 | Time 32.2695(31.6998) | Bit/dim 3.7690(3.7605) | Xent 0.2827(0.2825) | Loss 9.9658(10.6126) | Error 0.0944(0.0986) Steps 1186(1157.45) | Grad Norm 184.0621(80.9832) | Total Time 0.00(0.00)\n",
      "Iter 21540 | Time 30.8291(31.6994) | Bit/dim 3.7758(3.7612) | Xent 0.3052(0.2802) | Loss 9.9742(10.4297) | Error 0.1078(0.0983) Steps 1132(1154.75) | Grad Norm 118.8589(96.6204) | Total Time 0.00(0.00)\n",
      "Iter 21550 | Time 31.9740(31.8067) | Bit/dim 3.7953(3.7617) | Xent 0.2926(0.2812) | Loss 9.9539(10.3045) | Error 0.1056(0.0986) Steps 1186(1155.46) | Grad Norm 276.2556(98.9048) | Total Time 0.00(0.00)\n",
      "Iter 21560 | Time 31.4701(31.7106) | Bit/dim 3.7493(3.7595) | Xent 0.3024(0.2823) | Loss 9.6661(10.1834) | Error 0.1044(0.0984) Steps 1120(1151.19) | Grad Norm 168.1317(114.4905) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0392 | Time 127.5452, Epoch Time 1890.4589(1797.9392), Bit/dim 3.7713(best: 3.5709), Xent 0.7986, Loss 4.1706, Error 0.2280(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 21570 | Time 31.1850(31.7965) | Bit/dim 3.7686(3.7596) | Xent 0.2917(0.2814) | Loss 9.9552(10.9703) | Error 0.0989(0.0978) Steps 1132(1151.78) | Grad Norm 94.6862(100.4741) | Total Time 0.00(0.00)\n",
      "Iter 21580 | Time 32.7271(31.8581) | Bit/dim 3.7660(3.7577) | Xent 0.2303(0.2767) | Loss 9.9888(10.6926) | Error 0.0800(0.0954) Steps 1162(1152.40) | Grad Norm 29.7399(98.2634) | Total Time 0.00(0.00)\n",
      "Iter 21590 | Time 31.2699(31.7430) | Bit/dim 3.7311(3.7564) | Xent 0.2555(0.2757) | Loss 9.8197(10.4862) | Error 0.0911(0.0959) Steps 1120(1147.70) | Grad Norm 31.1809(111.2564) | Total Time 0.00(0.00)\n",
      "Iter 21600 | Time 31.7418(31.6877) | Bit/dim 3.7487(3.7524) | Xent 0.2547(0.2797) | Loss 9.8818(10.3194) | Error 0.1011(0.0976) Steps 1126(1146.45) | Grad Norm 35.1948(97.8923) | Total Time 0.00(0.00)\n",
      "Iter 21610 | Time 31.0666(31.6886) | Bit/dim 3.7482(3.7511) | Xent 0.2111(0.2745) | Loss 9.8403(10.2133) | Error 0.0744(0.0955) Steps 1144(1150.12) | Grad Norm 105.7929(94.3634) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0393 | Time 125.6227, Epoch Time 1888.6200(1800.6596), Bit/dim 3.7565(best: 3.5709), Xent 0.7910, Loss 4.1520, Error 0.2262(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 21620 | Time 31.9544(31.7013) | Bit/dim 3.7317(3.7491) | Xent 0.2336(0.2704) | Loss 9.7689(11.1231) | Error 0.0856(0.0937) Steps 1168(1147.78) | Grad Norm 70.5319(87.7860) | Total Time 0.00(0.00)\n",
      "Iter 21630 | Time 31.7398(31.6414) | Bit/dim 3.7510(3.7479) | Xent 0.2740(0.2695) | Loss 9.8537(10.8021) | Error 0.1022(0.0943) Steps 1138(1146.60) | Grad Norm 314.7995(91.7318) | Total Time 0.00(0.00)\n",
      "Iter 21640 | Time 31.0114(31.5438) | Bit/dim 3.7365(3.7472) | Xent 0.2942(0.2706) | Loss 9.9144(10.5582) | Error 0.1056(0.0956) Steps 1162(1145.74) | Grad Norm 38.5016(88.9074) | Total Time 0.00(0.00)\n",
      "Iter 21650 | Time 31.2263(31.5455) | Bit/dim 3.7560(3.7472) | Xent 0.2374(0.2703) | Loss 9.8331(10.3789) | Error 0.0800(0.0939) Steps 1126(1145.95) | Grad Norm 36.3659(82.7031) | Total Time 0.00(0.00)\n",
      "Iter 21660 | Time 30.6513(31.5147) | Bit/dim 3.7436(3.7477) | Xent 0.2524(0.2717) | Loss 9.9008(10.2571) | Error 0.0889(0.0945) Steps 1162(1145.04) | Grad Norm 409.7884(118.5466) | Total Time 0.00(0.00)\n",
      "Iter 21670 | Time 31.9339(31.5621) | Bit/dim 3.7606(3.7482) | Xent 0.2913(0.2723) | Loss 9.9168(10.1615) | Error 0.0978(0.0945) Steps 1156(1147.23) | Grad Norm 82.4575(115.1350) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0394 | Time 128.0629, Epoch Time 1880.8050(1803.0640), Bit/dim 3.7606(best: 3.5709), Xent 0.8017, Loss 4.1614, Error 0.2278(best: 0.2180)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 21680 | Time 31.4023(31.5432) | Bit/dim 3.7383(3.7487) | Xent 0.2894(0.2689) | Loss 9.9427(10.9421) | Error 0.0922(0.0938) Steps 1150(1147.73) | Grad Norm 332.2389(106.1965) | Total Time 0.00(0.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_rl.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_6_run1 --resume ../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_6_run1/epoch_270_checkpt.pth --seed 1 --lr 0.0001 --conditional True --controlled_tol False --train_mode semisup --log_freq 10 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5 --scale_fac 1.0 --scale_std 6.0\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
