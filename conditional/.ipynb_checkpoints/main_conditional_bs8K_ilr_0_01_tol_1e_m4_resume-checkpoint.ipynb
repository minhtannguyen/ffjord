{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_conditional.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.odenvp_conditional as odenvp\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=True, choices=[True, False])\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"./data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"./data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"./data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"./data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"./data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"./data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            './data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            './data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz = modules.GaussianDiag.logp(mean, logs, z).view(-1,1)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(z)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol},)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    # fixed_y = torch.randint(high=10, size=(100,)).type(torch.long).to(device, non_blocking=True)\n",
      "    fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "    fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "    with torch.no_grad():\n",
      "        mean, logs = model.module._prior(fixed_y_onehot)\n",
      "        fixed_z = modules.GaussianDiag.sample(mean, logs)\n",
      "    \n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    if args.conditional: best_error_score = float(\"inf\")\n",
      "    \n",
      "    itr = 0\n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                loss =  loss_nll + args.weight_y * loss_xent\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            loss.backward()\n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            xent_meter.update(loss_xent.item())\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits/dim', {'train': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses_xent.append(loss_xent.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'validation': time.time() - start}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits/dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}, Xent {:.4f}, Loss {:.4f}, Error {:.4f}\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, loss_xent, loss, error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=8000, batch_size_schedule='', begin_epoch=1, conditional=True, conv=True, data='mnist', dims='64,64,64', divergence_fn='approximate', dl2int=None, imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=1, lr=0.01, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume=None, rtol=1e-05, save='experiments/cnf_cond_bs8K_ilr_0_01_tol_1e_m5_wy_0_5_dev', solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=8000, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, val_freq=1, warmup_iters=113.0, weight_decay=0.0, weight_y=0.5)\n",
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=1568, bias=True)\n",
      "  (project_class): LinearZeros(in_features=784, out_features=10, bias=True)\n",
      ")\n",
      "Number of trainable parameters: 828890\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc.py:286: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "Iter 0000 | Time 67.5607(67.5607) | Bit/dim 37.6669(37.6669) | Xent 2.3026(2.3026) | Loss 38.8182(38.8182) | Error 0.9006(0.9006) Steps 410(410.00) | Grad Norm 318.0935(318.0935) | Total Time 10.00(10.00)\n",
      "Iter 0001 | Time 27.6850(66.3644) | Bit/dim 31.6880(37.4875) | Xent 2.2600(2.3013) | Loss 32.8180(38.6382) | Error 0.8805(0.9000) Steps 410(410.00) | Grad Norm 267.5649(316.5777) | Total Time 10.00(10.00)\n",
      "Iter 0002 | Time 26.9250(65.1812) | Bit/dim 22.7393(37.0451) | Xent 2.1587(2.2970) | Loss 23.8187(38.1936) | Error 0.5320(0.8890) Steps 410(410.00) | Grad Norm 186.6928(312.6811) | Total Time 10.00(10.00)\n",
      "Iter 0003 | Time 26.2542(64.0134) | Bit/dim 15.0882(36.3864) | Xent 2.0387(2.2893) | Loss 16.1075(37.5310) | Error 0.5930(0.8801) Steps 410(410.00) | Grad Norm 93.4476(306.1041) | Total Time 10.00(10.00)\n",
      "Iter 0004 | Time 26.6900(62.8937) | Bit/dim 12.8170(35.6793) | Xent 1.8861(2.2772) | Loss 13.7601(36.8179) | Error 0.3729(0.8649) Steps 410(410.00) | Grad Norm 38.6307(298.0799) | Total Time 10.00(10.00)\n",
      "Iter 0005 | Time 27.4359(61.8300) | Bit/dim 16.3991(35.1009) | Xent 1.7850(2.2624) | Loss 17.2916(36.2321) | Error 0.4735(0.8531) Steps 410(410.00) | Grad Norm 122.5345(292.8135) | Total Time 10.00(10.00)\n",
      "Iter 0006 | Time 27.9005(60.8121) | Bit/dim 19.0115(34.6182) | Xent 1.6928(2.2453) | Loss 19.8579(35.7409) | Error 0.4363(0.8406) Steps 416(410.18) | Grad Norm 155.3810(288.6906) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0001 | Time 9.0577, Epoch Time 252.2077(252.2077), Bit/dim 17.1263, Xent 1.5303, Loss 17.8915, Error 0.4398\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0007 | Time 30.8661(59.9137) | Bit/dim 17.2134(34.0961) | Xent 1.5304(2.2239) | Loss 17.9786(35.2080) | Error 0.2979(0.8244) Steps 416(410.35) | Grad Norm 131.3521(283.9704) | Total Time 10.00(10.00)\n",
      "Iter 0008 | Time 27.9831(58.9558) | Bit/dim 13.3676(33.4742) | Xent 1.3869(2.1988) | Loss 14.0610(34.5736) | Error 0.2335(0.8066) Steps 416(410.52) | Grad Norm 89.7136(278.1427) | Total Time 10.00(10.00)\n",
      "Iter 0009 | Time 27.2514(58.0047) | Bit/dim 10.0498(32.7715) | Xent 1.2694(2.1709) | Loss 10.6846(33.8569) | Error 0.2484(0.7899) Steps 410(410.51) | Grad Norm 51.6648(271.3484) | Total Time 10.00(10.00)\n",
      "Iter 0010 | Time 27.0510(57.0760) | Bit/dim 8.0531(32.0299) | Xent 1.1818(2.1412) | Loss 8.6440(33.1005) | Error 0.2665(0.7742) Steps 410(410.49) | Grad Norm 24.1109(263.9312) | Total Time 10.00(10.00)\n",
      "Iter 0011 | Time 26.7238(56.1655) | Bit/dim 7.3335(31.2890) | Xent 1.0796(2.1094) | Loss 7.8733(32.3437) | Error 0.2440(0.7583) Steps 410(410.48) | Grad Norm 18.4882(256.5680) | Total Time 10.00(10.00)\n",
      "Iter 0012 | Time 27.3937(55.3023) | Bit/dim 7.2937(30.5692) | Xent 0.9790(2.0755) | Loss 7.7832(31.6069) | Error 0.2107(0.7418) Steps 410(410.46) | Grad Norm 29.0822(249.7434) | Total Time 10.00(10.00)\n",
      "Iter 0013 | Time 26.9821(54.4527) | Bit/dim 7.4145(29.8745) | Xent 0.9017(2.0402) | Loss 7.8654(30.8947) | Error 0.1944(0.7254) Steps 410(410.45) | Grad Norm 36.6991(243.3521) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0002 | Time 8.7626, Epoch Time 215.7491(251.1139), Bit/dim 7.1906, Xent 0.8679, Loss 7.6245, Error 0.3524\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0014 | Time 27.1664(53.6341) | Bit/dim 7.2135(29.1947) | Xent 0.8628(2.0049) | Loss 7.6449(30.1972) | Error 0.1865(0.7093) Steps 410(410.44) | Grad Norm 38.5822(237.2090) | Total Time 10.00(10.00)\n",
      "Iter 0015 | Time 26.8204(52.8297) | Bit/dim 6.5542(28.5155) | Xent 0.8519(1.9703) | Loss 6.9802(29.5007) | Error 0.1899(0.6937) Steps 410(410.42) | Grad Norm 35.9484(231.1711) | Total Time 10.00(10.00)\n",
      "Iter 0016 | Time 26.7286(52.0467) | Bit/dim 5.6471(27.8294) | Xent 0.8489(1.9367) | Loss 6.0716(28.7978) | Error 0.1914(0.6786) Steps 410(410.41) | Grad Norm 29.8924(225.1328) | Total Time 10.00(10.00)\n",
      "Iter 0017 | Time 27.1543(51.2999) | Bit/dim 4.7020(27.1356) | Xent 0.9184(1.9061) | Loss 5.1612(28.0887) | Error 0.2041(0.6644) Steps 410(410.40) | Grad Norm 22.3024(219.0479) | Total Time 10.00(10.00)\n",
      "Iter 0018 | Time 28.0477(50.6023) | Bit/dim 3.9143(26.4390) | Xent 1.0003(1.8790) | Loss 4.4145(27.3785) | Error 0.2199(0.6510) Steps 422(410.75) | Grad Norm 15.0783(212.9288) | Total Time 10.00(10.00)\n",
      "Iter 0019 | Time 28.6760(49.9446) | Bit/dim 3.3660(25.7468) | Xent 1.1541(1.8572) | Loss 3.9430(26.6754) | Error 0.2596(0.6393) Steps 422(411.08) | Grad Norm 9.7965(206.8348) | Total Time 10.00(10.00)\n",
      "Iter 0020 | Time 28.9970(49.3161) | Bit/dim 3.0219(25.0650) | Xent 1.3512(1.8420) | Loss 3.6975(25.9861) | Error 0.3264(0.6299) Steps 428(411.59) | Grad Norm 6.9444(200.8381) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0003 | Time 9.2875, Epoch Time 215.5199(250.0461), Bit/dim 2.8108, Xent 1.5478, Loss 3.5848, Error 0.5458\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0021 | Time 29.8864(48.7332) | Bit/dim 2.8313(24.3980) | Xent 1.5477(1.8332) | Loss 3.6051(25.3146) | Error 0.4314(0.6240) Steps 434(412.26) | Grad Norm 5.8814(194.9894) | Total Time 10.00(10.00)\n",
      "Iter 0022 | Time 31.2148(48.2077) | Bit/dim 2.7517(23.7486) | Xent 1.7322(1.8302) | Loss 3.6178(24.6637) | Error 0.5623(0.6221) Steps 440(413.10) | Grad Norm 6.4230(189.3324) | Total Time 10.00(10.00)\n",
      "Iter 0023 | Time 32.9505(47.7500) | Bit/dim 2.7395(23.1184) | Xent 1.8605(1.8311) | Loss 3.6697(24.0339) | Error 0.6458(0.6228) Steps 452(414.26) | Grad Norm 7.7834(183.8859) | Total Time 10.00(10.00)\n",
      "Iter 0024 | Time 36.4144(47.4099) | Bit/dim 2.7624(22.5077) | Xent 1.9481(1.8346) | Loss 3.7364(23.4250) | Error 0.6512(0.6237) Steps 470(415.94) | Grad Norm 8.6489(178.6288) | Total Time 10.00(10.00)\n",
      "Iter 0025 | Time 35.4631(47.0515) | Bit/dim 2.7513(21.9150) | Xent 2.0035(1.8397) | Loss 3.7530(22.8348) | Error 0.5631(0.6218) Steps 482(417.92) | Grad Norm 8.0057(173.5101) | Total Time 10.00(10.00)\n",
      "Iter 0026 | Time 38.3355(46.7900) | Bit/dim 2.7051(21.3387) | Xent 2.0443(1.8458) | Loss 3.7272(22.2616) | Error 0.5440(0.6195) Steps 500(420.38) | Grad Norm 6.6389(168.5040) | Total Time 10.00(10.00)\n",
      "Iter 0027 | Time 40.9102(46.6136) | Bit/dim 2.6498(20.7780) | Xent 2.0957(1.8533) | Loss 3.6976(21.7047) | Error 0.6558(0.6206) Steps 506(422.95) | Grad Norm 5.1780(163.6042) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0004 | Time 10.5326, Epoch Time 268.2303(250.5916), Bit/dim 2.5914, Xent 2.1328, Loss 3.6578, Error 0.7544\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0028 | Time 39.0192(46.3858) | Bit/dim 2.6085(20.2329) | Xent 2.1198(1.8613) | Loss 3.6684(21.1636) | Error 0.6775(0.6223) Steps 506(425.44) | Grad Norm 4.2670(158.8241) | Total Time 10.00(10.00)\n",
      "Iter 0029 | Time 39.0837(46.1667) | Bit/dim 2.5774(19.7033) | Xent 2.1382(1.8696) | Loss 3.6465(20.6381) | Error 0.6835(0.6241) Steps 506(427.86) | Grad Norm 4.1933(154.1852) | Total Time 10.00(10.00)\n",
      "Iter 0030 | Time 39.6232(45.9704) | Bit/dim 2.5607(19.1890) | Xent 2.1395(1.8777) | Loss 3.6304(20.1278) | Error 0.6867(0.6260) Steps 506(430.20) | Grad Norm 4.7395(149.7018) | Total Time 10.00(10.00)\n",
      "Iter 0031 | Time 40.0555(45.7930) | Bit/dim 2.5304(18.6892) | Xent 2.0869(1.8840) | Loss 3.5738(19.6312) | Error 0.6565(0.6269) Steps 506(432.47) | Grad Norm 5.2293(145.3676) | Total Time 10.00(10.00)\n",
      "Iter 0032 | Time 39.7502(45.6117) | Bit/dim 2.4748(18.2028) | Xent 2.0160(1.8879) | Loss 3.4828(19.1468) | Error 0.6205(0.6267) Steps 488(434.14) | Grad Norm 5.4507(141.1701) | Total Time 10.00(10.00)\n",
      "Iter 0033 | Time 37.4978(45.3683) | Bit/dim 2.4102(17.7290) | Xent 1.8840(1.8878) | Loss 3.3523(18.6729) | Error 0.5417(0.6242) Steps 494(435.94) | Grad Norm 5.4475(137.0984) | Total Time 10.00(10.00)\n",
      "Iter 0034 | Time 37.8223(45.1419) | Bit/dim 2.3324(17.2671) | Xent 1.7059(1.8824) | Loss 3.1853(18.2083) | Error 0.4566(0.6192) Steps 488(437.50) | Grad Norm 5.1833(133.1410) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0005 | Time 10.0103, Epoch Time 295.5534(251.9405), Bit/dim 2.2493, Xent 1.5025, Loss 3.0005, Error 0.4817\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0035 | Time 33.9660(44.8066) | Bit/dim 2.2649(16.8171) | Xent 1.5004(1.8709) | Loss 3.0151(17.7525) | Error 0.3659(0.6116) Steps 470(438.47) | Grad Norm 4.9227(129.2944) | Total Time 10.00(10.00)\n",
      "Iter 0036 | Time 33.6006(44.4704) | Bit/dim 2.2186(16.3791) | Xent 1.2761(1.8531) | Loss 2.8567(17.3056) | Error 0.2799(0.6016) Steps 470(439.42) | Grad Norm 4.6566(125.5553) | Total Time 10.00(10.00)\n",
      "Iter 0037 | Time 32.5813(44.1138) | Bit/dim 2.2108(15.9541) | Xent 1.0469(1.8289) | Loss 2.7342(16.8685) | Error 0.2442(0.5909) Steps 452(439.80) | Grad Norm 3.8898(121.9053) | Total Time 10.00(10.00)\n",
      "Iter 0038 | Time 31.1159(43.7238) | Bit/dim 2.2520(15.5430) | Xent 0.8428(1.7993) | Loss 2.6734(16.4426) | Error 0.2075(0.5794) Steps 440(439.80) | Grad Norm 2.7792(118.3316) | Total Time 10.00(10.00)\n",
      "Iter 0039 | Time 28.1057(43.2553) | Bit/dim 2.3185(15.1463) | Xent 0.6932(1.7661) | Loss 2.6651(16.0293) | Error 0.1930(0.5678) Steps 422(439.27) | Grad Norm 4.7053(114.9228) | Total Time 10.00(10.00)\n",
      "Iter 0040 | Time 28.1576(42.8023) | Bit/dim 2.4115(14.7642) | Xent 0.6255(1.7319) | Loss 2.7243(15.6302) | Error 0.1849(0.5563) Steps 416(438.57) | Grad Norm 9.9965(111.7750) | Total Time 10.00(10.00)\n",
      "Iter 0041 | Time 27.2579(42.3360) | Bit/dim 2.3939(14.3931) | Xent 0.5716(1.6971) | Loss 2.6797(15.2416) | Error 0.1699(0.5447) Steps 416(437.89) | Grad Norm 9.4908(108.7065) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0006 | Time 9.5622, Epoch Time 236.8746(251.4885), Bit/dim 2.2584, Xent 0.6051, Loss 2.5610, Error 0.3316\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0042 | Time 29.0604(41.9377) | Bit/dim 2.2757(14.0296) | Xent 0.5958(1.6640) | Loss 2.5736(14.8616) | Error 0.1647(0.5333) Steps 434(437.78) | Grad Norm 5.3466(105.6057) | Total Time 10.00(10.00)\n",
      "Iter 0043 | Time 30.8042(41.6037) | Bit/dim 2.1796(13.6741) | Xent 0.7180(1.6357) | Loss 2.5386(14.4919) | Error 0.1869(0.5229) Steps 440(437.84) | Grad Norm 5.3509(102.5980) | Total Time 10.00(10.00)\n",
      "Iter 0044 | Time 31.5174(41.3011) | Bit/dim 2.1379(13.3280) | Xent 0.8282(1.6114) | Loss 2.5520(14.1337) | Error 0.1985(0.5132) Steps 440(437.91) | Grad Norm 5.9629(99.6990) | Total Time 10.00(10.00)\n",
      "Iter 0045 | Time 30.1374(40.9662) | Bit/dim 2.1203(12.9918) | Xent 0.8663(1.5891) | Loss 2.5535(13.7863) | Error 0.1941(0.5036) Steps 446(438.15) | Grad Norm 6.9653(96.9170) | Total Time 10.00(10.00)\n",
      "Iter 0046 | Time 31.1802(40.6727) | Bit/dim 2.1010(12.6650) | Xent 0.8330(1.5664) | Loss 2.5174(13.4482) | Error 0.1990(0.4945) Steps 446(438.39) | Grad Norm 7.6254(94.2382) | Total Time 10.00(10.00)\n",
      "Iter 0047 | Time 31.3341(40.3925) | Bit/dim 2.0862(12.3477) | Xent 0.6794(1.5398) | Loss 2.4260(13.1176) | Error 0.1717(0.4848) Steps 452(438.79) | Grad Norm 3.6958(91.5219) | Total Time 10.00(10.00)\n",
      "Iter 0048 | Time 31.4787(40.1251) | Bit/dim 2.1141(12.0407) | Xent 0.6083(1.5118) | Loss 2.4183(12.7966) | Error 0.1669(0.4753) Steps 458(439.37) | Grad Norm 2.9449(88.8646) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0007 | Time 9.4153, Epoch Time 237.4255(251.0666), Bit/dim 2.1425, Xent 0.5726, Loss 2.4288, Error 0.3358\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0049 | Time 31.4056(39.8635) | Bit/dim 2.1545(11.7441) | Xent 0.5660(1.4835) | Loss 2.4375(12.4858) | Error 0.1691(0.4661) Steps 452(439.75) | Grad Norm 5.4580(86.3624) | Total Time 10.00(10.00)\n",
      "Iter 0050 | Time 30.6690(39.5877) | Bit/dim 2.1561(11.4564) | Xent 0.5388(1.4551) | Loss 2.4255(12.1840) | Error 0.1640(0.4570) Steps 452(440.12) | Grad Norm 5.7179(83.9431) | Total Time 10.00(10.00)\n",
      "Iter 0051 | Time 31.3931(39.3418) | Bit/dim 2.1057(11.1759) | Xent 0.5352(1.4275) | Loss 2.3733(11.8897) | Error 0.1531(0.4479) Steps 464(440.83) | Grad Norm 3.5080(81.5300) | Total Time 10.00(10.00)\n",
      "Iter 0052 | Time 31.8593(39.1174) | Bit/dim 2.0655(10.9026) | Xent 0.5777(1.4020) | Loss 2.3544(11.6036) | Error 0.1528(0.4390) Steps 464(441.53) | Grad Norm 3.1848(79.1797) | Total Time 10.00(10.00)\n",
      "Iter 0053 | Time 31.7058(38.8950) | Bit/dim 2.0394(10.6367) | Xent 0.6076(1.3782) | Loss 2.3432(11.3258) | Error 0.1531(0.4305) Steps 464(442.20) | Grad Norm 4.1790(76.9297) | Total Time 10.00(10.00)\n",
      "Iter 0054 | Time 33.4629(38.7320) | Bit/dim 2.0233(10.3783) | Xent 0.6465(1.3563) | Loss 2.3465(11.0564) | Error 0.1623(0.4224) Steps 470(443.04) | Grad Norm 4.7108(74.7631) | Total Time 10.00(10.00)\n",
      "Iter 0055 | Time 31.4686(38.5141) | Bit/dim 2.0154(10.1274) | Xent 0.6109(1.3339) | Loss 2.3209(10.7944) | Error 0.1617(0.4146) Steps 470(443.85) | Grad Norm 3.5218(72.6259) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0008 | Time 9.8415, Epoch Time 244.3764(250.8659), Bit/dim 2.0147, Xent 0.5364, Loss 2.2829, Error 0.3191\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0056 | Time 32.1330(38.3227) | Bit/dim 2.0259(9.8844) | Xent 0.5258(1.3097) | Loss 2.2888(10.5392) | Error 0.1474(0.4066) Steps 470(444.63) | Grad Norm 1.9975(70.5070) | Total Time 10.00(10.00)\n",
      "Iter 0057 | Time 30.1931(38.0788) | Bit/dim 2.0526(9.6494) | Xent 0.4929(1.2851) | Loss 2.2990(10.2920) | Error 0.1411(0.3986) Steps 440(444.49) | Grad Norm 4.1332(68.5158) | Total Time 10.00(10.00)\n",
      "Iter 0058 | Time 31.1588(37.8712) | Bit/dim 2.0477(9.4214) | Xent 0.4722(1.2608) | Loss 2.2838(10.0518) | Error 0.1366(0.3908) Steps 440(444.36) | Grad Norm 3.6738(66.5705) | Total Time 10.00(10.00)\n",
      "Iter 0059 | Time 30.1417(37.6393) | Bit/dim 2.0120(9.1991) | Xent 0.4889(1.2376) | Loss 2.2564(9.8179) | Error 0.1416(0.3833) Steps 440(444.23) | Grad Norm 2.1666(64.6384) | Total Time 10.00(10.00)\n",
      "Iter 0060 | Time 29.8299(37.4050) | Bit/dim 1.9797(8.9825) | Xent 0.5350(1.2165) | Loss 2.2472(9.5908) | Error 0.1492(0.3763) Steps 440(444.10) | Grad Norm 2.0831(62.7618) | Total Time 10.00(10.00)\n",
      "Iter 0061 | Time 31.1174(37.2164) | Bit/dim 1.9594(8.7718) | Xent 0.5583(1.1968) | Loss 2.2385(9.3702) | Error 0.1524(0.3695) Steps 446(444.16) | Grad Norm 3.5302(60.9848) | Total Time 10.00(10.00)\n",
      "Iter 0062 | Time 32.2406(37.0671) | Bit/dim 1.9529(8.5672) | Xent 0.5307(1.1768) | Loss 2.2182(9.1556) | Error 0.1416(0.3627) Steps 464(444.75) | Grad Norm 3.1535(59.2499) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0009 | Time 9.6244, Epoch Time 238.7879(250.5036), Bit/dim 1.9375, Xent 0.5189, Loss 2.1969, Error 0.3177\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0063 | Time 30.9002(36.8821) | Bit/dim 1.9476(8.3687) | Xent 0.5032(1.1566) | Loss 2.1992(8.9470) | Error 0.1400(0.3560) Steps 440(444.61) | Grad Norm 1.1213(57.5060) | Total Time 10.00(10.00)\n",
      "Iter 0064 | Time 29.9676(36.6747) | Bit/dim 1.9650(8.1766) | Xent 0.4806(1.1363) | Loss 2.2054(8.7447) | Error 0.1435(0.3497) Steps 440(444.47) | Grad Norm 3.2141(55.8773) | Total Time 10.00(10.00)\n",
      "Iter 0065 | Time 31.1147(36.5079) | Bit/dim 1.9647(7.9902) | Xent 0.4818(1.1167) | Loss 2.2056(8.5485) | Error 0.1416(0.3434) Steps 440(444.34) | Grad Norm 3.4999(54.3059) | Total Time 10.00(10.00)\n",
      "Iter 0066 | Time 29.9019(36.3097) | Bit/dim 1.9366(7.8086) | Xent 0.4703(1.0973) | Loss 2.1718(8.3572) | Error 0.1315(0.3371) Steps 440(444.21) | Grad Norm 1.1576(52.7115) | Total Time 10.00(10.00)\n",
      "Iter 0067 | Time 31.9403(36.1786) | Bit/dim 1.9274(7.6321) | Xent 0.4866(1.0790) | Loss 2.1707(8.1716) | Error 0.1314(0.3309) Steps 440(444.08) | Grad Norm 2.8917(51.2169) | Total Time 10.00(10.00)\n",
      "Iter 0068 | Time 30.4207(36.0059) | Bit/dim 1.9195(7.4608) | Xent 0.4867(1.0612) | Loss 2.1628(7.9914) | Error 0.1359(0.3250) Steps 440(443.96) | Grad Norm 2.6291(49.7593) | Total Time 10.00(10.00)\n",
      "Iter 0069 | Time 30.5946(35.8436) | Bit/dim 1.9178(7.2945) | Xent 0.4534(1.0430) | Loss 2.1445(7.8160) | Error 0.1311(0.3192) Steps 440(443.84) | Grad Norm 1.3814(48.3079) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0010 | Time 9.3705, Epoch Time 237.1050(250.1016), Bit/dim 1.9108, Xent 0.4383, Loss 2.1299, Error 0.3036\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0070 | Time 30.1829(35.6737) | Bit/dim 1.9197(7.1332) | Xent 0.4232(1.0244) | Loss 2.1313(7.6454) | Error 0.1224(0.3133) Steps 440(443.72) | Grad Norm 2.0541(46.9203) | Total Time 10.00(10.00)\n",
      "Iter 0071 | Time 30.1817(35.5090) | Bit/dim 1.9219(6.9769) | Xent 0.4307(1.0066) | Loss 2.1373(7.4802) | Error 0.1330(0.3079) Steps 440(443.61) | Grad Norm 2.2283(45.5795) | Total Time 10.00(10.00)\n",
      "Iter 0072 | Time 31.0655(35.3757) | Bit/dim 1.9086(6.8248) | Xent 0.4495(0.9898) | Loss 2.1333(7.3198) | Error 0.1266(0.3025) Steps 440(443.50) | Grad Norm 1.1179(44.2457) | Total Time 10.00(10.00)\n",
      "Iter 0073 | Time 30.0271(35.2152) | Bit/dim 1.8879(6.6767) | Xent 0.4614(0.9740) | Loss 2.1186(7.1637) | Error 0.1325(0.2974) Steps 440(443.40) | Grad Norm 1.8243(42.9731) | Total Time 10.00(10.00)\n",
      "Iter 0074 | Time 31.3621(35.0996) | Bit/dim 1.8875(6.5331) | Xent 0.4451(0.9581) | Loss 2.1100(7.0121) | Error 0.1325(0.2924) Steps 440(443.30) | Grad Norm 1.7009(41.7349) | Total Time 10.00(10.00)\n",
      "Iter 0075 | Time 29.9990(34.9466) | Bit/dim 1.8870(6.3937) | Xent 0.4489(0.9428) | Loss 2.1115(6.8651) | Error 0.1319(0.2876) Steps 440(443.20) | Grad Norm 0.8607(40.5087) | Total Time 10.00(10.00)\n",
      "Iter 0076 | Time 29.8985(34.7952) | Bit/dim 1.8843(6.2584) | Xent 0.4314(0.9275) | Loss 2.1000(6.7222) | Error 0.1290(0.2828) Steps 440(443.10) | Grad Norm 1.5136(39.3388) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0011 | Time 9.2260, Epoch Time 234.5311(249.6345), Bit/dim 1.8624, Xent 0.4318, Loss 2.0783, Error 0.3023\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0077 | Time 29.9628(34.6502) | Bit/dim 1.8775(6.1270) | Xent 0.4254(0.9124) | Loss 2.0902(6.5832) | Error 0.1244(0.2781) Steps 440(443.01) | Grad Norm 1.3780(38.2000) | Total Time 10.00(10.00)\n",
      "Iter 0078 | Time 30.4892(34.5254) | Bit/dim 1.8633(5.9991) | Xent 0.4421(0.8983) | Loss 2.0844(6.4482) | Error 0.1246(0.2735) Steps 440(442.92) | Grad Norm 1.6743(37.1042) | Total Time 10.00(10.00)\n",
      "Iter 0079 | Time 31.6335(34.4386) | Bit/dim 1.8620(5.8749) | Xent 0.4375(0.8845) | Loss 2.0807(6.3172) | Error 0.1265(0.2691) Steps 452(443.19) | Grad Norm 1.8796(36.0475) | Total Time 10.00(10.00)\n",
      "Iter 0080 | Time 30.8900(34.3321) | Bit/dim 1.8703(5.7548) | Xent 0.4021(0.8700) | Loss 2.0713(6.1898) | Error 0.1226(0.2647) Steps 452(443.46) | Grad Norm 3.1250(35.0598) | Total Time 10.00(10.00)\n",
      "Iter 0081 | Time 32.6467(34.2816) | Bit/dim 1.8702(5.6383) | Xent 0.4187(0.8565) | Loss 2.0796(6.0665) | Error 0.1240(0.2605) Steps 452(443.71) | Grad Norm 5.2350(34.1651) | Total Time 10.00(10.00)\n",
      "Iter 0082 | Time 31.0547(34.1848) | Bit/dim 1.8762(5.5254) | Xent 0.4246(0.8435) | Loss 2.0885(5.9472) | Error 0.1285(0.2565) Steps 458(444.14) | Grad Norm 8.0414(33.3814) | Total Time 10.00(10.00)\n",
      "Iter 0083 | Time 31.5845(34.1068) | Bit/dim 1.8774(5.4160) | Xent 0.4643(0.8322) | Loss 2.1095(5.8320) | Error 0.1285(0.2527) Steps 452(444.38) | Grad Norm 11.9251(32.7377) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0012 | Time 9.3464, Epoch Time 240.3467(249.3559), Bit/dim 1.8940, Xent 0.4427, Loss 2.1154, Error 0.3059\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0084 | Time 30.4005(33.9956) | Bit/dim 1.9131(5.3109) | Xent 0.4396(0.8204) | Loss 2.1329(5.7211) | Error 0.1294(0.2490) Steps 446(444.42) | Grad Norm 13.7082(32.1668) | Total Time 10.00(10.00)\n",
      "Iter 0085 | Time 31.7179(33.9272) | Bit/dim 1.8746(5.2078) | Xent 0.4081(0.8080) | Loss 2.0786(5.6118) | Error 0.1199(0.2451) Steps 452(444.65) | Grad Norm 10.0360(31.5029) | Total Time 10.00(10.00)\n",
      "Iter 0086 | Time 32.1521(33.8740) | Bit/dim 1.8536(5.1072) | Xent 0.4102(0.7961) | Loss 2.0587(5.5052) | Error 0.1200(0.2413) Steps 452(444.87) | Grad Norm 1.5094(30.6031) | Total Time 10.00(10.00)\n",
      "Iter 0087 | Time 31.0286(33.7886) | Bit/dim 1.8690(5.0100) | Xent 0.3962(0.7841) | Loss 2.0671(5.4021) | Error 0.1191(0.2377) Steps 458(445.27) | Grad Norm 7.1979(29.9009) | Total Time 10.00(10.00)\n",
      "Iter 0088 | Time 31.9596(33.7338) | Bit/dim 1.8443(4.9151) | Xent 0.3941(0.7724) | Loss 2.0414(5.3012) | Error 0.1175(0.2341) Steps 458(445.65) | Grad Norm 4.8335(29.1489) | Total Time 10.00(10.00)\n",
      "Iter 0089 | Time 32.5612(33.6986) | Bit/dim 1.8452(4.8230) | Xent 0.4023(0.7613) | Loss 2.0463(5.2036) | Error 0.1164(0.2305) Steps 458(446.02) | Grad Norm 2.6126(28.3528) | Total Time 10.00(10.00)\n",
      "Iter 0090 | Time 31.2586(33.6254) | Bit/dim 1.8565(4.7340) | Xent 0.3803(0.7498) | Loss 2.0466(5.1089) | Error 0.1162(0.2271) Steps 458(446.38) | Grad Norm 6.0047(27.6823) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0013 | Time 9.7658, Epoch Time 243.2883(249.1738), Bit/dim 1.8207, Xent 0.4164, Loss 2.0289, Error 0.2978\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0091 | Time 32.3888(33.5883) | Bit/dim 1.8370(4.6471) | Xent 0.3955(0.7392) | Loss 2.0348(5.0167) | Error 0.1160(0.2238) Steps 458(446.73) | Grad Norm 4.1779(26.9772) | Total Time 10.00(10.00)\n",
      "Iter 0092 | Time 32.1527(33.5452) | Bit/dim 1.8387(4.5628) | Xent 0.3703(0.7281) | Loss 2.0238(4.9269) | Error 0.1138(0.2205) Steps 458(447.07) | Grad Norm 2.0306(26.2288) | Total Time 10.00(10.00)\n",
      "Iter 0093 | Time 30.7827(33.4623) | Bit/dim 1.8428(4.4812) | Xent 0.3742(0.7175) | Loss 2.0299(4.8400) | Error 0.1111(0.2172) Steps 452(447.21) | Grad Norm 4.9117(25.5893) | Total Time 10.00(10.00)\n",
      "Iter 0094 | Time 33.1059(33.4516) | Bit/dim 1.8330(4.4018) | Xent 0.3857(0.7076) | Loss 2.0259(4.7555) | Error 0.1126(0.2141) Steps 458(447.54) | Grad Norm 2.7181(24.9032) | Total Time 10.00(10.00)\n",
      "Iter 0095 | Time 32.4761(33.4224) | Bit/dim 1.8271(4.3245) | Xent 0.3998(0.6983) | Loss 2.0270(4.6737) | Error 0.1152(0.2111) Steps 458(447.85) | Grad Norm 1.9898(24.2158) | Total Time 10.00(10.00)\n",
      "Iter 0096 | Time 31.1614(33.3545) | Bit/dim 1.8233(4.2495) | Xent 0.3713(0.6885) | Loss 2.0090(4.5937) | Error 0.1110(0.2081) Steps 452(447.98) | Grad Norm 2.6979(23.5702) | Total Time 10.00(10.00)\n",
      "Iter 0097 | Time 32.7336(33.3359) | Bit/dim 1.8256(4.1768) | Xent 0.3848(0.6794) | Loss 2.0180(4.5165) | Error 0.1148(0.2053) Steps 458(448.28) | Grad Norm 3.9344(22.9812) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0014 | Time 9.6986, Epoch Time 247.2704(249.1167), Bit/dim 1.8090, Xent 0.3801, Loss 1.9990, Error 0.2912\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0098 | Time 31.1284(33.2697) | Bit/dim 1.8229(4.1061) | Xent 0.3683(0.6701) | Loss 2.0070(4.4412) | Error 0.1110(0.2025) Steps 458(448.57) | Grad Norm 3.1263(22.3855) | Total Time 10.00(10.00)\n",
      "Iter 0099 | Time 31.8334(33.2266) | Bit/dim 1.8224(4.0376) | Xent 0.3764(0.6613) | Loss 2.0106(4.3683) | Error 0.1171(0.1999) Steps 458(448.85) | Grad Norm 3.4202(21.8166) | Total Time 10.00(10.00)\n",
      "Iter 0100 | Time 32.3176(33.1993) | Bit/dim 1.8223(3.9712) | Xent 0.3514(0.6520) | Loss 1.9980(4.2972) | Error 0.0984(0.1969) Steps 458(449.13) | Grad Norm 3.4180(21.2646) | Total Time 10.00(10.00)\n",
      "Iter 0101 | Time 32.0271(33.1642) | Bit/dim 1.8171(3.9066) | Xent 0.3640(0.6433) | Loss 1.9991(4.2282) | Error 0.1086(0.1942) Steps 464(449.57) | Grad Norm 4.2429(20.7539) | Total Time 10.00(10.00)\n",
      "Iter 0102 | Time 31.1599(33.1040) | Bit/dim 1.8163(3.8438) | Xent 0.3726(0.6352) | Loss 2.0026(4.1615) | Error 0.1098(0.1917) Steps 458(449.82) | Grad Norm 4.5361(20.2674) | Total Time 10.00(10.00)\n",
      "Iter 0103 | Time 32.5382(33.0871) | Bit/dim 1.8075(3.7828) | Xent 0.3830(0.6277) | Loss 1.9990(4.0966) | Error 0.1149(0.1894) Steps 458(450.07) | Grad Norm 5.6579(19.8291) | Total Time 10.00(10.00)\n",
      "Iter 0104 | Time 33.7145(33.1059) | Bit/dim 1.8307(3.7242) | Xent 0.3664(0.6198) | Loss 2.0139(4.0341) | Error 0.1069(0.1869) Steps 458(450.31) | Grad Norm 8.0817(19.4767) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0015 | Time 9.7336, Epoch Time 247.3442(249.0635), Bit/dim 1.8188, Xent 0.4056, Loss 2.0216, Error 0.2953\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0105 | Time 32.3811(33.0841) | Bit/dim 1.8327(3.6675) | Xent 0.3952(0.6131) | Loss 2.0303(3.9740) | Error 0.1186(0.1848) Steps 464(450.72) | Grad Norm 11.0256(19.2232) | Total Time 10.00(10.00)\n",
      "Iter 0106 | Time 31.6114(33.0400) | Bit/dim 1.8694(3.6135) | Xent 0.3684(0.6057) | Loss 2.0536(3.9164) | Error 0.1025(0.1824) Steps 458(450.94) | Grad Norm 11.5231(18.9922) | Total Time 10.00(10.00)\n",
      "Iter 0107 | Time 32.6105(33.0271) | Bit/dim 1.8146(3.5595) | Xent 0.3805(0.5990) | Loss 2.0049(3.8590) | Error 0.1210(0.1805) Steps 458(451.15) | Grad Norm 6.9203(18.6300) | Total Time 10.00(10.00)\n",
      "Iter 0108 | Time 34.0570(33.0580) | Bit/dim 1.8215(3.5074) | Xent 0.3640(0.5919) | Loss 2.0035(3.8034) | Error 0.1100(0.1784) Steps 464(451.53) | Grad Norm 8.0808(18.3135) | Total Time 10.00(10.00)\n",
      "Iter 0109 | Time 32.3242(33.0360) | Bit/dim 1.8454(3.4575) | Xent 0.3698(0.5853) | Loss 2.0303(3.7502) | Error 0.1141(0.1765) Steps 464(451.91) | Grad Norm 9.2203(18.0407) | Total Time 10.00(10.00)\n",
      "Iter 0110 | Time 33.4095(33.0472) | Bit/dim 1.7976(3.4077) | Xent 0.3714(0.5789) | Loss 1.9833(3.6972) | Error 0.1062(0.1744) Steps 458(452.09) | Grad Norm 3.8100(17.6138) | Total Time 10.00(10.00)\n",
      "Iter 0111 | Time 32.2748(33.0240) | Bit/dim 1.8104(3.3598) | Xent 0.3675(0.5725) | Loss 1.9942(3.6461) | Error 0.1102(0.1725) Steps 464(452.45) | Grad Norm 5.3888(17.2471) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0016 | Time 9.7185, Epoch Time 251.1038(249.1248), Bit/dim 1.8116, Xent 0.3482, Loss 1.9857, Error 0.2834\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0112 | Time 30.9305(32.9612) | Bit/dim 1.8315(3.3140) | Xent 0.3461(0.5657) | Loss 2.0045(3.5968) | Error 0.1028(0.1704) Steps 458(452.61) | Grad Norm 6.6141(16.9281) | Total Time 10.00(10.00)\n",
      "Iter 0113 | Time 32.4636(32.9463) | Bit/dim 1.7979(3.2685) | Xent 0.3413(0.5590) | Loss 1.9686(3.5480) | Error 0.1066(0.1685) Steps 458(452.78) | Grad Norm 2.6323(16.4992) | Total Time 10.00(10.00)\n",
      "Iter 0114 | Time 32.7323(32.9398) | Bit/dim 1.8246(3.2252) | Xent 0.3573(0.5529) | Loss 2.0032(3.5016) | Error 0.1028(0.1665) Steps 464(453.11) | Grad Norm 8.9685(16.2733) | Total Time 10.00(10.00)\n",
      "Iter 0115 | Time 34.9416(32.9999) | Bit/dim 1.7956(3.1823) | Xent 0.3346(0.5464) | Loss 1.9629(3.4555) | Error 0.1014(0.1645) Steps 470(453.62) | Grad Norm 4.0762(15.9074) | Total Time 10.00(10.00)\n",
      "Iter 0116 | Time 33.6097(33.0182) | Bit/dim 1.8062(3.1410) | Xent 0.3624(0.5409) | Loss 1.9874(3.4114) | Error 0.1076(0.1628) Steps 470(454.11) | Grad Norm 4.6194(15.5687) | Total Time 10.00(10.00)\n",
      "Iter 0117 | Time 31.9169(32.9852) | Bit/dim 1.7964(3.1007) | Xent 0.3926(0.5364) | Loss 1.9927(3.3689) | Error 0.1129(0.1613) Steps 458(454.23) | Grad Norm 5.9916(15.2814) | Total Time 10.00(10.00)\n",
      "Iter 0118 | Time 31.0618(32.9275) | Bit/dim 1.7994(3.0616) | Xent 0.3477(0.5308) | Loss 1.9732(3.3270) | Error 0.1041(0.1596) Steps 458(454.34) | Grad Norm 4.3607(14.9538) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0017 | Time 9.7426, Epoch Time 250.0282(249.1519), Bit/dim 1.7741, Xent 0.3400, Loss 1.9441, Error 0.2770\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0119 | Time 31.4469(32.8830) | Bit/dim 1.7861(3.0234) | Xent 0.3286(0.5247) | Loss 1.9504(3.2857) | Error 0.0978(0.1578) Steps 458(454.45) | Grad Norm 2.3523(14.5757) | Total Time 10.00(10.00)\n",
      "Iter 0120 | Time 30.9821(32.8260) | Bit/dim 1.7898(2.9864) | Xent 0.3817(0.5204) | Loss 1.9807(3.2466) | Error 0.1175(0.1565) Steps 458(454.56) | Grad Norm 6.9352(14.3465) | Total Time 10.00(10.00)\n",
      "Iter 0121 | Time 31.3056(32.7804) | Bit/dim 1.8130(2.9512) | Xent 0.3367(0.5149) | Loss 1.9813(3.2086) | Error 0.0978(0.1548) Steps 458(454.66) | Grad Norm 6.9056(14.1233) | Total Time 10.00(10.00)\n",
      "Iter 0122 | Time 31.1673(32.7320) | Bit/dim 1.7751(2.9159) | Xent 0.3546(0.5101) | Loss 1.9523(3.1709) | Error 0.0985(0.1531) Steps 458(454.76) | Grad Norm 2.6560(13.7793) | Total Time 10.00(10.00)\n",
      "Iter 0123 | Time 32.9303(32.7380) | Bit/dim 1.7677(2.8814) | Xent 0.3520(0.5053) | Loss 1.9437(3.1341) | Error 0.1020(0.1516) Steps 464(455.04) | Grad Norm 5.3051(13.5251) | Total Time 10.00(10.00)\n",
      "Iter 0124 | Time 31.3809(32.6972) | Bit/dim 1.8173(2.8495) | Xent 0.3517(0.5007) | Loss 1.9932(3.0999) | Error 0.1071(0.1502) Steps 458(455.13) | Grad Norm 8.9075(13.3865) | Total Time 10.00(10.00)\n",
      "Iter 0125 | Time 31.0820(32.6488) | Bit/dim 1.7695(2.8171) | Xent 0.3157(0.4952) | Loss 1.9274(3.0647) | Error 0.0935(0.1485) Steps 458(455.21) | Grad Norm 4.3909(13.1167) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0018 | Time 9.8535, Epoch Time 242.7104(248.9586), Bit/dim 1.7575, Xent 0.3447, Loss 1.9298, Error 0.2850\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0126 | Time 31.2510(32.6068) | Bit/dim 1.7724(2.7858) | Xent 0.3195(0.4899) | Loss 1.9321(3.0307) | Error 0.0978(0.1470) Steps 458(455.30) | Grad Norm 4.3709(12.8543) | Total Time 10.00(10.00)\n",
      "Iter 0127 | Time 31.4396(32.5718) | Bit/dim 1.7878(2.7558) | Xent 0.3321(0.4852) | Loss 1.9539(2.9984) | Error 0.0980(0.1455) Steps 458(455.38) | Grad Norm 6.0068(12.6489) | Total Time 10.00(10.00)\n",
      "Iter 0128 | Time 30.8723(32.5208) | Bit/dim 1.7547(2.7258) | Xent 0.3473(0.4810) | Loss 1.9284(2.9663) | Error 0.1056(0.1443) Steps 458(455.46) | Grad Norm 2.7876(12.3530) | Total Time 10.00(10.00)\n",
      "Iter 0129 | Time 32.8861(32.5318) | Bit/dim 1.7537(2.6966) | Xent 0.3388(0.4768) | Loss 1.9231(2.9350) | Error 0.0990(0.1430) Steps 458(455.53) | Grad Norm 4.3194(12.1120) | Total Time 10.00(10.00)\n",
      "Iter 0130 | Time 31.4856(32.5004) | Bit/dim 1.7641(2.6687) | Xent 0.3107(0.4718) | Loss 1.9195(2.9045) | Error 0.0909(0.1414) Steps 458(455.61) | Grad Norm 5.4503(11.9122) | Total Time 10.00(10.00)\n",
      "Iter 0131 | Time 31.3566(32.4661) | Bit/dim 1.7448(2.6409) | Xent 0.3158(0.4671) | Loss 1.9027(2.8745) | Error 0.0970(0.1401) Steps 458(455.68) | Grad Norm 1.3916(11.5966) | Total Time 10.00(10.00)\n",
      "Iter 0132 | Time 30.9106(32.4194) | Bit/dim 1.7404(2.6139) | Xent 0.3410(0.4633) | Loss 1.9109(2.8456) | Error 0.1026(0.1390) Steps 458(455.75) | Grad Norm 5.4075(11.4109) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0019 | Time 9.7551, Epoch Time 242.5400(248.7661), Bit/dim 1.7347, Xent 0.3306, Loss 1.9000, Error 0.2771\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0133 | Time 31.6689(32.3969) | Bit/dim 1.7503(2.5880) | Xent 0.3186(0.4590) | Loss 1.9096(2.8175) | Error 0.0950(0.1376) Steps 464(456.00) | Grad Norm 4.9845(11.2181) | Total Time 10.00(10.00)\n",
      "Iter 0134 | Time 31.7454(32.3774) | Bit/dim 1.7270(2.5622) | Xent 0.3106(0.4545) | Loss 1.8823(2.7894) | Error 0.0915(0.1363) Steps 458(456.06) | Grad Norm 1.4866(10.9261) | Total Time 10.00(10.00)\n",
      "Iter 0135 | Time 31.5797(32.3534) | Bit/dim 1.7183(2.5369) | Xent 0.3484(0.4514) | Loss 1.8925(2.7625) | Error 0.1036(0.1353) Steps 458(456.11) | Grad Norm 4.3414(10.7286) | Total Time 10.00(10.00)\n",
      "Iter 0136 | Time 31.5994(32.3308) | Bit/dim 1.7334(2.5128) | Xent 0.3066(0.4470) | Loss 1.8867(2.7363) | Error 0.0926(0.1340) Steps 464(456.35) | Grad Norm 7.1090(10.6200) | Total Time 10.00(10.00)\n",
      "Iter 0137 | Time 30.9970(32.2908) | Bit/dim 1.7323(2.4893) | Xent 0.3429(0.4439) | Loss 1.9038(2.7113) | Error 0.1038(0.1331) Steps 458(456.40) | Grad Norm 11.9904(10.6611) | Total Time 10.00(10.00)\n",
      "Iter 0138 | Time 31.2092(32.2584) | Bit/dim 1.7760(2.4679) | Xent 0.3752(0.4418) | Loss 1.9636(2.6889) | Error 0.1178(0.1326) Steps 458(456.45) | Grad Norm 11.7612(10.6941) | Total Time 10.00(10.00)\n",
      "Iter 0139 | Time 30.9206(32.2182) | Bit/dim 1.7087(2.4452) | Xent 0.3628(0.4395) | Loss 1.8900(2.6649) | Error 0.1041(0.1318) Steps 458(456.49) | Grad Norm 7.8129(10.6077) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0020 | Time 9.7869, Epoch Time 242.2659(248.5711), Bit/dim 1.7152, Xent 0.3860, Loss 1.9081, Error 0.2887\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0140 | Time 33.6907(32.2624) | Bit/dim 1.7307(2.4237) | Xent 0.3866(0.4379) | Loss 1.9240(2.6427) | Error 0.1076(0.1310) Steps 464(456.72) | Grad Norm 12.5673(10.6665) | Total Time 10.00(10.00)\n",
      "Iter 0141 | Time 31.9953(32.2544) | Bit/dim 1.6846(2.4016) | Xent 0.3403(0.4349) | Loss 1.8547(2.6190) | Error 0.0998(0.1301) Steps 464(456.94) | Grad Norm 2.7486(10.4289) | Total Time 10.00(10.00)\n",
      "Iter 0142 | Time 33.8186(32.3013) | Bit/dim 1.6931(2.3803) | Xent 0.4205(0.4345) | Loss 1.9033(2.5976) | Error 0.1202(0.1298) Steps 464(457.15) | Grad Norm 7.8913(10.3528) | Total Time 10.00(10.00)\n",
      "Iter 0143 | Time 31.5573(32.2790) | Bit/dim 1.8078(2.3631) | Xent 0.4138(0.4339) | Loss 2.0147(2.5801) | Error 0.1254(0.1297) Steps 458(457.18) | Grad Norm 16.8740(10.5485) | Total Time 10.00(10.00)\n",
      "Iter 0144 | Time 32.9858(32.3002) | Bit/dim 1.8112(2.3466) | Xent 0.7176(0.4424) | Loss 2.1700(2.5678) | Error 0.2628(0.1337) Steps 470(457.56) | Grad Norm 29.5062(11.1172) | Total Time 10.00(10.00)\n",
      "Iter 0145 | Time 30.5405(32.2474) | Bit/dim 2.3054(2.3453) | Xent 0.4818(0.4436) | Loss 2.5463(2.5671) | Error 0.0949(0.1325) Steps 458(457.57) | Grad Norm 25.5119(11.5490) | Total Time 10.00(10.00)\n",
      "Iter 0146 | Time 31.8863(32.2366) | Bit/dim 1.9812(2.3344) | Xent 0.4992(0.4452) | Loss 2.2308(2.5570) | Error 0.1372(0.1327) Steps 458(457.59) | Grad Norm 16.0186(11.6831) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0021 | Time 9.7647, Epoch Time 248.8019(248.5780), Bit/dim 1.7897, Xent 0.5068, Loss 2.0431, Error 0.3190\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0147 | Time 32.7796(32.2529) | Bit/dim 1.8040(2.3185) | Xent 0.4744(0.4461) | Loss 2.0412(2.5416) | Error 0.1414(0.1329) Steps 482(458.32) | Grad Norm 7.1623(11.5475) | Total Time 10.00(10.00)\n",
      "Iter 0148 | Time 33.5915(32.2930) | Bit/dim 1.8904(2.3057) | Xent 0.7546(0.4554) | Loss 2.2677(2.5333) | Error 0.2005(0.1349) Steps 482(459.03) | Grad Norm 11.7964(11.5550) | Total Time 10.00(10.00)\n",
      "Iter 0149 | Time 33.6083(32.3325) | Bit/dim 1.8502(2.2920) | Xent 0.4130(0.4541) | Loss 2.0567(2.5190) | Error 0.1242(0.1346) Steps 488(459.90) | Grad Norm 5.6491(11.3778) | Total Time 10.00(10.00)\n",
      "Iter 0150 | Time 32.7743(32.3457) | Bit/dim 1.8856(2.2798) | Xent 0.3488(0.4509) | Loss 2.0600(2.5053) | Error 0.0984(0.1335) Steps 482(460.56) | Grad Norm 5.6868(11.2071) | Total Time 10.00(10.00)\n",
      "Iter 0151 | Time 33.2627(32.3732) | Bit/dim 1.9165(2.2689) | Xent 0.4786(0.4518) | Loss 2.1558(2.4948) | Error 0.1172(0.1330) Steps 464(460.66) | Grad Norm 12.0965(11.2337) | Total Time 10.00(10.00)\n",
      "Iter 0152 | Time 31.6993(32.3530) | Bit/dim 1.8381(2.2560) | Xent 0.3327(0.4482) | Loss 2.0045(2.4801) | Error 0.0958(0.1319) Steps 470(460.94) | Grad Norm 4.4480(11.0302) | Total Time 10.00(10.00)\n",
      "Iter 0153 | Time 31.5735(32.3296) | Bit/dim 1.7996(2.2423) | Xent 0.3437(0.4451) | Loss 1.9715(2.4648) | Error 0.1056(0.1311) Steps 470(461.22) | Grad Norm 2.0790(10.7616) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0022 | Time 9.8811, Epoch Time 251.8855(248.6772), Bit/dim 1.7909, Xent 0.4254, Loss 2.0036, Error 0.3056\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0154 | Time 33.5858(32.3673) | Bit/dim 1.8078(2.2293) | Xent 0.4131(0.4441) | Loss 2.0144(2.4513) | Error 0.1330(0.1312) Steps 470(461.48) | Grad Norm 7.0169(10.6493) | Total Time 10.00(10.00)\n",
      "Iter 0155 | Time 32.7634(32.3792) | Bit/dim 1.7700(2.2155) | Xent 0.3611(0.4416) | Loss 1.9505(2.4363) | Error 0.1090(0.1305) Steps 476(461.92) | Grad Norm 2.0159(10.3903) | Total Time 10.00(10.00)\n",
      "Iter 0156 | Time 34.5135(32.4432) | Bit/dim 1.7672(2.2020) | Xent 0.4000(0.4404) | Loss 1.9672(2.4222) | Error 0.1165(0.1301) Steps 482(462.52) | Grad Norm 6.8052(10.2827) | Total Time 10.00(10.00)\n",
      "Iter 0157 | Time 32.9606(32.4588) | Bit/dim 1.7546(2.1886) | Xent 0.3735(0.4384) | Loss 1.9413(2.4078) | Error 0.1156(0.1297) Steps 482(463.10) | Grad Norm 1.8073(10.0285) | Total Time 10.00(10.00)\n",
      "Iter 0158 | Time 32.1024(32.4481) | Bit/dim 1.7348(2.1750) | Xent 0.3892(0.4369) | Loss 1.9294(2.3934) | Error 0.1218(0.1294) Steps 470(463.31) | Grad Norm 4.9401(9.8758) | Total Time 10.00(10.00)\n",
      "Iter 0159 | Time 33.1024(32.4677) | Bit/dim 1.7225(2.1614) | Xent 0.3423(0.4341) | Loss 1.8937(2.3784) | Error 0.1092(0.1288) Steps 470(463.51) | Grad Norm 3.2935(9.6784) | Total Time 10.00(10.00)\n",
      "Iter 0160 | Time 32.7925(32.4774) | Bit/dim 1.6964(2.1475) | Xent 0.3341(0.4311) | Loss 1.8635(2.3630) | Error 0.0962(0.1279) Steps 470(463.70) | Grad Norm 2.4218(9.4607) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0023 | Time 9.6078, Epoch Time 253.9165(248.8344), Bit/dim 1.6728, Xent 0.4017, Loss 1.8737, Error 0.2991\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0161 | Time 31.5438(32.4494) | Bit/dim 1.6892(2.1337) | Xent 0.3661(0.4291) | Loss 1.8722(2.3483) | Error 0.1176(0.1275) Steps 452(463.35) | Grad Norm 5.9159(9.3543) | Total Time 10.00(10.00)\n",
      "Iter 0162 | Time 31.7828(32.4294) | Bit/dim 1.6807(2.1201) | Xent 0.3227(0.4259) | Loss 1.8420(2.3331) | Error 0.0976(0.1266) Steps 458(463.19) | Grad Norm 3.7476(9.1861) | Total Time 10.00(10.00)\n",
      "Iter 0163 | Time 32.7422(32.4388) | Bit/dim 1.6675(2.1065) | Xent 0.3314(0.4231) | Loss 1.8332(2.3181) | Error 0.1024(0.1259) Steps 452(462.86) | Grad Norm 2.4969(8.9854) | Total Time 10.00(10.00)\n",
      "Iter 0164 | Time 34.4028(32.4977) | Bit/dim 1.6556(2.0930) | Xent 0.3753(0.4217) | Loss 1.8433(2.3038) | Error 0.1154(0.1256) Steps 476(463.25) | Grad Norm 9.2767(8.9942) | Total Time 10.00(10.00)\n",
      "Iter 0165 | Time 32.0353(32.4839) | Bit/dim 1.6767(2.0805) | Xent 0.4311(0.4219) | Loss 1.8923(2.2915) | Error 0.1224(0.1255) Steps 458(463.09) | Grad Norm 14.1385(9.1485) | Total Time 10.00(10.00)\n",
      "Iter 0166 | Time 32.9866(32.4989) | Bit/dim 1.6534(2.0677) | Xent 0.4333(0.4223) | Loss 1.8700(2.2789) | Error 0.1339(0.1258) Steps 470(463.30) | Grad Norm 11.5598(9.2208) | Total Time 10.00(10.00)\n",
      "Iter 0167 | Time 33.1610(32.5188) | Bit/dim 1.6337(2.0547) | Xent 0.3487(0.4201) | Loss 1.8081(2.2647) | Error 0.1040(0.1251) Steps 464(463.32) | Grad Norm 2.9344(9.0322) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0024 | Time 9.5541, Epoch Time 251.5553(248.9160), Bit/dim 1.6334, Xent 0.3415, Loss 1.8042, Error 0.2817\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0168 | Time 33.6734(32.5535) | Bit/dim 1.6502(2.0426) | Xent 0.3278(0.4173) | Loss 1.8142(2.2512) | Error 0.1001(0.1244) Steps 476(463.70) | Grad Norm 5.3920(8.9230) | Total Time 10.00(10.00)\n",
      "Iter 0169 | Time 32.9196(32.5644) | Bit/dim 1.6154(2.0297) | Xent 0.3391(0.4150) | Loss 1.7850(2.2372) | Error 0.1042(0.1238) Steps 470(463.89) | Grad Norm 2.8447(8.7407) | Total Time 10.00(10.00)\n",
      "Iter 0170 | Time 34.1407(32.6117) | Bit/dim 1.6169(2.0174) | Xent 0.3363(0.4126) | Loss 1.7850(2.2237) | Error 0.1049(0.1232) Steps 476(464.25) | Grad Norm 3.6731(8.5887) | Total Time 10.00(10.00)\n",
      "Iter 0171 | Time 32.3526(32.6039) | Bit/dim 1.6097(2.0051) | Xent 0.3484(0.4107) | Loss 1.7839(2.2105) | Error 0.1091(0.1228) Steps 458(464.07) | Grad Norm 3.2368(8.4281) | Total Time 10.00(10.00)\n",
      "Iter 0172 | Time 33.4930(32.6306) | Bit/dim 1.6051(1.9931) | Xent 0.3375(0.4085) | Loss 1.7739(2.1974) | Error 0.0986(0.1220) Steps 470(464.24) | Grad Norm 1.2441(8.2126) | Total Time 10.00(10.00)\n",
      "Iter 0173 | Time 34.8887(32.6984) | Bit/dim 1.6124(1.9817) | Xent 0.3156(0.4057) | Loss 1.7702(2.1846) | Error 0.0966(0.1213) Steps 476(464.60) | Grad Norm 3.1786(8.0616) | Total Time 10.00(10.00)\n",
      "Iter 0174 | Time 32.8043(32.7015) | Bit/dim 1.5938(1.9701) | Xent 0.3201(0.4031) | Loss 1.7538(2.1716) | Error 0.0926(0.1204) Steps 464(464.58) | Grad Norm 2.5964(7.8976) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0025 | Time 9.5105, Epoch Time 256.3616(249.1394), Bit/dim 1.5731, Xent 0.3406, Loss 1.7434, Error 0.2813\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0175 | Time 34.4508(32.7540) | Bit/dim 1.5918(1.9587) | Xent 0.3193(0.4006) | Loss 1.7514(2.1590) | Error 0.1016(0.1199) Steps 488(465.28) | Grad Norm 1.5992(7.7087) | Total Time 10.00(10.00)\n",
      "Iter 0176 | Time 33.8221(32.7861) | Bit/dim 1.5786(1.9473) | Xent 0.3325(0.3986) | Loss 1.7448(2.1466) | Error 0.1035(0.1194) Steps 488(465.96) | Grad Norm 4.0821(7.5999) | Total Time 10.00(10.00)\n",
      "Iter 0177 | Time 32.3863(32.7741) | Bit/dim 1.5750(1.9362) | Xent 0.3293(0.3965) | Loss 1.7396(2.1344) | Error 0.0981(0.1187) Steps 464(465.90) | Grad Norm 2.9777(7.4612) | Total Time 10.00(10.00)\n",
      "Iter 0178 | Time 33.1149(32.7843) | Bit/dim 1.5559(1.9247) | Xent 0.3142(0.3940) | Loss 1.7130(2.1218) | Error 0.0960(0.1180) Steps 470(466.03) | Grad Norm 1.3078(7.2766) | Total Time 10.00(10.00)\n",
      "Iter 0179 | Time 34.2630(32.8287) | Bit/dim 1.5537(1.9136) | Xent 0.3335(0.3922) | Loss 1.7204(2.1097) | Error 0.1012(0.1175) Steps 488(466.69) | Grad Norm 4.6311(7.1972) | Total Time 10.00(10.00)\n",
      "Iter 0180 | Time 32.8386(32.8290) | Bit/dim 1.5690(1.9033) | Xent 0.3126(0.3898) | Loss 1.7252(2.0982) | Error 0.0955(0.1169) Steps 464(466.61) | Grad Norm 4.9290(7.1292) | Total Time 10.00(10.00)\n",
      "Iter 0181 | Time 33.5197(32.8497) | Bit/dim 1.5447(1.8925) | Xent 0.3134(0.3875) | Loss 1.7014(2.0863) | Error 0.0945(0.1162) Steps 482(467.07) | Grad Norm 1.4263(6.9581) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0026 | Time 10.1834, Epoch Time 257.3148(249.3846), Bit/dim 1.5210, Xent 0.3428, Loss 1.6923, Error 0.2844\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0182 | Time 34.8392(32.9094) | Bit/dim 1.5305(1.8817) | Xent 0.3274(0.3857) | Loss 1.6942(2.0745) | Error 0.0992(0.1157) Steps 488(467.70) | Grad Norm 4.0619(6.8712) | Total Time 10.00(10.00)\n",
      "Iter 0183 | Time 33.0379(32.9132) | Bit/dim 1.5557(1.8719) | Xent 0.3045(0.3833) | Loss 1.7079(2.0635) | Error 0.0874(0.1148) Steps 470(467.77) | Grad Norm 6.8344(6.8701) | Total Time 10.00(10.00)\n",
      "Iter 0184 | Time 34.9645(32.9748) | Bit/dim 1.5369(1.8618) | Xent 0.3455(0.3822) | Loss 1.7096(2.0529) | Error 0.1058(0.1146) Steps 488(468.37) | Grad Norm 5.5738(6.8312) | Total Time 10.00(10.00)\n",
      "Iter 0185 | Time 33.7271(32.9973) | Bit/dim 1.5362(1.8521) | Xent 0.2928(0.3795) | Loss 1.6826(2.0418) | Error 0.0843(0.1137) Steps 476(468.60) | Grad Norm 1.9954(6.6861) | Total Time 10.00(10.00)\n",
      "Iter 0186 | Time 34.2757(33.0357) | Bit/dim 1.5219(1.8421) | Xent 0.2922(0.3768) | Loss 1.6680(2.0306) | Error 0.0850(0.1128) Steps 482(469.00) | Grad Norm 2.2815(6.5540) | Total Time 10.00(10.00)\n",
      "Iter 0187 | Time 32.9025(33.0317) | Bit/dim 1.5226(1.8326) | Xent 0.3135(0.3749) | Loss 1.6793(2.0200) | Error 0.0945(0.1123) Steps 476(469.21) | Grad Norm 6.1188(6.5410) | Total Time 10.00(10.00)\n",
      "Iter 0188 | Time 32.2694(33.0088) | Bit/dim 1.5495(1.8241) | Xent 0.3094(0.3730) | Loss 1.7042(2.0106) | Error 0.0903(0.1116) Steps 470(469.24) | Grad Norm 8.4004(6.5967) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0027 | Time 10.2627, Epoch Time 258.7074(249.6643), Bit/dim 1.5020, Xent 0.3404, Loss 1.6722, Error 0.2811\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0189 | Time 33.2944(33.0174) | Bit/dim 1.5129(1.8147) | Xent 0.3215(0.3714) | Loss 1.6737(2.0005) | Error 0.0989(0.1112) Steps 476(469.44) | Grad Norm 6.1827(6.5843) | Total Time 10.00(10.00)\n",
      "Iter 0190 | Time 33.5405(33.0331) | Bit/dim 1.5122(1.8057) | Xent 0.2789(0.3687) | Loss 1.6517(1.9900) | Error 0.0829(0.1104) Steps 470(469.46) | Grad Norm 2.1411(6.4510) | Total Time 10.00(10.00)\n",
      "Iter 0191 | Time 33.8692(33.0582) | Bit/dim 1.5108(1.7968) | Xent 0.2915(0.3663) | Loss 1.6566(1.9800) | Error 0.0846(0.1096) Steps 476(469.65) | Grad Norm 2.1605(6.3223) | Total Time 10.00(10.00)\n",
      "Iter 0192 | Time 34.1333(33.0904) | Bit/dim 1.5023(1.7880) | Xent 0.3098(0.3646) | Loss 1.6572(1.9703) | Error 0.0950(0.1092) Steps 482(470.02) | Grad Norm 4.6049(6.2708) | Total Time 10.00(10.00)\n",
      "Iter 0193 | Time 33.6977(33.1086) | Bit/dim 1.5079(1.7796) | Xent 0.2903(0.3624) | Loss 1.6531(1.9608) | Error 0.0884(0.1085) Steps 476(470.20) | Grad Norm 3.7912(6.1964) | Total Time 10.00(10.00)\n",
      "Iter 0194 | Time 32.8142(33.0998) | Bit/dim 1.4919(1.7709) | Xent 0.3056(0.3607) | Loss 1.6447(1.9513) | Error 0.0946(0.1081) Steps 470(470.20) | Grad Norm 1.0548(6.0421) | Total Time 10.00(10.00)\n",
      "Iter 0195 | Time 34.1551(33.1315) | Bit/dim 1.4853(1.7624) | Xent 0.2864(0.3585) | Loss 1.6285(1.9416) | Error 0.0870(0.1075) Steps 482(470.55) | Grad Norm 2.1262(5.9247) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0028 | Time 9.6561, Epoch Time 257.8150(249.9088), Bit/dim 1.4729, Xent 0.2970, Loss 1.6214, Error 0.2755\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0196 | Time 31.6823(33.0880) | Bit/dim 1.4886(1.7542) | Xent 0.2899(0.3564) | Loss 1.6336(1.9324) | Error 0.0880(0.1069) Steps 464(470.35) | Grad Norm 3.4739(5.8511) | Total Time 10.00(10.00)\n",
      "Iter 0197 | Time 33.5468(33.1017) | Bit/dim 1.4881(1.7462) | Xent 0.3001(0.3547) | Loss 1.6382(1.9236) | Error 0.0917(0.1064) Steps 482(470.70) | Grad Norm 6.0360(5.8567) | Total Time 10.00(10.00)\n",
      "Iter 0198 | Time 32.5272(33.0845) | Bit/dim 1.5499(1.7403) | Xent 0.2921(0.3529) | Loss 1.6959(1.9167) | Error 0.0827(0.1057) Steps 458(470.32) | Grad Norm 10.3322(5.9910) | Total Time 10.00(10.00)\n",
      "Iter 0199 | Time 34.4343(33.1250) | Bit/dim 1.5242(1.7338) | Xent 0.3440(0.3526) | Loss 1.6963(1.9101) | Error 0.1025(0.1056) Steps 470(470.31) | Grad Norm 11.7464(6.1636) | Total Time 10.00(10.00)\n",
      "Iter 0200 | Time 32.4937(33.1061) | Bit/dim 1.5118(1.7272) | Xent 0.2786(0.3504) | Loss 1.6511(1.9023) | Error 0.0821(0.1049) Steps 470(470.30) | Grad Norm 6.9048(6.1859) | Total Time 10.00(10.00)\n",
      "Iter 0201 | Time 31.8011(33.0669) | Bit/dim 1.4893(1.7200) | Xent 0.2747(0.3481) | Loss 1.6266(1.8941) | Error 0.0834(0.1043) Steps 464(470.11) | Grad Norm 2.3452(6.0706) | Total Time 10.00(10.00)\n",
      "Iter 0202 | Time 33.2125(33.0713) | Bit/dim 1.4935(1.7132) | Xent 0.3032(0.3468) | Loss 1.6451(1.8866) | Error 0.0871(0.1038) Steps 476(470.29) | Grad Norm 5.1789(6.0439) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0029 | Time 9.7956, Epoch Time 252.1275(249.9754), Bit/dim 1.4631, Xent 0.2950, Loss 1.6106, Error 0.2729\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0203 | Time 32.6973(33.0601) | Bit/dim 1.4785(1.7062) | Xent 0.2943(0.3452) | Loss 1.6257(1.8788) | Error 0.0921(0.1034) Steps 470(470.28) | Grad Norm 1.2856(5.9011) | Total Time 10.00(10.00)\n",
      "Iter 0204 | Time 32.6715(33.0484) | Bit/dim 1.4889(1.6997) | Xent 0.2838(0.3433) | Loss 1.6308(1.8713) | Error 0.0839(0.1028) Steps 476(470.45) | Grad Norm 3.8965(5.8410) | Total Time 10.00(10.00)\n",
      "Iter 0205 | Time 31.3299(32.9968) | Bit/dim 1.4706(1.6928) | Xent 0.2670(0.3411) | Loss 1.6041(1.8633) | Error 0.0831(0.1022) Steps 464(470.26) | Grad Norm 0.8678(5.6918) | Total Time 10.00(10.00)\n",
      "Iter 0206 | Time 33.9579(33.0257) | Bit/dim 1.4692(1.6861) | Xent 0.2816(0.3393) | Loss 1.6101(1.8557) | Error 0.0845(0.1017) Steps 482(470.61) | Grad Norm 2.9321(5.6090) | Total Time 10.00(10.00)\n",
      "Iter 0207 | Time 31.2101(32.9712) | Bit/dim 1.4616(1.6794) | Xent 0.2658(0.3371) | Loss 1.5945(1.8479) | Error 0.0809(0.1011) Steps 464(470.41) | Grad Norm 0.7908(5.4645) | Total Time 10.00(10.00)\n",
      "Iter 0208 | Time 31.9530(32.9407) | Bit/dim 1.4664(1.6730) | Xent 0.2664(0.3349) | Loss 1.5997(1.8404) | Error 0.0769(0.1004) Steps 470(470.40) | Grad Norm 3.5360(5.4066) | Total Time 10.00(10.00)\n",
      "Iter 0209 | Time 33.4016(32.9545) | Bit/dim 1.4429(1.6661) | Xent 0.2663(0.3329) | Loss 1.5761(1.8325) | Error 0.0816(0.0998) Steps 482(470.75) | Grad Norm 2.6162(5.3229) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0030 | Time 10.1539, Epoch Time 250.3495(249.9866), Bit/dim 1.4356, Xent 0.2920, Loss 1.5816, Error 0.2696\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0210 | Time 33.4346(32.9689) | Bit/dim 1.4470(1.6595) | Xent 0.2842(0.3314) | Loss 1.5891(1.8252) | Error 0.0870(0.0994) Steps 482(471.09) | Grad Norm 1.9842(5.2227) | Total Time 10.00(10.00)\n",
      "Iter 0211 | Time 33.0484(32.9713) | Bit/dim 1.4747(1.6539) | Xent 0.2960(0.3304) | Loss 1.6227(1.8191) | Error 0.0916(0.0992) Steps 482(471.41) | Grad Norm 6.3995(5.2580) | Total Time 10.00(10.00)\n",
      "Iter 0212 | Time 31.7599(32.9349) | Bit/dim 1.7124(1.6557) | Xent 0.2742(0.3287) | Loss 1.8495(1.8200) | Error 0.0766(0.0985) Steps 464(471.19) | Grad Norm 17.6234(5.6290) | Total Time 10.00(10.00)\n",
      "Iter 0213 | Time 32.8237(32.9316) | Bit/dim 1.5937(1.6538) | Xent 0.3876(0.3304) | Loss 1.7876(1.8191) | Error 0.1174(0.0991) Steps 470(471.16) | Grad Norm 15.1991(5.9161) | Total Time 10.00(10.00)\n",
      "Iter 0214 | Time 31.4331(32.8867) | Bit/dim 1.4887(1.6489) | Xent 0.2864(0.3291) | Loss 1.6319(1.8135) | Error 0.0890(0.0988) Steps 464(470.94) | Grad Norm 2.6526(5.8182) | Total Time 10.00(10.00)\n",
      "Iter 0215 | Time 32.6153(32.8785) | Bit/dim 1.5657(1.6464) | Xent 0.2850(0.3278) | Loss 1.7082(1.8103) | Error 0.0854(0.0984) Steps 476(471.09) | Grad Norm 5.9206(5.8213) | Total Time 10.00(10.00)\n",
      "Iter 0216 | Time 33.0593(32.8839) | Bit/dim 1.5543(1.6436) | Xent 0.2960(0.3269) | Loss 1.7023(1.8071) | Error 0.0879(0.0980) Steps 482(471.42) | Grad Norm 3.6244(5.7554) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0031 | Time 10.6141, Epoch Time 251.3978(250.0290), Bit/dim 1.4980, Xent 0.3304, Loss 1.6633, Error 0.2823\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0217 | Time 35.4030(32.9595) | Bit/dim 1.5119(1.6397) | Xent 0.3158(0.3265) | Loss 1.6698(1.8029) | Error 0.0979(0.0980) Steps 506(472.46) | Grad Norm 2.6325(5.6617) | Total Time 10.00(10.00)\n",
      "Iter 0218 | Time 33.7232(32.9824) | Bit/dim 1.5246(1.6362) | Xent 0.3296(0.3266) | Loss 1.6894(1.7995) | Error 0.1070(0.0983) Steps 494(473.10) | Grad Norm 4.3458(5.6222) | Total Time 10.00(10.00)\n",
      "Iter 0219 | Time 32.8062(32.9771) | Bit/dim 1.5031(1.6322) | Xent 0.2822(0.3253) | Loss 1.6442(1.7949) | Error 0.0863(0.0980) Steps 476(473.19) | Grad Norm 1.6529(5.5031) | Total Time 10.00(10.00)\n",
      "Iter 0220 | Time 32.9154(32.9753) | Bit/dim 1.5133(1.6287) | Xent 0.2558(0.3232) | Loss 1.6412(1.7903) | Error 0.0771(0.0973) Steps 476(473.28) | Grad Norm 2.2478(5.4055) | Total Time 10.00(10.00)\n",
      "Iter 0221 | Time 32.8178(32.9706) | Bit/dim 1.5018(1.6249) | Xent 0.2876(0.3221) | Loss 1.6456(1.7859) | Error 0.0865(0.0970) Steps 470(473.18) | Grad Norm 2.2525(5.3109) | Total Time 10.00(10.00)\n",
      "Iter 0222 | Time 33.4133(32.9838) | Bit/dim 1.4823(1.6206) | Xent 0.2707(0.3206) | Loss 1.6177(1.7809) | Error 0.0797(0.0965) Steps 470(473.08) | Grad Norm 1.3677(5.1926) | Total Time 10.00(10.00)\n",
      "Iter 0223 | Time 32.3262(32.9641) | Bit/dim 1.4819(1.6164) | Xent 0.2890(0.3196) | Loss 1.6264(1.7762) | Error 0.0856(0.0962) Steps 476(473.17) | Grad Norm 1.9497(5.0953) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0032 | Time 10.3845, Epoch Time 256.5306(250.2240), Bit/dim 1.4503, Xent 0.2938, Loss 1.5972, Error 0.2704\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0224 | Time 33.7665(32.9882) | Bit/dim 1.4619(1.6118) | Xent 0.2725(0.3182) | Loss 1.5981(1.7709) | Error 0.0829(0.0958) Steps 482(473.43) | Grad Norm 2.5304(5.0183) | Total Time 10.00(10.00)\n",
      "Iter 0225 | Time 34.3688(33.0296) | Bit/dim 1.4572(1.6071) | Xent 0.2942(0.3175) | Loss 1.6042(1.7659) | Error 0.0896(0.0956) Steps 482(473.69) | Grad Norm 2.2703(4.9359) | Total Time 10.00(10.00)\n",
      "Iter 0226 | Time 33.3606(33.0395) | Bit/dim 1.4541(1.6026) | Xent 0.2696(0.3161) | Loss 1.5889(1.7606) | Error 0.0791(0.0951) Steps 482(473.94) | Grad Norm 2.1307(4.8517) | Total Time 10.00(10.00)\n",
      "Iter 0227 | Time 33.2547(33.0460) | Bit/dim 1.4482(1.5979) | Xent 0.2830(0.3151) | Loss 1.5897(1.7555) | Error 0.0890(0.0949) Steps 482(474.18) | Grad Norm 2.6716(4.7863) | Total Time 10.00(10.00)\n",
      "Iter 0228 | Time 33.4743(33.0588) | Bit/dim 1.4401(1.5932) | Xent 0.2583(0.3134) | Loss 1.5692(1.7499) | Error 0.0805(0.0945) Steps 488(474.60) | Grad Norm 2.0109(4.7031) | Total Time 10.00(10.00)\n",
      "Iter 0229 | Time 33.5002(33.0721) | Bit/dim 1.4203(1.5880) | Xent 0.3015(0.3130) | Loss 1.5710(1.7445) | Error 0.0951(0.0945) Steps 482(474.82) | Grad Norm 3.2452(4.6593) | Total Time 10.00(10.00)\n",
      "Iter 0230 | Time 32.9026(33.0670) | Bit/dim 1.4572(1.5841) | Xent 0.2787(0.3120) | Loss 1.5965(1.7401) | Error 0.0855(0.0942) Steps 476(474.85) | Grad Norm 6.3027(4.7086) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0033 | Time 10.0870, Epoch Time 257.4433(250.4406), Bit/dim 1.4941, Xent 0.4239, Loss 1.7061, Error 0.3144\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0231 | Time 33.7284(33.0868) | Bit/dim 1.5132(1.5820) | Xent 0.4045(0.3148) | Loss 1.7154(1.7393) | Error 0.1355(0.0955) Steps 482(475.07) | Grad Norm 18.2677(5.1154) | Total Time 10.00(10.00)\n",
      "Iter 0232 | Time 31.7446(33.0466) | Bit/dim 1.9775(1.5938) | Xent 0.5147(0.3208) | Loss 2.2348(1.7542) | Error 0.1391(0.0968) Steps 470(474.92) | Grad Norm 16.7318(5.4639) | Total Time 10.00(10.00)\n",
      "Iter 0233 | Time 32.7117(33.0365) | Bit/dim 1.8085(1.6003) | Xent 0.3085(0.3204) | Loss 1.9628(1.7605) | Error 0.0899(0.0966) Steps 476(474.95) | Grad Norm 7.7124(5.5314) | Total Time 10.00(10.00)\n",
      "Iter 0234 | Time 33.7865(33.0590) | Bit/dim 1.6284(1.6011) | Xent 0.3306(0.3207) | Loss 1.7937(1.7614) | Error 0.1021(0.0967) Steps 494(475.52) | Grad Norm 3.9106(5.4827) | Total Time 10.00(10.00)\n",
      "Iter 0235 | Time 35.4628(33.1311) | Bit/dim 1.7048(1.6042) | Xent 0.5779(0.3284) | Loss 1.9937(1.7684) | Error 0.1860(0.0994) Steps 500(476.25) | Grad Norm 13.3302(5.7182) | Total Time 10.00(10.00)\n",
      "Iter 0236 | Time 35.1721(33.1924) | Bit/dim 1.6578(1.6058) | Xent 0.5076(0.3338) | Loss 1.9116(1.7727) | Error 0.1509(0.1009) Steps 524(477.69) | Grad Norm 4.8994(5.6936) | Total Time 10.00(10.00)\n",
      "Iter 0237 | Time 36.1764(33.2819) | Bit/dim 1.6176(1.6062) | Xent 0.4093(0.3361) | Loss 1.8222(1.7742) | Error 0.1164(0.1014) Steps 512(478.72) | Grad Norm 3.3814(5.6242) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0034 | Time 10.5406, Epoch Time 261.9070(250.7846), Bit/dim 1.6288, Xent 0.3014, Loss 1.7795, Error 0.2695\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0238 | Time 34.1909(33.3092) | Bit/dim 1.6410(1.6072) | Xent 0.3093(0.3353) | Loss 1.7957(1.7748) | Error 0.0886(0.1010) Steps 518(479.90) | Grad Norm 2.6492(5.5350) | Total Time 10.00(10.00)\n",
      "Iter 0239 | Time 34.7912(33.3536) | Bit/dim 1.6420(1.6083) | Xent 0.3184(0.3347) | Loss 1.8012(1.7756) | Error 0.0991(0.1010) Steps 518(481.04) | Grad Norm 2.4540(5.4426) | Total Time 10.00(10.00)\n",
      "Iter 0240 | Time 34.5853(33.3906) | Bit/dim 1.5925(1.6078) | Xent 0.2993(0.3337) | Loss 1.7422(1.7746) | Error 0.0900(0.1006) Steps 530(482.51) | Grad Norm 2.0027(5.3394) | Total Time 10.00(10.00)\n",
      "Iter 0241 | Time 36.2085(33.4751) | Bit/dim 1.5833(1.6071) | Xent 0.3052(0.3328) | Loss 1.7359(1.7735) | Error 0.0940(0.1004) Steps 524(483.75) | Grad Norm 2.7855(5.2627) | Total Time 10.00(10.00)\n",
      "Iter 0242 | Time 35.6232(33.5395) | Bit/dim 1.5577(1.6056) | Xent 0.2727(0.3310) | Loss 1.6940(1.7711) | Error 0.0860(0.1000) Steps 524(484.96) | Grad Norm 2.1600(5.1697) | Total Time 10.00(10.00)\n",
      "Iter 0243 | Time 34.2546(33.5610) | Bit/dim 1.5380(1.6035) | Xent 0.3289(0.3310) | Loss 1.7025(1.7690) | Error 0.0980(0.0999) Steps 518(485.95) | Grad Norm 2.6350(5.0936) | Total Time 10.00(10.00)\n",
      "Iter 0244 | Time 33.8819(33.5706) | Bit/dim 1.5422(1.6017) | Xent 0.2884(0.3297) | Loss 1.6864(1.7665) | Error 0.0849(0.0995) Steps 506(486.55) | Grad Norm 2.1675(5.0058) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0035 | Time 10.4689, Epoch Time 266.4609(251.2549), Bit/dim 1.5132, Xent 0.3093, Loss 1.6678, Error 0.2738\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0245 | Time 34.0111(33.5838) | Bit/dim 1.5274(1.5995) | Xent 0.2879(0.3284) | Loss 1.6713(1.7637) | Error 0.0890(0.0992) Steps 500(486.96) | Grad Norm 2.1988(4.9216) | Total Time 10.00(10.00)\n",
      "Iter 0246 | Time 33.4761(33.5806) | Bit/dim 1.5121(1.5969) | Xent 0.2949(0.3274) | Loss 1.6595(1.7606) | Error 0.0910(0.0989) Steps 500(487.35) | Grad Norm 2.2494(4.8415) | Total Time 10.00(10.00)\n",
      "Iter 0247 | Time 35.1657(33.6282) | Bit/dim 1.4880(1.5936) | Xent 0.2989(0.3266) | Loss 1.6375(1.7569) | Error 0.0910(0.0987) Steps 506(487.91) | Grad Norm 1.4583(4.7400) | Total Time 10.00(10.00)\n",
      "Iter 0248 | Time 34.1371(33.6434) | Bit/dim 1.4806(1.5902) | Xent 0.3069(0.3260) | Loss 1.6340(1.7532) | Error 0.0961(0.0986) Steps 512(488.63) | Grad Norm 1.7613(4.6506) | Total Time 10.00(10.00)\n",
      "Iter 0249 | Time 35.1067(33.6873) | Bit/dim 1.4745(1.5867) | Xent 0.3405(0.3264) | Loss 1.6448(1.7499) | Error 0.1081(0.0989) Steps 506(489.15) | Grad Norm 4.0291(4.6320) | Total Time 10.00(10.00)\n",
      "Iter 0250 | Time 35.3366(33.7368) | Bit/dim 1.4813(1.5836) | Xent 0.3979(0.3286) | Loss 1.6803(1.7478) | Error 0.1214(0.0996) Steps 506(489.66) | Grad Norm 7.1293(4.7069) | Total Time 10.00(10.00)\n",
      "Iter 0251 | Time 33.8832(33.7412) | Bit/dim 1.4939(1.5809) | Xent 0.5295(0.3346) | Loss 1.7587(1.7482) | Error 0.1566(0.1013) Steps 500(489.97) | Grad Norm 9.0076(4.8359) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0036 | Time 10.8532, Epoch Time 264.8326(251.6622), Bit/dim 1.4906, Xent 0.3816, Loss 1.6814, Error 0.2859\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0252 | Time 33.7501(33.7415) | Bit/dim 1.5082(1.5787) | Xent 0.3633(0.3354) | Loss 1.6898(1.7464) | Error 0.0992(0.1012) Steps 506(490.45) | Grad Norm 5.8075(4.8650) | Total Time 10.00(10.00)\n",
      "Iter 0253 | Time 34.3965(33.7611) | Bit/dim 1.4705(1.5754) | Xent 0.4013(0.3374) | Loss 1.6712(1.7442) | Error 0.1204(0.1018) Steps 518(491.27) | Grad Norm 5.4992(4.8841) | Total Time 10.00(10.00)\n",
      "Iter 0254 | Time 35.9749(33.8275) | Bit/dim 1.4456(1.5716) | Xent 0.2873(0.3359) | Loss 1.5892(1.7395) | Error 0.0889(0.1014) Steps 506(491.72) | Grad Norm 1.3143(4.7770) | Total Time 10.00(10.00)\n",
      "Iter 0255 | Time 34.0469(33.8341) | Bit/dim 1.4631(1.5683) | Xent 0.3409(0.3361) | Loss 1.6335(1.7363) | Error 0.1042(0.1015) Steps 512(492.32) | Grad Norm 2.8161(4.7182) | Total Time 10.00(10.00)\n",
      "Iter 0256 | Time 34.2714(33.8472) | Bit/dim 1.4552(1.5649) | Xent 0.3044(0.3351) | Loss 1.6074(1.7325) | Error 0.0920(0.1012) Steps 512(492.91) | Grad Norm 2.1454(4.6410) | Total Time 10.00(10.00)\n",
      "Iter 0257 | Time 34.1109(33.8551) | Bit/dim 1.4402(1.5612) | Xent 0.2790(0.3334) | Loss 1.5797(1.7279) | Error 0.0851(0.1007) Steps 512(493.49) | Grad Norm 1.6363(4.5508) | Total Time 10.00(10.00)\n",
      "Iter 0258 | Time 34.6064(33.8777) | Bit/dim 1.4315(1.5573) | Xent 0.2597(0.3312) | Loss 1.5613(1.7229) | Error 0.0784(0.1001) Steps 500(493.68) | Grad Norm 2.1482(4.4787) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0037 | Time 11.4850, Epoch Time 265.4387(252.0755), Bit/dim 1.4116, Xent 0.3043, Loss 1.5638, Error 0.2711\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0259 | Time 37.0450(33.9727) | Bit/dim 1.4289(1.5534) | Xent 0.2822(0.3298) | Loss 1.5700(1.7183) | Error 0.0893(0.0997) Steps 530(494.77) | Grad Norm 2.5880(4.4220) | Total Time 10.00(10.00)\n",
      "Iter 0260 | Time 33.6864(33.9641) | Bit/dim 1.4338(1.5498) | Xent 0.2657(0.3278) | Loss 1.5667(1.7137) | Error 0.0794(0.0991) Steps 506(495.11) | Grad Norm 3.2991(4.3883) | Total Time 10.00(10.00)\n",
      "Iter 0261 | Time 34.6839(33.9857) | Bit/dim 1.4081(1.5456) | Xent 0.2920(0.3268) | Loss 1.5541(1.7090) | Error 0.0921(0.0989) Steps 518(495.80) | Grad Norm 3.8951(4.3735) | Total Time 10.00(10.00)\n",
      "Iter 0262 | Time 34.9159(34.0136) | Bit/dim 1.4308(1.5421) | Xent 0.2830(0.3254) | Loss 1.5723(1.7049) | Error 0.0853(0.0985) Steps 512(496.28) | Grad Norm 5.0795(4.3947) | Total Time 10.00(10.00)\n",
      "Iter 0263 | Time 35.8858(34.0698) | Bit/dim 1.4245(1.5386) | Xent 0.3140(0.3251) | Loss 1.5815(1.7012) | Error 0.1035(0.0987) Steps 524(497.11) | Grad Norm 10.3405(4.5731) | Total Time 10.00(10.00)\n",
      "Iter 0264 | Time 34.5543(34.0843) | Bit/dim 1.6108(1.5408) | Xent 0.3018(0.3244) | Loss 1.7617(1.7030) | Error 0.0837(0.0982) Steps 518(497.74) | Grad Norm 9.8414(4.7311) | Total Time 10.00(10.00)\n",
      "Iter 0265 | Time 35.0953(34.1146) | Bit/dim 1.4707(1.5387) | Xent 0.2772(0.3230) | Loss 1.6093(1.7002) | Error 0.0824(0.0977) Steps 524(498.53) | Grad Norm 6.0710(4.7713) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0038 | Time 10.9225, Epoch Time 269.3290(252.5931), Bit/dim 1.6661, Xent 0.7027, Loss 2.0175, Error 0.3884\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0266 | Time 38.6088(34.2495) | Bit/dim 1.6798(1.5429) | Xent 0.6812(0.3337) | Loss 2.0204(1.7098) | Error 0.2240(0.1015) Steps 542(499.83) | Grad Norm 24.6115(5.3665) | Total Time 10.00(10.00)\n",
      "Iter 0267 | Time 33.3676(34.2230) | Bit/dim 1.7203(1.5482) | Xent 0.2828(0.3322) | Loss 1.8617(1.7143) | Error 0.0840(0.1010) Steps 500(499.84) | Grad Norm 6.6312(5.4045) | Total Time 10.00(10.00)\n",
      "Iter 0268 | Time 31.9107(34.1536) | Bit/dim 1.8724(1.5580) | Xent 0.3188(0.3318) | Loss 2.0318(1.7239) | Error 0.0894(0.1006) Steps 488(499.48) | Grad Norm 5.3144(5.4018) | Total Time 10.00(10.00)\n",
      "Iter 0269 | Time 33.3079(34.1283) | Bit/dim 1.8208(1.5658) | Xent 0.2780(0.3302) | Loss 1.9598(1.7309) | Error 0.0821(0.1001) Steps 518(500.04) | Grad Norm 3.5030(5.3448) | Total Time 10.00(10.00)\n",
      "Iter 0270 | Time 35.1134(34.1578) | Bit/dim 1.7642(1.5718) | Xent 0.2754(0.3285) | Loss 1.9020(1.7361) | Error 0.0843(0.0996) Steps 518(500.58) | Grad Norm 2.1654(5.2494) | Total Time 10.00(10.00)\n",
      "Iter 0271 | Time 34.1716(34.1582) | Bit/dim 1.7401(1.5768) | Xent 0.3253(0.3284) | Loss 1.9028(1.7411) | Error 0.0975(0.0996) Steps 512(500.92) | Grad Norm 2.3250(5.1617) | Total Time 10.00(10.00)\n",
      "Iter 0272 | Time 34.7503(34.1760) | Bit/dim 1.7391(1.5817) | Xent 0.3201(0.3282) | Loss 1.8992(1.7458) | Error 0.0972(0.0995) Steps 512(501.25) | Grad Norm 2.1816(5.0723) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0039 | Time 10.0915, Epoch Time 264.0535(252.9369), Bit/dim 1.7306, Xent 0.3133, Loss 1.8872, Error 0.2780\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0273 | Time 35.7730(34.2239) | Bit/dim 1.7435(1.5866) | Xent 0.3066(0.3275) | Loss 1.8968(1.7503) | Error 0.0949(0.0993) Steps 512(501.57) | Grad Norm 2.0904(4.9828) | Total Time 10.00(10.00)\n",
      "Iter 0274 | Time 34.3874(34.2288) | Bit/dim 1.7218(1.5906) | Xent 0.3143(0.3271) | Loss 1.8789(1.7542) | Error 0.0944(0.0992) Steps 518(502.07) | Grad Norm 1.8021(4.8874) | Total Time 10.00(10.00)\n",
      "Iter 0275 | Time 35.0729(34.2541) | Bit/dim 1.6998(1.5939) | Xent 0.2743(0.3256) | Loss 1.8370(1.7567) | Error 0.0840(0.0987) Steps 506(502.18) | Grad Norm 1.3774(4.7821) | Total Time 10.00(10.00)\n",
      "Iter 0276 | Time 33.5673(34.2335) | Bit/dim 1.6756(1.5963) | Xent 0.2685(0.3239) | Loss 1.8098(1.7583) | Error 0.0820(0.0982) Steps 506(502.30) | Grad Norm 1.3212(4.6783) | Total Time 10.00(10.00)\n",
      "Iter 0277 | Time 34.1007(34.2295) | Bit/dim 1.6607(1.5983) | Xent 0.2865(0.3227) | Loss 1.8040(1.7596) | Error 0.0841(0.0978) Steps 494(502.05) | Grad Norm 1.5625(4.5848) | Total Time 10.00(10.00)\n",
      "Iter 0278 | Time 32.7914(34.1864) | Bit/dim 1.6280(1.5992) | Xent 0.2787(0.3214) | Loss 1.7673(1.7599) | Error 0.0873(0.0975) Steps 494(501.81) | Grad Norm 1.1080(4.4805) | Total Time 10.00(10.00)\n",
      "Iter 0279 | Time 32.6720(34.1410) | Bit/dim 1.6105(1.5995) | Xent 0.2682(0.3198) | Loss 1.7446(1.7594) | Error 0.0851(0.0971) Steps 488(501.39) | Grad Norm 1.1478(4.3805) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0040 | Time 9.9748, Epoch Time 260.8483(253.1743), Bit/dim 1.5795, Xent 0.2881, Loss 1.7236, Error 0.2682\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0280 | Time 34.5686(34.1538) | Bit/dim 1.5920(1.5993) | Xent 0.2574(0.3179) | Loss 1.7207(1.7583) | Error 0.0795(0.0966) Steps 506(501.53) | Grad Norm 1.2969(4.2880) | Total Time 10.00(10.00)\n",
      "Iter 0281 | Time 34.2225(34.1559) | Bit/dim 1.5796(1.5987) | Xent 0.2552(0.3161) | Loss 1.7072(1.7567) | Error 0.0801(0.0961) Steps 500(501.49) | Grad Norm 1.3184(4.1989) | Total Time 10.00(10.00)\n",
      "Iter 0282 | Time 34.1976(34.1571) | Bit/dim 1.5787(1.5981) | Xent 0.2371(0.3137) | Loss 1.6972(1.7549) | Error 0.0751(0.0955) Steps 500(501.44) | Grad Norm 1.0828(4.1055) | Total Time 10.00(10.00)\n",
      "Iter 0283 | Time 33.6863(34.1430) | Bit/dim 1.5610(1.5970) | Xent 0.2341(0.3113) | Loss 1.6780(1.7526) | Error 0.0749(0.0949) Steps 482(500.86) | Grad Norm 1.2040(4.0184) | Total Time 10.00(10.00)\n",
      "Iter 0284 | Time 33.9165(34.1362) | Bit/dim 1.5426(1.5953) | Xent 0.2475(0.3094) | Loss 1.6663(1.7500) | Error 0.0744(0.0942) Steps 482(500.29) | Grad Norm 1.2763(3.9361) | Total Time 10.00(10.00)\n",
      "Iter 0285 | Time 32.8505(34.0976) | Bit/dim 1.5348(1.5935) | Xent 0.2356(0.3072) | Loss 1.6526(1.7471) | Error 0.0740(0.0936) Steps 488(499.92) | Grad Norm 1.4700(3.8622) | Total Time 10.00(10.00)\n",
      "Iter 0286 | Time 34.1171(34.0982) | Bit/dim 1.5197(1.5913) | Xent 0.2519(0.3055) | Loss 1.6457(1.7441) | Error 0.0759(0.0931) Steps 488(499.57) | Grad Norm 0.9846(3.7758) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0041 | Time 10.1477, Epoch Time 260.2437(253.3863), Bit/dim 1.4981, Xent 0.2543, Loss 1.6252, Error 0.2636\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0287 | Time 33.0097(34.0655) | Bit/dim 1.5102(1.5889) | Xent 0.2373(0.3035) | Loss 1.6288(1.7406) | Error 0.0754(0.0926) Steps 488(499.22) | Grad Norm 1.1718(3.6977) | Total Time 10.00(10.00)\n",
      "Iter 0288 | Time 34.6456(34.0829) | Bit/dim 1.5011(1.5862) | Xent 0.2196(0.3010) | Loss 1.6110(1.7367) | Error 0.0663(0.0918) Steps 494(499.06) | Grad Norm 1.9711(3.6459) | Total Time 10.00(10.00)\n",
      "Iter 0289 | Time 33.2296(34.0573) | Bit/dim 1.4920(1.5834) | Xent 0.2358(0.2990) | Loss 1.6099(1.7329) | Error 0.0715(0.0912) Steps 488(498.73) | Grad Norm 1.2578(3.5743) | Total Time 10.00(10.00)\n",
      "Iter 0290 | Time 34.4425(34.0689) | Bit/dim 1.4859(1.5805) | Xent 0.2366(0.2971) | Loss 1.6042(1.7291) | Error 0.0745(0.0907) Steps 494(498.59) | Grad Norm 2.0971(3.5300) | Total Time 10.00(10.00)\n",
      "Iter 0291 | Time 33.0024(34.0369) | Bit/dim 1.4772(1.5774) | Xent 0.2227(0.2949) | Loss 1.5886(1.7248) | Error 0.0691(0.0900) Steps 494(498.45) | Grad Norm 2.7738(3.5073) | Total Time 10.00(10.00)\n",
      "Iter 0292 | Time 33.5816(34.0232) | Bit/dim 1.4732(1.5743) | Xent 0.2776(0.2944) | Loss 1.6120(1.7215) | Error 0.0847(0.0899) Steps 488(498.14) | Grad Norm 8.0168(3.6426) | Total Time 10.00(10.00)\n",
      "Iter 0293 | Time 31.1801(33.9380) | Bit/dim 1.5501(1.5735) | Xent 0.4471(0.2990) | Loss 1.7736(1.7230) | Error 0.1185(0.0907) Steps 476(497.47) | Grad Norm 12.0034(3.8934) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0042 | Time 10.2235, Epoch Time 256.0982(253.4677), Bit/dim 1.4649, Xent 0.3027, Loss 1.6163, Error 0.2748\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0294 | Time 33.0129(33.9102) | Bit/dim 1.4757(1.5706) | Xent 0.3060(0.2992) | Loss 1.6287(1.7202) | Error 0.0968(0.0909) Steps 494(497.37) | Grad Norm 7.3992(3.9986) | Total Time 10.00(10.00)\n",
      "Iter 0295 | Time 33.1480(33.8873) | Bit/dim 1.4635(1.5674) | Xent 0.2146(0.2966) | Loss 1.5708(1.7157) | Error 0.0655(0.0901) Steps 494(497.27) | Grad Norm 1.3009(3.9176) | Total Time 10.00(10.00)\n",
      "Iter 0296 | Time 33.3308(33.8706) | Bit/dim 1.4693(1.5645) | Xent 0.2138(0.2941) | Loss 1.5762(1.7115) | Error 0.0659(0.0894) Steps 494(497.17) | Grad Norm 1.9149(3.8575) | Total Time 10.00(10.00)\n",
      "Iter 0297 | Time 33.4197(33.8571) | Bit/dim 1.4668(1.5615) | Xent 0.2103(0.2916) | Loss 1.5720(1.7073) | Error 0.0647(0.0887) Steps 494(497.08) | Grad Norm 1.3752(3.7831) | Total Time 10.00(10.00)\n",
      "Iter 0298 | Time 33.6648(33.8513) | Bit/dim 1.4490(1.5581) | Xent 0.2295(0.2898) | Loss 1.5637(1.7030) | Error 0.0725(0.0882) Steps 494(496.98) | Grad Norm 1.0900(3.7023) | Total Time 10.00(10.00)\n",
      "Iter 0299 | Time 33.3705(33.8369) | Bit/dim 1.4535(1.5550) | Xent 0.2266(0.2879) | Loss 1.5668(1.6989) | Error 0.0685(0.0876) Steps 500(497.07) | Grad Norm 1.2481(3.6287) | Total Time 10.00(10.00)\n",
      "Iter 0300 | Time 33.9729(33.8410) | Bit/dim 1.4408(1.5516) | Xent 0.2232(0.2859) | Loss 1.5524(1.6945) | Error 0.0661(0.0870) Steps 500(497.16) | Grad Norm 1.7658(3.5728) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0043 | Time 9.8975, Epoch Time 256.4067(253.5559), Bit/dim 1.4138, Xent 0.2250, Loss 1.5263, Error 0.2539\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0301 | Time 33.9589(33.8445) | Bit/dim 1.4314(1.5480) | Xent 0.2090(0.2836) | Loss 1.5359(1.6898) | Error 0.0621(0.0862) Steps 500(497.25) | Grad Norm 0.8234(3.4903) | Total Time 10.00(10.00)\n",
      "Iter 0302 | Time 33.7194(33.8408) | Bit/dim 1.4315(1.5445) | Xent 0.2185(0.2817) | Loss 1.5408(1.6853) | Error 0.0664(0.0856) Steps 500(497.33) | Grad Norm 1.7593(3.4384) | Total Time 10.00(10.00)\n",
      "Iter 0303 | Time 33.9857(33.8451) | Bit/dim 1.4221(1.5408) | Xent 0.1997(0.2792) | Loss 1.5219(1.6804) | Error 0.0624(0.0849) Steps 500(497.41) | Grad Norm 1.4413(3.3784) | Total Time 10.00(10.00)\n",
      "Iter 0304 | Time 33.7341(33.8418) | Bit/dim 1.4094(1.5369) | Xent 0.2107(0.2772) | Loss 1.5147(1.6754) | Error 0.0641(0.0843) Steps 500(497.49) | Grad Norm 1.4279(3.3199) | Total Time 10.00(10.00)\n",
      "Iter 0305 | Time 35.6731(33.8967) | Bit/dim 1.4020(1.5328) | Xent 0.2188(0.2754) | Loss 1.5114(1.6705) | Error 0.0683(0.0838) Steps 506(497.74) | Grad Norm 1.7856(3.2739) | Total Time 10.00(10.00)\n",
      "Iter 0306 | Time 34.4563(33.9135) | Bit/dim 1.3989(1.5288) | Xent 0.2217(0.2738) | Loss 1.5098(1.6657) | Error 0.0706(0.0834) Steps 506(497.99) | Grad Norm 1.3516(3.2162) | Total Time 10.00(10.00)\n",
      "Iter 0307 | Time 35.8543(33.9717) | Bit/dim 1.3939(1.5248) | Xent 0.1959(0.2715) | Loss 1.4918(1.6605) | Error 0.0613(0.0828) Steps 512(498.41) | Grad Norm 1.0075(3.1500) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0044 | Time 10.4757, Epoch Time 264.2945(253.8780), Bit/dim 1.3748, Xent 0.2174, Loss 1.4835, Error 0.2522\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0308 | Time 37.4139(34.0750) | Bit/dim 1.3921(1.5208) | Xent 0.1995(0.2693) | Loss 1.4918(1.6554) | Error 0.0620(0.0821) Steps 512(498.82) | Grad Norm 0.9081(3.0827) | Total Time 10.00(10.00)\n",
      "Iter 0309 | Time 35.6363(34.1219) | Bit/dim 1.3789(1.5165) | Xent 0.2062(0.2674) | Loss 1.4820(1.6502) | Error 0.0665(0.0817) Steps 512(499.21) | Grad Norm 1.0460(3.0216) | Total Time 10.00(10.00)\n",
      "Iter 0310 | Time 36.9042(34.2053) | Bit/dim 1.3847(1.5126) | Xent 0.2067(0.2656) | Loss 1.4880(1.6454) | Error 0.0693(0.0813) Steps 518(499.78) | Grad Norm 2.2108(2.9973) | Total Time 10.00(10.00)\n",
      "Iter 0311 | Time 36.3417(34.2694) | Bit/dim 1.3788(1.5086) | Xent 0.1909(0.2633) | Loss 1.4742(1.6402) | Error 0.0585(0.0806) Steps 524(500.50) | Grad Norm 2.7363(2.9895) | Total Time 10.00(10.00)\n",
      "Iter 0312 | Time 36.7259(34.3431) | Bit/dim 1.3816(1.5047) | Xent 0.2138(0.2619) | Loss 1.4885(1.6357) | Error 0.0665(0.0802) Steps 512(500.85) | Grad Norm 5.3212(3.0594) | Total Time 10.00(10.00)\n",
      "Iter 0313 | Time 33.9093(34.3301) | Bit/dim 1.4594(1.5034) | Xent 0.2064(0.2602) | Loss 1.5626(1.6335) | Error 0.0607(0.0796) Steps 500(500.82) | Grad Norm 11.3719(3.3088) | Total Time 10.00(10.00)\n",
      "Iter 0314 | Time 36.7416(34.4024) | Bit/dim 1.4412(1.5015) | Xent 0.2742(0.2606) | Loss 1.5783(1.6318) | Error 0.0869(0.0798) Steps 524(501.52) | Grad Norm 13.3222(3.6092) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0045 | Time 10.2469, Epoch Time 277.0068(254.5719), Bit/dim 1.4617, Xent 0.2149, Loss 1.5691, Error 0.2534\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0315 | Time 33.8368(34.3855) | Bit/dim 1.4772(1.5008) | Xent 0.2069(0.2590) | Loss 1.5807(1.6303) | Error 0.0665(0.0794) Steps 494(501.29) | Grad Norm 6.0364(3.6820) | Total Time 10.00(10.00)\n",
      "Iter 0316 | Time 33.8154(34.3684) | Bit/dim 1.4785(1.5001) | Xent 0.1874(0.2569) | Loss 1.5722(1.6286) | Error 0.0599(0.0788) Steps 500(501.25) | Grad Norm 5.7537(3.7442) | Total Time 10.00(10.00)\n",
      "Iter 0317 | Time 35.6637(34.4072) | Bit/dim 1.3896(1.4968) | Xent 0.1931(0.2549) | Loss 1.4862(1.6243) | Error 0.0584(0.0782) Steps 512(501.58) | Grad Norm 3.0868(3.7244) | Total Time 10.00(10.00)\n",
      "Iter 0318 | Time 36.3651(34.4660) | Bit/dim 1.4738(1.4961) | Xent 0.2300(0.2542) | Loss 1.5888(1.6232) | Error 0.0717(0.0780) Steps 524(502.25) | Grad Norm 11.7330(3.9647) | Total Time 10.00(10.00)\n",
      "Iter 0319 | Time 34.5797(34.4694) | Bit/dim 1.4916(1.4960) | Xent 0.1800(0.2520) | Loss 1.5816(1.6220) | Error 0.0526(0.0773) Steps 494(502.00) | Grad Norm 5.4041(4.0079) | Total Time 10.00(10.00)\n",
      "Iter 0320 | Time 32.3439(34.4056) | Bit/dim 1.5445(1.4974) | Xent 0.1775(0.2497) | Loss 1.6332(1.6223) | Error 0.0564(0.0766) Steps 476(501.22) | Grad Norm 5.4354(4.0507) | Total Time 10.00(10.00)\n",
      "Iter 0321 | Time 32.7119(34.3548) | Bit/dim 1.4953(1.4974) | Xent 0.2058(0.2484) | Loss 1.5982(1.6216) | Error 0.0655(0.0763) Steps 488(500.83) | Grad Norm 2.4253(4.0019) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0046 | Time 10.3811, Epoch Time 262.2674(254.8027), Bit/dim 1.4538, Xent 0.2145, Loss 1.5611, Error 0.2525\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0322 | Time 33.5198(34.3298) | Bit/dim 1.4707(1.4966) | Xent 0.2113(0.2473) | Loss 1.5763(1.6202) | Error 0.0651(0.0760) Steps 500(500.80) | Grad Norm 1.8001(3.9359) | Total Time 10.00(10.00)\n",
      "Iter 0323 | Time 34.2550(34.3275) | Bit/dim 1.4669(1.4957) | Xent 0.2096(0.2462) | Loss 1.5717(1.6188) | Error 0.0644(0.0756) Steps 512(501.14) | Grad Norm 3.0386(3.9090) | Total Time 10.00(10.00)\n",
      "Iter 0324 | Time 35.2069(34.3539) | Bit/dim 1.4677(1.4948) | Xent 0.1864(0.2444) | Loss 1.5609(1.6170) | Error 0.0591(0.0751) Steps 512(501.46) | Grad Norm 3.2560(3.8894) | Total Time 10.00(10.00)\n",
      "Iter 0325 | Time 34.9982(34.3732) | Bit/dim 1.4528(1.4936) | Xent 0.1776(0.2424) | Loss 1.5416(1.6148) | Error 0.0563(0.0746) Steps 500(501.42) | Grad Norm 1.5457(3.8191) | Total Time 10.00(10.00)\n",
      "Iter 0326 | Time 34.1477(34.3665) | Bit/dim 1.4526(1.4924) | Xent 0.1898(0.2408) | Loss 1.5475(1.6128) | Error 0.0564(0.0740) Steps 500(501.38) | Grad Norm 2.2211(3.7711) | Total Time 10.00(10.00)\n",
      "Iter 0327 | Time 34.0446(34.3568) | Bit/dim 1.4349(1.4906) | Xent 0.1707(0.2387) | Loss 1.5202(1.6100) | Error 0.0521(0.0734) Steps 506(501.51) | Grad Norm 1.6140(3.7064) | Total Time 10.00(10.00)\n",
      "Iter 0328 | Time 33.9243(34.3438) | Bit/dim 1.4256(1.4887) | Xent 0.1785(0.2369) | Loss 1.5148(1.6071) | Error 0.0575(0.0729) Steps 500(501.47) | Grad Norm 1.5813(3.6427) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0047 | Time 10.5220, Epoch Time 263.6647(255.0686), Bit/dim 1.4000, Xent 0.2038, Loss 1.5019, Error 0.2508\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0329 | Time 36.6035(34.4116) | Bit/dim 1.4123(1.4864) | Xent 0.1800(0.2352) | Loss 1.5023(1.6040) | Error 0.0534(0.0723) Steps 506(501.60) | Grad Norm 2.2267(3.6002) | Total Time 10.00(10.00)\n",
      "Iter 0330 | Time 35.6461(34.4487) | Bit/dim 1.4037(1.4839) | Xent 0.1784(0.2335) | Loss 1.4929(1.6006) | Error 0.0564(0.0718) Steps 506(501.74) | Grad Norm 0.9985(3.5221) | Total Time 10.00(10.00)\n",
      "Iter 0331 | Time 33.8685(34.4312) | Bit/dim 1.4094(1.4817) | Xent 0.1915(0.2322) | Loss 1.5051(1.5978) | Error 0.0587(0.0714) Steps 506(501.86) | Grad Norm 2.1139(3.4799) | Total Time 10.00(10.00)\n",
      "Iter 0332 | Time 34.0211(34.4189) | Bit/dim 1.3886(1.4789) | Xent 0.1808(0.2307) | Loss 1.4790(1.5942) | Error 0.0557(0.0710) Steps 506(501.99) | Grad Norm 1.8495(3.4310) | Total Time 10.00(10.00)\n",
      "Iter 0333 | Time 34.7551(34.4290) | Bit/dim 1.3749(1.4758) | Xent 0.1727(0.2289) | Loss 1.4612(1.5902) | Error 0.0544(0.0705) Steps 500(501.93) | Grad Norm 1.4826(3.3725) | Total Time 10.00(10.00)\n",
      "Iter 0334 | Time 34.0867(34.4188) | Bit/dim 1.3776(1.4728) | Xent 0.1812(0.2275) | Loss 1.4681(1.5866) | Error 0.0560(0.0700) Steps 500(501.87) | Grad Norm 1.7598(3.3241) | Total Time 10.00(10.00)\n",
      "Iter 0335 | Time 33.9701(34.4053) | Bit/dim 1.3735(1.4698) | Xent 0.1792(0.2261) | Loss 1.4631(1.5829) | Error 0.0564(0.0696) Steps 506(502.00) | Grad Norm 1.4073(3.2666) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0048 | Time 10.3107, Epoch Time 265.9142(255.3940), Bit/dim 1.3502, Xent 0.1782, Loss 1.4393, Error 0.2451\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0336 | Time 37.3269(34.4929) | Bit/dim 1.3623(1.4666) | Xent 0.1727(0.2245) | Loss 1.4486(1.5788) | Error 0.0559(0.0692) Steps 518(502.48) | Grad Norm 1.0759(3.2009) | Total Time 10.00(10.00)\n",
      "Iter 0337 | Time 36.3202(34.5478) | Bit/dim 1.3626(1.4635) | Xent 0.1773(0.2230) | Loss 1.4513(1.5750) | Error 0.0550(0.0688) Steps 524(503.12) | Grad Norm 1.1910(3.1406) | Total Time 10.00(10.00)\n",
      "Iter 0338 | Time 36.5920(34.6091) | Bit/dim 1.3572(1.4603) | Xent 0.1787(0.2217) | Loss 1.4465(1.5712) | Error 0.0554(0.0684) Steps 518(503.57) | Grad Norm 2.3380(3.1165) | Total Time 10.00(10.00)\n",
      "Iter 0339 | Time 37.9131(34.7082) | Bit/dim 1.3483(1.4569) | Xent 0.1653(0.2200) | Loss 1.4310(1.5669) | Error 0.0519(0.0679) Steps 524(504.18) | Grad Norm 2.4022(3.0951) | Total Time 10.00(10.00)\n",
      "Iter 0340 | Time 35.3853(34.7285) | Bit/dim 1.3509(1.4538) | Xent 0.1728(0.2186) | Loss 1.4373(1.5631) | Error 0.0523(0.0674) Steps 494(503.87) | Grad Norm 2.2647(3.0702) | Total Time 10.00(10.00)\n",
      "Iter 0341 | Time 36.7882(34.7903) | Bit/dim 1.3428(1.4504) | Xent 0.1911(0.2178) | Loss 1.4384(1.5593) | Error 0.0587(0.0672) Steps 524(504.48) | Grad Norm 4.4810(3.1125) | Total Time 10.00(10.00)\n",
      "Iter 0342 | Time 34.1561(34.7713) | Bit/dim 1.3942(1.4487) | Xent 0.1597(0.2160) | Loss 1.4740(1.5568) | Error 0.0476(0.0666) Steps 506(504.52) | Grad Norm 7.4054(3.2413) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0049 | Time 10.8021, Epoch Time 277.9607(256.0710), Bit/dim 1.3915, Xent 0.2322, Loss 1.5076, Error 0.2580\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0343 | Time 37.3520(34.8487) | Bit/dim 1.4047(1.4474) | Xent 0.2236(0.2163) | Loss 1.5165(1.5555) | Error 0.0684(0.0666) Steps 524(505.11) | Grad Norm 11.3472(3.4845) | Total Time 10.00(10.00)\n",
      "Iter 0344 | Time 37.9305(34.9412) | Bit/dim 1.5413(1.4502) | Xent 0.1657(0.2147) | Loss 1.6241(1.5576) | Error 0.0504(0.0661) Steps 524(505.68) | Grad Norm 10.9805(3.7094) | Total Time 10.00(10.00)\n",
      "Iter 0345 | Time 34.7951(34.9368) | Bit/dim 1.3955(1.4486) | Xent 0.1721(0.2135) | Loss 1.4816(1.5553) | Error 0.0549(0.0658) Steps 512(505.87) | Grad Norm 3.6055(3.7062) | Total Time 10.00(10.00)\n",
      "Iter 0346 | Time 39.8214(35.0833) | Bit/dim 1.4896(1.4498) | Xent 0.2750(0.2153) | Loss 1.6271(1.5575) | Error 0.0825(0.0663) Steps 542(506.95) | Grad Norm 12.2569(3.9628) | Total Time 10.00(10.00)\n",
      "Iter 0347 | Time 35.3342(35.0908) | Bit/dim 1.4170(1.4488) | Xent 0.2467(0.2162) | Loss 1.5404(1.5570) | Error 0.0695(0.0664) Steps 506(506.92) | Grad Norm 8.2389(4.0910) | Total Time 10.00(10.00)\n",
      "Iter 0348 | Time 35.7813(35.1116) | Bit/dim 1.4879(1.4500) | Xent 0.2510(0.2173) | Loss 1.6134(1.5587) | Error 0.0767(0.0667) Steps 512(507.07) | Grad Norm 7.6211(4.1970) | Total Time 10.00(10.00)\n",
      "Iter 0349 | Time 34.9463(35.1066) | Bit/dim 1.4493(1.4500) | Xent 0.1925(0.2165) | Loss 1.5456(1.5583) | Error 0.0587(0.0665) Steps 518(507.40) | Grad Norm 5.1996(4.2270) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0050 | Time 10.3256, Epoch Time 278.9577(256.7576), Bit/dim 1.3836, Xent 0.2204, Loss 1.4938, Error 0.2560\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0350 | Time 36.6164(35.1519) | Bit/dim 1.3942(1.4483) | Xent 0.1915(0.2158) | Loss 1.4899(1.5562) | Error 0.0637(0.0664) Steps 524(507.90) | Grad Norm 1.5274(4.1460) | Total Time 10.00(10.00)\n",
      "Iter 0351 | Time 37.7091(35.2286) | Bit/dim 1.4335(1.4479) | Xent 0.2436(0.2166) | Loss 1.5552(1.5562) | Error 0.0745(0.0666) Steps 536(508.74) | Grad Norm 3.9796(4.1410) | Total Time 10.00(10.00)\n",
      "Iter 0352 | Time 37.1976(35.2877) | Bit/dim 1.4276(1.4473) | Xent 0.2147(0.2166) | Loss 1.5349(1.5556) | Error 0.0685(0.0667) Steps 536(509.56) | Grad Norm 3.4258(4.1196) | Total Time 10.00(10.00)\n",
      "Iter 0353 | Time 34.6079(35.2673) | Bit/dim 1.4068(1.4461) | Xent 0.1930(0.2159) | Loss 1.5033(1.5540) | Error 0.0593(0.0665) Steps 530(510.17) | Grad Norm 1.3189(4.0356) | Total Time 10.00(10.00)\n",
      "Iter 0354 | Time 36.2509(35.2968) | Bit/dim 1.4018(1.4447) | Xent 0.1684(0.2144) | Loss 1.4861(1.5519) | Error 0.0510(0.0660) Steps 536(510.95) | Grad Norm 2.7984(3.9985) | Total Time 10.00(10.00)\n",
      "Iter 0355 | Time 38.6885(35.3985) | Bit/dim 1.3983(1.4433) | Xent 0.1756(0.2133) | Loss 1.4861(1.5500) | Error 0.0557(0.0657) Steps 548(512.06) | Grad Norm 2.6247(3.9572) | Total Time 10.00(10.00)\n",
      "Iter 0356 | Time 37.0826(35.4491) | Bit/dim 1.4130(1.4424) | Xent 0.1770(0.2122) | Loss 1.5015(1.5485) | Error 0.0540(0.0653) Steps 542(512.96) | Grad Norm 2.0022(3.8986) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0051 | Time 10.6974, Epoch Time 281.5933(257.5026), Bit/dim 1.3667, Xent 0.1831, Loss 1.4583, Error 0.2465\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0357 | Time 38.5146(35.5410) | Bit/dim 1.3780(1.4405) | Xent 0.1751(0.2111) | Loss 1.4655(1.5460) | Error 0.0580(0.0651) Steps 542(513.83) | Grad Norm 1.7397(3.8338) | Total Time 10.00(10.00)\n",
      "Iter 0358 | Time 37.5889(35.6025) | Bit/dim 1.3825(1.4388) | Xent 0.1744(0.2100) | Loss 1.4698(1.5437) | Error 0.0530(0.0648) Steps 542(514.67) | Grad Norm 1.9007(3.7758) | Total Time 10.00(10.00)\n",
      "Iter 0359 | Time 37.5718(35.6615) | Bit/dim 1.3751(1.4368) | Xent 0.1656(0.2086) | Loss 1.4579(1.5412) | Error 0.0513(0.0643) Steps 548(515.67) | Grad Norm 1.2947(3.7014) | Total Time 10.00(10.00)\n",
      "Iter 0360 | Time 36.9818(35.7012) | Bit/dim 1.3566(1.4344) | Xent 0.1837(0.2079) | Loss 1.4485(1.5384) | Error 0.0551(0.0641) Steps 536(516.28) | Grad Norm 2.9926(3.6801) | Total Time 10.00(10.00)\n",
      "Iter 0361 | Time 36.6220(35.7288) | Bit/dim 1.3586(1.4322) | Xent 0.1815(0.2071) | Loss 1.4493(1.5357) | Error 0.0579(0.0639) Steps 530(516.69) | Grad Norm 1.6143(3.6182) | Total Time 10.00(10.00)\n",
      "Iter 0362 | Time 36.4473(35.7503) | Bit/dim 1.3602(1.4300) | Xent 0.1645(0.2058) | Loss 1.4425(1.5329) | Error 0.0514(0.0635) Steps 524(516.91) | Grad Norm 2.8787(3.5960) | Total Time 10.00(10.00)\n",
      "Iter 0363 | Time 37.2352(35.7949) | Bit/dim 1.3421(1.4274) | Xent 0.1812(0.2051) | Loss 1.4327(1.5299) | Error 0.0591(0.0634) Steps 524(517.13) | Grad Norm 2.0648(3.5500) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0052 | Time 10.7855, Epoch Time 284.4799(258.3120), Bit/dim 1.3262, Xent 0.1826, Loss 1.4175, Error 0.2457\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0364 | Time 36.7285(35.8229) | Bit/dim 1.3399(1.4247) | Xent 0.1617(0.2038) | Loss 1.4207(1.5266) | Error 0.0493(0.0630) Steps 524(517.33) | Grad Norm 1.0288(3.4744) | Total Time 10.00(10.00)\n",
      "Iter 0365 | Time 36.0318(35.8292) | Bit/dim 1.3425(1.4223) | Xent 0.1612(0.2025) | Loss 1.4231(1.5235) | Error 0.0511(0.0626) Steps 530(517.71) | Grad Norm 1.8104(3.4245) | Total Time 10.00(10.00)\n",
      "Iter 0366 | Time 37.0160(35.8648) | Bit/dim 1.3315(1.4195) | Xent 0.1704(0.2015) | Loss 1.4166(1.5203) | Error 0.0530(0.0623) Steps 524(517.90) | Grad Norm 2.7384(3.4039) | Total Time 10.00(10.00)\n",
      "Iter 0367 | Time 36.0684(35.8709) | Bit/dim 1.3346(1.4170) | Xent 0.1770(0.2008) | Loss 1.4232(1.5174) | Error 0.0560(0.0621) Steps 530(518.26) | Grad Norm 3.5061(3.4070) | Total Time 10.00(10.00)\n",
      "Iter 0368 | Time 35.7412(35.8670) | Bit/dim 1.3280(1.4143) | Xent 0.1830(0.2003) | Loss 1.4195(1.5145) | Error 0.0559(0.0619) Steps 518(518.26) | Grad Norm 2.8058(3.3889) | Total Time 10.00(10.00)\n",
      "Iter 0369 | Time 37.7290(35.9228) | Bit/dim 1.3323(1.4119) | Xent 0.1500(0.1988) | Loss 1.4073(1.5113) | Error 0.0465(0.0615) Steps 524(518.43) | Grad Norm 2.8045(3.3714) | Total Time 10.00(10.00)\n",
      "Iter 0370 | Time 36.2374(35.9323) | Bit/dim 1.3285(1.4094) | Xent 0.1593(0.1976) | Loss 1.4081(1.5082) | Error 0.0529(0.0612) Steps 524(518.60) | Grad Norm 4.3743(3.4015) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0053 | Time 10.8194, Epoch Time 278.9959(258.9325), Bit/dim 1.3369, Xent 0.1637, Loss 1.4188, Error 0.2416\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0371 | Time 37.0010(35.9643) | Bit/dim 1.3494(1.4076) | Xent 0.1477(0.1961) | Loss 1.4233(1.5056) | Error 0.0450(0.0607) Steps 530(518.94) | Grad Norm 6.1610(3.4843) | Total Time 10.00(10.00)\n",
      "Iter 0372 | Time 36.2459(35.9728) | Bit/dim 1.3500(1.4058) | Xent 0.1850(0.1958) | Loss 1.4425(1.5037) | Error 0.0560(0.0606) Steps 536(519.45) | Grad Norm 10.0846(3.6823) | Total Time 10.00(10.00)\n",
      "Iter 0373 | Time 35.8683(35.9697) | Bit/dim 1.4979(1.4086) | Xent 0.1690(0.1950) | Loss 1.5825(1.5061) | Error 0.0551(0.0604) Steps 494(518.69) | Grad Norm 9.9189(3.8694) | Total Time 10.00(10.00)\n",
      "Iter 0374 | Time 35.9233(35.9683) | Bit/dim 1.3709(1.4075) | Xent 0.1485(0.1936) | Loss 1.4452(1.5043) | Error 0.0463(0.0600) Steps 518(518.67) | Grad Norm 4.9921(3.9031) | Total Time 10.00(10.00)\n",
      "Iter 0375 | Time 38.0951(36.0321) | Bit/dim 1.5290(1.4111) | Xent 0.5587(0.2045) | Loss 1.8083(1.5134) | Error 0.1926(0.0640) Steps 548(519.55) | Grad Norm 24.8305(4.5309) | Total Time 10.00(10.00)\n",
      "Iter 0376 | Time 35.2700(36.0092) | Bit/dim 1.8497(1.4243) | Xent 0.4119(0.2107) | Loss 2.0556(1.5296) | Error 0.0849(0.0646) Steps 500(518.96) | Grad Norm 15.1326(4.8489) | Total Time 10.00(10.00)\n",
      "Iter 0377 | Time 35.5748(35.9962) | Bit/dim 1.8037(1.4357) | Xent 0.1707(0.2095) | Loss 1.8891(1.5404) | Error 0.0439(0.0640) Steps 512(518.75) | Grad Norm 6.5035(4.8986) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0054 | Time 10.1095, Epoch Time 276.6285(259.4634), Bit/dim 1.7382, Xent 0.3057, Loss 1.8911, Error 0.2730\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0378 | Time 33.1179(35.9098) | Bit/dim 1.7523(1.4452) | Xent 0.2836(0.2118) | Loss 1.8941(1.5510) | Error 0.0857(0.0646) Steps 494(518.01) | Grad Norm 4.8423(4.8969) | Total Time 10.00(10.00)\n",
      "Iter 0379 | Time 35.4835(35.8970) | Bit/dim 1.6648(1.4518) | Xent 0.3172(0.2149) | Loss 1.8235(1.5592) | Error 0.1054(0.0659) Steps 506(517.65) | Grad Norm 3.3910(4.8517) | Total Time 10.00(10.00)\n",
      "Iter 0380 | Time 33.9839(35.8396) | Bit/dim 1.6128(1.4566) | Xent 0.2769(0.2168) | Loss 1.7512(1.5650) | Error 0.0889(0.0665) Steps 512(517.48) | Grad Norm 2.0796(4.7685) | Total Time 10.00(10.00)\n",
      "Iter 0381 | Time 36.1040(35.8476) | Bit/dim 1.5920(1.4606) | Xent 0.3753(0.2215) | Loss 1.7797(1.5714) | Error 0.1211(0.0682) Steps 530(517.85) | Grad Norm 2.7347(4.7075) | Total Time 10.00(10.00)\n",
      "Iter 0382 | Time 35.8482(35.8476) | Bit/dim 1.5795(1.4642) | Xent 0.3422(0.2252) | Loss 1.7506(1.5768) | Error 0.1099(0.0694) Steps 530(518.22) | Grad Norm 2.4274(4.6391) | Total Time 10.00(10.00)\n",
      "Iter 0383 | Time 35.9585(35.8509) | Bit/dim 1.6045(1.4684) | Xent 0.2457(0.2258) | Loss 1.7273(1.5813) | Error 0.0779(0.0697) Steps 530(518.57) | Grad Norm 2.5334(4.5760) | Total Time 10.00(10.00)\n",
      "Iter 0384 | Time 39.4686(35.9594) | Bit/dim 1.6308(1.4733) | Xent 0.2334(0.2260) | Loss 1.7475(1.5863) | Error 0.0726(0.0698) Steps 566(519.99) | Grad Norm 2.6979(4.5196) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0055 | Time 10.9012, Epoch Time 273.4912(259.8842), Bit/dim 1.5602, Xent 0.2290, Loss 1.6747, Error 0.2539\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0385 | Time 38.1688(36.0257) | Bit/dim 1.5687(1.4761) | Xent 0.2073(0.2254) | Loss 1.6723(1.5889) | Error 0.0645(0.0696) Steps 560(521.19) | Grad Norm 1.8052(4.4382) | Total Time 10.00(10.00)\n",
      "Iter 0386 | Time 40.0574(36.1467) | Bit/dim 1.5340(1.4779) | Xent 0.2233(0.2254) | Loss 1.6456(1.5906) | Error 0.0664(0.0695) Steps 572(522.72) | Grad Norm 1.9380(4.3632) | Total Time 10.00(10.00)\n",
      "Iter 0387 | Time 38.3214(36.2119) | Bit/dim 1.5378(1.4797) | Xent 0.2070(0.2248) | Loss 1.6412(1.5921) | Error 0.0599(0.0692) Steps 566(524.02) | Grad Norm 1.9942(4.2921) | Total Time 10.00(10.00)\n",
      "Iter 0388 | Time 38.8564(36.2913) | Bit/dim 1.5345(1.4813) | Xent 0.1913(0.2238) | Loss 1.6301(1.5932) | Error 0.0589(0.0689) Steps 560(525.10) | Grad Norm 1.6097(4.2116) | Total Time 10.00(10.00)\n",
      "Iter 0389 | Time 35.9273(36.2803) | Bit/dim 1.5169(1.4824) | Xent 0.1916(0.2228) | Loss 1.6127(1.5938) | Error 0.0596(0.0686) Steps 536(525.42) | Grad Norm 1.4841(4.1298) | Total Time 10.00(10.00)\n",
      "Iter 0390 | Time 36.4616(36.2858) | Bit/dim 1.4845(1.4825) | Xent 0.1847(0.2217) | Loss 1.5768(1.5933) | Error 0.0613(0.0684) Steps 524(525.38) | Grad Norm 1.1764(4.0412) | Total Time 10.00(10.00)\n",
      "Iter 0391 | Time 35.1870(36.2528) | Bit/dim 1.4638(1.4819) | Xent 0.1833(0.2206) | Loss 1.5555(1.5922) | Error 0.0540(0.0680) Steps 530(525.52) | Grad Norm 1.1612(3.9548) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0056 | Time 10.3103, Epoch Time 285.8953(260.6645), Bit/dim 1.4589, Xent 0.1937, Loss 1.5557, Error 0.2477\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0392 | Time 35.0059(36.2154) | Bit/dim 1.4712(1.4816) | Xent 0.1756(0.2192) | Loss 1.5590(1.5912) | Error 0.0559(0.0676) Steps 530(525.65) | Grad Norm 1.6475(3.8856) | Total Time 10.00(10.00)\n",
      "Iter 0393 | Time 35.3339(36.1890) | Bit/dim 1.4660(1.4811) | Xent 0.1881(0.2183) | Loss 1.5600(1.5902) | Error 0.0584(0.0673) Steps 524(525.60) | Grad Norm 1.6583(3.8188) | Total Time 10.00(10.00)\n",
      "Iter 0394 | Time 36.2608(36.1911) | Bit/dim 1.4286(1.4795) | Xent 0.1873(0.2173) | Loss 1.5222(1.5882) | Error 0.0570(0.0670) Steps 518(525.38) | Grad Norm 0.9692(3.7333) | Total Time 10.00(10.00)\n",
      "Iter 0395 | Time 35.1395(36.1596) | Bit/dim 1.4264(1.4779) | Xent 0.1686(0.2159) | Loss 1.5107(1.5859) | Error 0.0530(0.0666) Steps 518(525.16) | Grad Norm 1.5195(3.6669) | Total Time 10.00(10.00)\n",
      "Iter 0396 | Time 36.8922(36.1815) | Bit/dim 1.4361(1.4767) | Xent 0.1696(0.2145) | Loss 1.5209(1.5839) | Error 0.0520(0.0662) Steps 524(525.12) | Grad Norm 1.5016(3.6019) | Total Time 10.00(10.00)\n",
      "Iter 0397 | Time 37.3472(36.2165) | Bit/dim 1.4134(1.4748) | Xent 0.1816(0.2135) | Loss 1.5043(1.5815) | Error 0.0563(0.0659) Steps 542(525.63) | Grad Norm 1.1603(3.5287) | Total Time 10.00(10.00)\n",
      "Iter 0398 | Time 37.3248(36.2498) | Bit/dim 1.3934(1.4723) | Xent 0.1726(0.2123) | Loss 1.4797(1.5785) | Error 0.0541(0.0655) Steps 542(526.12) | Grad Norm 1.1775(3.4581) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0057 | Time 10.7037, Epoch Time 276.7468(261.1470), Bit/dim 1.3776, Xent 0.1793, Loss 1.4673, Error 0.2423\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0399 | Time 38.5049(36.3174) | Bit/dim 1.3973(1.4701) | Xent 0.1694(0.2110) | Loss 1.4820(1.5756) | Error 0.0540(0.0652) Steps 548(526.77) | Grad Norm 1.2270(3.3912) | Total Time 10.00(10.00)\n",
      "Iter 0400 | Time 37.5527(36.3545) | Bit/dim 1.3862(1.4676) | Xent 0.1617(0.2095) | Loss 1.4670(1.5723) | Error 0.0507(0.0647) Steps 548(527.41) | Grad Norm 1.1033(3.3226) | Total Time 10.00(10.00)\n",
      "Iter 0401 | Time 37.7326(36.3958) | Bit/dim 1.3775(1.4649) | Xent 0.1514(0.2078) | Loss 1.4532(1.5688) | Error 0.0450(0.0642) Steps 542(527.85) | Grad Norm 1.1971(3.2588) | Total Time 10.00(10.00)\n",
      "Iter 0402 | Time 37.2281(36.4208) | Bit/dim 1.3703(1.4620) | Xent 0.1706(0.2067) | Loss 1.4556(1.5654) | Error 0.0547(0.0639) Steps 530(527.91) | Grad Norm 1.0865(3.1936) | Total Time 10.00(10.00)\n",
      "Iter 0403 | Time 37.7560(36.4608) | Bit/dim 1.3749(1.4594) | Xent 0.1566(0.2052) | Loss 1.4532(1.5620) | Error 0.0510(0.0635) Steps 548(528.52) | Grad Norm 1.3388(3.1380) | Total Time 10.00(10.00)\n",
      "Iter 0404 | Time 37.3766(36.4883) | Bit/dim 1.3545(1.4563) | Xent 0.1713(0.2041) | Loss 1.4402(1.5583) | Error 0.0545(0.0632) Steps 542(528.92) | Grad Norm 1.1450(3.0782) | Total Time 10.00(10.00)\n",
      "Iter 0405 | Time 37.5752(36.5209) | Bit/dim 1.3454(1.4530) | Xent 0.1683(0.2031) | Loss 1.4295(1.5545) | Error 0.0514(0.0629) Steps 536(529.13) | Grad Norm 1.0119(3.0162) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0058 | Time 10.7121, Epoch Time 287.1323(261.9266), Bit/dim 1.3286, Xent 0.1738, Loss 1.4155, Error 0.2438\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0406 | Time 36.7147(36.5267) | Bit/dim 1.3405(1.4496) | Xent 0.1558(0.2016) | Loss 1.4184(1.5504) | Error 0.0487(0.0624) Steps 530(529.16) | Grad Norm 0.9861(2.9553) | Total Time 10.00(10.00)\n",
      "Iter 0407 | Time 37.8887(36.5676) | Bit/dim 1.3432(1.4464) | Xent 0.1508(0.2001) | Loss 1.4186(1.5464) | Error 0.0464(0.0620) Steps 548(529.72) | Grad Norm 1.0604(2.8985) | Total Time 10.00(10.00)\n",
      "Iter 0408 | Time 37.0389(36.5817) | Bit/dim 1.3389(1.4432) | Xent 0.1587(0.1989) | Loss 1.4183(1.5426) | Error 0.0505(0.0616) Steps 536(529.91) | Grad Norm 0.7934(2.8353) | Total Time 10.00(10.00)\n",
      "Iter 0409 | Time 36.8054(36.5884) | Bit/dim 1.3279(1.4397) | Xent 0.1742(0.1981) | Loss 1.4150(1.5388) | Error 0.0544(0.0614) Steps 536(530.10) | Grad Norm 1.1409(2.7845) | Total Time 10.00(10.00)\n",
      "Iter 0410 | Time 38.2484(36.6382) | Bit/dim 1.3223(1.4362) | Xent 0.1615(0.1970) | Loss 1.4031(1.5347) | Error 0.0487(0.0610) Steps 548(530.63) | Grad Norm 0.8126(2.7253) | Total Time 10.00(10.00)\n",
      "Iter 0411 | Time 36.5912(36.6368) | Bit/dim 1.3203(1.4327) | Xent 0.1725(0.1963) | Loss 1.4066(1.5309) | Error 0.0555(0.0609) Steps 542(530.97) | Grad Norm 1.3691(2.6846) | Total Time 10.00(10.00)\n",
      "Iter 0412 | Time 36.4460(36.6311) | Bit/dim 1.3132(1.4291) | Xent 0.1622(0.1953) | Loss 1.3943(1.5268) | Error 0.0511(0.0606) Steps 530(530.94) | Grad Norm 0.9825(2.6336) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0059 | Time 10.8212, Epoch Time 283.1044(262.5619), Bit/dim 1.2995, Xent 0.1712, Loss 1.3851, Error 0.2426\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0413 | Time 36.2622(36.6200) | Bit/dim 1.3153(1.4257) | Xent 0.1654(0.1944) | Loss 1.3980(1.5229) | Error 0.0507(0.0603) Steps 530(530.92) | Grad Norm 2.1281(2.6184) | Total Time 10.00(10.00)\n",
      "Iter 0414 | Time 37.1934(36.6372) | Bit/dim 1.3214(1.4226) | Xent 0.1703(0.1937) | Loss 1.4066(1.5194) | Error 0.0531(0.0601) Steps 530(530.89) | Grad Norm 5.3859(2.7014) | Total Time 10.00(10.00)\n",
      "Iter 0415 | Time 38.4398(36.6913) | Bit/dim 1.4238(1.4226) | Xent 0.1889(0.1935) | Loss 1.5183(1.5194) | Error 0.0554(0.0599) Steps 554(531.58) | Grad Norm 12.4987(2.9953) | Total Time 10.00(10.00)\n",
      "Iter 0416 | Time 37.9227(36.7283) | Bit/dim 1.4922(1.4247) | Xent 0.6404(0.2069) | Loss 1.8124(1.5282) | Error 0.2151(0.0646) Steps 536(531.71) | Grad Norm 25.4659(3.6695) | Total Time 10.00(10.00)\n",
      "Iter 0417 | Time 37.0783(36.7388) | Bit/dim 1.8229(1.4367) | Xent 0.2688(0.2088) | Loss 1.9573(1.5410) | Error 0.0733(0.0648) Steps 542(532.02) | Grad Norm 9.4809(3.8438) | Total Time 10.00(10.00)\n",
      "Iter 0418 | Time 38.2733(36.7848) | Bit/dim 1.8752(1.4498) | Xent 0.3354(0.2126) | Loss 2.0429(1.5561) | Error 0.0916(0.0656) Steps 542(532.32) | Grad Norm 9.3455(4.0089) | Total Time 10.00(10.00)\n",
      "Iter 0419 | Time 37.7478(36.8137) | Bit/dim 1.7162(1.4578) | Xent 0.2780(0.2145) | Loss 1.8552(1.5651) | Error 0.0936(0.0665) Steps 554(532.97) | Grad Norm 3.4268(3.9914) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0060 | Time 11.0475, Epoch Time 286.4022(263.2771), Bit/dim 1.6776, Xent 0.3937, Loss 1.8745, Error 0.2982\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0420 | Time 36.9994(36.8193) | Bit/dim 1.6932(1.4649) | Xent 0.3732(0.2193) | Loss 1.8798(1.5745) | Error 0.1156(0.0679) Steps 560(533.78) | Grad Norm 3.2161(3.9681) | Total Time 10.00(10.00)\n",
      "Iter 0421 | Time 37.3455(36.8350) | Bit/dim 1.6602(1.4707) | Xent 0.2927(0.2215) | Loss 1.8065(1.5815) | Error 0.0930(0.0687) Steps 554(534.39) | Grad Norm 2.8167(3.9336) | Total Time 10.00(10.00)\n",
      "Iter 0422 | Time 36.8280(36.8348) | Bit/dim 1.6351(1.4757) | Xent 0.3461(0.2252) | Loss 1.8082(1.5883) | Error 0.1125(0.0700) Steps 554(534.98) | Grad Norm 3.5269(3.9214) | Total Time 10.00(10.00)\n",
      "Iter 0423 | Time 37.9061(36.8670) | Bit/dim 1.6548(1.4810) | Xent 0.4676(0.2325) | Loss 1.8886(1.5973) | Error 0.1495(0.0724) Steps 572(536.09) | Grad Norm 8.9796(4.0731) | Total Time 10.00(10.00)\n",
      "Iter 0424 | Time 36.2201(36.8476) | Bit/dim 1.6390(1.4858) | Xent 0.9962(0.2554) | Loss 2.1371(1.6135) | Error 0.2386(0.0774) Steps 542(536.27) | Grad Norm 17.8887(4.4876) | Total Time 10.00(10.00)\n",
      "Iter 0425 | Time 38.2473(36.8896) | Bit/dim 1.7295(1.4931) | Xent 0.8838(0.2743) | Loss 2.1714(1.6302) | Error 0.2034(0.0812) Steps 572(537.34) | Grad Norm 9.3952(4.6348) | Total Time 10.00(10.00)\n",
      "Iter 0426 | Time 38.7198(36.9445) | Bit/dim 1.6753(1.4985) | Xent 0.4789(0.2804) | Loss 1.9148(1.6388) | Error 0.1326(0.0827) Steps 596(539.10) | Grad Norm 4.4852(4.6303) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0061 | Time 11.5464, Epoch Time 286.3900(263.9705), Bit/dim 1.6436, Xent 0.2902, Loss 1.7887, Error 0.2649\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0427 | Time 39.4408(37.0193) | Bit/dim 1.6566(1.5033) | Xent 0.2776(0.2803) | Loss 1.7954(1.6435) | Error 0.0814(0.0827) Steps 584(540.45) | Grad Norm 3.4469(4.5948) | Total Time 10.00(10.00)\n",
      "Iter 0428 | Time 38.0480(37.0502) | Bit/dim 1.6313(1.5071) | Xent 0.2307(0.2788) | Loss 1.7467(1.6465) | Error 0.0670(0.0822) Steps 572(541.39) | Grad Norm 2.4972(4.5319) | Total Time 10.00(10.00)\n",
      "Iter 0429 | Time 37.9590(37.0775) | Bit/dim 1.5958(1.5098) | Xent 0.2949(0.2793) | Loss 1.7433(1.6495) | Error 0.0837(0.0822) Steps 578(542.49) | Grad Norm 2.9086(4.4832) | Total Time 10.00(10.00)\n",
      "Iter 0430 | Time 38.3496(37.1156) | Bit/dim 1.5541(1.5111) | Xent 0.3475(0.2814) | Loss 1.7278(1.6518) | Error 0.1030(0.0829) Steps 572(543.38) | Grad Norm 3.5616(4.4556) | Total Time 10.00(10.00)\n",
      "Iter 0431 | Time 37.0338(37.1132) | Bit/dim 1.5517(1.5123) | Xent 0.3068(0.2821) | Loss 1.7051(1.6534) | Error 0.0972(0.0833) Steps 542(543.33) | Grad Norm 2.8855(4.4085) | Total Time 10.00(10.00)\n",
      "Iter 0432 | Time 36.6510(37.0993) | Bit/dim 1.5384(1.5131) | Xent 0.4193(0.2862) | Loss 1.7480(1.6562) | Error 0.1384(0.0849) Steps 542(543.29) | Grad Norm 6.8525(4.4818) | Total Time 10.00(10.00)\n",
      "Iter 0433 | Time 37.3177(37.1059) | Bit/dim 1.5657(1.5147) | Xent 1.0893(0.3103) | Loss 2.1103(1.6699) | Error 0.2660(0.0904) Steps 560(543.80) | Grad Norm 13.5261(4.7531) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0062 | Time 10.6998, Epoch Time 288.2637(264.6993), Bit/dim 1.5631, Xent 0.5583, Loss 1.8422, Error 0.3181\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0434 | Time 36.4701(37.0868) | Bit/dim 1.5852(1.5168) | Xent 0.6040(0.3191) | Loss 1.8872(1.6764) | Error 0.1476(0.0921) Steps 554(544.10) | Grad Norm 7.1805(4.8259) | Total Time 10.00(10.00)\n",
      "Iter 0435 | Time 36.6600(37.0740) | Bit/dim 1.6603(1.5211) | Xent 0.4038(0.3217) | Loss 1.8621(1.6820) | Error 0.0970(0.0922) Steps 554(544.40) | Grad Norm 5.1345(4.8352) | Total Time 10.00(10.00)\n",
      "Iter 0436 | Time 36.5312(37.0577) | Bit/dim 1.6416(1.5247) | Xent 0.3247(0.3218) | Loss 1.8040(1.6856) | Error 0.0794(0.0919) Steps 536(544.15) | Grad Norm 4.1275(4.8140) | Total Time 10.00(10.00)\n",
      "Iter 0437 | Time 36.3270(37.0358) | Bit/dim 1.5692(1.5261) | Xent 0.3225(0.3218) | Loss 1.7305(1.6870) | Error 0.0933(0.0919) Steps 542(544.08) | Grad Norm 3.2953(4.7684) | Total Time 10.00(10.00)\n",
      "Iter 0438 | Time 36.2284(37.0116) | Bit/dim 1.5294(1.5262) | Xent 0.3383(0.3223) | Loss 1.6985(1.6873) | Error 0.1079(0.0924) Steps 542(544.02) | Grad Norm 2.2707(4.6935) | Total Time 10.00(10.00)\n",
      "Iter 0439 | Time 37.1085(37.0145) | Bit/dim 1.5185(1.5259) | Xent 0.3656(0.3236) | Loss 1.7013(1.6877) | Error 0.1154(0.0931) Steps 554(544.32) | Grad Norm 4.0490(4.6741) | Total Time 10.00(10.00)\n",
      "Iter 0440 | Time 36.4700(36.9981) | Bit/dim 1.4862(1.5247) | Xent 0.2972(0.3228) | Loss 1.6348(1.6861) | Error 0.1009(0.0933) Steps 554(544.61) | Grad Norm 2.2174(4.6004) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0063 | Time 11.1946, Epoch Time 279.6666(265.1483), Bit/dim 1.4803, Xent 0.3123, Loss 1.6364, Error 0.2818\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0441 | Time 37.0752(37.0004) | Bit/dim 1.4918(1.5238) | Xent 0.2852(0.3217) | Loss 1.6344(1.6846) | Error 0.0901(0.0932) Steps 548(544.71) | Grad Norm 1.8307(4.5173) | Total Time 10.00(10.00)\n",
      "Iter 0442 | Time 38.6475(37.0499) | Bit/dim 1.5072(1.5233) | Xent 0.2745(0.3202) | Loss 1.6445(1.6834) | Error 0.0864(0.0930) Steps 560(545.17) | Grad Norm 1.9760(4.4411) | Total Time 10.00(10.00)\n",
      "Iter 0443 | Time 38.1202(37.0820) | Bit/dim 1.4774(1.5219) | Xent 0.2186(0.3172) | Loss 1.5868(1.6805) | Error 0.0679(0.0923) Steps 572(545.97) | Grad Norm 1.8003(4.3619) | Total Time 10.00(10.00)\n",
      "Iter 0444 | Time 37.0863(37.0821) | Bit/dim 1.4667(1.5202) | Xent 0.2394(0.3149) | Loss 1.5864(1.6777) | Error 0.0731(0.0917) Steps 560(546.40) | Grad Norm 1.3724(4.2722) | Total Time 10.00(10.00)\n",
      "Iter 0445 | Time 38.7433(37.1319) | Bit/dim 1.4806(1.5190) | Xent 0.2646(0.3134) | Loss 1.6129(1.6757) | Error 0.0846(0.0915) Steps 560(546.80) | Grad Norm 2.7962(4.2279) | Total Time 10.00(10.00)\n",
      "Iter 0446 | Time 37.2844(37.1365) | Bit/dim 1.4357(1.5165) | Xent 0.2298(0.3108) | Loss 1.5506(1.6720) | Error 0.0717(0.0909) Steps 560(547.20) | Grad Norm 1.9576(4.1598) | Total Time 10.00(10.00)\n",
      "Iter 0447 | Time 38.1460(37.1668) | Bit/dim 1.4368(1.5141) | Xent 0.2408(0.3087) | Loss 1.5572(1.6685) | Error 0.0706(0.0903) Steps 554(547.40) | Grad Norm 2.1899(4.1007) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0064 | Time 11.2778, Epoch Time 288.7830(265.8573), Bit/dim 1.4162, Xent 0.2410, Loss 1.5367, Error 0.2603\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0448 | Time 37.4706(37.1759) | Bit/dim 1.4348(1.5118) | Xent 0.2206(0.3061) | Loss 1.5451(1.6648) | Error 0.0685(0.0896) Steps 554(547.60) | Grad Norm 1.6945(4.0285) | Total Time 10.00(10.00)\n",
      "Iter 0449 | Time 36.7778(37.1640) | Bit/dim 1.4204(1.5090) | Xent 0.2386(0.3041) | Loss 1.5398(1.6611) | Error 0.0769(0.0892) Steps 548(547.61) | Grad Norm 2.8983(3.9946) | Total Time 10.00(10.00)\n",
      "Iter 0450 | Time 38.3512(37.1996) | Bit/dim 1.3937(1.5056) | Xent 0.2227(0.3016) | Loss 1.5051(1.6564) | Error 0.0727(0.0887) Steps 572(548.34) | Grad Norm 1.1137(3.9082) | Total Time 10.00(10.00)\n",
      "Iter 0451 | Time 38.0061(37.2238) | Bit/dim 1.3906(1.5021) | Xent 0.2332(0.2996) | Loss 1.5072(1.6519) | Error 0.0735(0.0883) Steps 578(549.23) | Grad Norm 1.7997(3.8449) | Total Time 10.00(10.00)\n",
      "Iter 0452 | Time 38.9999(37.2771) | Bit/dim 1.3870(1.4987) | Xent 0.2204(0.2972) | Loss 1.4972(1.6473) | Error 0.0705(0.0877) Steps 572(549.92) | Grad Norm 0.9962(3.7595) | Total Time 10.00(10.00)\n",
      "Iter 0453 | Time 37.9798(37.2981) | Bit/dim 1.3822(1.4952) | Xent 0.2283(0.2951) | Loss 1.4963(1.6427) | Error 0.0709(0.0872) Steps 572(550.58) | Grad Norm 1.1718(3.6818) | Total Time 10.00(10.00)\n",
      "Iter 0454 | Time 38.1193(37.3228) | Bit/dim 1.3852(1.4919) | Xent 0.2131(0.2927) | Loss 1.4918(1.6382) | Error 0.0685(0.0867) Steps 572(551.22) | Grad Norm 1.1445(3.6057) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0065 | Time 11.1587, Epoch Time 289.4540(266.5652), Bit/dim 1.3588, Xent 0.2278, Loss 1.4727, Error 0.2573\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0455 | Time 38.3839(37.3546) | Bit/dim 1.3716(1.4883) | Xent 0.2102(0.2902) | Loss 1.4767(1.6334) | Error 0.0665(0.0861) Steps 572(551.85) | Grad Norm 1.0866(3.5301) | Total Time 10.00(10.00)\n",
      "Iter 0456 | Time 37.9738(37.3732) | Bit/dim 1.3671(1.4846) | Xent 0.2100(0.2878) | Loss 1.4721(1.6285) | Error 0.0653(0.0854) Steps 560(552.09) | Grad Norm 0.8559(3.4499) | Total Time 10.00(10.00)\n",
      "Iter 0457 | Time 39.9110(37.4493) | Bit/dim 1.3687(1.4812) | Xent 0.2096(0.2855) | Loss 1.4735(1.6239) | Error 0.0640(0.0848) Steps 572(552.69) | Grad Norm 0.8949(3.3733) | Total Time 10.00(10.00)\n",
      "Iter 0458 | Time 39.2871(37.5045) | Bit/dim 1.3608(1.4775) | Xent 0.2007(0.2829) | Loss 1.4612(1.6190) | Error 0.0626(0.0841) Steps 578(553.45) | Grad Norm 1.7630(3.3250) | Total Time 10.00(10.00)\n",
      "Iter 0459 | Time 40.0262(37.5801) | Bit/dim 1.3505(1.4737) | Xent 0.2087(0.2807) | Loss 1.4549(1.6141) | Error 0.0651(0.0836) Steps 590(554.54) | Grad Norm 0.8039(3.2493) | Total Time 10.00(10.00)\n",
      "Iter 0460 | Time 39.8897(37.6494) | Bit/dim 1.3427(1.4698) | Xent 0.1951(0.2781) | Loss 1.4403(1.6089) | Error 0.0640(0.0830) Steps 566(554.89) | Grad Norm 1.1224(3.1855) | Total Time 10.00(10.00)\n",
      "Iter 0461 | Time 40.6373(37.7390) | Bit/dim 1.3476(1.4661) | Xent 0.1914(0.2755) | Loss 1.4434(1.6039) | Error 0.0599(0.0823) Steps 578(555.58) | Grad Norm 1.7468(3.1424) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0066 | Time 11.3756, Epoch Time 300.0373(267.5694), Bit/dim 1.3238, Xent 0.2241, Loss 1.4359, Error 0.2591\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0462 | Time 39.8186(37.8014) | Bit/dim 1.3370(1.4623) | Xent 0.2028(0.2733) | Loss 1.4384(1.5989) | Error 0.0646(0.0818) Steps 578(556.25) | Grad Norm 3.6143(3.1565) | Total Time 10.00(10.00)\n",
      "Iter 0463 | Time 39.9152(37.8648) | Bit/dim 1.3610(1.4592) | Xent 0.1947(0.2710) | Loss 1.4584(1.5947) | Error 0.0610(0.0811) Steps 578(556.91) | Grad Norm 5.5690(3.2289) | Total Time 10.00(10.00)\n",
      "Iter 0464 | Time 38.9185(37.8964) | Bit/dim 1.3472(1.4559) | Xent 0.2676(0.2709) | Loss 1.4810(1.5913) | Error 0.0879(0.0813) Steps 572(557.36) | Grad Norm 8.5989(3.3900) | Total Time 10.00(10.00)\n",
      "Iter 0465 | Time 40.5928(37.9773) | Bit/dim 1.4576(1.4559) | Xent 0.2543(0.2704) | Loss 1.5847(1.5911) | Error 0.0700(0.0810) Steps 596(558.52) | Grad Norm 10.0683(3.5903) | Total Time 10.00(10.00)\n",
      "Iter 0466 | Time 39.2841(38.0165) | Bit/dim 1.3328(1.4522) | Xent 0.1894(0.2679) | Loss 1.4275(1.5862) | Error 0.0590(0.0803) Steps 572(558.92) | Grad Norm 0.8755(3.5089) | Total Time 10.00(10.00)\n",
      "Iter 0467 | Time 38.5586(38.0328) | Bit/dim 1.4053(1.4508) | Xent 0.4129(0.2723) | Loss 1.6118(1.5870) | Error 0.1438(0.0822) Steps 560(558.95) | Grad Norm 17.4401(3.9268) | Total Time 10.00(10.00)\n",
      "Iter 0468 | Time 39.3340(38.0718) | Bit/dim 1.6733(1.4575) | Xent 0.2323(0.2711) | Loss 1.7894(1.5930) | Error 0.0633(0.0817) Steps 578(559.53) | Grad Norm 8.7322(4.0710) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0067 | Time 10.9865, Epoch Time 299.9678(268.5414), Bit/dim 1.7091, Xent 0.2082, Loss 1.8132, Error 0.2458\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0469 | Time 40.3759(38.1410) | Bit/dim 1.7267(1.4656) | Xent 0.2135(0.2694) | Loss 1.8335(1.6003) | Error 0.0590(0.0810) Steps 590(560.44) | Grad Norm 6.9755(4.1581) | Total Time 10.00(10.00)\n",
      "Iter 0470 | Time 41.1153(38.2302) | Bit/dim 1.5791(1.4690) | Xent 0.1787(0.2666) | Loss 1.6684(1.6023) | Error 0.0536(0.0802) Steps 602(561.69) | Grad Norm 4.3501(4.1639) | Total Time 10.00(10.00)\n",
      "Iter 0471 | Time 41.9138(38.3407) | Bit/dim 1.4888(1.4696) | Xent 0.2466(0.2660) | Loss 1.6121(1.6026) | Error 0.0830(0.0803) Steps 578(562.18) | Grad Norm 2.4556(4.1126) | Total Time 10.00(10.00)\n",
      "Iter 0472 | Time 40.1948(38.3963) | Bit/dim 1.5016(1.4705) | Xent 0.2684(0.2661) | Loss 1.6358(1.6036) | Error 0.0854(0.0804) Steps 572(562.47) | Grad Norm 2.8533(4.0749) | Total Time 10.00(10.00)\n",
      "Iter 0473 | Time 38.0192(38.3850) | Bit/dim 1.5635(1.4733) | Xent 0.2566(0.2658) | Loss 1.6918(1.6062) | Error 0.0810(0.0804) Steps 572(562.76) | Grad Norm 4.2109(4.0789) | Total Time 10.00(10.00)\n",
      "Iter 0474 | Time 39.3419(38.4137) | Bit/dim 1.5284(1.4750) | Xent 0.2087(0.2641) | Loss 1.6328(1.6070) | Error 0.0671(0.0800) Steps 578(563.21) | Grad Norm 3.1872(4.0522) | Total Time 10.00(10.00)\n",
      "Iter 0475 | Time 37.2457(38.3787) | Bit/dim 1.4881(1.4754) | Xent 0.1901(0.2619) | Loss 1.5832(1.6063) | Error 0.0581(0.0794) Steps 566(563.30) | Grad Norm 1.7612(3.9835) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0068 | Time 10.9709, Epoch Time 301.8272(269.5399), Bit/dim 1.4641, Xent 0.1760, Loss 1.5521, Error 0.2445\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0476 | Time 38.4150(38.3798) | Bit/dim 1.4745(1.4753) | Xent 0.1675(0.2591) | Loss 1.5583(1.6049) | Error 0.0523(0.0786) Steps 566(563.38) | Grad Norm 1.6120(3.9123) | Total Time 10.00(10.00)\n",
      "Iter 0477 | Time 37.7462(38.3608) | Bit/dim 1.4903(1.4758) | Xent 0.1701(0.2564) | Loss 1.5754(1.6040) | Error 0.0513(0.0777) Steps 554(563.10) | Grad Norm 2.0395(3.8561) | Total Time 10.00(10.00)\n",
      "Iter 0478 | Time 38.4293(38.3628) | Bit/dim 1.4833(1.4760) | Xent 0.1826(0.2542) | Loss 1.5746(1.6031) | Error 0.0560(0.0771) Steps 566(563.18) | Grad Norm 2.1209(3.8041) | Total Time 10.00(10.00)\n",
      "Iter 0479 | Time 38.5056(38.3671) | Bit/dim 1.4482(1.4752) | Xent 0.1674(0.2516) | Loss 1.5319(1.6010) | Error 0.0506(0.0763) Steps 572(563.45) | Grad Norm 1.5392(3.7361) | Total Time 10.00(10.00)\n",
      "Iter 0480 | Time 39.3788(38.3974) | Bit/dim 1.4264(1.4737) | Xent 0.1608(0.2489) | Loss 1.5068(1.5981) | Error 0.0517(0.0756) Steps 572(563.71) | Grad Norm 1.1442(3.6584) | Total Time 10.00(10.00)\n",
      "Iter 0481 | Time 38.0133(38.3859) | Bit/dim 1.4224(1.4722) | Xent 0.1740(0.2466) | Loss 1.5094(1.5955) | Error 0.0569(0.0750) Steps 566(563.77) | Grad Norm 2.0009(3.6086) | Total Time 10.00(10.00)\n",
      "Iter 0482 | Time 37.3612(38.3552) | Bit/dim 1.4341(1.4710) | Xent 0.1815(0.2447) | Loss 1.5249(1.5934) | Error 0.0607(0.0746) Steps 548(563.30) | Grad Norm 2.6443(3.5797) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0069 | Time 10.8302, Epoch Time 291.4183(270.1963), Bit/dim 1.3926, Xent 0.1686, Loss 1.4769, Error 0.2413\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0483 | Time 38.9240(38.3722) | Bit/dim 1.4058(1.4691) | Xent 0.1434(0.2416) | Loss 1.4775(1.5899) | Error 0.0455(0.0737) Steps 548(562.84) | Grad Norm 1.4002(3.5143) | Total Time 10.00(10.00)\n",
      "Iter 0484 | Time 38.5540(38.3777) | Bit/dim 1.3985(1.4670) | Xent 0.1649(0.2393) | Loss 1.4809(1.5866) | Error 0.0523(0.0731) Steps 560(562.76) | Grad Norm 1.6084(3.4572) | Total Time 10.00(10.00)\n",
      "Iter 0485 | Time 38.5348(38.3824) | Bit/dim 1.4080(1.4652) | Xent 0.1649(0.2371) | Loss 1.4905(1.5837) | Error 0.0503(0.0724) Steps 560(562.67) | Grad Norm 2.4386(3.4266) | Total Time 10.00(10.00)\n",
      "Iter 0486 | Time 38.2155(38.3774) | Bit/dim 1.3903(1.4629) | Xent 0.1617(0.2348) | Loss 1.4711(1.5804) | Error 0.0476(0.0716) Steps 560(562.59) | Grad Norm 1.9065(3.3810) | Total Time 10.00(10.00)\n",
      "Iter 0487 | Time 37.2888(38.3447) | Bit/dim 1.3743(1.4603) | Xent 0.1661(0.2328) | Loss 1.4573(1.5767) | Error 0.0513(0.0710) Steps 542(561.98) | Grad Norm 0.8730(3.3058) | Total Time 10.00(10.00)\n",
      "Iter 0488 | Time 36.7428(38.2967) | Bit/dim 1.3744(1.4577) | Xent 0.1677(0.2308) | Loss 1.4583(1.5731) | Error 0.0535(0.0705) Steps 536(561.20) | Grad Norm 2.9554(3.2952) | Total Time 10.00(10.00)\n",
      "Iter 0489 | Time 36.1858(38.2334) | Bit/dim 1.3678(1.4550) | Xent 0.1692(0.2290) | Loss 1.4524(1.5695) | Error 0.0543(0.0700) Steps 542(560.62) | Grad Norm 2.0810(3.2588) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0070 | Time 10.8338, Epoch Time 288.0694(270.7325), Bit/dim 1.3479, Xent 0.1591, Loss 1.4275, Error 0.2401\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0490 | Time 37.5792(38.2137) | Bit/dim 1.3619(1.4522) | Xent 0.1529(0.2267) | Loss 1.4384(1.5656) | Error 0.0477(0.0693) Steps 548(560.24) | Grad Norm 1.0027(3.1911) | Total Time 10.00(10.00)\n",
      "Iter 0491 | Time 37.8461(38.2027) | Bit/dim 1.3653(1.4496) | Xent 0.1615(0.2247) | Loss 1.4460(1.5620) | Error 0.0509(0.0688) Steps 554(560.06) | Grad Norm 2.1161(3.1589) | Total Time 10.00(10.00)\n",
      "Iter 0492 | Time 38.8627(38.2225) | Bit/dim 1.3597(1.4469) | Xent 0.1625(0.2229) | Loss 1.4410(1.5583) | Error 0.0486(0.0682) Steps 548(559.69) | Grad Norm 1.5673(3.1111) | Total Time 10.00(10.00)\n",
      "Iter 0493 | Time 36.2272(38.1626) | Bit/dim 1.3449(1.4439) | Xent 0.1465(0.2206) | Loss 1.4182(1.5541) | Error 0.0484(0.0676) Steps 536(558.98) | Grad Norm 1.0603(3.0496) | Total Time 10.00(10.00)\n",
      "Iter 0494 | Time 36.7106(38.1191) | Bit/dim 1.3455(1.4409) | Xent 0.1742(0.2192) | Loss 1.4327(1.5505) | Error 0.0545(0.0672) Steps 542(558.47) | Grad Norm 1.6575(3.0078) | Total Time 10.00(10.00)\n",
      "Iter 0495 | Time 36.8828(38.0820) | Bit/dim 1.3423(1.4379) | Xent 0.1495(0.2171) | Loss 1.4170(1.5465) | Error 0.0466(0.0666) Steps 542(557.98) | Grad Norm 1.0675(2.9496) | Total Time 10.00(10.00)\n",
      "Iter 0496 | Time 40.2487(38.1470) | Bit/dim 1.3415(1.4351) | Xent 0.1474(0.2150) | Loss 1.4152(1.5426) | Error 0.0461(0.0660) Steps 548(557.68) | Grad Norm 1.0267(2.8920) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0071 | Time 10.7351, Epoch Time 287.7863(271.2441), Bit/dim 1.3206, Xent 0.1572, Loss 1.3992, Error 0.2407\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0497 | Time 37.4208(38.1252) | Bit/dim 1.3347(1.4320) | Xent 0.1447(0.2129) | Loss 1.4071(1.5385) | Error 0.0469(0.0654) Steps 548(557.39) | Grad Norm 1.1638(2.8401) | Total Time 10.00(10.00)\n",
      "Iter 0498 | Time 37.7097(38.1127) | Bit/dim 1.3251(1.4288) | Xent 0.1375(0.2106) | Loss 1.3939(1.5341) | Error 0.0420(0.0647) Steps 554(557.29) | Grad Norm 0.8883(2.7816) | Total Time 10.00(10.00)\n",
      "Iter 0499 | Time 38.0748(38.1116) | Bit/dim 1.3245(1.4257) | Xent 0.1475(0.2087) | Loss 1.3983(1.5301) | Error 0.0470(0.0642) Steps 554(557.19) | Grad Norm 0.9917(2.7279) | Total Time 10.00(10.00)\n",
      "Iter 0500 | Time 38.5242(38.1240) | Bit/dim 1.3266(1.4227) | Xent 0.1574(0.2072) | Loss 1.4052(1.5263) | Error 0.0506(0.0637) Steps 554(557.09) | Grad Norm 0.9592(2.6748) | Total Time 10.00(10.00)\n",
      "Iter 0501 | Time 37.6430(38.1096) | Bit/dim 1.3182(1.4196) | Xent 0.1450(0.2053) | Loss 1.3907(1.5223) | Error 0.0466(0.0632) Steps 554(557.00) | Grad Norm 0.9398(2.6227) | Total Time 10.00(10.00)\n",
      "Iter 0502 | Time 37.5738(38.0935) | Bit/dim 1.3086(1.4163) | Xent 0.1594(0.2039) | Loss 1.3883(1.5182) | Error 0.0484(0.0628) Steps 554(556.91) | Grad Norm 0.9691(2.5731) | Total Time 10.00(10.00)\n",
      "Iter 0503 | Time 39.0588(38.1224) | Bit/dim 1.3123(1.4131) | Xent 0.1539(0.2024) | Loss 1.3893(1.5144) | Error 0.0485(0.0624) Steps 554(556.82) | Grad Norm 1.0906(2.5287) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0072 | Time 11.0493, Epoch Time 289.7181(271.7983), Bit/dim 1.2924, Xent 0.1617, Loss 1.3732, Error 0.2415\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0504 | Time 38.2065(38.1250) | Bit/dim 1.3050(1.4099) | Xent 0.1446(0.2007) | Loss 1.3773(1.5103) | Error 0.0449(0.0618) Steps 554(556.74) | Grad Norm 0.8398(2.4780) | Total Time 10.00(10.00)\n",
      "Iter 0505 | Time 38.0136(38.1216) | Bit/dim 1.3109(1.4069) | Xent 0.1324(0.1987) | Loss 1.3771(1.5063) | Error 0.0441(0.0613) Steps 560(556.84) | Grad Norm 0.7486(2.4261) | Total Time 10.00(10.00)\n",
      "Iter 0506 | Time 38.4772(38.1323) | Bit/dim 1.3069(1.4039) | Xent 0.1564(0.1974) | Loss 1.3851(1.5026) | Error 0.0483(0.0609) Steps 560(556.93) | Grad Norm 1.0106(2.3836) | Total Time 10.00(10.00)\n",
      "Iter 0507 | Time 38.2520(38.1359) | Bit/dim 1.2995(1.4008) | Xent 0.1377(0.1956) | Loss 1.3683(1.4986) | Error 0.0423(0.0604) Steps 560(557.02) | Grad Norm 0.5371(2.3283) | Total Time 10.00(10.00)\n",
      "Iter 0508 | Time 38.8544(38.1574) | Bit/dim 1.3077(1.3980) | Xent 0.1529(0.1943) | Loss 1.3841(1.4952) | Error 0.0463(0.0599) Steps 554(556.93) | Grad Norm 1.0125(2.2888) | Total Time 10.00(10.00)\n",
      "Iter 0509 | Time 40.1435(38.2170) | Bit/dim 1.2918(1.3948) | Xent 0.1395(0.1927) | Loss 1.3616(1.4912) | Error 0.0430(0.0594) Steps 578(557.56) | Grad Norm 0.9449(2.2485) | Total Time 10.00(10.00)\n",
      "Iter 0510 | Time 38.1591(38.2153) | Bit/dim 1.3003(1.3920) | Xent 0.1465(0.1913) | Loss 1.3735(1.4876) | Error 0.0473(0.0591) Steps 560(557.64) | Grad Norm 0.7735(2.2042) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0073 | Time 11.0554, Epoch Time 293.5497(272.4509), Bit/dim 1.2802, Xent 0.1493, Loss 1.3549, Error 0.2377\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0511 | Time 38.3242(38.2186) | Bit/dim 1.3000(1.3892) | Xent 0.1353(0.1896) | Loss 1.3676(1.4840) | Error 0.0423(0.0586) Steps 560(557.71) | Grad Norm 1.1925(2.1739) | Total Time 10.00(10.00)\n",
      "Iter 0512 | Time 40.1094(38.2753) | Bit/dim 1.2930(1.3863) | Xent 0.1558(0.1886) | Loss 1.3709(1.4806) | Error 0.0481(0.0582) Steps 578(558.32) | Grad Norm 1.3275(2.1485) | Total Time 10.00(10.00)\n",
      "Iter 0513 | Time 38.7469(38.2894) | Bit/dim 1.2892(1.3834) | Xent 0.1425(0.1872) | Loss 1.3604(1.4770) | Error 0.0465(0.0579) Steps 572(558.73) | Grad Norm 1.0936(2.1168) | Total Time 10.00(10.00)\n",
      "Iter 0514 | Time 39.9896(38.3404) | Bit/dim 1.2881(1.3806) | Xent 0.1475(0.1860) | Loss 1.3619(1.4736) | Error 0.0454(0.0575) Steps 578(559.31) | Grad Norm 0.5888(2.0710) | Total Time 10.00(10.00)\n",
      "Iter 0515 | Time 40.0015(38.3903) | Bit/dim 1.2828(1.3776) | Xent 0.1391(0.1846) | Loss 1.3523(1.4699) | Error 0.0425(0.0571) Steps 578(559.87) | Grad Norm 1.3268(2.0487) | Total Time 10.00(10.00)\n",
      "Iter 0516 | Time 38.0483(38.3800) | Bit/dim 1.2853(1.3749) | Xent 0.1280(0.1829) | Loss 1.3493(1.4663) | Error 0.0434(0.0567) Steps 554(559.69) | Grad Norm 1.9663(2.0462) | Total Time 10.00(10.00)\n",
      "Iter 0517 | Time 40.0294(38.4295) | Bit/dim 1.2783(1.3720) | Xent 0.1523(0.1820) | Loss 1.3545(1.4630) | Error 0.0497(0.0564) Steps 578(560.24) | Grad Norm 2.0528(2.0464) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0074 | Time 11.1362, Epoch Time 298.7908(273.2411), Bit/dim 1.2747, Xent 0.1372, Loss 1.3433, Error 0.2363\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0518 | Time 39.3481(38.4570) | Bit/dim 1.2870(1.3694) | Xent 0.1422(0.1808) | Loss 1.3581(1.4598) | Error 0.0414(0.0560) Steps 554(560.05) | Grad Norm 2.4301(2.0579) | Total Time 10.00(10.00)\n",
      "Iter 0519 | Time 39.7624(38.4962) | Bit/dim 1.2835(1.3668) | Xent 0.1538(0.1800) | Loss 1.3604(1.4568) | Error 0.0485(0.0558) Steps 578(560.59) | Grad Norm 3.4445(2.0995) | Total Time 10.00(10.00)\n",
      "Iter 0520 | Time 38.7501(38.5038) | Bit/dim 1.2995(1.3648) | Xent 0.1290(0.1785) | Loss 1.3640(1.4540) | Error 0.0386(0.0553) Steps 554(560.39) | Grad Norm 4.1186(2.1601) | Total Time 10.00(10.00)\n",
      "Iter 0521 | Time 39.8704(38.5448) | Bit/dim 1.2866(1.3625) | Xent 0.1603(0.1779) | Loss 1.3668(1.4514) | Error 0.0483(0.0550) Steps 572(560.74) | Grad Norm 4.6290(2.2341) | Total Time 10.00(10.00)\n",
      "Iter 0522 | Time 38.2213(38.5351) | Bit/dim 1.3181(1.3611) | Xent 0.1299(0.1765) | Loss 1.3830(1.4494) | Error 0.0415(0.0546) Steps 554(560.54) | Grad Norm 5.3465(2.3275) | Total Time 10.00(10.00)\n",
      "Iter 0523 | Time 39.7953(38.5729) | Bit/dim 1.2850(1.3589) | Xent 0.1530(0.1758) | Loss 1.3616(1.4467) | Error 0.0459(0.0544) Steps 578(561.06) | Grad Norm 4.6223(2.3964) | Total Time 10.00(10.00)\n",
      "Iter 0524 | Time 37.2756(38.5340) | Bit/dim 1.2904(1.3568) | Xent 0.1341(0.1745) | Loss 1.3574(1.4441) | Error 0.0419(0.0540) Steps 554(560.85) | Grad Norm 3.1344(2.4185) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0075 | Time 10.9990, Epoch Time 296.5893(273.9415), Bit/dim 1.2609, Xent 0.1531, Loss 1.3374, Error 0.2406\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0525 | Time 40.4042(38.5901) | Bit/dim 1.2767(1.3544) | Xent 0.1509(0.1738) | Loss 1.3522(1.4413) | Error 0.0436(0.0537) Steps 572(561.19) | Grad Norm 1.6083(2.3942) | Total Time 10.00(10.00)\n",
      "Iter 0526 | Time 39.3367(38.6125) | Bit/dim 1.2727(1.3519) | Xent 0.1317(0.1726) | Loss 1.3385(1.4382) | Error 0.0391(0.0533) Steps 572(561.51) | Grad Norm 1.1092(2.3556) | Total Time 10.00(10.00)\n",
      "Iter 0527 | Time 37.4840(38.5787) | Bit/dim 1.2814(1.3498) | Xent 0.1193(0.1710) | Loss 1.3411(1.4353) | Error 0.0364(0.0527) Steps 554(561.28) | Grad Norm 2.8951(2.3718) | Total Time 10.00(10.00)\n",
      "Iter 0528 | Time 39.7712(38.6144) | Bit/dim 1.2794(1.3477) | Xent 0.1501(0.1703) | Loss 1.3544(1.4329) | Error 0.0467(0.0526) Steps 572(561.61) | Grad Norm 4.2785(2.4290) | Total Time 10.00(10.00)\n",
      "Iter 0529 | Time 37.8762(38.5923) | Bit/dim 1.3149(1.3467) | Xent 0.1298(0.1691) | Loss 1.3798(1.4313) | Error 0.0401(0.0522) Steps 548(561.20) | Grad Norm 5.2108(2.5125) | Total Time 10.00(10.00)\n",
      "Iter 0530 | Time 40.7868(38.6581) | Bit/dim 1.2675(1.3444) | Xent 0.1361(0.1681) | Loss 1.3355(1.4284) | Error 0.0427(0.0519) Steps 572(561.52) | Grad Norm 2.9679(2.5261) | Total Time 10.00(10.00)\n",
      "Iter 0531 | Time 40.2210(38.7050) | Bit/dim 1.2737(1.3422) | Xent 0.1347(0.1671) | Loss 1.3411(1.4258) | Error 0.0436(0.0517) Steps 566(561.66) | Grad Norm 1.5446(2.4967) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0076 | Time 10.9856, Epoch Time 299.5190(274.7088), Bit/dim 1.2764, Xent 0.1289, Loss 1.3409, Error 0.2320\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0532 | Time 37.7548(38.6765) | Bit/dim 1.2862(1.3406) | Xent 0.1288(0.1660) | Loss 1.3506(1.4235) | Error 0.0405(0.0513) Steps 554(561.43) | Grad Norm 3.9161(2.5393) | Total Time 10.00(10.00)\n",
      "Iter 0533 | Time 39.8052(38.7104) | Bit/dim 1.2921(1.3391) | Xent 0.1431(0.1653) | Loss 1.3636(1.4217) | Error 0.0464(0.0512) Steps 572(561.74) | Grad Norm 6.0172(2.6436) | Total Time 10.00(10.00)\n",
      "Iter 0534 | Time 39.2191(38.7256) | Bit/dim 1.3651(1.3399) | Xent 0.1203(0.1639) | Loss 1.4253(1.4219) | Error 0.0396(0.0508) Steps 548(561.33) | Grad Norm 7.2297(2.7812) | Total Time 10.00(10.00)\n",
      "Iter 0535 | Time 36.9149(38.6713) | Bit/dim 1.2757(1.3380) | Xent 0.1377(0.1631) | Loss 1.3446(1.4195) | Error 0.0441(0.0506) Steps 548(560.93) | Grad Norm 1.1056(2.7309) | Total Time 10.00(10.00)\n",
      "Iter 0536 | Time 40.4985(38.7261) | Bit/dim 1.4013(1.3399) | Xent 0.1917(0.1640) | Loss 1.4972(1.4219) | Error 0.0591(0.0509) Steps 560(560.90) | Grad Norm 14.2123(3.0754) | Total Time 10.00(10.00)\n",
      "Iter 0537 | Time 37.3840(38.6859) | Bit/dim 1.6166(1.3482) | Xent 0.1185(0.1626) | Loss 1.6759(1.4295) | Error 0.0366(0.0505) Steps 536(560.16) | Grad Norm 8.6452(3.2425) | Total Time 10.00(10.00)\n",
      "Iter 0538 | Time 37.9093(38.6626) | Bit/dim 1.6817(1.3582) | Xent 0.1088(0.1610) | Loss 1.7361(1.4387) | Error 0.0337(0.0500) Steps 536(559.43) | Grad Norm 6.9014(3.3522) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0077 | Time 10.5527, Epoch Time 292.4113(275.2399), Bit/dim 1.5237, Xent 0.1233, Loss 1.5853, Error 0.2316\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0539 | Time 37.8411(38.6379) | Bit/dim 1.5320(1.3634) | Xent 0.1149(0.1596) | Loss 1.5895(1.4432) | Error 0.0356(0.0495) Steps 548(559.09) | Grad Norm 3.2980(3.3506) | Total Time 10.00(10.00)\n",
      "Iter 0540 | Time 37.7162(38.6103) | Bit/dim 1.4550(1.3661) | Xent 0.1457(0.1592) | Loss 1.5279(1.4457) | Error 0.0446(0.0494) Steps 554(558.94) | Grad Norm 2.2569(3.3178) | Total Time 10.00(10.00)\n",
      "Iter 0541 | Time 37.7264(38.5837) | Bit/dim 1.4626(1.3690) | Xent 0.1910(0.1602) | Loss 1.5581(1.4491) | Error 0.0580(0.0496) Steps 560(558.97) | Grad Norm 4.0782(3.3406) | Total Time 10.00(10.00)\n",
      "Iter 0542 | Time 42.1753(38.6915) | Bit/dim 1.4803(1.3724) | Xent 0.1752(0.1606) | Loss 1.5679(1.4527) | Error 0.0581(0.0499) Steps 584(559.72) | Grad Norm 4.8517(3.3859) | Total Time 10.00(10.00)\n",
      "Iter 0543 | Time 39.1866(38.7063) | Bit/dim 1.4428(1.3745) | Xent 0.1571(0.1605) | Loss 1.5213(1.4547) | Error 0.0474(0.0498) Steps 560(559.73) | Grad Norm 2.4763(3.3587) | Total Time 10.00(10.00)\n",
      "Iter 0544 | Time 37.2177(38.6617) | Bit/dim 1.4465(1.3766) | Xent 0.1329(0.1597) | Loss 1.5130(1.4565) | Error 0.0420(0.0496) Steps 548(559.38) | Grad Norm 1.9966(3.3178) | Total Time 10.00(10.00)\n",
      "Iter 0545 | Time 36.8524(38.6074) | Bit/dim 1.4511(1.3789) | Xent 0.1303(0.1588) | Loss 1.5163(1.4583) | Error 0.0401(0.0493) Steps 542(558.85) | Grad Norm 2.6851(3.2988) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0078 | Time 10.5145, Epoch Time 291.9437(275.7410), Bit/dim 1.4125, Xent 0.1282, Loss 1.4766, Error 0.2321\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0546 | Time 37.1019(38.5622) | Bit/dim 1.4298(1.3804) | Xent 0.1473(0.1585) | Loss 1.5034(1.4596) | Error 0.0415(0.0491) Steps 542(558.35) | Grad Norm 2.0534(3.2615) | Total Time 10.00(10.00)\n",
      "Iter 0547 | Time 37.8666(38.5414) | Bit/dim 1.4099(1.3813) | Xent 0.1438(0.1580) | Loss 1.4818(1.4603) | Error 0.0469(0.0490) Steps 536(557.68) | Grad Norm 1.6194(3.2122) | Total Time 10.00(10.00)\n",
      "Iter 0548 | Time 36.5878(38.4828) | Bit/dim 1.3877(1.3815) | Xent 0.1497(0.1578) | Loss 1.4625(1.4604) | Error 0.0464(0.0489) Steps 542(557.21) | Grad Norm 1.7492(3.1683) | Total Time 10.00(10.00)\n",
      "Iter 0549 | Time 36.7185(38.4298) | Bit/dim 1.3676(1.3811) | Xent 0.1456(0.1574) | Loss 1.4404(1.4598) | Error 0.0423(0.0487) Steps 542(556.75) | Grad Norm 1.4653(3.1172) | Total Time 10.00(10.00)\n",
      "Iter 0550 | Time 36.7297(38.3788) | Bit/dim 1.3602(1.3804) | Xent 0.1339(0.1567) | Loss 1.4271(1.4588) | Error 0.0411(0.0485) Steps 542(556.31) | Grad Norm 1.1390(3.0579) | Total Time 10.00(10.00)\n",
      "Iter 0551 | Time 37.4098(38.3498) | Bit/dim 1.3525(1.3796) | Xent 0.1218(0.1557) | Loss 1.4134(1.4574) | Error 0.0385(0.0482) Steps 548(556.06) | Grad Norm 1.1803(3.0015) | Total Time 10.00(10.00)\n",
      "Iter 0552 | Time 37.0949(38.3121) | Bit/dim 1.3590(1.3790) | Xent 0.1269(0.1548) | Loss 1.4225(1.4564) | Error 0.0413(0.0480) Steps 548(555.82) | Grad Norm 1.4301(2.9544) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0079 | Time 10.8846, Epoch Time 282.9874(275.9584), Bit/dim 1.3393, Xent 0.1217, Loss 1.4002, Error 0.2315\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0553 | Time 39.9277(38.3606) | Bit/dim 1.3541(1.3782) | Xent 0.1120(0.1535) | Loss 1.4101(1.4550) | Error 0.0351(0.0476) Steps 560(555.94) | Grad Norm 1.2723(2.9039) | Total Time 10.00(10.00)\n",
      "Iter 0554 | Time 39.5183(38.3953) | Bit/dim 1.3441(1.3772) | Xent 0.1104(0.1522) | Loss 1.3993(1.4533) | Error 0.0349(0.0472) Steps 566(556.25) | Grad Norm 1.3434(2.8571) | Total Time 10.00(10.00)\n",
      "Iter 0555 | Time 37.2408(38.3607) | Bit/dim 1.3436(1.3762) | Xent 0.1045(0.1508) | Loss 1.3959(1.4516) | Error 0.0337(0.0468) Steps 548(556.00) | Grad Norm 1.2361(2.8085) | Total Time 10.00(10.00)\n",
      "Iter 0556 | Time 37.9043(38.3470) | Bit/dim 1.3353(1.3750) | Xent 0.1258(0.1500) | Loss 1.3982(1.4500) | Error 0.0393(0.0466) Steps 554(555.94) | Grad Norm 1.1660(2.7592) | Total Time 10.00(10.00)\n",
      "Iter 0557 | Time 39.6159(38.3851) | Bit/dim 1.3173(1.3732) | Xent 0.1406(0.1497) | Loss 1.3875(1.4481) | Error 0.0455(0.0466) Steps 560(556.06) | Grad Norm 1.2261(2.7132) | Total Time 10.00(10.00)\n",
      "Iter 0558 | Time 39.1264(38.4073) | Bit/dim 1.3098(1.3713) | Xent 0.1392(0.1494) | Loss 1.3794(1.4461) | Error 0.0429(0.0464) Steps 560(556.18) | Grad Norm 0.8561(2.6575) | Total Time 10.00(10.00)\n",
      "Iter 0559 | Time 38.7302(38.4170) | Bit/dim 1.3101(1.3695) | Xent 0.1316(0.1489) | Loss 1.3759(1.4440) | Error 0.0415(0.0463) Steps 566(556.47) | Grad Norm 0.8276(2.6026) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0080 | Time 11.2572, Epoch Time 295.9319(276.5576), Bit/dim 1.2925, Xent 0.1387, Loss 1.3619, Error 0.2349\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0560 | Time 39.3766(38.4458) | Bit/dim 1.3051(1.3676) | Xent 0.1185(0.1480) | Loss 1.3643(1.4416) | Error 0.0380(0.0460) Steps 566(556.76) | Grad Norm 0.9223(2.5522) | Total Time 10.00(10.00)\n",
      "Iter 0561 | Time 39.2373(38.4695) | Bit/dim 1.3026(1.3656) | Xent 0.1170(0.1471) | Loss 1.3611(1.4392) | Error 0.0377(0.0458) Steps 566(557.04) | Grad Norm 0.8624(2.5015) | Total Time 10.00(10.00)\n",
      "Iter 0562 | Time 38.5293(38.4713) | Bit/dim 1.2977(1.3636) | Xent 0.1164(0.1461) | Loss 1.3559(1.4367) | Error 0.0374(0.0455) Steps 566(557.30) | Grad Norm 0.7647(2.4494) | Total Time 10.00(10.00)\n",
      "Iter 0563 | Time 39.9655(38.5161) | Bit/dim 1.2894(1.3614) | Xent 0.1332(0.1457) | Loss 1.3560(1.4342) | Error 0.0423(0.0454) Steps 566(557.57) | Grad Norm 0.7860(2.3995) | Total Time 10.00(10.00)\n",
      "Iter 0564 | Time 38.6010(38.5187) | Bit/dim 1.2877(1.3592) | Xent 0.1292(0.1453) | Loss 1.3523(1.4318) | Error 0.0396(0.0453) Steps 554(557.46) | Grad Norm 0.6560(2.3472) | Total Time 10.00(10.00)\n",
      "Iter 0565 | Time 38.6132(38.5215) | Bit/dim 1.2788(1.3567) | Xent 0.1336(0.1449) | Loss 1.3457(1.4292) | Error 0.0425(0.0452) Steps 554(557.35) | Grad Norm 0.5889(2.2944) | Total Time 10.00(10.00)\n",
      "Iter 0566 | Time 38.5004(38.5209) | Bit/dim 1.2783(1.3544) | Xent 0.1287(0.1444) | Loss 1.3426(1.4266) | Error 0.0421(0.0451) Steps 566(557.61) | Grad Norm 0.7384(2.2478) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0081 | Time 11.5196, Epoch Time 297.4437(277.1842), Bit/dim 1.2611, Xent 0.1402, Loss 1.3311, Error 0.2364\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0567 | Time 38.7481(38.5277) | Bit/dim 1.2696(1.3518) | Xent 0.1276(0.1439) | Loss 1.3334(1.4238) | Error 0.0403(0.0449) Steps 566(557.87) | Grad Norm 0.7082(2.2016) | Total Time 10.00(10.00)\n",
      "Iter 0568 | Time 38.9607(38.5407) | Bit/dim 1.2709(1.3494) | Xent 0.1220(0.1433) | Loss 1.3319(1.4210) | Error 0.0386(0.0448) Steps 566(558.11) | Grad Norm 0.5484(2.1520) | Total Time 10.00(10.00)\n",
      "Iter 0569 | Time 39.9827(38.5840) | Bit/dim 1.2654(1.3469) | Xent 0.1321(0.1429) | Loss 1.3314(1.4184) | Error 0.0437(0.0447) Steps 572(558.53) | Grad Norm 0.6280(2.1063) | Total Time 10.00(10.00)\n",
      "Iter 0570 | Time 39.2373(38.6036) | Bit/dim 1.2664(1.3445) | Xent 0.1314(0.1426) | Loss 1.3321(1.4158) | Error 0.0425(0.0447) Steps 572(558.93) | Grad Norm 0.6878(2.0637) | Total Time 10.00(10.00)\n",
      "Iter 0571 | Time 40.9680(38.6745) | Bit/dim 1.2726(1.3423) | Xent 0.1228(0.1420) | Loss 1.3340(1.4133) | Error 0.0381(0.0445) Steps 572(559.32) | Grad Norm 0.8220(2.0264) | Total Time 10.00(10.00)\n",
      "Iter 0572 | Time 39.8314(38.7092) | Bit/dim 1.2649(1.3400) | Xent 0.1251(0.1415) | Loss 1.3275(1.4107) | Error 0.0405(0.0443) Steps 572(559.70) | Grad Norm 1.1614(2.0005) | Total Time 10.00(10.00)\n",
      "Iter 0573 | Time 39.8045(38.7421) | Bit/dim 1.2648(1.3377) | Xent 0.1153(0.1407) | Loss 1.3224(1.4081) | Error 0.0387(0.0442) Steps 572(560.07) | Grad Norm 1.4940(1.9853) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0082 | Time 11.6577, Epoch Time 301.8358(277.9237), Bit/dim 1.2453, Xent 0.1396, Loss 1.3151, Error 0.2344\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0574 | Time 39.6512(38.7693) | Bit/dim 1.2589(1.3354) | Xent 0.1359(0.1406) | Loss 1.3269(1.4057) | Error 0.0436(0.0442) Steps 572(560.43) | Grad Norm 1.9532(1.9843) | Total Time 10.00(10.00)\n",
      "Iter 0575 | Time 39.9735(38.8055) | Bit/dim 1.2700(1.3334) | Xent 0.1237(0.1400) | Loss 1.3318(1.4034) | Error 0.0376(0.0440) Steps 566(560.60) | Grad Norm 3.3545(2.0254) | Total Time 10.00(10.00)\n",
      "Iter 0576 | Time 39.7273(38.8331) | Bit/dim 1.2684(1.3315) | Xent 0.1478(0.1403) | Loss 1.3423(1.4016) | Error 0.0479(0.0441) Steps 572(560.94) | Grad Norm 6.8994(2.1717) | Total Time 10.00(10.00)\n",
      "Iter 0577 | Time 38.3662(38.8191) | Bit/dim 1.3927(1.3333) | Xent 0.1133(0.1395) | Loss 1.4494(1.4030) | Error 0.0349(0.0438) Steps 560(560.91) | Grad Norm 8.1446(2.3508) | Total Time 10.00(10.00)\n",
      "Iter 0578 | Time 38.9796(38.8239) | Bit/dim 1.2605(1.3311) | Xent 0.1208(0.1389) | Loss 1.3209(1.4006) | Error 0.0384(0.0436) Steps 566(561.06) | Grad Norm 1.2054(2.3165) | Total Time 10.00(10.00)\n",
      "Iter 0579 | Time 40.2605(38.8670) | Bit/dim 1.4278(1.3340) | Xent 0.1855(0.1403) | Loss 1.5205(1.4042) | Error 0.0587(0.0441) Steps 572(561.39) | Grad Norm 15.6145(2.7154) | Total Time 10.00(10.00)\n",
      "Iter 0580 | Time 38.9697(38.8701) | Bit/dim 1.6006(1.3420) | Xent 0.1294(0.1400) | Loss 1.6653(1.4120) | Error 0.0381(0.0439) Steps 548(560.99) | Grad Norm 8.2906(2.8827) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0083 | Time 10.4181, Epoch Time 299.1957(278.5619), Bit/dim 1.6652, Xent 0.1311, Loss 1.7308, Error 0.2310\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0581 | Time 38.6159(38.8625) | Bit/dim 1.6777(1.3521) | Xent 0.1158(0.1393) | Loss 1.7356(1.4217) | Error 0.0354(0.0437) Steps 554(560.78) | Grad Norm 6.8689(3.0023) | Total Time 10.00(10.00)\n",
      "Iter 0582 | Time 38.9122(38.8640) | Bit/dim 1.5498(1.3580) | Xent 0.1224(0.1387) | Loss 1.6110(1.4274) | Error 0.0370(0.0435) Steps 560(560.76) | Grad Norm 4.0334(3.0332) | Total Time 10.00(10.00)\n",
      "Iter 0583 | Time 39.9901(38.8977) | Bit/dim 1.4660(1.3613) | Xent 0.1354(0.1386) | Loss 1.5336(1.4306) | Error 0.0401(0.0434) Steps 572(561.09) | Grad Norm 2.3409(3.0124) | Total Time 10.00(10.00)\n",
      "Iter 0584 | Time 39.0881(38.9035) | Bit/dim 1.4362(1.3635) | Xent 0.1575(0.1392) | Loss 1.5149(1.4331) | Error 0.0481(0.0435) Steps 566(561.24) | Grad Norm 3.1888(3.0177) | Total Time 10.00(10.00)\n",
      "Iter 0585 | Time 42.4846(39.0109) | Bit/dim 1.4501(1.3661) | Xent 0.1935(0.1408) | Loss 1.5469(1.4365) | Error 0.0586(0.0440) Steps 584(561.92) | Grad Norm 4.4590(3.0610) | Total Time 10.00(10.00)\n",
      "Iter 0586 | Time 42.8851(39.1271) | Bit/dim 1.4608(1.3689) | Xent 0.1792(0.1420) | Loss 1.5504(1.4399) | Error 0.0557(0.0443) Steps 578(562.41) | Grad Norm 3.5936(3.0769) | Total Time 10.00(10.00)\n",
      "Iter 0587 | Time 39.7337(39.1453) | Bit/dim 1.4464(1.3713) | Xent 0.1461(0.1421) | Loss 1.5195(1.4423) | Error 0.0439(0.0443) Steps 566(562.51) | Grad Norm 2.0443(3.0460) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0084 | Time 11.1740, Epoch Time 305.5068(279.3702), Bit/dim 1.4289, Xent 0.1348, Loss 1.4963, Error 0.2342\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0588 | Time 42.0453(39.2323) | Bit/dim 1.4396(1.3733) | Xent 0.1206(0.1415) | Loss 1.4999(1.4441) | Error 0.0373(0.0441) Steps 578(562.98) | Grad Norm 2.2193(3.0212) | Total Time 10.00(10.00)\n",
      "Iter 0589 | Time 38.0573(39.1971) | Bit/dim 1.4212(1.3748) | Xent 0.1258(0.1410) | Loss 1.4841(1.4453) | Error 0.0390(0.0439) Steps 554(562.71) | Grad Norm 2.0489(2.9920) | Total Time 10.00(10.00)\n",
      "Iter 0590 | Time 37.3998(39.1431) | Bit/dim 1.4048(1.3757) | Xent 0.1208(0.1404) | Loss 1.4652(1.4459) | Error 0.0363(0.0437) Steps 560(562.63) | Grad Norm 1.5872(2.9498) | Total Time 10.00(10.00)\n",
      "Iter 0591 | Time 37.1558(39.0835) | Bit/dim 1.4006(1.3764) | Xent 0.1350(0.1402) | Loss 1.4681(1.4465) | Error 0.0425(0.0437) Steps 548(562.19) | Grad Norm 1.9263(2.9191) | Total Time 10.00(10.00)\n",
      "Iter 0592 | Time 36.6763(39.0113) | Bit/dim 1.3806(1.3765) | Xent 0.1411(0.1403) | Loss 1.4511(1.4467) | Error 0.0466(0.0438) Steps 548(561.76) | Grad Norm 1.8037(2.8857) | Total Time 10.00(10.00)\n",
      "Iter 0593 | Time 36.8979(38.9479) | Bit/dim 1.3533(1.3758) | Xent 0.1295(0.1399) | Loss 1.4181(1.4458) | Error 0.0413(0.0437) Steps 548(561.35) | Grad Norm 1.1669(2.8341) | Total Time 10.00(10.00)\n",
      "Iter 0594 | Time 36.9720(38.8886) | Bit/dim 1.3517(1.3751) | Xent 0.1321(0.1397) | Loss 1.4178(1.4450) | Error 0.0413(0.0436) Steps 542(560.77) | Grad Norm 1.5022(2.7942) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0085 | Time 10.8459, Epoch Time 288.5924(279.6469), Bit/dim 1.3341, Xent 0.1342, Loss 1.4012, Error 0.2335\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0595 | Time 38.6670(38.8820) | Bit/dim 1.3483(1.3743) | Xent 0.1170(0.1390) | Loss 1.4067(1.4438) | Error 0.0343(0.0433) Steps 542(560.21) | Grad Norm 1.4962(2.7552) | Total Time 10.00(10.00)\n",
      "Iter 0596 | Time 38.7401(38.8777) | Bit/dim 1.3457(1.3734) | Xent 0.1022(0.1379) | Loss 1.3968(1.4424) | Error 0.0316(0.0430) Steps 554(560.02) | Grad Norm 1.3673(2.7136) | Total Time 10.00(10.00)\n",
      "Iter 0597 | Time 39.8269(38.9062) | Bit/dim 1.3372(1.3724) | Xent 0.1096(0.1371) | Loss 1.3921(1.4409) | Error 0.0363(0.0428) Steps 572(560.38) | Grad Norm 1.4943(2.6770) | Total Time 10.00(10.00)\n",
      "Iter 0598 | Time 38.1295(38.8829) | Bit/dim 1.3306(1.3711) | Xent 0.1052(0.1361) | Loss 1.3832(1.4392) | Error 0.0349(0.0425) Steps 560(560.37) | Grad Norm 1.1250(2.6304) | Total Time 10.00(10.00)\n",
      "Iter 0599 | Time 38.8616(38.8823) | Bit/dim 1.3215(1.3696) | Xent 0.1144(0.1355) | Loss 1.3787(1.4373) | Error 0.0359(0.0423) Steps 560(560.36) | Grad Norm 1.6410(2.6008) | Total Time 10.00(10.00)\n",
      "Iter 0600 | Time 38.4296(38.8687) | Bit/dim 1.3197(1.3681) | Xent 0.1189(0.1350) | Loss 1.3792(1.4356) | Error 0.0387(0.0422) Steps 560(560.35) | Grad Norm 1.1629(2.5576) | Total Time 10.00(10.00)\n",
      "Iter 0601 | Time 41.0751(38.9349) | Bit/dim 1.3109(1.3664) | Xent 0.1244(0.1346) | Loss 1.3731(1.4337) | Error 0.0403(0.0422) Steps 560(560.34) | Grad Norm 1.3697(2.5220) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0086 | Time 11.1713, Epoch Time 297.9733(280.1967), Bit/dim 1.2840, Xent 0.1296, Loss 1.3488, Error 0.2336\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0602 | Time 38.0070(38.9070) | Bit/dim 1.2970(1.3643) | Xent 0.1286(0.1345) | Loss 1.3613(1.4316) | Error 0.0385(0.0421) Steps 560(560.33) | Grad Norm 1.0191(2.4769) | Total Time 10.00(10.00)\n",
      "Iter 0603 | Time 38.8335(38.9048) | Bit/dim 1.3022(1.3625) | Xent 0.1162(0.1339) | Loss 1.3604(1.4294) | Error 0.0376(0.0419) Steps 560(560.32) | Grad Norm 0.9538(2.4312) | Total Time 10.00(10.00)\n",
      "Iter 0604 | Time 40.2351(38.9447) | Bit/dim 1.2988(1.3605) | Xent 0.1211(0.1335) | Loss 1.3593(1.4273) | Error 0.0374(0.0418) Steps 566(560.49) | Grad Norm 1.1294(2.3921) | Total Time 10.00(10.00)\n",
      "Iter 0605 | Time 40.0925(38.9792) | Bit/dim 1.2895(1.3584) | Xent 0.1122(0.1329) | Loss 1.3455(1.4249) | Error 0.0360(0.0416) Steps 572(560.83) | Grad Norm 1.0920(2.3531) | Total Time 10.00(10.00)\n",
      "Iter 0606 | Time 38.8530(38.9754) | Bit/dim 1.2813(1.3561) | Xent 0.1086(0.1322) | Loss 1.3356(1.4222) | Error 0.0365(0.0415) Steps 566(560.99) | Grad Norm 1.2524(2.3201) | Total Time 10.00(10.00)\n",
      "Iter 0607 | Time 40.6936(39.0269) | Bit/dim 1.2764(1.3537) | Xent 0.1181(0.1317) | Loss 1.3355(1.4196) | Error 0.0386(0.0414) Steps 566(561.14) | Grad Norm 0.8505(2.2760) | Total Time 10.00(10.00)\n",
      "Iter 0608 | Time 39.1303(39.0300) | Bit/dim 1.2755(1.3514) | Xent 0.1124(0.1312) | Loss 1.3317(1.4169) | Error 0.0359(0.0412) Steps 566(561.28) | Grad Norm 1.1045(2.2409) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0087 | Time 10.8375, Epoch Time 299.1704(280.7659), Bit/dim 1.2536, Xent 0.1265, Loss 1.3168, Error 0.2321\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0609 | Time 40.1749(39.0644) | Bit/dim 1.2669(1.3488) | Xent 0.1061(0.1304) | Loss 1.3199(1.4140) | Error 0.0343(0.0410) Steps 572(561.61) | Grad Norm 0.6346(2.1927) | Total Time 10.00(10.00)\n",
      "Iter 0610 | Time 42.1951(39.1583) | Bit/dim 1.2642(1.3463) | Xent 0.1245(0.1302) | Loss 1.3264(1.4114) | Error 0.0383(0.0409) Steps 572(561.92) | Grad Norm 0.8079(2.1512) | Total Time 10.00(10.00)\n",
      "Iter 0611 | Time 41.3959(39.2254) | Bit/dim 1.2641(1.3438) | Xent 0.1159(0.1298) | Loss 1.3220(1.4087) | Error 0.0380(0.0408) Steps 566(562.04) | Grad Norm 1.0205(2.1172) | Total Time 10.00(10.00)\n",
      "Iter 0612 | Time 38.4616(39.2025) | Bit/dim 1.2625(1.3414) | Xent 0.1166(0.1294) | Loss 1.3208(1.4061) | Error 0.0361(0.0407) Steps 572(562.34) | Grad Norm 0.6405(2.0729) | Total Time 10.00(10.00)\n",
      "Iter 0613 | Time 39.4637(39.2103) | Bit/dim 1.2600(1.3389) | Xent 0.1085(0.1288) | Loss 1.3143(1.4033) | Error 0.0345(0.0405) Steps 584(562.99) | Grad Norm 1.1715(2.0459) | Total Time 10.00(10.00)\n",
      "Iter 0614 | Time 39.6418(39.2233) | Bit/dim 1.2573(1.3365) | Xent 0.1208(0.1285) | Loss 1.3177(1.4008) | Error 0.0391(0.0405) Steps 572(563.26) | Grad Norm 1.1585(2.0193) | Total Time 10.00(10.00)\n",
      "Iter 0615 | Time 40.4161(39.2591) | Bit/dim 1.2524(1.3340) | Xent 0.1105(0.1280) | Loss 1.3076(1.3980) | Error 0.0377(0.0404) Steps 584(563.88) | Grad Norm 0.6536(1.9783) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0088 | Time 11.4839, Epoch Time 305.5547(281.5096), Bit/dim 1.2391, Xent 0.1220, Loss 1.3001, Error 0.2316\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0616 | Time 40.7598(39.3041) | Bit/dim 1.2518(1.3315) | Xent 0.1108(0.1275) | Loss 1.3072(1.3952) | Error 0.0335(0.0402) Steps 584(564.48) | Grad Norm 1.5612(1.9658) | Total Time 10.00(10.00)\n",
      "Iter 0617 | Time 38.7816(39.2884) | Bit/dim 1.2565(1.3293) | Xent 0.1179(0.1272) | Loss 1.3154(1.3929) | Error 0.0371(0.0401) Steps 566(564.53) | Grad Norm 1.8355(1.9619) | Total Time 10.00(10.00)\n",
      "Iter 0618 | Time 40.8302(39.3347) | Bit/dim 1.2507(1.3269) | Xent 0.1174(0.1269) | Loss 1.3094(1.3903) | Error 0.0353(0.0399) Steps 584(565.11) | Grad Norm 1.5700(1.9501) | Total Time 10.00(10.00)\n",
      "Iter 0619 | Time 39.4819(39.3391) | Bit/dim 1.2481(1.3245) | Xent 0.1050(0.1262) | Loss 1.3006(1.3877) | Error 0.0357(0.0398) Steps 584(565.68) | Grad Norm 1.2679(1.9297) | Total Time 10.00(10.00)\n",
      "Iter 0620 | Time 40.5324(39.3749) | Bit/dim 1.2410(1.3220) | Xent 0.1207(0.1261) | Loss 1.3013(1.3851) | Error 0.0396(0.0398) Steps 584(566.23) | Grad Norm 1.2320(1.9087) | Total Time 10.00(10.00)\n",
      "Iter 0621 | Time 41.4258(39.4364) | Bit/dim 1.2408(1.3196) | Xent 0.1167(0.1258) | Loss 1.2992(1.3825) | Error 0.0384(0.0398) Steps 572(566.40) | Grad Norm 1.3766(1.8928) | Total Time 10.00(10.00)\n",
      "Iter 0622 | Time 40.9710(39.4825) | Bit/dim 1.2424(1.3173) | Xent 0.1178(0.1256) | Loss 1.3013(1.3801) | Error 0.0370(0.0397) Steps 584(566.93) | Grad Norm 2.2903(1.9047) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0089 | Time 11.0723, Epoch Time 306.5862(282.2619), Bit/dim 1.2388, Xent 0.1128, Loss 1.2953, Error 0.2304\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0623 | Time 39.0702(39.4701) | Bit/dim 1.2552(1.3154) | Xent 0.1002(0.1248) | Loss 1.3053(1.3778) | Error 0.0336(0.0395) Steps 566(566.90) | Grad Norm 3.4550(1.9512) | Total Time 10.00(10.00)\n",
      "Iter 0624 | Time 42.2274(39.5528) | Bit/dim 1.2569(1.3137) | Xent 0.1301(0.1250) | Loss 1.3220(1.3761) | Error 0.0389(0.0395) Steps 584(567.42) | Grad Norm 7.2637(2.1106) | Total Time 10.00(10.00)\n",
      "Iter 0625 | Time 38.7777(39.5296) | Bit/dim 1.3741(1.3155) | Xent 0.0992(0.1242) | Loss 1.4237(1.3776) | Error 0.0320(0.0393) Steps 554(567.01) | Grad Norm 8.2814(2.2957) | Total Time 10.00(10.00)\n",
      "Iter 0626 | Time 38.4330(39.4967) | Bit/dim 1.2524(1.3136) | Xent 0.1121(0.1238) | Loss 1.3084(1.3755) | Error 0.0349(0.0391) Steps 566(566.98) | Grad Norm 3.0343(2.3179) | Total Time 10.00(10.00)\n",
      "Iter 0627 | Time 41.9613(39.5706) | Bit/dim 1.5121(1.3195) | Xent 0.2259(0.1269) | Loss 1.6251(1.3830) | Error 0.0716(0.0401) Steps 566(566.95) | Grad Norm 17.4807(2.7727) | Total Time 10.00(10.00)\n",
      "Iter 0628 | Time 39.9064(39.5807) | Bit/dim 1.5303(1.3259) | Xent 0.1412(0.1273) | Loss 1.6009(1.3895) | Error 0.0447(0.0402) Steps 566(566.93) | Grad Norm 7.6435(2.9189) | Total Time 10.00(10.00)\n",
      "Iter 0629 | Time 39.3581(39.5740) | Bit/dim 1.6443(1.3354) | Xent 0.1032(0.1266) | Loss 1.6959(1.3987) | Error 0.0316(0.0400) Steps 566(566.90) | Grad Norm 6.1601(3.0161) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0090 | Time 10.6471, Epoch Time 302.8946(282.8809), Bit/dim 1.5593, Xent 0.1114, Loss 1.6150, Error 0.2280\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0630 | Time 39.5050(39.5719) | Bit/dim 1.5721(1.3425) | Xent 0.1095(0.1261) | Loss 1.6269(1.4056) | Error 0.0312(0.0397) Steps 572(567.05) | Grad Norm 4.1351(3.0497) | Total Time 10.00(10.00)\n",
      "Iter 0631 | Time 40.5681(39.6018) | Bit/dim 1.4773(1.3466) | Xent 0.1174(0.1258) | Loss 1.5360(1.4095) | Error 0.0336(0.0395) Steps 584(567.56) | Grad Norm 2.2626(3.0261) | Total Time 10.00(10.00)\n",
      "Iter 0632 | Time 39.4277(39.5966) | Bit/dim 1.4333(1.3492) | Xent 0.1443(0.1264) | Loss 1.5054(1.4123) | Error 0.0456(0.0397) Steps 572(567.69) | Grad Norm 2.6471(3.0147) | Total Time 10.00(10.00)\n",
      "Iter 0633 | Time 40.0407(39.6099) | Bit/dim 1.4221(1.3514) | Xent 0.1616(0.1274) | Loss 1.5029(1.4151) | Error 0.0479(0.0400) Steps 566(567.64) | Grad Norm 3.1150(3.0177) | Total Time 10.00(10.00)\n",
      "Iter 0634 | Time 40.3816(39.6331) | Bit/dim 1.4344(1.3538) | Xent 0.1817(0.1290) | Loss 1.5253(1.4184) | Error 0.0584(0.0405) Steps 566(567.59) | Grad Norm 3.4861(3.0317) | Total Time 10.00(10.00)\n",
      "Iter 0635 | Time 40.1276(39.6479) | Bit/dim 1.4356(1.3563) | Xent 0.1465(0.1296) | Loss 1.5089(1.4211) | Error 0.0475(0.0407) Steps 566(567.54) | Grad Norm 2.6295(3.0197) | Total Time 10.00(10.00)\n",
      "Iter 0636 | Time 38.9344(39.6265) | Bit/dim 1.4162(1.3581) | Xent 0.1069(0.1289) | Loss 1.4697(1.4225) | Error 0.0351(0.0406) Steps 566(567.50) | Grad Norm 1.8457(2.9845) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0091 | Time 11.0622, Epoch Time 302.7036(283.4755), Bit/dim 1.3968, Xent 0.1176, Loss 1.4556, Error 0.2300\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0637 | Time 37.5551(39.5643) | Bit/dim 1.4090(1.3596) | Xent 0.0957(0.1279) | Loss 1.4569(1.4236) | Error 0.0312(0.0403) Steps 554(567.09) | Grad Norm 1.9404(2.9531) | Total Time 10.00(10.00)\n",
      "Iter 0638 | Time 39.8356(39.5725) | Bit/dim 1.3969(1.3607) | Xent 0.1104(0.1274) | Loss 1.4521(1.4244) | Error 0.0333(0.0401) Steps 566(567.06) | Grad Norm 1.9501(2.9230) | Total Time 10.00(10.00)\n",
      "Iter 0639 | Time 39.5690(39.5724) | Bit/dim 1.3781(1.3613) | Xent 0.1133(0.1270) | Loss 1.4348(1.4247) | Error 0.0324(0.0398) Steps 572(567.21) | Grad Norm 1.6234(2.8841) | Total Time 10.00(10.00)\n",
      "Iter 0640 | Time 37.3260(39.5050) | Bit/dim 1.3633(1.3613) | Xent 0.1153(0.1266) | Loss 1.4210(1.4246) | Error 0.0384(0.0398) Steps 560(566.99) | Grad Norm 1.5015(2.8426) | Total Time 10.00(10.00)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0644 | Time 40.4573(39.3574) | Bit/dim 1.3399(1.3592) | Xent 0.1009(0.1250) | Loss 1.3903(1.4218) | Error 0.0331(0.0394) Steps 566(565.69) | Grad Norm 1.2443(2.6655) | Total Time 10.00(10.00)\n",
      "Iter 0645 | Time 38.8592(39.3424) | Bit/dim 1.3308(1.3584) | Xent 0.1027(0.1243) | Loss 1.3821(1.4206) | Error 0.0333(0.0392) Steps 572(565.88) | Grad Norm 1.6771(2.6359) | Total Time 10.00(10.00)\n",
      "Iter 0646 | Time 39.4307(39.3451) | Bit/dim 1.3109(1.3570) | Xent 0.1011(0.1236) | Loss 1.3615(1.4188) | Error 0.0319(0.0390) Steps 572(566.07) | Grad Norm 1.0260(2.5876) | Total Time 10.00(10.00)\n",
      "Iter 0647 | Time 38.6513(39.3243) | Bit/dim 1.3094(1.3555) | Xent 0.0987(0.1229) | Loss 1.3588(1.4170) | Error 0.0317(0.0388) Steps 566(566.06) | Grad Norm 1.8656(2.5659) | Total Time 10.00(10.00)\n",
      "Iter 0648 | Time 38.9114(39.3119) | Bit/dim 1.3097(1.3542) | Xent 0.1166(0.1227) | Loss 1.3680(1.4155) | Error 0.0360(0.0387) Steps 566(566.06) | Grad Norm 1.3762(2.5302) | Total Time 10.00(10.00)\n",
      "Iter 0649 | Time 41.6874(39.3832) | Bit/dim 1.2958(1.3524) | Xent 0.1103(0.1223) | Loss 1.3509(1.4136) | Error 0.0353(0.0386) Steps 590(566.78) | Grad Norm 1.4316(2.4973) | Total Time 10.00(10.00)\n",
      "Iter 0650 | Time 40.8196(39.4262) | Bit/dim 1.2843(1.3504) | Xent 0.1134(0.1221) | Loss 1.3410(1.4114) | Error 0.0356(0.0385) Steps 578(567.12) | Grad Norm 1.1246(2.4561) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0093 | Time 10.9898, Epoch Time 302.4409(284.2340), Bit/dim 1.2738, Xent 0.1155, Loss 1.3315, Error 0.2293\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0651 | Time 38.4710(39.3976) | Bit/dim 1.2869(1.3485) | Xent 0.0998(0.1214) | Loss 1.3368(1.4092) | Error 0.0325(0.0383) Steps 572(567.26) | Grad Norm 1.0882(2.4151) | Total Time 10.00(10.00)\n",
      "Iter 0652 | Time 38.7505(39.3782) | Bit/dim 1.2808(1.3464) | Xent 0.1097(0.1211) | Loss 1.3356(1.4070) | Error 0.0350(0.0382) Steps 566(567.22) | Grad Norm 1.0461(2.3740) | Total Time 10.00(10.00)\n",
      "Iter 0653 | Time 39.7190(39.3884) | Bit/dim 1.2729(1.3442) | Xent 0.1160(0.1209) | Loss 1.3309(1.4047) | Error 0.0365(0.0382) Steps 584(567.73) | Grad Norm 1.5424(2.3490) | Total Time 10.00(10.00)\n",
      "Iter 0654 | Time 40.7114(39.4281) | Bit/dim 1.2690(1.3420) | Xent 0.1181(0.1208) | Loss 1.3281(1.4024) | Error 0.0354(0.0381) Steps 578(568.04) | Grad Norm 1.1845(2.3141) | Total Time 10.00(10.00)\n",
      "Iter 1607 | Time 45.5962(42.9906) | Bit/dim 1.1486(1.1846) | Xent 0.0631(0.0473) | Loss 1.1802(1.2083) | Error 0.0206(0.0152) Steps 614(590.07) | Grad Norm 3.6303(2.5909) | Total Time 10.00(10.00)\n",
      "Iter 1608 | Time 42.1127(42.9643) | Bit/dim 1.1701(1.1842) | Xent 0.0519(0.0474) | Loss 1.1961(1.2079) | Error 0.0162(0.0152) Steps 590(590.07) | Grad Norm 3.8851(2.6297) | Total Time 10.00(10.00)\n",
      "Iter 1609 | Time 45.1861(43.0309) | Bit/dim 1.1411(1.1829) | Xent 0.0416(0.0473) | Loss 1.1619(1.2065) | Error 0.0116(0.0151) Steps 602(590.42) | Grad Norm 0.7223(2.5725) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0230 | Time 11.8651, Epoch Time 330.4976(320.3223), Bit/dim 1.1534, Xent 0.0695, Loss 1.1882, Error 0.2192\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1610 | Time 42.6947(43.0209) | Bit/dim 1.1645(1.1824) | Xent 0.0551(0.0475) | Loss 1.1921(1.2061) | Error 0.0182(0.0152) Steps 590(590.41) | Grad Norm 5.6063(2.6635) | Total Time 10.00(10.00)\n",
      "Iter 1611 | Time 40.9553(42.9589) | Bit/dim 1.2029(1.1830) | Xent 0.0457(0.0474) | Loss 1.2258(1.2067) | Error 0.0142(0.0152) Steps 578(590.04) | Grad Norm 4.9222(2.7313) | Total Time 10.00(10.00)\n",
      "Iter 1612 | Time 43.2965(42.9690) | Bit/dim 1.1723(1.1826) | Xent 0.0442(0.0473) | Loss 1.1945(1.2063) | Error 0.0146(0.0152) Steps 596(590.22) | Grad Norm 3.0735(2.7415) | Total Time 10.00(10.00)\n",
      "Iter 1613 | Time 41.9851(42.9395) | Bit/dim 1.1971(1.1831) | Xent 0.0616(0.0478) | Loss 1.2279(1.2070) | Error 0.0221(0.0154) Steps 596(590.39) | Grad Norm 6.7816(2.8627) | Total Time 10.00(10.00)\n",
      "Iter 1614 | Time 42.9491(42.9398) | Bit/dim 1.1617(1.1824) | Xent 0.0381(0.0475) | Loss 1.1808(1.2062) | Error 0.0131(0.0153) Steps 590(590.38) | Grad Norm 2.2749(2.8451) | Total Time 10.00(10.00)\n",
      "Iter 1615 | Time 39.5122(42.8370) | Bit/dim 1.1663(1.1820) | Xent 0.0444(0.0474) | Loss 1.1885(1.2057) | Error 0.0146(0.0153) Steps 572(589.83) | Grad Norm 3.0639(2.8517) | Total Time 10.00(10.00)\n",
      "Iter 1616 | Time 40.6128(42.7702) | Bit/dim 1.1577(1.1812) | Xent 0.0439(0.0473) | Loss 1.1796(1.2049) | Error 0.0148(0.0153) Steps 572(589.29) | Grad Norm 1.6321(2.8151) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0231 | Time 11.8699, Epoch Time 316.5350(320.2087), Bit/dim 1.1461, Xent 0.0686, Loss 1.1804, Error 0.2175\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1617 | Time 42.9824(42.7766) | Bit/dim 1.1588(1.1806) | Xent 0.0574(0.0476) | Loss 1.1875(1.2043) | Error 0.0180(0.0153) Steps 596(589.49) | Grad Norm 3.4073(2.8328) | Total Time 10.00(10.00)\n",
      "Iter 1618 | Time 41.5230(42.7390) | Bit/dim 1.1609(1.1800) | Xent 0.0352(0.0472) | Loss 1.1785(1.2036) | Error 0.0118(0.0152) Steps 572(588.97) | Grad Norm 2.6124(2.8262) | Total Time 10.00(10.00)\n",
      "Iter 1619 | Time 40.5773(42.6741) | Bit/dim 1.1677(1.1796) | Xent 0.0339(0.0468) | Loss 1.1846(1.2030) | Error 0.0110(0.0151) Steps 578(588.64) | Grad Norm 2.4263(2.8142) | Total Time 10.00(10.00)\n",
      "Iter 1620 | Time 42.5816(42.6714) | Bit/dim 1.1574(1.1789) | Xent 0.0461(0.0468) | Loss 1.1804(1.2023) | Error 0.0138(0.0151) Steps 590(588.68) | Grad Norm 3.1108(2.8231) | Total Time 10.00(10.00)\n",
      "Iter 1621 | Time 43.6076(42.6995) | Bit/dim 1.1399(1.1778) | Xent 0.0385(0.0465) | Loss 1.1592(1.2010) | Error 0.0122(0.0150) Steps 584(588.54) | Grad Norm 1.1575(2.7732) | Total Time 10.00(10.00)\n",
      "Iter 1622 | Time 43.7259(42.7302) | Bit/dim 1.1502(1.1769) | Xent 0.0397(0.0463) | Loss 1.1701(1.2001) | Error 0.0135(0.0149) Steps 596(588.76) | Grad Norm 2.6051(2.7681) | Total Time 10.00(10.00)\n",
      "Iter 1623 | Time 40.9689(42.6774) | Bit/dim 1.1414(1.1759) | Xent 0.0454(0.0463) | Loss 1.1640(1.1990) | Error 0.0141(0.0149) Steps 566(588.08) | Grad Norm 0.8195(2.7097) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0232 | Time 11.7617, Epoch Time 320.1903(320.2081), Bit/dim 1.1377, Xent 0.0606, Loss 1.1680, Error 0.2159\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1624 | Time 42.7725(42.6803) | Bit/dim 1.1496(1.1751) | Xent 0.0474(0.0463) | Loss 1.1732(1.1982) | Error 0.0131(0.0149) Steps 584(587.96) | Grad Norm 3.9786(2.7477) | Total Time 10.00(10.00)\n",
      "Iter 1625 | Time 43.0519(42.6914) | Bit/dim 1.1640(1.1747) | Xent 0.0238(0.0457) | Loss 1.1759(1.1976) | Error 0.0089(0.0147) Steps 590(588.02) | Grad Norm 3.5167(2.7708) | Total Time 10.00(10.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p train_cnf_conditional.py --data mnist --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 8000 --test_batch_size 8000 --save experiments/cnf_cond_bs8K_ilr_0_01_tol_1e_m5_wy_0_5_dev --conditional True --lr 0.01 --warmup_iters 113 --atol 1e-5  --rtol 1e-5 --weight_y 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
