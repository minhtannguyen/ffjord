{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='4,5,6,7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_rl.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "GATES = [\"cnn1\", \"cnn2\", \"rnn\"]\n",
      "\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--gate', type=str, default='cnn1', choices=GATES)\n",
      "parser.add_argument('--scale', type=float, default=1.0)\n",
      "parser.add_argument('--scale_fac', type=float, default=1.0)\n",
      "parser.add_argument('--scale_std', type=float, default=1.0)\n",
      "parser.add_argument('--eta', default=0.1, type=float,\n",
      "                        help='tuning parameter that allows us to trade-off the competing goals of' \n",
      "                                'minimizing the prediction loss and maximizing the gate rewards ')\n",
      "parser.add_argument('--rl-weight', default=0.01, type=float,\n",
      "                        help='rl weight')\n",
      "\n",
      "parser.add_argument('--gamma', default=0.99, type=float,\n",
      "                        help='discount factor, default: (0.99)')\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "import lib.odenvp_conditional_rl as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol, \"scale\": args.scale, \"scale_fac\": args.scale_fac, \"scale_std\": args.scale_std, \"gate\": args.gate},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            # re-weight the gate rewards\n",
      "            normalized_eta = args.eta / len(logp_actions)\n",
      "            \n",
      "            # collect cumulative future rewards\n",
      "            R = - loss\n",
      "            cum_rewards = []\n",
      "            for r in nfe[::-1]:\n",
      "                R = -normalized_eta * r.view(-1,1) + args.gamma * R\n",
      "                cum_rewards.insert(0,R)\n",
      "            \n",
      "            # apply REINFORCE\n",
      "            rl_loss = 0\n",
      "            for lpa, r in zip(logp_actions, cum_rewards):\n",
      "                rl_loss = rl_loss - lpa.view(-1,1) * args.rl_weight * r\n",
      "                \n",
      "            loss = loss + rl_loss.mean()\n",
      "            \n",
      "            loss.backward()\n",
      "            \n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "            \n",
      "            for idx in range(len(model.module.transforms)):\n",
      "                for layer in model.module.transforms[idx].chain:\n",
      "                    if hasattr(layer, 'atol'):\n",
      "                        layer.odefunc.after_odeint()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'train': atol[tol_indx].mean()}, itr)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'train': rtol[tol_indx].mean()}, itr)\n",
      "                    \n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples, atol, rtol, logp_actions, nfe = model(fixed_z, reverse=True)\n",
      "            generated_samples = generated_samples.view(-1, *data_shape)\n",
      "            for tol_indx in range(len(atol)):\n",
      "                writer.add_scalars('atol_gen_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                writer.add_scalars('rtol_gen_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, condition_ratio=0.5, conditional=True, controlled_tol=False, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, eta=0.1, gamma=0.99, gate='cnn1', imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume=None, rl_weight=0.01, rtol=1e-05, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_15_run1', scale=1.0, scale_fac=1.0, scale_std=15.0, seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1450732\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc_rl.py:291: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Iter 0010 | Time 10.6973(30.0061) | Bit/dim 8.6886(8.9520) | Xent 2.2806(2.3001) | Loss 19.5435(19.9099) | Error 0.7944(0.8599) Steps 0(0.00) | Grad Norm 20.8304(26.5399) | Total Time 0.00(0.00)\n",
      "Iter 0020 | Time 10.8047(25.0488) | Bit/dim 8.4866(8.8629) | Xent 2.2265(2.2874) | Loss 19.0976(19.7163) | Error 0.7278(0.8323) Steps 0(0.00) | Grad Norm 8.6701(22.9722) | Total Time 0.00(0.00)\n",
      "Iter 0030 | Time 11.4270(21.4680) | Bit/dim 8.3930(8.7513) | Xent 2.1741(2.2634) | Loss 19.1561(19.4918) | Error 0.7522(0.8090) Steps 0(0.00) | Grad Norm 8.6939(18.8716) | Total Time 0.00(0.00)\n",
      "Iter 0040 | Time 10.8941(18.7864) | Bit/dim 8.1911(8.6233) | Xent 2.1140(2.2347) | Loss 18.3685(19.2311) | Error 0.7267(0.7909) Steps 0(0.00) | Grad Norm 5.3244(15.4913) | Total Time 0.00(0.00)\n",
      "Iter 0050 | Time 11.1301(16.8279) | Bit/dim 7.9736(8.4737) | Xent 2.1020(2.2021) | Loss 17.7667(18.9233) | Error 0.7100(0.7753) Steps 0(0.00) | Grad Norm 5.1657(12.8684) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0001 | Time 72.6754, Epoch Time 734.1291(734.1291), Bit/dim 7.7697(best: inf), Xent 2.0788, Loss 8.8091, Error 0.7005(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0060 | Time 10.5665(15.4074) | Bit/dim 7.6771(8.2913) | Xent 2.0790(2.1723) | Loss 17.1006(18.9730) | Error 0.7044(0.7590) Steps 0(0.00) | Grad Norm 5.3616(10.8622) | Total Time 0.00(0.00)\n",
      "Iter 0070 | Time 11.6469(14.4185) | Bit/dim 7.3715(8.0805) | Xent 2.0570(2.1465) | Loss 16.8507(18.4528) | Error 0.6767(0.7426) Steps 0(0.00) | Grad Norm 3.8826(9.1807) | Total Time 0.00(0.00)\n",
      "Iter 0080 | Time 12.7227(13.7235) | Bit/dim 7.1865(7.8636) | Xent 2.0719(2.1262) | Loss 16.4913(17.9304) | Error 0.6689(0.7270) Steps 0(0.00) | Grad Norm 3.1832(7.6736) | Total Time 0.00(0.00)\n",
      "Iter 0090 | Time 12.5761(13.2616) | Bit/dim 7.0822(7.6689) | Xent 2.0852(2.1133) | Loss 16.3351(17.4757) | Error 0.7222(0.7188) Steps 0(0.00) | Grad Norm 1.9975(6.3176) | Total Time 0.00(0.00)\n",
      "Iter 0100 | Time 12.9507(12.9819) | Bit/dim 7.0015(7.5061) | Xent 2.0644(2.1038) | Loss 16.1791(17.1208) | Error 0.6967(0.7127) Steps 0(0.00) | Grad Norm 3.2096(5.4140) | Total Time 0.00(0.00)\n",
      "Iter 0110 | Time 12.8197(12.7654) | Bit/dim 7.0036(7.3753) | Xent 2.0604(2.0951) | Loss 16.2592(16.8471) | Error 0.7311(0.7123) Steps 0(0.00) | Grad Norm 2.1871(4.5700) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0002 | Time 69.9037, Epoch Time 741.5997(734.3532), Bit/dim 6.9921(best: 7.7697), Xent 2.0586, Loss 8.0214, Error 0.6957(best: 0.7005)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0120 | Time 12.1064(12.6258) | Bit/dim 6.9410(7.2695) | Xent 2.0531(2.0855) | Loss 15.7597(17.0411) | Error 0.7078(0.7103) Steps 0(0.00) | Grad Norm 1.6506(4.0015) | Total Time 0.00(0.00)\n",
      "Iter 0130 | Time 13.5289(12.7624) | Bit/dim 6.9200(7.1828) | Xent 2.0412(2.0747) | Loss 15.9615(16.7363) | Error 0.6911(0.7055) Steps 0(0.00) | Grad Norm 2.7969(3.5606) | Total Time 0.00(0.00)\n",
      "Iter 0140 | Time 12.8718(12.8672) | Bit/dim 6.8949(7.1070) | Xent 2.0491(2.0650) | Loss 15.7938(16.4861) | Error 0.6889(0.7017) Steps 0(0.00) | Grad Norm 7.5657(3.6852) | Total Time 0.00(0.00)\n",
      "Iter 0150 | Time 13.6847(12.9393) | Bit/dim 6.7857(7.0356) | Xent 2.0148(2.0567) | Loss 15.3612(16.2502) | Error 0.7078(0.6999) Steps 0(0.00) | Grad Norm 4.1000(4.6414) | Total Time 0.00(0.00)\n",
      "Iter 0160 | Time 13.1112(12.9693) | Bit/dim 6.7048(6.9608) | Xent 2.0414(2.0520) | Loss 15.6261(16.0718) | Error 0.7222(0.6992) Steps 0(0.00) | Grad Norm 21.8740(7.1413) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0003 | Time 71.2809, Epoch Time 803.4177(736.4252), Bit/dim 6.6374(best: 6.9921), Xent 2.0400, Loss 7.6574, Error 0.7075(best: 0.6957)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0170 | Time 13.5230(13.0659) | Bit/dim 6.6057(6.8781) | Xent 2.0422(2.0530) | Loss 15.5331(16.3961) | Error 0.7011(0.7040) Steps 0(0.00) | Grad Norm 35.4927(13.8122) | Total Time 0.00(0.00)\n",
      "Iter 0180 | Time 12.4033(13.2141) | Bit/dim 6.4237(6.7765) | Xent 2.1094(2.0534) | Loss 14.9091(16.0535) | Error 0.7956(0.7130) Steps 0(0.00) | Grad Norm 68.8160(21.0815) | Total Time 0.00(0.00)\n",
      "Iter 0190 | Time 12.6363(13.2268) | Bit/dim 6.2562(6.6552) | Xent 2.2767(2.0554) | Loss 14.8681(15.7342) | Error 0.8244(0.7159) Steps 0(0.00) | Grad Norm 150.1368(27.7370) | Total Time 0.00(0.00)\n",
      "Iter 0200 | Time 14.4832(13.2630) | Bit/dim 6.2186(6.5712) | Xent 2.1675(2.1076) | Loss 14.5931(15.5381) | Error 0.8167(0.7353) Steps 0(0.00) | Grad Norm 37.6645(47.9214) | Total Time 0.00(0.00)\n",
      "Iter 0210 | Time 13.5363(13.2803) | Bit/dim 6.0744(6.4555) | Xent 2.1108(2.1024) | Loss 14.4776(15.2366) | Error 0.7644(0.7340) Steps 0(0.00) | Grad Norm 17.4450(40.1855) | Total Time 0.00(0.00)\n",
      "Iter 0220 | Time 12.5404(13.2181) | Bit/dim 5.9024(6.3219) | Xent 2.0416(2.0927) | Loss 14.1410(14.9279) | Error 0.7044(0.7302) Steps 0(0.00) | Grad Norm 19.3078(33.8291) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0004 | Time 71.6715, Epoch Time 822.1209(738.9960), Bit/dim 5.9462(best: 6.6374), Xent 2.0494, Loss 6.9709, Error 0.6892(best: 0.6957)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0230 | Time 13.5198(13.2571) | Bit/dim 5.8434(6.2044) | Xent 2.0694(2.0784) | Loss 13.7481(15.0644) | Error 0.7244(0.7232) Steps 0(0.00) | Grad Norm 27.7617(32.7478) | Total Time 0.00(0.00)\n",
      "Iter 0240 | Time 13.8614(13.2230) | Bit/dim 5.7768(6.0906) | Xent 2.0001(2.0619) | Loss 13.7945(14.6698) | Error 0.6878(0.7165) Steps 0(0.00) | Grad Norm 18.6044(30.2089) | Total Time 0.00(0.00)\n",
      "Iter 0250 | Time 12.6374(13.0948) | Bit/dim 5.7025(5.9944) | Xent 1.9695(2.0461) | Loss 13.4303(14.3772) | Error 0.6600(0.7065) Steps 0(0.00) | Grad Norm 18.4843(28.2629) | Total Time 0.00(0.00)\n",
      "Iter 0260 | Time 12.2431(13.0110) | Bit/dim 5.6609(5.9129) | Xent 1.9890(2.0453) | Loss 13.3708(14.1706) | Error 0.6711(0.7104) Steps 0(0.00) | Grad Norm 31.0070(36.3872) | Total Time 0.00(0.00)\n",
      "Iter 0270 | Time 12.5154(12.9088) | Bit/dim 5.6364(5.8473) | Xent 1.9984(2.0375) | Loss 12.9355(13.9898) | Error 0.6944(0.7083) Steps 0(0.00) | Grad Norm 44.4505(37.7340) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0005 | Time 71.9694, Epoch Time 802.2983(740.8951), Bit/dim 5.6316(best: 5.9462), Xent 1.9822, Loss 6.6227, Error 0.6567(best: 0.6892)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0280 | Time 13.0661(12.9278) | Bit/dim 5.5567(5.7842) | Xent 1.9773(2.0264) | Loss 13.3692(14.2653) | Error 0.6778(0.7042) Steps 0(0.00) | Grad Norm 26.8122(34.8353) | Total Time 0.00(0.00)\n",
      "Iter 0290 | Time 12.7860(12.8476) | Bit/dim 5.6111(5.7390) | Xent 1.9241(2.0094) | Loss 13.3960(14.0136) | Error 0.6433(0.6929) Steps 0(0.00) | Grad Norm 7.7523(29.9090) | Total Time 0.00(0.00)\n",
      "Iter 0300 | Time 12.6340(12.8042) | Bit/dim 5.5459(5.6985) | Xent 1.9113(1.9905) | Loss 13.0651(13.8198) | Error 0.6689(0.6826) Steps 0(0.00) | Grad Norm 15.9661(24.7474) | Total Time 0.00(0.00)\n",
      "Iter 0310 | Time 12.3757(12.7563) | Bit/dim 5.5182(5.6623) | Xent 1.9593(1.9899) | Loss 12.7777(13.6764) | Error 0.6733(0.6850) Steps 0(0.00) | Grad Norm 11.1501(29.6635) | Total Time 0.00(0.00)\n",
      "Iter 0320 | Time 13.1995(12.7494) | Bit/dim 5.5457(5.6374) | Xent 2.0755(2.0172) | Loss 13.2613(13.5810) | Error 0.7400(0.6995) Steps 0(0.00) | Grad Norm 17.3056(32.1774) | Total Time 0.00(0.00)\n",
      "Iter 0330 | Time 12.4650(12.7130) | Bit/dim 5.5622(5.6106) | Xent 2.0066(2.0218) | Loss 13.2844(13.4880) | Error 0.7200(0.6986) Steps 0(0.00) | Grad Norm 12.8395(27.1794) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0006 | Time 71.5186, Epoch Time 785.8797(742.2446), Bit/dim 5.5019(best: 5.6316), Xent 1.9996, Loss 6.5017, Error 0.6785(best: 0.6567)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0340 | Time 13.4990(12.7072) | Bit/dim 5.4871(5.5826) | Xent 2.0445(2.0223) | Loss 13.2572(13.8341) | Error 0.6956(0.7016) Steps 0(0.00) | Grad Norm 30.7362(29.5952) | Total Time 0.00(0.00)\n",
      "Iter 0350 | Time 12.7179(12.6846) | Bit/dim 5.4131(5.5454) | Xent 1.9553(2.0108) | Loss 12.9764(13.6298) | Error 0.6433(0.6971) Steps 0(0.00) | Grad Norm 7.8226(29.6984) | Total Time 0.00(0.00)\n",
      "Iter 0360 | Time 13.0400(12.6918) | Bit/dim 5.3733(5.5101) | Xent 1.9898(1.9992) | Loss 12.7685(13.4367) | Error 0.7022(0.6922) Steps 0(0.00) | Grad Norm 28.9904(28.2293) | Total Time 0.00(0.00)\n",
      "Iter 0370 | Time 12.7729(12.7136) | Bit/dim 5.3747(5.4748) | Xent 1.9096(1.9844) | Loss 12.7283(13.2771) | Error 0.6656(0.6856) Steps 0(0.00) | Grad Norm 10.2275(25.1937) | Total Time 0.00(0.00)\n",
      "Iter 0380 | Time 12.7054(12.8126) | Bit/dim 5.3227(5.4377) | Xent 1.9130(1.9708) | Loss 12.5059(13.1398) | Error 0.6611(0.6802) Steps 0(0.00) | Grad Norm 8.0196(21.9914) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0007 | Time 73.1888, Epoch Time 796.8814(743.8837), Bit/dim 5.2992(best: 5.5019), Xent 1.8874, Loss 6.2429, Error 0.6265(best: 0.6567)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0390 | Time 13.5763(12.9706) | Bit/dim 5.2450(5.4059) | Xent 1.8751(1.9580) | Loss 12.6570(13.5014) | Error 0.6467(0.6753) Steps 0(0.00) | Grad Norm 22.5119(21.7436) | Total Time 0.00(0.00)\n",
      "Iter 0400 | Time 13.2893(13.0627) | Bit/dim 5.3097(5.3676) | Xent 1.9045(1.9387) | Loss 12.8780(13.2640) | Error 0.6578(0.6684) Steps 0(0.00) | Grad Norm 48.6807(21.0185) | Total Time 0.00(0.00)\n",
      "Iter 0410 | Time 13.1179(13.0766) | Bit/dim 5.3333(5.3456) | Xent 1.9925(1.9554) | Loss 12.5608(13.1304) | Error 0.6956(0.6790) Steps 0(0.00) | Grad Norm 24.8985(28.1216) | Total Time 0.00(0.00)\n",
      "Iter 0420 | Time 13.8440(13.2494) | Bit/dim 5.2169(5.3136) | Xent 1.9040(1.9610) | Loss 12.6612(13.0001) | Error 0.6844(0.6849) Steps 0(0.00) | Grad Norm 9.7766(26.4476) | Total Time 0.00(0.00)\n",
      "Iter 0430 | Time 13.4369(13.2779) | Bit/dim 5.2166(5.2786) | Xent 1.9402(1.9540) | Loss 12.6566(12.8523) | Error 0.6756(0.6814) Steps 0(0.00) | Grad Norm 23.1317(22.6293) | Total Time 0.00(0.00)\n",
      "Iter 0440 | Time 13.5673(13.3005) | Bit/dim 5.2302(5.2461) | Xent 1.9025(1.9411) | Loss 12.4943(12.7287) | Error 0.6656(0.6769) Steps 0(0.00) | Grad Norm 25.8856(21.1606) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0008 | Time 72.5714, Epoch Time 827.0932(746.3800), Bit/dim 5.1212(best: 5.2992), Xent 1.8727, Loss 6.0575, Error 0.6343(best: 0.6265)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0450 | Time 14.6112(13.4143) | Bit/dim 5.1216(5.2146) | Xent 1.9177(1.9305) | Loss 12.1100(13.0496) | Error 0.6700(0.6727) Steps 0(0.00) | Grad Norm 16.8206(20.8736) | Total Time 0.00(0.00)\n",
      "Iter 0460 | Time 15.6295(13.6006) | Bit/dim 5.0927(5.1803) | Xent 1.9166(1.9187) | Loss 12.4349(12.8585) | Error 0.6867(0.6689) Steps 0(0.00) | Grad Norm 30.8509(22.6746) | Total Time 0.00(0.00)\n",
      "Iter 0470 | Time 14.0512(13.7356) | Bit/dim 5.0584(5.1506) | Xent 1.8663(1.9086) | Loss 12.0425(12.6961) | Error 0.6556(0.6664) Steps 0(0.00) | Grad Norm 14.4520(22.7394) | Total Time 0.00(0.00)\n",
      "Iter 0480 | Time 14.3084(13.7241) | Bit/dim 5.0022(5.1179) | Xent 1.8538(1.9013) | Loss 12.1507(12.5422) | Error 0.6411(0.6659) Steps 0(0.00) | Grad Norm 16.8721(23.0701) | Total Time 0.00(0.00)\n",
      "Iter 0490 | Time 13.3909(13.7580) | Bit/dim 5.1185(5.1034) | Xent 1.9785(1.9130) | Loss 12.5461(12.4836) | Error 0.7189(0.6724) Steps 0(0.00) | Grad Norm 27.8989(28.8946) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0009 | Time 76.5698, Epoch Time 859.4785(749.7730), Bit/dim 4.9801(best: 5.1212), Xent 1.8956, Loss 5.9279, Error 0.6501(best: 0.6265)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0500 | Time 13.4315(13.8232) | Bit/dim 4.9617(5.0724) | Xent 1.9196(1.9217) | Loss 11.8627(12.8225) | Error 0.6822(0.6755) Steps 0(0.00) | Grad Norm 10.6595(26.1087) | Total Time 0.00(0.00)\n",
      "Iter 0510 | Time 14.3348(13.8674) | Bit/dim 4.9481(5.0397) | Xent 1.8572(1.9133) | Loss 12.1348(12.6104) | Error 0.6489(0.6727) Steps 0(0.00) | Grad Norm 9.3348(22.0978) | Total Time 0.00(0.00)\n",
      "Iter 0520 | Time 15.2216(13.9923) | Bit/dim 5.0723(5.0140) | Xent 1.8917(1.9018) | Loss 12.4349(12.4587) | Error 0.6900(0.6676) Steps 0(0.00) | Grad Norm 69.0774(23.2961) | Total Time 0.00(0.00)\n",
      "Iter 0530 | Time 14.3988(14.1189) | Bit/dim 4.8669(4.9817) | Xent 1.8739(1.8956) | Loss 11.6347(12.3266) | Error 0.6556(0.6647) Steps 0(0.00) | Grad Norm 6.3812(22.3976) | Total Time 0.00(0.00)\n",
      "Iter 0540 | Time 13.6473(14.1995) | Bit/dim 4.8976(4.9575) | Xent 1.8580(1.8847) | Loss 12.0075(12.2011) | Error 0.6511(0.6613) Steps 0(0.00) | Grad Norm 43.9802(23.2392) | Total Time 0.00(0.00)\n",
      "Iter 0550 | Time 15.7369(14.2799) | Bit/dim 4.8179(4.9406) | Xent 1.9684(1.8887) | Loss 11.8797(12.1306) | Error 0.6722(0.6620) Steps 0(0.00) | Grad Norm 33.7017(28.2219) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0010 | Time 75.5814, Epoch Time 881.1318(753.7137), Bit/dim 4.8451(best: 4.9801), Xent 1.8414, Loss 5.7658, Error 0.6388(best: 0.6265)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0560 | Time 14.1698(14.4604) | Bit/dim 4.8770(4.9148) | Xent 1.8278(1.8754) | Loss 11.7402(12.4705) | Error 0.6511(0.6576) Steps 0(0.00) | Grad Norm 45.1837(28.8089) | Total Time 0.00(0.00)\n",
      "Iter 0570 | Time 13.8703(14.3574) | Bit/dim 4.8368(4.9090) | Xent 1.9006(1.8738) | Loss 11.6783(12.2984) | Error 0.6711(0.6582) Steps 0(0.00) | Grad Norm 22.6397(31.6152) | Total Time 0.00(0.00)\n",
      "Iter 0580 | Time 14.4947(14.4820) | Bit/dim 4.8603(4.8910) | Xent 1.8843(1.8712) | Loss 11.7529(12.1903) | Error 0.6511(0.6571) Steps 0(0.00) | Grad Norm 20.7486(29.3640) | Total Time 0.00(0.00)\n",
      "Iter 0590 | Time 13.5938(14.5408) | Bit/dim 4.8186(4.8654) | Xent 1.7601(1.8544) | Loss 11.3483(12.0429) | Error 0.6178(0.6542) Steps 0(0.00) | Grad Norm 22.3954(26.9536) | Total Time 0.00(0.00)\n",
      "Iter 0600 | Time 16.8052(14.5500) | Bit/dim 4.8121(4.8437) | Xent 1.8042(1.8380) | Loss 11.5370(11.9251) | Error 0.6311(0.6475) Steps 0(0.00) | Grad Norm 34.2669(27.3335) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0011 | Time 78.6089, Epoch Time 901.2662(758.1403), Bit/dim 4.7461(best: 4.8451), Xent 1.7333, Loss 5.6128, Error 0.6070(best: 0.6265)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0610 | Time 13.9705(14.6501) | Bit/dim 4.8230(4.8238) | Xent 1.8501(1.8374) | Loss 11.3986(12.3501) | Error 0.6533(0.6501) Steps 0(0.00) | Grad Norm 38.9699(29.7518) | Total Time 0.00(0.00)\n",
      "Iter 0620 | Time 13.8333(14.7246) | Bit/dim 4.6915(4.7987) | Xent 1.7312(1.8277) | Loss 11.4358(12.1440) | Error 0.6044(0.6471) Steps 0(0.00) | Grad Norm 10.4029(27.8180) | Total Time 0.00(0.00)\n",
      "Iter 0630 | Time 15.7803(14.8039) | Bit/dim 4.8002(4.7910) | Xent 1.7127(1.8139) | Loss 11.4211(11.9885) | Error 0.6089(0.6426) Steps 0(0.00) | Grad Norm 36.2003(29.4299) | Total Time 0.00(0.00)\n",
      "Iter 0640 | Time 15.2706(14.9562) | Bit/dim 4.7116(4.7755) | Xent 1.7705(1.8145) | Loss 11.6408(11.8811) | Error 0.6233(0.6421) Steps 0(0.00) | Grad Norm 6.6072(29.1770) | Total Time 0.00(0.00)\n",
      "Iter 0650 | Time 14.2448(15.1192) | Bit/dim 4.6960(4.7550) | Xent 1.7821(1.8017) | Loss 11.3186(11.7645) | Error 0.6500(0.6391) Steps 0(0.00) | Grad Norm 20.7053(26.9820) | Total Time 0.00(0.00)\n",
      "Iter 0660 | Time 14.0401(15.0063) | Bit/dim 4.6766(4.7396) | Xent 1.7938(1.7960) | Loss 11.5169(11.6867) | Error 0.6233(0.6377) Steps 0(0.00) | Grad Norm 22.4942(26.8967) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0012 | Time 81.3736, Epoch Time 931.3052(763.3353), Bit/dim 4.6772(best: 4.7461), Xent 1.6847, Loss 5.5195, Error 0.5952(best: 0.6070)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0670 | Time 15.2245(15.0502) | Bit/dim 4.6581(4.7196) | Xent 1.6865(1.7751) | Loss 11.4159(12.0148) | Error 0.6311(0.6314) Steps 0(0.00) | Grad Norm 19.0394(25.6978) | Total Time 0.00(0.00)\n",
      "Iter 0680 | Time 14.1695(15.2262) | Bit/dim 4.7374(4.7245) | Xent 2.1427(1.8005) | Loss 11.8727(11.9191) | Error 0.7456(0.6378) Steps 0(0.00) | Grad Norm 67.1840(31.1238) | Total Time 0.00(0.00)\n",
      "Iter 0690 | Time 15.7745(15.2541) | Bit/dim 4.7051(4.7233) | Xent 1.8371(1.8206) | Loss 11.3312(11.8345) | Error 0.6511(0.6466) Steps 0(0.00) | Grad Norm 16.8844(28.6011) | Total Time 0.00(0.00)\n",
      "Iter 0700 | Time 15.5765(15.2744) | Bit/dim 4.6743(4.7030) | Xent 1.7716(1.8090) | Loss 11.3487(11.6965) | Error 0.6356(0.6436) Steps 0(0.00) | Grad Norm 17.2229(24.0132) | Total Time 0.00(0.00)\n",
      "Iter 0710 | Time 15.6926(15.1873) | Bit/dim 4.6299(4.6820) | Xent 1.6376(1.7852) | Loss 11.3904(11.6000) | Error 0.5678(0.6338) Steps 0(0.00) | Grad Norm 17.7896(20.6650) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0013 | Time 82.1749, Epoch Time 946.5112(768.8305), Bit/dim 4.7312(best: 4.6772), Xent 1.6922, Loss 5.5773, Error 0.6035(best: 0.5952)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0720 | Time 16.0469(15.3469) | Bit/dim 4.6369(4.6798) | Xent 1.7104(1.7827) | Loss 11.1803(12.0540) | Error 0.6156(0.6330) Steps 0(0.00) | Grad Norm 18.9190(23.8633) | Total Time 0.00(0.00)\n",
      "Iter 0730 | Time 14.3231(15.1649) | Bit/dim 4.5878(4.6651) | Xent 1.6960(1.7638) | Loss 10.8165(11.8282) | Error 0.6011(0.6261) Steps 0(0.00) | Grad Norm 17.0050(22.7442) | Total Time 0.00(0.00)\n",
      "Iter 0740 | Time 14.6496(15.2694) | Bit/dim 4.5407(4.6461) | Xent 1.7112(1.7506) | Loss 11.2654(11.6812) | Error 0.5967(0.6205) Steps 0(0.00) | Grad Norm 33.3835(22.9440) | Total Time 0.00(0.00)\n",
      "Iter 0750 | Time 14.8175(15.2534) | Bit/dim 4.5685(4.6292) | Xent 1.6798(1.7375) | Loss 11.0763(11.5415) | Error 0.5989(0.6168) Steps 0(0.00) | Grad Norm 21.5695(22.9284) | Total Time 0.00(0.00)\n",
      "Iter 0760 | Time 15.7420(15.2907) | Bit/dim 4.5548(4.6149) | Xent 1.6738(1.7235) | Loss 11.2991(11.4510) | Error 0.6022(0.6122) Steps 0(0.00) | Grad Norm 6.3341(22.8263) | Total Time 0.00(0.00)\n",
      "Iter 0770 | Time 15.3473(15.3241) | Bit/dim 4.5871(4.6081) | Xent 1.8361(1.7512) | Loss 11.3421(11.4124) | Error 0.6711(0.6225) Steps 0(0.00) | Grad Norm 26.5414(26.5222) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0014 | Time 81.4744, Epoch Time 939.0024(773.9357), Bit/dim 4.5595(best: 4.6772), Xent 1.7128, Loss 5.4159, Error 0.6029(best: 0.5952)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0780 | Time 15.0383(15.4127) | Bit/dim 4.5455(4.5928) | Xent 1.6848(1.7395) | Loss 10.8313(11.7869) | Error 0.6156(0.6209) Steps 0(0.00) | Grad Norm 19.6771(24.1940) | Total Time 0.00(0.00)\n",
      "Iter 0790 | Time 15.8170(15.4471) | Bit/dim 4.5533(4.5795) | Xent 1.6660(1.7182) | Loss 11.2887(11.6121) | Error 0.6011(0.6138) Steps 0(0.00) | Grad Norm 29.7649(22.3082) | Total Time 0.00(0.00)\n",
      "Iter 0800 | Time 14.6501(15.4627) | Bit/dim 4.5327(4.5681) | Xent 1.6905(1.7130) | Loss 10.9855(11.4767) | Error 0.6022(0.6103) Steps 0(0.00) | Grad Norm 17.2134(22.7693) | Total Time 0.00(0.00)\n",
      "Iter 0810 | Time 18.3067(15.5185) | Bit/dim 4.4783(4.5515) | Xent 1.7032(1.7006) | Loss 10.9822(11.3670) | Error 0.5900(0.6084) Steps 0(0.00) | Grad Norm 8.9763(21.2312) | Total Time 0.00(0.00)\n",
      "Iter 0820 | Time 14.4202(15.4614) | Bit/dim 4.4933(4.5378) | Xent 1.6665(1.6915) | Loss 10.8963(11.2626) | Error 0.5989(0.6073) Steps 0(0.00) | Grad Norm 7.7047(19.9388) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0015 | Time 82.6638, Epoch Time 960.4525(779.5312), Bit/dim 4.4884(best: 4.5595), Xent 1.5655, Loss 5.2712, Error 0.5587(best: 0.5952)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0830 | Time 14.3290(15.6318) | Bit/dim 4.6591(4.5333) | Xent 1.7141(1.6841) | Loss 11.2699(11.7637) | Error 0.6256(0.6038) Steps 0(0.00) | Grad Norm 36.0957(21.7534) | Total Time 0.00(0.00)\n",
      "Iter 0840 | Time 16.8993(15.6439) | Bit/dim 4.5530(4.5498) | Xent 1.7057(1.6858) | Loss 11.0726(11.6128) | Error 0.6244(0.6043) Steps 0(0.00) | Grad Norm 9.7864(21.7360) | Total Time 0.00(0.00)\n",
      "Iter 0850 | Time 15.8959(15.6132) | Bit/dim 4.4794(4.5404) | Xent 1.6305(1.6778) | Loss 10.9171(11.4559) | Error 0.6000(0.6039) Steps 0(0.00) | Grad Norm 13.0854(19.6964) | Total Time 0.00(0.00)\n",
      "Iter 0860 | Time 14.8736(15.5307) | Bit/dim 4.4561(4.5230) | Xent 1.6497(1.6629) | Loss 10.7893(11.2973) | Error 0.6011(0.5988) Steps 0(0.00) | Grad Norm 9.3167(19.0060) | Total Time 0.00(0.00)\n",
      "Iter 0870 | Time 14.7452(15.5403) | Bit/dim 4.4618(4.5030) | Xent 1.6334(1.6533) | Loss 10.9480(11.2215) | Error 0.5789(0.5959) Steps 0(0.00) | Grad Norm 10.8395(18.0583) | Total Time 0.00(0.00)\n",
      "Iter 0880 | Time 15.7711(15.5185) | Bit/dim 4.4039(4.4830) | Xent 1.5971(1.6438) | Loss 10.8080(11.1439) | Error 0.5667(0.5919) Steps 0(0.00) | Grad Norm 14.3204(17.8658) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0016 | Time 83.5728, Epoch Time 955.6990(784.8162), Bit/dim 4.4240(best: 4.4884), Xent 1.5242, Loss 5.1861, Error 0.5542(best: 0.5587)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0890 | Time 14.9133(15.4863) | Bit/dim 4.4236(4.4706) | Xent 1.6007(1.6364) | Loss 10.7533(11.5221) | Error 0.5678(0.5895) Steps 0(0.00) | Grad Norm 15.4735(18.0416) | Total Time 0.00(0.00)\n",
      "Iter 0900 | Time 14.3753(15.4978) | Bit/dim 4.3996(4.4566) | Xent 1.5497(1.6231) | Loss 10.5427(11.3330) | Error 0.5344(0.5835) Steps 0(0.00) | Grad Norm 17.0222(18.0248) | Total Time 0.00(0.00)\n",
      "Iter 0910 | Time 17.4752(15.7439) | Bit/dim 4.4020(4.4480) | Xent 1.5835(1.6320) | Loss 10.9312(11.2435) | Error 0.5800(0.5867) Steps 0(0.00) | Grad Norm 6.9325(19.7164) | Total Time 0.00(0.00)\n",
      "Iter 0920 | Time 16.7134(15.7996) | Bit/dim 4.3578(4.4313) | Xent 1.6292(1.6262) | Loss 10.4698(11.0970) | Error 0.5900(0.5853) Steps 0(0.00) | Grad Norm 8.7705(17.9005) | Total Time 0.00(0.00)\n",
      "Iter 0930 | Time 13.9149(15.6397) | Bit/dim 4.3831(4.4200) | Xent 1.6184(1.6192) | Loss 10.6253(11.0073) | Error 0.5800(0.5828) Steps 0(0.00) | Grad Norm 17.8769(17.3498) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0017 | Time 84.2700, Epoch Time 965.4179(790.2343), Bit/dim 4.3665(best: 4.4240), Xent 1.5283, Loss 5.1307, Error 0.5527(best: 0.5542)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0940 | Time 16.3080(15.6135) | Bit/dim 4.3751(4.4062) | Xent 1.5374(1.6084) | Loss 10.6944(11.4373) | Error 0.5589(0.5797) Steps 0(0.00) | Grad Norm 11.4548(17.2605) | Total Time 0.00(0.00)\n",
      "Iter 0950 | Time 16.4336(15.6853) | Bit/dim 4.3438(4.3929) | Xent 1.5088(1.5972) | Loss 10.7607(11.2283) | Error 0.5367(0.5762) Steps 0(0.00) | Grad Norm 18.3286(17.9012) | Total Time 0.00(0.00)\n",
      "Iter 0960 | Time 15.3111(15.6291) | Bit/dim 4.3969(4.3799) | Xent 1.6443(1.5938) | Loss 10.5129(11.0456) | Error 0.5922(0.5763) Steps 0(0.00) | Grad Norm 28.9993(17.6888) | Total Time 0.00(0.00)\n",
      "Iter 0970 | Time 16.6984(15.7124) | Bit/dim 4.3949(4.3733) | Xent 1.6252(1.5941) | Loss 10.7782(10.9539) | Error 0.5822(0.5754) Steps 0(0.00) | Grad Norm 27.9871(18.4133) | Total Time 0.00(0.00)\n",
      "Iter 0980 | Time 14.8445(15.7046) | Bit/dim 4.3605(4.3771) | Xent 1.5794(1.5927) | Loss 10.6068(10.8934) | Error 0.5678(0.5744) Steps 0(0.00) | Grad Norm 11.4694(18.6222) | Total Time 0.00(0.00)\n",
      "Iter 0990 | Time 14.5315(15.5677) | Bit/dim 4.3019(4.3705) | Xent 1.6905(1.6052) | Loss 10.6675(10.8388) | Error 0.6211(0.5801) Steps 0(0.00) | Grad Norm 29.0898(20.3006) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0018 | Time 82.8169, Epoch Time 960.2897(795.3359), Bit/dim 4.3483(best: 4.3665), Xent 1.5120, Loss 5.1043, Error 0.5355(best: 0.5527)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1000 | Time 15.8563(15.6494) | Bit/dim 4.2852(4.3551) | Xent 1.4809(1.5900) | Loss 10.6431(11.2200) | Error 0.5533(0.5768) Steps 0(0.00) | Grad Norm 5.5204(18.2691) | Total Time 0.00(0.00)\n",
      "Iter 1010 | Time 14.3967(15.5749) | Bit/dim 4.3059(4.3400) | Xent 1.5659(1.5779) | Loss 10.4200(11.0241) | Error 0.5822(0.5715) Steps 0(0.00) | Grad Norm 13.4041(15.7200) | Total Time 0.00(0.00)\n",
      "Iter 1020 | Time 16.6888(15.6854) | Bit/dim 4.3187(4.3291) | Xent 1.5751(1.5664) | Loss 10.6852(10.9033) | Error 0.5867(0.5658) Steps 0(0.00) | Grad Norm 35.4124(15.8350) | Total Time 0.00(0.00)\n",
      "Iter 1030 | Time 16.4881(15.7082) | Bit/dim 4.2531(4.3214) | Xent 1.5871(1.5654) | Loss 10.4810(10.8216) | Error 0.5811(0.5662) Steps 0(0.00) | Grad Norm 21.1793(16.8160) | Total Time 0.00(0.00)\n",
      "Iter 1040 | Time 16.1635(15.7772) | Bit/dim 4.3098(4.3125) | Xent 1.5855(1.5684) | Loss 10.5357(10.7447) | Error 0.5811(0.5680) Steps 0(0.00) | Grad Norm 23.0652(17.4406) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0019 | Time 81.9481, Epoch Time 967.2705(800.4940), Bit/dim 4.2704(best: 4.3483), Xent 1.4471, Loss 4.9940, Error 0.5203(best: 0.5355)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1050 | Time 14.9416(15.7813) | Bit/dim 4.2494(4.3054) | Xent 1.5544(1.5703) | Loss 10.4007(11.1746) | Error 0.5622(0.5684) Steps 0(0.00) | Grad Norm 9.9633(17.9648) | Total Time 0.00(0.00)\n",
      "Iter 1060 | Time 16.6087(15.8029) | Bit/dim 4.2626(4.2960) | Xent 1.5325(1.5639) | Loss 10.5887(10.9958) | Error 0.5500(0.5655) Steps 0(0.00) | Grad Norm 22.6913(18.1686) | Total Time 0.00(0.00)\n",
      "Iter 1070 | Time 16.0526(15.8673) | Bit/dim 4.2315(4.2813) | Xent 1.5125(1.5556) | Loss 10.3643(10.8464) | Error 0.5433(0.5628) Steps 0(0.00) | Grad Norm 17.5134(17.8380) | Total Time 0.00(0.00)\n",
      "Iter 1080 | Time 16.4022(15.8669) | Bit/dim 4.2402(4.2705) | Xent 1.5461(1.5456) | Loss 10.5076(10.7323) | Error 0.5544(0.5581) Steps 0(0.00) | Grad Norm 17.9369(16.9525) | Total Time 0.00(0.00)\n",
      "Iter 1090 | Time 15.8959(15.8428) | Bit/dim 4.2014(4.2584) | Xent 1.5395(1.5357) | Loss 10.4181(10.6252) | Error 0.5478(0.5566) Steps 0(0.00) | Grad Norm 7.4526(15.3353) | Total Time 0.00(0.00)\n",
      "Iter 1100 | Time 17.1548(15.9091) | Bit/dim 4.1992(4.2475) | Xent 1.5233(1.5228) | Loss 10.2381(10.5389) | Error 0.5478(0.5529) Steps 0(0.00) | Grad Norm 7.4909(14.4564) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0020 | Time 82.4904, Epoch Time 976.5155(805.7746), Bit/dim 4.2083(best: 4.2704), Xent 1.3905, Loss 4.9035, Error 0.5035(best: 0.5203)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1110 | Time 18.1151(16.1105) | Bit/dim 4.2374(4.2349) | Xent 1.4614(1.5038) | Loss 10.2706(10.9092) | Error 0.5256(0.5455) Steps 0(0.00) | Grad Norm 19.7331(13.1051) | Total Time 0.00(0.00)\n",
      "Iter 1120 | Time 15.4356(16.0527) | Bit/dim 4.1969(4.2225) | Xent 1.3822(1.4949) | Loss 10.2223(10.7388) | Error 0.4933(0.5429) Steps 0(0.00) | Grad Norm 14.8963(13.8835) | Total Time 0.00(0.00)\n",
      "Iter 1130 | Time 14.8670(15.8074) | Bit/dim 4.2045(4.2182) | Xent 1.4660(1.4838) | Loss 10.0946(10.6124) | Error 0.5322(0.5401) Steps 0(0.00) | Grad Norm 15.5998(15.0371) | Total Time 0.00(0.00)\n",
      "Iter 1140 | Time 15.5841(15.7778) | Bit/dim 4.2028(4.2106) | Xent 1.5207(1.4785) | Loss 10.1366(10.5205) | Error 0.5789(0.5373) Steps 0(0.00) | Grad Norm 25.7333(14.2827) | Total Time 0.00(0.00)\n",
      "Iter 1150 | Time 16.0700(15.7482) | Bit/dim 4.1819(4.2033) | Xent 1.5195(1.4833) | Loss 10.3213(10.4226) | Error 0.5467(0.5380) Steps 0(0.00) | Grad Norm 26.3363(15.8988) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0021 | Time 83.8206, Epoch Time 973.7109(810.8127), Bit/dim 4.1719(best: 4.2083), Xent 1.3537, Loss 4.8488, Error 0.4906(best: 0.5035)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1160 | Time 16.6528(15.9212) | Bit/dim 4.1317(4.1949) | Xent 1.4577(1.4746) | Loss 10.1010(10.8896) | Error 0.5122(0.5330) Steps 0(0.00) | Grad Norm 7.0642(14.8354) | Total Time 0.00(0.00)\n",
      "Iter 1170 | Time 15.9119(15.8029) | Bit/dim 4.1559(4.1884) | Xent 1.4218(1.4606) | Loss 10.2631(10.7051) | Error 0.5111(0.5279) Steps 0(0.00) | Grad Norm 16.9616(13.5489) | Total Time 0.00(0.00)\n",
      "Iter 1180 | Time 14.6573(15.7727) | Bit/dim 4.1573(4.1844) | Xent 1.3790(1.4490) | Loss 9.8370(10.5519) | Error 0.5122(0.5256) Steps 0(0.00) | Grad Norm 13.7672(13.9191) | Total Time 0.00(0.00)\n",
      "Iter 1190 | Time 15.7948(15.7127) | Bit/dim 4.1552(4.1707) | Xent 1.5086(1.4367) | Loss 10.1291(10.4164) | Error 0.5500(0.5228) Steps 0(0.00) | Grad Norm 16.6770(13.6017) | Total Time 0.00(0.00)\n",
      "Iter 1200 | Time 14.8755(15.7084) | Bit/dim 4.1536(4.1603) | Xent 1.4563(1.4352) | Loss 10.2214(10.3433) | Error 0.5256(0.5218) Steps 0(0.00) | Grad Norm 17.9244(14.1117) | Total Time 0.00(0.00)\n",
      "Iter 1210 | Time 14.8080(15.6196) | Bit/dim 4.1281(4.1518) | Xent 1.4012(1.4406) | Loss 10.0399(10.2711) | Error 0.5089(0.5230) Steps 0(0.00) | Grad Norm 18.4607(15.0107) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0022 | Time 82.5252, Epoch Time 960.1125(815.2917), Bit/dim 4.1386(best: 4.1719), Xent 1.3211, Loss 4.7991, Error 0.4793(best: 0.4906)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1220 | Time 14.5129(15.5510) | Bit/dim 4.1063(4.1421) | Xent 1.4289(1.4259) | Loss 10.1001(10.6609) | Error 0.5200(0.5186) Steps 0(0.00) | Grad Norm 14.1525(14.8648) | Total Time 0.00(0.00)\n",
      "Iter 1230 | Time 15.6030(15.4946) | Bit/dim 4.1147(4.1380) | Xent 1.4411(1.4169) | Loss 9.8206(10.4819) | Error 0.5167(0.5132) Steps 0(0.00) | Grad Norm 12.5887(14.5900) | Total Time 0.00(0.00)\n",
      "Iter 1240 | Time 14.7721(15.4032) | Bit/dim 4.1231(4.1318) | Xent 1.3908(1.4134) | Loss 10.0139(10.3316) | Error 0.5167(0.5128) Steps 0(0.00) | Grad Norm 20.4295(14.8122) | Total Time 0.00(0.00)\n",
      "Iter 1250 | Time 15.5546(15.4623) | Bit/dim 4.1230(4.1312) | Xent 1.3328(1.4086) | Loss 10.1590(10.2682) | Error 0.4778(0.5111) Steps 0(0.00) | Grad Norm 14.1578(14.3517) | Total Time 0.00(0.00)\n",
      "Iter 1260 | Time 14.9689(15.3318) | Bit/dim 4.1093(4.1237) | Xent 1.3518(1.3985) | Loss 9.7944(10.1733) | Error 0.4967(0.5097) Steps 0(0.00) | Grad Norm 8.6387(13.5206) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0023 | Time 81.3339, Epoch Time 943.8885(819.1496), Bit/dim 4.1026(best: 4.1386), Xent 1.2768, Loss 4.7410, Error 0.4660(best: 0.4793)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1270 | Time 17.1853(15.5244) | Bit/dim 4.0920(4.1177) | Xent 1.4082(1.3907) | Loss 10.1970(10.6588) | Error 0.5267(0.5064) Steps 0(0.00) | Grad Norm 17.8422(12.8799) | Total Time 0.00(0.00)\n",
      "Iter 1280 | Time 15.0305(15.5383) | Bit/dim 4.1130(4.1128) | Xent 1.4224(1.3916) | Loss 9.8286(10.4827) | Error 0.5211(0.5072) Steps 0(0.00) | Grad Norm 18.4685(14.0001) | Total Time 0.00(0.00)\n",
      "Iter 1290 | Time 14.7411(15.5417) | Bit/dim 4.0890(4.1107) | Xent 1.3923(1.3888) | Loss 9.9819(10.3667) | Error 0.5122(0.5065) Steps 0(0.00) | Grad Norm 7.2140(14.9106) | Total Time 0.00(0.00)\n",
      "Iter 1300 | Time 14.4333(15.4207) | Bit/dim 4.0845(4.1008) | Xent 1.3614(1.3854) | Loss 9.8103(10.2426) | Error 0.4978(0.5074) Steps 0(0.00) | Grad Norm 10.2429(13.9683) | Total Time 0.00(0.00)\n",
      "Iter 1310 | Time 15.6470(15.4576) | Bit/dim 4.0995(4.0975) | Xent 1.3999(1.3735) | Loss 10.2915(10.1613) | Error 0.5167(0.5038) Steps 0(0.00) | Grad Norm 15.0902(13.6373) | Total Time 0.00(0.00)\n",
      "Iter 1320 | Time 14.8104(15.3925) | Bit/dim 4.1069(4.0929) | Xent 1.3522(1.3752) | Loss 10.1428(10.0939) | Error 0.4822(0.5019) Steps 0(0.00) | Grad Norm 20.4091(14.2542) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0024 | Time 80.3919, Epoch Time 950.2028(823.0812), Bit/dim 4.0792(best: 4.1026), Xent 1.3306, Loss 4.7445, Error 0.4851(best: 0.4660)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1330 | Time 15.1063(15.3844) | Bit/dim 4.0791(4.0875) | Xent 1.2434(1.3638) | Loss 9.9340(10.4982) | Error 0.4533(0.4965) Steps 0(0.00) | Grad Norm 9.5058(13.2426) | Total Time 0.00(0.00)\n",
      "Iter 1340 | Time 13.7586(15.3136) | Bit/dim 4.0612(4.0812) | Xent 1.3539(1.3545) | Loss 9.5946(10.3218) | Error 0.5000(0.4930) Steps 0(0.00) | Grad Norm 13.6928(13.4204) | Total Time 0.00(0.00)\n",
      "Iter 1350 | Time 14.6259(15.3556) | Bit/dim 4.0600(4.0749) | Xent 1.3153(1.3441) | Loss 9.7757(10.2047) | Error 0.4889(0.4902) Steps 0(0.00) | Grad Norm 8.9334(12.1438) | Total Time 0.00(0.00)\n",
      "Iter 1360 | Time 15.7734(15.5319) | Bit/dim 4.0676(4.0684) | Xent 1.3406(1.3388) | Loss 9.9421(10.1186) | Error 0.4733(0.4879) Steps 0(0.00) | Grad Norm 8.5887(11.2143) | Total Time 0.00(0.00)\n",
      "Iter 1370 | Time 16.2818(15.5605) | Bit/dim 4.0156(4.0596) | Xent 1.3326(1.3396) | Loss 9.8126(10.0575) | Error 0.4900(0.4880) Steps 0(0.00) | Grad Norm 10.1857(11.1088) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0025 | Time 82.1817, Epoch Time 953.0230(826.9795), Bit/dim 4.0366(best: 4.0792), Xent 1.2688, Loss 4.6709, Error 0.4608(best: 0.4660)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1380 | Time 15.4244(15.5298) | Bit/dim 4.0114(4.0537) | Xent 1.3468(1.3344) | Loss 9.8603(10.5125) | Error 0.5089(0.4864) Steps 0(0.00) | Grad Norm 9.6159(11.7930) | Total Time 0.00(0.00)\n",
      "Iter 1390 | Time 16.9930(15.5653) | Bit/dim 4.0470(4.0475) | Xent 1.4182(1.3351) | Loss 10.1536(10.3317) | Error 0.5022(0.4850) Steps 0(0.00) | Grad Norm 24.3408(13.1021) | Total Time 0.00(0.00)\n",
      "Iter 1400 | Time 14.5085(15.5260) | Bit/dim 4.0392(4.0434) | Xent 1.2525(1.3306) | Loss 9.7392(10.1936) | Error 0.4544(0.4834) Steps 0(0.00) | Grad Norm 6.9210(12.3541) | Total Time 0.00(0.00)\n",
      "Iter 1410 | Time 14.2910(15.3972) | Bit/dim 4.0404(4.0403) | Xent 1.2696(1.3291) | Loss 9.6818(10.0803) | Error 0.4611(0.4839) Steps 0(0.00) | Grad Norm 9.3022(11.8734) | Total Time 0.00(0.00)\n",
      "Iter 1420 | Time 16.9800(15.4395) | Bit/dim 4.0182(4.0358) | Xent 1.2503(1.3163) | Loss 9.8662(10.0075) | Error 0.4444(0.4782) Steps 0(0.00) | Grad Norm 6.1937(11.1395) | Total Time 0.00(0.00)\n",
      "Iter 1430 | Time 17.2375(15.5470) | Bit/dim 4.0111(4.0345) | Xent 1.3208(1.3149) | Loss 9.9409(9.9555) | Error 0.4922(0.4785) Steps 0(0.00) | Grad Norm 16.5257(11.8110) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0026 | Time 80.6897, Epoch Time 951.5548(830.7167), Bit/dim 4.0270(best: 4.0366), Xent 1.2577, Loss 4.6558, Error 0.4542(best: 0.4608)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1440 | Time 16.2611(15.6281) | Bit/dim 4.0393(4.0281) | Xent 1.2334(1.3069) | Loss 9.7752(10.3215) | Error 0.4478(0.4742) Steps 0(0.00) | Grad Norm 19.6387(11.8800) | Total Time 0.00(0.00)\n",
      "Iter 1450 | Time 14.8479(15.6079) | Bit/dim 3.9932(4.0236) | Xent 1.2391(1.2952) | Loss 9.4584(10.1511) | Error 0.4489(0.4695) Steps 0(0.00) | Grad Norm 4.2530(10.7287) | Total Time 0.00(0.00)\n",
      "Iter 1460 | Time 17.0648(15.6549) | Bit/dim 3.9494(4.0154) | Xent 1.2508(1.2908) | Loss 9.5850(10.0374) | Error 0.4722(0.4681) Steps 0(0.00) | Grad Norm 9.4754(10.7085) | Total Time 0.00(0.00)\n",
      "Iter 1470 | Time 15.7209(15.6895) | Bit/dim 4.0024(4.0130) | Xent 1.2751(1.2862) | Loss 9.9621(9.9768) | Error 0.4556(0.4673) Steps 0(0.00) | Grad Norm 13.5415(11.1028) | Total Time 0.00(0.00)\n",
      "Iter 1480 | Time 14.1987(15.7123) | Bit/dim 3.9880(4.0106) | Xent 1.3580(1.2881) | Loss 9.3625(9.9002) | Error 0.4767(0.4666) Steps 0(0.00) | Grad Norm 10.4177(11.2509) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0027 | Time 80.8889, Epoch Time 965.7622(834.7681), Bit/dim 4.0104(best: 4.0270), Xent 1.2574, Loss 4.6391, Error 0.4504(best: 0.4542)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1490 | Time 16.0835(15.6230) | Bit/dim 3.9859(4.0082) | Xent 1.3259(1.2879) | Loss 9.8055(10.3728) | Error 0.4767(0.4650) Steps 0(0.00) | Grad Norm 19.9370(12.7224) | Total Time 0.00(0.00)\n",
      "Iter 1500 | Time 16.7864(15.7012) | Bit/dim 3.9983(4.0049) | Xent 1.3054(1.2890) | Loss 9.6531(10.2035) | Error 0.4856(0.4660) Steps 0(0.00) | Grad Norm 22.2454(13.1958) | Total Time 0.00(0.00)\n",
      "Iter 1510 | Time 15.6241(15.7380) | Bit/dim 3.9852(4.0018) | Xent 1.2495(1.2848) | Loss 9.6178(10.0773) | Error 0.4422(0.4653) Steps 0(0.00) | Grad Norm 11.5750(12.9971) | Total Time 0.00(0.00)\n",
      "Iter 1520 | Time 17.0824(15.7131) | Bit/dim 4.0021(3.9979) | Xent 1.3284(1.2905) | Loss 9.7346(9.9845) | Error 0.4889(0.4686) Steps 0(0.00) | Grad Norm 15.1830(13.2235) | Total Time 0.00(0.00)\n",
      "Iter 1530 | Time 15.0707(15.7653) | Bit/dim 4.0132(3.9940) | Xent 1.2030(1.2821) | Loss 9.7368(9.9001) | Error 0.4222(0.4651) Steps 0(0.00) | Grad Norm 17.3936(13.1143) | Total Time 0.00(0.00)\n",
      "Iter 1540 | Time 14.7972(15.5690) | Bit/dim 4.0002(3.9912) | Xent 1.2673(1.2799) | Loss 9.4298(9.8155) | Error 0.4456(0.4625) Steps 0(0.00) | Grad Norm 7.4520(12.5585) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0028 | Time 81.3649, Epoch Time 957.0712(838.4372), Bit/dim 3.9721(best: 4.0104), Xent 1.1846, Loss 4.5644, Error 0.4272(best: 0.4504)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1550 | Time 15.2545(15.6404) | Bit/dim 3.9831(3.9872) | Xent 1.2162(1.2692) | Loss 9.6863(10.2395) | Error 0.4367(0.4585) Steps 0(0.00) | Grad Norm 9.0244(12.0943) | Total Time 0.00(0.00)\n",
      "Iter 1560 | Time 14.7789(15.5393) | Bit/dim 3.9589(3.9814) | Xent 1.2382(1.2654) | Loss 9.4110(10.0541) | Error 0.4467(0.4561) Steps 0(0.00) | Grad Norm 13.3768(11.4682) | Total Time 0.00(0.00)\n",
      "Iter 1570 | Time 15.5061(15.5288) | Bit/dim 3.9549(3.9779) | Xent 1.1685(1.2545) | Loss 9.6317(9.9404) | Error 0.4256(0.4512) Steps 0(0.00) | Grad Norm 9.0742(11.1175) | Total Time 0.00(0.00)\n",
      "Iter 1580 | Time 14.9756(15.5112) | Bit/dim 3.9684(3.9750) | Xent 1.2810(1.2547) | Loss 9.5386(9.8661) | Error 0.4589(0.4515) Steps 0(0.00) | Grad Norm 20.9301(11.8022) | Total Time 0.00(0.00)\n",
      "Iter 1590 | Time 14.6254(15.4552) | Bit/dim 3.9989(3.9722) | Xent 1.2029(1.2589) | Loss 9.5790(9.7922) | Error 0.4389(0.4540) Steps 0(0.00) | Grad Norm 13.3903(12.4613) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0029 | Time 76.4809, Epoch Time 946.9751(841.6933), Bit/dim 3.9517(best: 3.9721), Xent 1.1852, Loss 4.5443, Error 0.4359(best: 0.4272)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1600 | Time 14.0088(15.4490) | Bit/dim 3.9677(3.9694) | Xent 1.2289(1.2539) | Loss 9.5112(10.2537) | Error 0.4511(0.4530) Steps 0(0.00) | Grad Norm 8.9060(11.6409) | Total Time 0.00(0.00)\n",
      "Iter 1610 | Time 15.1429(15.2565) | Bit/dim 3.9454(3.9639) | Xent 1.1813(1.2371) | Loss 9.2146(10.0367) | Error 0.4289(0.4467) Steps 0(0.00) | Grad Norm 9.7982(10.3254) | Total Time 0.00(0.00)\n",
      "Iter 1620 | Time 15.8907(15.2190) | Bit/dim 3.9480(3.9617) | Xent 1.2565(1.2369) | Loss 9.5373(9.9171) | Error 0.4433(0.4465) Steps 0(0.00) | Grad Norm 4.9023(10.9044) | Total Time 0.00(0.00)\n",
      "Iter 1630 | Time 15.1949(15.1379) | Bit/dim 3.9339(3.9588) | Xent 1.2493(1.2415) | Loss 9.4626(9.8204) | Error 0.4322(0.4470) Steps 0(0.00) | Grad Norm 21.8235(11.4208) | Total Time 0.00(0.00)\n",
      "Iter 1640 | Time 14.5837(15.0498) | Bit/dim 3.9698(3.9585) | Xent 1.2447(1.2421) | Loss 9.4979(9.7451) | Error 0.4411(0.4464) Steps 0(0.00) | Grad Norm 5.3019(11.7376) | Total Time 0.00(0.00)\n",
      "Iter 1650 | Time 14.8008(14.9836) | Bit/dim 3.9526(3.9554) | Xent 1.2335(1.2512) | Loss 9.6139(9.7071) | Error 0.4433(0.4498) Steps 0(0.00) | Grad Norm 16.5470(12.8234) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0030 | Time 76.3808, Epoch Time 911.7968(843.7964), Bit/dim 3.9714(best: 3.9517), Xent 1.1522, Loss 4.5475, Error 0.4178(best: 0.4272)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1660 | Time 13.7797(14.9804) | Bit/dim 3.9334(3.9539) | Xent 1.2057(1.2463) | Loss 9.4316(10.0955) | Error 0.4367(0.4478) Steps 0(0.00) | Grad Norm 7.8435(12.4448) | Total Time 0.00(0.00)\n",
      "Iter 1670 | Time 14.0654(14.8449) | Bit/dim 3.9234(3.9517) | Xent 1.1478(1.2338) | Loss 9.4265(9.9184) | Error 0.4433(0.4431) Steps 0(0.00) | Grad Norm 6.8928(11.7913) | Total Time 0.00(0.00)\n",
      "Iter 1680 | Time 14.6439(14.7853) | Bit/dim 3.9239(3.9482) | Xent 1.2195(1.2245) | Loss 9.3504(9.8090) | Error 0.4600(0.4396) Steps 0(0.00) | Grad Norm 11.1911(11.1684) | Total Time 0.00(0.00)\n",
      "Iter 1690 | Time 14.4295(14.7695) | Bit/dim 3.9694(3.9479) | Xent 1.1959(1.2239) | Loss 9.5688(9.7377) | Error 0.4333(0.4395) Steps 0(0.00) | Grad Norm 6.9700(11.2866) | Total Time 0.00(0.00)\n",
      "Iter 1700 | Time 13.4606(14.7371) | Bit/dim 3.9123(3.9423) | Xent 1.2165(1.2249) | Loss 9.3216(9.6625) | Error 0.4122(0.4382) Steps 0(0.00) | Grad Norm 14.0389(11.1725) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0031 | Time 75.8448, Epoch Time 905.2449(845.6399), Bit/dim 3.9325(best: 3.9517), Xent 1.1551, Loss 4.5101, Error 0.4195(best: 0.4178)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1710 | Time 15.4924(14.8492) | Bit/dim 3.9396(3.9375) | Xent 1.1825(1.2178) | Loss 9.6890(10.1308) | Error 0.4256(0.4375) Steps 0(0.00) | Grad Norm 17.0453(11.6071) | Total Time 0.00(0.00)\n",
      "Iter 1720 | Time 14.5594(15.0389) | Bit/dim 3.9483(3.9379) | Xent 1.2139(1.2114) | Loss 9.5135(9.9648) | Error 0.4233(0.4341) Steps 0(0.00) | Grad Norm 11.3343(11.2852) | Total Time 0.00(0.00)\n",
      "Iter 1730 | Time 14.6913(15.0428) | Bit/dim 3.9299(3.9346) | Xent 1.1548(1.2100) | Loss 9.5912(9.8446) | Error 0.4144(0.4337) Steps 0(0.00) | Grad Norm 8.5749(11.0427) | Total Time 0.00(0.00)\n",
      "Iter 1740 | Time 15.9033(15.1207) | Bit/dim 3.9620(3.9320) | Xent 1.2394(1.2124) | Loss 9.4472(9.7557) | Error 0.4489(0.4349) Steps 0(0.00) | Grad Norm 14.9452(11.4503) | Total Time 0.00(0.00)\n",
      "Iter 1750 | Time 14.3596(15.0650) | Bit/dim 3.9672(3.9305) | Xent 1.1773(1.2170) | Loss 9.5410(9.6912) | Error 0.4378(0.4369) Steps 0(0.00) | Grad Norm 12.1122(11.7893) | Total Time 0.00(0.00)\n",
      "Iter 1760 | Time 14.8390(15.1235) | Bit/dim 3.9039(3.9255) | Xent 1.1438(1.2137) | Loss 9.2454(9.6325) | Error 0.4089(0.4357) Steps 0(0.00) | Grad Norm 8.4878(11.2238) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0032 | Time 76.7567, Epoch Time 928.3509(848.1212), Bit/dim 3.9150(best: 3.9325), Xent 1.1489, Loss 4.4895, Error 0.4121(best: 0.4178)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1770 | Time 14.6580(15.0482) | Bit/dim 3.9029(3.9238) | Xent 1.0966(1.2016) | Loss 9.2747(10.0403) | Error 0.4067(0.4315) Steps 0(0.00) | Grad Norm 5.5062(10.7469) | Total Time 0.00(0.00)\n",
      "Iter 1780 | Time 14.7901(15.0449) | Bit/dim 3.9470(3.9227) | Xent 1.1974(1.1973) | Loss 9.4921(9.8941) | Error 0.4122(0.4313) Steps 0(0.00) | Grad Norm 14.1618(11.0122) | Total Time 0.00(0.00)\n",
      "Iter 1790 | Time 14.3566(15.0166) | Bit/dim 3.9031(3.9172) | Xent 1.1803(1.1954) | Loss 9.2535(9.7576) | Error 0.4244(0.4308) Steps 0(0.00) | Grad Norm 7.2794(11.1148) | Total Time 0.00(0.00)\n",
      "Iter 1800 | Time 14.6678(15.0855) | Bit/dim 3.9147(3.9169) | Xent 1.2463(1.1961) | Loss 9.4028(9.6815) | Error 0.4367(0.4303) Steps 0(0.00) | Grad Norm 17.8397(11.4349) | Total Time 0.00(0.00)\n",
      "Iter 1810 | Time 15.3769(15.1860) | Bit/dim 3.9182(3.9126) | Xent 1.1324(1.1925) | Loss 9.4902(9.5906) | Error 0.3800(0.4282) Steps 0(0.00) | Grad Norm 13.1171(11.2107) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0033 | Time 76.7555, Epoch Time 921.0491(850.3090), Bit/dim 3.9109(best: 3.9150), Xent 1.1015, Loss 4.4616, Error 0.3986(best: 0.4121)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1820 | Time 14.2241(15.0613) | Bit/dim 3.9148(3.9120) | Xent 1.1159(1.1851) | Loss 9.4993(10.0603) | Error 0.4244(0.4270) Steps 0(0.00) | Grad Norm 7.1915(10.9666) | Total Time 0.00(0.00)\n",
      "Iter 1830 | Time 13.6942(15.0704) | Bit/dim 3.8629(3.9091) | Xent 1.1163(1.1742) | Loss 9.1861(9.8837) | Error 0.3911(0.4232) Steps 0(0.00) | Grad Norm 13.5256(10.2760) | Total Time 0.00(0.00)\n",
      "Iter 1840 | Time 14.8342(15.0981) | Bit/dim 3.8929(3.9065) | Xent 1.2965(1.1767) | Loss 9.0318(9.7456) | Error 0.4567(0.4236) Steps 0(0.00) | Grad Norm 27.9641(11.4599) | Total Time 0.00(0.00)\n",
      "Iter 1850 | Time 16.6910(15.1496) | Bit/dim 3.8881(3.9095) | Xent 1.1926(1.1862) | Loss 9.6272(9.6775) | Error 0.4389(0.4284) Steps 0(0.00) | Grad Norm 9.9098(12.4548) | Total Time 0.00(0.00)\n",
      "Iter 1860 | Time 16.2761(15.0330) | Bit/dim 3.8824(3.9049) | Xent 1.1255(1.1839) | Loss 9.2607(9.5902) | Error 0.4189(0.4255) Steps 0(0.00) | Grad Norm 13.2640(12.0591) | Total Time 0.00(0.00)\n",
      "Iter 1870 | Time 15.4961(15.1473) | Bit/dim 3.8995(3.9042) | Xent 1.1518(1.1789) | Loss 9.3327(9.5527) | Error 0.4011(0.4233) Steps 0(0.00) | Grad Norm 6.9028(10.8245) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0034 | Time 76.7200, Epoch Time 925.1732(852.5550), Bit/dim 3.8898(best: 3.9109), Xent 1.1137, Loss 4.4467, Error 0.4056(best: 0.3986)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1880 | Time 16.2392(15.1868) | Bit/dim 3.9113(3.9020) | Xent 1.1040(1.1682) | Loss 9.3422(9.9388) | Error 0.4011(0.4187) Steps 0(0.00) | Grad Norm 9.7510(10.1517) | Total Time 0.00(0.00)\n",
      "Iter 1890 | Time 14.8628(15.0969) | Bit/dim 3.8836(3.8990) | Xent 1.0617(1.1540) | Loss 9.0964(9.7698) | Error 0.3833(0.4140) Steps 0(0.00) | Grad Norm 8.6538(9.0613) | Total Time 0.00(0.00)\n",
      "Iter 1900 | Time 14.6910(15.2503) | Bit/dim 3.8766(3.8960) | Xent 1.1026(1.1481) | Loss 9.0289(9.6532) | Error 0.4133(0.4119) Steps 0(0.00) | Grad Norm 10.0492(9.3284) | Total Time 0.00(0.00)\n",
      "Iter 1910 | Time 15.2104(15.2364) | Bit/dim 3.8825(3.8932) | Xent 1.1514(1.1471) | Loss 9.5252(9.5763) | Error 0.3856(0.4125) Steps 0(0.00) | Grad Norm 13.4734(9.6174) | Total Time 0.00(0.00)\n",
      "Iter 1920 | Time 14.7405(15.2243) | Bit/dim 3.8868(3.8868) | Xent 1.1750(1.1481) | Loss 9.3379(9.5102) | Error 0.4089(0.4125) Steps 0(0.00) | Grad Norm 9.8725(9.7568) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0035 | Time 75.6601, Epoch Time 933.1579(854.9731), Bit/dim 3.9032(best: 3.8898), Xent 1.1417, Loss 4.4740, Error 0.4120(best: 0.3986)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1930 | Time 16.3440(15.4002) | Bit/dim 3.8877(3.8901) | Xent 1.2880(1.1596) | Loss 9.5256(10.0071) | Error 0.4756(0.4174) Steps 0(0.00) | Grad Norm 15.2945(10.9972) | Total Time 0.00(0.00)\n",
      "Iter 1940 | Time 14.6514(15.3625) | Bit/dim 3.8698(3.8868) | Xent 1.1366(1.1561) | Loss 9.2024(9.8257) | Error 0.4244(0.4172) Steps 0(0.00) | Grad Norm 8.3045(11.0175) | Total Time 0.00(0.00)\n",
      "Iter 1950 | Time 15.5529(15.2104) | Bit/dim 3.9039(3.8880) | Xent 1.1791(1.1553) | Loss 9.4798(9.6975) | Error 0.4289(0.4161) Steps 0(0.00) | Grad Norm 15.8729(11.3469) | Total Time 0.00(0.00)\n",
      "Iter 1960 | Time 15.5590(15.1458) | Bit/dim 3.8653(3.8842) | Xent 1.1116(1.1461) | Loss 9.2882(9.5870) | Error 0.4156(0.4125) Steps 0(0.00) | Grad Norm 5.9039(10.4225) | Total Time 0.00(0.00)\n",
      "Iter 1970 | Time 14.9201(15.1222) | Bit/dim 3.8675(3.8791) | Xent 1.1135(1.1375) | Loss 9.2120(9.4933) | Error 0.4033(0.4081) Steps 0(0.00) | Grad Norm 12.2474(9.9950) | Total Time 0.00(0.00)\n",
      "Iter 1980 | Time 15.8228(15.2859) | Bit/dim 3.8821(3.8807) | Xent 1.1039(1.1316) | Loss 9.4464(9.4324) | Error 0.3944(0.4065) Steps 0(0.00) | Grad Norm 10.8660(10.4474) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0036 | Time 75.9569, Epoch Time 929.5796(857.2113), Bit/dim 3.8703(best: 3.8898), Xent 1.0503, Loss 4.3954, Error 0.3794(best: 0.3986)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1990 | Time 17.9035(15.3491) | Bit/dim 3.8793(3.8787) | Xent 1.1324(1.1249) | Loss 9.5600(9.8454) | Error 0.4033(0.4024) Steps 0(0.00) | Grad Norm 9.9587(10.2225) | Total Time 0.00(0.00)\n",
      "Iter 2000 | Time 17.4361(15.4203) | Bit/dim 3.8583(3.8773) | Xent 1.1227(1.1146) | Loss 9.3738(9.7108) | Error 0.3967(0.3988) Steps 0(0.00) | Grad Norm 11.2341(9.8655) | Total Time 0.00(0.00)\n",
      "Iter 2010 | Time 14.3714(15.4234) | Bit/dim 3.8769(3.8754) | Xent 1.1020(1.1145) | Loss 9.1733(9.6020) | Error 0.4067(0.4003) Steps 0(0.00) | Grad Norm 6.0479(9.5950) | Total Time 0.00(0.00)\n",
      "Iter 2020 | Time 15.2780(15.3440) | Bit/dim 3.8780(3.8714) | Xent 1.1559(1.1054) | Loss 9.3605(9.5002) | Error 0.4189(0.3959) Steps 0(0.00) | Grad Norm 7.7023(9.5439) | Total Time 0.00(0.00)\n",
      "Iter 2030 | Time 14.6024(15.3706) | Bit/dim 3.9319(3.8695) | Xent 1.1645(1.1103) | Loss 9.1740(9.4449) | Error 0.4089(0.3956) Steps 0(0.00) | Grad Norm 7.0188(10.1776) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0037 | Time 76.6771, Epoch Time 943.1656(859.7899), Bit/dim 3.8724(best: 3.8703), Xent 1.0965, Loss 4.4207, Error 0.3957(best: 0.3794)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2040 | Time 15.1919(15.4431) | Bit/dim 3.8866(3.8702) | Xent 1.0935(1.1128) | Loss 9.3971(9.9312) | Error 0.3833(0.3971) Steps 0(0.00) | Grad Norm 8.0708(10.8115) | Total Time 0.00(0.00)\n",
      "Iter 2050 | Time 15.7155(15.4055) | Bit/dim 3.8718(3.8688) | Xent 1.0999(1.1077) | Loss 9.3703(9.7529) | Error 0.3744(0.3946) Steps 0(0.00) | Grad Norm 10.7806(10.8960) | Total Time 0.00(0.00)\n",
      "Iter 2060 | Time 15.1076(15.4512) | Bit/dim 3.8693(3.8692) | Xent 1.1890(1.1141) | Loss 9.2960(9.6533) | Error 0.4344(0.3975) Steps 0(0.00) | Grad Norm 10.8737(11.2442) | Total Time 0.00(0.00)\n",
      "Iter 2070 | Time 14.1025(15.4029) | Bit/dim 3.8420(3.8672) | Xent 1.0786(1.1083) | Loss 9.0986(9.5416) | Error 0.3922(0.3953) Steps 0(0.00) | Grad Norm 6.5827(11.1743) | Total Time 0.00(0.00)\n",
      "Iter 2080 | Time 14.1507(15.3391) | Bit/dim 3.8749(3.8687) | Xent 1.0762(1.1015) | Loss 9.1522(9.4701) | Error 0.3811(0.3941) Steps 0(0.00) | Grad Norm 9.5345(10.3622) | Total Time 0.00(0.00)\n",
      "Iter 2090 | Time 15.6464(15.2243) | Bit/dim 3.8122(3.8632) | Xent 1.1170(1.1011) | Loss 9.3008(9.4272) | Error 0.4078(0.3936) Steps 0(0.00) | Grad Norm 13.5926(10.4404) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0038 | Time 77.2893, Epoch Time 933.3557(861.9969), Bit/dim 3.8594(best: 3.8703), Xent 1.0213, Loss 4.3700, Error 0.3677(best: 0.3794)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2100 | Time 15.8584(15.2618) | Bit/dim 3.8667(3.8614) | Xent 1.0646(1.0969) | Loss 9.2729(9.8351) | Error 0.3656(0.3927) Steps 0(0.00) | Grad Norm 7.6731(10.7959) | Total Time 0.00(0.00)\n",
      "Iter 2110 | Time 15.7142(15.1891) | Bit/dim 3.8601(3.8636) | Xent 1.1616(1.0888) | Loss 9.3287(9.6509) | Error 0.4344(0.3913) Steps 0(0.00) | Grad Norm 8.4698(10.9397) | Total Time 0.00(0.00)\n",
      "Iter 2120 | Time 14.9186(15.1341) | Bit/dim 3.8376(3.8599) | Xent 1.0452(1.0809) | Loss 9.2865(9.5375) | Error 0.3644(0.3896) Steps 0(0.00) | Grad Norm 5.1311(10.3456) | Total Time 0.00(0.00)\n",
      "Iter 2130 | Time 16.5206(15.1074) | Bit/dim 3.8317(3.8576) | Xent 1.0655(1.0858) | Loss 9.1868(9.4678) | Error 0.3889(0.3897) Steps 0(0.00) | Grad Norm 6.5684(10.0683) | Total Time 0.00(0.00)\n",
      "Iter 2140 | Time 14.3775(15.1546) | Bit/dim 3.8842(3.8573) | Xent 1.0795(1.0869) | Loss 9.0749(9.4045) | Error 0.3900(0.3895) Steps 0(0.00) | Grad Norm 13.3836(10.7064) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0039 | Time 77.4327, Epoch Time 923.4474(863.8404), Bit/dim 3.8594(best: 3.8594), Xent 1.0062, Loss 4.3625, Error 0.3600(best: 0.3677)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2150 | Time 17.2279(15.2342) | Bit/dim 3.8452(3.8554) | Xent 1.0708(1.0786) | Loss 9.1894(9.8723) | Error 0.3733(0.3859) Steps 0(0.00) | Grad Norm 8.4935(9.8522) | Total Time 0.00(0.00)\n",
      "Iter 2160 | Time 15.3653(15.4833) | Bit/dim 3.8649(3.8558) | Xent 1.0119(1.0654) | Loss 9.1918(9.7109) | Error 0.3567(0.3799) Steps 0(0.00) | Grad Norm 12.2479(9.6136) | Total Time 0.00(0.00)\n",
      "Iter 2170 | Time 16.1909(15.5351) | Bit/dim 3.8343(3.8571) | Xent 1.0234(1.0618) | Loss 9.3028(9.5823) | Error 0.3733(0.3788) Steps 0(0.00) | Grad Norm 6.7578(9.9081) | Total Time 0.00(0.00)\n",
      "Iter 2180 | Time 15.0119(15.4529) | Bit/dim 3.8330(3.8550) | Xent 1.0358(1.0655) | Loss 9.2253(9.4840) | Error 0.3633(0.3816) Steps 0(0.00) | Grad Norm 12.3671(9.7989) | Total Time 0.00(0.00)\n",
      "Iter 2190 | Time 17.5836(15.4589) | Bit/dim 3.8195(3.8514) | Xent 1.0948(1.0625) | Loss 9.3884(9.4295) | Error 0.4133(0.3803) Steps 0(0.00) | Grad Norm 10.4671(9.5819) | Total Time 0.00(0.00)\n",
      "Iter 2200 | Time 14.4667(15.4773) | Bit/dim 3.8285(3.8476) | Xent 1.0427(1.0575) | Loss 9.0448(9.3682) | Error 0.3722(0.3789) Steps 0(0.00) | Grad Norm 14.5098(9.3355) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0040 | Time 77.5909, Epoch Time 954.0986(866.5481), Bit/dim 3.8504(best: 3.8594), Xent 1.0315, Loss 4.3662, Error 0.3617(best: 0.3600)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2210 | Time 15.4416(15.3985) | Bit/dim 3.8567(3.8445) | Xent 1.0004(1.0493) | Loss 9.1136(9.7309) | Error 0.3522(0.3759) Steps 0(0.00) | Grad Norm 10.2235(9.9860) | Total Time 0.00(0.00)\n",
      "Iter 2220 | Time 14.1594(15.3236) | Bit/dim 3.8309(3.8425) | Xent 1.0945(1.0437) | Loss 9.0153(9.5642) | Error 0.3867(0.3720) Steps 0(0.00) | Grad Norm 9.5561(10.4652) | Total Time 0.00(0.00)\n",
      "Iter 2230 | Time 14.8796(15.2874) | Bit/dim 3.8482(3.8422) | Xent 1.0840(1.0448) | Loss 9.2429(9.4714) | Error 0.3833(0.3713) Steps 0(0.00) | Grad Norm 6.6267(10.6095) | Total Time 0.00(0.00)\n",
      "Iter 2240 | Time 13.8865(15.1692) | Bit/dim 3.8289(3.8419) | Xent 1.0403(1.0516) | Loss 8.8867(9.3729) | Error 0.3700(0.3746) Steps 0(0.00) | Grad Norm 13.0353(11.0476) | Total Time 0.00(0.00)\n",
      "Iter 2250 | Time 16.1081(15.2267) | Bit/dim 3.8311(3.8426) | Xent 1.0633(1.0477) | Loss 9.2985(9.3435) | Error 0.3511(0.3734) Steps 0(0.00) | Grad Norm 13.9491(10.7822) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0041 | Time 75.8775, Epoch Time 925.5691(868.3188), Bit/dim 3.8352(best: 3.8504), Xent 0.9808, Loss 4.3256, Error 0.3516(best: 0.3600)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2260 | Time 14.7751(15.1899) | Bit/dim 3.8305(3.8427) | Xent 0.9637(1.0413) | Loss 9.0411(9.8090) | Error 0.3389(0.3706) Steps 0(0.00) | Grad Norm 7.8546(10.9242) | Total Time 0.00(0.00)\n",
      "Iter 2270 | Time 15.3268(15.2098) | Bit/dim 3.8416(3.8410) | Xent 1.0640(1.0391) | Loss 9.0852(9.6313) | Error 0.3711(0.3688) Steps 0(0.00) | Grad Norm 10.3462(11.0074) | Total Time 0.00(0.00)\n",
      "Iter 2280 | Time 14.5607(15.1419) | Bit/dim 3.8389(3.8403) | Xent 0.9870(1.0332) | Loss 9.1691(9.4981) | Error 0.3589(0.3663) Steps 0(0.00) | Grad Norm 14.0382(10.8486) | Total Time 0.00(0.00)\n",
      "Iter 2290 | Time 15.4736(15.0979) | Bit/dim 3.7953(3.8361) | Xent 1.0627(1.0330) | Loss 9.0142(9.3956) | Error 0.3756(0.3667) Steps 0(0.00) | Grad Norm 7.9897(10.5051) | Total Time 0.00(0.00)\n",
      "Iter 2300 | Time 15.8488(15.2505) | Bit/dim 3.8375(3.8329) | Xent 1.0343(1.0399) | Loss 9.0299(9.3332) | Error 0.3733(0.3712) Steps 0(0.00) | Grad Norm 10.9570(10.3485) | Total Time 0.00(0.00)\n",
      "Iter 2310 | Time 16.3646(15.3136) | Bit/dim 3.8215(3.8305) | Xent 1.0486(1.0290) | Loss 9.3629(9.2877) | Error 0.3678(0.3689) Steps 0(0.00) | Grad Norm 11.9482(9.5898) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0042 | Time 76.5166, Epoch Time 930.8943(870.1960), Bit/dim 3.8271(best: 3.8352), Xent 1.0024, Loss 4.3283, Error 0.3556(best: 0.3516)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2320 | Time 15.1950(15.3016) | Bit/dim 3.8323(3.8300) | Xent 0.9522(1.0305) | Loss 9.0590(9.6867) | Error 0.3256(0.3691) Steps 0(0.00) | Grad Norm 6.6797(9.3268) | Total Time 0.00(0.00)\n",
      "Iter 2330 | Time 19.6389(15.3900) | Bit/dim 3.8133(3.8298) | Xent 0.9698(1.0210) | Loss 9.3701(9.5339) | Error 0.3411(0.3656) Steps 0(0.00) | Grad Norm 9.6232(9.8734) | Total Time 0.00(0.00)\n",
      "Iter 2340 | Time 14.6319(15.2907) | Bit/dim 3.8226(3.8299) | Xent 0.9757(1.0148) | Loss 9.2029(9.4322) | Error 0.3389(0.3627) Steps 0(0.00) | Grad Norm 9.0553(10.3120) | Total Time 0.00(0.00)\n",
      "Iter 2350 | Time 17.1953(15.6049) | Bit/dim 3.8190(3.8275) | Xent 1.0468(1.0211) | Loss 9.2461(9.3715) | Error 0.3789(0.3641) Steps 0(0.00) | Grad Norm 7.6670(10.4473) | Total Time 0.00(0.00)\n",
      "Iter 2360 | Time 16.4152(15.5557) | Bit/dim 3.7854(3.8262) | Xent 1.0336(1.0172) | Loss 9.0240(9.3125) | Error 0.3533(0.3642) Steps 0(0.00) | Grad Norm 12.3440(10.0023) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0043 | Time 76.9661, Epoch Time 947.1091(872.5034), Bit/dim 3.8201(best: 3.8271), Xent 0.9740, Loss 4.3071, Error 0.3472(best: 0.3516)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2370 | Time 15.4689(15.4750) | Bit/dim 3.8378(3.8255) | Xent 0.9066(1.0134) | Loss 9.2179(9.7851) | Error 0.3233(0.3639) Steps 0(0.00) | Grad Norm 11.8960(9.7218) | Total Time 0.00(0.00)\n",
      "Iter 2380 | Time 16.1413(15.3527) | Bit/dim 3.8609(3.8294) | Xent 1.0175(1.0062) | Loss 9.3320(9.6112) | Error 0.3556(0.3606) Steps 0(0.00) | Grad Norm 9.5904(9.6976) | Total Time 0.00(0.00)\n",
      "Iter 2390 | Time 15.2915(15.2947) | Bit/dim 3.7959(3.8259) | Xent 1.0218(1.0088) | Loss 9.0891(9.4935) | Error 0.3822(0.3610) Steps 0(0.00) | Grad Norm 16.4715(9.8108) | Total Time 0.00(0.00)\n",
      "Iter 2400 | Time 14.9757(15.1930) | Bit/dim 3.8162(3.8237) | Xent 1.0014(1.0102) | Loss 9.1791(9.3881) | Error 0.3644(0.3621) Steps 0(0.00) | Grad Norm 10.1652(9.9828) | Total Time 0.00(0.00)\n",
      "Iter 2410 | Time 14.6144(15.1438) | Bit/dim 3.8006(3.8207) | Xent 1.0432(1.0079) | Loss 9.0837(9.2879) | Error 0.3656(0.3609) Steps 0(0.00) | Grad Norm 10.2066(10.0163) | Total Time 0.00(0.00)\n",
      "Iter 2420 | Time 14.6675(15.2045) | Bit/dim 3.8034(3.8178) | Xent 0.9041(0.9993) | Loss 9.0648(9.2389) | Error 0.3278(0.3585) Steps 0(0.00) | Grad Norm 8.7231(9.4717) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0044 | Time 77.2174, Epoch Time 924.6449(874.0677), Bit/dim 3.8179(best: 3.8201), Xent 0.9577, Loss 4.2967, Error 0.3396(best: 0.3472)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2430 | Time 14.9818(15.1184) | Bit/dim 3.8094(3.8175) | Xent 0.9383(0.9895) | Loss 9.1915(9.6508) | Error 0.3522(0.3556) Steps 0(0.00) | Grad Norm 10.9937(9.4211) | Total Time 0.00(0.00)\n",
      "Iter 2440 | Time 15.8731(15.2843) | Bit/dim 3.8203(3.8154) | Xent 1.0296(0.9932) | Loss 9.1745(9.5065) | Error 0.3756(0.3568) Steps 0(0.00) | Grad Norm 11.3755(10.0155) | Total Time 0.00(0.00)\n",
      "Iter 2450 | Time 15.8694(15.2496) | Bit/dim 3.8209(3.8150) | Xent 0.9306(0.9925) | Loss 9.0784(9.4034) | Error 0.3233(0.3561) Steps 0(0.00) | Grad Norm 13.2572(10.2694) | Total Time 0.00(0.00)\n",
      "Iter 2460 | Time 15.3116(15.2479) | Bit/dim 3.8286(3.8158) | Xent 1.0036(0.9939) | Loss 9.2250(9.3320) | Error 0.3644(0.3558) Steps 0(0.00) | Grad Norm 8.9827(10.4959) | Total Time 0.00(0.00)\n",
      "Iter 2470 | Time 16.6370(15.2509) | Bit/dim 3.8119(3.8150) | Xent 1.0050(0.9941) | Loss 9.0494(9.2501) | Error 0.3589(0.3573) Steps 0(0.00) | Grad Norm 10.7358(10.6415) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0045 | Time 76.1513, Epoch Time 929.6703(875.7357), Bit/dim 3.8255(best: 3.8179), Xent 1.0022, Loss 4.3265, Error 0.3539(best: 0.3396)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2480 | Time 14.4042(15.2013) | Bit/dim 3.7797(3.8143) | Xent 0.9969(0.9961) | Loss 8.6815(9.7297) | Error 0.3644(0.3570) Steps 0(0.00) | Grad Norm 8.8049(10.4711) | Total Time 0.00(0.00)\n",
      "Iter 2490 | Time 15.6764(15.0951) | Bit/dim 3.8295(3.8135) | Xent 1.0595(0.9931) | Loss 9.1976(9.5500) | Error 0.3900(0.3560) Steps 0(0.00) | Grad Norm 11.0954(10.1601) | Total Time 0.00(0.00)\n",
      "Iter 2500 | Time 14.2454(15.1561) | Bit/dim 3.7920(3.8131) | Xent 1.0056(0.9880) | Loss 9.0578(9.4154) | Error 0.3544(0.3543) Steps 0(0.00) | Grad Norm 6.2470(9.7899) | Total Time 0.00(0.00)\n",
      "Iter 2510 | Time 15.5577(15.2406) | Bit/dim 3.8093(3.8141) | Xent 1.0379(0.9847) | Loss 9.3953(9.3424) | Error 0.3733(0.3522) Steps 0(0.00) | Grad Norm 10.4263(9.7955) | Total Time 0.00(0.00)\n",
      "Iter 2520 | Time 13.9713(15.1345) | Bit/dim 3.8148(3.8115) | Xent 1.0184(0.9868) | Loss 8.9840(9.2607) | Error 0.3611(0.3519) Steps 0(0.00) | Grad Norm 10.7205(10.1026) | Total Time 0.00(0.00)\n",
      "Iter 2530 | Time 16.9537(15.1896) | Bit/dim 3.7822(3.8106) | Xent 0.9087(0.9866) | Loss 9.0338(9.2071) | Error 0.3222(0.3512) Steps 0(0.00) | Grad Norm 11.7531(10.6730) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0046 | Time 76.7687, Epoch Time 925.9346(877.2417), Bit/dim 3.8042(best: 3.8179), Xent 0.9301, Loss 4.2693, Error 0.3284(best: 0.3396)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2540 | Time 17.0342(15.3080) | Bit/dim 3.8141(3.8096) | Xent 0.9749(0.9821) | Loss 8.8477(9.6188) | Error 0.3489(0.3496) Steps 0(0.00) | Grad Norm 11.1217(10.4342) | Total Time 0.00(0.00)\n",
      "Iter 2550 | Time 15.6154(15.3421) | Bit/dim 3.8019(3.8107) | Xent 1.0238(0.9810) | Loss 9.2089(9.4808) | Error 0.3633(0.3491) Steps 0(0.00) | Grad Norm 14.6140(10.3573) | Total Time 0.00(0.00)\n",
      "Iter 2560 | Time 15.6992(15.3144) | Bit/dim 3.8327(3.8074) | Xent 0.9068(0.9710) | Loss 9.0452(9.3415) | Error 0.3211(0.3457) Steps 0(0.00) | Grad Norm 11.9104(10.0743) | Total Time 0.00(0.00)\n",
      "Iter 2570 | Time 14.9236(15.2351) | Bit/dim 3.8299(3.8082) | Xent 1.0283(0.9686) | Loss 8.9369(9.2630) | Error 0.3589(0.3453) Steps 0(0.00) | Grad Norm 10.8406(10.4224) | Total Time 0.00(0.00)\n",
      "Iter 2580 | Time 16.5944(15.3654) | Bit/dim 3.7997(3.8055) | Xent 0.9370(0.9645) | Loss 9.1836(9.2182) | Error 0.3178(0.3441) Steps 0(0.00) | Grad Norm 12.4079(10.0029) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0047 | Time 78.6386, Epoch Time 946.5624(879.3213), Bit/dim 3.8070(best: 3.8042), Xent 0.9152, Loss 4.2646, Error 0.3219(best: 0.3284)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2590 | Time 15.6762(15.4737) | Bit/dim 3.7610(3.8000) | Xent 0.8718(0.9657) | Loss 8.8914(9.7156) | Error 0.3078(0.3442) Steps 0(0.00) | Grad Norm 11.7586(10.3611) | Total Time 0.00(0.00)\n",
      "Iter 2600 | Time 15.4547(15.5112) | Bit/dim 3.7927(3.8012) | Xent 0.9208(0.9606) | Loss 8.7346(9.5031) | Error 0.3389(0.3410) Steps 0(0.00) | Grad Norm 16.7440(10.9071) | Total Time 0.00(0.00)\n",
      "Iter 2610 | Time 16.3803(15.4043) | Bit/dim 3.8095(3.8019) | Xent 0.9415(0.9646) | Loss 9.2478(9.3836) | Error 0.3133(0.3427) Steps 0(0.00) | Grad Norm 9.9917(10.5322) | Total Time 0.00(0.00)\n",
      "Iter 2620 | Time 13.9154(15.3263) | Bit/dim 3.7862(3.8005) | Xent 0.9702(0.9719) | Loss 8.9306(9.2987) | Error 0.3500(0.3457) Steps 0(0.00) | Grad Norm 10.4439(10.9274) | Total Time 0.00(0.00)\n",
      "Iter 2630 | Time 15.1779(15.2554) | Bit/dim 3.7762(3.8005) | Xent 1.0418(0.9672) | Loss 9.0603(9.2094) | Error 0.3644(0.3449) Steps 0(0.00) | Grad Norm 6.0819(10.0541) | Total Time 0.00(0.00)\n",
      "Iter 2640 | Time 14.0028(15.3012) | Bit/dim 3.8324(3.8011) | Xent 0.9478(0.9649) | Loss 8.8557(9.1463) | Error 0.3267(0.3433) Steps 0(0.00) | Grad Norm 10.4744(9.7146) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0048 | Time 77.2905, Epoch Time 933.1576(880.9364), Bit/dim 3.7923(best: 3.8042), Xent 0.9310, Loss 4.2578, Error 0.3281(best: 0.3219)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2650 | Time 16.8722(15.3340) | Bit/dim 3.7506(3.7967) | Xent 0.9352(0.9567) | Loss 9.2240(9.5480) | Error 0.3467(0.3411) Steps 0(0.00) | Grad Norm 5.0620(9.2714) | Total Time 0.00(0.00)\n",
      "Iter 2660 | Time 15.3352(15.4427) | Bit/dim 3.7846(3.7954) | Xent 0.9343(0.9487) | Loss 9.0953(9.4076) | Error 0.3378(0.3385) Steps 0(0.00) | Grad Norm 12.1873(8.9273) | Total Time 0.00(0.00)\n",
      "Iter 2670 | Time 15.8904(15.3459) | Bit/dim 3.8039(3.7942) | Xent 1.0156(0.9467) | Loss 9.1051(9.2981) | Error 0.3400(0.3374) Steps 0(0.00) | Grad Norm 12.6709(9.1466) | Total Time 0.00(0.00)\n",
      "Iter 2680 | Time 17.2712(15.4445) | Bit/dim 3.7940(3.7943) | Xent 0.9161(0.9388) | Loss 9.1370(9.2242) | Error 0.3144(0.3347) Steps 0(0.00) | Grad Norm 8.3765(8.7931) | Total Time 0.00(0.00)\n",
      "Iter 2690 | Time 15.0818(15.4688) | Bit/dim 3.7454(3.7907) | Xent 1.0190(0.9501) | Loss 9.0372(9.1664) | Error 0.3622(0.3387) Steps 0(0.00) | Grad Norm 11.7245(8.9712) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0049 | Time 76.8913, Epoch Time 942.3872(882.7799), Bit/dim 3.7880(best: 3.7923), Xent 0.9355, Loss 4.2557, Error 0.3349(best: 0.3219)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2700 | Time 15.4726(15.4617) | Bit/dim 3.7892(3.7914) | Xent 0.9754(0.9533) | Loss 8.9894(9.6660) | Error 0.3533(0.3411) Steps 0(0.00) | Grad Norm 11.4912(9.4430) | Total Time 0.00(0.00)\n",
      "Iter 2710 | Time 15.1697(15.4433) | Bit/dim 3.7867(3.7940) | Xent 0.9016(0.9513) | Loss 9.0499(9.4944) | Error 0.3133(0.3397) Steps 0(0.00) | Grad Norm 8.3376(9.8843) | Total Time 0.00(0.00)\n",
      "Iter 2720 | Time 14.5161(15.4315) | Bit/dim 3.7767(3.7941) | Xent 0.8968(0.9417) | Loss 9.0300(9.3586) | Error 0.3133(0.3344) Steps 0(0.00) | Grad Norm 10.7770(9.1121) | Total Time 0.00(0.00)\n",
      "Iter 2730 | Time 15.2111(15.3990) | Bit/dim 3.7662(3.7894) | Xent 0.8833(0.9366) | Loss 8.8533(9.2484) | Error 0.3156(0.3333) Steps 0(0.00) | Grad Norm 10.4700(9.0968) | Total Time 0.00(0.00)\n",
      "Iter 2740 | Time 14.1168(15.3443) | Bit/dim 3.8032(3.7871) | Xent 1.0240(0.9477) | Loss 9.1322(9.2014) | Error 0.3711(0.3373) Steps 0(0.00) | Grad Norm 13.8758(9.8088) | Total Time 0.00(0.00)\n",
      "Iter 2750 | Time 15.0795(15.3782) | Bit/dim 3.8253(3.7873) | Xent 1.0169(0.9495) | Loss 8.9042(9.1340) | Error 0.3722(0.3392) Steps 0(0.00) | Grad Norm 16.8172(9.8089) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0050 | Time 79.0774, Epoch Time 941.3505(884.5371), Bit/dim 3.7908(best: 3.7880), Xent 1.0129, Loss 4.2973, Error 0.3580(best: 0.3219)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2760 | Time 15.7563(15.3306) | Bit/dim 3.7834(3.7867) | Xent 0.9226(0.9506) | Loss 9.0201(9.5488) | Error 0.3233(0.3398) Steps 0(0.00) | Grad Norm 6.2694(10.5735) | Total Time 0.00(0.00)\n",
      "Iter 2770 | Time 14.9508(15.4074) | Bit/dim 3.7938(3.7863) | Xent 0.9651(0.9512) | Loss 9.0481(9.3805) | Error 0.3500(0.3406) Steps 0(0.00) | Grad Norm 8.8537(10.1013) | Total Time 0.00(0.00)\n",
      "Iter 2780 | Time 15.8806(15.4516) | Bit/dim 3.7833(3.7856) | Xent 0.9972(0.9450) | Loss 8.9691(9.2834) | Error 0.3478(0.3371) Steps 0(0.00) | Grad Norm 12.9076(10.0400) | Total Time 0.00(0.00)\n",
      "Iter 2790 | Time 15.3348(15.5807) | Bit/dim 3.7827(3.7863) | Xent 0.9128(0.9365) | Loss 9.1055(9.2197) | Error 0.3200(0.3342) Steps 0(0.00) | Grad Norm 11.5363(9.7198) | Total Time 0.00(0.00)\n",
      "Iter 2800 | Time 14.9427(15.4598) | Bit/dim 3.7719(3.7857) | Xent 0.8677(0.9298) | Loss 8.8036(9.1583) | Error 0.2811(0.3316) Steps 0(0.00) | Grad Norm 5.6693(9.3653) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0051 | Time 77.5806, Epoch Time 945.4319(886.3639), Bit/dim 3.7827(best: 3.7880), Xent 0.8950, Loss 4.2302, Error 0.3194(best: 0.3219)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2810 | Time 15.5713(15.5485) | Bit/dim 3.7906(3.7868) | Xent 0.9823(0.9295) | Loss 9.1507(9.6531) | Error 0.3478(0.3322) Steps 0(0.00) | Grad Norm 8.4202(9.1641) | Total Time 0.00(0.00)\n",
      "Iter 2820 | Time 15.3546(15.3556) | Bit/dim 3.7681(3.7859) | Xent 0.9122(0.9264) | Loss 8.9068(9.4540) | Error 0.3178(0.3320) Steps 0(0.00) | Grad Norm 7.7015(9.3572) | Total Time 0.00(0.00)\n",
      "Iter 2830 | Time 14.7126(15.1675) | Bit/dim 3.7708(3.7841) | Xent 0.8544(0.9245) | Loss 8.8240(9.3113) | Error 0.3067(0.3316) Steps 0(0.00) | Grad Norm 12.3066(9.3085) | Total Time 0.00(0.00)\n",
      "Iter 2840 | Time 14.6367(15.2115) | Bit/dim 3.7693(3.7820) | Xent 0.8689(0.9182) | Loss 8.9155(9.2027) | Error 0.3133(0.3295) Steps 0(0.00) | Grad Norm 8.5724(9.3089) | Total Time 0.00(0.00)\n",
      "Iter 2850 | Time 14.6007(15.2698) | Bit/dim 3.7699(3.7826) | Xent 0.9991(0.9187) | Loss 8.9752(9.1355) | Error 0.3367(0.3294) Steps 0(0.00) | Grad Norm 11.9354(9.4722) | Total Time 0.00(0.00)\n",
      "Iter 2860 | Time 15.2035(15.2640) | Bit/dim 3.7870(3.7806) | Xent 0.9089(0.9277) | Loss 8.9956(9.0994) | Error 0.3100(0.3310) Steps 0(0.00) | Grad Norm 5.9252(9.9427) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0052 | Time 78.9246, Epoch Time 931.6290(887.7219), Bit/dim 3.7811(best: 3.7827), Xent 0.9586, Loss 4.2604, Error 0.3365(best: 0.3194)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2870 | Time 15.7833(15.2413) | Bit/dim 3.7602(3.7804) | Xent 0.9804(0.9278) | Loss 9.1332(9.5314) | Error 0.3511(0.3312) Steps 0(0.00) | Grad Norm 18.1016(10.4825) | Total Time 0.00(0.00)\n",
      "Iter 2880 | Time 14.6356(15.2188) | Bit/dim 3.7495(3.7799) | Xent 0.8842(0.9239) | Loss 8.9581(9.3805) | Error 0.3022(0.3277) Steps 0(0.00) | Grad Norm 5.7303(9.9851) | Total Time 0.00(0.00)\n",
      "Iter 2890 | Time 14.8990(15.2596) | Bit/dim 3.7516(3.7782) | Xent 0.8317(0.9166) | Loss 8.9821(9.2732) | Error 0.2944(0.3254) Steps 0(0.00) | Grad Norm 3.6938(8.8140) | Total Time 0.00(0.00)\n",
      "Iter 2900 | Time 14.8442(15.1843) | Bit/dim 3.7720(3.7762) | Xent 0.9643(0.9167) | Loss 9.0006(9.1818) | Error 0.3422(0.3258) Steps 0(0.00) | Grad Norm 10.6812(8.7778) | Total Time 0.00(0.00)\n",
      "Iter 2910 | Time 14.2540(15.1308) | Bit/dim 3.7619(3.7747) | Xent 0.9098(0.9132) | Loss 8.9960(9.0962) | Error 0.3300(0.3258) Steps 0(0.00) | Grad Norm 5.6354(8.8783) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0053 | Time 78.0810, Epoch Time 924.8267(888.8350), Bit/dim 3.7676(best: 3.7811), Xent 0.8817, Loss 4.2084, Error 0.3154(best: 0.3194)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2920 | Time 15.6739(15.0377) | Bit/dim 3.7813(3.7759) | Xent 0.8997(0.9072) | Loss 8.9758(9.5695) | Error 0.3033(0.3238) Steps 0(0.00) | Grad Norm 9.4729(8.9108) | Total Time 0.00(0.00)\n",
      "Iter 2930 | Time 15.7552(15.1697) | Bit/dim 3.7619(3.7740) | Xent 0.8945(0.9060) | Loss 8.8996(9.4131) | Error 0.3278(0.3240) Steps 0(0.00) | Grad Norm 11.7164(9.2338) | Total Time 0.00(0.00)\n",
      "Iter 2940 | Time 16.5054(15.1350) | Bit/dim 3.7747(3.7724) | Xent 0.9278(0.9004) | Loss 8.9690(9.2773) | Error 0.3267(0.3201) Steps 0(0.00) | Grad Norm 9.6363(9.3416) | Total Time 0.00(0.00)\n",
      "Iter 2950 | Time 15.7125(15.3337) | Bit/dim 3.7599(3.7709) | Xent 0.9460(0.9042) | Loss 8.6867(9.1779) | Error 0.3300(0.3217) Steps 0(0.00) | Grad Norm 8.5314(8.7101) | Total Time 0.00(0.00)\n",
      "Iter 2960 | Time 17.1691(15.3218) | Bit/dim 3.7477(3.7682) | Xent 0.8487(0.9016) | Loss 8.9501(9.0979) | Error 0.3000(0.3207) Steps 0(0.00) | Grad Norm 9.4964(8.8722) | Total Time 0.00(0.00)\n",
      "Iter 2970 | Time 17.0162(15.4221) | Bit/dim 3.7718(3.7694) | Xent 0.9315(0.9014) | Loss 9.0996(9.0479) | Error 0.3089(0.3201) Steps 0(0.00) | Grad Norm 16.9316(9.1737) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0054 | Time 78.2204, Epoch Time 941.3419(890.4102), Bit/dim 3.7736(best: 3.7676), Xent 0.8934, Loss 4.2203, Error 0.3209(best: 0.3154)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2980 | Time 14.5920(15.6764) | Bit/dim 3.7542(3.7718) | Xent 0.9286(0.8976) | Loss 8.9146(9.4855) | Error 0.3300(0.3172) Steps 0(0.00) | Grad Norm 9.1924(9.2543) | Total Time 0.00(0.00)\n",
      "Iter 2990 | Time 14.6018(15.4213) | Bit/dim 3.7649(3.7716) | Xent 0.8653(0.8882) | Loss 8.8309(9.3153) | Error 0.3078(0.3155) Steps 0(0.00) | Grad Norm 13.8795(9.4079) | Total Time 0.00(0.00)\n",
      "Iter 3000 | Time 14.2725(15.3540) | Bit/dim 3.8142(3.7744) | Xent 0.8403(0.8845) | Loss 8.8001(9.1965) | Error 0.2989(0.3139) Steps 0(0.00) | Grad Norm 10.1254(9.7256) | Total Time 0.00(0.00)\n",
      "Iter 3010 | Time 15.9813(15.3152) | Bit/dim 3.7615(3.7688) | Xent 0.8440(0.8824) | Loss 8.9224(9.1192) | Error 0.2778(0.3119) Steps 0(0.00) | Grad Norm 4.5407(9.1205) | Total Time 0.00(0.00)\n",
      "Iter 3020 | Time 15.2531(15.1716) | Bit/dim 3.7558(3.7667) | Xent 0.9786(0.8836) | Loss 9.0629(9.0653) | Error 0.3456(0.3123) Steps 0(0.00) | Grad Norm 9.5304(8.7642) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0055 | Time 77.7872, Epoch Time 929.9077(891.5951), Bit/dim 3.7660(best: 3.7676), Xent 0.8839, Loss 4.2079, Error 0.3131(best: 0.3154)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3030 | Time 15.9907(15.1021) | Bit/dim 3.7661(3.7636) | Xent 0.8999(0.8891) | Loss 9.1227(9.5386) | Error 0.3333(0.3151) Steps 0(0.00) | Grad Norm 6.2973(8.9267) | Total Time 0.00(0.00)\n",
      "Iter 3040 | Time 15.3205(15.1097) | Bit/dim 3.7740(3.7654) | Xent 0.8702(0.8928) | Loss 8.9180(9.3733) | Error 0.3156(0.3155) Steps 0(0.00) | Grad Norm 7.1412(9.7274) | Total Time 0.00(0.00)\n",
      "Iter 3050 | Time 15.7822(15.1065) | Bit/dim 3.7755(3.7651) | Xent 0.8360(0.8932) | Loss 8.8431(9.2490) | Error 0.3078(0.3186) Steps 0(0.00) | Grad Norm 6.8555(9.2411) | Total Time 0.00(0.00)\n",
      "Iter 3060 | Time 16.1617(15.2269) | Bit/dim 3.7812(3.7658) | Xent 0.9307(0.8928) | Loss 9.1829(9.1686) | Error 0.3322(0.3188) Steps 0(0.00) | Grad Norm 13.6316(9.7203) | Total Time 0.00(0.00)\n",
      "Iter 3070 | Time 14.9255(15.3042) | Bit/dim 3.7939(3.7637) | Xent 0.8944(0.8935) | Loss 8.8838(9.0955) | Error 0.3089(0.3188) Steps 0(0.00) | Grad Norm 7.9184(9.6828) | Total Time 0.00(0.00)\n",
      "Iter 3080 | Time 15.7658(15.4560) | Bit/dim 3.7626(3.7621) | Xent 0.8587(0.8916) | Loss 8.9141(9.0409) | Error 0.3067(0.3175) Steps 0(0.00) | Grad Norm 9.7665(9.4224) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0056 | Time 79.3665, Epoch Time 942.0198(893.1079), Bit/dim 3.7567(best: 3.7660), Xent 0.8451, Loss 4.1793, Error 0.3016(best: 0.3131)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3090 | Time 16.6926(15.7517) | Bit/dim 3.7606(3.7628) | Xent 0.9398(0.8890) | Loss 9.0342(9.4823) | Error 0.3433(0.3165) Steps 0(0.00) | Grad Norm 14.0163(9.6182) | Total Time 0.00(0.00)\n",
      "Iter 3100 | Time 14.7001(15.6628) | Bit/dim 3.7321(3.7613) | Xent 0.8998(0.8910) | Loss 8.7739(9.3242) | Error 0.3122(0.3170) Steps 0(0.00) | Grad Norm 12.6244(10.1185) | Total Time 0.00(0.00)\n",
      "Iter 3110 | Time 15.2785(15.6167) | Bit/dim 3.7908(3.7600) | Xent 0.8799(0.8862) | Loss 8.9856(9.2117) | Error 0.3300(0.3163) Steps 0(0.00) | Grad Norm 4.1220(10.1450) | Total Time 0.00(0.00)\n",
      "Iter 3120 | Time 13.9658(15.5668) | Bit/dim 3.7313(3.7580) | Xent 0.8912(0.8792) | Loss 8.7363(9.1138) | Error 0.2989(0.3122) Steps 0(0.00) | Grad Norm 6.5001(9.2081) | Total Time 0.00(0.00)\n",
      "Iter 3130 | Time 16.0653(15.4810) | Bit/dim 3.7567(3.7564) | Xent 0.8216(0.8813) | Loss 8.8494(9.0431) | Error 0.2800(0.3121) Steps 0(0.00) | Grad Norm 6.2467(9.1029) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0057 | Time 78.1648, Epoch Time 953.3767(894.9159), Bit/dim 3.7605(best: 3.7567), Xent 0.8687, Loss 4.1949, Error 0.3097(best: 0.3016)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3140 | Time 15.1664(15.4046) | Bit/dim 3.7428(3.7541) | Xent 0.8648(0.8765) | Loss 8.9964(9.4878) | Error 0.3011(0.3101) Steps 0(0.00) | Grad Norm 6.4277(9.1640) | Total Time 0.00(0.00)\n",
      "Iter 3150 | Time 16.9661(15.5284) | Bit/dim 3.7429(3.7515) | Xent 0.8244(0.8760) | Loss 8.7408(9.3222) | Error 0.2989(0.3108) Steps 0(0.00) | Grad Norm 8.5672(9.0799) | Total Time 0.00(0.00)\n",
      "Iter 3160 | Time 15.6643(15.6265) | Bit/dim 3.7870(3.7540) | Xent 0.8886(0.8729) | Loss 9.0646(9.2141) | Error 0.3111(0.3101) Steps 0(0.00) | Grad Norm 9.7771(8.8234) | Total Time 0.00(0.00)\n",
      "Iter 3170 | Time 14.8570(15.4753) | Bit/dim 3.7374(3.7529) | Xent 0.8734(0.8675) | Loss 8.8258(9.1256) | Error 0.3089(0.3076) Steps 0(0.00) | Grad Norm 6.7194(8.7023) | Total Time 0.00(0.00)\n",
      "Iter 3180 | Time 14.4678(15.4644) | Bit/dim 3.7530(3.7531) | Xent 0.8918(0.8661) | Loss 9.0036(9.0644) | Error 0.3211(0.3084) Steps 0(0.00) | Grad Norm 8.5227(8.8227) | Total Time 0.00(0.00)\n",
      "Iter 3190 | Time 14.5710(15.3710) | Bit/dim 3.7685(3.7527) | Xent 0.8690(0.8609) | Loss 9.0199(9.0064) | Error 0.3078(0.3073) Steps 0(0.00) | Grad Norm 5.1775(8.3035) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0058 | Time 79.8007, Epoch Time 944.5552(896.4051), Bit/dim 3.7501(best: 3.7567), Xent 0.8478, Loss 4.1740, Error 0.3025(best: 0.3016)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3200 | Time 16.0684(15.4683) | Bit/dim 3.7497(3.7523) | Xent 0.8148(0.8539) | Loss 8.7221(9.4286) | Error 0.3033(0.3048) Steps 0(0.00) | Grad Norm 8.8882(8.0685) | Total Time 0.00(0.00)\n",
      "Iter 3210 | Time 16.1101(15.4198) | Bit/dim 3.7365(3.7484) | Xent 0.8187(0.8533) | Loss 8.8531(9.2641) | Error 0.2844(0.3038) Steps 0(0.00) | Grad Norm 7.9095(8.3544) | Total Time 0.00(0.00)\n",
      "Iter 3220 | Time 16.4020(15.4656) | Bit/dim 3.7608(3.7494) | Xent 0.8595(0.8553) | Loss 8.8484(9.1667) | Error 0.2989(0.3032) Steps 0(0.00) | Grad Norm 12.1260(8.8444) | Total Time 0.00(0.00)\n",
      "Iter 3230 | Time 16.9975(15.6746) | Bit/dim 3.7625(3.7518) | Xent 0.8799(0.8630) | Loss 9.0271(9.0896) | Error 0.3122(0.3061) Steps 0(0.00) | Grad Norm 9.2094(9.0122) | Total Time 0.00(0.00)\n",
      "Iter 3240 | Time 16.5985(15.7267) | Bit/dim 3.7543(3.7542) | Xent 0.9233(0.8660) | Loss 8.9340(9.0371) | Error 0.3189(0.3069) Steps 0(0.00) | Grad Norm 10.0823(8.8681) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0059 | Time 79.0939, Epoch Time 956.3998(898.2050), Bit/dim 3.7557(best: 3.7501), Xent 0.8514, Loss 4.1814, Error 0.3011(best: 0.3016)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3250 | Time 14.0565(15.4705) | Bit/dim 3.7800(3.7547) | Xent 0.8627(0.8634) | Loss 8.8146(9.4985) | Error 0.3056(0.3062) Steps 0(0.00) | Grad Norm 11.3954(9.3571) | Total Time 0.00(0.00)\n",
      "Iter 3260 | Time 13.8864(15.3079) | Bit/dim 3.7237(3.7527) | Xent 0.8247(0.8657) | Loss 8.5507(9.3198) | Error 0.2856(0.3056) Steps 0(0.00) | Grad Norm 7.5279(9.4972) | Total Time 0.00(0.00)\n",
      "Iter 3270 | Time 17.7987(15.5104) | Bit/dim 3.7383(3.7497) | Xent 0.8511(0.8568) | Loss 8.8652(9.1984) | Error 0.2933(0.3024) Steps 0(0.00) | Grad Norm 9.4433(9.1065) | Total Time 0.00(0.00)\n",
      "Iter 3280 | Time 18.1795(15.6772) | Bit/dim 3.7686(3.7523) | Xent 0.7778(0.8452) | Loss 8.7869(9.1032) | Error 0.2756(0.2983) Steps 0(0.00) | Grad Norm 8.2649(8.6117) | Total Time 0.00(0.00)\n",
      "Iter 3290 | Time 15.5286(15.7023) | Bit/dim 3.7220(3.7470) | Xent 0.8672(0.8609) | Loss 8.7820(9.0381) | Error 0.3089(0.3048) Steps 0(0.00) | Grad Norm 8.2703(9.6249) | Total Time 0.00(0.00)\n",
      "Iter 3300 | Time 14.8530(15.6307) | Bit/dim 3.7848(3.7504) | Xent 0.8596(0.8629) | Loss 8.9629(8.9851) | Error 0.3256(0.3070) Steps 0(0.00) | Grad Norm 7.3671(9.2004) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0060 | Time 78.2196, Epoch Time 950.8039(899.7829), Bit/dim 3.7460(best: 3.7501), Xent 0.8421, Loss 4.1670, Error 0.2981(best: 0.3011)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3310 | Time 13.8636(15.6063) | Bit/dim 3.7176(3.7484) | Xent 0.8613(0.8594) | Loss 8.6475(9.4026) | Error 0.2944(0.3057) Steps 0(0.00) | Grad Norm 5.6464(9.0664) | Total Time 0.00(0.00)\n",
      "Iter 3320 | Time 14.8579(15.5797) | Bit/dim 3.7498(3.7451) | Xent 0.8472(0.8508) | Loss 8.7745(9.2447) | Error 0.2922(0.3037) Steps 0(0.00) | Grad Norm 12.6058(8.6790) | Total Time 0.00(0.00)\n",
      "Iter 3330 | Time 16.2955(15.5701) | Bit/dim 3.7272(3.7420) | Xent 0.7718(0.8472) | Loss 8.9170(9.1274) | Error 0.2667(0.3026) Steps 0(0.00) | Grad Norm 6.6424(8.6020) | Total Time 0.00(0.00)\n",
      "Iter 3340 | Time 15.7270(15.6048) | Bit/dim 3.7624(3.7442) | Xent 0.7939(0.8491) | Loss 8.8117(9.0520) | Error 0.2733(0.3039) Steps 0(0.00) | Grad Norm 7.7947(8.6165) | Total Time 0.00(0.00)\n",
      "Iter 3350 | Time 15.5245(15.5563) | Bit/dim 3.7175(3.7470) | Xent 0.8429(0.8532) | Loss 8.7890(8.9904) | Error 0.3111(0.3057) Steps 0(0.00) | Grad Norm 4.9357(8.9586) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0061 | Time 80.4478, Epoch Time 947.4789(901.2138), Bit/dim 3.7442(best: 3.7460), Xent 0.8270, Loss 4.1577, Error 0.2941(best: 0.2981)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3360 | Time 15.4695(15.3630) | Bit/dim 3.7566(3.7457) | Xent 0.8145(0.8444) | Loss 8.8381(9.4653) | Error 0.2867(0.2994) Steps 0(0.00) | Grad Norm 12.5583(8.9999) | Total Time 0.00(0.00)\n",
      "Iter 3370 | Time 15.8452(15.4083) | Bit/dim 3.7482(3.7436) | Xent 0.7757(0.8332) | Loss 8.7131(9.2819) | Error 0.2878(0.2957) Steps 0(0.00) | Grad Norm 5.7058(8.6567) | Total Time 0.00(0.00)\n",
      "Iter 3380 | Time 16.4794(15.4021) | Bit/dim 3.7126(3.7389) | Xent 0.8499(0.8334) | Loss 8.5675(9.1598) | Error 0.3189(0.2970) Steps 0(0.00) | Grad Norm 6.6044(8.8967) | Total Time 0.00(0.00)\n",
      "Iter 3390 | Time 15.5174(15.5067) | Bit/dim 3.7514(3.7403) | Xent 0.8166(0.8355) | Loss 8.8439(9.0706) | Error 0.2978(0.2995) Steps 0(0.00) | Grad Norm 4.7358(9.0590) | Total Time 0.00(0.00)\n",
      "Iter 3400 | Time 15.5058(15.5176) | Bit/dim 3.7381(3.7412) | Xent 0.7762(0.8304) | Loss 8.6943(8.9778) | Error 0.2800(0.2982) Steps 0(0.00) | Grad Norm 5.7174(8.6397) | Total Time 0.00(0.00)\n",
      "Iter 3410 | Time 14.8238(15.5395) | Bit/dim 3.7415(3.7422) | Xent 0.8416(0.8263) | Loss 8.8205(8.9278) | Error 0.2978(0.2960) Steps 0(0.00) | Grad Norm 10.5096(8.3427) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0062 | Time 81.2862, Epoch Time 949.6598(902.6672), Bit/dim 3.7442(best: 3.7442), Xent 0.8450, Loss 4.1667, Error 0.2977(best: 0.2941)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3420 | Time 15.3044(15.5531) | Bit/dim 3.7446(3.7400) | Xent 0.8227(0.8283) | Loss 8.9251(9.3826) | Error 0.2978(0.2969) Steps 0(0.00) | Grad Norm 9.1047(8.4681) | Total Time 0.00(0.00)\n",
      "Iter 3430 | Time 15.4279(15.4662) | Bit/dim 3.7048(3.7405) | Xent 0.8116(0.8218) | Loss 8.6796(9.2253) | Error 0.2878(0.2954) Steps 0(0.00) | Grad Norm 11.0701(8.2448) | Total Time 0.00(0.00)\n",
      "Iter 3440 | Time 15.5739(15.5406) | Bit/dim 3.7489(3.7384) | Xent 0.8437(0.8214) | Loss 8.7014(9.0886) | Error 0.3111(0.2949) Steps 0(0.00) | Grad Norm 10.6119(8.1639) | Total Time 0.00(0.00)\n",
      "Iter 3450 | Time 16.4942(15.4866) | Bit/dim 3.7301(3.7371) | Xent 0.8011(0.8171) | Loss 8.6902(8.9912) | Error 0.2822(0.2930) Steps 0(0.00) | Grad Norm 7.1335(8.2846) | Total Time 0.00(0.00)\n",
      "Iter 3460 | Time 15.6164(15.5984) | Bit/dim 3.7701(3.7389) | Xent 0.7868(0.8265) | Loss 8.8893(8.9523) | Error 0.2767(0.2958) Steps 0(0.00) | Grad Norm 7.7719(8.8062) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0063 | Time 81.6295, Epoch Time 955.3295(904.2471), Bit/dim 3.7357(best: 3.7442), Xent 0.8452, Loss 4.1583, Error 0.2977(best: 0.2941)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3470 | Time 17.0900(15.6424) | Bit/dim 3.7295(3.7383) | Xent 0.8535(0.8209) | Loss 8.8629(9.4416) | Error 0.3311(0.2964) Steps 0(0.00) | Grad Norm 10.6288(8.8369) | Total Time 0.00(0.00)\n",
      "Iter 3480 | Time 14.8277(15.7639) | Bit/dim 3.6802(3.7384) | Xent 0.8274(0.8223) | Loss 8.6297(9.2863) | Error 0.2867(0.2963) Steps 0(0.00) | Grad Norm 5.0374(8.6087) | Total Time 0.00(0.00)\n",
      "Iter 3490 | Time 15.2972(15.5751) | Bit/dim 3.7454(3.7354) | Xent 0.8223(0.8239) | Loss 8.7613(9.1381) | Error 0.2856(0.2959) Steps 0(0.00) | Grad Norm 8.5867(9.6199) | Total Time 0.00(0.00)\n",
      "Iter 3500 | Time 15.8951(15.6125) | Bit/dim 3.7210(3.7363) | Xent 0.8219(0.8262) | Loss 8.7591(9.0532) | Error 0.2822(0.2952) Steps 0(0.00) | Grad Norm 10.0460(9.4445) | Total Time 0.00(0.00)\n",
      "Iter 3510 | Time 16.3442(15.6580) | Bit/dim 3.7771(3.7390) | Xent 0.7792(0.8305) | Loss 8.6011(9.0004) | Error 0.2722(0.2961) Steps 0(0.00) | Grad Norm 11.8905(9.9973) | Total Time 0.00(0.00)\n",
      "Iter 3520 | Time 15.3631(15.4718) | Bit/dim 3.7396(3.7413) | Xent 0.8143(0.8259) | Loss 8.7845(8.9664) | Error 0.2789(0.2934) Steps 0(0.00) | Grad Norm 6.2371(9.8488) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0064 | Time 79.5721, Epoch Time 949.1481(905.5941), Bit/dim 3.7428(best: 3.7357), Xent 0.8249, Loss 4.1553, Error 0.2892(best: 0.2941)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3530 | Time 14.5768(15.4640) | Bit/dim 3.7637(3.7384) | Xent 0.8164(0.8214) | Loss 8.8437(9.4010) | Error 0.2922(0.2916) Steps 0(0.00) | Grad Norm 7.1358(9.3991) | Total Time 0.00(0.00)\n",
      "Iter 3540 | Time 15.0464(15.4106) | Bit/dim 3.7735(3.7418) | Xent 0.8167(0.8086) | Loss 8.6946(9.2452) | Error 0.2967(0.2869) Steps 0(0.00) | Grad Norm 7.7790(9.0405) | Total Time 0.00(0.00)\n",
      "Iter 3550 | Time 15.9602(15.4533) | Bit/dim 3.7585(3.7406) | Xent 0.7791(0.8112) | Loss 8.7321(9.1195) | Error 0.2767(0.2887) Steps 0(0.00) | Grad Norm 14.0373(9.3298) | Total Time 0.00(0.00)\n",
      "Iter 3560 | Time 16.7111(15.6012) | Bit/dim 3.7238(3.7390) | Xent 0.8727(0.8232) | Loss 8.9341(9.0497) | Error 0.3089(0.2929) Steps 0(0.00) | Grad Norm 10.0432(10.2227) | Total Time 0.00(0.00)\n",
      "Iter 3570 | Time 16.2677(15.6337) | Bit/dim 3.7290(3.7369) | Xent 0.9416(0.8339) | Loss 9.0713(8.9876) | Error 0.3289(0.2967) Steps 0(0.00) | Grad Norm 12.7500(10.2420) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0065 | Time 81.5009, Epoch Time 957.4159(907.1487), Bit/dim 3.7307(best: 3.7357), Xent 0.8545, Loss 4.1580, Error 0.3058(best: 0.2892)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3580 | Time 15.1842(15.5811) | Bit/dim 3.7258(3.7361) | Xent 0.8147(0.8305) | Loss 8.7354(9.4613) | Error 0.2733(0.2948) Steps 0(0.00) | Grad Norm 8.0971(9.9459) | Total Time 0.00(0.00)\n",
      "Iter 3590 | Time 15.8615(15.6383) | Bit/dim 3.7448(3.7358) | Xent 0.7123(0.8221) | Loss 8.7062(9.2780) | Error 0.2411(0.2918) Steps 0(0.00) | Grad Norm 6.2768(9.8445) | Total Time 0.00(0.00)\n",
      "Iter 3600 | Time 15.8800(15.8104) | Bit/dim 3.7287(3.7347) | Xent 0.7889(0.8130) | Loss 8.7847(9.1600) | Error 0.2778(0.2885) Steps 0(0.00) | Grad Norm 12.0870(9.4625) | Total Time 0.00(0.00)\n",
      "Iter 3610 | Time 15.9195(15.8806) | Bit/dim 3.7796(3.7341) | Xent 0.7740(0.8172) | Loss 8.9112(9.0840) | Error 0.2944(0.2912) Steps 0(0.00) | Grad Norm 12.1430(9.5547) | Total Time 0.00(0.00)\n",
      "Iter 3620 | Time 14.6692(15.7677) | Bit/dim 3.7407(3.7344) | Xent 0.8129(0.8137) | Loss 8.7860(9.0121) | Error 0.2978(0.2900) Steps 0(0.00) | Grad Norm 6.4554(9.2030) | Total Time 0.00(0.00)\n",
      "Iter 3630 | Time 15.2522(15.6691) | Bit/dim 3.7441(3.7349) | Xent 0.7938(0.8111) | Loss 8.7546(8.9460) | Error 0.2833(0.2892) Steps 0(0.00) | Grad Norm 8.5882(9.1951) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0066 | Time 81.8243, Epoch Time 963.0009(908.8243), Bit/dim 3.7251(best: 3.7307), Xent 0.8033, Loss 4.1268, Error 0.2827(best: 0.2892)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3640 | Time 14.6400(15.7740) | Bit/dim 3.7717(3.7349) | Xent 0.8139(0.8050) | Loss 8.7707(9.3408) | Error 0.2789(0.2861) Steps 0(0.00) | Grad Norm 10.5054(8.9111) | Total Time 0.00(0.00)\n",
      "Iter 3650 | Time 15.2914(16.0045) | Bit/dim 3.7200(3.7298) | Xent 0.8734(0.8056) | Loss 8.9343(9.1978) | Error 0.3033(0.2857) Steps 0(0.00) | Grad Norm 10.0833(9.0839) | Total Time 0.00(0.00)\n",
      "Iter 3660 | Time 15.8566(15.8863) | Bit/dim 3.7169(3.7330) | Xent 0.8149(0.7998) | Loss 8.8750(9.0761) | Error 0.2989(0.2838) Steps 0(0.00) | Grad Norm 7.4697(8.5821) | Total Time 0.00(0.00)\n",
      "Iter 3670 | Time 14.7580(15.8028) | Bit/dim 3.7020(3.7322) | Xent 0.7672(0.7948) | Loss 8.7322(8.9910) | Error 0.2911(0.2831) Steps 0(0.00) | Grad Norm 6.2026(8.5005) | Total Time 0.00(0.00)\n",
      "Iter 3680 | Time 16.0116(15.8346) | Bit/dim 3.7111(3.7261) | Xent 0.8053(0.7945) | Loss 8.8014(8.9346) | Error 0.2833(0.2838) Steps 0(0.00) | Grad Norm 6.5336(8.0608) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0067 | Time 82.4368, Epoch Time 977.2197(910.8762), Bit/dim 3.7281(best: 3.7251), Xent 0.7939, Loss 4.1251, Error 0.2807(best: 0.2827)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3690 | Time 16.5051(15.8380) | Bit/dim 3.7456(3.7276) | Xent 0.8038(0.7908) | Loss 8.8433(9.4279) | Error 0.2689(0.2824) Steps 0(0.00) | Grad Norm 7.5741(8.0071) | Total Time 0.00(0.00)\n",
      "Iter 3700 | Time 15.9728(15.8162) | Bit/dim 3.7223(3.7267) | Xent 0.7954(0.7859) | Loss 8.8717(9.2489) | Error 0.2678(0.2802) Steps 0(0.00) | Grad Norm 11.3578(8.0430) | Total Time 0.00(0.00)\n",
      "Iter 3710 | Time 15.2517(15.8884) | Bit/dim 3.7209(3.7238) | Xent 0.8042(0.7905) | Loss 8.4800(9.1131) | Error 0.2811(0.2813) Steps 0(0.00) | Grad Norm 10.4288(8.4982) | Total Time 0.00(0.00)\n",
      "Iter 3720 | Time 14.6718(15.9296) | Bit/dim 3.7539(3.7229) | Xent 0.8016(0.7911) | Loss 8.7477(9.0193) | Error 0.3044(0.2829) Steps 0(0.00) | Grad Norm 6.5143(8.0373) | Total Time 0.00(0.00)\n",
      "Iter 3730 | Time 16.5480(15.8349) | Bit/dim 3.6915(3.7212) | Xent 0.8630(0.7915) | Loss 8.7655(8.9531) | Error 0.3189(0.2827) Steps 0(0.00) | Grad Norm 9.5204(8.1090) | Total Time 0.00(0.00)\n",
      "Iter 3740 | Time 15.8607(15.7315) | Bit/dim 3.7145(3.7225) | Xent 0.7736(0.7886) | Loss 8.9155(8.9040) | Error 0.2678(0.2824) Steps 0(0.00) | Grad Norm 4.5340(7.6773) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0068 | Time 81.1381, Epoch Time 964.8191(912.4945), Bit/dim 3.7242(best: 3.7251), Xent 0.8083, Loss 4.1284, Error 0.2873(best: 0.2807)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3750 | Time 16.0635(15.6332) | Bit/dim 3.7364(3.7216) | Xent 0.7412(0.7908) | Loss 8.8821(9.3381) | Error 0.2533(0.2806) Steps 0(0.00) | Grad Norm 9.8938(8.3563) | Total Time 0.00(0.00)\n",
      "Iter 3760 | Time 16.7475(15.6946) | Bit/dim 3.7394(3.7244) | Xent 0.8200(0.8025) | Loss 9.0894(9.2018) | Error 0.2944(0.2848) Steps 0(0.00) | Grad Norm 17.0027(9.8481) | Total Time 0.00(0.00)\n",
      "Iter 3770 | Time 18.4191(15.7445) | Bit/dim 3.7654(3.7274) | Xent 0.8770(0.8043) | Loss 9.0934(9.0940) | Error 0.3089(0.2863) Steps 0(0.00) | Grad Norm 12.7222(10.1245) | Total Time 0.00(0.00)\n",
      "Iter 3780 | Time 17.2512(15.8038) | Bit/dim 3.7470(3.7310) | Xent 0.7563(0.8033) | Loss 8.7649(9.0255) | Error 0.2622(0.2869) Steps 0(0.00) | Grad Norm 5.6886(9.4551) | Total Time 0.00(0.00)\n",
      "Iter 3790 | Time 16.6294(15.8531) | Bit/dim 3.7222(3.7289) | Xent 0.8612(0.8003) | Loss 8.9678(8.9584) | Error 0.2944(0.2855) Steps 0(0.00) | Grad Norm 8.6593(9.0971) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0069 | Time 81.7438, Epoch Time 967.5406(914.1458), Bit/dim 3.7141(best: 3.7242), Xent 0.7869, Loss 4.1075, Error 0.2797(best: 0.2807)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3800 | Time 16.1255(15.8913) | Bit/dim 3.7247(3.7273) | Xent 0.7185(0.7887) | Loss 8.7474(9.4592) | Error 0.2556(0.2818) Steps 0(0.00) | Grad Norm 7.8429(8.3033) | Total Time 0.00(0.00)\n",
      "Iter 3810 | Time 15.2378(15.8057) | Bit/dim 3.7190(3.7236) | Xent 0.7275(0.7833) | Loss 8.5064(9.2501) | Error 0.2667(0.2796) Steps 0(0.00) | Grad Norm 4.4364(8.1671) | Total Time 0.00(0.00)\n",
      "Iter 3820 | Time 15.2719(15.9116) | Bit/dim 3.7361(3.7243) | Xent 0.8084(0.7843) | Loss 8.8618(9.1383) | Error 0.3056(0.2806) Steps 0(0.00) | Grad Norm 8.8393(8.1601) | Total Time 0.00(0.00)\n",
      "Iter 3830 | Time 15.4966(16.1527) | Bit/dim 3.7247(3.7248) | Xent 0.7724(0.7862) | Loss 8.6055(9.0435) | Error 0.2533(0.2808) Steps 0(0.00) | Grad Norm 7.7815(8.4246) | Total Time 0.00(0.00)\n",
      "Iter 3840 | Time 15.7448(15.9975) | Bit/dim 3.7150(3.7208) | Xent 0.8235(0.7845) | Loss 8.9531(8.9645) | Error 0.3011(0.2801) Steps 0(0.00) | Grad Norm 7.7189(8.1691) | Total Time 0.00(0.00)\n",
      "Iter 3850 | Time 15.3081(15.9694) | Bit/dim 3.7059(3.7173) | Xent 0.7551(0.7772) | Loss 8.5437(8.8806) | Error 0.2633(0.2778) Steps 0(0.00) | Grad Norm 9.5022(7.9973) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0070 | Time 80.6613, Epoch Time 977.0017(916.0315), Bit/dim 3.7178(best: 3.7141), Xent 0.7941, Loss 4.1148, Error 0.2810(best: 0.2797)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3860 | Time 15.8457(15.8806) | Bit/dim 3.7463(3.7179) | Xent 0.7488(0.7747) | Loss 8.6253(9.2989) | Error 0.2500(0.2764) Steps 0(0.00) | Grad Norm 6.3011(7.8172) | Total Time 0.00(0.00)\n",
      "Iter 3870 | Time 17.8386(15.9524) | Bit/dim 3.7241(3.7182) | Xent 0.7846(0.7648) | Loss 8.8028(9.1455) | Error 0.2778(0.2725) Steps 0(0.00) | Grad Norm 10.3479(7.8735) | Total Time 0.00(0.00)\n",
      "Iter 3880 | Time 15.7893(15.9077) | Bit/dim 3.7062(3.7148) | Xent 0.7543(0.7686) | Loss 8.7019(9.0398) | Error 0.2756(0.2737) Steps 0(0.00) | Grad Norm 9.2007(8.4339) | Total Time 0.00(0.00)\n",
      "Iter 3890 | Time 14.9858(15.8064) | Bit/dim 3.7072(3.7178) | Xent 0.8096(0.7702) | Loss 8.7059(8.9623) | Error 0.2767(0.2735) Steps 0(0.00) | Grad Norm 8.5428(8.5841) | Total Time 0.00(0.00)\n",
      "Iter 3900 | Time 14.9232(15.8906) | Bit/dim 3.7069(3.7161) | Xent 0.7973(0.7747) | Loss 8.7077(8.9150) | Error 0.2911(0.2753) Steps 0(0.00) | Grad Norm 7.8979(8.6288) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0071 | Time 82.6263, Epoch Time 968.3223(917.6002), Bit/dim 3.7201(best: 3.7141), Xent 0.7906, Loss 4.1154, Error 0.2797(best: 0.2797)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3910 | Time 17.3247(15.8211) | Bit/dim 3.7117(3.7149) | Xent 0.7969(0.7748) | Loss 8.6833(9.3873) | Error 0.2644(0.2749) Steps 0(0.00) | Grad Norm 9.1321(8.6864) | Total Time 0.00(0.00)\n",
      "Iter 3920 | Time 15.1808(15.8898) | Bit/dim 3.7404(3.7119) | Xent 0.7378(0.7654) | Loss 8.7162(9.1910) | Error 0.2611(0.2707) Steps 0(0.00) | Grad Norm 6.4300(8.1597) | Total Time 0.00(0.00)\n",
      "Iter 3930 | Time 17.1180(16.0585) | Bit/dim 3.7165(3.7127) | Xent 0.7456(0.7687) | Loss 8.7316(9.0734) | Error 0.2689(0.2719) Steps 0(0.00) | Grad Norm 7.3149(9.0857) | Total Time 0.00(0.00)\n",
      "Iter 3940 | Time 15.7211(16.1121) | Bit/dim 3.6919(3.7108) | Xent 0.8278(0.7733) | Loss 8.6479(8.9813) | Error 0.2844(0.2746) Steps 0(0.00) | Grad Norm 9.9874(9.2871) | Total Time 0.00(0.00)\n",
      "Iter 3950 | Time 15.6973(15.9733) | Bit/dim 3.7519(3.7158) | Xent 0.7602(0.7669) | Loss 8.9006(8.8987) | Error 0.2700(0.2730) Steps 0(0.00) | Grad Norm 6.7836(8.5190) | Total Time 0.00(0.00)\n",
      "Iter 3960 | Time 15.1794(15.9939) | Bit/dim 3.7369(3.7172) | Xent 0.7277(0.7644) | Loss 8.7518(8.8628) | Error 0.2600(0.2730) Steps 0(0.00) | Grad Norm 5.5892(8.1329) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0072 | Time 80.9011, Epoch Time 981.1575(919.5070), Bit/dim 3.7245(best: 3.7141), Xent 0.7635, Loss 4.1063, Error 0.2706(best: 0.2797)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3970 | Time 15.6075(15.9215) | Bit/dim 3.7178(3.7180) | Xent 0.8256(0.7687) | Loss 8.8810(9.3138) | Error 0.3122(0.2744) Steps 0(0.00) | Grad Norm 13.8873(9.1439) | Total Time 0.00(0.00)\n",
      "Iter 3980 | Time 15.3115(16.0543) | Bit/dim 3.7511(3.7214) | Xent 0.7263(0.7721) | Loss 8.4025(9.1606) | Error 0.2589(0.2762) Steps 0(0.00) | Grad Norm 5.6283(9.2658) | Total Time 0.00(0.00)\n",
      "Iter 3990 | Time 16.9237(15.9185) | Bit/dim 3.7197(3.7187) | Xent 0.7884(0.7635) | Loss 8.6973(9.0413) | Error 0.2789(0.2726) Steps 0(0.00) | Grad Norm 4.7292(8.3310) | Total Time 0.00(0.00)\n",
      "Iter 4000 | Time 17.1162(15.9515) | Bit/dim 3.7015(3.7149) | Xent 0.7790(0.7543) | Loss 8.9043(8.9408) | Error 0.2844(0.2686) Steps 0(0.00) | Grad Norm 7.6669(8.1002) | Total Time 0.00(0.00)\n",
      "Iter 4010 | Time 17.2908(15.8782) | Bit/dim 3.7289(3.7126) | Xent 0.7972(0.7435) | Loss 8.9658(8.8658) | Error 0.2633(0.2642) Steps 0(0.00) | Grad Norm 7.2927(7.5424) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0073 | Time 82.1592, Epoch Time 967.6380(920.9509), Bit/dim 3.7119(best: 3.7141), Xent 0.7749, Loss 4.0994, Error 0.2720(best: 0.2706)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4020 | Time 16.2743(15.8196) | Bit/dim 3.7111(3.7107) | Xent 0.6965(0.7411) | Loss 8.7524(9.3616) | Error 0.2389(0.2630) Steps 0(0.00) | Grad Norm 7.8006(7.5608) | Total Time 0.00(0.00)\n",
      "Iter 4030 | Time 15.8261(15.9879) | Bit/dim 3.7391(3.7102) | Xent 0.7144(0.7384) | Loss 8.9218(9.2038) | Error 0.2489(0.2624) Steps 0(0.00) | Grad Norm 6.7271(7.4906) | Total Time 0.00(0.00)\n",
      "Iter 4040 | Time 16.2300(16.0648) | Bit/dim 3.7140(3.7077) | Xent 0.7467(0.7423) | Loss 8.7562(9.0730) | Error 0.2600(0.2645) Steps 0(0.00) | Grad Norm 7.2030(8.1459) | Total Time 0.00(0.00)\n",
      "Iter 4050 | Time 16.5344(16.0477) | Bit/dim 3.6804(3.7087) | Xent 0.7640(0.7479) | Loss 8.6544(8.9747) | Error 0.2589(0.2665) Steps 0(0.00) | Grad Norm 9.8718(8.4170) | Total Time 0.00(0.00)\n",
      "Iter 4060 | Time 15.9952(16.2868) | Bit/dim 3.7087(3.7077) | Xent 0.7268(0.7512) | Loss 8.6931(8.9218) | Error 0.2644(0.2669) Steps 0(0.00) | Grad Norm 9.5462(8.7335) | Total Time 0.00(0.00)\n",
      "Iter 4070 | Time 16.4428(16.3521) | Bit/dim 3.7077(3.7085) | Xent 0.7086(0.7433) | Loss 8.8267(8.8639) | Error 0.2478(0.2647) Steps 0(0.00) | Grad Norm 7.4429(8.1967) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0074 | Time 83.9619, Epoch Time 1003.2384(923.4195), Bit/dim 3.7115(best: 3.7119), Xent 0.7733, Loss 4.0981, Error 0.2740(best: 0.2706)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4080 | Time 16.5998(16.3485) | Bit/dim 3.6847(3.7072) | Xent 0.6831(0.7405) | Loss 8.4207(9.3087) | Error 0.2489(0.2640) Steps 0(0.00) | Grad Norm 5.0180(8.2318) | Total Time 0.00(0.00)\n",
      "Iter 4090 | Time 17.2520(16.2823) | Bit/dim 3.6793(3.7066) | Xent 0.6925(0.7316) | Loss 8.6733(9.1368) | Error 0.2511(0.2611) Steps 0(0.00) | Grad Norm 8.1400(7.9459) | Total Time 0.00(0.00)\n",
      "Iter 4100 | Time 16.1622(16.3062) | Bit/dim 3.7288(3.7087) | Xent 0.7422(0.7353) | Loss 8.6421(9.0306) | Error 0.2722(0.2632) Steps 0(0.00) | Grad Norm 14.0558(8.7634) | Total Time 0.00(0.00)\n",
      "Iter 4110 | Time 15.5525(16.3940) | Bit/dim 3.7166(3.7085) | Xent 0.7667(0.7413) | Loss 8.7138(8.9511) | Error 0.2756(0.2652) Steps 0(0.00) | Grad Norm 7.0106(8.6982) | Total Time 0.00(0.00)\n",
      "Iter 4120 | Time 16.3534(16.1912) | Bit/dim 3.6732(3.7079) | Xent 0.7396(0.7476) | Loss 8.4955(8.8799) | Error 0.2633(0.2672) Steps 0(0.00) | Grad Norm 6.8093(8.6611) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0075 | Time 83.2646, Epoch Time 992.0750(925.4792), Bit/dim 3.7084(best: 3.7115), Xent 0.7895, Loss 4.1031, Error 0.2755(best: 0.2706)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4130 | Time 15.3327(16.2119) | Bit/dim 3.7338(3.7106) | Xent 0.7952(0.7582) | Loss 8.9373(9.4317) | Error 0.2678(0.2714) Steps 0(0.00) | Grad Norm 11.3661(9.2920) | Total Time 0.00(0.00)\n",
      "Iter 4140 | Time 18.4543(16.3526) | Bit/dim 3.6902(3.7118) | Xent 0.7642(0.7547) | Loss 8.7830(9.2296) | Error 0.2700(0.2709) Steps 0(0.00) | Grad Norm 8.0072(9.2546) | Total Time 0.00(0.00)\n",
      "Iter 4150 | Time 15.3862(16.1734) | Bit/dim 3.7329(3.7131) | Xent 0.7011(0.7509) | Loss 8.6901(9.1026) | Error 0.2678(0.2698) Steps 0(0.00) | Grad Norm 9.6781(9.0717) | Total Time 0.00(0.00)\n",
      "Iter 4160 | Time 15.5108(16.3336) | Bit/dim 3.7118(3.7104) | Xent 0.8210(0.7487) | Loss 8.8310(9.0052) | Error 0.3000(0.2673) Steps 0(0.00) | Grad Norm 6.1847(8.4839) | Total Time 0.00(0.00)\n",
      "Iter 4170 | Time 15.4557(16.2143) | Bit/dim 3.7181(3.7095) | Xent 0.8312(0.7424) | Loss 8.8555(8.9277) | Error 0.2989(0.2649) Steps 0(0.00) | Grad Norm 15.7911(8.4533) | Total Time 0.00(0.00)\n",
      "Iter 4180 | Time 15.7972(16.1817) | Bit/dim 3.6864(3.7041) | Xent 0.6793(0.7407) | Loss 8.3850(8.8539) | Error 0.2333(0.2641) Steps 0(0.00) | Grad Norm 7.8381(8.3766) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0076 | Time 83.1987, Epoch Time 990.2827(927.4233), Bit/dim 3.7077(best: 3.7084), Xent 0.7688, Loss 4.0921, Error 0.2711(best: 0.2706)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4190 | Time 14.8741(16.1943) | Bit/dim 3.6965(3.7026) | Xent 0.7971(0.7406) | Loss 8.5061(9.2671) | Error 0.2833(0.2632) Steps 0(0.00) | Grad Norm 13.8974(8.9401) | Total Time 0.00(0.00)\n",
      "Iter 4200 | Time 16.3154(16.0376) | Bit/dim 3.6868(3.7028) | Xent 0.6889(0.7335) | Loss 8.6888(9.1148) | Error 0.2511(0.2621) Steps 0(0.00) | Grad Norm 7.1429(8.5535) | Total Time 0.00(0.00)\n",
      "Iter 4210 | Time 17.1754(16.1329) | Bit/dim 3.6992(3.7028) | Xent 0.7321(0.7418) | Loss 8.8188(9.0307) | Error 0.2744(0.2658) Steps 0(0.00) | Grad Norm 8.6008(8.8894) | Total Time 0.00(0.00)\n",
      "Iter 4220 | Time 15.5904(15.9992) | Bit/dim 3.6931(3.7029) | Xent 0.7348(0.7450) | Loss 8.6235(8.9393) | Error 0.2589(0.2668) Steps 0(0.00) | Grad Norm 14.8577(9.5323) | Total Time 0.00(0.00)\n",
      "Iter 4230 | Time 15.4547(15.9636) | Bit/dim 3.6981(3.7061) | Xent 0.7318(0.7388) | Loss 8.7717(8.8771) | Error 0.2600(0.2649) Steps 0(0.00) | Grad Norm 7.1981(9.3459) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0077 | Time 85.1356, Epoch Time 982.5779(929.0779), Bit/dim 3.7038(best: 3.7077), Xent 0.7913, Loss 4.0995, Error 0.2781(best: 0.2706)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4240 | Time 16.0731(16.2169) | Bit/dim 3.7122(3.7007) | Xent 0.7666(0.7373) | Loss 8.7018(9.3733) | Error 0.2733(0.2627) Steps 0(0.00) | Grad Norm 8.5502(9.2500) | Total Time 0.00(0.00)\n",
      "Iter 4250 | Time 17.5983(16.2175) | Bit/dim 3.7262(3.7039) | Xent 0.6681(0.7243) | Loss 8.7198(9.1870) | Error 0.2389(0.2597) Steps 0(0.00) | Grad Norm 6.5544(8.7239) | Total Time 0.00(0.00)\n",
      "Iter 4260 | Time 16.2813(16.1331) | Bit/dim 3.6862(3.7032) | Xent 0.7246(0.7201) | Loss 8.6545(9.0425) | Error 0.2644(0.2578) Steps 0(0.00) | Grad Norm 6.3165(8.1281) | Total Time 0.00(0.00)\n",
      "Iter 4270 | Time 15.0313(16.1956) | Bit/dim 3.7112(3.7034) | Xent 0.7543(0.7200) | Loss 8.6783(8.9407) | Error 0.2789(0.2581) Steps 0(0.00) | Grad Norm 9.1397(8.0925) | Total Time 0.00(0.00)\n",
      "Iter 4280 | Time 15.2516(16.1799) | Bit/dim 3.6874(3.7023) | Xent 0.7913(0.7188) | Loss 8.7148(8.8777) | Error 0.2956(0.2573) Steps 0(0.00) | Grad Norm 4.5624(7.6915) | Total Time 0.00(0.00)\n",
      "Iter 4290 | Time 15.2890(16.2816) | Bit/dim 3.6959(3.7008) | Xent 0.7210(0.7149) | Loss 8.5775(8.8202) | Error 0.2622(0.2564) Steps 0(0.00) | Grad Norm 5.4557(7.4476) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0078 | Time 85.7555, Epoch Time 999.2092(931.1819), Bit/dim 3.6971(best: 3.7038), Xent 0.7624, Loss 4.0783, Error 0.2690(best: 0.2706)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4300 | Time 15.2045(16.2859) | Bit/dim 3.7066(3.6985) | Xent 0.6562(0.7096) | Loss 8.3018(9.2783) | Error 0.2378(0.2550) Steps 0(0.00) | Grad Norm 8.4854(7.8618) | Total Time 0.00(0.00)\n",
      "Iter 4310 | Time 16.3664(16.4743) | Bit/dim 3.6726(3.6982) | Xent 0.6813(0.7095) | Loss 8.6627(9.1406) | Error 0.2544(0.2551) Steps 0(0.00) | Grad Norm 8.9075(8.0207) | Total Time 0.00(0.00)\n",
      "Iter 4320 | Time 14.8724(16.3494) | Bit/dim 3.7037(3.7006) | Xent 0.7461(0.7124) | Loss 8.6863(9.0063) | Error 0.2433(0.2539) Steps 0(0.00) | Grad Norm 5.8310(7.8561) | Total Time 0.00(0.00)\n",
      "Iter 4330 | Time 16.5264(16.4228) | Bit/dim 3.6827(3.6998) | Xent 0.7912(0.7152) | Loss 8.7081(8.9106) | Error 0.2789(0.2560) Steps 0(0.00) | Grad Norm 13.6643(8.1985) | Total Time 0.00(0.00)\n",
      "Iter 4340 | Time 19.3369(16.5215) | Bit/dim 3.6612(3.6997) | Xent 0.7079(0.7226) | Loss 8.6743(8.8684) | Error 0.2544(0.2581) Steps 0(0.00) | Grad Norm 4.4514(7.9673) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0079 | Time 85.4658, Epoch Time 1009.5672(933.5334), Bit/dim 3.6968(best: 3.6971), Xent 0.7380, Loss 4.0658, Error 0.2578(best: 0.2690)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4350 | Time 16.6870(16.3910) | Bit/dim 3.6763(3.6964) | Xent 0.6513(0.7170) | Loss 8.4713(9.3922) | Error 0.2289(0.2569) Steps 0(0.00) | Grad Norm 4.2851(7.5249) | Total Time 0.00(0.00)\n",
      "Iter 4360 | Time 16.7948(16.3628) | Bit/dim 3.6845(3.6961) | Xent 0.6995(0.7095) | Loss 8.8677(9.2073) | Error 0.2500(0.2541) Steps 0(0.00) | Grad Norm 5.7947(7.2061) | Total Time 0.00(0.00)\n",
      "Iter 4370 | Time 15.6515(16.2875) | Bit/dim 3.7107(3.6966) | Xent 0.7222(0.7078) | Loss 8.5702(9.0611) | Error 0.2533(0.2521) Steps 0(0.00) | Grad Norm 9.9722(7.9143) | Total Time 0.00(0.00)\n",
      "Iter 4380 | Time 15.9104(16.2497) | Bit/dim 3.6922(3.6932) | Xent 0.7190(0.7051) | Loss 8.4329(8.9410) | Error 0.2600(0.2521) Steps 0(0.00) | Grad Norm 6.0986(7.7229) | Total Time 0.00(0.00)\n",
      "Iter 4390 | Time 15.5609(16.2490) | Bit/dim 3.7412(3.6941) | Xent 0.7510(0.7083) | Loss 8.7989(8.8825) | Error 0.2878(0.2546) Steps 0(0.00) | Grad Norm 11.4797(8.0978) | Total Time 0.00(0.00)\n",
      "Iter 4400 | Time 15.7205(16.2935) | Bit/dim 3.6619(3.6965) | Xent 0.7566(0.7117) | Loss 8.6337(8.8389) | Error 0.2678(0.2563) Steps 0(0.00) | Grad Norm 5.5509(7.8517) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0080 | Time 83.9824, Epoch Time 991.2443(935.2647), Bit/dim 3.6998(best: 3.6968), Xent 0.7437, Loss 4.0716, Error 0.2656(best: 0.2578)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4410 | Time 16.4325(16.5416) | Bit/dim 3.7072(3.6931) | Xent 0.6646(0.7080) | Loss 8.5581(9.2989) | Error 0.2311(0.2541) Steps 0(0.00) | Grad Norm 7.3274(7.8888) | Total Time 0.00(0.00)\n",
      "Iter 4420 | Time 17.4294(16.5785) | Bit/dim 3.6892(3.6953) | Xent 0.6247(0.7009) | Loss 8.6779(9.1272) | Error 0.2300(0.2512) Steps 0(0.00) | Grad Norm 6.0903(7.6004) | Total Time 0.00(0.00)\n",
      "Iter 4430 | Time 16.1940(16.6282) | Bit/dim 3.7099(3.6965) | Xent 0.7278(0.7023) | Loss 8.8895(9.0224) | Error 0.2600(0.2512) Steps 0(0.00) | Grad Norm 13.4734(7.8963) | Total Time 0.00(0.00)\n",
      "Iter 4440 | Time 16.3930(16.5330) | Bit/dim 3.6641(3.6939) | Xent 0.7257(0.7036) | Loss 8.8004(8.9341) | Error 0.2633(0.2515) Steps 0(0.00) | Grad Norm 8.4871(7.9787) | Total Time 0.00(0.00)\n",
      "Iter 4450 | Time 16.0763(16.5339) | Bit/dim 3.6865(3.6924) | Xent 0.6542(0.7004) | Loss 8.7624(8.8445) | Error 0.2456(0.2516) Steps 0(0.00) | Grad Norm 6.4811(7.9091) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0081 | Time 86.3891, Epoch Time 1021.5336(937.8528), Bit/dim 3.6948(best: 3.6968), Xent 0.7989, Loss 4.0942, Error 0.2806(best: 0.2578)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4460 | Time 16.3284(16.5475) | Bit/dim 3.6989(3.6909) | Xent 0.7197(0.6976) | Loss 8.9442(9.3649) | Error 0.2567(0.2499) Steps 0(0.00) | Grad Norm 7.5220(7.6043) | Total Time 0.00(0.00)\n",
      "Iter 4470 | Time 15.9210(16.7063) | Bit/dim 3.6634(3.6878) | Xent 0.6855(0.6916) | Loss 8.6448(9.1880) | Error 0.2322(0.2462) Steps 0(0.00) | Grad Norm 8.2303(7.2365) | Total Time 0.00(0.00)\n",
      "Iter 4480 | Time 18.1333(16.7221) | Bit/dim 3.6749(3.6880) | Xent 0.8249(0.6980) | Loss 8.7517(9.0725) | Error 0.2833(0.2488) Steps 0(0.00) | Grad Norm 8.6153(7.5914) | Total Time 0.00(0.00)\n",
      "Iter 4490 | Time 18.0242(16.6949) | Bit/dim 3.7005(3.6901) | Xent 0.7208(0.6999) | Loss 8.8092(8.9695) | Error 0.2622(0.2490) Steps 0(0.00) | Grad Norm 14.2627(8.2020) | Total Time 0.00(0.00)\n",
      "Iter 4500 | Time 16.6862(16.6002) | Bit/dim 3.6991(3.6910) | Xent 0.6818(0.7055) | Loss 8.8263(8.8933) | Error 0.2367(0.2501) Steps 0(0.00) | Grad Norm 7.0075(8.3884) | Total Time 0.00(0.00)\n",
      "Iter 4510 | Time 15.8925(16.5981) | Bit/dim 3.6949(3.6928) | Xent 0.6677(0.7034) | Loss 8.6194(8.8206) | Error 0.2333(0.2495) Steps 0(0.00) | Grad Norm 14.4854(8.2674) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0082 | Time 87.3414, Epoch Time 1019.8459(940.3126), Bit/dim 3.6936(best: 3.6948), Xent 0.7825, Loss 4.0848, Error 0.2754(best: 0.2578)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4520 | Time 15.8837(16.5745) | Bit/dim 3.6551(3.6908) | Xent 0.7418(0.7047) | Loss 8.6185(9.2876) | Error 0.2522(0.2494) Steps 0(0.00) | Grad Norm 7.8273(8.4460) | Total Time 0.00(0.00)\n",
      "Iter 4530 | Time 17.5201(16.5434) | Bit/dim 3.6843(3.6908) | Xent 0.7203(0.6994) | Loss 8.6267(9.1229) | Error 0.2411(0.2469) Steps 0(0.00) | Grad Norm 5.7970(7.8685) | Total Time 0.00(0.00)\n",
      "Iter 4540 | Time 15.2402(16.5506) | Bit/dim 3.7283(3.6897) | Xent 0.7423(0.6991) | Loss 8.7114(9.0112) | Error 0.2733(0.2491) Steps 0(0.00) | Grad Norm 7.8862(7.9610) | Total Time 0.00(0.00)\n",
      "Iter 4550 | Time 15.1078(16.5925) | Bit/dim 3.6789(3.6910) | Xent 0.7254(0.6964) | Loss 8.5777(8.9278) | Error 0.2478(0.2474) Steps 0(0.00) | Grad Norm 7.2791(7.8372) | Total Time 0.00(0.00)\n",
      "Iter 4560 | Time 16.4075(16.6479) | Bit/dim 3.6773(3.6928) | Xent 0.6520(0.6914) | Loss 8.3487(8.8518) | Error 0.2367(0.2460) Steps 0(0.00) | Grad Norm 5.6994(7.6087) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0083 | Time 88.7658, Epoch Time 1020.1888(942.7089), Bit/dim 3.6956(best: 3.6936), Xent 0.8061, Loss 4.0987, Error 0.2793(best: 0.2578)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4570 | Time 16.9027(16.6825) | Bit/dim 3.6675(3.6942) | Xent 0.6541(0.6890) | Loss 8.6719(9.3959) | Error 0.2300(0.2450) Steps 0(0.00) | Grad Norm 9.5990(7.7266) | Total Time 0.00(0.00)\n",
      "Iter 4580 | Time 15.9716(16.6371) | Bit/dim 3.7288(3.6935) | Xent 0.6833(0.6850) | Loss 8.4224(9.1964) | Error 0.2556(0.2440) Steps 0(0.00) | Grad Norm 6.2972(7.5764) | Total Time 0.00(0.00)\n",
      "Iter 4590 | Time 18.2919(16.7421) | Bit/dim 3.6930(3.6886) | Xent 0.6585(0.6885) | Loss 8.7192(9.0570) | Error 0.2444(0.2456) Steps 0(0.00) | Grad Norm 7.2261(8.0982) | Total Time 0.00(0.00)\n",
      "Iter 4600 | Time 17.9924(16.8572) | Bit/dim 3.6733(3.6885) | Xent 0.6256(0.6820) | Loss 8.5104(8.9440) | Error 0.2222(0.2438) Steps 0(0.00) | Grad Norm 3.8811(7.5585) | Total Time 0.00(0.00)\n",
      "Iter 4610 | Time 15.5222(16.8549) | Bit/dim 3.6638(3.6849) | Xent 0.7411(0.6851) | Loss 8.7684(8.8762) | Error 0.2789(0.2445) Steps 0(0.00) | Grad Norm 16.0315(7.8159) | Total Time 0.00(0.00)\n",
      "Iter 4620 | Time 16.3104(16.6436) | Bit/dim 3.7217(3.6868) | Xent 0.7536(0.6869) | Loss 8.8517(8.8137) | Error 0.2767(0.2455) Steps 0(0.00) | Grad Norm 9.4788(7.9384) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0084 | Time 87.3051, Epoch Time 1022.6349(945.1067), Bit/dim 3.6868(best: 3.6936), Xent 0.7605, Loss 4.0670, Error 0.2688(best: 0.2578)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4630 | Time 17.1809(16.6598) | Bit/dim 3.6913(3.6867) | Xent 0.6270(0.6842) | Loss 8.5730(9.2732) | Error 0.2322(0.2442) Steps 0(0.00) | Grad Norm 3.8346(8.6911) | Total Time 0.00(0.00)\n",
      "Iter 4640 | Time 17.6354(16.6732) | Bit/dim 3.6685(3.6850) | Xent 0.6642(0.6791) | Loss 8.6299(9.1121) | Error 0.2422(0.2422) Steps 0(0.00) | Grad Norm 11.6466(8.4860) | Total Time 0.00(0.00)\n",
      "Iter 4650 | Time 18.0994(16.7112) | Bit/dim 3.6588(3.6831) | Xent 0.6731(0.6792) | Loss 8.6727(8.9859) | Error 0.2389(0.2423) Steps 0(0.00) | Grad Norm 6.0035(8.0872) | Total Time 0.00(0.00)\n",
      "Iter 4660 | Time 18.2916(16.6725) | Bit/dim 3.7081(3.6839) | Xent 0.6702(0.6727) | Loss 9.0140(8.9016) | Error 0.2444(0.2408) Steps 0(0.00) | Grad Norm 5.4894(7.7106) | Total Time 0.00(0.00)\n",
      "Iter 4670 | Time 15.5768(16.6816) | Bit/dim 3.6670(3.6869) | Xent 0.6960(0.6780) | Loss 8.6430(8.8349) | Error 0.2456(0.2425) Steps 0(0.00) | Grad Norm 7.1258(8.1974) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0085 | Time 86.7190, Epoch Time 1020.6892(947.3741), Bit/dim 3.6909(best: 3.6868), Xent 0.7257, Loss 4.0538, Error 0.2532(best: 0.2578)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4680 | Time 16.7205(16.7198) | Bit/dim 3.7004(3.6865) | Xent 0.6435(0.6736) | Loss 8.7354(9.3579) | Error 0.2189(0.2407) Steps 0(0.00) | Grad Norm 6.6805(7.8314) | Total Time 0.00(0.00)\n",
      "Iter 4690 | Time 17.7092(16.8560) | Bit/dim 3.6737(3.6846) | Xent 0.6945(0.6754) | Loss 8.4529(9.1624) | Error 0.2489(0.2415) Steps 0(0.00) | Grad Norm 8.3790(7.8810) | Total Time 0.00(0.00)\n",
      "Iter 4700 | Time 16.1078(16.8685) | Bit/dim 3.6818(3.6855) | Xent 0.6860(0.6813) | Loss 8.7126(9.0338) | Error 0.2533(0.2430) Steps 0(0.00) | Grad Norm 9.4602(8.5633) | Total Time 0.00(0.00)\n",
      "Iter 4710 | Time 18.4719(17.0570) | Bit/dim 3.7125(3.6880) | Xent 0.7999(0.6884) | Loss 8.9398(8.9633) | Error 0.3000(0.2459) Steps 0(0.00) | Grad Norm 10.3501(8.7631) | Total Time 0.00(0.00)\n",
      "Iter 4720 | Time 15.6292(16.8923) | Bit/dim 3.6770(3.6890) | Xent 0.7065(0.6866) | Loss 8.6177(8.8714) | Error 0.2444(0.2444) Steps 0(0.00) | Grad Norm 6.5010(8.4341) | Total Time 0.00(0.00)\n",
      "Iter 4730 | Time 17.0613(17.0165) | Bit/dim 3.6725(3.6860) | Xent 0.7222(0.6825) | Loss 8.7693(8.8196) | Error 0.2589(0.2438) Steps 0(0.00) | Grad Norm 10.9372(8.3976) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0086 | Time 86.7296, Epoch Time 1043.3677(950.2539), Bit/dim 3.6865(best: 3.6868), Xent 0.7620, Loss 4.0675, Error 0.2689(best: 0.2532)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4740 | Time 19.0503(17.1315) | Bit/dim 3.7222(3.6882) | Xent 0.5831(0.6724) | Loss 8.8527(9.2728) | Error 0.2122(0.2408) Steps 0(0.00) | Grad Norm 5.2578(8.0313) | Total Time 0.00(0.00)\n",
      "Iter 4750 | Time 15.3309(17.1350) | Bit/dim 3.6610(3.6852) | Xent 0.6916(0.6686) | Loss 8.5881(9.1135) | Error 0.2400(0.2392) Steps 0(0.00) | Grad Norm 7.9828(7.6861) | Total Time 0.00(0.00)\n",
      "Iter 4760 | Time 18.4623(17.1061) | Bit/dim 3.6488(3.6833) | Xent 0.6630(0.6662) | Loss 8.3325(8.9791) | Error 0.2400(0.2376) Steps 0(0.00) | Grad Norm 9.5746(8.0477) | Total Time 0.00(0.00)\n",
      "Iter 4770 | Time 19.0925(17.1507) | Bit/dim 3.6523(3.6822) | Xent 0.6422(0.6759) | Loss 8.6788(8.8911) | Error 0.2211(0.2407) Steps 0(0.00) | Grad Norm 9.5700(8.6343) | Total Time 0.00(0.00)\n",
      "Iter 4780 | Time 15.2089(16.9761) | Bit/dim 3.7261(3.6864) | Xent 0.7643(0.6919) | Loss 8.5856(8.8472) | Error 0.2944(0.2466) Steps 0(0.00) | Grad Norm 11.6596(9.3279) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0087 | Time 85.3977, Epoch Time 1043.4841(953.0509), Bit/dim 3.6924(best: 3.6865), Xent 0.7429, Loss 4.0639, Error 0.2565(best: 0.2532)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4790 | Time 17.5530(17.1131) | Bit/dim 3.6550(3.6857) | Xent 0.7089(0.6907) | Loss 8.8273(9.3916) | Error 0.2344(0.2461) Steps 0(0.00) | Grad Norm 10.6192(9.2903) | Total Time 0.00(0.00)\n",
      "Iter 4800 | Time 17.3089(17.0840) | Bit/dim 3.7064(3.6837) | Xent 0.5868(0.6825) | Loss 8.5567(9.1904) | Error 0.1967(0.2436) Steps 0(0.00) | Grad Norm 7.0081(8.8856) | Total Time 0.00(0.00)\n",
      "Iter 4810 | Time 17.2951(16.9517) | Bit/dim 3.6383(3.6848) | Xent 0.6844(0.6786) | Loss 8.5100(9.0476) | Error 0.2556(0.2428) Steps 0(0.00) | Grad Norm 6.8989(8.5132) | Total Time 0.00(0.00)\n",
      "Iter 4820 | Time 15.3515(16.8910) | Bit/dim 3.6791(3.6850) | Xent 0.7520(0.6809) | Loss 8.4436(8.9471) | Error 0.2778(0.2429) Steps 0(0.00) | Grad Norm 12.2179(8.6756) | Total Time 0.00(0.00)\n",
      "Iter 4830 | Time 16.9520(16.7395) | Bit/dim 3.6868(3.6833) | Xent 0.6771(0.6731) | Loss 8.5815(8.8526) | Error 0.2222(0.2387) Steps 0(0.00) | Grad Norm 7.8742(8.1994) | Total Time 0.00(0.00)\n",
      "Iter 4840 | Time 17.7529(16.7793) | Bit/dim 3.7019(3.6830) | Xent 0.7089(0.6732) | Loss 8.6896(8.7864) | Error 0.2478(0.2384) Steps 0(0.00) | Grad Norm 6.7695(7.8996) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0088 | Time 86.6149, Epoch Time 1025.4278(955.2222), Bit/dim 3.6793(best: 3.6865), Xent 0.7146, Loss 4.0366, Error 0.2518(best: 0.2532)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4850 | Time 16.6957(17.0240) | Bit/dim 3.7013(3.6807) | Xent 0.6502(0.6614) | Loss 8.4231(9.2177) | Error 0.2289(0.2348) Steps 0(0.00) | Grad Norm 6.0926(7.5339) | Total Time 0.00(0.00)\n",
      "Iter 4860 | Time 16.6296(17.0249) | Bit/dim 3.6488(3.6779) | Xent 0.6109(0.6575) | Loss 8.5559(9.0615) | Error 0.2267(0.2345) Steps 0(0.00) | Grad Norm 5.6423(7.9597) | Total Time 0.00(0.00)\n",
      "Iter 4870 | Time 15.4967(17.0094) | Bit/dim 3.6708(3.6770) | Xent 0.6650(0.6564) | Loss 8.5730(8.9564) | Error 0.2244(0.2332) Steps 0(0.00) | Grad Norm 7.4818(7.7596) | Total Time 0.00(0.00)\n",
      "Iter 4880 | Time 16.0995(16.8455) | Bit/dim 3.7070(3.6769) | Xent 0.6918(0.6605) | Loss 8.6320(8.8654) | Error 0.2433(0.2359) Steps 0(0.00) | Grad Norm 10.5984(8.3173) | Total Time 0.00(0.00)\n",
      "Iter 4890 | Time 17.2509(16.9389) | Bit/dim 3.7515(3.6823) | Xent 0.6806(0.6757) | Loss 8.8241(8.8362) | Error 0.2500(0.2414) Steps 0(0.00) | Grad Norm 7.4439(8.3774) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0089 | Time 87.3396, Epoch Time 1042.2467(957.8329), Bit/dim 3.6938(best: 3.6793), Xent 0.7190, Loss 4.0533, Error 0.2503(best: 0.2518)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4900 | Time 17.5697(16.8992) | Bit/dim 3.6374(3.6839) | Xent 0.6517(0.6746) | Loss 8.5436(9.3488) | Error 0.2322(0.2411) Steps 0(0.00) | Grad Norm 6.0464(8.0947) | Total Time 0.00(0.00)\n",
      "Iter 4910 | Time 16.4533(16.9732) | Bit/dim 3.6877(3.6808) | Xent 0.6398(0.6650) | Loss 8.6107(9.1624) | Error 0.2378(0.2374) Steps 0(0.00) | Grad Norm 6.1366(7.6684) | Total Time 0.00(0.00)\n",
      "Iter 4920 | Time 16.9924(17.1338) | Bit/dim 3.6867(3.6809) | Xent 0.6015(0.6579) | Loss 8.6993(9.0250) | Error 0.2033(0.2348) Steps 0(0.00) | Grad Norm 7.2193(7.8045) | Total Time 0.00(0.00)\n",
      "Iter 4930 | Time 16.0495(16.9933) | Bit/dim 3.7127(3.6821) | Xent 0.6912(0.6605) | Loss 8.5534(8.9193) | Error 0.2411(0.2370) Steps 0(0.00) | Grad Norm 7.8194(7.9323) | Total Time 0.00(0.00)\n",
      "Iter 4940 | Time 16.1125(17.1906) | Bit/dim 3.6465(3.6803) | Xent 0.6496(0.6611) | Loss 8.4598(8.8455) | Error 0.2322(0.2371) Steps 0(0.00) | Grad Norm 13.2020(8.3438) | Total Time 0.00(0.00)\n",
      "Iter 4950 | Time 18.5127(17.3362) | Bit/dim 3.6752(3.6771) | Xent 0.6454(0.6608) | Loss 8.7665(8.7976) | Error 0.2222(0.2353) Steps 0(0.00) | Grad Norm 6.3640(7.9105) | Total Time 0.00(0.00)\n",
      "validating...\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_rl.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_15_run1 --resume ../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_15_run1/current_checkpt.pth --seed 1 --lr 0.001 --conditional True --controlled_tol False --train_mode semisup --log_freq 10 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5 --scale_fac 1.0 --scale_std 15.0\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
