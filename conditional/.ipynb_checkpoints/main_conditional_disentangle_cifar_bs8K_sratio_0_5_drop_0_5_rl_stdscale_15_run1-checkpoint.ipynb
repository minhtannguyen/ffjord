{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_rl.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "GATES = [\"cnn1\", \"cnn2\", \"rnn\"]\n",
      "\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--gate', type=str, default='cnn1', choices=GATES)\n",
      "parser.add_argument('--scale', type=float, default=1.0)\n",
      "parser.add_argument('--scale_fac', type=float, default=1.0)\n",
      "parser.add_argument('--scale_std', type=float, default=1.0)\n",
      "parser.add_argument('--eta', default=0.1, type=float,\n",
      "                        help='tuning parameter that allows us to trade-off the competing goals of' \n",
      "                                'minimizing the prediction loss and maximizing the gate rewards ')\n",
      "parser.add_argument('--rl-weight', default=0.01, type=float,\n",
      "                        help='rl weight')\n",
      "\n",
      "parser.add_argument('--gamma', default=0.99, type=float,\n",
      "                        help='discount factor, default: (0.99)')\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "import lib.odenvp_conditional_rl as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol, \"scale\": args.scale, \"scale_fac\": args.scale_fac, \"scale_std\": args.scale_std, \"gate\": args.gate},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            # re-weight the gate rewards\n",
      "            normalized_eta = args.eta / len(logp_actions)\n",
      "            \n",
      "            # collect cumulative future rewards\n",
      "            R = - loss\n",
      "            cum_rewards = []\n",
      "            for r in nfe[::-1]:\n",
      "                R = -normalized_eta * r.view(-1,1) + args.gamma * R\n",
      "                cum_rewards.insert(0,R)\n",
      "            \n",
      "            # apply REINFORCE\n",
      "            rl_loss = 0\n",
      "            for lpa, r in zip(logp_actions, cum_rewards):\n",
      "                rl_loss = rl_loss - lpa.view(-1,1) * args.rl_weight * r\n",
      "                \n",
      "            loss = loss + rl_loss.mean()\n",
      "            \n",
      "            loss.backward()\n",
      "            \n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "            \n",
      "            for idx in range(len(model.module.transforms)):\n",
      "                for layer in model.module.transforms[idx].chain:\n",
      "                    if hasattr(layer, 'atol'):\n",
      "                        layer.odefunc.after_odeint()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'train': atol[tol_indx].mean()}, itr)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'train': rtol[tol_indx].mean()}, itr)\n",
      "                    \n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples, atol, rtol, logp_actions, nfe = model(fixed_z, reverse=True)\n",
      "            generated_samples = generated_samples.view(-1, *data_shape)\n",
      "            for tol_indx in range(len(atol)):\n",
      "                writer.add_scalars('atol_gen_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                writer.add_scalars('rtol_gen_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=0.0001, autoencode=False, batch_norm=False, batch_size=8000, batch_size_schedule='', begin_epoch=1, condition_ratio=0.5, conditional=True, controlled_tol=True, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, eta=0.1, gamma=0.99, gate='cnn1', imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=1, lr=0.01, max_grad_norm=20.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume='../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_rl_stdscale_15_run1/epoch_33_checkpt.pth', rl_weight=0.01, rtol=0.0001, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_rl_stdscale_15_run1', scale=1.0, scale_fac=1.0, scale_std=15.0, seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=5000, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000.0, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1450732\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc_rl.py:291: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Iter 0199 | Time 315.5266(266.8311) | Bit/dim 4.7650(5.0679) | Xent 1.8220(1.9313) | Loss 13.3930(12.6895) | Error 0.6389(0.6787) Steps 0(0.00) | Grad Norm 2.9951(10.6853) | Total Time 0.00(0.00)\n",
      "Iter 0200 | Time 280.5382(267.2423) | Bit/dim 4.7420(5.0581) | Xent 1.8106(1.9277) | Loss 11.6744(12.6590) | Error 0.6424(0.6777) Steps 0(0.00) | Grad Norm 6.1846(10.5503) | Total Time 0.00(0.00)\n",
      "Iter 0201 | Time 299.8735(268.2213) | Bit/dim 4.7481(5.0488) | Xent 1.8124(1.9243) | Loss 11.7083(12.6305) | Error 0.6326(0.6763) Steps 0(0.00) | Grad Norm 5.8031(10.4078) | Total Time 0.00(0.00)\n",
      "Iter 0202 | Time 282.6323(268.6536) | Bit/dim 4.7580(5.0401) | Xent 1.7860(1.9201) | Loss 11.3437(12.5919) | Error 0.6299(0.6749) Steps 0(0.00) | Grad Norm 6.7034(10.2967) | Total Time 0.00(0.00)\n",
      "Iter 0203 | Time 258.6580(268.3537) | Bit/dim 4.7489(5.0313) | Xent 1.8138(1.9169) | Loss 11.5241(12.5599) | Error 0.6362(0.6737) Steps 0(0.00) | Grad Norm 13.6500(10.3973) | Total Time 0.00(0.00)\n",
      "Iter 0204 | Time 261.0793(268.1355) | Bit/dim 4.7580(5.0231) | Xent 1.9194(1.9170) | Loss 11.6014(12.5311) | Error 0.6809(0.6740) Steps 0(0.00) | Grad Norm 29.0568(10.9571) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0034 | Time 94.2650, Epoch Time 1809.0549(1572.7548), Bit/dim 4.7331(best: inf), Xent 1.7860, Loss 5.6261, Error 0.6313(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0205 | Time 336.7550(270.1941) | Bit/dim 4.7392(5.0146) | Xent 1.8340(1.9145) | Loss 14.3469(12.5856) | Error 0.6472(0.6732) Steps 0(0.00) | Grad Norm 21.5577(11.2751) | Total Time 0.00(0.00)\n",
      "Iter 0206 | Time 280.2372(270.4954) | Bit/dim 4.7210(5.0058) | Xent 1.8273(1.9119) | Loss 11.6100(12.5563) | Error 0.6398(0.6722) Steps 0(0.00) | Grad Norm 22.3632(11.6078) | Total Time 0.00(0.00)\n",
      "Iter 0207 | Time 300.6330(271.3995) | Bit/dim 4.7102(4.9970) | Xent 1.8423(1.9098) | Loss 11.3253(12.5194) | Error 0.6570(0.6717) Steps 0(0.00) | Grad Norm 20.0014(11.8596) | Total Time 0.00(0.00)\n",
      "Iter 0208 | Time 273.3130(271.4569) | Bit/dim 4.7258(4.9888) | Xent 1.9152(1.9100) | Loss 11.5445(12.4902) | Error 0.6744(0.6718) Steps 0(0.00) | Grad Norm 29.0334(12.3748) | Total Time 0.00(0.00)\n",
      "Iter 0209 | Time 286.7483(271.9157) | Bit/dim 4.7367(4.9813) | Xent 1.8557(1.9083) | Loss 11.6049(12.4636) | Error 0.6614(0.6715) Steps 0(0.00) | Grad Norm 17.0637(12.5155) | Total Time 0.00(0.00)\n",
      "Iter 0210 | Time 271.8105(271.9125) | Bit/dim 4.6972(4.9727) | Xent 1.8119(1.9054) | Loss 11.4850(12.4342) | Error 0.6447(0.6707) Steps 0(0.00) | Grad Norm 14.3472(12.5704) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0035 | Time 81.1974, Epoch Time 1846.7995(1580.9762), Bit/dim 4.6909(best: 4.7331), Xent 1.8131, Loss 5.5974, Error 0.6474(best: 0.6313)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0211 | Time 305.1067(272.9083) | Bit/dim 4.6727(4.9637) | Xent 1.8545(1.9039) | Loss 14.0751(12.4835) | Error 0.6571(0.6703) Steps 0(0.00) | Grad Norm 15.4279(12.6561) | Total Time 0.00(0.00)\n",
      "Iter 0212 | Time 285.6379(273.2902) | Bit/dim 4.7359(4.9569) | Xent 1.8787(1.9032) | Loss 11.4839(12.4535) | Error 0.6650(0.6701) Steps 0(0.00) | Grad Norm 16.8741(12.7827) | Total Time 0.00(0.00)\n",
      "Iter 0213 | Time 273.4715(273.2956) | Bit/dim 4.6595(4.9480) | Xent 1.8094(1.9003) | Loss 11.0734(12.4121) | Error 0.6405(0.6692) Steps 0(0.00) | Grad Norm 6.5148(12.5946) | Total Time 0.00(0.00)\n",
      "Iter 0214 | Time 374.3939(276.3286) | Bit/dim 4.6667(4.9395) | Xent 1.8706(1.8995) | Loss 11.5738(12.3869) | Error 0.6730(0.6693) Steps 0(0.00) | Grad Norm 14.2502(12.6443) | Total Time 0.00(0.00)\n",
      "Iter 0215 | Time 258.9234(275.8064) | Bit/dim 4.6893(4.9320) | Xent 1.8155(1.8969) | Loss 11.3828(12.3568) | Error 0.6365(0.6683) Steps 0(0.00) | Grad Norm 6.9391(12.4731) | Total Time 0.00(0.00)\n",
      "Iter 0216 | Time 314.6898(276.9729) | Bit/dim 4.6648(4.9240) | Xent 1.8425(1.8953) | Loss 11.6342(12.3351) | Error 0.6526(0.6679) Steps 0(0.00) | Grad Norm 12.9814(12.4884) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0036 | Time 101.6472, Epoch Time 1929.5008(1591.4319), Bit/dim 4.6632(best: 4.6909), Xent 1.8123, Loss 5.5694, Error 0.6435(best: 0.6313)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0217 | Time 301.0620(277.6956) | Bit/dim 4.6719(4.9164) | Xent 1.8339(1.8935) | Loss 14.2813(12.3935) | Error 0.6458(0.6672) Steps 0(0.00) | Grad Norm 18.9141(12.6812) | Total Time 0.00(0.00)\n",
      "Iter 0218 | Time 335.9334(279.4427) | Bit/dim 4.6503(4.9085) | Xent 1.8125(1.8910) | Loss 11.4368(12.3648) | Error 0.6478(0.6666) Steps 0(0.00) | Grad Norm 13.9152(12.7182) | Total Time 0.00(0.00)\n",
      "Iter 0219 | Time 315.6664(280.5295) | Bit/dim 4.6397(4.9004) | Xent 1.7810(1.8877) | Loss 11.0083(12.3241) | Error 0.6300(0.6655) Steps 0(0.00) | Grad Norm 4.5997(12.4746) | Total Time 0.00(0.00)\n",
      "Iter 0220 | Time 299.2883(281.0922) | Bit/dim 4.6188(4.8920) | Xent 1.8086(1.8854) | Loss 11.3568(12.2951) | Error 0.6394(0.6647) Steps 0(0.00) | Grad Norm 8.8947(12.3672) | Total Time 0.00(0.00)\n",
      "Iter 0221 | Time 264.5756(280.5967) | Bit/dim 4.6268(4.8840) | Xent 1.7698(1.8819) | Loss 10.9745(12.2555) | Error 0.6274(0.6636) Steps 0(0.00) | Grad Norm 8.5219(12.2519) | Total Time 0.00(0.00)\n",
      "Iter 0222 | Time 345.0990(282.5318) | Bit/dim 4.6349(4.8765) | Xent 1.7999(1.8794) | Loss 11.4816(12.2323) | Error 0.6424(0.6630) Steps 0(0.00) | Grad Norm 13.0779(12.2766) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0037 | Time 86.9145, Epoch Time 1965.1261(1602.6427), Bit/dim 4.7780(best: 4.6632), Xent 1.7733, Loss 5.6646, Error 0.6205(best: 0.6313)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0223 | Time 325.1162(283.8093) | Bit/dim 4.7725(4.8734) | Xent 1.7994(1.8770) | Loss 14.5460(12.3017) | Error 0.6387(0.6623) Steps 0(0.00) | Grad Norm 18.5015(12.4634) | Total Time 0.00(0.00)\n",
      "Iter 0224 | Time 338.6601(285.4548) | Bit/dim 4.7952(4.8711) | Xent 1.8203(1.8753) | Loss 11.7734(12.2858) | Error 0.6359(0.6615) Steps 0(0.00) | Grad Norm 28.8068(12.9537) | Total Time 0.00(0.00)\n",
      "Iter 0225 | Time 273.9510(285.1097) | Bit/dim 4.6652(4.8649) | Xent 1.8165(1.8736) | Loss 11.6191(12.2658) | Error 0.6418(0.6609) Steps 0(0.00) | Grad Norm 19.9246(13.1628) | Total Time 0.00(0.00)\n",
      "Iter 0226 | Time 289.1216(285.2301) | Bit/dim 4.7212(4.8606) | Xent 1.8164(1.8718) | Loss 11.6669(12.2479) | Error 0.6431(0.6603) Steps 0(0.00) | Grad Norm 24.6826(13.5084) | Total Time 0.00(0.00)\n",
      "Iter 0227 | Time 323.2380(286.3703) | Bit/dim 4.6550(4.8544) | Xent 1.7957(1.8696) | Loss 11.6057(12.2286) | Error 0.6327(0.6595) Steps 0(0.00) | Grad Norm 13.4735(13.5074) | Total Time 0.00(0.00)\n",
      "Iter 0228 | Time 292.5604(286.5560) | Bit/dim 4.6461(4.8482) | Xent 1.7754(1.8667) | Loss 11.3244(12.2015) | Error 0.6231(0.6584) Steps 0(0.00) | Grad Norm 7.2682(13.3202) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0038 | Time 84.4681, Epoch Time 1943.1128(1612.8568), Bit/dim 4.6177(best: 4.6632), Xent 1.7336, Loss 5.4845, Error 0.6027(best: 0.6205)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0229 | Time 308.7116(287.2207) | Bit/dim 4.6139(4.8411) | Xent 1.7719(1.8639) | Loss 13.8902(12.2521) | Error 0.6292(0.6575) Steps 0(0.00) | Grad Norm 10.7991(13.2446) | Total Time 0.00(0.00)\n",
      "Iter 0230 | Time 348.4174(289.0566) | Bit/dim 4.6331(4.8349) | Xent 1.8013(1.8620) | Loss 11.3599(12.2254) | Error 0.6389(0.6570) Steps 0(0.00) | Grad Norm 12.8909(13.2340) | Total Time 0.00(0.00)\n",
      "Iter 0231 | Time 306.7683(289.5879) | Bit/dim 4.5768(4.8271) | Xent 1.7724(1.8593) | Loss 11.1515(12.1931) | Error 0.6280(0.6561) Steps 0(0.00) | Grad Norm 5.2385(12.9941) | Total Time 0.00(0.00)\n",
      "Iter 0232 | Time 276.7441(289.2026) | Bit/dim 4.5982(4.8203) | Xent 1.7907(1.8573) | Loss 11.1796(12.1627) | Error 0.6430(0.6557) Steps 0(0.00) | Grad Norm 21.2423(13.2415) | Total Time 0.00(0.00)\n",
      "Iter 0233 | Time 300.3801(289.5380) | Bit/dim 4.6349(4.8147) | Xent 1.8841(1.8581) | Loss 11.5978(12.1458) | Error 0.6725(0.6562) Steps 0(0.00) | Grad Norm 25.5167(13.6098) | Total Time 0.00(0.00)\n",
      "Iter 0234 | Time 265.1713(288.8070) | Bit/dim 4.5878(4.8079) | Xent 1.8229(1.8570) | Loss 11.2497(12.1189) | Error 0.6501(0.6560) Steps 0(0.00) | Grad Norm 10.9183(13.5290) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0039 | Time 79.8702, Epoch Time 1902.2377(1621.5383), Bit/dim 4.6428(best: 4.6177), Xent 1.7857, Loss 5.5357, Error 0.6283(best: 0.6027)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0235 | Time 267.7008(288.1738) | Bit/dim 4.6482(4.8031) | Xent 1.8042(1.8554) | Loss 13.2820(12.1538) | Error 0.6356(0.6554) Steps 0(0.00) | Grad Norm 21.2575(13.7609) | Total Time 0.00(0.00)\n",
      "Iter 0236 | Time 296.2710(288.4167) | Bit/dim 4.5895(4.7967) | Xent 1.7923(1.8535) | Loss 11.1716(12.1243) | Error 0.6329(0.6548) Steps 0(0.00) | Grad Norm 10.1982(13.6540) | Total Time 0.00(0.00)\n",
      "Iter 0237 | Time 315.0268(289.2150) | Bit/dim 4.5960(4.7907) | Xent 1.8369(1.8530) | Loss 11.0134(12.0910) | Error 0.6694(0.6552) Steps 0(0.00) | Grad Norm 15.2817(13.7028) | Total Time 0.00(0.00)\n",
      "Iter 0238 | Time 280.2311(288.9455) | Bit/dim 4.5768(4.7843) | Xent 1.8087(1.8517) | Loss 11.1822(12.0637) | Error 0.6411(0.6548) Steps 0(0.00) | Grad Norm 13.8603(13.7076) | Total Time 0.00(0.00)\n",
      "Iter 0239 | Time 281.4395(288.7203) | Bit/dim 4.6055(4.7789) | Xent 1.7796(1.8495) | Loss 11.5041(12.0470) | Error 0.6312(0.6541) Steps 0(0.00) | Grad Norm 13.8636(13.7123) | Total Time 0.00(0.00)\n",
      "Iter 0240 | Time 313.5489(289.4652) | Bit/dim 4.5740(4.7728) | Xent 1.8763(1.8503) | Loss 11.2093(12.0218) | Error 0.6691(0.6545) Steps 0(0.00) | Grad Norm 17.2975(13.8198) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0040 | Time 81.2353, Epoch Time 1851.4114(1628.4344), Bit/dim 4.5401(best: 4.6177), Xent 1.7350, Loss 5.4076, Error 0.6134(best: 0.6027)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0241 | Time 326.3609(290.5720) | Bit/dim 4.5417(4.7658) | Xent 1.7729(1.8480) | Loss 13.6840(12.0717) | Error 0.6279(0.6537) Steps 0(0.00) | Grad Norm 5.1537(13.5598) | Total Time 0.00(0.00)\n",
      "Iter 0242 | Time 288.8254(290.5196) | Bit/dim 4.5413(4.7591) | Xent 1.8408(1.8478) | Loss 11.1433(12.0438) | Error 0.6531(0.6537) Steps 0(0.00) | Grad Norm 14.8220(13.5977) | Total Time 0.00(0.00)\n",
      "Iter 0243 | Time 339.2514(291.9816) | Bit/dim 4.5588(4.7531) | Xent 1.8385(1.8475) | Loss 11.4466(12.0259) | Error 0.6506(0.6536) Steps 0(0.00) | Grad Norm 17.1442(13.7041) | Total Time 0.00(0.00)\n",
      "Iter 0244 | Time 319.6121(292.8105) | Bit/dim 4.5681(4.7475) | Xent 1.7964(1.8460) | Loss 11.1061(11.9983) | Error 0.6399(0.6532) Steps 0(0.00) | Grad Norm 14.6815(13.7334) | Total Time 0.00(0.00)\n",
      "Iter 0245 | Time 294.1203(292.8498) | Bit/dim 4.5501(4.7416) | Xent 1.7559(1.8433) | Loss 11.1132(11.9718) | Error 0.6225(0.6523) Steps 0(0.00) | Grad Norm 10.0470(13.6228) | Total Time 0.00(0.00)\n",
      "Iter 0246 | Time 400.3940(296.0761) | Bit/dim 4.5269(4.7352) | Xent 1.7666(1.8410) | Loss 11.3412(11.9529) | Error 0.6252(0.6515) Steps 0(0.00) | Grad Norm 7.0408(13.4254) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0041 | Time 71.0929, Epoch Time 2055.4287(1641.2443), Bit/dim 4.5209(best: 4.5401), Xent 1.7105, Loss 5.3762, Error 0.6011(best: 0.6027)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0247 | Time 281.7563(295.6465) | Bit/dim 4.5289(4.7290) | Xent 1.7572(1.8385) | Loss 13.4102(11.9966) | Error 0.6188(0.6505) Steps 0(0.00) | Grad Norm 8.3635(13.2735) | Total Time 0.00(0.00)\n",
      "Iter 0248 | Time 359.4826(297.5616) | Bit/dim 4.4854(4.7217) | Xent 1.7239(1.8350) | Loss 11.0855(11.9692) | Error 0.6096(0.6493) Steps 0(0.00) | Grad Norm 3.9658(12.9943) | Total Time 0.00(0.00)\n",
      "Iter 0249 | Time 286.7510(297.2373) | Bit/dim 4.4927(4.7148) | Xent 1.7405(1.8322) | Loss 10.9495(11.9386) | Error 0.6219(0.6484) Steps 0(0.00) | Grad Norm 9.7444(12.8968) | Total Time 0.00(0.00)\n",
      "Iter 0250 | Time 269.3870(296.4018) | Bit/dim 4.4934(4.7082) | Xent 1.7333(1.8292) | Loss 10.4169(11.8930) | Error 0.6134(0.6474) Steps 0(0.00) | Grad Norm 8.3578(12.7606) | Total Time 0.00(0.00)\n",
      "Iter 0251 | Time 286.9138(296.1171) | Bit/dim 4.5238(4.7026) | Xent 1.7346(1.8264) | Loss 11.0899(11.8689) | Error 0.6156(0.6464) Steps 0(0.00) | Grad Norm 12.4354(12.7508) | Total Time 0.00(0.00)\n",
      "Iter 0252 | Time 286.8516(295.8392) | Bit/dim 4.6484(4.7010) | Xent 1.8072(1.8258) | Loss 11.3357(11.8529) | Error 0.6480(0.6465) Steps 0(0.00) | Grad Norm 20.3294(12.9782) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0042 | Time 74.9114, Epoch Time 1861.9847(1647.8665), Bit/dim 4.6204(best: 4.5209), Xent 1.6806, Loss 5.4607, Error 0.6019(best: 0.6011)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0253 | Time 342.6186(297.2426) | Bit/dim 4.6287(4.6988) | Xent 1.7795(1.8244) | Loss 14.0335(11.9183) | Error 0.6346(0.6461) Steps 0(0.00) | Grad Norm 16.5566(13.0856) | Total Time 0.00(0.00)\n",
      "Iter 0254 | Time 292.7973(297.1092) | Bit/dim 4.6752(4.6981) | Xent 1.7475(1.8221) | Loss 11.4687(11.9048) | Error 0.6191(0.6453) Steps 0(0.00) | Grad Norm 12.3683(13.0640) | Total Time 0.00(0.00)\n",
      "Iter 0255 | Time 296.6359(297.0950) | Bit/dim 4.6228(4.6959) | Xent 1.7798(1.8209) | Loss 11.0949(11.8805) | Error 0.6335(0.6450) Steps 0(0.00) | Grad Norm 20.6199(13.2907) | Total Time 0.00(0.00)\n",
      "Iter 0256 | Time 310.9405(297.5104) | Bit/dim 4.5468(4.6914) | Xent 1.8700(1.8223) | Loss 11.1897(11.8598) | Error 0.6589(0.6454) Steps 0(0.00) | Grad Norm 26.5698(13.6891) | Total Time 0.00(0.00)\n",
      "Iter 0257 | Time 301.4113(297.6274) | Bit/dim 4.5963(4.6885) | Xent 1.7990(1.8216) | Loss 11.3130(11.8434) | Error 0.6486(0.6455) Steps 0(0.00) | Grad Norm 14.3654(13.7094) | Total Time 0.00(0.00)\n",
      "Iter 0258 | Time 267.4974(296.7235) | Bit/dim 4.5515(4.6844) | Xent 1.7878(1.8206) | Loss 11.1654(11.8231) | Error 0.6325(0.6451) Steps 0(0.00) | Grad Norm 17.1736(13.8133) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0043 | Time 83.2750, Epoch Time 1911.1813(1655.7659), Bit/dim 4.6373(best: 4.5209), Xent 1.7442, Loss 5.5094, Error 0.6175(best: 0.6011)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0259 | Time 267.8109(295.8561) | Bit/dim 4.6330(4.6829) | Xent 1.7655(1.8190) | Loss 14.0161(11.8889) | Error 0.6246(0.6445) Steps 0(0.00) | Grad Norm 8.8425(13.6642) | Total Time 0.00(0.00)\n",
      "Iter 0260 | Time 323.2899(296.6791) | Bit/dim 4.5056(4.6776) | Xent 1.7786(1.8178) | Loss 10.9118(11.8596) | Error 0.6384(0.6443) Steps 0(0.00) | Grad Norm 12.7877(13.6379) | Total Time 0.00(0.00)\n",
      "Iter 0261 | Time 293.5866(296.5864) | Bit/dim 4.6066(4.6754) | Xent 1.8899(1.8199) | Loss 11.0051(11.8339) | Error 0.6759(0.6452) Steps 0(0.00) | Grad Norm 30.2182(14.1353) | Total Time 0.00(0.00)\n",
      "Iter 0262 | Time 352.5987(298.2667) | Bit/dim 4.5208(4.6708) | Xent 1.7918(1.8191) | Loss 11.2078(11.8151) | Error 0.6352(0.6449) Steps 0(0.00) | Grad Norm 16.2308(14.1982) | Total Time 0.00(0.00)\n",
      "Iter 0263 | Time 293.1148(298.1122) | Bit/dim 4.5746(4.6679) | Xent 1.7371(1.8166) | Loss 11.2066(11.7969) | Error 0.6218(0.6442) Steps 0(0.00) | Grad Norm 13.9016(14.1893) | Total Time 0.00(0.00)\n",
      "Iter 0264 | Time 314.2268(298.5956) | Bit/dim 4.5489(4.6643) | Xent 1.8130(1.8165) | Loss 11.2351(11.7800) | Error 0.6516(0.6445) Steps 0(0.00) | Grad Norm 20.1002(14.3666) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0044 | Time 90.4678, Epoch Time 1951.3989(1664.6349), Bit/dim 4.5013(best: 4.5209), Xent 1.6988, Loss 5.3508, Error 0.6052(best: 0.6011)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0265 | Time 337.7652(299.7707) | Bit/dim 4.4977(4.6593) | Xent 1.7489(1.8145) | Loss 13.6203(11.8352) | Error 0.6295(0.6440) Steps 0(0.00) | Grad Norm 7.1207(14.1492) | Total Time 0.00(0.00)\n",
      "Iter 0266 | Time 316.0820(300.2600) | Bit/dim 4.4881(4.6542) | Xent 1.8174(1.8146) | Loss 11.3486(11.8206) | Error 0.6460(0.6441) Steps 0(0.00) | Grad Norm 19.7824(14.3182) | Total Time 0.00(0.00)\n",
      "Iter 0267 | Time 342.2293(301.5191) | Bit/dim 4.4904(4.6493) | Xent 1.7546(1.8128) | Loss 11.2579(11.8038) | Error 0.6175(0.6433) Steps 0(0.00) | Grad Norm 11.3892(14.2303) | Total Time 0.00(0.00)\n",
      "Iter 0268 | Time 318.9749(302.0428) | Bit/dim 4.5117(4.6452) | Xent 1.7536(1.8110) | Loss 11.2831(11.7881) | Error 0.6280(0.6428) Steps 0(0.00) | Grad Norm 19.5149(14.3889) | Total Time 0.00(0.00)\n",
      "Iter 0269 | Time 295.4155(301.8440) | Bit/dim 4.5528(4.6424) | Xent 1.8574(1.8124) | Loss 11.2010(11.7705) | Error 0.6556(0.6432) Steps 0(0.00) | Grad Norm 20.8226(14.5819) | Total Time 0.00(0.00)\n",
      "Iter 0270 | Time 313.8345(302.2037) | Bit/dim 4.5329(4.6391) | Xent 1.8804(1.8144) | Loss 11.2964(11.7563) | Error 0.6490(0.6434) Steps 0(0.00) | Grad Norm 20.9409(14.7727) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0045 | Time 84.6014, Epoch Time 2024.7136(1675.4373), Bit/dim 4.4850(best: 4.5013), Xent 1.8045, Loss 5.3872, Error 0.6481(best: 0.6011)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0271 | Time 285.3533(301.6982) | Bit/dim 4.4795(4.6343) | Xent 1.8473(1.8154) | Loss 13.2319(11.8006) | Error 0.6570(0.6438) Steps 0(0.00) | Grad Norm 15.0122(14.7798) | Total Time 0.00(0.00)\n",
      "Iter 0272 | Time 335.2644(302.7052) | Bit/dim 4.4868(4.6299) | Xent 1.8274(1.8158) | Loss 11.1654(11.7815) | Error 0.6445(0.6438) Steps 0(0.00) | Grad Norm 15.3676(14.7975) | Total Time 0.00(0.00)\n",
      "Iter 0273 | Time 290.4125(302.3364) | Bit/dim 4.4855(4.6256) | Xent 1.8037(1.8154) | Loss 10.7171(11.7496) | Error 0.6384(0.6436) Steps 0(0.00) | Grad Norm 15.1046(14.8067) | Total Time 0.00(0.00)\n",
      "Iter 0274 | Time 324.4375(302.9994) | Bit/dim 4.4825(4.6213) | Xent 1.7864(1.8145) | Loss 11.1197(11.7307) | Error 0.6360(0.6434) Steps 0(0.00) | Grad Norm 12.5072(14.7377) | Total Time 0.00(0.00)\n",
      "Iter 0275 | Time 313.8862(303.3260) | Bit/dim 4.4166(4.6151) | Xent 1.7900(1.8138) | Loss 10.8433(11.7041) | Error 0.6416(0.6434) Steps 0(0.00) | Grad Norm 5.0516(14.4471) | Total Time 0.00(0.00)\n",
      "Iter 0276 | Time 332.6069(304.2044) | Bit/dim 4.4601(4.6105) | Xent 1.8045(1.8135) | Loss 11.1037(11.6860) | Error 0.6344(0.6431) Steps 0(0.00) | Grad Norm 12.9098(14.4010) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0046 | Time 76.9534, Epoch Time 1974.5846(1684.4117), Bit/dim 4.4552(best: 4.4850), Xent 1.7215, Loss 5.3159, Error 0.5965(best: 0.6011)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0277 | Time 327.7783(304.9117) | Bit/dim 4.4560(4.6058) | Xent 1.7566(1.8118) | Loss 13.7774(11.7488) | Error 0.6141(0.6422) Steps 0(0.00) | Grad Norm 8.1232(14.2127) | Total Time 0.00(0.00)\n",
      "Iter 0278 | Time 328.4501(305.6178) | Bit/dim 4.4165(4.6002) | Xent 1.7354(1.8095) | Loss 11.0293(11.7272) | Error 0.6149(0.6414) Steps 0(0.00) | Grad Norm 4.9798(13.9357) | Total Time 0.00(0.00)\n",
      "Iter 0279 | Time 274.4908(304.6840) | Bit/dim 4.4700(4.5963) | Xent 1.7676(1.8083) | Loss 11.0472(11.7068) | Error 0.6275(0.6410) Steps 0(0.00) | Grad Norm 15.1957(13.9735) | Total Time 0.00(0.00)\n",
      "Iter 0280 | Time 310.8932(304.8703) | Bit/dim 4.4488(4.5918) | Xent 1.7501(1.8065) | Loss 10.8218(11.6803) | Error 0.6239(0.6405) Steps 0(0.00) | Grad Norm 15.0813(14.0067) | Total Time 0.00(0.00)\n",
      "Iter 0281 | Time 268.0150(303.7646) | Bit/dim 4.4326(4.5871) | Xent 1.7239(1.8040) | Loss 10.7660(11.6528) | Error 0.6138(0.6397) Steps 0(0.00) | Grad Norm 9.8301(13.8814) | Total Time 0.00(0.00)\n",
      "Iter 0282 | Time 309.8833(303.9482) | Bit/dim 4.4406(4.5827) | Xent 1.7370(1.8020) | Loss 10.9008(11.6303) | Error 0.6078(0.6387) Steps 0(0.00) | Grad Norm 12.1894(13.8307) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0047 | Time 79.2522, Epoch Time 1914.7544(1691.3220), Bit/dim 4.3949(best: 4.4552), Xent 1.6862, Loss 5.2381, Error 0.5946(best: 0.5965)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0283 | Time 290.1272(303.5335) | Bit/dim 4.3991(4.5772) | Xent 1.7448(1.8003) | Loss 13.2459(11.6787) | Error 0.6169(0.6381) Steps 0(0.00) | Grad Norm 13.3393(13.8159) | Total Time 0.00(0.00)\n",
      "Iter 0284 | Time 291.0232(303.1582) | Bit/dim 4.3941(4.5717) | Xent 1.7021(1.7974) | Loss 10.7322(11.6503) | Error 0.6075(0.6371) Steps 0(0.00) | Grad Norm 7.0965(13.6143) | Total Time 0.00(0.00)\n",
      "Iter 0285 | Time 314.0948(303.4863) | Bit/dim 4.3849(4.5661) | Xent 1.7405(1.7957) | Loss 11.0098(11.6311) | Error 0.6188(0.6366) Steps 0(0.00) | Grad Norm 14.5192(13.6415) | Total Time 0.00(0.00)\n",
      "Iter 0286 | Time 347.0578(304.7935) | Bit/dim 4.3684(4.5601) | Xent 1.7172(1.7933) | Loss 10.7248(11.6039) | Error 0.6170(0.6360) Steps 0(0.00) | Grad Norm 8.5023(13.4873) | Total Time 0.00(0.00)\n",
      "Iter 0287 | Time 366.2359(306.6368) | Bit/dim 4.3946(4.5552) | Xent 1.7389(1.7917) | Loss 10.7266(11.5776) | Error 0.6238(0.6356) Steps 0(0.00) | Grad Norm 13.0813(13.4751) | Total Time 0.00(0.00)\n",
      "Iter 0288 | Time 301.7556(306.4903) | Bit/dim 4.3478(4.5489) | Xent 1.6895(1.7886) | Loss 10.6397(11.5495) | Error 0.6014(0.6346) Steps 0(0.00) | Grad Norm 6.3458(13.2612) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0048 | Time 94.3913, Epoch Time 2020.2618(1701.1902), Bit/dim 4.3631(best: 4.3949), Xent 1.6405, Loss 5.1833, Error 0.5881(best: 0.5946)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0289 | Time 292.0890(306.0583) | Bit/dim 4.3659(4.5435) | Xent 1.6983(1.7859) | Loss 12.8938(11.5898) | Error 0.6018(0.6336) Steps 0(0.00) | Grad Norm 9.1647(13.1383) | Total Time 0.00(0.00)\n",
      "Iter 0290 | Time 335.9989(306.9565) | Bit/dim 4.4037(4.5393) | Xent 1.6749(1.7826) | Loss 10.5591(11.5589) | Error 0.5951(0.6325) Steps 0(0.00) | Grad Norm 10.9664(13.0732) | Total Time 0.00(0.00)\n",
      "Iter 0291 | Time 364.6676(308.6878) | Bit/dim 4.3471(4.5335) | Xent 1.6734(1.7793) | Loss 10.6814(11.5326) | Error 0.5982(0.6314) Steps 0(0.00) | Grad Norm 4.9373(12.8291) | Total Time 0.00(0.00)\n",
      "Iter 0292 | Time 292.3608(308.1980) | Bit/dim 4.3324(4.5275) | Xent 1.6595(1.7757) | Loss 10.6377(11.5057) | Error 0.5884(0.6301) Steps 0(0.00) | Grad Norm 5.7324(12.6162) | Total Time 0.00(0.00)\n",
      "Iter 0293 | Time 321.5826(308.5996) | Bit/dim 4.3297(4.5215) | Xent 1.6501(1.7719) | Loss 10.6728(11.4807) | Error 0.5845(0.6288) Steps 0(0.00) | Grad Norm 4.5152(12.3732) | Total Time 0.00(0.00)\n",
      "Iter 0294 | Time 304.9213(308.4892) | Bit/dim 4.3407(4.5161) | Xent 1.6558(1.7685) | Loss 10.5959(11.4542) | Error 0.5915(0.6277) Steps 0(0.00) | Grad Norm 7.7435(12.2343) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0049 | Time 92.5191, Epoch Time 2020.0882(1710.7571), Bit/dim 4.3597(best: 4.3631), Xent 1.5984, Loss 5.1589, Error 0.5583(best: 0.5881)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0295 | Time 308.5016(308.4896) | Bit/dim 4.3651(4.5116) | Xent 1.6296(1.7643) | Loss 13.5571(11.5173) | Error 0.5766(0.6261) Steps 0(0.00) | Grad Norm 8.4401(12.1205) | Total Time 0.00(0.00)\n",
      "Iter 0296 | Time 358.7903(309.9986) | Bit/dim 4.3413(4.5065) | Xent 1.6653(1.7613) | Loss 10.5135(11.4872) | Error 0.5970(0.6253) Steps 0(0.00) | Grad Norm 13.3188(12.1564) | Total Time 0.00(0.00)\n",
      "Iter 0297 | Time 340.4398(310.9118) | Bit/dim 4.3591(4.5020) | Xent 1.7818(1.7619) | Loss 10.8688(11.4686) | Error 0.6424(0.6258) Steps 0(0.00) | Grad Norm 26.2488(12.5792) | Total Time 0.00(0.00)\n",
      "Iter 0298 | Time 301.2867(310.6231) | Bit/dim 4.3289(4.4969) | Xent 1.6851(1.7596) | Loss 10.6440(11.4439) | Error 0.6045(0.6251) Steps 0(0.00) | Grad Norm 17.5706(12.7289) | Total Time 0.00(0.00)\n",
      "Iter 0299 | Time 300.1960(310.3103) | Bit/dim 4.3285(4.4918) | Xent 1.9083(1.7641) | Loss 11.0015(11.4306) | Error 0.6661(0.6264) Steps 0(0.00) | Grad Norm 34.2263(13.3738) | Total Time 0.00(0.00)\n",
      "Iter 0300 | Time 311.1614(310.3358) | Bit/dim 4.3294(4.4869) | Xent 1.7469(1.7636) | Loss 10.4658(11.4017) | Error 0.6195(0.6262) Steps 0(0.00) | Grad Norm 7.5827(13.2001) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0050 | Time 80.6136, Epoch Time 2016.9075(1719.9416), Bit/dim 4.3210(best: 4.3597), Xent 1.7241, Loss 5.1831, Error 0.6056(best: 0.5583)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0301 | Time 295.0661(309.8777) | Bit/dim 4.3246(4.4821) | Xent 1.7787(1.7640) | Loss 13.2039(11.4557) | Error 0.6295(0.6263) Steps 0(0.00) | Grad Norm 15.6818(13.2746) | Total Time 0.00(0.00)\n",
      "Iter 0302 | Time 300.3072(309.5906) | Bit/dim 4.3049(4.4768) | Xent 1.7485(1.7636) | Loss 10.5349(11.4281) | Error 0.6341(0.6265) Steps 0(0.00) | Grad Norm 12.7596(13.2591) | Total Time 0.00(0.00)\n",
      "Iter 0303 | Time 306.2483(309.4903) | Bit/dim 4.3159(4.4719) | Xent 1.6555(1.7603) | Loss 10.5768(11.4026) | Error 0.5887(0.6254) Steps 0(0.00) | Grad Norm 6.7955(13.0652) | Total Time 0.00(0.00)\n",
      "Iter 0304 | Time 324.4384(309.9388) | Bit/dim 4.3096(4.4671) | Xent 1.7280(1.7594) | Loss 10.5897(11.3782) | Error 0.6129(0.6250) Steps 0(0.00) | Grad Norm 16.9782(13.1826) | Total Time 0.00(0.00)\n",
      "Iter 0305 | Time 312.9155(310.0281) | Bit/dim 4.3903(4.4648) | Xent 1.7671(1.7596) | Loss 10.9287(11.3647) | Error 0.6372(0.6254) Steps 0(0.00) | Grad Norm 32.1683(13.7522) | Total Time 0.00(0.00)\n",
      "Iter 0306 | Time 383.0403(312.2184) | Bit/dim 4.3095(4.4601) | Xent 1.7013(1.7578) | Loss 10.7816(11.3472) | Error 0.6076(0.6248) Steps 0(0.00) | Grad Norm 14.2386(13.7668) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0051 | Time 76.8395, Epoch Time 2014.9267(1728.7912), Bit/dim 4.3819(best: 4.3210), Xent 1.6223, Loss 5.1930, Error 0.5649(best: 0.5583)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0307 | Time 312.6275(312.2307) | Bit/dim 4.3824(4.4578) | Xent 1.6552(1.7548) | Loss 13.6618(11.4166) | Error 0.5882(0.6237) Steps 0(0.00) | Grad Norm 13.6930(13.7645) | Total Time 0.00(0.00)\n",
      "Iter 0308 | Time 307.6679(312.0938) | Bit/dim 4.3885(4.4557) | Xent 1.7681(1.7552) | Loss 10.4811(11.3886) | Error 0.6325(0.6240) Steps 0(0.00) | Grad Norm 20.6151(13.9701) | Total Time 0.00(0.00)\n",
      "Iter 0309 | Time 320.0889(312.3337) | Bit/dim 4.3899(4.4537) | Xent 1.7088(1.7538) | Loss 10.8882(11.3736) | Error 0.6010(0.6233) Steps 0(0.00) | Grad Norm 13.2516(13.9485) | Total Time 0.00(0.00)\n",
      "Iter 0310 | Time 293.9865(311.7833) | Bit/dim 4.3312(4.4500) | Xent 1.7484(1.7536) | Loss 10.9044(11.3595) | Error 0.6248(0.6233) Steps 0(0.00) | Grad Norm 17.2061(14.0462) | Total Time 0.00(0.00)\n",
      "Iter 0311 | Time 314.1919(311.8555) | Bit/dim 4.3801(4.4479) | Xent 1.7577(1.7537) | Loss 10.7822(11.3422) | Error 0.6304(0.6236) Steps 0(0.00) | Grad Norm 23.5181(14.3304) | Total Time 0.00(0.00)\n",
      "Iter 0312 | Time 304.2550(311.6275) | Bit/dim 4.2970(4.4434) | Xent 1.6834(1.7516) | Loss 10.6924(11.3227) | Error 0.6028(0.6229) Steps 0(0.00) | Grad Norm 9.2321(14.1774) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0052 | Time 79.2114, Epoch Time 1948.0248(1735.3682), Bit/dim 4.3296(best: 4.3210), Xent 1.6587, Loss 5.1589, Error 0.5909(best: 0.5583)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0313 | Time 316.3622(311.7695) | Bit/dim 4.3172(4.4396) | Xent 1.7025(1.7501) | Loss 13.5971(11.3909) | Error 0.6151(0.6227) Steps 0(0.00) | Grad Norm 11.3964(14.0940) | Total Time 0.00(0.00)\n",
      "Iter 0314 | Time 306.2894(311.6051) | Bit/dim 4.3007(4.4355) | Xent 1.6744(1.7479) | Loss 10.5203(11.3648) | Error 0.5984(0.6220) Steps 0(0.00) | Grad Norm 10.9149(13.9986) | Total Time 0.00(0.00)\n",
      "Iter 0315 | Time 296.4120(311.1493) | Bit/dim 4.2903(4.4311) | Xent 1.6940(1.7463) | Loss 10.5080(11.3391) | Error 0.5960(0.6212) Steps 0(0.00) | Grad Norm 11.5991(13.9267) | Total Time 0.00(0.00)\n",
      "Iter 0316 | Time 338.8155(311.9793) | Bit/dim 4.2837(4.4267) | Xent 1.6717(1.7440) | Loss 10.4617(11.3128) | Error 0.5958(0.6204) Steps 0(0.00) | Grad Norm 6.3333(13.6989) | Total Time 0.00(0.00)\n",
      "Iter 0317 | Time 306.5023(311.8150) | Bit/dim 4.2810(4.4223) | Xent 1.6352(1.7408) | Loss 10.6121(11.2917) | Error 0.5911(0.6195) Steps 0(0.00) | Grad Norm 4.8465(13.4333) | Total Time 0.00(0.00)\n",
      "Iter 0318 | Time 319.3064(312.0398) | Bit/dim 4.2710(4.4178) | Xent 1.6528(1.7381) | Loss 10.6181(11.2715) | Error 0.5976(0.6189) Steps 0(0.00) | Grad Norm 6.4302(13.2232) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0053 | Time 92.5019, Epoch Time 1992.3496(1743.0776), Bit/dim 4.2595(best: 4.3210), Xent 1.6063, Loss 5.0627, Error 0.5708(best: 0.5583)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0319 | Time 312.4104(312.0509) | Bit/dim 4.2579(4.4130) | Xent 1.6690(1.7360) | Loss 13.1748(11.3286) | Error 0.5933(0.6181) Steps 0(0.00) | Grad Norm 8.0950(13.0693) | Total Time 0.00(0.00)\n",
      "Iter 0320 | Time 338.7287(312.8512) | Bit/dim 4.2605(4.4084) | Xent 1.6414(1.7332) | Loss 10.6637(11.3087) | Error 0.5840(0.6171) Steps 0(0.00) | Grad Norm 9.4783(12.9616) | Total Time 0.00(0.00)\n",
      "Iter 0321 | Time 315.3766(312.9270) | Bit/dim 4.3403(4.4064) | Xent 1.6424(1.7305) | Loss 10.7366(11.2915) | Error 0.5776(0.6159) Steps 0(0.00) | Grad Norm 17.5922(13.1005) | Total Time 0.00(0.00)\n",
      "Iter 0322 | Time 370.4657(314.6531) | Bit/dim 4.4342(4.4072) | Xent 1.7447(1.7309) | Loss 10.9394(11.2810) | Error 0.6120(0.6158) Steps 0(0.00) | Grad Norm 27.5327(13.5335) | Total Time 0.00(0.00)\n",
      "Iter 0323 | Time 283.1553(313.7082) | Bit/dim 4.3057(4.4041) | Xent 1.6864(1.7296) | Loss 10.2173(11.2490) | Error 0.6008(0.6153) Steps 0(0.00) | Grad Norm 6.9360(13.3356) | Total Time 0.00(0.00)\n",
      "Iter 0324 | Time 307.0105(313.5073) | Bit/dim 4.3267(4.4018) | Xent 1.6845(1.7282) | Loss 10.7309(11.2335) | Error 0.6010(0.6149) Steps 0(0.00) | Grad Norm 7.6283(13.1644) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0054 | Time 76.8435, Epoch Time 2020.1798(1751.3907), Bit/dim 4.3004(best: 4.2595), Xent 1.6331, Loss 5.1170, Error 0.5854(best: 0.5583)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0325 | Time 316.4867(313.5967) | Bit/dim 4.2949(4.3986) | Xent 1.7043(1.7275) | Loss 13.4312(11.2994) | Error 0.6149(0.6149) Steps 0(0.00) | Grad Norm 11.1010(13.1025) | Total Time 0.00(0.00)\n",
      "Iter 0326 | Time 314.1752(313.6140) | Bit/dim 4.2714(4.3948) | Xent 1.7211(1.7273) | Loss 10.6364(11.2795) | Error 0.6109(0.6148) Steps 0(0.00) | Grad Norm 18.2685(13.2574) | Total Time 0.00(0.00)\n",
      "Iter 0327 | Time 319.8113(313.7999) | Bit/dim 4.3296(4.3928) | Xent 1.6952(1.7263) | Loss 10.6818(11.2616) | Error 0.6020(0.6144) Steps 0(0.00) | Grad Norm 17.1229(13.3734) | Total Time 0.00(0.00)\n",
      "Iter 0328 | Time 296.7201(313.2875) | Bit/dim 4.2725(4.3892) | Xent 1.6728(1.7247) | Loss 10.3793(11.2351) | Error 0.5971(0.6139) Steps 0(0.00) | Grad Norm 11.7638(13.3251) | Total Time 0.00(0.00)\n",
      "Iter 0329 | Time 309.3778(313.1702) | Bit/dim 4.2571(4.3853) | Xent 1.6582(1.7227) | Loss 10.6554(11.2177) | Error 0.5934(0.6133) Steps 0(0.00) | Grad Norm 7.3050(13.1445) | Total Time 0.00(0.00)\n",
      "Iter 0330 | Time 301.3999(312.8171) | Bit/dim 4.2565(4.3814) | Xent 1.6496(1.7205) | Loss 10.6804(11.2016) | Error 0.5936(0.6127) Steps 0(0.00) | Grad Norm 13.7406(13.1624) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0055 | Time 82.7415, Epoch Time 1956.6710(1757.5491), Bit/dim 4.2664(best: 4.2595), Xent 1.6860, Loss 5.1094, Error 0.6058(best: 0.5583)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0331 | Time 327.7141(313.2640) | Bit/dim 4.2581(4.3777) | Xent 1.7407(1.7212) | Loss 13.2622(11.2634) | Error 0.6139(0.6127) Steps 0(0.00) | Grad Norm 22.9039(13.4546) | Total Time 0.00(0.00)\n",
      "Iter 0332 | Time 299.7100(312.8574) | Bit/dim 4.2450(4.3737) | Xent 1.6833(1.7200) | Loss 10.5400(11.2417) | Error 0.5984(0.6123) Steps 0(0.00) | Grad Norm 15.3101(13.5103) | Total Time 0.00(0.00)\n",
      "Iter 0333 | Time 285.3165(312.0312) | Bit/dim 4.2290(4.3694) | Xent 1.6551(1.7181) | Loss 10.4763(11.2188) | Error 0.5964(0.6118) Steps 0(0.00) | Grad Norm 8.3532(13.3556) | Total Time 0.00(0.00)\n",
      "Iter 0334 | Time 285.9205(311.2479) | Bit/dim 4.2429(4.3656) | Xent 1.6274(1.7153) | Loss 10.4393(11.1954) | Error 0.5796(0.6108) Steps 0(0.00) | Grad Norm 6.8431(13.1602) | Total Time 0.00(0.00)\n",
      "Iter 0335 | Time 383.8890(313.4271) | Bit/dim 4.2223(4.3613) | Xent 1.6147(1.7123) | Loss 10.3388(11.1697) | Error 0.5820(0.6100) Steps 0(0.00) | Grad Norm 8.2156(13.0119) | Total Time 0.00(0.00)\n",
      "Iter 0336 | Time 320.6875(313.6449) | Bit/dim 4.2301(4.3574) | Xent 1.6118(1.7093) | Loss 10.4165(11.1471) | Error 0.5816(0.6091) Steps 0(0.00) | Grad Norm 8.4547(12.8752) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0056 | Time 92.4374, Epoch Time 2011.7283(1765.1745), Bit/dim 4.2463(best: 4.2595), Xent 1.5716, Loss 5.0321, Error 0.5571(best: 0.5583)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0337 | Time 333.0553(314.2272) | Bit/dim 4.2414(4.3539) | Xent 1.6243(1.7068) | Loss 13.3793(11.2141) | Error 0.5805(0.6083) Steps 0(0.00) | Grad Norm 11.8477(12.8443) | Total Time 0.00(0.00)\n",
      "Iter 0338 | Time 337.9386(314.9386) | Bit/dim 4.2407(4.3505) | Xent 1.6335(1.7046) | Loss 10.6641(11.1976) | Error 0.5853(0.6076) Steps 0(0.00) | Grad Norm 13.8281(12.8738) | Total Time 0.00(0.00)\n",
      "Iter 0339 | Time 317.5302(315.0163) | Bit/dim 4.2399(4.3472) | Xent 1.6346(1.7025) | Loss 10.2541(11.1693) | Error 0.5940(0.6072) Steps 0(0.00) | Grad Norm 15.7072(12.9589) | Total Time 0.00(0.00)\n",
      "Iter 0340 | Time 313.3762(314.9671) | Bit/dim 4.2154(4.3432) | Xent 1.6603(1.7012) | Loss 10.0903(11.1369) | Error 0.5881(0.6066) Steps 0(0.00) | Grad Norm 16.7532(13.0727) | Total Time 0.00(0.00)\n",
      "Iter 0341 | Time 332.4333(315.4911) | Bit/dim 4.2086(4.3392) | Xent 1.6359(1.6992) | Loss 10.6460(11.1222) | Error 0.5841(0.6059) Steps 0(0.00) | Grad Norm 10.3838(12.9920) | Total Time 0.00(0.00)\n",
      "Iter 0342 | Time 343.4314(316.3293) | Bit/dim 4.2052(4.3351) | Xent 1.6254(1.6970) | Loss 10.3456(11.0989) | Error 0.5816(0.6052) Steps 0(0.00) | Grad Norm 9.4462(12.8856) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0057 | Time 81.2518, Epoch Time 2074.8579(1774.4650), Bit/dim 4.1920(best: 4.2463), Xent 1.5689, Loss 4.9765, Error 0.5541(best: 0.5571)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0343 | Time 325.4100(316.6017) | Bit/dim 4.1996(4.3311) | Xent 1.6251(1.6949) | Loss 12.9317(11.1539) | Error 0.5854(0.6046) Steps 0(0.00) | Grad Norm 9.2758(12.7773) | Total Time 0.00(0.00)\n",
      "Iter 0344 | Time 298.0966(316.0466) | Bit/dim 4.1906(4.3269) | Xent 1.5976(1.6920) | Loss 10.0723(11.1214) | Error 0.5754(0.6037) Steps 0(0.00) | Grad Norm 8.6772(12.6543) | Total Time 0.00(0.00)\n",
      "Iter 0345 | Time 318.9733(316.1344) | Bit/dim 4.1972(4.3230) | Xent 1.6062(1.6894) | Loss 10.2742(11.0960) | Error 0.5734(0.6028) Steps 0(0.00) | Grad Norm 11.1277(12.6085) | Total Time 0.00(0.00)\n",
      "Iter 0346 | Time 350.6732(317.1705) | Bit/dim 4.1921(4.3191) | Xent 1.5912(1.6864) | Loss 10.5002(11.0781) | Error 0.5789(0.6021) Steps 0(0.00) | Grad Norm 8.7908(12.4940) | Total Time 0.00(0.00)\n",
      "Iter 0347 | Time 346.3985(318.0474) | Bit/dim 4.2003(4.3155) | Xent 1.5949(1.6837) | Loss 10.4104(11.0581) | Error 0.5760(0.6013) Steps 0(0.00) | Grad Norm 9.8351(12.4142) | Total Time 0.00(0.00)\n",
      "Iter 0348 | Time 386.3135(320.0954) | Bit/dim 4.1716(4.3112) | Xent 1.6277(1.6820) | Loss 10.2848(11.0349) | Error 0.5805(0.6007) Steps 0(0.00) | Grad Norm 19.6194(12.6304) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0058 | Time 93.2406, Epoch Time 2139.5193(1785.4166), Bit/dim 4.2019(best: 4.1920), Xent 1.6507, Loss 5.0273, Error 0.5911(best: 0.5541)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0349 | Time 325.2258(320.2493) | Bit/dim 4.2043(4.3080) | Xent 1.7024(1.6826) | Loss 13.6028(11.1119) | Error 0.5939(0.6005) Steps 0(0.00) | Grad Norm 23.9275(12.9693) | Total Time 0.00(0.00)\n",
      "Iter 0350 | Time 333.2378(320.6389) | Bit/dim 4.1562(4.3034) | Xent 1.5794(1.6795) | Loss 10.0075(11.0788) | Error 0.5699(0.5996) Steps 0(0.00) | Grad Norm 4.8076(12.7245) | Total Time 0.00(0.00)\n",
      "Iter 0351 | Time 282.3845(319.4913) | Bit/dim 4.1678(4.2993) | Xent 1.6621(1.6790) | Loss 10.2507(11.0540) | Error 0.5951(0.5994) Steps 0(0.00) | Grad Norm 14.9019(12.7898) | Total Time 0.00(0.00)\n",
      "Iter 0352 | Time 313.7000(319.3176) | Bit/dim 4.1456(4.2947) | Xent 1.6430(1.6779) | Loss 10.0295(11.0232) | Error 0.5880(0.5991) Steps 0(0.00) | Grad Norm 8.4385(12.6592) | Total Time 0.00(0.00)\n",
      "Iter 0353 | Time 326.9858(319.5476) | Bit/dim 4.1531(4.2905) | Xent 1.6085(1.6758) | Loss 9.9731(10.9917) | Error 0.5726(0.5983) Steps 0(0.00) | Grad Norm 8.1343(12.5235) | Total Time 0.00(0.00)\n",
      "Iter 0354 | Time 334.5715(319.9983) | Bit/dim 4.1541(4.2864) | Xent 1.6099(1.6739) | Loss 10.2759(10.9702) | Error 0.5775(0.5977) Steps 0(0.00) | Grad Norm 6.6940(12.3486) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0059 | Time 90.6851, Epoch Time 2022.2259(1792.5209), Bit/dim 4.1480(best: 4.1920), Xent 1.5599, Loss 4.9280, Error 0.5620(best: 0.5541)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0355 | Time 333.7811(320.4118) | Bit/dim 4.1576(4.2825) | Xent 1.6296(1.6725) | Loss 12.8012(11.0252) | Error 0.5893(0.5974) Steps 0(0.00) | Grad Norm 11.6326(12.3271) | Total Time 0.00(0.00)\n",
      "Iter 0356 | Time 298.7216(319.7611) | Bit/dim 4.3008(4.2831) | Xent 1.7679(1.6754) | Loss 10.7577(11.0171) | Error 0.6387(0.5987) Steps 0(0.00) | Grad Norm 23.7070(12.6685) | Total Time 0.00(0.00)\n",
      "Iter 0357 | Time 361.8298(321.0232) | Bit/dim 4.2127(4.2810) | Xent 1.7679(1.6782) | Loss 10.6248(11.0054) | Error 0.6361(0.5998) Steps 0(0.00) | Grad Norm 21.6418(12.9377) | Total Time 0.00(0.00)\n",
      "Iter 0358 | Time 277.7735(319.7257) | Bit/dim 4.2093(4.2788) | Xent 1.6884(1.6785) | Loss 10.2656(10.9832) | Error 0.6098(0.6001) Steps 0(0.00) | Grad Norm 12.6814(12.9300) | Total Time 0.00(0.00)\n",
      "Iter 0359 | Time 327.2911(319.9526) | Bit/dim 4.2571(4.2782) | Xent 1.6919(1.6789) | Loss 10.8075(10.9779) | Error 0.6154(0.6005) Steps 0(0.00) | Grad Norm 13.5359(12.9482) | Total Time 0.00(0.00)\n",
      "Iter 0360 | Time 317.5438(319.8804) | Bit/dim 4.1833(4.2753) | Xent 1.7336(1.6805) | Loss 10.0903(10.9513) | Error 0.6306(0.6014) Steps 0(0.00) | Grad Norm 15.1574(13.0145) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0060 | Time 82.0315, Epoch Time 2014.7627(1799.1881), Bit/dim 4.2033(best: 4.1480), Xent 1.6797, Loss 5.0432, Error 0.6049(best: 0.5541)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0361 | Time 301.1802(319.3194) | Bit/dim 4.1874(4.2727) | Xent 1.7427(1.6824) | Loss 13.3909(11.0245) | Error 0.6238(0.6021) Steps 0(0.00) | Grad Norm 18.4908(13.1788) | Total Time 0.00(0.00)\n",
      "Iter 0362 | Time 287.8458(318.3752) | Bit/dim 4.2402(4.2717) | Xent 1.6959(1.6828) | Loss 10.5629(11.0106) | Error 0.6089(0.6023) Steps 0(0.00) | Grad Norm 15.7459(13.2558) | Total Time 0.00(0.00)\n",
      "Iter 0363 | Time 294.9407(317.6721) | Bit/dim 4.1863(4.2691) | Xent 1.5905(1.6800) | Loss 10.2084(10.9866) | Error 0.5804(0.6017) Steps 0(0.00) | Grad Norm 5.5832(13.0256) | Total Time 0.00(0.00)\n",
      "Iter 0364 | Time 266.8782(316.1483) | Bit/dim 4.1653(4.2660) | Xent 1.6711(1.6798) | Loss 10.2479(10.9644) | Error 0.5969(0.6015) Steps 0(0.00) | Grad Norm 15.1255(13.0886) | Total Time 0.00(0.00)\n",
      "Iter 0365 | Time 347.6665(317.0939) | Bit/dim 4.1858(4.2636) | Xent 1.6160(1.6778) | Loss 10.4866(10.9501) | Error 0.5756(0.6007) Steps 0(0.00) | Grad Norm 8.8876(12.9626) | Total Time 0.00(0.00)\n",
      "Iter 0366 | Time 331.4615(317.5249) | Bit/dim 4.1790(4.2611) | Xent 1.6600(1.6773) | Loss 10.2068(10.9278) | Error 0.6028(0.6008) Steps 0(0.00) | Grad Norm 9.9323(12.8717) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0061 | Time 81.3236, Epoch Time 1926.8309(1803.0174), Bit/dim 4.1510(best: 4.1480), Xent 1.5669, Loss 4.9344, Error 0.5653(best: 0.5541)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0367 | Time 377.8358(319.3342) | Bit/dim 4.1686(4.2583) | Xent 1.6259(1.6758) | Loss 13.0250(10.9907) | Error 0.5851(0.6003) Steps 0(0.00) | Grad Norm 10.0373(12.7866) | Total Time 0.00(0.00)\n",
      "Iter 0368 | Time 322.0759(319.4165) | Bit/dim 4.1487(4.2550) | Xent 1.5805(1.6729) | Loss 10.2101(10.9673) | Error 0.5691(0.5994) Steps 0(0.00) | Grad Norm 5.9226(12.5807) | Total Time 0.00(0.00)\n",
      "Iter 0369 | Time 338.5232(319.9897) | Bit/dim 4.1580(4.2521) | Xent 1.6001(1.6707) | Loss 10.2389(10.9454) | Error 0.5806(0.5988) Steps 0(0.00) | Grad Norm 11.2932(12.5421) | Total Time 0.00(0.00)\n",
      "Iter 0370 | Time 334.6453(320.4293) | Bit/dim 4.1505(4.2491) | Xent 1.5870(1.6682) | Loss 10.4999(10.9321) | Error 0.5789(0.5982) Steps 0(0.00) | Grad Norm 7.1250(12.3796) | Total Time 0.00(0.00)\n",
      "Iter 0371 | Time 362.3617(321.6873) | Bit/dim 4.1172(4.2451) | Xent 1.5825(1.6656) | Loss 9.9880(10.9037) | Error 0.5740(0.5975) Steps 0(0.00) | Grad Norm 10.0437(12.3095) | Total Time 0.00(0.00)\n",
      "Iter 0372 | Time 340.2637(322.2446) | Bit/dim 4.1317(4.2417) | Xent 1.6182(1.6642) | Loss 10.1467(10.8810) | Error 0.5869(0.5972) Steps 0(0.00) | Grad Norm 11.6801(12.2906) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0062 | Time 88.9520, Epoch Time 2180.9842(1814.3564), Bit/dim 4.1238(best: 4.1480), Xent 1.5094, Loss 4.8786, Error 0.5406(best: 0.5541)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0373 | Time 341.3521(322.8178) | Bit/dim 4.1198(4.2381) | Xent 1.5721(1.6615) | Loss 12.8568(10.9403) | Error 0.5613(0.5961) Steps 0(0.00) | Grad Norm 8.0684(12.1640) | Total Time 0.00(0.00)\n",
      "Iter 0374 | Time 299.5650(322.1202) | Bit/dim 4.1004(4.2339) | Xent 1.5442(1.6579) | Loss 10.1442(10.9164) | Error 0.5665(0.5952) Steps 0(0.00) | Grad Norm 4.3039(11.9282) | Total Time 0.00(0.00)\n",
      "Iter 0375 | Time 329.4484(322.3401) | Bit/dim 4.1004(4.2299) | Xent 1.5565(1.6549) | Loss 10.2137(10.8953) | Error 0.5577(0.5941) Steps 0(0.00) | Grad Norm 6.7713(11.7734) | Total Time 0.00(0.00)\n",
      "Iter 0376 | Time 348.2189(323.1164) | Bit/dim 4.0970(4.2259) | Xent 1.5453(1.6516) | Loss 9.9218(10.8661) | Error 0.5526(0.5928) Steps 0(0.00) | Grad Norm 5.6744(11.5905) | Total Time 0.00(0.00)\n",
      "Iter 0377 | Time 324.2432(323.1502) | Bit/dim 4.0871(4.2218) | Xent 1.5330(1.6480) | Loss 9.8592(10.8359) | Error 0.5549(0.5917) Steps 0(0.00) | Grad Norm 3.6176(11.3513) | Total Time 0.00(0.00)\n",
      "Iter 0378 | Time 344.8462(323.8011) | Bit/dim 4.0917(4.2179) | Xent 1.5382(1.6448) | Loss 9.8778(10.8072) | Error 0.5516(0.5905) Steps 0(0.00) | Grad Norm 4.3395(11.1409) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0063 | Time 83.2041, Epoch Time 2086.8266(1822.5305), Bit/dim 4.0847(best: 4.1238), Xent 1.4713, Loss 4.8204, Error 0.5306(best: 0.5406)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0379 | Time 309.7912(323.3808) | Bit/dim 4.0773(4.2136) | Xent 1.5075(1.6406) | Loss 12.7318(10.8649) | Error 0.5454(0.5892) Steps 0(0.00) | Grad Norm 3.8632(10.9226) | Total Time 0.00(0.00)\n",
      "Iter 0380 | Time 393.7761(325.4927) | Bit/dim 4.0834(4.2097) | Xent 1.5462(1.6378) | Loss 10.2473(10.8464) | Error 0.5529(0.5881) Steps 0(0.00) | Grad Norm 9.1994(10.8709) | Total Time 0.00(0.00)\n",
      "Iter 0381 | Time 323.1413(325.4221) | Bit/dim 4.1285(4.2073) | Xent 1.6429(1.6380) | Loss 10.1525(10.8256) | Error 0.5899(0.5881) Steps 0(0.00) | Grad Norm 20.1789(11.1501) | Total Time 0.00(0.00)\n",
      "Iter 0382 | Time 311.9239(325.0172) | Bit/dim 4.3186(4.2106) | Xent 1.8246(1.6436) | Loss 10.9128(10.8282) | Error 0.6515(0.5900) Steps 0(0.00) | Grad Norm 25.8605(11.5915) | Total Time 0.00(0.00)\n",
      "Iter 0383 | Time 311.3229(324.6064) | Bit/dim 4.1515(4.2089) | Xent 1.6992(1.6452) | Loss 10.4522(10.8169) | Error 0.6116(0.5907) Steps 0(0.00) | Grad Norm 13.0708(11.6358) | Total Time 0.00(0.00)\n",
      "Iter 0384 | Time 357.9921(325.6079) | Bit/dim 4.3598(4.2134) | Xent 2.3215(1.6655) | Loss 11.3694(10.8335) | Error 0.7109(0.5943) Steps 0(0.00) | Grad Norm 63.1747(13.1820) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0064 | Time 80.3257, Epoch Time 2104.4346(1830.9877), Bit/dim 4.2780(best: 4.0847), Xent 1.5983, Loss 5.0771, Error 0.5728(best: 0.5306)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0385 | Time 321.9327(325.4977) | Bit/dim 4.2844(4.2155) | Xent 1.6671(1.6656) | Loss 13.1137(10.9019) | Error 0.5954(0.5943) Steps 0(0.00) | Grad Norm 13.6384(13.1957) | Total Time 0.00(0.00)\n",
      "Iter 0386 | Time 328.5595(325.5895) | Bit/dim 4.3218(4.2187) | Xent 1.6936(1.6664) | Loss 10.7649(10.8978) | Error 0.6125(0.5949) Steps 0(0.00) | Grad Norm 8.9887(13.0695) | Total Time 0.00(0.00)\n",
      "Iter 0387 | Time 340.3167(326.0313) | Bit/dim 4.2018(4.2182) | Xent 1.7426(1.6687) | Loss 10.6694(10.8909) | Error 0.6305(0.5959) Steps 0(0.00) | Grad Norm 14.8228(13.1221) | Total Time 0.00(0.00)\n",
      "Iter 0388 | Time 337.0222(326.3611) | Bit/dim 4.2314(4.2186) | Xent 1.7192(1.6702) | Loss 10.2724(10.8724) | Error 0.6146(0.5965) Steps 0(0.00) | Grad Norm 9.8606(13.0242) | Total Time 0.00(0.00)\n",
      "Iter 0389 | Time 308.8682(325.8363) | Bit/dim 4.2146(4.2185) | Xent 1.8648(1.6760) | Loss 10.4131(10.8586) | Error 0.6589(0.5984) Steps 0(0.00) | Grad Norm 31.5705(13.5806) | Total Time 0.00(0.00)\n",
      "Iter 0390 | Time 300.4034(325.0733) | Bit/dim 4.2583(4.2197) | Xent 1.6726(1.6759) | Loss 10.6849(10.8534) | Error 0.6041(0.5985) Steps 0(0.00) | Grad Norm 11.6500(13.5227) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0065 | Time 77.6080, Epoch Time 2030.4309(1836.9710), Bit/dim 4.1715(best: 4.0847), Xent 1.6123, Loss 4.9777, Error 0.5891(best: 0.5306)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0391 | Time 330.6843(325.2416) | Bit/dim 4.1730(4.2183) | Xent 1.6615(1.6755) | Loss 13.0414(10.9190) | Error 0.6074(0.5988) Steps 0(0.00) | Grad Norm 10.3185(13.4266) | Total Time 0.00(0.00)\n",
      "Iter 0392 | Time 385.9414(327.0626) | Bit/dim 4.1780(4.2171) | Xent 1.7107(1.6766) | Loss 10.5637(10.9084) | Error 0.6265(0.5996) Steps 0(0.00) | Grad Norm 12.8257(13.4086) | Total Time 0.00(0.00)\n",
      "Iter 0393 | Time 325.0021(327.0008) | Bit/dim 4.1469(4.2150) | Xent 1.6554(1.6759) | Loss 10.3873(10.8927) | Error 0.5941(0.5995) Steps 0(0.00) | Grad Norm 6.7594(13.2091) | Total Time 0.00(0.00)\n",
      "Iter 0394 | Time 340.1576(327.3955) | Bit/dim 4.1433(4.2128) | Xent 1.5820(1.6731) | Loss 10.2400(10.8731) | Error 0.5695(0.5986) Steps 0(0.00) | Grad Norm 5.0175(12.9633) | Total Time 0.00(0.00)\n",
      "Iter 0395 | Time 314.1901(326.9993) | Bit/dim 4.1631(4.2113) | Xent 1.5872(1.6705) | Loss 10.1027(10.8500) | Error 0.5733(0.5978) Steps 0(0.00) | Grad Norm 4.9931(12.7242) | Total Time 0.00(0.00)\n",
      "Iter 0396 | Time 350.0661(327.6913) | Bit/dim 4.1451(4.2093) | Xent 1.5892(1.6681) | Loss 10.2723(10.8327) | Error 0.5782(0.5972) Steps 0(0.00) | Grad Norm 6.1802(12.5279) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0066 | Time 79.8730, Epoch Time 2142.0151(1846.1223), Bit/dim 4.1213(best: 4.0847), Xent 1.5249, Loss 4.8837, Error 0.5599(best: 0.5306)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0397 | Time 327.1509(327.6751) | Bit/dim 4.1042(4.2062) | Xent 1.5817(1.6655) | Loss 12.9328(10.8957) | Error 0.5694(0.5964) Steps 0(0.00) | Grad Norm 5.8971(12.3290) | Total Time 0.00(0.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_rl.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 8000 --test_batch_size 5000 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_rl_stdscale_15_run1 --seed 1 --conditional True --controlled_tol True --train_mode semisup --lr 0.01 --warmup_iters 1000 --atol 1e-4  --rtol 1e-4 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5 --scale_fac 1.0 --scale_std 15.0 --max_grad_norm 20.0\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
