{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_rl.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time, count_nfe_gate\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "GATES = [\"cnn1\", \"cnn2\", \"rnn\"]\n",
      "\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--gate', type=str, default='cnn1', choices=GATES)\n",
      "parser.add_argument('--scale', type=float, default=1.0)\n",
      "parser.add_argument('--scale_fac', type=float, default=1.0)\n",
      "parser.add_argument('--scale_std', type=float, default=1.0)\n",
      "parser.add_argument('--eta', default=0.1, type=float,\n",
      "                        help='tuning parameter that allows us to trade-off the competing goals of' \n",
      "                                'minimizing the prediction loss and maximizing the gate rewards ')\n",
      "parser.add_argument('--rl-weight', default=0.01, type=float,\n",
      "                        help='rl weight')\n",
      "\n",
      "parser.add_argument('--gamma', default=0.99, type=float,\n",
      "                        help='discount factor, default: (0.99)')\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=500)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "parser.add_argument(\"--annealing_std\", type=eval, default=False, choices=[True, False])\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "import lib.odenvp_conditional_rl as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "        \n",
      "def update_scale_std(model, epoch):\n",
      "    epoch_frac = 1.0 - float(epoch - 1) / max(args.num_epochs + 1, 1)\n",
      "    scale_std = args.scale_std * epoch_frac\n",
      "    model.set_scale_std(scale_std)\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol, \"scale\": args.scale, \"scale_fac\": args.scale_fac, \"scale_std\": args.scale_std, \"gate\": args.gate},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        if args.annealing_std:\n",
      "            update_scale_std(model.module, epoch)\n",
      "            \n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            # re-weight the gate rewards\n",
      "            normalized_eta = args.eta / len(logp_actions)\n",
      "            \n",
      "            # collect cumulative future rewards\n",
      "            R = - loss\n",
      "            cum_rewards = []\n",
      "            for r in nfe[::-1]:\n",
      "                R = -normalized_eta * r.view(-1,1) + args.gamma * R\n",
      "                cum_rewards.insert(0,R)\n",
      "            \n",
      "            # apply REINFORCE\n",
      "            rl_loss = 0\n",
      "            for lpa, r in zip(logp_actions, cum_rewards):\n",
      "                rl_loss = rl_loss - lpa.view(-1,1) * args.rl_weight * r\n",
      "                \n",
      "            loss = loss + rl_loss.mean()\n",
      "            \n",
      "            loss.backward()\n",
      "            \n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe_gate(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            for idx in range(len(model.module.transforms)):\n",
      "                for layer in model.module.transforms[idx].chain:\n",
      "                    if hasattr(layer, 'atol'):\n",
      "                        layer.odefunc.after_odeint()\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'train': atol[tol_indx].mean()}, itr)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'train': rtol[tol_indx].mean()}, itr)\n",
      "                    \n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples, atol, rtol, logp_actions, nfe = model(fixed_z, reverse=True)\n",
      "            generated_samples = generated_samples.view(-1, *data_shape)\n",
      "            for tol_indx in range(len(atol)):\n",
      "                writer.add_scalars('atol_gen_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                writer.add_scalars('rtol_gen_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, annealing_std=False, atol=0.0001, autoencode=False, batch_norm=False, batch_size=8000, batch_size_schedule='', begin_epoch=1, condition_ratio=0.5, conditional=True, controlled_tol=True, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, eta=0.1, gamma=0.99, gate='cnn2', imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=1, lr=0.001, max_grad_norm=20.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=500, parallel=False, rademacher=True, residual=False, resume='../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_rl_stdlearnscale_30_run1/epoch_190_checkpt.pth', rl_weight=0.01, rtol=0.0001, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_rl_stdlearnscale_30_run1', scale=1.0, scale_fac=1.0, scale_std=30.0, seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=5000, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000.0, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1450886\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc_rl.py:291: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Iter 1141 | Time 118.0653(59.9669) | Bit/dim 3.7076(3.7110) | Xent 0.9817(1.0154) | Loss 11.4554(9.8939) | Error 0.3439(0.3619) Steps 610(631.83) | Grad Norm 2.6041(3.2628) | Total Time 0.00(0.00)\n",
      "Iter 1142 | Time 60.0604(59.9697) | Bit/dim 3.7185(3.7112) | Xent 1.0114(1.0153) | Loss 9.4318(9.8800) | Error 0.3612(0.3619) Steps 658(632.61) | Grad Norm 7.9675(3.4039) | Total Time 0.00(0.00)\n",
      "Iter 1143 | Time 57.8920(59.9073) | Bit/dim 3.6946(3.7107) | Xent 1.0243(1.0156) | Loss 9.4196(9.8662) | Error 0.3629(0.3619) Steps 634(632.65) | Grad Norm 4.5881(3.4394) | Total Time 0.00(0.00)\n",
      "Iter 1144 | Time 60.8173(59.9346) | Bit/dim 3.7186(3.7110) | Xent 1.0151(1.0156) | Loss 9.2571(9.8479) | Error 0.3625(0.3619) Steps 622(632.33) | Grad Norm 6.1245(3.5200) | Total Time 0.00(0.00)\n",
      "Iter 1145 | Time 61.3271(59.9764) | Bit/dim 3.7029(3.7107) | Xent 1.0242(1.0158) | Loss 9.3513(9.8330) | Error 0.3685(0.3621) Steps 652(632.92) | Grad Norm 4.2828(3.5429) | Total Time 0.00(0.00)\n",
      "Iter 1146 | Time 56.0253(59.8579) | Bit/dim 3.7143(3.7108) | Xent 1.0280(1.0162) | Loss 9.3274(9.8179) | Error 0.3695(0.3624) Steps 628(632.78) | Grad Norm 6.2119(3.6229) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0191 | Time 39.2125, Epoch Time 473.4963(387.8294), Bit/dim 3.7088(best: inf), Xent 0.9929, Loss 4.2053, Error 0.3530(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1147 | Time 67.8250(60.0969) | Bit/dim 3.7161(3.7110) | Xent 1.0099(1.0160) | Loss 12.5905(9.9010) | Error 0.3672(0.3625) Steps 628(632.63) | Grad Norm 3.7640(3.6272) | Total Time 0.00(0.00)\n",
      "Iter 1148 | Time 62.5853(60.1715) | Bit/dim 3.7081(3.7109) | Xent 1.0084(1.0158) | Loss 9.4448(9.8873) | Error 0.3575(0.3624) Steps 622(632.31) | Grad Norm 4.1716(3.6435) | Total Time 0.00(0.00)\n",
      "Iter 1149 | Time 55.7503(60.0389) | Bit/dim 3.7030(3.7107) | Xent 1.0291(1.0162) | Loss 9.1774(9.8661) | Error 0.3665(0.3625) Steps 634(632.36) | Grad Norm 6.1517(3.7187) | Total Time 0.00(0.00)\n",
      "Iter 1150 | Time 57.7992(59.9717) | Bit/dim 3.7071(3.7106) | Xent 1.0029(1.0158) | Loss 9.3207(9.8497) | Error 0.3575(0.3623) Steps 658(633.13) | Grad Norm 3.2502(3.7047) | Total Time 0.00(0.00)\n",
      "Iter 1151 | Time 59.6935(59.9634) | Bit/dim 3.7113(3.7106) | Xent 1.0189(1.0159) | Loss 9.3702(9.8353) | Error 0.3690(0.3625) Steps 610(632.44) | Grad Norm 9.4260(3.8763) | Total Time 0.00(0.00)\n",
      "Iter 1152 | Time 56.1075(59.8477) | Bit/dim 3.6951(3.7101) | Xent 1.0122(1.0158) | Loss 9.3774(9.8216) | Error 0.3624(0.3625) Steps 634(632.49) | Grad Norm 3.7709(3.8732) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0192 | Time 24.0801, Epoch Time 401.1921(388.2302), Bit/dim 3.7089(best: 3.7088), Xent 1.0096, Loss 4.2137, Error 0.3538(best: 0.3530)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1153 | Time 60.4435(59.8656) | Bit/dim 3.6968(3.7097) | Xent 1.0106(1.0156) | Loss 12.4555(9.9006) | Error 0.3500(0.3621) Steps 652(633.07) | Grad Norm 7.5035(3.9821) | Total Time 0.00(0.00)\n",
      "Iter 1154 | Time 54.5176(59.7051) | Bit/dim 3.7034(3.7095) | Xent 0.9857(1.0147) | Loss 9.2162(9.8801) | Error 0.3486(0.3617) Steps 610(632.38) | Grad Norm 4.5366(3.9987) | Total Time 0.00(0.00)\n",
      "Iter 1155 | Time 55.9815(59.5934) | Bit/dim 3.7134(3.7096) | Xent 1.0076(1.0145) | Loss 9.0891(9.8563) | Error 0.3554(0.3616) Steps 646(632.79) | Grad Norm 5.1507(4.0333) | Total Time 0.00(0.00)\n",
      "Iter 1156 | Time 58.0786(59.5480) | Bit/dim 3.7070(3.7096) | Xent 1.0062(1.0143) | Loss 9.3802(9.8420) | Error 0.3586(0.3615) Steps 634(632.82) | Grad Norm 4.4630(4.0462) | Total Time 0.00(0.00)\n",
      "Iter 1157 | Time 60.7878(59.5852) | Bit/dim 3.7030(3.7094) | Xent 1.0245(1.0146) | Loss 9.2694(9.8249) | Error 0.3748(0.3619) Steps 610(632.14) | Grad Norm 3.8569(4.0405) | Total Time 0.00(0.00)\n",
      "Iter 1158 | Time 59.3293(59.5775) | Bit/dim 3.7042(3.7092) | Xent 0.9990(1.0141) | Loss 9.4362(9.8132) | Error 0.3630(0.3619) Steps 646(632.56) | Grad Norm 3.2423(4.0165) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0193 | Time 24.2674, Epoch Time 389.5802(388.2707), Bit/dim 3.7031(best: 3.7088), Xent 0.9952, Loss 4.2007, Error 0.3541(best: 0.3530)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1159 | Time 62.3228(59.6599) | Bit/dim 3.7000(3.7089) | Xent 1.0114(1.0140) | Loss 12.7137(9.9002) | Error 0.3575(0.3618) Steps 640(632.78) | Grad Norm 2.9858(3.9856) | Total Time 0.00(0.00)\n",
      "Iter 1160 | Time 65.8010(59.8441) | Bit/dim 3.7031(3.7088) | Xent 0.9892(1.0133) | Loss 9.3869(9.8848) | Error 0.3528(0.3615) Steps 646(633.18) | Grad Norm 1.9488(3.9245) | Total Time 0.00(0.00)\n",
      "Iter 1161 | Time 66.5220(60.0444) | Bit/dim 3.7023(3.7086) | Xent 0.9963(1.0128) | Loss 9.0636(9.8602) | Error 0.3492(0.3611) Steps 610(632.48) | Grad Norm 2.8798(3.8932) | Total Time 0.00(0.00)\n",
      "Iter 1162 | Time 59.8455(60.0385) | Bit/dim 3.7069(3.7085) | Xent 0.9943(1.0122) | Loss 9.3667(9.8454) | Error 0.3521(0.3609) Steps 628(632.35) | Grad Norm 3.5622(3.8832) | Total Time 0.00(0.00)\n",
      "Iter 1163 | Time 51.2901(59.7760) | Bit/dim 3.7142(3.7087) | Xent 1.0057(1.0120) | Loss 9.0966(9.8229) | Error 0.3560(0.3607) Steps 610(631.68) | Grad Norm 2.7040(3.8479) | Total Time 0.00(0.00)\n",
      "Iter 1164 | Time 54.5850(59.6203) | Bit/dim 3.7012(3.7085) | Xent 1.0009(1.0117) | Loss 9.3742(9.8095) | Error 0.3574(0.3606) Steps 610(631.03) | Grad Norm 3.8865(3.8490) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0194 | Time 23.8133, Epoch Time 400.7612(388.6455), Bit/dim 3.7023(best: 3.7031), Xent 0.9834, Loss 4.1940, Error 0.3485(best: 0.3530)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1165 | Time 59.5455(59.6180) | Bit/dim 3.6935(3.7080) | Xent 1.0011(1.0114) | Loss 12.7035(9.8963) | Error 0.3549(0.3604) Steps 610(630.39) | Grad Norm 3.1146(3.8270) | Total Time 0.00(0.00)\n",
      "Iter 1166 | Time 58.5402(59.5857) | Bit/dim 3.7020(3.7078) | Xent 0.9993(1.0110) | Loss 9.4329(9.8824) | Error 0.3554(0.3603) Steps 634(630.50) | Grad Norm 2.8182(3.7967) | Total Time 0.00(0.00)\n",
      "Iter 1167 | Time 62.5719(59.6753) | Bit/dim 3.7101(3.7079) | Xent 0.9747(1.0099) | Loss 9.4578(9.8696) | Error 0.3525(0.3601) Steps 658(631.33) | Grad Norm 3.8697(3.7989) | Total Time 0.00(0.00)\n",
      "Iter 1168 | Time 59.3799(59.6664) | Bit/dim 3.7091(3.7079) | Xent 0.9932(1.0094) | Loss 9.3793(9.8549) | Error 0.3591(0.3600) Steps 622(631.05) | Grad Norm 2.3470(3.7554) | Total Time 0.00(0.00)\n",
      "Iter 1169 | Time 64.7963(59.8203) | Bit/dim 3.7164(3.7082) | Xent 0.9896(1.0088) | Loss 9.5380(9.8454) | Error 0.3559(0.3599) Steps 670(632.22) | Grad Norm 4.0153(3.7632) | Total Time 0.00(0.00)\n",
      "Iter 1170 | Time 57.3233(59.7454) | Bit/dim 3.6949(3.7078) | Xent 1.0082(1.0088) | Loss 9.3408(9.8303) | Error 0.3571(0.3598) Steps 634(632.27) | Grad Norm 2.9246(3.7380) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0195 | Time 23.7544, Epoch Time 402.3199(389.0557), Bit/dim 3.7073(best: 3.7023), Xent 0.9772, Loss 4.1959, Error 0.3487(best: 0.3485)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1171 | Time 57.4766(59.6773) | Bit/dim 3.6991(3.7075) | Xent 0.9980(1.0085) | Loss 12.5172(9.9109) | Error 0.3571(0.3597) Steps 646(632.68) | Grad Norm 2.3451(3.6962) | Total Time 0.00(0.00)\n",
      "Iter 1172 | Time 60.2996(59.6960) | Bit/dim 3.7120(3.7077) | Xent 0.9883(1.0079) | Loss 9.3520(9.8941) | Error 0.3516(0.3595) Steps 616(632.18) | Grad Norm 4.2577(3.7131) | Total Time 0.00(0.00)\n",
      "Iter 1173 | Time 57.1697(59.6202) | Bit/dim 3.7070(3.7076) | Xent 0.9895(1.0073) | Loss 9.1987(9.8733) | Error 0.3558(0.3594) Steps 604(631.34) | Grad Norm 3.5551(3.7083) | Total Time 0.00(0.00)\n",
      "Iter 1174 | Time 53.9097(59.4489) | Bit/dim 3.6938(3.7072) | Xent 0.9976(1.0070) | Loss 9.3014(9.8561) | Error 0.3489(0.3591) Steps 622(631.06) | Grad Norm 3.1377(3.6912) | Total Time 0.00(0.00)\n",
      "Iter 1175 | Time 58.3726(59.4166) | Bit/dim 3.7090(3.7073) | Xent 0.9853(1.0064) | Loss 9.4614(9.8443) | Error 0.3569(0.3590) Steps 640(631.32) | Grad Norm 3.8695(3.6965) | Total Time 0.00(0.00)\n",
      "Iter 1176 | Time 57.5531(59.3607) | Bit/dim 3.7120(3.7074) | Xent 0.9924(1.0060) | Loss 9.3244(9.8287) | Error 0.3488(0.3587) Steps 616(630.86) | Grad Norm 3.5302(3.6916) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0196 | Time 23.1319, Epoch Time 384.2097(388.9103), Bit/dim 3.7067(best: 3.7023), Xent 0.9761, Loss 4.1947, Error 0.3463(best: 0.3485)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1177 | Time 55.7714(59.2530) | Bit/dim 3.6970(3.7071) | Xent 0.9827(1.0053) | Loss 11.8229(9.8885) | Error 0.3522(0.3585) Steps 652(631.50) | Grad Norm 2.3346(3.6509) | Total Time 0.00(0.00)\n",
      "Iter 1178 | Time 59.9990(59.2754) | Bit/dim 3.6915(3.7066) | Xent 0.9723(1.0043) | Loss 9.2185(9.8684) | Error 0.3515(0.3583) Steps 658(632.29) | Grad Norm 3.5144(3.6468) | Total Time 0.00(0.00)\n",
      "Iter 1179 | Time 57.3150(59.2166) | Bit/dim 3.7113(3.7068) | Xent 0.9803(1.0036) | Loss 9.0506(9.8439) | Error 0.3469(0.3579) Steps 610(631.62) | Grad Norm 2.2498(3.6048) | Total Time 0.00(0.00)\n",
      "Iter 1180 | Time 59.8183(59.2347) | Bit/dim 3.7020(3.7066) | Xent 1.0064(1.0036) | Loss 9.3151(9.8280) | Error 0.3591(0.3580) Steps 634(631.70) | Grad Norm 3.6835(3.6072) | Total Time 0.00(0.00)\n",
      "Iter 1181 | Time 57.2321(59.1746) | Bit/dim 3.7136(3.7069) | Xent 0.9972(1.0034) | Loss 9.4871(9.8178) | Error 0.3588(0.3580) Steps 610(631.04) | Grad Norm 4.1449(3.6233) | Total Time 0.00(0.00)\n",
      "Iter 1182 | Time 57.3100(59.1186) | Bit/dim 3.6998(3.7066) | Xent 0.9791(1.0027) | Loss 9.1854(9.7988) | Error 0.3480(0.3577) Steps 652(631.67) | Grad Norm 1.3571(3.5554) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0197 | Time 22.6175, Epoch Time 386.1308(388.8269), Bit/dim 3.7050(best: 3.7023), Xent 0.9777, Loss 4.1938, Error 0.3476(best: 0.3463)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1183 | Time 58.3283(59.0949) | Bit/dim 3.7137(3.7069) | Xent 0.9774(1.0020) | Loss 12.3097(9.8741) | Error 0.3510(0.3575) Steps 640(631.92) | Grad Norm 3.5621(3.5556) | Total Time 0.00(0.00)\n",
      "Iter 1184 | Time 57.2678(59.0401) | Bit/dim 3.6962(3.7065) | Xent 0.9583(1.0006) | Loss 9.2420(9.8552) | Error 0.3407(0.3570) Steps 628(631.81) | Grad Norm 2.8477(3.5343) | Total Time 0.00(0.00)\n",
      "Iter 1185 | Time 60.2434(59.0762) | Bit/dim 3.7043(3.7065) | Xent 0.9860(1.0002) | Loss 9.4968(9.8444) | Error 0.3499(0.3568) Steps 604(630.97) | Grad Norm 2.9938(3.5181) | Total Time 0.00(0.00)\n",
      "Iter 1186 | Time 58.8214(59.0686) | Bit/dim 3.7137(3.7067) | Xent 0.9957(1.0001) | Loss 9.2469(9.8265) | Error 0.3568(0.3568) Steps 652(631.60) | Grad Norm 2.2022(3.4786) | Total Time 0.00(0.00)\n",
      "Iter 1187 | Time 58.6675(59.0565) | Bit/dim 3.7089(3.7068) | Xent 0.9869(0.9997) | Loss 9.3184(9.8112) | Error 0.3525(0.3567) Steps 634(631.67) | Grad Norm 3.7078(3.4855) | Total Time 0.00(0.00)\n",
      "Iter 1188 | Time 56.5684(58.9819) | Bit/dim 3.7001(3.7066) | Xent 0.9937(0.9995) | Loss 9.4052(9.7991) | Error 0.3516(0.3565) Steps 622(631.38) | Grad Norm 3.8229(3.4956) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0198 | Time 23.5445, Epoch Time 389.6011(388.8502), Bit/dim 3.7042(best: 3.7023), Xent 0.9701, Loss 4.1893, Error 0.3466(best: 0.3463)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1189 | Time 57.6724(58.9426) | Bit/dim 3.7064(3.7065) | Xent 0.9798(0.9989) | Loss 12.2280(9.8719) | Error 0.3427(0.3561) Steps 646(631.82) | Grad Norm 1.2971(3.4297) | Total Time 0.00(0.00)\n",
      "Iter 1190 | Time 58.2008(58.9204) | Bit/dim 3.6970(3.7063) | Xent 0.9935(0.9987) | Loss 9.3436(9.8561) | Error 0.3470(0.3558) Steps 628(631.71) | Grad Norm 2.9197(3.4144) | Total Time 0.00(0.00)\n",
      "Iter 1191 | Time 57.7845(58.8863) | Bit/dim 3.7060(3.7063) | Xent 0.9758(0.9981) | Loss 9.2755(9.8387) | Error 0.3510(0.3557) Steps 634(631.78) | Grad Norm 2.9763(3.4012) | Total Time 0.00(0.00)\n",
      "Iter 1192 | Time 55.8124(58.7941) | Bit/dim 3.7062(3.7062) | Xent 0.9913(0.9979) | Loss 8.7968(9.8074) | Error 0.3544(0.3556) Steps 634(631.84) | Grad Norm 2.5320(3.3752) | Total Time 0.00(0.00)\n",
      "Iter 1193 | Time 58.2692(58.7783) | Bit/dim 3.7024(3.7061) | Xent 0.9774(0.9972) | Loss 9.3356(9.7933) | Error 0.3485(0.3554) Steps 658(632.63) | Grad Norm 2.2393(3.3411) | Total Time 0.00(0.00)\n",
      "Iter 1194 | Time 57.1720(58.7301) | Bit/dim 3.7054(3.7061) | Xent 0.9769(0.9966) | Loss 9.2445(9.7768) | Error 0.3506(0.3553) Steps 646(633.03) | Grad Norm 2.3408(3.3111) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0199 | Time 22.7875, Epoch Time 383.9185(388.7022), Bit/dim 3.7078(best: 3.7023), Xent 0.9795, Loss 4.1976, Error 0.3505(best: 0.3463)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1195 | Time 55.8460(58.6436) | Bit/dim 3.7108(3.7063) | Xent 0.9809(0.9962) | Loss 12.6290(9.8624) | Error 0.3510(0.3552) Steps 640(633.24) | Grad Norm 4.4309(3.3447) | Total Time 0.00(0.00)\n",
      "Iter 1196 | Time 62.5913(58.7620) | Bit/dim 3.7059(3.7062) | Xent 0.9758(0.9955) | Loss 9.3983(9.8484) | Error 0.3466(0.3549) Steps 652(633.80) | Grad Norm 4.0641(3.3662) | Total Time 0.00(0.00)\n",
      "Iter 1197 | Time 60.0760(58.8015) | Bit/dim 3.7007(3.7061) | Xent 0.9774(0.9950) | Loss 9.0753(9.8252) | Error 0.3521(0.3548) Steps 640(633.99) | Grad Norm 1.2927(3.3040) | Total Time 0.00(0.00)\n",
      "Iter 1198 | Time 56.9324(58.7454) | Bit/dim 3.7084(3.7061) | Xent 0.9961(0.9950) | Loss 9.3239(9.8102) | Error 0.3562(0.3549) Steps 628(633.81) | Grad Norm 6.0157(3.3854) | Total Time 0.00(0.00)\n",
      "Iter 1199 | Time 60.5072(58.7982) | Bit/dim 3.6917(3.7057) | Xent 1.0106(0.9955) | Loss 9.3657(9.7969) | Error 0.3606(0.3550) Steps 616(633.27) | Grad Norm 7.0551(3.4955) | Total Time 0.00(0.00)\n",
      "Iter 1200 | Time 55.4033(58.6964) | Bit/dim 3.6973(3.7055) | Xent 0.9978(0.9956) | Loss 9.3047(9.7821) | Error 0.3550(0.3550) Steps 628(633.12) | Grad Norm 3.3878(3.4922) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0200 | Time 23.6547, Epoch Time 390.8684(388.7672), Bit/dim 3.7027(best: 3.7023), Xent 0.9727, Loss 4.1890, Error 0.3466(best: 0.3463)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1201 | Time 55.3724(58.5967) | Bit/dim 3.7004(3.7053) | Xent 0.9869(0.9953) | Loss 12.4698(9.8627) | Error 0.3505(0.3549) Steps 604(632.24) | Grad Norm 4.0281(3.5083) | Total Time 0.00(0.00)\n",
      "Iter 1202 | Time 62.5514(58.7153) | Bit/dim 3.6940(3.7050) | Xent 1.0225(0.9961) | Loss 9.1969(9.8428) | Error 0.3605(0.3551) Steps 622(631.93) | Grad Norm 5.6188(3.5716) | Total Time 0.00(0.00)\n",
      "Iter 1203 | Time 53.8086(58.5681) | Bit/dim 3.6992(3.7048) | Xent 0.9958(0.9961) | Loss 9.0018(9.8175) | Error 0.3570(0.3551) Steps 634(632.00) | Grad Norm 4.4326(3.5975) | Total Time 0.00(0.00)\n",
      "Iter 1204 | Time 60.5376(58.6272) | Bit/dim 3.7040(3.7048) | Xent 0.9847(0.9958) | Loss 9.4022(9.8051) | Error 0.3476(0.3549) Steps 634(632.06) | Grad Norm 2.3157(3.5590) | Total Time 0.00(0.00)\n",
      "Iter 1205 | Time 56.3353(58.5584) | Bit/dim 3.7030(3.7047) | Xent 0.9997(0.9959) | Loss 9.3555(9.7916) | Error 0.3592(0.3550) Steps 622(631.75) | Grad Norm 5.5136(3.6177) | Total Time 0.00(0.00)\n",
      "Iter 1206 | Time 66.4313(58.7946) | Bit/dim 3.7148(3.7050) | Xent 0.9891(0.9957) | Loss 9.4044(9.7800) | Error 0.3556(0.3550) Steps 592(630.56) | Grad Norm 6.0739(3.6913) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0201 | Time 23.5838, Epoch Time 394.7642(388.9471), Bit/dim 3.7045(best: 3.7023), Xent 0.9842, Loss 4.1966, Error 0.3491(best: 0.3463)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1207 | Time 54.8108(58.6751) | Bit/dim 3.7057(3.7050) | Xent 1.0139(0.9962) | Loss 12.3835(9.8581) | Error 0.3611(0.3552) Steps 628(630.49) | Grad Norm 4.5529(3.7172) | Total Time 0.00(0.00)\n",
      "Iter 1208 | Time 59.4002(58.6969) | Bit/dim 3.6961(3.7048) | Xent 1.0281(0.9972) | Loss 9.5369(9.8484) | Error 0.3645(0.3555) Steps 658(631.31) | Grad Norm 7.1052(3.8188) | Total Time 0.00(0.00)\n",
      "Iter 1209 | Time 57.4167(58.6584) | Bit/dim 3.6985(3.7046) | Xent 0.9989(0.9972) | Loss 9.4167(9.8355) | Error 0.3594(0.3556) Steps 646(631.75) | Grad Norm 5.7664(3.8773) | Total Time 0.00(0.00)\n",
      "Iter 1210 | Time 55.4871(58.5633) | Bit/dim 3.7035(3.7046) | Xent 0.9775(0.9966) | Loss 9.4923(9.8252) | Error 0.3450(0.3553) Steps 634(631.82) | Grad Norm 5.1213(3.9146) | Total Time 0.00(0.00)\n",
      "Iter 1211 | Time 60.0518(58.6080) | Bit/dim 3.7072(3.7046) | Xent 0.9874(0.9964) | Loss 9.3235(9.8101) | Error 0.3508(0.3552) Steps 622(631.52) | Grad Norm 6.6950(3.9980) | Total Time 0.00(0.00)\n",
      "Iter 1212 | Time 56.2125(58.5361) | Bit/dim 3.7152(3.7050) | Xent 0.9892(0.9962) | Loss 9.3957(9.7977) | Error 0.3550(0.3552) Steps 628(631.42) | Grad Norm 3.3138(3.9775) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0202 | Time 23.3012, Epoch Time 385.6218(388.8473), Bit/dim 3.7050(best: 3.7023), Xent 0.9966, Loss 4.2033, Error 0.3543(best: 0.3463)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1213 | Time 58.2780(58.5284) | Bit/dim 3.7058(3.7050) | Xent 1.0018(0.9963) | Loss 11.9984(9.8637) | Error 0.3576(0.3552) Steps 658(632.22) | Grad Norm 5.8665(4.0341) | Total Time 0.00(0.00)\n",
      "Iter 1214 | Time 58.1395(58.5167) | Bit/dim 3.6955(3.7047) | Xent 0.9989(0.9964) | Loss 9.4282(9.8507) | Error 0.3491(0.3550) Steps 640(632.45) | Grad Norm 3.5209(4.0187) | Total Time 0.00(0.00)\n",
      "Iter 1215 | Time 56.8650(58.4671) | Bit/dim 3.6898(3.7042) | Xent 0.9908(0.9962) | Loss 8.9774(9.8245) | Error 0.3590(0.3552) Steps 622(632.14) | Grad Norm 4.0077(4.0184) | Total Time 0.00(0.00)\n",
      "Iter 1216 | Time 53.3346(58.3132) | Bit/dim 3.7076(3.7043) | Xent 0.9742(0.9956) | Loss 9.3837(9.8112) | Error 0.3409(0.3547) Steps 640(632.37) | Grad Norm 2.3503(3.9684) | Total Time 0.00(0.00)\n",
      "Iter 1217 | Time 57.8781(58.3001) | Bit/dim 3.7067(3.7044) | Xent 1.0003(0.9957) | Loss 9.3137(9.7963) | Error 0.3616(0.3549) Steps 634(632.42) | Grad Norm 5.6067(4.0175) | Total Time 0.00(0.00)\n",
      "Iter 1218 | Time 56.1999(58.2371) | Bit/dim 3.6902(3.7040) | Xent 0.9994(0.9958) | Loss 9.3789(9.7838) | Error 0.3594(0.3551) Steps 616(631.93) | Grad Norm 4.8510(4.0425) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0203 | Time 23.5065, Epoch Time 380.3622(388.5928), Bit/dim 3.6985(best: 3.7023), Xent 0.9820, Loss 4.1895, Error 0.3484(best: 0.3463)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1219 | Time 58.0663(58.2320) | Bit/dim 3.7024(3.7039) | Xent 0.9685(0.9950) | Loss 12.6214(9.8689) | Error 0.3421(0.3547) Steps 640(632.17) | Grad Norm 5.8760(4.0975) | Total Time 0.00(0.00)\n",
      "Iter 1220 | Time 58.8997(58.2520) | Bit/dim 3.7132(3.7042) | Xent 0.9971(0.9951) | Loss 9.5184(9.8584) | Error 0.3506(0.3546) Steps 640(632.41) | Grad Norm 3.9665(4.0936) | Total Time 0.00(0.00)\n",
      "Iter 1221 | Time 58.5615(58.2613) | Bit/dim 3.7006(3.7041) | Xent 0.9735(0.9944) | Loss 9.3134(9.8421) | Error 0.3500(0.3544) Steps 640(632.63) | Grad Norm 5.9091(4.1481) | Total Time 0.00(0.00)\n",
      "Iter 1222 | Time 61.9590(58.3722) | Bit/dim 3.7043(3.7041) | Xent 0.9900(0.9943) | Loss 9.2706(9.8249) | Error 0.3555(0.3545) Steps 628(632.49) | Grad Norm 5.2078(4.1798) | Total Time 0.00(0.00)\n",
      "Iter 1223 | Time 54.7027(58.2621) | Bit/dim 3.6968(3.7039) | Xent 0.9825(0.9939) | Loss 9.2205(9.8068) | Error 0.3499(0.3543) Steps 616(632.00) | Grad Norm 4.7188(4.1960) | Total Time 0.00(0.00)\n",
      "Iter 1224 | Time 55.5830(58.1818) | Bit/dim 3.6931(3.7036) | Xent 0.9751(0.9934) | Loss 9.2558(9.7902) | Error 0.3516(0.3542) Steps 628(631.88) | Grad Norm 3.0858(4.1627) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0204 | Time 23.8364, Epoch Time 387.7746(388.5682), Bit/dim 3.7024(best: 3.6985), Xent 0.9757, Loss 4.1903, Error 0.3493(best: 0.3463)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1225 | Time 58.4667(58.1903) | Bit/dim 3.6956(3.7033) | Xent 0.9763(0.9929) | Loss 12.2859(9.8651) | Error 0.3482(0.3541) Steps 622(631.58) | Grad Norm 4.1483(4.1623) | Total Time 0.00(0.00)\n",
      "Iter 1226 | Time 57.5844(58.1721) | Bit/dim 3.7005(3.7032) | Xent 0.9606(0.9919) | Loss 9.2479(9.8466) | Error 0.3406(0.3537) Steps 640(631.84) | Grad Norm 2.7721(4.1206) | Total Time 0.00(0.00)\n",
      "Iter 1227 | Time 60.3115(58.2363) | Bit/dim 3.7096(3.7034) | Xent 0.9804(0.9915) | Loss 9.4937(9.8360) | Error 0.3544(0.3537) Steps 652(632.44) | Grad Norm 4.0272(4.1178) | Total Time 0.00(0.00)\n",
      "Iter 1228 | Time 60.4206(58.3018) | Bit/dim 3.6950(3.7032) | Xent 0.9875(0.9914) | Loss 9.3167(9.8204) | Error 0.3529(0.3537) Steps 646(632.85) | Grad Norm 4.2896(4.1229) | Total Time 0.00(0.00)\n",
      "Iter 1229 | Time 57.5950(58.2806) | Bit/dim 3.7015(3.7031) | Xent 0.9798(0.9911) | Loss 9.1899(9.8015) | Error 0.3505(0.3536) Steps 640(633.06) | Grad Norm 3.7907(4.1130) | Total Time 0.00(0.00)\n",
      "Iter 1230 | Time 56.9377(58.2404) | Bit/dim 3.7054(3.7032) | Xent 0.9775(0.9907) | Loss 9.2556(9.7851) | Error 0.3471(0.3534) Steps 634(633.09) | Grad Norm 3.7301(4.1015) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0205 | Time 23.9875, Epoch Time 391.4667(388.6552), Bit/dim 3.7000(best: 3.6985), Xent 0.9621, Loss 4.1810, Error 0.3427(best: 0.3463)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1231 | Time 58.5148(58.2486) | Bit/dim 3.7074(3.7033) | Xent 0.9717(0.9901) | Loss 11.9694(9.8507) | Error 0.3440(0.3531) Steps 646(633.48) | Grad Norm 3.1609(4.0733) | Total Time 0.00(0.00)\n",
      "Iter 1232 | Time 57.1429(58.2154) | Bit/dim 3.6987(3.7032) | Xent 0.9481(0.9888) | Loss 9.0605(9.8270) | Error 0.3324(0.3525) Steps 640(633.67) | Grad Norm 3.0465(4.0425) | Total Time 0.00(0.00)\n",
      "Iter 1233 | Time 56.3465(58.1593) | Bit/dim 3.7113(3.7034) | Xent 0.9801(0.9886) | Loss 9.3595(9.8129) | Error 0.3499(0.3524) Steps 634(633.68) | Grad Norm 4.4278(4.0540) | Total Time 0.00(0.00)\n",
      "Iter 1234 | Time 63.3872(58.3162) | Bit/dim 3.6995(3.7033) | Xent 0.9796(0.9883) | Loss 9.2681(9.7966) | Error 0.3522(0.3524) Steps 646(634.05) | Grad Norm 4.6440(4.0717) | Total Time 0.00(0.00)\n",
      "Iter 1235 | Time 59.8690(58.3628) | Bit/dim 3.7071(3.7034) | Xent 0.9803(0.9881) | Loss 9.3395(9.7829) | Error 0.3534(0.3524) Steps 616(633.51) | Grad Norm 7.7483(4.1820) | Total Time 0.00(0.00)\n",
      "Iter 1236 | Time 61.9691(58.4710) | Bit/dim 3.6827(3.7028) | Xent 0.9581(0.9872) | Loss 9.2329(9.7664) | Error 0.3454(0.3522) Steps 616(632.99) | Grad Norm 4.4038(4.1887) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0206 | Time 24.3483, Epoch Time 398.3903(388.9472), Bit/dim 3.7059(best: 3.6985), Xent 0.9654, Loss 4.1886, Error 0.3424(best: 0.3427)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1237 | Time 57.0590(58.4286) | Bit/dim 3.7116(3.7031) | Xent 0.9558(0.9862) | Loss 12.5308(9.8493) | Error 0.3458(0.3520) Steps 634(633.02) | Grad Norm 3.7466(4.1754) | Total Time 0.00(0.00)\n",
      "Iter 1238 | Time 55.7364(58.3478) | Bit/dim 3.6986(3.7029) | Xent 0.9567(0.9853) | Loss 9.1219(9.8275) | Error 0.3367(0.3516) Steps 628(632.87) | Grad Norm 5.4679(4.2142) | Total Time 0.00(0.00)\n",
      "Iter 1239 | Time 59.7779(58.3907) | Bit/dim 3.7075(3.7031) | Xent 0.9564(0.9845) | Loss 9.3183(9.8122) | Error 0.3427(0.3513) Steps 634(632.90) | Grad Norm 1.4481(4.1312) | Total Time 0.00(0.00)\n",
      "Iter 1240 | Time 58.3396(58.3892) | Bit/dim 3.6930(3.7028) | Xent 0.9688(0.9840) | Loss 9.2879(9.7965) | Error 0.3450(0.3511) Steps 646(633.29) | Grad Norm 5.7083(4.1785) | Total Time 0.00(0.00)\n",
      "Iter 1241 | Time 60.1330(58.4415) | Bit/dim 3.6940(3.7025) | Xent 1.0025(0.9846) | Loss 9.4201(9.7852) | Error 0.3604(0.3514) Steps 604(632.41) | Grad Norm 6.6000(4.2512) | Total Time 0.00(0.00)\n",
      "Iter 1242 | Time 59.3844(58.4698) | Bit/dim 3.7102(3.7027) | Xent 1.0289(0.9859) | Loss 9.1434(9.7659) | Error 0.3696(0.3519) Steps 628(632.28) | Grad Norm 6.7139(4.3250) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0207 | Time 23.9063, Epoch Time 390.3025(388.9879), Bit/dim 3.6955(best: 3.6985), Xent 0.9611, Loss 4.1760, Error 0.3416(best: 0.3424)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1243 | Time 57.1306(58.4296) | Bit/dim 3.7069(3.7029) | Xent 0.9739(0.9855) | Loss 12.2711(9.8411) | Error 0.3429(0.3517) Steps 640(632.51) | Grad Norm 2.3147(4.2647) | Total Time 0.00(0.00)\n",
      "Iter 1244 | Time 60.5844(58.4943) | Bit/dim 3.6979(3.7027) | Xent 0.9769(0.9853) | Loss 9.1984(9.8218) | Error 0.3486(0.3516) Steps 628(632.38) | Grad Norm 5.4858(4.3014) | Total Time 0.00(0.00)\n",
      "Iter 1245 | Time 60.5946(58.5573) | Bit/dim 3.6974(3.7026) | Xent 1.0186(0.9863) | Loss 9.3156(9.8066) | Error 0.3691(0.3521) Steps 646(632.79) | Grad Norm 6.0629(4.3542) | Total Time 0.00(0.00)\n",
      "Iter 1246 | Time 54.4406(58.4338) | Bit/dim 3.6939(3.7023) | Xent 0.9715(0.9858) | Loss 9.2396(9.7896) | Error 0.3460(0.3519) Steps 628(632.64) | Grad Norm 4.3404(4.3538) | Total Time 0.00(0.00)\n",
      "Iter 1247 | Time 57.9355(58.4188) | Bit/dim 3.7000(3.7022) | Xent 0.9973(0.9862) | Loss 9.3752(9.7772) | Error 0.3555(0.3520) Steps 652(633.22) | Grad Norm 6.9622(4.4320) | Total Time 0.00(0.00)\n",
      "Iter 1248 | Time 60.0232(58.4670) | Bit/dim 3.7010(3.7022) | Xent 0.9927(0.9864) | Loss 9.4582(9.7676) | Error 0.3566(0.3522) Steps 646(633.61) | Grad Norm 4.8372(4.4442) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0208 | Time 23.3718, Epoch Time 390.9250(389.0460), Bit/dim 3.7008(best: 3.6955), Xent 0.9733, Loss 4.1874, Error 0.3484(best: 0.3416)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1249 | Time 60.2326(58.5199) | Bit/dim 3.6982(3.7021) | Xent 1.0014(0.9868) | Loss 12.8862(9.8612) | Error 0.3588(0.3524) Steps 628(633.44) | Grad Norm 5.1006(4.4639) | Total Time 0.00(0.00)\n",
      "Iter 1250 | Time 59.4448(58.5477) | Bit/dim 3.7066(3.7022) | Xent 0.9916(0.9870) | Loss 9.0599(9.8371) | Error 0.3510(0.3523) Steps 616(632.92) | Grad Norm 4.9938(4.4798) | Total Time 0.00(0.00)\n",
      "Iter 1251 | Time 57.2558(58.5089) | Bit/dim 3.7003(3.7022) | Xent 0.9814(0.9868) | Loss 9.3993(9.8240) | Error 0.3464(0.3521) Steps 616(632.41) | Grad Norm 6.4839(4.5399) | Total Time 0.00(0.00)\n",
      "Iter 1252 | Time 59.7318(58.5456) | Bit/dim 3.6961(3.7020) | Xent 0.9927(0.9870) | Loss 9.4663(9.8133) | Error 0.3586(0.3523) Steps 622(632.10) | Grad Norm 5.1893(4.5594) | Total Time 0.00(0.00)\n",
      "Iter 1253 | Time 56.8668(58.4952) | Bit/dim 3.7059(3.7021) | Xent 0.9877(0.9870) | Loss 9.2841(9.7974) | Error 0.3511(0.3523) Steps 652(632.69) | Grad Norm 5.6786(4.5930) | Total Time 0.00(0.00)\n",
      "Iter 1254 | Time 55.3950(58.4022) | Bit/dim 3.6929(3.7018) | Xent 0.9709(0.9865) | Loss 9.3206(9.7831) | Error 0.3478(0.3522) Steps 622(632.37) | Grad Norm 6.6704(4.6553) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0209 | Time 23.5162, Epoch Time 390.3894(389.0863), Bit/dim 3.6970(best: 3.6955), Xent 0.9755, Loss 4.1847, Error 0.3452(best: 0.3416)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1255 | Time 56.9920(58.3599) | Bit/dim 3.6945(3.7016) | Xent 0.9698(0.9860) | Loss 12.6164(9.8681) | Error 0.3489(0.3521) Steps 634(632.42) | Grad Norm 5.9593(4.6944) | Total Time 0.00(0.00)\n",
      "Iter 1256 | Time 57.6812(58.3396) | Bit/dim 3.7007(3.7016) | Xent 0.9771(0.9857) | Loss 9.1603(9.8469) | Error 0.3496(0.3520) Steps 640(632.65) | Grad Norm 6.6169(4.7521) | Total Time 0.00(0.00)\n",
      "Iter 1257 | Time 59.9699(58.3885) | Bit/dim 3.6927(3.7013) | Xent 0.9889(0.9858) | Loss 9.2401(9.8287) | Error 0.3509(0.3520) Steps 658(633.41) | Grad Norm 5.1995(4.7655) | Total Time 0.00(0.00)\n",
      "Iter 1258 | Time 57.1453(58.3512) | Bit/dim 3.7021(3.7013) | Xent 0.9685(0.9853) | Loss 9.1781(9.8091) | Error 0.3424(0.3517) Steps 634(633.43) | Grad Norm 4.1105(4.7459) | Total Time 0.00(0.00)\n",
      "Iter 1259 | Time 59.5318(58.3866) | Bit/dim 3.7024(3.7014) | Xent 0.9600(0.9846) | Loss 9.3596(9.7957) | Error 0.3411(0.3513) Steps 616(632.90) | Grad Norm 4.8072(4.7477) | Total Time 0.00(0.00)\n",
      "Iter 1260 | Time 58.5223(58.3907) | Bit/dim 3.7037(3.7014) | Xent 0.9768(0.9843) | Loss 9.4259(9.7846) | Error 0.3474(0.3512) Steps 616(632.40) | Grad Norm 4.5445(4.7416) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0210 | Time 23.8482, Epoch Time 390.1093(389.1170), Bit/dim 3.7034(best: 3.6955), Xent 0.9637, Loss 4.1852, Error 0.3435(best: 0.3416)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1261 | Time 58.4125(58.3913) | Bit/dim 3.6985(3.7013) | Xent 0.9655(0.9838) | Loss 12.6335(9.8700) | Error 0.3410(0.3509) Steps 640(632.62) | Grad Norm 5.5726(4.7665) | Total Time 0.00(0.00)\n",
      "Iter 1262 | Time 62.5922(58.5174) | Bit/dim 3.7061(3.7015) | Xent 0.9773(0.9836) | Loss 9.5024(9.8590) | Error 0.3508(0.3509) Steps 640(632.85) | Grad Norm 2.3940(4.6954) | Total Time 0.00(0.00)\n",
      "Iter 1263 | Time 56.2857(58.4504) | Bit/dim 3.7018(3.7015) | Xent 0.9699(0.9832) | Loss 9.4073(9.8455) | Error 0.3396(0.3506) Steps 658(633.60) | Grad Norm 5.1421(4.7088) | Total Time 0.00(0.00)\n",
      "Iter 1264 | Time 63.6018(58.6049) | Bit/dim 3.6968(3.7013) | Xent 0.9698(0.9828) | Loss 9.2992(9.8291) | Error 0.3407(0.3503) Steps 592(632.35) | Grad Norm 2.3527(4.6381) | Total Time 0.00(0.00)\n",
      "Iter 1265 | Time 59.2764(58.6251) | Bit/dim 3.6938(3.7011) | Xent 0.9753(0.9825) | Loss 8.9291(9.8021) | Error 0.3488(0.3502) Steps 664(633.30) | Grad Norm 5.7184(4.6705) | Total Time 0.00(0.00)\n",
      "Iter 1266 | Time 57.1201(58.5799) | Bit/dim 3.6961(3.7010) | Xent 0.9353(0.9811) | Loss 9.2847(9.7865) | Error 0.3347(0.3498) Steps 640(633.50) | Grad Norm 4.9272(4.6782) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0211 | Time 23.5679, Epoch Time 397.2041(389.3596), Bit/dim 3.6971(best: 3.6955), Xent 0.9585, Loss 4.1764, Error 0.3403(best: 0.3416)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1267 | Time 56.2592(58.5103) | Bit/dim 3.6979(3.7009) | Xent 0.9745(0.9809) | Loss 12.6075(9.8712) | Error 0.3444(0.3496) Steps 598(632.44) | Grad Norm 4.1410(4.6621) | Total Time 0.00(0.00)\n",
      "Iter 1268 | Time 57.9951(58.4949) | Bit/dim 3.6965(3.7008) | Xent 0.9556(0.9802) | Loss 9.2872(9.8537) | Error 0.3459(0.3495) Steps 622(632.12) | Grad Norm 3.9692(4.6413) | Total Time 0.00(0.00)\n",
      "Iter 1269 | Time 58.5678(58.4971) | Bit/dim 3.7137(3.7011) | Xent 0.9781(0.9801) | Loss 9.3338(9.8381) | Error 0.3499(0.3495) Steps 652(632.72) | Grad Norm 3.0982(4.5950) | Total Time 0.00(0.00)\n",
      "Iter 1270 | Time 56.7195(58.4437) | Bit/dim 3.6963(3.7010) | Xent 0.9559(0.9794) | Loss 9.1297(9.8168) | Error 0.3424(0.3493) Steps 634(632.76) | Grad Norm 4.8644(4.6031) | Total Time 0.00(0.00)\n",
      "Iter 1271 | Time 60.7399(58.5126) | Bit/dim 3.6920(3.7007) | Xent 0.9509(0.9785) | Loss 9.4315(9.8053) | Error 0.3387(0.3490) Steps 652(633.34) | Grad Norm 4.3457(4.5954) | Total Time 0.00(0.00)\n",
      "Iter 1272 | Time 59.9356(58.5553) | Bit/dim 3.6884(3.7004) | Xent 0.9572(0.9779) | Loss 9.3905(9.7928) | Error 0.3488(0.3490) Steps 616(632.82) | Grad Norm 3.5787(4.5649) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0212 | Time 23.9918, Epoch Time 390.5431(389.3951), Bit/dim 3.7069(best: 3.6955), Xent 0.9607, Loss 4.1873, Error 0.3430(best: 0.3403)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1273 | Time 58.8951(58.5655) | Bit/dim 3.7041(3.7005) | Xent 0.9584(0.9773) | Loss 12.4872(9.8736) | Error 0.3438(0.3488) Steps 640(633.03) | Grad Norm 5.8251(4.6027) | Total Time 0.00(0.00)\n",
      "Iter 1274 | Time 60.1619(58.6134) | Bit/dim 3.6974(3.7004) | Xent 0.9527(0.9766) | Loss 9.3266(9.8572) | Error 0.3365(0.3484) Steps 640(633.24) | Grad Norm 2.1122(4.5279) | Total Time 0.00(0.00)\n",
      "Iter 1275 | Time 59.0502(58.6265) | Bit/dim 3.6940(3.7002) | Xent 0.9712(0.9764) | Loss 9.3185(9.8411) | Error 0.3406(0.3482) Steps 628(633.08) | Grad Norm 4.7122(4.5335) | Total Time 0.00(0.00)\n",
      "Iter 1276 | Time 55.8790(58.5441) | Bit/dim 3.6948(3.7000) | Xent 0.9452(0.9755) | Loss 9.2836(9.8243) | Error 0.3409(0.3480) Steps 628(632.93) | Grad Norm 4.7060(4.5387) | Total Time 0.00(0.00)\n",
      "Iter 1277 | Time 58.4404(58.5410) | Bit/dim 3.7017(3.7001) | Xent 0.9673(0.9752) | Loss 9.2395(9.8068) | Error 0.3367(0.3477) Steps 646(633.32) | Grad Norm 3.7041(4.5136) | Total Time 0.00(0.00)\n",
      "Iter 1278 | Time 59.8923(58.5815) | Bit/dim 3.6891(3.6997) | Xent 0.9682(0.9750) | Loss 9.3081(9.7918) | Error 0.3491(0.3477) Steps 628(633.16) | Grad Norm 3.6008(4.4862) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0213 | Time 23.8435, Epoch Time 392.4065(389.4855), Bit/dim 3.6988(best: 3.6955), Xent 0.9549, Loss 4.1762, Error 0.3391(best: 0.3403)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1279 | Time 60.4355(58.6371) | Bit/dim 3.7000(3.6997) | Xent 0.9444(0.9741) | Loss 12.7343(9.8801) | Error 0.3386(0.3474) Steps 622(632.83) | Grad Norm 3.6811(4.4621) | Total Time 0.00(0.00)\n",
      "Iter 1280 | Time 61.7601(58.7308) | Bit/dim 3.6962(3.6996) | Xent 0.9595(0.9736) | Loss 9.4859(9.8683) | Error 0.3436(0.3473) Steps 604(631.96) | Grad Norm 2.4245(4.4010) | Total Time 0.00(0.00)\n",
      "Iter 1281 | Time 59.5051(58.7540) | Bit/dim 3.6945(3.6995) | Xent 0.9747(0.9737) | Loss 9.1332(9.8462) | Error 0.3417(0.3471) Steps 646(632.38) | Grad Norm 3.5934(4.3767) | Total Time 0.00(0.00)\n",
      "Iter 1282 | Time 58.4910(58.7461) | Bit/dim 3.7048(3.6996) | Xent 0.9602(0.9733) | Loss 8.9498(9.8193) | Error 0.3426(0.3470) Steps 646(632.79) | Grad Norm 2.5401(4.3216) | Total Time 0.00(0.00)\n",
      "Iter 1283 | Time 66.1235(58.9675) | Bit/dim 3.6849(3.6992) | Xent 0.9773(0.9734) | Loss 9.4882(9.8094) | Error 0.3472(0.3470) Steps 610(632.11) | Grad Norm 6.2149(4.3784) | Total Time 0.00(0.00)\n",
      "Iter 1284 | Time 56.5707(58.8956) | Bit/dim 3.7019(3.6993) | Xent 0.9651(0.9731) | Loss 9.2730(9.7933) | Error 0.3414(0.3468) Steps 616(631.63) | Grad Norm 5.6061(4.4153) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0214 | Time 24.2260, Epoch Time 403.4650(389.9049), Bit/dim 3.7031(best: 3.6955), Xent 0.9497, Loss 4.1780, Error 0.3333(best: 0.3391)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1285 | Time 65.2121(59.0851) | Bit/dim 3.7152(3.6998) | Xent 0.9494(0.9724) | Loss 12.3291(9.8694) | Error 0.3414(0.3467) Steps 610(630.98) | Grad Norm 2.8423(4.3681) | Total Time 0.00(0.00)\n",
      "Iter 1286 | Time 61.0870(59.1451) | Bit/dim 3.7011(3.6998) | Xent 1.0124(0.9736) | Loss 9.1463(9.8477) | Error 0.3611(0.3471) Steps 622(630.71) | Grad Norm 5.1520(4.3916) | Total Time 0.00(0.00)\n",
      "Iter 1287 | Time 56.3717(59.0619) | Bit/dim 3.7004(3.6998) | Xent 0.9739(0.9736) | Loss 9.2255(9.8290) | Error 0.3530(0.3473) Steps 616(630.27) | Grad Norm 6.8366(4.4649) | Total Time 0.00(0.00)\n",
      "Iter 1288 | Time 59.3126(59.0694) | Bit/dim 3.6829(3.6993) | Xent 0.9710(0.9736) | Loss 9.4409(9.8174) | Error 0.3451(0.3472) Steps 658(631.10) | Grad Norm 4.4920(4.4657) | Total Time 0.00(0.00)\n",
      "Iter 1289 | Time 60.8078(59.1216) | Bit/dim 3.6890(3.6990) | Xent 0.9631(0.9732) | Loss 9.3186(9.8024) | Error 0.3445(0.3471) Steps 628(631.01) | Grad Norm 4.8924(4.4785) | Total Time 0.00(0.00)\n",
      "Iter 1290 | Time 58.1162(59.0914) | Bit/dim 3.6975(3.6990) | Xent 0.9386(0.9722) | Loss 9.2315(9.7853) | Error 0.3365(0.3468) Steps 664(632.00) | Grad Norm 3.1350(4.4382) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0215 | Time 23.7880, Epoch Time 400.9181(390.2352), Bit/dim 3.6954(best: 3.6955), Xent 0.9517, Loss 4.1713, Error 0.3370(best: 0.3333)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1291 | Time 60.3434(59.1290) | Bit/dim 3.6910(3.6987) | Xent 0.9669(0.9720) | Loss 12.7851(9.8753) | Error 0.3396(0.3466) Steps 652(632.60) | Grad Norm 3.9692(4.4242) | Total Time 0.00(0.00)\n",
      "Iter 1292 | Time 57.8525(59.0907) | Bit/dim 3.6924(3.6985) | Xent 0.9777(0.9722) | Loss 9.1309(9.8530) | Error 0.3445(0.3465) Steps 658(633.36) | Grad Norm 4.5129(4.4268) | Total Time 0.00(0.00)\n",
      "Iter 1293 | Time 60.9367(59.1461) | Bit/dim 3.7005(3.6986) | Xent 0.9285(0.9709) | Loss 9.2025(9.8334) | Error 0.3301(0.3461) Steps 634(633.38) | Grad Norm 3.8331(4.4090) | Total Time 0.00(0.00)\n",
      "Iter 1294 | Time 58.2601(59.1195) | Bit/dim 3.6946(3.6985) | Xent 0.9582(0.9705) | Loss 9.0814(9.8109) | Error 0.3426(0.3460) Steps 634(633.40) | Grad Norm 3.0870(4.3694) | Total Time 0.00(0.00)\n",
      "Iter 1295 | Time 58.5769(59.1032) | Bit/dim 3.7050(3.6987) | Xent 0.9520(0.9700) | Loss 9.0324(9.7875) | Error 0.3446(0.3459) Steps 616(632.87) | Grad Norm 3.8330(4.3533) | Total Time 0.00(0.00)\n",
      "Iter 1296 | Time 61.1634(59.1650) | Bit/dim 3.6978(3.6986) | Xent 0.9581(0.9696) | Loss 9.2763(9.7722) | Error 0.3376(0.3457) Steps 634(632.91) | Grad Norm 5.0753(4.3749) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0216 | Time 23.0481, Epoch Time 396.7602(390.4310), Bit/dim 3.6967(best: 3.6954), Xent 0.9622, Loss 4.1778, Error 0.3420(best: 0.3333)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1297 | Time 59.1736(59.1653) | Bit/dim 3.7003(3.6987) | Xent 0.9606(0.9693) | Loss 12.3852(9.8506) | Error 0.3423(0.3456) Steps 646(633.30) | Grad Norm 4.4275(4.3765) | Total Time 0.00(0.00)\n",
      "Iter 1298 | Time 61.8198(59.2449) | Bit/dim 3.7034(3.6988) | Xent 0.9346(0.9683) | Loss 9.2950(9.8339) | Error 0.3329(0.3452) Steps 616(632.78) | Grad Norm 2.8467(4.3306) | Total Time 0.00(0.00)\n",
      "Iter 1299 | Time 59.3230(59.2473) | Bit/dim 3.6939(3.6987) | Xent 0.9498(0.9677) | Loss 9.3008(9.8179) | Error 0.3369(0.3449) Steps 622(632.46) | Grad Norm 2.7255(4.2825) | Total Time 0.00(0.00)\n",
      "Iter 1300 | Time 55.9044(59.1470) | Bit/dim 3.6979(3.6987) | Xent 0.9508(0.9672) | Loss 9.0938(9.7962) | Error 0.3374(0.3447) Steps 640(632.68) | Grad Norm 3.1781(4.2493) | Total Time 0.00(0.00)\n",
      "Iter 1301 | Time 61.8315(59.2275) | Bit/dim 3.6880(3.6983) | Xent 0.9551(0.9669) | Loss 9.4746(9.7865) | Error 0.3394(0.3445) Steps 646(633.08) | Grad Norm 2.1245(4.1856) | Total Time 0.00(0.00)\n",
      "Iter 1302 | Time 56.7517(59.1532) | Bit/dim 3.6833(3.6979) | Xent 0.9748(0.9671) | Loss 8.9935(9.7628) | Error 0.3480(0.3446) Steps 652(633.65) | Grad Norm 4.3795(4.1914) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0217 | Time 23.2185, Epoch Time 393.8853(390.5346), Bit/dim 3.7006(best: 3.6954), Xent 0.9547, Loss 4.1779, Error 0.3407(best: 0.3333)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1303 | Time 55.8305(59.0535) | Bit/dim 3.6909(3.6977) | Xent 0.9545(0.9667) | Loss 12.6166(9.8484) | Error 0.3434(0.3446) Steps 634(633.66) | Grad Norm 4.8969(4.2126) | Total Time 0.00(0.00)\n",
      "Iter 1304 | Time 58.7211(59.0436) | Bit/dim 3.6967(3.6976) | Xent 0.9441(0.9660) | Loss 9.3157(9.8324) | Error 0.3347(0.3443) Steps 646(634.03) | Grad Norm 5.3104(4.2455) | Total Time 0.00(0.00)\n",
      "Iter 1305 | Time 59.2516(59.0498) | Bit/dim 3.7016(3.6978) | Xent 0.9668(0.9661) | Loss 9.2413(9.8147) | Error 0.3410(0.3442) Steps 664(634.93) | Grad Norm 5.1481(4.2726) | Total Time 0.00(0.00)\n",
      "Iter 1306 | Time 58.2433(59.0256) | Bit/dim 3.7071(3.6980) | Xent 0.9576(0.9658) | Loss 9.2299(9.7971) | Error 0.3396(0.3441) Steps 646(635.26) | Grad Norm 1.7590(4.1972) | Total Time 0.00(0.00)\n",
      "Iter 1307 | Time 59.8058(59.0490) | Bit/dim 3.6974(3.6980) | Xent 0.9753(0.9661) | Loss 9.3863(9.7848) | Error 0.3489(0.3442) Steps 616(634.69) | Grad Norm 6.1839(4.2568) | Total Time 0.00(0.00)\n",
      "Iter 1308 | Time 57.7323(59.0095) | Bit/dim 3.6884(3.6977) | Xent 0.9841(0.9666) | Loss 9.1675(9.7663) | Error 0.3486(0.3444) Steps 616(634.12) | Grad Norm 8.5781(4.3864) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0218 | Time 23.2832, Epoch Time 388.8886(390.4852), Bit/dim 3.6939(best: 3.6954), Xent 0.9853, Loss 4.1865, Error 0.3472(best: 0.3333)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1309 | Time 57.0170(58.9497) | Bit/dim 3.7003(3.6978) | Xent 1.0004(0.9677) | Loss 12.5063(9.8485) | Error 0.3516(0.3446) Steps 634(634.12) | Grad Norm 9.3863(4.5364) | Total Time 0.00(0.00)\n",
      "Iter 1310 | Time 58.7867(58.9449) | Bit/dim 3.6889(3.6975) | Xent 0.9414(0.9669) | Loss 9.2047(9.8292) | Error 0.3387(0.3444) Steps 634(634.12) | Grad Norm 5.9903(4.5800) | Total Time 0.00(0.00)\n",
      "Iter 1311 | Time 61.4858(59.0211) | Bit/dim 3.6862(3.6972) | Xent 1.0199(0.9685) | Loss 9.3224(9.8140) | Error 0.3558(0.3447) Steps 616(633.57) | Grad Norm 12.9548(4.8313) | Total Time 0.00(0.00)\n",
      "Iter 1312 | Time 61.5772(59.0978) | Bit/dim 3.6995(3.6973) | Xent 1.0217(0.9701) | Loss 9.5255(9.8053) | Error 0.3591(0.3452) Steps 646(633.95) | Grad Norm 10.1681(4.9914) | Total Time 0.00(0.00)\n",
      "Iter 1313 | Time 58.1885(59.0705) | Bit/dim 3.6910(3.6971) | Xent 0.9738(0.9702) | Loss 9.1105(9.7845) | Error 0.3485(0.3453) Steps 628(633.77) | Grad Norm 6.5356(5.0377) | Total Time 0.00(0.00)\n",
      "Iter 1314 | Time 59.7500(59.0909) | Bit/dim 3.7038(3.6973) | Xent 0.9719(0.9702) | Loss 9.2376(9.7681) | Error 0.3514(0.3455) Steps 640(633.96) | Grad Norm 8.2192(5.1331) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0219 | Time 23.8469, Epoch Time 396.4619(390.6645), Bit/dim 3.7028(best: 3.6939), Xent 0.9612, Loss 4.1835, Error 0.3434(best: 0.3333)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1315 | Time 56.7914(59.0219) | Bit/dim 3.6978(3.6973) | Xent 0.9598(0.9699) | Loss 12.6242(9.8537) | Error 0.3405(0.3453) Steps 646(634.32) | Grad Norm 4.3297(5.1090) | Total Time 0.00(0.00)\n",
      "Iter 1316 | Time 56.7167(58.9527) | Bit/dim 3.6923(3.6972) | Xent 0.9494(0.9693) | Loss 9.3253(9.8379) | Error 0.3403(0.3452) Steps 634(634.31) | Grad Norm 4.5087(5.0910) | Total Time 0.00(0.00)\n",
      "Iter 1317 | Time 58.4035(58.9363) | Bit/dim 3.6951(3.6971) | Xent 0.9638(0.9691) | Loss 9.3419(9.8230) | Error 0.3481(0.3452) Steps 634(634.30) | Grad Norm 3.3628(5.0392) | Total Time 0.00(0.00)\n",
      "Iter 1318 | Time 57.0961(58.8811) | Bit/dim 3.7016(3.6972) | Xent 0.9496(0.9685) | Loss 9.1504(9.8028) | Error 0.3295(0.3448) Steps 646(634.65) | Grad Norm 3.6297(4.9969) | Total Time 0.00(0.00)\n",
      "Iter 1319 | Time 57.1839(58.8301) | Bit/dim 3.6919(3.6971) | Xent 0.9483(0.9679) | Loss 9.1029(9.7818) | Error 0.3376(0.3446) Steps 622(634.27) | Grad Norm 2.4503(4.9205) | Total Time 0.00(0.00)\n",
      "Iter 1320 | Time 61.8906(58.9220) | Bit/dim 3.6971(3.6971) | Xent 0.9484(0.9673) | Loss 9.1283(9.7622) | Error 0.3391(0.3444) Steps 652(634.80) | Grad Norm 3.7990(4.8869) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0220 | Time 23.1759, Epoch Time 387.1330(390.5586), Bit/dim 3.6938(best: 3.6939), Xent 0.9474, Loss 4.1675, Error 0.3360(best: 0.3333)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1321 | Time 58.2255(58.9011) | Bit/dim 3.6869(3.6968) | Xent 0.9351(0.9664) | Loss 12.5720(9.8465) | Error 0.3340(0.3441) Steps 634(634.78) | Grad Norm 3.6081(4.8485) | Total Time 0.00(0.00)\n",
      "Iter 1322 | Time 61.0911(58.9668) | Bit/dim 3.6963(3.6968) | Xent 0.9348(0.9654) | Loss 9.4288(9.8340) | Error 0.3303(0.3437) Steps 622(634.39) | Grad Norm 3.0682(4.7951) | Total Time 0.00(0.00)\n",
      "Iter 1323 | Time 59.9749(58.9970) | Bit/dim 3.7034(3.6970) | Xent 0.9430(0.9648) | Loss 9.2118(9.8153) | Error 0.3313(0.3433) Steps 610(633.66) | Grad Norm 4.4524(4.7848) | Total Time 0.00(0.00)\n",
      "Iter 1324 | Time 57.9600(58.9659) | Bit/dim 3.6955(3.6969) | Xent 0.9356(0.9639) | Loss 9.3315(9.8008) | Error 0.3350(0.3430) Steps 634(633.67) | Grad Norm 3.1574(4.7360) | Total Time 0.00(0.00)\n",
      "Iter 1325 | Time 60.9633(59.0258) | Bit/dim 3.6879(3.6966) | Xent 0.9754(0.9642) | Loss 9.3967(9.7887) | Error 0.3427(0.3430) Steps 652(634.22) | Grad Norm 3.8620(4.7098) | Total Time 0.00(0.00)\n",
      "Iter 1326 | Time 63.3136(59.1545) | Bit/dim 3.6950(3.6966) | Xent 0.9386(0.9635) | Loss 9.3117(9.7744) | Error 0.3369(0.3429) Steps 640(634.40) | Grad Norm 2.0083(4.6287) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0221 | Time 23.2892, Epoch Time 400.4234(390.8545), Bit/dim 3.6974(best: 3.6938), Xent 0.9592, Loss 4.1770, Error 0.3399(best: 0.3333)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1327 | Time 60.2607(59.1876) | Bit/dim 3.7064(3.6969) | Xent 0.9736(0.9638) | Loss 12.3853(9.8527) | Error 0.3464(0.3430) Steps 658(635.10) | Grad Norm 4.6060(4.6280) | Total Time 0.00(0.00)\n",
      "Iter 1328 | Time 60.2215(59.2187) | Bit/dim 3.6903(3.6967) | Xent 0.9270(0.9627) | Loss 9.2485(9.8346) | Error 0.3323(0.3426) Steps 628(634.89) | Grad Norm 2.1372(4.5533) | Total Time 0.00(0.00)\n",
      "Iter 1329 | Time 62.5275(59.3179) | Bit/dim 3.6871(3.6964) | Xent 0.9380(0.9619) | Loss 9.4070(9.8218) | Error 0.3387(0.3425) Steps 634(634.86) | Grad Norm 3.2120(4.5131) | Total Time 0.00(0.00)\n",
      "Iter 1330 | Time 62.1081(59.4016) | Bit/dim 3.6850(3.6961) | Xent 0.9533(0.9617) | Loss 9.0512(9.7986) | Error 0.3411(0.3425) Steps 640(635.02) | Grad Norm 2.9828(4.4672) | Total Time 0.00(0.00)\n",
      "Iter 1331 | Time 59.0358(59.3907) | Bit/dim 3.7007(3.6962) | Xent 0.9664(0.9618) | Loss 9.1350(9.7787) | Error 0.3435(0.3425) Steps 652(635.53) | Grad Norm 3.9588(4.4519) | Total Time 0.00(0.00)\n",
      "Iter 1332 | Time 62.8521(59.4945) | Bit/dim 3.7012(3.6963) | Xent 0.9407(0.9612) | Loss 9.4160(9.7678) | Error 0.3420(0.3425) Steps 646(635.84) | Grad Norm 3.8816(4.4348) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0222 | Time 22.7476, Epoch Time 409.5966(391.4168), Bit/dim 3.6976(best: 3.6938), Xent 0.9446, Loss 4.1699, Error 0.3359(best: 0.3333)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1333 | Time 52.6789(59.2900) | Bit/dim 3.6908(3.6962) | Xent 0.9555(0.9610) | Loss 12.5810(9.8522) | Error 0.3397(0.3424) Steps 616(635.25) | Grad Norm 4.2815(4.4302) | Total Time 0.00(0.00)\n",
      "Iter 1334 | Time 58.4680(59.2654) | Bit/dim 3.6979(3.6962) | Xent 0.9563(0.9609) | Loss 9.4003(9.8387) | Error 0.3410(0.3424) Steps 634(635.21) | Grad Norm 4.5733(4.4345) | Total Time 0.00(0.00)\n",
      "Iter 1335 | Time 57.4172(59.2099) | Bit/dim 3.6971(3.6963) | Xent 0.9267(0.9598) | Loss 9.3797(9.8249) | Error 0.3304(0.3420) Steps 664(636.07) | Grad Norm 4.3491(4.4319) | Total Time 0.00(0.00)\n",
      "Iter 1336 | Time 63.1612(59.3285) | Bit/dim 3.6869(3.6960) | Xent 0.9386(0.9592) | Loss 9.2824(9.8086) | Error 0.3310(0.3417) Steps 622(635.65) | Grad Norm 4.4742(4.4332) | Total Time 0.00(0.00)\n",
      "Iter 1337 | Time 60.5790(59.3660) | Bit/dim 3.7029(3.6962) | Xent 0.9372(0.9585) | Loss 9.1776(9.7897) | Error 0.3291(0.3413) Steps 628(635.42) | Grad Norm 4.5005(4.4352) | Total Time 0.00(0.00)\n",
      "Iter 1338 | Time 62.8330(59.4700) | Bit/dim 3.6913(3.6960) | Xent 0.9460(0.9582) | Loss 9.3149(9.7755) | Error 0.3343(0.3411) Steps 628(635.20) | Grad Norm 4.9712(4.4513) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0223 | Time 23.4112, Epoch Time 394.7710(391.5174), Bit/dim 3.6952(best: 3.6938), Xent 0.9444, Loss 4.1674, Error 0.3373(best: 0.3333)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1339 | Time 57.8899(59.4226) | Bit/dim 3.6929(3.6959) | Xent 0.9575(0.9581) | Loss 12.6950(9.8630) | Error 0.3430(0.3411) Steps 628(634.98) | Grad Norm 5.3369(4.4779) | Total Time 0.00(0.00)\n",
      "Iter 1340 | Time 62.3813(59.5113) | Bit/dim 3.6854(3.6956) | Xent 0.9357(0.9575) | Loss 9.1664(9.8421) | Error 0.3321(0.3409) Steps 640(635.13) | Grad Norm 3.2948(4.4424) | Total Time 0.00(0.00)\n",
      "Iter 1341 | Time 60.0359(59.5271) | Bit/dim 3.6986(3.6957) | Xent 0.9461(0.9571) | Loss 9.3542(9.8275) | Error 0.3391(0.3408) Steps 634(635.10) | Grad Norm 2.8138(4.3935) | Total Time 0.00(0.00)\n",
      "Iter 1342 | Time 58.6240(59.5000) | Bit/dim 3.6907(3.6956) | Xent 0.9377(0.9565) | Loss 9.4611(9.8165) | Error 0.3361(0.3407) Steps 652(635.61) | Grad Norm 4.9589(4.4105) | Total Time 0.00(0.00)\n",
      "Iter 1343 | Time 63.2721(59.6131) | Bit/dim 3.7055(3.6959) | Xent 0.9458(0.9562) | Loss 9.2012(9.7981) | Error 0.3386(0.3406) Steps 658(636.28) | Grad Norm 4.0089(4.3984) | Total Time 0.00(0.00)\n",
      "Iter 1344 | Time 61.6704(59.6749) | Bit/dim 3.6890(3.6957) | Xent 0.9389(0.9557) | Loss 9.3098(9.7834) | Error 0.3339(0.3404) Steps 646(636.57) | Grad Norm 1.5620(4.3133) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0224 | Time 23.6235, Epoch Time 403.7738(391.8851), Bit/dim 3.6920(best: 3.6938), Xent 0.9438, Loss 4.1639, Error 0.3348(best: 0.3333)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1345 | Time 60.3418(59.6949) | Bit/dim 3.6885(3.6954) | Xent 0.9243(0.9548) | Loss 12.4742(9.8641) | Error 0.3220(0.3399) Steps 658(637.21) | Grad Norm 4.9384(4.3321) | Total Time 0.00(0.00)\n",
      "Iter 1346 | Time 67.2076(59.9203) | Bit/dim 3.6884(3.6952) | Xent 0.9349(0.9542) | Loss 9.3674(9.8492) | Error 0.3277(0.3395) Steps 664(638.02) | Grad Norm 4.6378(4.3413) | Total Time 0.00(0.00)\n",
      "Iter 1347 | Time 60.2464(59.9300) | Bit/dim 3.6782(3.6947) | Xent 0.9412(0.9538) | Loss 9.2587(9.8315) | Error 0.3314(0.3393) Steps 646(638.26) | Grad Norm 3.5617(4.3179) | Total Time 0.00(0.00)\n",
      "Iter 1348 | Time 58.7229(59.8938) | Bit/dim 3.7024(3.6950) | Xent 0.9477(0.9536) | Loss 9.3071(9.8158) | Error 0.3416(0.3393) Steps 658(638.85) | Grad Norm 3.7026(4.2994) | Total Time 0.00(0.00)\n",
      "Iter 1349 | Time 56.2208(59.7836) | Bit/dim 3.7001(3.6951) | Xent 0.9336(0.9530) | Loss 9.1076(9.7945) | Error 0.3310(0.3391) Steps 628(638.52) | Grad Norm 1.9187(4.2280) | Total Time 0.00(0.00)\n",
      "Iter 1350 | Time 56.3932(59.6819) | Bit/dim 3.6878(3.6949) | Xent 0.9721(0.9536) | Loss 9.2694(9.7788) | Error 0.3494(0.3394) Steps 634(638.39) | Grad Norm 5.6696(4.2712) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0225 | Time 23.1773, Epoch Time 399.0532(392.1002), Bit/dim 3.6958(best: 3.6920), Xent 0.9546, Loss 4.1731, Error 0.3401(best: 0.3333)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1351 | Time 59.9942(59.6913) | Bit/dim 3.6838(3.6946) | Xent 0.9570(0.9537) | Loss 13.0482(9.8769) | Error 0.3386(0.3394) Steps 652(638.80) | Grad Norm 6.0385(4.3243) | Total Time 0.00(0.00)\n",
      "Iter 1352 | Time 59.1790(59.6759) | Bit/dim 3.6988(3.6947) | Xent 0.9515(0.9536) | Loss 9.2680(9.8586) | Error 0.3419(0.3394) Steps 634(638.65) | Grad Norm 5.7580(4.3673) | Total Time 0.00(0.00)\n",
      "Iter 1353 | Time 59.0023(59.6557) | Bit/dim 3.6941(3.6947) | Xent 0.9368(0.9531) | Loss 9.1856(9.8384) | Error 0.3415(0.3395) Steps 634(638.51) | Grad Norm 5.1904(4.3920) | Total Time 0.00(0.00)\n",
      "Iter 1354 | Time 63.0581(59.7578) | Bit/dim 3.6884(3.6945) | Xent 0.9603(0.9533) | Loss 9.2975(9.8222) | Error 0.3434(0.3396) Steps 646(638.74) | Grad Norm 6.2269(4.4470) | Total Time 0.00(0.00)\n",
      "Iter 1355 | Time 61.9335(59.8231) | Bit/dim 3.6980(3.6946) | Xent 0.9576(0.9534) | Loss 9.3561(9.8082) | Error 0.3421(0.3397) Steps 640(638.77) | Grad Norm 8.1736(4.5588) | Total Time 0.00(0.00)\n",
      "Iter 1356 | Time 62.2863(59.8970) | Bit/dim 3.7026(3.6948) | Xent 0.9473(0.9533) | Loss 9.3004(9.7930) | Error 0.3424(0.3398) Steps 640(638.81) | Grad Norm 4.9278(4.5699) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0226 | Time 23.3384, Epoch Time 405.0505(392.4887), Bit/dim 3.6956(best: 3.6920), Xent 0.9394, Loss 4.1653, Error 0.3325(best: 0.3333)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1357 | Time 61.7897(59.9537) | Bit/dim 3.7024(3.6951) | Xent 0.9450(0.9530) | Loss 12.4193(9.8718) | Error 0.3451(0.3399) Steps 640(638.85) | Grad Norm 4.1904(4.5585) | Total Time 0.00(0.00)\n",
      "Iter 1358 | Time 61.5874(60.0027) | Bit/dim 3.6837(3.6947) | Xent 0.9473(0.9528) | Loss 9.1298(9.8495) | Error 0.3366(0.3398) Steps 658(639.42) | Grad Norm 5.2263(4.5785) | Total Time 0.00(0.00)\n",
      "Iter 1359 | Time 60.2899(60.0114) | Bit/dim 3.6892(3.6945) | Xent 0.9472(0.9527) | Loss 9.4031(9.8361) | Error 0.3391(0.3398) Steps 652(639.80) | Grad Norm 4.4192(4.5738) | Total Time 0.00(0.00)\n",
      "Iter 1360 | Time 62.4724(60.0852) | Bit/dim 3.6870(3.6943) | Xent 0.9381(0.9522) | Loss 9.3533(9.8216) | Error 0.3341(0.3396) Steps 640(639.80) | Grad Norm 3.1921(4.5323) | Total Time 0.00(0.00)\n",
      "Iter 1361 | Time 61.0242(60.1134) | Bit/dim 3.6958(3.6944) | Xent 0.9323(0.9516) | Loss 9.4144(9.8094) | Error 0.3244(0.3392) Steps 670(640.71) | Grad Norm 4.5053(4.5315) | Total Time 0.00(0.00)\n",
      "Iter 1362 | Time 62.2667(60.1780) | Bit/dim 3.6879(3.6942) | Xent 0.9238(0.9508) | Loss 9.0754(9.7874) | Error 0.3323(0.3390) Steps 640(640.69) | Grad Norm 2.4788(4.4699) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0227 | Time 23.1913, Epoch Time 408.6355(392.9731), Bit/dim 3.6944(best: 3.6920), Xent 0.9331, Loss 4.1609, Error 0.3306(best: 0.3325)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1363 | Time 60.3437(60.1829) | Bit/dim 3.6977(3.6943) | Xent 0.9332(0.9503) | Loss 12.2253(9.8605) | Error 0.3313(0.3387) Steps 658(641.21) | Grad Norm 2.8415(4.4211) | Total Time 0.00(0.00)\n",
      "Iter 1364 | Time 58.0118(60.1178) | Bit/dim 3.6904(3.6942) | Xent 0.9243(0.9495) | Loss 9.1857(9.8403) | Error 0.3330(0.3386) Steps 646(641.35) | Grad Norm 3.0138(4.3788) | Total Time 0.00(0.00)\n",
      "Iter 1365 | Time 61.7366(60.1664) | Bit/dim 3.6884(3.6940) | Xent 0.9444(0.9493) | Loss 9.0570(9.8168) | Error 0.3287(0.3383) Steps 652(641.67) | Grad Norm 3.6701(4.3576) | Total Time 0.00(0.00)\n",
      "Iter 1366 | Time 60.6965(60.1823) | Bit/dim 3.6857(3.6937) | Xent 0.9468(0.9493) | Loss 9.3260(9.8021) | Error 0.3383(0.3383) Steps 640(641.62) | Grad Norm 3.5157(4.3323) | Total Time 0.00(0.00)\n",
      "Iter 1367 | Time 59.9905(60.1765) | Bit/dim 3.6900(3.6936) | Xent 0.9320(0.9487) | Loss 9.2820(9.7865) | Error 0.3361(0.3382) Steps 622(641.03) | Grad Norm 4.5509(4.3389) | Total Time 0.00(0.00)\n",
      "Iter 1368 | Time 60.4867(60.1858) | Bit/dim 3.6931(3.6936) | Xent 0.9384(0.9484) | Loss 9.2881(9.7715) | Error 0.3344(0.3381) Steps 646(641.18) | Grad Norm 3.9964(4.3286) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0228 | Time 23.3024, Epoch Time 400.5926(393.2017), Bit/dim 3.6906(best: 3.6920), Xent 0.9340, Loss 4.1576, Error 0.3289(best: 0.3306)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1369 | Time 58.9966(60.1501) | Bit/dim 3.6875(3.6934) | Xent 0.9270(0.9478) | Loss 12.4332(9.8514) | Error 0.3281(0.3378) Steps 610(640.25) | Grad Norm 3.4135(4.3012) | Total Time 0.00(0.00)\n",
      "Iter 1370 | Time 59.0716(60.1178) | Bit/dim 3.6809(3.6930) | Xent 0.9251(0.9471) | Loss 9.2255(9.8326) | Error 0.3284(0.3375) Steps 652(640.60) | Grad Norm 4.6153(4.3106) | Total Time 0.00(0.00)\n",
      "Iter 1371 | Time 63.5760(60.2215) | Bit/dim 3.6969(3.6932) | Xent 0.9555(0.9474) | Loss 9.3810(9.8190) | Error 0.3417(0.3376) Steps 640(640.58) | Grad Norm 5.2523(4.3388) | Total Time 0.00(0.00)\n",
      "Iter 1372 | Time 61.9619(60.2737) | Bit/dim 3.6823(3.6928) | Xent 0.9363(0.9470) | Loss 9.2960(9.8033) | Error 0.3256(0.3373) Steps 658(641.10) | Grad Norm 4.4489(4.3421) | Total Time 0.00(0.00)\n",
      "Iter 1373 | Time 58.2624(60.2134) | Bit/dim 3.6901(3.6928) | Xent 0.9325(0.9466) | Loss 9.2988(9.7882) | Error 0.3376(0.3373) Steps 616(640.35) | Grad Norm 2.6557(4.2915) | Total Time 0.00(0.00)\n",
      "Iter 1374 | Time 62.9978(60.2969) | Bit/dim 3.6928(3.6928) | Xent 0.9489(0.9467) | Loss 9.2629(9.7724) | Error 0.3371(0.3373) Steps 640(640.34) | Grad Norm 4.5129(4.2982) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0229 | Time 23.5486, Epoch Time 404.8069(393.5498), Bit/dim 3.6961(best: 3.6906), Xent 0.9309, Loss 4.1616, Error 0.3288(best: 0.3289)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1375 | Time 61.3342(60.3281) | Bit/dim 3.6907(3.6927) | Xent 0.9299(0.9462) | Loss 12.6484(9.8587) | Error 0.3297(0.3371) Steps 652(640.69) | Grad Norm 2.4546(4.2429) | Total Time 0.00(0.00)\n",
      "Iter 1376 | Time 63.3144(60.4176) | Bit/dim 3.7054(3.6931) | Xent 0.9256(0.9455) | Loss 9.3672(9.8440) | Error 0.3276(0.3368) Steps 628(640.31) | Grad Norm 4.9096(4.2629) | Total Time 0.00(0.00)\n",
      "Iter 1377 | Time 59.0666(60.3771) | Bit/dim 3.6833(3.6928) | Xent 0.9433(0.9455) | Loss 9.4221(9.8313) | Error 0.3330(0.3367) Steps 646(640.48) | Grad Norm 5.0184(4.2855) | Total Time 0.00(0.00)\n",
      "Iter 1378 | Time 63.2531(60.4634) | Bit/dim 3.6937(3.6928) | Xent 0.9417(0.9454) | Loss 9.4636(9.8203) | Error 0.3395(0.3368) Steps 664(641.19) | Grad Norm 4.4898(4.2917) | Total Time 0.00(0.00)\n",
      "Iter 1379 | Time 60.0558(60.4512) | Bit/dim 3.6876(3.6927) | Xent 0.9132(0.9444) | Loss 8.9172(9.7932) | Error 0.3231(0.3363) Steps 676(642.23) | Grad Norm 2.3043(4.2321) | Total Time 0.00(0.00)\n",
      "Iter 1380 | Time 58.0778(60.3800) | Bit/dim 3.6813(3.6923) | Xent 0.9198(0.9437) | Loss 8.9728(9.7686) | Error 0.3287(0.3361) Steps 616(641.44) | Grad Norm 1.1849(4.1406) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0230 | Time 23.1595, Epoch Time 404.4669(393.8773), Bit/dim 3.6941(best: 3.6906), Xent 0.9260, Loss 4.1571, Error 0.3307(best: 0.3288)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1381 | Time 58.6350(60.3276) | Bit/dim 3.6940(3.6924) | Xent 0.9339(0.9434) | Loss 12.1232(9.8392) | Error 0.3365(0.3361) Steps 646(641.58) | Grad Norm 1.7975(4.0703) | Total Time 0.00(0.00)\n",
      "Iter 1382 | Time 60.8131(60.3422) | Bit/dim 3.6865(3.6922) | Xent 0.9032(0.9422) | Loss 9.1094(9.8173) | Error 0.3250(0.3358) Steps 640(641.53) | Grad Norm 1.9278(4.0061) | Total Time 0.00(0.00)\n",
      "Iter 1383 | Time 64.9416(60.4802) | Bit/dim 3.6942(3.6922) | Xent 0.9112(0.9412) | Loss 9.2293(9.7997) | Error 0.3191(0.3353) Steps 646(641.67) | Grad Norm 2.9400(3.9741) | Total Time 0.00(0.00)\n",
      "Iter 1384 | Time 59.0671(60.4378) | Bit/dim 3.6897(3.6922) | Xent 0.9227(0.9407) | Loss 9.0524(9.7773) | Error 0.3279(0.3351) Steps 640(641.62) | Grad Norm 3.4053(3.9570) | Total Time 0.00(0.00)\n",
      "Iter 1385 | Time 62.1116(60.4880) | Bit/dim 3.6969(3.6923) | Xent 0.9558(0.9411) | Loss 9.3778(9.7653) | Error 0.3384(0.3352) Steps 616(640.85) | Grad Norm 3.4516(3.9419) | Total Time 0.00(0.00)\n",
      "Iter 1386 | Time 63.0810(60.5658) | Bit/dim 3.6869(3.6921) | Xent 0.9368(0.9410) | Loss 9.2241(9.7490) | Error 0.3351(0.3352) Steps 628(640.46) | Grad Norm 4.7767(3.9669) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0231 | Time 23.2423, Epoch Time 408.2778(394.3093), Bit/dim 3.6957(best: 3.6906), Xent 0.9468, Loss 4.1691, Error 0.3373(best: 0.3288)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1387 | Time 61.0477(60.5802) | Bit/dim 3.6963(3.6923) | Xent 0.9379(0.9409) | Loss 12.6939(9.8374) | Error 0.3340(0.3351) Steps 640(640.45) | Grad Norm 6.8818(4.0543) | Total Time 0.00(0.00)\n",
      "Iter 1388 | Time 62.9031(60.6499) | Bit/dim 3.6864(3.6921) | Xent 0.9795(0.9421) | Loss 9.3203(9.8219) | Error 0.3496(0.3356) Steps 634(640.26) | Grad Norm 7.3897(4.1544) | Total Time 0.00(0.00)\n",
      "Iter 1389 | Time 58.5601(60.5872) | Bit/dim 3.6975(3.6923) | Xent 0.9131(0.9412) | Loss 9.1739(9.8024) | Error 0.3234(0.3352) Steps 628(639.89) | Grad Norm 3.5917(4.1375) | Total Time 0.00(0.00)\n",
      "Iter 1390 | Time 60.0607(60.5714) | Bit/dim 3.6895(3.6922) | Xent 0.9271(0.9408) | Loss 9.1108(9.7817) | Error 0.3341(0.3352) Steps 658(640.43) | Grad Norm 3.2020(4.1095) | Total Time 0.00(0.00)\n",
      "Iter 1391 | Time 58.4907(60.5090) | Bit/dim 3.6813(3.6919) | Xent 0.9418(0.9408) | Loss 9.1535(9.7628) | Error 0.3354(0.3352) Steps 658(640.96) | Grad Norm 5.3247(4.1459) | Total Time 0.00(0.00)\n",
      "Iter 1392 | Time 59.5643(60.4807) | Bit/dim 3.6917(3.6918) | Xent 0.9457(0.9410) | Loss 9.1155(9.7434) | Error 0.3419(0.3354) Steps 628(640.57) | Grad Norm 6.7146(4.2230) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0232 | Time 23.6933, Epoch Time 400.2538(394.4877), Bit/dim 3.6944(best: 3.6906), Xent 0.9490, Loss 4.1689, Error 0.3355(best: 0.3288)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1393 | Time 56.5408(60.3625) | Bit/dim 3.6931(3.6919) | Xent 0.9541(0.9413) | Loss 12.3065(9.8203) | Error 0.3393(0.3355) Steps 622(640.01) | Grad Norm 7.3754(4.3176) | Total Time 0.00(0.00)\n",
      "Iter 1394 | Time 60.4944(60.3664) | Bit/dim 3.7038(3.6922) | Xent 0.9136(0.9405) | Loss 9.1344(9.7997) | Error 0.3231(0.3351) Steps 652(640.37) | Grad Norm 4.7931(4.3318) | Total Time 0.00(0.00)\n",
      "Iter 1395 | Time 58.7913(60.3192) | Bit/dim 3.6952(3.6923) | Xent 0.9556(0.9410) | Loss 9.1462(9.7801) | Error 0.3415(0.3353) Steps 616(639.64) | Grad Norm 3.9344(4.3199) | Total Time 0.00(0.00)\n",
      "Iter 1396 | Time 63.5780(60.4169) | Bit/dim 3.6885(3.6922) | Xent 0.9403(0.9409) | Loss 9.0602(9.7585) | Error 0.3290(0.3351) Steps 652(640.01) | Grad Norm 4.0974(4.3132) | Total Time 0.00(0.00)\n",
      "Iter 1397 | Time 58.9136(60.3718) | Bit/dim 3.6884(3.6921) | Xent 0.9157(0.9402) | Loss 8.9954(9.7356) | Error 0.3290(0.3349) Steps 658(640.55) | Grad Norm 5.4978(4.3488) | Total Time 0.00(0.00)\n",
      "Iter 1398 | Time 63.1318(60.4546) | Bit/dim 3.6925(3.6921) | Xent 0.9376(0.9401) | Loss 9.4055(9.7257) | Error 0.3331(0.3349) Steps 682(641.79) | Grad Norm 5.1709(4.3734) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0233 | Time 23.6213, Epoch Time 401.6058(394.7012), Bit/dim 3.6929(best: 3.6906), Xent 0.9467, Loss 4.1662, Error 0.3362(best: 0.3288)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1399 | Time 59.6572(60.4307) | Bit/dim 3.6794(3.6917) | Xent 0.9425(0.9402) | Loss 12.5272(9.8098) | Error 0.3361(0.3349) Steps 628(641.38) | Grad Norm 5.1405(4.3964) | Total Time 0.00(0.00)\n",
      "Iter 1400 | Time 56.2242(60.3045) | Bit/dim 3.7051(3.6921) | Xent 0.9346(0.9400) | Loss 9.3031(9.7946) | Error 0.3297(0.3348) Steps 634(641.16) | Grad Norm 4.9557(4.4132) | Total Time 0.00(0.00)\n",
      "Iter 1401 | Time 57.1956(60.2113) | Bit/dim 3.6970(3.6923) | Xent 0.9387(0.9400) | Loss 9.3129(9.7801) | Error 0.3315(0.3347) Steps 616(640.40) | Grad Norm 4.1775(4.4061) | Total Time 0.00(0.00)\n",
      "Iter 1402 | Time 64.6422(60.3442) | Bit/dim 3.6805(3.6919) | Xent 0.9351(0.9398) | Loss 9.3673(9.7677) | Error 0.3324(0.3346) Steps 694(642.01) | Grad Norm 3.8254(4.3887) | Total Time 0.00(0.00)\n",
      "Iter 1403 | Time 61.5603(60.3807) | Bit/dim 3.6888(3.6918) | Xent 0.9177(0.9392) | Loss 9.3424(9.7550) | Error 0.3241(0.3343) Steps 640(641.95) | Grad Norm 5.3325(4.4170) | Total Time 0.00(0.00)\n",
      "Iter 1404 | Time 62.9844(60.4588) | Bit/dim 3.6871(3.6917) | Xent 0.9379(0.9391) | Loss 9.1676(9.7374) | Error 0.3421(0.3345) Steps 628(641.53) | Grad Norm 3.0743(4.3767) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0234 | Time 23.2776, Epoch Time 402.1439(394.9245), Bit/dim 3.6855(best: 3.6906), Xent 0.9328, Loss 4.1520, Error 0.3294(best: 0.3288)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1405 | Time 59.6838(60.4355) | Bit/dim 3.6914(3.6917) | Xent 0.9250(0.9387) | Loss 12.5015(9.8203) | Error 0.3237(0.3342) Steps 652(641.85) | Grad Norm 3.3396(4.3456) | Total Time 0.00(0.00)\n",
      "Iter 1406 | Time 60.5649(60.4394) | Bit/dim 3.6845(3.6915) | Xent 0.9259(0.9383) | Loss 9.4690(9.8097) | Error 0.3339(0.3342) Steps 646(641.97) | Grad Norm 5.4042(4.3774) | Total Time 0.00(0.00)\n",
      "Iter 1407 | Time 61.7052(60.4774) | Bit/dim 3.6923(3.6915) | Xent 0.9209(0.9378) | Loss 9.3172(9.7950) | Error 0.3335(0.3342) Steps 640(641.91) | Grad Norm 4.2207(4.3727) | Total Time 0.00(0.00)\n",
      "Iter 1408 | Time 59.1885(60.4387) | Bit/dim 3.6967(3.6916) | Xent 0.9134(0.9371) | Loss 9.2672(9.7791) | Error 0.3229(0.3338) Steps 634(641.68) | Grad Norm 3.5574(4.3482) | Total Time 0.00(0.00)\n",
      "Iter 1409 | Time 62.1937(60.4914) | Bit/dim 3.6885(3.6915) | Xent 0.9137(0.9364) | Loss 9.3055(9.7649) | Error 0.3251(0.3336) Steps 640(641.63) | Grad Norm 3.9382(4.3359) | Total Time 0.00(0.00)\n",
      "Iter 1410 | Time 61.7488(60.5291) | Bit/dim 3.6886(3.6915) | Xent 0.9418(0.9365) | Loss 9.3040(9.7511) | Error 0.3364(0.3337) Steps 646(641.76) | Grad Norm 3.7832(4.3193) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0235 | Time 23.1305, Epoch Time 404.5401(395.2130), Bit/dim 3.6883(best: 3.6855), Xent 0.9300, Loss 4.1533, Error 0.3304(best: 0.3288)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1411 | Time 63.4904(60.6179) | Bit/dim 3.6983(3.6917) | Xent 0.9159(0.9359) | Loss 12.6556(9.8382) | Error 0.3304(0.3336) Steps 652(642.06) | Grad Norm 4.4104(4.3221) | Total Time 0.00(0.00)\n",
      "Iter 1412 | Time 59.6823(60.5899) | Bit/dim 3.6883(3.6916) | Xent 0.9217(0.9355) | Loss 9.3596(9.8239) | Error 0.3269(0.3334) Steps 634(641.82) | Grad Norm 3.5525(4.2990) | Total Time 0.00(0.00)\n",
      "Iter 1413 | Time 64.4608(60.7060) | Bit/dim 3.6850(3.6914) | Xent 0.9194(0.9350) | Loss 9.0599(9.8010) | Error 0.3295(0.3332) Steps 676(642.85) | Grad Norm 2.9744(4.2593) | Total Time 0.00(0.00)\n",
      "Iter 1414 | Time 58.8480(60.6503) | Bit/dim 3.7005(3.6916) | Xent 0.9152(0.9344) | Loss 9.2292(9.7838) | Error 0.3281(0.3331) Steps 640(642.76) | Grad Norm 3.6907(4.2422) | Total Time 0.00(0.00)\n",
      "Iter 1415 | Time 59.9751(60.6300) | Bit/dim 3.6858(3.6915) | Xent 0.9152(0.9338) | Loss 9.3158(9.7698) | Error 0.3240(0.3328) Steps 652(643.04) | Grad Norm 2.0897(4.1776) | Total Time 0.00(0.00)\n",
      "Iter 1416 | Time 61.1534(60.6457) | Bit/dim 3.6763(3.6910) | Xent 0.9218(0.9335) | Loss 9.2831(9.7552) | Error 0.3301(0.3327) Steps 634(642.77) | Grad Norm 3.7504(4.1648) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0236 | Time 24.0878, Epoch Time 408.1179(395.6001), Bit/dim 3.6873(best: 3.6855), Xent 0.9290, Loss 4.1518, Error 0.3279(best: 0.3288)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1417 | Time 63.4360(60.7294) | Bit/dim 3.6791(3.6907) | Xent 0.9295(0.9334) | Loss 12.5230(9.8382) | Error 0.3253(0.3325) Steps 634(642.50) | Grad Norm 4.3094(4.1691) | Total Time 0.00(0.00)\n",
      "Iter 1418 | Time 60.3108(60.7169) | Bit/dim 3.6951(3.6908) | Xent 0.9263(0.9331) | Loss 9.0984(9.8160) | Error 0.3277(0.3324) Steps 610(641.53) | Grad Norm 3.5820(4.1515) | Total Time 0.00(0.00)\n",
      "Iter 1419 | Time 58.3967(60.6472) | Bit/dim 3.7005(3.6911) | Xent 0.9199(0.9327) | Loss 9.3307(9.8015) | Error 0.3333(0.3324) Steps 646(641.66) | Grad Norm 3.8641(4.1429) | Total Time 0.00(0.00)\n",
      "Iter 1420 | Time 57.5194(60.5534) | Bit/dim 3.6817(3.6908) | Xent 0.9265(0.9326) | Loss 9.1700(9.7825) | Error 0.3230(0.3321) Steps 652(641.97) | Grad Norm 2.8448(4.1040) | Total Time 0.00(0.00)\n",
      "Iter 1421 | Time 62.8407(60.6220) | Bit/dim 3.6864(3.6907) | Xent 0.9249(0.9323) | Loss 9.2133(9.7654) | Error 0.3310(0.3321) Steps 640(641.91) | Grad Norm 3.6985(4.0918) | Total Time 0.00(0.00)\n",
      "Iter 1422 | Time 62.7495(60.6859) | Bit/dim 3.6946(3.6908) | Xent 0.9095(0.9316) | Loss 9.3150(9.7519) | Error 0.3225(0.3318) Steps 664(642.58) | Grad Norm 5.2474(4.1265) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0237 | Time 23.1210, Epoch Time 404.7203(395.8737), Bit/dim 3.6878(best: 3.6855), Xent 0.9438, Loss 4.1597, Error 0.3350(best: 0.3279)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1423 | Time 60.8687(60.6913) | Bit/dim 3.6911(3.6908) | Xent 0.9479(0.9321) | Loss 12.3744(9.8306) | Error 0.3411(0.3321) Steps 640(642.50) | Grad Norm 6.7291(4.2045) | Total Time 0.00(0.00)\n",
      "Iter 1424 | Time 65.9227(60.8483) | Bit/dim 3.6940(3.6909) | Xent 0.9617(0.9330) | Loss 9.2252(9.8124) | Error 0.3455(0.3325) Steps 646(642.60) | Grad Norm 7.7295(4.3103) | Total Time 0.00(0.00)\n",
      "Iter 1425 | Time 58.6478(60.7823) | Bit/dim 3.6792(3.6905) | Xent 0.9511(0.9336) | Loss 9.2391(9.7952) | Error 0.3405(0.3327) Steps 646(642.71) | Grad Norm 6.3906(4.3727) | Total Time 0.00(0.00)\n",
      "Iter 1426 | Time 66.4566(60.9525) | Bit/dim 3.6889(3.6905) | Xent 0.9676(0.9346) | Loss 9.1543(9.7760) | Error 0.3373(0.3328) Steps 604(641.55) | Grad Norm 11.8195(4.5961) | Total Time 0.00(0.00)\n",
      "Iter 1427 | Time 59.1753(60.8992) | Bit/dim 3.6976(3.6907) | Xent 0.9681(0.9356) | Loss 9.2753(9.7610) | Error 0.3440(0.3332) Steps 640(641.50) | Grad Norm 10.3947(4.7701) | Total Time 0.00(0.00)\n",
      "Iter 1428 | Time 58.8293(60.8371) | Bit/dim 3.6769(3.6903) | Xent 0.9309(0.9354) | Loss 8.9003(9.7352) | Error 0.3325(0.3332) Steps 658(641.99) | Grad Norm 5.0456(4.7783) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0238 | Time 22.8875, Epoch Time 409.2845(396.2761), Bit/dim 3.6923(best: 3.6855), Xent 0.9644, Loss 4.1745, Error 0.3399(best: 0.3279)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1429 | Time 65.0666(60.9640) | Bit/dim 3.6857(3.6902) | Xent 0.9672(0.9364) | Loss 12.6618(9.8230) | Error 0.3436(0.3335) Steps 646(642.11) | Grad Norm 11.9230(4.9927) | Total Time 0.00(0.00)\n",
      "Iter 1430 | Time 64.7845(61.0786) | Bit/dim 3.6841(3.6900) | Xent 0.9586(0.9371) | Loss 9.3764(9.8096) | Error 0.3387(0.3336) Steps 634(641.87) | Grad Norm 8.8514(5.1084) | Total Time 0.00(0.00)\n",
      "Iter 1431 | Time 63.7362(61.1583) | Bit/dim 3.7026(3.6903) | Xent 0.9506(0.9375) | Loss 9.2998(9.7943) | Error 0.3375(0.3338) Steps 652(642.17) | Grad Norm 9.3361(5.2353) | Total Time 0.00(0.00)\n",
      "Iter 1432 | Time 61.6265(61.1724) | Bit/dim 3.7041(3.6908) | Xent 1.0052(0.9395) | Loss 9.2679(9.7785) | Error 0.3528(0.3343) Steps 646(642.29) | Grad Norm 10.0127(5.3786) | Total Time 0.00(0.00)\n",
      "Iter 1433 | Time 67.4333(61.3602) | Bit/dim 3.6861(3.6906) | Xent 0.9414(0.9396) | Loss 9.1734(9.7603) | Error 0.3400(0.3345) Steps 628(641.86) | Grad Norm 5.2850(5.3758) | Total Time 0.00(0.00)\n",
      "Iter 1434 | Time 64.6859(61.4600) | Bit/dim 3.6909(3.6906) | Xent 0.9596(0.9402) | Loss 9.1524(9.7421) | Error 0.3400(0.3347) Steps 670(642.70) | Grad Norm 8.9353(5.4826) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0239 | Time 23.6414, Epoch Time 426.9089(397.1950), Bit/dim 3.6825(best: 3.6855), Xent 0.9458, Loss 4.1554, Error 0.3342(best: 0.3279)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1435 | Time 62.6520(61.4957) | Bit/dim 3.6814(3.6903) | Xent 0.9350(0.9400) | Loss 12.2597(9.8176) | Error 0.3355(0.3347) Steps 658(643.16) | Grad Norm 4.9829(5.4676) | Total Time 0.00(0.00)\n",
      "Iter 1436 | Time 63.3999(61.5528) | Bit/dim 3.6919(3.6904) | Xent 0.9461(0.9402) | Loss 9.3232(9.8028) | Error 0.3390(0.3348) Steps 646(643.25) | Grad Norm 8.1610(5.5484) | Total Time 0.00(0.00)\n",
      "Iter 1437 | Time 67.2468(61.7237) | Bit/dim 3.6772(3.6900) | Xent 0.9356(0.9400) | Loss 9.3542(9.7893) | Error 0.3310(0.3347) Steps 646(643.33) | Grad Norm 5.1752(5.5372) | Total Time 0.00(0.00)\n",
      "Iter 1438 | Time 61.9809(61.7314) | Bit/dim 3.6920(3.6901) | Xent 0.9607(0.9407) | Loss 9.1096(9.7689) | Error 0.3471(0.3351) Steps 664(643.95) | Grad Norm 6.9368(5.5792) | Total Time 0.00(0.00)\n",
      "Iter 1439 | Time 63.5680(61.7865) | Bit/dim 3.6981(3.6903) | Xent 0.9193(0.9400) | Loss 9.0875(9.7485) | Error 0.3303(0.3349) Steps 676(644.91) | Grad Norm 5.3867(5.5734) | Total Time 0.00(0.00)\n",
      "Iter 1440 | Time 64.6556(61.8725) | Bit/dim 3.6919(3.6904) | Xent 0.9454(0.9402) | Loss 9.3065(9.7352) | Error 0.3334(0.3349) Steps 646(644.95) | Grad Norm 4.2116(5.5325) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0240 | Time 22.9576, Epoch Time 423.1858(397.9748), Bit/dim 3.6915(best: 3.6825), Xent 0.9281, Loss 4.1555, Error 0.3278(best: 0.3279)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1441 | Time 59.4333(61.7994) | Bit/dim 3.6866(3.6902) | Xent 0.9273(0.9398) | Loss 11.8709(9.7993) | Error 0.3319(0.3348) Steps 628(644.44) | Grad Norm 3.7267(5.4784) | Total Time 0.00(0.00)\n",
      "Iter 1442 | Time 64.7473(61.8878) | Bit/dim 3.6848(3.6901) | Xent 0.9185(0.9392) | Loss 9.2217(9.7820) | Error 0.3237(0.3345) Steps 646(644.48) | Grad Norm 3.9489(5.4325) | Total Time 0.00(0.00)\n",
      "Iter 1443 | Time 63.8049(61.9453) | Bit/dim 3.6905(3.6901) | Xent 0.9284(0.9388) | Loss 9.2738(9.7667) | Error 0.3345(0.3345) Steps 628(643.99) | Grad Norm 4.7644(5.4124) | Total Time 0.00(0.00)\n",
      "Iter 1444 | Time 62.4798(61.9614) | Bit/dim 3.6838(3.6899) | Xent 0.9149(0.9381) | Loss 9.2248(9.7505) | Error 0.3334(0.3344) Steps 634(643.69) | Grad Norm 5.9564(5.4288) | Total Time 0.00(0.00)\n",
      "Iter 1445 | Time 65.1764(62.0578) | Bit/dim 3.6937(3.6900) | Xent 0.9192(0.9376) | Loss 9.4103(9.7403) | Error 0.3244(0.3341) Steps 676(644.66) | Grad Norm 3.4256(5.3687) | Total Time 0.00(0.00)\n",
      "Iter 1446 | Time 62.6794(62.0765) | Bit/dim 3.6947(3.6902) | Xent 0.9292(0.9373) | Loss 9.1779(9.7234) | Error 0.3319(0.3341) Steps 622(643.98) | Grad Norm 7.8677(5.4436) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0241 | Time 23.5941, Epoch Time 418.2393(398.5827), Bit/dim 3.6879(best: 3.6825), Xent 0.9282, Loss 4.1521, Error 0.3308(best: 0.3278)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1447 | Time 58.2974(61.9631) | Bit/dim 3.6983(3.6904) | Xent 0.9340(0.9372) | Loss 12.3181(9.8012) | Error 0.3304(0.3339) Steps 646(644.04) | Grad Norm 5.0232(5.4310) | Total Time 0.00(0.00)\n",
      "Iter 1448 | Time 59.5469(61.8906) | Bit/dim 3.6697(3.6898) | Xent 0.8969(0.9360) | Loss 9.2039(9.7833) | Error 0.3204(0.3335) Steps 646(644.10) | Grad Norm 3.4842(5.3726) | Total Time 0.00(0.00)\n",
      "Iter 1449 | Time 59.7331(61.8259) | Bit/dim 3.6794(3.6895) | Xent 0.9179(0.9355) | Loss 9.1099(9.7631) | Error 0.3315(0.3335) Steps 646(644.16) | Grad Norm 1.8999(5.2684) | Total Time 0.00(0.00)\n",
      "Iter 1450 | Time 65.0994(61.9241) | Bit/dim 3.6868(3.6894) | Xent 0.9100(0.9347) | Loss 9.2627(9.7481) | Error 0.3217(0.3331) Steps 646(644.21) | Grad Norm 3.2461(5.2078) | Total Time 0.00(0.00)\n",
      "Iter 1451 | Time 64.1015(61.9894) | Bit/dim 3.6923(3.6895) | Xent 0.9577(0.9354) | Loss 9.2623(9.7335) | Error 0.3420(0.3334) Steps 658(644.62) | Grad Norm 6.3758(5.2428) | Total Time 0.00(0.00)\n",
      "Iter 1452 | Time 60.2232(61.9364) | Bit/dim 3.6931(3.6896) | Xent 0.9397(0.9355) | Loss 9.2049(9.7177) | Error 0.3351(0.3334) Steps 664(645.21) | Grad Norm 9.3864(5.3671) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0242 | Time 23.2692, Epoch Time 406.5671(398.8222), Bit/dim 3.6839(best: 3.6825), Xent 0.9520, Loss 4.1599, Error 0.3383(best: 0.3278)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1453 | Time 55.5452(61.7447) | Bit/dim 3.6807(3.6893) | Xent 0.9418(0.9357) | Loss 12.3464(9.7965) | Error 0.3364(0.3335) Steps 616(644.33) | Grad Norm 7.0812(5.4185) | Total Time 0.00(0.00)\n",
      "Iter 1454 | Time 66.6488(61.8918) | Bit/dim 3.6928(3.6894) | Xent 0.8925(0.9344) | Loss 9.0339(9.7737) | Error 0.3174(0.3330) Steps 640(644.20) | Grad Norm 2.4964(5.3309) | Total Time 0.00(0.00)\n",
      "Iter 1455 | Time 61.7045(61.8862) | Bit/dim 3.6961(3.6896) | Xent 0.9264(0.9342) | Loss 9.3412(9.7607) | Error 0.3315(0.3330) Steps 646(644.25) | Grad Norm 6.3577(5.3617) | Total Time 0.00(0.00)\n",
      "Iter 1456 | Time 63.0792(61.9220) | Bit/dim 3.6743(3.6892) | Xent 0.9542(0.9348) | Loss 9.3926(9.7496) | Error 0.3429(0.3333) Steps 670(645.03) | Grad Norm 8.3719(5.4520) | Total Time 0.00(0.00)\n",
      "Iter 1457 | Time 64.4140(61.9967) | Bit/dim 3.6932(3.6893) | Xent 0.9600(0.9355) | Loss 9.4904(9.7419) | Error 0.3353(0.3334) Steps 646(645.06) | Grad Norm 8.6642(5.5484) | Total Time 0.00(0.00)\n",
      "Iter 1458 | Time 58.1342(61.8809) | Bit/dim 3.6896(3.6893) | Xent 0.9352(0.9355) | Loss 9.0364(9.7207) | Error 0.3341(0.3334) Steps 634(644.72) | Grad Norm 5.8519(5.5575) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0243 | Time 23.4932, Epoch Time 410.8821(399.1840), Bit/dim 3.6787(best: 3.6825), Xent 0.9449, Loss 4.1512, Error 0.3354(best: 0.3278)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1459 | Time 61.4297(61.8673) | Bit/dim 3.6974(3.6895) | Xent 0.9268(0.9353) | Loss 12.6563(9.8088) | Error 0.3297(0.3333) Steps 682(645.84) | Grad Norm 7.3103(5.6100) | Total Time 0.00(0.00)\n",
      "Iter 1460 | Time 62.6681(61.8913) | Bit/dim 3.6833(3.6893) | Xent 0.9509(0.9357) | Loss 9.2472(9.7919) | Error 0.3375(0.3334) Steps 652(646.03) | Grad Norm 8.5462(5.6981) | Total Time 0.00(0.00)\n",
      "Iter 1461 | Time 65.3392(61.9948) | Bit/dim 3.6876(3.6893) | Xent 0.9271(0.9355) | Loss 9.3926(9.7799) | Error 0.3289(0.3333) Steps 664(646.57) | Grad Norm 6.3041(5.7163) | Total Time 0.00(0.00)\n",
      "Iter 1462 | Time 67.0047(62.1451) | Bit/dim 3.6902(3.6893) | Xent 0.9761(0.9367) | Loss 9.3920(9.7683) | Error 0.3519(0.3338) Steps 646(646.55) | Grad Norm 7.2222(5.7615) | Total Time 0.00(0.00)\n",
      "Iter 1463 | Time 59.4416(62.0640) | Bit/dim 3.6864(3.6892) | Xent 0.9288(0.9365) | Loss 9.0930(9.7480) | Error 0.3314(0.3337) Steps 652(646.71) | Grad Norm 4.8738(5.7349) | Total Time 0.00(0.00)\n",
      "Iter 1464 | Time 59.9690(62.0011) | Bit/dim 3.6727(3.6887) | Xent 0.9341(0.9364) | Loss 9.1650(9.7305) | Error 0.3291(0.3336) Steps 658(647.05) | Grad Norm 7.7811(5.7962) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0244 | Time 23.3078, Epoch Time 415.4255(399.6713), Bit/dim 3.6869(best: 3.6787), Xent 0.9400, Loss 4.1569, Error 0.3312(best: 0.3278)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1465 | Time 63.7836(62.0546) | Bit/dim 3.6836(3.6886) | Xent 0.9288(0.9362) | Loss 12.6271(9.8174) | Error 0.3301(0.3335) Steps 634(646.66) | Grad Norm 5.1999(5.7784) | Total Time 0.00(0.00)\n",
      "Iter 1466 | Time 62.0743(62.0552) | Bit/dim 3.6889(3.6886) | Xent 0.9314(0.9360) | Loss 8.8452(9.7883) | Error 0.3271(0.3333) Steps 628(646.10) | Grad Norm 6.8736(5.8112) | Total Time 0.00(0.00)\n",
      "Iter 1467 | Time 64.2993(62.1225) | Bit/dim 3.6809(3.6884) | Xent 0.9180(0.9355) | Loss 9.1089(9.7679) | Error 0.3303(0.3332) Steps 658(646.46) | Grad Norm 4.3823(5.7683) | Total Time 0.00(0.00)\n",
      "Iter 1468 | Time 62.1729(62.1240) | Bit/dim 3.6916(3.6885) | Xent 0.9345(0.9354) | Loss 9.2362(9.7519) | Error 0.3336(0.3332) Steps 652(646.62) | Grad Norm 6.4927(5.7901) | Total Time 0.00(0.00)\n",
      "Iter 1469 | Time 62.8185(62.1449) | Bit/dim 3.6855(3.6884) | Xent 0.9214(0.9350) | Loss 9.2453(9.7367) | Error 0.3291(0.3331) Steps 670(647.32) | Grad Norm 4.6811(5.7568) | Total Time 0.00(0.00)\n",
      "Iter 1470 | Time 60.4341(62.0935) | Bit/dim 3.6791(3.6881) | Xent 0.9181(0.9345) | Loss 9.0059(9.7148) | Error 0.3334(0.3331) Steps 640(647.10) | Grad Norm 6.5481(5.7805) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0245 | Time 23.2260, Epoch Time 415.1915(400.1369), Bit/dim 3.6881(best: 3.6787), Xent 0.9342, Loss 4.1552, Error 0.3313(best: 0.3278)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1471 | Time 62.7087(62.1120) | Bit/dim 3.6832(3.6879) | Xent 0.9236(0.9342) | Loss 12.6770(9.8037) | Error 0.3346(0.3332) Steps 646(647.07) | Grad Norm 6.3710(5.7983) | Total Time 0.00(0.00)\n",
      "Iter 1472 | Time 62.7614(62.1315) | Bit/dim 3.6976(3.6882) | Xent 0.9151(0.9336) | Loss 9.3115(9.7889) | Error 0.3286(0.3330) Steps 664(647.58) | Grad Norm 4.6622(5.7642) | Total Time 0.00(0.00)\n",
      "Iter 1473 | Time 60.7252(62.0893) | Bit/dim 3.6788(3.6880) | Xent 0.9488(0.9341) | Loss 9.1615(9.7701) | Error 0.3386(0.3332) Steps 658(647.89) | Grad Norm 5.6680(5.7613) | Total Time 0.00(0.00)\n",
      "Iter 1474 | Time 60.1725(62.0318) | Bit/dim 3.6872(3.6879) | Xent 0.9176(0.9336) | Loss 9.0738(9.7492) | Error 0.3227(0.3329) Steps 664(648.38) | Grad Norm 4.5884(5.7261) | Total Time 0.00(0.00)\n",
      "Iter 1475 | Time 63.0694(62.0629) | Bit/dim 3.6756(3.6876) | Xent 0.9117(0.9329) | Loss 9.2367(9.7338) | Error 0.3296(0.3328) Steps 688(649.56) | Grad Norm 4.8842(5.7008) | Total Time 0.00(0.00)\n",
      "Iter 1476 | Time 66.9986(62.2110) | Bit/dim 3.6853(3.6875) | Xent 0.9118(0.9323) | Loss 9.1112(9.7152) | Error 0.3261(0.3326) Steps 652(649.64) | Grad Norm 6.1485(5.7143) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0246 | Time 24.0433, Epoch Time 416.8245(400.6375), Bit/dim 3.6871(best: 3.6787), Xent 0.9269, Loss 4.1506, Error 0.3296(best: 0.3278)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1477 | Time 60.3787(62.1560) | Bit/dim 3.6766(3.6872) | Xent 0.9139(0.9317) | Loss 11.4928(9.7685) | Error 0.3270(0.3324) Steps 676(650.43) | Grad Norm 5.4380(5.7060) | Total Time 0.00(0.00)\n",
      "Iter 1478 | Time 62.4564(62.1650) | Bit/dim 3.6911(3.6873) | Xent 0.9230(0.9315) | Loss 9.2343(9.7525) | Error 0.3250(0.3322) Steps 634(649.94) | Grad Norm 5.6694(5.7049) | Total Time 0.00(0.00)\n",
      "Iter 1479 | Time 60.2765(62.1084) | Bit/dim 3.6761(3.6869) | Xent 0.9147(0.9310) | Loss 9.0701(9.7320) | Error 0.3336(0.3322) Steps 640(649.64) | Grad Norm 6.1447(5.7181) | Total Time 0.00(0.00)\n",
      "Iter 1480 | Time 64.2373(62.1722) | Bit/dim 3.6848(3.6869) | Xent 0.9248(0.9308) | Loss 9.4325(9.7230) | Error 0.3299(0.3322) Steps 646(649.53) | Grad Norm 3.4649(5.6505) | Total Time 0.00(0.00)\n",
      "Iter 1481 | Time 64.4289(62.2399) | Bit/dim 3.6877(3.6869) | Xent 0.9212(0.9305) | Loss 9.4004(9.7133) | Error 0.3289(0.3321) Steps 628(648.88) | Grad Norm 4.6576(5.6207) | Total Time 0.00(0.00)\n",
      "Iter 1482 | Time 59.6802(62.1631) | Bit/dim 3.6849(3.6868) | Xent 0.8957(0.9295) | Loss 8.6585(9.6817) | Error 0.3274(0.3319) Steps 652(648.98) | Grad Norm 3.2352(5.5491) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0247 | Time 23.7213, Epoch Time 411.1556(400.9530), Bit/dim 3.6896(best: 3.6787), Xent 0.9216, Loss 4.1504, Error 0.3240(best: 0.3278)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1483 | Time 68.6106(62.3566) | Bit/dim 3.6956(3.6871) | Xent 0.9049(0.9287) | Loss 12.7392(9.7734) | Error 0.3236(0.3317) Steps 694(650.33) | Grad Norm 4.0651(5.5046) | Total Time 0.00(0.00)\n",
      "Iter 1484 | Time 62.0732(62.3481) | Bit/dim 3.6882(3.6871) | Xent 0.8971(0.9278) | Loss 9.2462(9.7576) | Error 0.3197(0.3313) Steps 634(649.84) | Grad Norm 2.6102(5.4178) | Total Time 0.00(0.00)\n",
      "Iter 1485 | Time 63.2203(62.3742) | Bit/dim 3.6808(3.6870) | Xent 0.9042(0.9271) | Loss 9.2136(9.7413) | Error 0.3237(0.3311) Steps 646(649.72) | Grad Norm 3.7783(5.3686) | Total Time 0.00(0.00)\n",
      "Iter 1486 | Time 62.8346(62.3880) | Bit/dim 3.6727(3.6865) | Xent 0.8887(0.9259) | Loss 9.2068(9.7252) | Error 0.3141(0.3306) Steps 652(649.79) | Grad Norm 2.4279(5.2804) | Total Time 0.00(0.00)\n",
      "Iter 1487 | Time 63.6350(62.4255) | Bit/dim 3.6815(3.6864) | Xent 0.9319(0.9261) | Loss 9.1537(9.7081) | Error 0.3300(0.3306) Steps 658(650.04) | Grad Norm 2.4631(5.1959) | Total Time 0.00(0.00)\n",
      "Iter 1488 | Time 64.3728(62.4839) | Bit/dim 3.6759(3.6861) | Xent 0.8995(0.9253) | Loss 9.3743(9.6981) | Error 0.3203(0.3303) Steps 640(649.74) | Grad Norm 1.9898(5.0997) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0248 | Time 22.9455, Epoch Time 423.7223(401.6361), Bit/dim 3.6822(best: 3.6787), Xent 0.9152, Loss 4.1398, Error 0.3255(best: 0.3240)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1489 | Time 63.9993(62.5293) | Bit/dim 3.6789(3.6858) | Xent 0.9207(0.9252) | Loss 12.5478(9.7836) | Error 0.3316(0.3303) Steps 652(649.80) | Grad Norm 3.5126(5.0521) | Total Time 0.00(0.00)\n",
      "Iter 1490 | Time 58.7695(62.4165) | Bit/dim 3.6788(3.6856) | Xent 0.9012(0.9244) | Loss 9.2127(9.7664) | Error 0.3241(0.3301) Steps 646(649.69) | Grad Norm 5.1902(5.0562) | Total Time 0.00(0.00)\n",
      "Iter 1491 | Time 60.6216(62.3627) | Bit/dim 3.6940(3.6859) | Xent 0.9100(0.9240) | Loss 9.2313(9.7504) | Error 0.3246(0.3299) Steps 646(649.58) | Grad Norm 4.7541(5.0471) | Total Time 0.00(0.00)\n",
      "Iter 1492 | Time 62.9889(62.3815) | Bit/dim 3.6812(3.6857) | Xent 0.9223(0.9240) | Loss 9.1744(9.7331) | Error 0.3314(0.3300) Steps 658(649.83) | Grad Norm 5.1026(5.0488) | Total Time 0.00(0.00)\n",
      "Iter 1493 | Time 62.1630(62.3749) | Bit/dim 3.6925(3.6859) | Xent 0.9240(0.9240) | Loss 9.0908(9.7138) | Error 0.3229(0.3298) Steps 652(649.90) | Grad Norm 7.2792(5.1157) | Total Time 0.00(0.00)\n",
      "Iter 1494 | Time 58.6330(62.2627) | Bit/dim 3.6784(3.6857) | Xent 0.9292(0.9241) | Loss 9.0457(9.6938) | Error 0.3315(0.3298) Steps 640(649.60) | Grad Norm 8.9674(5.2313) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0249 | Time 23.8690, Epoch Time 406.8016(401.7911), Bit/dim 3.6822(best: 3.6787), Xent 0.9156, Loss 4.1400, Error 0.3274(best: 0.3240)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1495 | Time 61.6071(62.2430) | Bit/dim 3.6962(3.6860) | Xent 0.9101(0.9237) | Loss 12.8078(9.7872) | Error 0.3230(0.3296) Steps 634(649.13) | Grad Norm 5.0548(5.2260) | Total Time 0.00(0.00)\n",
      "Iter 1496 | Time 65.3216(62.3354) | Bit/dim 3.6884(3.6861) | Xent 0.9164(0.9235) | Loss 9.3292(9.7735) | Error 0.3256(0.3295) Steps 628(648.50) | Grad Norm 3.1619(5.1640) | Total Time 0.00(0.00)\n",
      "Iter 1497 | Time 63.5375(62.3714) | Bit/dim 3.6805(3.6859) | Xent 0.9226(0.9234) | Loss 9.0883(9.7529) | Error 0.3309(0.3295) Steps 634(648.06) | Grad Norm 6.4342(5.2022) | Total Time 0.00(0.00)\n",
      "Iter 1498 | Time 58.8828(62.2668) | Bit/dim 3.6724(3.6855) | Xent 0.9356(0.9238) | Loss 9.0457(9.7317) | Error 0.3341(0.3297) Steps 634(647.64) | Grad Norm 5.5323(5.2121) | Total Time 0.00(0.00)\n",
      "Iter 1499 | Time 62.7053(62.2799) | Bit/dim 3.6838(3.6855) | Xent 0.9177(0.9236) | Loss 9.0261(9.7105) | Error 0.3306(0.3297) Steps 664(648.13) | Grad Norm 4.4090(5.1880) | Total Time 0.00(0.00)\n",
      "Iter 1500 | Time 62.9366(62.2996) | Bit/dim 3.6746(3.6852) | Xent 0.8956(0.9228) | Loss 9.1130(9.6926) | Error 0.3211(0.3295) Steps 664(648.61) | Grad Norm 3.1086(5.1256) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0250 | Time 23.2206, Epoch Time 414.4316(402.1703), Bit/dim 3.6805(best: 3.6787), Xent 0.9320, Loss 4.1465, Error 0.3336(best: 0.3240)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1501 | Time 59.7523(62.2232) | Bit/dim 3.6886(3.6853) | Xent 0.9218(0.9228) | Loss 12.7205(9.7834) | Error 0.3334(0.3296) Steps 658(648.89) | Grad Norm 5.9919(5.1516) | Total Time 0.00(0.00)\n",
      "Iter 1502 | Time 62.1051(62.2197) | Bit/dim 3.6846(3.6852) | Xent 0.9271(0.9229) | Loss 9.1215(9.7636) | Error 0.3285(0.3295) Steps 634(648.44) | Grad Norm 5.3197(5.1566) | Total Time 0.00(0.00)\n",
      "Iter 1503 | Time 60.4131(62.1655) | Bit/dim 3.6741(3.6849) | Xent 0.9022(0.9223) | Loss 9.1010(9.7437) | Error 0.3215(0.3293) Steps 652(648.55) | Grad Norm 3.3709(5.1030) | Total Time 0.00(0.00)\n",
      "Iter 1504 | Time 64.1737(62.2257) | Bit/dim 3.6810(3.6848) | Xent 0.9654(0.9236) | Loss 8.8045(9.7155) | Error 0.3415(0.3297) Steps 670(649.19) | Grad Norm 8.3061(5.1991) | Total Time 0.00(0.00)\n",
      "Iter 1505 | Time 60.0755(62.1612) | Bit/dim 3.6891(3.6849) | Xent 0.9469(0.9243) | Loss 9.3061(9.7032) | Error 0.3393(0.3300) Steps 616(648.20) | Grad Norm 8.5318(5.2991) | Total Time 0.00(0.00)\n",
      "Iter 1506 | Time 63.7213(62.2080) | Bit/dim 3.6803(3.6848) | Xent 0.8912(0.9233) | Loss 9.3264(9.6919) | Error 0.3227(0.3297) Steps 640(647.95) | Grad Norm 4.8135(5.2845) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0251 | Time 23.2460, Epoch Time 409.4191(402.3878), Bit/dim 3.6882(best: 3.6787), Xent 0.9448, Loss 4.1605, Error 0.3356(best: 0.3240)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1507 | Time 65.9030(62.3189) | Bit/dim 3.6781(3.6846) | Xent 0.9394(0.9238) | Loss 12.3170(9.7707) | Error 0.3297(0.3297) Steps 640(647.71) | Grad Norm 7.2398(5.3432) | Total Time 0.00(0.00)\n",
      "Iter 1508 | Time 60.2339(62.2563) | Bit/dim 3.6788(3.6844) | Xent 0.9157(0.9235) | Loss 9.1656(9.7525) | Error 0.3330(0.3298) Steps 640(647.48) | Grad Norm 5.0131(5.3333) | Total Time 0.00(0.00)\n",
      "Iter 1509 | Time 63.9366(62.3067) | Bit/dim 3.6841(3.6844) | Xent 0.9216(0.9235) | Loss 9.2673(9.7380) | Error 0.3306(0.3299) Steps 670(648.16) | Grad Norm 6.6629(5.3732) | Total Time 0.00(0.00)\n",
      "Iter 1510 | Time 62.7919(62.3213) | Bit/dim 3.6757(3.6841) | Xent 0.9084(0.9230) | Loss 9.2625(9.7237) | Error 0.3214(0.3296) Steps 670(648.81) | Grad Norm 5.6142(5.3804) | Total Time 0.00(0.00)\n",
      "Iter 1511 | Time 60.8460(62.2770) | Bit/dim 3.6796(3.6840) | Xent 0.9259(0.9231) | Loss 9.1712(9.7071) | Error 0.3355(0.3298) Steps 652(648.91) | Grad Norm 3.7516(5.3316) | Total Time 0.00(0.00)\n",
      "Iter 1512 | Time 66.7422(62.4110) | Bit/dim 3.6776(3.6838) | Xent 0.9347(0.9234) | Loss 9.2618(9.6938) | Error 0.3331(0.3299) Steps 658(649.18) | Grad Norm 8.8009(5.4356) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0252 | Time 23.9206, Epoch Time 421.9580(402.9749), Bit/dim 3.6823(best: 3.6787), Xent 0.9224, Loss 4.1435, Error 0.3230(best: 0.3240)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1513 | Time 62.7307(62.4206) | Bit/dim 3.6839(3.6838) | Xent 0.9181(0.9233) | Loss 12.1437(9.7673) | Error 0.3329(0.3300) Steps 646(649.08) | Grad Norm 6.0037(5.4527) | Total Time 0.00(0.00)\n",
      "Iter 1514 | Time 64.6507(62.4875) | Bit/dim 3.6889(3.6840) | Xent 0.9121(0.9229) | Loss 9.2773(9.7526) | Error 0.3323(0.3300) Steps 616(648.09) | Grad Norm 5.6684(5.4592) | Total Time 0.00(0.00)\n",
      "Iter 1515 | Time 68.2706(62.6610) | Bit/dim 3.6909(3.6842) | Xent 0.9352(0.9233) | Loss 9.3305(9.7399) | Error 0.3369(0.3302) Steps 676(648.93) | Grad Norm 10.4674(5.6094) | Total Time 0.00(0.00)\n",
      "Iter 1516 | Time 60.9120(62.6085) | Bit/dim 3.6719(3.6838) | Xent 0.9466(0.9240) | Loss 9.0925(9.7205) | Error 0.3416(0.3306) Steps 622(648.12) | Grad Norm 7.9664(5.6801) | Total Time 0.00(0.00)\n",
      "Iter 1517 | Time 57.5002(62.4552) | Bit/dim 3.6711(3.6834) | Xent 0.8809(0.9227) | Loss 9.1151(9.7023) | Error 0.3081(0.3299) Steps 646(648.06) | Grad Norm 4.7668(5.6527) | Total Time 0.00(0.00)\n",
      "Iter 1518 | Time 63.0283(62.4724) | Bit/dim 3.6725(3.6831) | Xent 0.9505(0.9236) | Loss 9.3010(9.6903) | Error 0.3391(0.3302) Steps 652(648.18) | Grad Norm 8.2790(5.7315) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0253 | Time 23.6673, Epoch Time 417.3369(403.4057), Bit/dim 3.6763(best: 3.6787), Xent 0.9342, Loss 4.1434, Error 0.3328(best: 0.3230)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1519 | Time 65.6713(62.5684) | Bit/dim 3.6769(3.6829) | Xent 0.9115(0.9232) | Loss 12.8456(9.7849) | Error 0.3267(0.3301) Steps 676(649.01) | Grad Norm 5.0457(5.7109) | Total Time 0.00(0.00)\n",
      "Iter 1520 | Time 65.5475(62.6578) | Bit/dim 3.6647(3.6824) | Xent 0.9105(0.9228) | Loss 9.2777(9.7697) | Error 0.3225(0.3299) Steps 634(648.56) | Grad Norm 5.6821(5.7101) | Total Time 0.00(0.00)\n",
      "Iter 1521 | Time 61.4128(62.6204) | Bit/dim 3.6734(3.6821) | Xent 0.9165(0.9226) | Loss 8.9056(9.7438) | Error 0.3260(0.3297) Steps 646(648.48) | Grad Norm 5.7766(5.7121) | Total Time 0.00(0.00)\n",
      "Iter 1522 | Time 58.0303(62.4827) | Bit/dim 3.6818(3.6821) | Xent 0.9116(0.9223) | Loss 9.0793(9.7239) | Error 0.3234(0.3296) Steps 610(647.33) | Grad Norm 3.3960(5.6426) | Total Time 0.00(0.00)\n",
      "Iter 1523 | Time 58.8528(62.3738) | Bit/dim 3.6828(3.6821) | Xent 0.9303(0.9225) | Loss 9.2216(9.7088) | Error 0.3346(0.3297) Steps 640(647.11) | Grad Norm 6.8401(5.6785) | Total Time 0.00(0.00)\n",
      "Iter 1524 | Time 58.6751(62.2629) | Bit/dim 3.6985(3.6826) | Xent 0.8983(0.9218) | Loss 9.1608(9.6924) | Error 0.3226(0.3295) Steps 634(646.72) | Grad Norm 4.6373(5.6473) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0254 | Time 23.2498, Epoch Time 407.7370(403.5357), Bit/dim 3.6884(best: 3.6763), Xent 0.9118, Loss 4.1443, Error 0.3243(best: 0.3230)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1525 | Time 63.9752(62.3142) | Bit/dim 3.6890(3.6828) | Xent 0.9143(0.9216) | Loss 12.5035(9.7767) | Error 0.3279(0.3294) Steps 646(646.69) | Grad Norm 3.7727(5.5910) | Total Time 0.00(0.00)\n",
      "Iter 1526 | Time 60.1669(62.2498) | Bit/dim 3.6784(3.6827) | Xent 0.9187(0.9215) | Loss 9.2401(9.7606) | Error 0.3336(0.3296) Steps 646(646.67) | Grad Norm 6.0748(5.6055) | Total Time 0.00(0.00)\n",
      "Iter 1527 | Time 59.0710(62.1544) | Bit/dim 3.6815(3.6826) | Xent 0.9032(0.9209) | Loss 9.1677(9.7428) | Error 0.3211(0.3293) Steps 652(646.83) | Grad Norm 3.6244(5.5461) | Total Time 0.00(0.00)\n",
      "Iter 1528 | Time 66.6771(62.2901) | Bit/dim 3.6798(3.6825) | Xent 0.9226(0.9210) | Loss 8.9504(9.7190) | Error 0.3305(0.3293) Steps 634(646.45) | Grad Norm 6.4872(5.5743) | Total Time 0.00(0.00)\n",
      "Iter 1529 | Time 66.3108(62.4107) | Bit/dim 3.6785(3.6824) | Xent 0.9027(0.9204) | Loss 9.3270(9.7073) | Error 0.3224(0.3291) Steps 670(647.16) | Grad Norm 5.9992(5.5871) | Total Time 0.00(0.00)\n",
      "Iter 1530 | Time 65.8832(62.5149) | Bit/dim 3.6801(3.6823) | Xent 0.9121(0.9202) | Loss 9.2748(9.6943) | Error 0.3330(0.3293) Steps 676(648.02) | Grad Norm 5.2033(5.5756) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0255 | Time 23.1320, Epoch Time 421.5788(404.0770), Bit/dim 3.6840(best: 3.6763), Xent 0.9209, Loss 4.1445, Error 0.3273(best: 0.3230)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1531 | Time 64.2046(62.5656) | Bit/dim 3.6797(3.6823) | Xent 0.9004(0.9196) | Loss 12.4830(9.7780) | Error 0.3164(0.3289) Steps 652(648.14) | Grad Norm 6.5449(5.6046) | Total Time 0.00(0.00)\n",
      "Iter 1532 | Time 56.6491(62.3881) | Bit/dim 3.6852(3.6824) | Xent 0.9045(0.9191) | Loss 9.1874(9.7602) | Error 0.3220(0.3287) Steps 628(647.54) | Grad Norm 2.9856(5.5261) | Total Time 0.00(0.00)\n",
      "Iter 1533 | Time 59.5107(62.3018) | Bit/dim 3.6871(3.6825) | Xent 0.9165(0.9191) | Loss 9.1854(9.7430) | Error 0.3264(0.3286) Steps 664(648.03) | Grad Norm 6.6741(5.5605) | Total Time 0.00(0.00)\n",
      "Iter 1534 | Time 62.6090(62.3110) | Bit/dim 3.6772(3.6823) | Xent 0.8948(0.9183) | Loss 9.1611(9.7255) | Error 0.3234(0.3284) Steps 628(647.43) | Grad Norm 4.5470(5.5301) | Total Time 0.00(0.00)\n",
      "Iter 1535 | Time 65.5364(62.4078) | Bit/dim 3.6851(3.6824) | Xent 0.9234(0.9185) | Loss 9.1752(9.7090) | Error 0.3286(0.3284) Steps 610(646.31) | Grad Norm 4.6924(5.5050) | Total Time 0.00(0.00)\n",
      "Iter 1536 | Time 60.3474(62.3460) | Bit/dim 3.6801(3.6823) | Xent 0.8990(0.9179) | Loss 9.2015(9.6938) | Error 0.3294(0.3285) Steps 652(646.48) | Grad Norm 4.2431(5.4671) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0256 | Time 23.0315, Epoch Time 407.7923(404.1884), Bit/dim 3.6784(best: 3.6763), Xent 0.9252, Loss 4.1410, Error 0.3291(best: 0.3230)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1537 | Time 61.1804(62.3110) | Bit/dim 3.6797(3.6823) | Xent 0.9173(0.9179) | Loss 12.4061(9.7752) | Error 0.3250(0.3284) Steps 652(646.64) | Grad Norm 5.3046(5.4622) | Total Time 0.00(0.00)\n",
      "Iter 1538 | Time 60.4851(62.2562) | Bit/dim 3.6795(3.6822) | Xent 0.8941(0.9172) | Loss 9.0268(9.7527) | Error 0.3189(0.3281) Steps 658(646.98) | Grad Norm 3.8840(5.4149) | Total Time 0.00(0.00)\n",
      "Iter 1539 | Time 64.7162(62.3300) | Bit/dim 3.6846(3.6823) | Xent 0.8836(0.9162) | Loss 9.2961(9.7390) | Error 0.3171(0.3278) Steps 628(646.41) | Grad Norm 3.4532(5.3561) | Total Time 0.00(0.00)\n",
      "Iter 1540 | Time 58.2801(62.2085) | Bit/dim 3.6912(3.6825) | Xent 0.9243(0.9164) | Loss 9.1153(9.7203) | Error 0.3311(0.3279) Steps 664(646.94) | Grad Norm 4.1608(5.3202) | Total Time 0.00(0.00)\n",
      "Iter 1541 | Time 63.3082(62.2415) | Bit/dim 3.6730(3.6822) | Xent 0.8998(0.9159) | Loss 9.1069(9.7019) | Error 0.3197(0.3276) Steps 622(646.19) | Grad Norm 2.7595(5.2434) | Total Time 0.00(0.00)\n",
      "Iter 1542 | Time 64.9747(62.3235) | Bit/dim 3.6796(3.6822) | Xent 0.8992(0.9154) | Loss 9.1503(9.6854) | Error 0.3239(0.3275) Steps 640(646.01) | Grad Norm 2.8180(5.1706) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0257 | Time 23.8365, Epoch Time 412.9612(404.4516), Bit/dim 3.6766(best: 3.6763), Xent 0.9049, Loss 4.1290, Error 0.3212(best: 0.3230)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1543 | Time 63.8481(62.3692) | Bit/dim 3.6889(3.6824) | Xent 0.8793(0.9143) | Loss 12.4298(9.7677) | Error 0.3193(0.3273) Steps 652(646.19) | Grad Norm 2.6012(5.0935) | Total Time 0.00(0.00)\n",
      "Iter 1544 | Time 63.0199(62.3888) | Bit/dim 3.6760(3.6822) | Xent 0.8927(0.9137) | Loss 9.1802(9.7501) | Error 0.3277(0.3273) Steps 658(646.54) | Grad Norm 4.7082(5.0820) | Total Time 0.00(0.00)\n",
      "Iter 1545 | Time 63.3374(62.4172) | Bit/dim 3.6805(3.6821) | Xent 0.8987(0.9132) | Loss 9.3255(9.7373) | Error 0.3263(0.3272) Steps 640(646.35) | Grad Norm 4.4766(5.0638) | Total Time 0.00(0.00)\n",
      "Iter 1546 | Time 61.2648(62.3826) | Bit/dim 3.6833(3.6822) | Xent 0.9028(0.9129) | Loss 9.2888(9.7239) | Error 0.3193(0.3270) Steps 652(646.51) | Grad Norm 4.0855(5.0345) | Total Time 0.00(0.00)\n",
      "Iter 1547 | Time 63.2035(62.4073) | Bit/dim 3.6779(3.6820) | Xent 0.9144(0.9130) | Loss 9.2700(9.7103) | Error 0.3290(0.3271) Steps 628(645.96) | Grad Norm 6.9683(5.0925) | Total Time 0.00(0.00)\n",
      "Iter 1548 | Time 63.1041(62.4282) | Bit/dim 3.6750(3.6818) | Xent 0.9082(0.9128) | Loss 9.3239(9.6987) | Error 0.3223(0.3269) Steps 682(647.04) | Grad Norm 8.6658(5.1997) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0258 | Time 23.4059, Epoch Time 417.6399(404.8473), Bit/dim 3.6785(best: 3.6763), Xent 0.9065, Loss 4.1317, Error 0.3216(best: 0.3212)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1549 | Time 59.9672(62.3543) | Bit/dim 3.6843(3.6819) | Xent 0.8965(0.9123) | Loss 12.3110(9.7770) | Error 0.3213(0.3267) Steps 658(647.37) | Grad Norm 3.5136(5.1491) | Total Time 0.00(0.00)\n",
      "Iter 1550 | Time 60.6547(62.3034) | Bit/dim 3.6702(3.6815) | Xent 0.9164(0.9124) | Loss 8.9872(9.7533) | Error 0.3314(0.3269) Steps 670(648.05) | Grad Norm 4.4402(5.1278) | Total Time 0.00(0.00)\n",
      "Iter 1551 | Time 62.8711(62.3204) | Bit/dim 3.6771(3.6814) | Xent 0.8737(0.9113) | Loss 9.1326(9.7347) | Error 0.3117(0.3264) Steps 646(647.99) | Grad Norm 4.6320(5.1130) | Total Time 0.00(0.00)\n",
      "Iter 1552 | Time 65.2498(62.4083) | Bit/dim 3.6785(3.6813) | Xent 0.8974(0.9109) | Loss 9.2276(9.7195) | Error 0.3221(0.3263) Steps 640(647.75) | Grad Norm 5.7568(5.1323) | Total Time 0.00(0.00)\n",
      "Iter 1553 | Time 66.0379(62.5172) | Bit/dim 3.6857(3.6815) | Xent 0.9333(0.9115) | Loss 9.2688(9.7060) | Error 0.3321(0.3265) Steps 652(647.87) | Grad Norm 8.8411(5.2435) | Total Time 0.00(0.00)\n",
      "Iter 1554 | Time 59.0986(62.4146) | Bit/dim 3.6770(3.6813) | Xent 0.9203(0.9118) | Loss 9.1859(9.6904) | Error 0.3255(0.3264) Steps 646(647.82) | Grad Norm 7.3842(5.3078) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0259 | Time 23.1610, Epoch Time 413.2611(405.0997), Bit/dim 3.6796(best: 3.6763), Xent 0.9105, Loss 4.1349, Error 0.3234(best: 0.3212)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1555 | Time 61.6262(62.3910) | Bit/dim 3.6792(3.6813) | Xent 0.8854(0.9110) | Loss 12.4508(9.7732) | Error 0.3167(0.3262) Steps 670(648.48) | Grad Norm 2.9058(5.2357) | Total Time 0.00(0.00)\n",
      "Iter 1556 | Time 61.0057(62.3494) | Bit/dim 3.6771(3.6811) | Xent 0.9159(0.9112) | Loss 9.2510(9.7575) | Error 0.3220(0.3260) Steps 622(647.69) | Grad Norm 9.4210(5.3613) | Total Time 0.00(0.00)\n",
      "Iter 1557 | Time 61.3335(62.3189) | Bit/dim 3.6790(3.6811) | Xent 0.9331(0.9118) | Loss 9.4149(9.7473) | Error 0.3304(0.3262) Steps 640(647.46) | Grad Norm 8.2218(5.4471) | Total Time 0.00(0.00)\n",
      "Iter 1558 | Time 63.6244(62.3581) | Bit/dim 3.6920(3.6814) | Xent 0.8914(0.9112) | Loss 8.9337(9.7228) | Error 0.3184(0.3259) Steps 682(648.50) | Grad Norm 3.9753(5.4029) | Total Time 0.00(0.00)\n",
      "Iter 1559 | Time 63.9457(62.4057) | Bit/dim 3.6731(3.6811) | Xent 0.9045(0.9110) | Loss 9.1476(9.7056) | Error 0.3226(0.3258) Steps 658(648.78) | Grad Norm 9.5150(5.5263) | Total Time 0.00(0.00)\n",
      "Iter 1560 | Time 60.1446(62.3379) | Bit/dim 3.6635(3.6806) | Xent 0.8943(0.9105) | Loss 9.2160(9.6909) | Error 0.3201(0.3257) Steps 640(648.52) | Grad Norm 5.1849(5.5160) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0260 | Time 23.5088, Epoch Time 411.5418(405.2929), Bit/dim 3.6848(best: 3.6763), Xent 0.9211, Loss 4.1453, Error 0.3257(best: 0.3212)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1561 | Time 57.7364(62.1998) | Bit/dim 3.6769(3.6805) | Xent 0.9056(0.9104) | Loss 12.4419(9.7734) | Error 0.3241(0.3256) Steps 628(647.90) | Grad Norm 6.3269(5.5404) | Total Time 0.00(0.00)\n",
      "Iter 1562 | Time 65.0018(62.2839) | Bit/dim 3.6752(3.6803) | Xent 0.9179(0.9106) | Loss 9.2334(9.7572) | Error 0.3355(0.3259) Steps 634(647.48) | Grad Norm 8.2370(5.6213) | Total Time 0.00(0.00)\n",
      "Iter 1563 | Time 66.3769(62.4067) | Bit/dim 3.6739(3.6802) | Xent 0.8921(0.9100) | Loss 9.3009(9.7435) | Error 0.3197(0.3257) Steps 652(647.62) | Grad Norm 7.9212(5.6903) | Total Time 0.00(0.00)\n",
      "Iter 1564 | Time 58.5524(62.2911) | Bit/dim 3.6914(3.6805) | Xent 0.9061(0.9099) | Loss 9.2087(9.7275) | Error 0.3193(0.3255) Steps 634(647.21) | Grad Norm 4.7027(5.6606) | Total Time 0.00(0.00)\n",
      "Iter 1565 | Time 61.6158(62.2708) | Bit/dim 3.6765(3.6804) | Xent 0.9180(0.9102) | Loss 9.2957(9.7145) | Error 0.3273(0.3256) Steps 640(646.99) | Grad Norm 6.6988(5.6918) | Total Time 0.00(0.00)\n",
      "Iter 1566 | Time 61.0333(62.2337) | Bit/dim 3.6765(3.6803) | Xent 0.8733(0.9090) | Loss 9.1954(9.6990) | Error 0.3111(0.3251) Steps 670(647.68) | Grad Norm 6.1322(5.7050) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0261 | Time 22.9860, Epoch Time 409.5928(405.4219), Bit/dim 3.6825(best: 3.6763), Xent 0.9062, Loss 4.1356, Error 0.3200(best: 0.3212)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1567 | Time 61.4583(62.2104) | Bit/dim 3.6718(3.6800) | Xent 0.8656(0.9077) | Loss 12.0067(9.7682) | Error 0.3101(0.3247) Steps 658(647.99) | Grad Norm 3.7667(5.6468) | Total Time 0.00(0.00)\n",
      "Iter 1568 | Time 62.7484(62.2265) | Bit/dim 3.6838(3.6801) | Xent 0.8868(0.9071) | Loss 9.1694(9.7502) | Error 0.3190(0.3245) Steps 658(648.29) | Grad Norm 5.6163(5.6459) | Total Time 0.00(0.00)\n",
      "Iter 1569 | Time 64.2019(62.2858) | Bit/dim 3.6841(3.6802) | Xent 0.8820(0.9064) | Loss 9.3212(9.7374) | Error 0.3093(0.3241) Steps 676(649.13) | Grad Norm 2.5199(5.5521) | Total Time 0.00(0.00)\n",
      "Iter 1570 | Time 62.7908(62.3010) | Bit/dim 3.6747(3.6801) | Xent 0.8922(0.9059) | Loss 9.1830(9.7207) | Error 0.3237(0.3241) Steps 634(648.67) | Grad Norm 5.9523(5.5642) | Total Time 0.00(0.00)\n",
      "Iter 1571 | Time 64.3659(62.3629) | Bit/dim 3.6707(3.6798) | Xent 0.8953(0.9056) | Loss 9.0984(9.7021) | Error 0.3215(0.3240) Steps 646(648.59) | Grad Norm 3.5038(5.5023) | Total Time 0.00(0.00)\n",
      "Iter 1572 | Time 63.7108(62.4033) | Bit/dim 3.6797(3.6798) | Xent 0.8987(0.9054) | Loss 9.3257(9.6908) | Error 0.3247(0.3240) Steps 640(648.33) | Grad Norm 3.2097(5.4336) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0262 | Time 23.3032, Epoch Time 423.7379(405.9714), Bit/dim 3.6778(best: 3.6763), Xent 0.9005, Loss 4.1281, Error 0.3197(best: 0.3200)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1573 | Time 61.2663(62.3692) | Bit/dim 3.6784(3.6797) | Xent 0.8967(0.9052) | Loss 12.4262(9.7728) | Error 0.3180(0.3238) Steps 658(648.62) | Grad Norm 4.0196(5.3911) | Total Time 0.00(0.00)\n",
      "Iter 1574 | Time 63.2322(62.3951) | Bit/dim 3.6751(3.6796) | Xent 0.8986(0.9050) | Loss 9.3556(9.7603) | Error 0.3266(0.3239) Steps 646(648.55) | Grad Norm 3.9636(5.3483) | Total Time 0.00(0.00)\n",
      "Iter 1575 | Time 63.4956(62.4281) | Bit/dim 3.6752(3.6795) | Xent 0.8941(0.9046) | Loss 9.2151(9.7440) | Error 0.3216(0.3238) Steps 634(648.11) | Grad Norm 4.9092(5.3351) | Total Time 0.00(0.00)\n",
      "Iter 1576 | Time 62.4493(62.4288) | Bit/dim 3.6774(3.6794) | Xent 0.9013(0.9045) | Loss 9.4191(9.7342) | Error 0.3277(0.3240) Steps 652(648.23) | Grad Norm 5.5735(5.3423) | Total Time 0.00(0.00)\n",
      "Iter 1577 | Time 59.5642(62.3428) | Bit/dim 3.6776(3.6794) | Xent 0.8771(0.9037) | Loss 9.0907(9.7149) | Error 0.3139(0.3237) Steps 658(648.52) | Grad Norm 6.8717(5.3882) | Total Time 0.00(0.00)\n",
      "Iter 1578 | Time 56.6075(62.1708) | Bit/dim 3.6785(3.6793) | Xent 0.8968(0.9035) | Loss 9.1074(9.6967) | Error 0.3227(0.3236) Steps 664(648.98) | Grad Norm 3.5397(5.3327) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0263 | Time 23.2524, Epoch Time 405.5339(405.9583), Bit/dim 3.6782(best: 3.6763), Xent 0.9067, Loss 4.1316, Error 0.3194(best: 0.3197)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1579 | Time 64.1951(62.2315) | Bit/dim 3.6785(3.6793) | Xent 0.8929(0.9032) | Loss 12.0784(9.7681) | Error 0.3224(0.3236) Steps 664(649.43) | Grad Norm 4.1570(5.2974) | Total Time 0.00(0.00)\n",
      "Iter 1580 | Time 64.7951(62.3084) | Bit/dim 3.6841(3.6795) | Xent 0.8870(0.9027) | Loss 9.1971(9.7510) | Error 0.3167(0.3234) Steps 616(648.43) | Grad Norm 3.4718(5.2427) | Total Time 0.00(0.00)\n",
      "Iter 1581 | Time 60.6565(62.2589) | Bit/dim 3.6805(3.6795) | Xent 0.8939(0.9024) | Loss 9.1690(9.7335) | Error 0.3221(0.3233) Steps 670(649.08) | Grad Norm 3.9603(5.2042) | Total Time 0.00(0.00)\n",
      "Iter 1582 | Time 66.0871(62.3737) | Bit/dim 3.6779(3.6794) | Xent 0.8958(0.9022) | Loss 9.1199(9.7151) | Error 0.3233(0.3233) Steps 664(649.53) | Grad Norm 4.9752(5.1973) | Total Time 0.00(0.00)\n",
      "Iter 1583 | Time 59.8547(62.2981) | Bit/dim 3.6660(3.6790) | Xent 0.8890(0.9018) | Loss 9.1404(9.6979) | Error 0.3105(0.3230) Steps 664(649.96) | Grad Norm 6.8190(5.2460) | Total Time 0.00(0.00)\n",
      "Iter 1584 | Time 58.5773(62.1865) | Bit/dim 3.6776(3.6790) | Xent 0.8989(0.9018) | Loss 9.0815(9.6794) | Error 0.3216(0.3229) Steps 640(649.66) | Grad Norm 7.1210(5.3022) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0264 | Time 23.4080, Epoch Time 413.9271(406.1973), Bit/dim 3.6788(best: 3.6763), Xent 0.9043, Loss 4.1310, Error 0.3207(best: 0.3194)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1585 | Time 68.3920(62.3727) | Bit/dim 3.6857(3.6792) | Xent 0.8929(0.9015) | Loss 12.5436(9.7653) | Error 0.3231(0.3229) Steps 694(650.99) | Grad Norm 5.0210(5.2938) | Total Time 0.00(0.00)\n",
      "Iter 1586 | Time 62.2631(62.3694) | Bit/dim 3.6769(3.6791) | Xent 0.9060(0.9016) | Loss 9.0528(9.7440) | Error 0.3194(0.3228) Steps 658(651.20) | Grad Norm 5.6700(5.3051) | Total Time 0.00(0.00)\n",
      "Iter 1587 | Time 61.8741(62.3545) | Bit/dim 3.6789(3.6791) | Xent 0.8837(0.9011) | Loss 9.1342(9.7257) | Error 0.3146(0.3226) Steps 670(651.77) | Grad Norm 2.5764(5.2232) | Total Time 0.00(0.00)\n",
      "Iter 1588 | Time 61.1802(62.3193) | Bit/dim 3.6793(3.6791) | Xent 0.9004(0.9011) | Loss 9.1724(9.7091) | Error 0.3243(0.3226) Steps 640(651.41) | Grad Norm 7.4559(5.2902) | Total Time 0.00(0.00)\n",
      "Iter 1589 | Time 63.3756(62.3510) | Bit/dim 3.6731(3.6789) | Xent 0.9088(0.9013) | Loss 9.1981(9.6937) | Error 0.3237(0.3227) Steps 646(651.25) | Grad Norm 7.8989(5.3685) | Total Time 0.00(0.00)\n",
      "Iter 1590 | Time 65.5048(62.4456) | Bit/dim 3.6679(3.6786) | Xent 0.9329(0.9022) | Loss 9.2313(9.6799) | Error 0.3336(0.3230) Steps 622(650.37) | Grad Norm 8.9070(5.4746) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0265 | Time 23.3519, Epoch Time 430.5712(406.9286), Bit/dim 3.6859(best: 3.6763), Xent 0.9425, Loss 4.1571, Error 0.3319(best: 0.3194)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1591 | Time 62.6408(62.4515) | Bit/dim 3.6909(3.6790) | Xent 0.9509(0.9037) | Loss 12.5757(9.7667) | Error 0.3374(0.3234) Steps 646(650.24) | Grad Norm 11.7648(5.6633) | Total Time 0.00(0.00)\n",
      "Iter 1592 | Time 60.9651(62.4069) | Bit/dim 3.6755(3.6789) | Xent 0.9054(0.9038) | Loss 9.1346(9.7478) | Error 0.3221(0.3234) Steps 622(649.39) | Grad Norm 8.6243(5.7522) | Total Time 0.00(0.00)\n",
      "Iter 1593 | Time 65.7367(62.5068) | Bit/dim 3.6839(3.6790) | Xent 0.8779(0.9030) | Loss 9.0709(9.7275) | Error 0.3119(0.3230) Steps 622(648.57) | Grad Norm 3.5985(5.6876) | Total Time 0.00(0.00)\n",
      "Iter 1594 | Time 59.2357(62.4086) | Bit/dim 3.6733(3.6789) | Xent 0.9136(0.9033) | Loss 9.2061(9.7118) | Error 0.3273(0.3232) Steps 640(648.32) | Grad Norm 5.4575(5.6806) | Total Time 0.00(0.00)\n",
      "Iter 1595 | Time 61.3173(62.3759) | Bit/dim 3.6673(3.6785) | Xent 0.9012(0.9032) | Loss 9.2002(9.6965) | Error 0.3281(0.3233) Steps 640(648.07) | Grad Norm 4.3113(5.6396) | Total Time 0.00(0.00)\n",
      "Iter 1596 | Time 60.8147(62.3291) | Bit/dim 3.6755(3.6784) | Xent 0.8977(0.9031) | Loss 9.1027(9.6787) | Error 0.3136(0.3230) Steps 646(648.00) | Grad Norm 5.3977(5.6323) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0266 | Time 23.3382, Epoch Time 411.3301(407.0606), Bit/dim 3.6764(best: 3.6763), Xent 0.9181, Loss 4.1355, Error 0.3264(best: 0.3194)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1597 | Time 58.8250(62.2239) | Bit/dim 3.6817(3.6785) | Xent 0.9138(0.9034) | Loss 11.5064(9.7335) | Error 0.3350(0.3234) Steps 628(647.40) | Grad Norm 7.2348(5.6804) | Total Time 0.00(0.00)\n",
      "Iter 1598 | Time 63.4750(62.2615) | Bit/dim 3.6883(3.6788) | Xent 0.8689(0.9024) | Loss 9.1053(9.7146) | Error 0.3044(0.3228) Steps 664(647.90) | Grad Norm 4.2981(5.6389) | Total Time 0.00(0.00)\n",
      "Iter 1599 | Time 64.9583(62.3424) | Bit/dim 3.6755(3.6787) | Xent 0.9050(0.9024) | Loss 9.2960(9.7021) | Error 0.3265(0.3229) Steps 670(648.56) | Grad Norm 4.6014(5.6078) | Total Time 0.00(0.00)\n",
      "Iter 1600 | Time 63.9771(62.3914) | Bit/dim 3.6822(3.6788) | Xent 0.8606(0.9012) | Loss 9.3159(9.6905) | Error 0.3061(0.3224) Steps 652(648.67) | Grad Norm 6.6267(5.6384) | Total Time 0.00(0.00)\n",
      "Iter 1601 | Time 63.9454(62.4380) | Bit/dim 3.6721(3.6786) | Xent 0.8826(0.9006) | Loss 9.0968(9.6727) | Error 0.3191(0.3223) Steps 670(649.31) | Grad Norm 4.5814(5.6066) | Total Time 0.00(0.00)\n",
      "Iter 1602 | Time 61.0781(62.3972) | Bit/dim 3.6702(3.6784) | Xent 0.8828(0.9001) | Loss 9.2478(9.6599) | Error 0.3153(0.3221) Steps 664(649.75) | Grad Norm 3.7972(5.5524) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0267 | Time 22.9592, Epoch Time 418.1904(407.3945), Bit/dim 3.6758(best: 3.6763), Xent 0.8955, Loss 4.1236, Error 0.3191(best: 0.3194)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1603 | Time 57.8662(62.2613) | Bit/dim 3.6724(3.6782) | Xent 0.8747(0.8993) | Loss 12.2195(9.7367) | Error 0.3155(0.3219) Steps 646(649.64) | Grad Norm 2.4914(5.4605) | Total Time 0.00(0.00)\n",
      "Iter 1604 | Time 60.3076(62.2027) | Bit/dim 3.6680(3.6779) | Xent 0.8856(0.8989) | Loss 9.2435(9.7219) | Error 0.3139(0.3217) Steps 628(648.99) | Grad Norm 3.9811(5.4162) | Total Time 0.00(0.00)\n",
      "Iter 1605 | Time 62.2895(62.2053) | Bit/dim 3.6656(3.6775) | Xent 0.8789(0.8983) | Loss 9.2603(9.7081) | Error 0.3124(0.3214) Steps 628(648.36) | Grad Norm 1.6878(5.3043) | Total Time 0.00(0.00)\n",
      "Iter 1606 | Time 59.8206(62.1338) | Bit/dim 3.6809(3.6776) | Xent 0.9114(0.8987) | Loss 9.3456(9.6972) | Error 0.3261(0.3215) Steps 664(648.83) | Grad Norm 3.5653(5.2521) | Total Time 0.00(0.00)\n",
      "Iter 1607 | Time 61.2137(62.1062) | Bit/dim 3.6799(3.6777) | Xent 0.8803(0.8982) | Loss 9.0435(9.6776) | Error 0.3163(0.3214) Steps 658(649.10) | Grad Norm 4.7995(5.2386) | Total Time 0.00(0.00)\n",
      "Iter 1608 | Time 62.1185(62.1065) | Bit/dim 3.6729(3.6775) | Xent 0.8914(0.8980) | Loss 9.1983(9.6632) | Error 0.3199(0.3213) Steps 622(648.29) | Grad Norm 7.1804(5.2968) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0268 | Time 22.9922, Epoch Time 407.2425(407.3899), Bit/dim 3.6783(best: 3.6758), Xent 0.8974, Loss 4.1270, Error 0.3177(best: 0.3191)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1609 | Time 62.7006(62.1243) | Bit/dim 3.6868(3.6778) | Xent 0.8731(0.8972) | Loss 12.4824(9.7478) | Error 0.3109(0.3210) Steps 640(648.04) | Grad Norm 4.0726(5.2601) | Total Time 0.00(0.00)\n",
      "Iter 1610 | Time 64.6248(62.1994) | Bit/dim 3.6647(3.6774) | Xent 0.8629(0.8962) | Loss 9.1236(9.7291) | Error 0.3054(0.3205) Steps 646(647.98) | Grad Norm 3.3718(5.2034) | Total Time 0.00(0.00)\n",
      "Iter 1611 | Time 60.7324(62.1553) | Bit/dim 3.6658(3.6771) | Xent 0.8754(0.8956) | Loss 9.0448(9.7085) | Error 0.3101(0.3202) Steps 634(647.56) | Grad Norm 2.4781(5.1217) | Total Time 0.00(0.00)\n",
      "Iter 1612 | Time 56.8342(61.9957) | Bit/dim 3.6874(3.6774) | Xent 0.8978(0.8956) | Loss 9.2229(9.6940) | Error 0.3154(0.3201) Steps 634(647.15) | Grad Norm 3.6173(5.0765) | Total Time 0.00(0.00)\n",
      "Iter 1613 | Time 65.9123(62.1132) | Bit/dim 3.6644(3.6770) | Xent 0.8771(0.8951) | Loss 9.1326(9.6771) | Error 0.3123(0.3198) Steps 640(646.94) | Grad Norm 1.6190(4.9728) | Total Time 0.00(0.00)\n",
      "Iter 1614 | Time 67.8067(62.2840) | Bit/dim 3.6846(3.6772) | Xent 0.8709(0.8943) | Loss 9.2122(9.6632) | Error 0.3110(0.3196) Steps 646(646.91) | Grad Norm 3.4183(4.9262) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0269 | Time 23.0926, Epoch Time 417.7609(407.7011), Bit/dim 3.6721(best: 3.6758), Xent 0.8950, Loss 4.1196, Error 0.3154(best: 0.3177)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1615 | Time 61.3157(62.2550) | Bit/dim 3.6743(3.6771) | Xent 0.8910(0.8942) | Loss 12.1402(9.7375) | Error 0.3173(0.3195) Steps 634(646.52) | Grad Norm 2.1781(4.8437) | Total Time 0.00(0.00)\n",
      "Iter 1616 | Time 59.3052(62.1665) | Bit/dim 3.6812(3.6773) | Xent 0.8573(0.8931) | Loss 8.9157(9.7128) | Error 0.3050(0.3191) Steps 634(646.15) | Grad Norm 3.3450(4.7988) | Total Time 0.00(0.00)\n",
      "Iter 1617 | Time 59.8868(62.0981) | Bit/dim 3.6778(3.6773) | Xent 0.8708(0.8925) | Loss 9.2028(9.6975) | Error 0.3104(0.3188) Steps 646(646.14) | Grad Norm 3.3999(4.7568) | Total Time 0.00(0.00)\n",
      "Iter 1618 | Time 62.3419(62.1054) | Bit/dim 3.6703(3.6771) | Xent 0.8731(0.8919) | Loss 9.1826(9.6821) | Error 0.3059(0.3184) Steps 652(646.32) | Grad Norm 2.6270(4.6929) | Total Time 0.00(0.00)\n",
      "Iter 1619 | Time 66.2751(62.2305) | Bit/dim 3.6669(3.6768) | Xent 0.8827(0.8916) | Loss 9.2494(9.6691) | Error 0.3157(0.3183) Steps 670(647.03) | Grad Norm 2.9761(4.6414) | Total Time 0.00(0.00)\n",
      "Iter 1620 | Time 65.0652(62.3155) | Bit/dim 3.6668(3.6765) | Xent 0.8878(0.8915) | Loss 8.7978(9.6430) | Error 0.3171(0.3183) Steps 616(646.10) | Grad Norm 3.7136(4.6136) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0270 | Time 23.1498, Epoch Time 413.6888(407.8807), Bit/dim 3.6792(best: 3.6721), Xent 0.8965, Loss 4.1274, Error 0.3224(best: 0.3154)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1621 | Time 62.7387(62.3282) | Bit/dim 3.6667(3.6762) | Xent 0.8855(0.8913) | Loss 12.4671(9.7277) | Error 0.3190(0.3183) Steps 664(646.63) | Grad Norm 2.6354(4.5542) | Total Time 0.00(0.00)\n",
      "Iter 1622 | Time 58.4616(62.2122) | Bit/dim 3.6760(3.6762) | Xent 0.8892(0.8912) | Loss 9.0832(9.7084) | Error 0.3177(0.3183) Steps 658(646.98) | Grad Norm 6.5768(4.6149) | Total Time 0.00(0.00)\n",
      "Iter 1623 | Time 62.1556(62.2105) | Bit/dim 3.6605(3.6757) | Xent 0.9282(0.8924) | Loss 9.3267(9.6969) | Error 0.3273(0.3186) Steps 628(646.41) | Grad Norm 8.4388(4.7296) | Total Time 0.00(0.00)\n",
      "Iter 1624 | Time 62.8031(62.2283) | Bit/dim 3.6864(3.6760) | Xent 0.9390(0.8938) | Loss 9.2181(9.6825) | Error 0.3320(0.3190) Steps 640(646.21) | Grad Norm 11.3287(4.9276) | Total Time 0.00(0.00)\n",
      "Iter 1625 | Time 61.3014(62.2005) | Bit/dim 3.6834(3.6762) | Xent 0.9671(0.8960) | Loss 9.1525(9.6666) | Error 0.3500(0.3199) Steps 634(645.85) | Grad Norm 11.2289(5.1166) | Total Time 0.00(0.00)\n",
      "Iter 1626 | Time 57.5232(62.0602) | Bit/dim 3.6869(3.6766) | Xent 0.8907(0.8958) | Loss 9.1001(9.6496) | Error 0.3166(0.3198) Steps 646(645.85) | Grad Norm 6.0384(5.1443) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0271 | Time 22.9794, Epoch Time 403.9427(407.7626), Bit/dim 3.6820(best: 3.6721), Xent 0.9218, Loss 4.1429, Error 0.3250(best: 0.3154)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1627 | Time 64.8190(62.1429) | Bit/dim 3.6668(3.6763) | Xent 0.8890(0.8956) | Loss 12.5312(9.7361) | Error 0.3216(0.3199) Steps 658(646.22) | Grad Norm 6.1689(5.1750) | Total Time 0.00(0.00)\n",
      "Iter 1628 | Time 64.0367(62.1998) | Bit/dim 3.6852(3.6765) | Xent 0.9378(0.8969) | Loss 9.2117(9.7204) | Error 0.3340(0.3203) Steps 640(646.03) | Grad Norm 7.8117(5.2541) | Total Time 0.00(0.00)\n",
      "Iter 1629 | Time 62.4652(62.2077) | Bit/dim 3.6780(3.6766) | Xent 0.9253(0.8977) | Loss 9.1528(9.7033) | Error 0.3305(0.3206) Steps 640(645.85) | Grad Norm 6.8412(5.3017) | Total Time 0.00(0.00)\n",
      "Iter 1630 | Time 62.6843(62.2220) | Bit/dim 3.6727(3.6765) | Xent 0.9203(0.8984) | Loss 9.3225(9.6919) | Error 0.3313(0.3209) Steps 658(646.21) | Grad Norm 8.8631(5.4086) | Total Time 0.00(0.00)\n",
      "Iter 1631 | Time 64.3168(62.2849) | Bit/dim 3.6733(3.6764) | Xent 0.9222(0.8991) | Loss 9.0770(9.6735) | Error 0.3274(0.3211) Steps 652(646.39) | Grad Norm 6.6520(5.4459) | Total Time 0.00(0.00)\n",
      "Iter 1632 | Time 65.4427(62.3796) | Bit/dim 3.6673(3.6761) | Xent 0.9166(0.8996) | Loss 9.2645(9.6612) | Error 0.3314(0.3214) Steps 652(646.56) | Grad Norm 9.4781(5.5669) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0272 | Time 23.4283, Epoch Time 422.9729(408.2189), Bit/dim 3.6801(best: 3.6721), Xent 0.9646, Loss 4.1624, Error 0.3422(best: 0.3154)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1633 | Time 61.0311(62.3391) | Bit/dim 3.6764(3.6761) | Xent 0.9544(0.9013) | Loss 12.7281(9.7532) | Error 0.3361(0.3219) Steps 646(646.54) | Grad Norm 11.4581(5.7436) | Total Time 0.00(0.00)\n",
      "Iter 1634 | Time 61.9830(62.3285) | Bit/dim 3.6972(3.6767) | Xent 0.9078(0.9015) | Loss 9.1860(9.7362) | Error 0.3273(0.3220) Steps 646(646.52) | Grad Norm 8.3881(5.8229) | Total Time 0.00(0.00)\n",
      "Iter 1635 | Time 67.1428(62.4729) | Bit/dim 3.6666(3.6764) | Xent 0.9062(0.9016) | Loss 9.2283(9.7209) | Error 0.3201(0.3220) Steps 658(646.87) | Grad Norm 10.8393(5.9734) | Total Time 0.00(0.00)\n",
      "Iter 1636 | Time 63.7453(62.5111) | Bit/dim 3.6871(3.6768) | Xent 0.8993(0.9015) | Loss 9.0990(9.7023) | Error 0.3156(0.3218) Steps 640(646.66) | Grad Norm 9.6858(6.0848) | Total Time 0.00(0.00)\n",
      "Iter 1637 | Time 60.6643(62.4557) | Bit/dim 3.6755(3.6767) | Xent 0.9234(0.9022) | Loss 9.1232(9.6849) | Error 0.3263(0.3219) Steps 610(645.56) | Grad Norm 7.1744(6.1175) | Total Time 0.00(0.00)\n",
      "Iter 1638 | Time 60.9487(62.4105) | Bit/dim 3.6729(3.6766) | Xent 0.8920(0.9019) | Loss 9.2001(9.6704) | Error 0.3205(0.3219) Steps 652(645.75) | Grad Norm 6.6930(6.1347) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0273 | Time 24.3549, Epoch Time 416.0542(408.4539), Bit/dim 3.6793(best: 3.6721), Xent 0.9241, Loss 4.1413, Error 0.3277(best: 0.3154)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1639 | Time 62.9853(62.4277) | Bit/dim 3.6723(3.6765) | Xent 0.9006(0.9019) | Loss 12.1122(9.7436) | Error 0.3265(0.3220) Steps 676(646.66) | Grad Norm 8.6229(6.2094) | Total Time 0.00(0.00)\n",
      "Iter 1640 | Time 60.9513(62.3834) | Bit/dim 3.6911(3.6769) | Xent 0.9050(0.9019) | Loss 9.1634(9.7262) | Error 0.3269(0.3222) Steps 688(647.90) | Grad Norm 4.3598(6.1539) | Total Time 0.00(0.00)\n",
      "Iter 1641 | Time 58.8809(62.2783) | Bit/dim 3.6885(3.6773) | Xent 0.9177(0.9024) | Loss 9.3679(9.7155) | Error 0.3271(0.3223) Steps 646(647.85) | Grad Norm 10.6956(6.2902) | Total Time 0.00(0.00)\n",
      "Iter 1642 | Time 56.2360(62.0971) | Bit/dim 3.6633(3.6768) | Xent 0.8869(0.9020) | Loss 9.0226(9.6947) | Error 0.3206(0.3223) Steps 634(647.43) | Grad Norm 5.6815(6.2719) | Total Time 0.00(0.00)\n",
      "Iter 1643 | Time 60.0919(62.0369) | Bit/dim 3.6799(3.6769) | Xent 0.8951(0.9017) | Loss 8.9524(9.6724) | Error 0.3209(0.3222) Steps 658(647.75) | Grad Norm 7.8229(6.3184) | Total Time 0.00(0.00)\n",
      "Iter 1644 | Time 61.1895(62.0115) | Bit/dim 3.6702(3.6767) | Xent 0.8796(0.9011) | Loss 9.2836(9.6608) | Error 0.3150(0.3220) Steps 616(646.79) | Grad Norm 4.4655(6.2628) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0274 | Time 23.9597, Epoch Time 400.4945(408.2152), Bit/dim 3.6774(best: 3.6721), Xent 0.9060, Loss 4.1304, Error 0.3202(best: 0.3154)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1645 | Time 62.2178(62.0177) | Bit/dim 3.6763(3.6767) | Xent 0.8964(0.9009) | Loss 12.9043(9.7581) | Error 0.3166(0.3218) Steps 610(645.69) | Grad Norm 7.2943(6.2938) | Total Time 0.00(0.00)\n",
      "Iter 1646 | Time 59.6109(61.9455) | Bit/dim 3.6853(3.6770) | Xent 0.8932(0.9007) | Loss 9.0020(9.7354) | Error 0.3216(0.3218) Steps 646(645.70) | Grad Norm 5.3701(6.2661) | Total Time 0.00(0.00)\n",
      "Iter 1647 | Time 63.0984(61.9801) | Bit/dim 3.6791(3.6770) | Xent 0.8692(0.8998) | Loss 9.1259(9.7171) | Error 0.3136(0.3216) Steps 658(646.07) | Grad Norm 5.1278(6.2319) | Total Time 0.00(0.00)\n",
      "Iter 1648 | Time 61.2048(61.9568) | Bit/dim 3.6702(3.6768) | Xent 0.8884(0.8994) | Loss 9.1885(9.7012) | Error 0.3145(0.3214) Steps 652(646.25) | Grad Norm 6.1654(6.2299) | Total Time 0.00(0.00)\n",
      "Iter 1649 | Time 59.7999(61.8921) | Bit/dim 3.6727(3.6767) | Xent 0.8857(0.8990) | Loss 9.2356(9.6873) | Error 0.3165(0.3212) Steps 652(646.42) | Grad Norm 6.0421(6.2243) | Total Time 0.00(0.00)\n",
      "Iter 1650 | Time 62.3149(61.9048) | Bit/dim 3.6729(3.6766) | Xent 0.9011(0.8991) | Loss 9.2551(9.6743) | Error 0.3185(0.3211) Steps 652(646.59) | Grad Norm 5.4108(6.1999) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0275 | Time 23.7385, Epoch Time 408.2824(408.2172), Bit/dim 3.6745(best: 3.6721), Xent 0.8915, Loss 4.1202, Error 0.3149(best: 0.3154)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1651 | Time 62.8633(61.9335) | Bit/dim 3.6670(3.6763) | Xent 0.8818(0.8986) | Loss 12.7480(9.7665) | Error 0.3190(0.3211) Steps 652(646.75) | Grad Norm 3.5314(6.1198) | Total Time 0.00(0.00)\n",
      "Iter 1652 | Time 64.6769(62.0158) | Bit/dim 3.6713(3.6762) | Xent 0.8767(0.8979) | Loss 9.0602(9.7453) | Error 0.3199(0.3210) Steps 664(647.27) | Grad Norm 4.2083(6.0625) | Total Time 0.00(0.00)\n",
      "Iter 1653 | Time 63.5272(62.0612) | Bit/dim 3.6895(3.6766) | Xent 0.8885(0.8976) | Loss 9.2649(9.7309) | Error 0.3167(0.3209) Steps 628(646.69) | Grad Norm 3.3209(5.9802) | Total Time 0.00(0.00)\n",
      "Iter 1654 | Time 66.8078(62.2036) | Bit/dim 3.6706(3.6764) | Xent 0.8672(0.8967) | Loss 9.1124(9.7124) | Error 0.3033(0.3204) Steps 700(648.29) | Grad Norm 6.1270(5.9846) | Total Time 0.00(0.00)\n",
      "Iter 1655 | Time 58.4534(62.0911) | Bit/dim 3.6739(3.6763) | Xent 0.8962(0.8967) | Loss 9.2909(9.6997) | Error 0.3219(0.3204) Steps 652(648.40) | Grad Norm 6.4949(5.9999) | Total Time 0.00(0.00)\n",
      "Iter 1656 | Time 65.2688(62.1864) | Bit/dim 3.6663(3.6760) | Xent 0.8780(0.8961) | Loss 9.3398(9.6889) | Error 0.3105(0.3201) Steps 646(648.33) | Grad Norm 7.4018(6.0420) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0276 | Time 23.1429, Epoch Time 420.7924(408.5944), Bit/dim 3.6748(best: 3.6721), Xent 0.8929, Loss 4.1213, Error 0.3155(best: 0.3149)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1657 | Time 62.0119(62.1812) | Bit/dim 3.6695(3.6758) | Xent 0.8887(0.8959) | Loss 12.5298(9.7741) | Error 0.3131(0.3199) Steps 658(648.62) | Grad Norm 3.2529(5.9583) | Total Time 0.00(0.00)\n",
      "Iter 1658 | Time 61.3418(62.1560) | Bit/dim 3.6828(3.6760) | Xent 0.8895(0.8957) | Loss 9.0633(9.7528) | Error 0.3165(0.3198) Steps 652(648.72) | Grad Norm 8.4113(6.0319) | Total Time 0.00(0.00)\n",
      "Iter 1659 | Time 64.3734(62.2225) | Bit/dim 3.6752(3.6760) | Xent 0.8864(0.8954) | Loss 9.1656(9.7352) | Error 0.3185(0.3198) Steps 634(648.28) | Grad Norm 6.9302(6.0589) | Total Time 0.00(0.00)\n",
      "Iter 1660 | Time 62.2231(62.2225) | Bit/dim 3.6775(3.6760) | Xent 0.8738(0.8948) | Loss 9.1475(9.7176) | Error 0.3104(0.3195) Steps 646(648.21) | Grad Norm 6.0225(6.0578) | Total Time 0.00(0.00)\n",
      "Iter 1661 | Time 63.4940(62.2607) | Bit/dim 3.6756(3.6760) | Xent 0.8862(0.8945) | Loss 9.2699(9.7041) | Error 0.3113(0.3192) Steps 640(647.96) | Grad Norm 5.0741(6.0283) | Total Time 0.00(0.00)\n",
      "Iter 1662 | Time 61.6592(62.2426) | Bit/dim 3.6741(3.6760) | Xent 0.8921(0.8945) | Loss 9.0096(9.6833) | Error 0.3184(0.3192) Steps 622(647.18) | Grad Norm 7.3283(6.0673) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0277 | Time 23.4880, Epoch Time 414.9448(408.7849), Bit/dim 3.6749(best: 3.6721), Xent 0.8938, Loss 4.1218, Error 0.3174(best: 0.3149)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1663 | Time 62.0929(62.2381) | Bit/dim 3.6700(3.6758) | Xent 0.8793(0.8940) | Loss 12.5199(9.7684) | Error 0.3161(0.3191) Steps 664(647.69) | Grad Norm 4.2898(6.0139) | Total Time 0.00(0.00)\n",
      "Iter 1664 | Time 64.7993(62.3150) | Bit/dim 3.6710(3.6756) | Xent 0.8816(0.8936) | Loss 9.0871(9.7480) | Error 0.3120(0.3189) Steps 634(647.28) | Grad Norm 5.0427(5.9848) | Total Time 0.00(0.00)\n",
      "Iter 1665 | Time 66.1752(62.4308) | Bit/dim 3.6822(3.6758) | Xent 0.8725(0.8930) | Loss 9.2524(9.7331) | Error 0.3097(0.3186) Steps 670(647.96) | Grad Norm 8.1156(6.0487) | Total Time 0.00(0.00)\n",
      "Iter 1666 | Time 61.7180(62.4094) | Bit/dim 3.6860(3.6761) | Xent 0.8671(0.8922) | Loss 9.0875(9.7137) | Error 0.3100(0.3184) Steps 676(648.80) | Grad Norm 5.4687(6.0313) | Total Time 0.00(0.00)\n",
      "Iter 1667 | Time 61.5909(62.3848) | Bit/dim 3.6810(3.6763) | Xent 0.8879(0.8921) | Loss 8.7430(9.6846) | Error 0.3183(0.3184) Steps 622(648.00) | Grad Norm 5.6696(6.0205) | Total Time 0.00(0.00)\n",
      "Iter 1668 | Time 60.2240(62.3200) | Bit/dim 3.6640(3.6759) | Xent 0.8863(0.8919) | Loss 9.2064(9.6703) | Error 0.3143(0.3183) Steps 616(647.04) | Grad Norm 5.6618(6.0097) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0278 | Time 23.1888, Epoch Time 415.7471(408.9938), Bit/dim 3.6732(best: 3.6721), Xent 0.9103, Loss 4.1283, Error 0.3241(best: 0.3149)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1669 | Time 67.3905(62.4721) | Bit/dim 3.6642(3.6756) | Xent 0.9112(0.8925) | Loss 12.3709(9.7513) | Error 0.3196(0.3183) Steps 652(647.19) | Grad Norm 5.3467(5.9898) | Total Time 0.00(0.00)\n",
      "Iter 1670 | Time 65.0258(62.5487) | Bit/dim 3.6650(3.6753) | Xent 0.8590(0.8915) | Loss 9.0830(9.7312) | Error 0.3026(0.3178) Steps 676(648.05) | Grad Norm 3.6716(5.9203) | Total Time 0.00(0.00)\n",
      "Iter 1671 | Time 64.5514(62.6088) | Bit/dim 3.6881(3.6756) | Xent 0.8559(0.8904) | Loss 9.2967(9.7182) | Error 0.3043(0.3174) Steps 676(648.89) | Grad Norm 3.4984(5.8476) | Total Time 0.00(0.00)\n",
      "Iter 1672 | Time 58.6746(62.4908) | Bit/dim 3.6659(3.6754) | Xent 0.8662(0.8897) | Loss 9.1749(9.7019) | Error 0.3121(0.3173) Steps 604(647.54) | Grad Norm 4.4889(5.8069) | Total Time 0.00(0.00)\n",
      "Iter 1673 | Time 67.7031(62.6472) | Bit/dim 3.6765(3.6754) | Xent 0.8799(0.8894) | Loss 9.1526(9.6854) | Error 0.3103(0.3170) Steps 628(646.96) | Grad Norm 6.1452(5.8170) | Total Time 0.00(0.00)\n",
      "Iter 1674 | Time 61.9075(62.6250) | Bit/dim 3.6723(3.6753) | Xent 0.8747(0.8890) | Loss 8.9872(9.6645) | Error 0.3110(0.3169) Steps 640(646.75) | Grad Norm 3.2308(5.7394) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0279 | Time 22.8427, Epoch Time 424.3548(409.4546), Bit/dim 3.6760(best: 3.6721), Xent 0.8945, Loss 4.1233, Error 0.3132(best: 0.3149)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1675 | Time 58.0470(62.4876) | Bit/dim 3.6726(3.6752) | Xent 0.8665(0.8883) | Loss 12.0826(9.7370) | Error 0.3136(0.3168) Steps 646(646.72) | Grad Norm 4.7508(5.7098) | Total Time 0.00(0.00)\n",
      "Iter 1676 | Time 67.6202(62.6416) | Bit/dim 3.6665(3.6750) | Xent 0.8850(0.8882) | Loss 9.1659(9.7199) | Error 0.3116(0.3166) Steps 622(645.98) | Grad Norm 6.5069(5.7337) | Total Time 0.00(0.00)\n",
      "Iter 1677 | Time 60.6374(62.5815) | Bit/dim 3.6766(3.6750) | Xent 0.8831(0.8880) | Loss 9.0605(9.7001) | Error 0.3163(0.3166) Steps 646(645.98) | Grad Norm 4.7683(5.7047) | Total Time 0.00(0.00)\n",
      "Iter 1678 | Time 62.8513(62.5896) | Bit/dim 3.6817(3.6752) | Xent 0.8714(0.8875) | Loss 9.1404(9.6833) | Error 0.3075(0.3163) Steps 658(646.34) | Grad Norm 2.7359(5.6157) | Total Time 0.00(0.00)\n",
      "Iter 1679 | Time 64.0983(62.6348) | Bit/dim 3.6793(3.6753) | Xent 0.8725(0.8871) | Loss 9.0956(9.6657) | Error 0.3089(0.3161) Steps 652(646.51) | Grad Norm 3.6588(5.5570) | Total Time 0.00(0.00)\n",
      "Iter 1680 | Time 64.4884(62.6904) | Bit/dim 3.6633(3.6750) | Xent 0.8772(0.8868) | Loss 9.1012(9.6487) | Error 0.3141(0.3160) Steps 664(647.04) | Grad Norm 4.9060(5.5374) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0280 | Time 23.6496, Epoch Time 417.6159(409.6995), Bit/dim 3.6767(best: 3.6721), Xent 0.8895, Loss 4.1214, Error 0.3135(best: 0.3132)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1681 | Time 63.9889(62.7294) | Bit/dim 3.6669(3.6747) | Xent 0.8626(0.8861) | Loss 12.4653(9.7332) | Error 0.3039(0.3157) Steps 610(645.93) | Grad Norm 5.3375(5.5314) | Total Time 0.00(0.00)\n",
      "Iter 1682 | Time 61.3122(62.6869) | Bit/dim 3.6756(3.6748) | Xent 0.8882(0.8861) | Loss 9.2420(9.7185) | Error 0.3211(0.3158) Steps 640(645.75) | Grad Norm 7.0854(5.5780) | Total Time 0.00(0.00)\n",
      "Iter 1683 | Time 64.7572(62.7490) | Bit/dim 3.6642(3.6744) | Xent 0.8556(0.8852) | Loss 9.0798(9.6993) | Error 0.3045(0.3155) Steps 640(645.58) | Grad Norm 3.0218(5.5014) | Total Time 0.00(0.00)\n",
      "Iter 1684 | Time 67.2080(62.8828) | Bit/dim 3.6878(3.6748) | Xent 0.8623(0.8845) | Loss 9.1986(9.6843) | Error 0.3055(0.3152) Steps 670(646.31) | Grad Norm 6.8128(5.5407) | Total Time 0.00(0.00)\n",
      "Iter 1685 | Time 60.0165(62.7968) | Bit/dim 3.6686(3.6747) | Xent 0.8944(0.8848) | Loss 9.0544(9.6654) | Error 0.3180(0.3153) Steps 652(646.48) | Grad Norm 8.5030(5.6296) | Total Time 0.00(0.00)\n",
      "Iter 1686 | Time 62.7704(62.7960) | Bit/dim 3.6676(3.6744) | Xent 0.8867(0.8849) | Loss 9.1085(9.6487) | Error 0.3250(0.3156) Steps 664(647.01) | Grad Norm 3.4498(5.5642) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0281 | Time 23.7320, Epoch Time 419.8842(410.0050), Bit/dim 3.6749(best: 3.6721), Xent 0.9025, Loss 4.1262, Error 0.3166(best: 0.3132)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1687 | Time 64.8660(62.8581) | Bit/dim 3.6710(3.6743) | Xent 0.8691(0.8844) | Loss 12.3893(9.7309) | Error 0.3081(0.3154) Steps 664(647.52) | Grad Norm 8.6863(5.6578) | Total Time 0.00(0.00)\n",
      "Iter 1688 | Time 58.8548(62.7380) | Bit/dim 3.6696(3.6742) | Xent 0.8862(0.8845) | Loss 9.2944(9.7178) | Error 0.3144(0.3153) Steps 622(646.75) | Grad Norm 8.0450(5.7295) | Total Time 0.00(0.00)\n",
      "Iter 1689 | Time 58.0051(62.5960) | Bit/dim 3.6673(3.6740) | Xent 0.8784(0.8843) | Loss 9.1619(9.7012) | Error 0.3105(0.3152) Steps 646(646.73) | Grad Norm 4.8096(5.7019) | Total Time 0.00(0.00)\n",
      "Iter 1690 | Time 62.6569(62.5978) | Bit/dim 3.6785(3.6741) | Xent 0.8809(0.8842) | Loss 9.1055(9.6833) | Error 0.3135(0.3151) Steps 628(646.17) | Grad Norm 8.0279(5.7716) | Total Time 0.00(0.00)\n",
      "Iter 1691 | Time 61.3500(62.5604) | Bit/dim 3.6687(3.6740) | Xent 0.8980(0.8846) | Loss 9.3304(9.6727) | Error 0.3245(0.3154) Steps 652(646.34) | Grad Norm 9.9801(5.8979) | Total Time 0.00(0.00)\n",
      "Iter 1692 | Time 61.3829(62.5251) | Bit/dim 3.6747(3.6740) | Xent 0.8956(0.8849) | Loss 9.1694(9.6576) | Error 0.3167(0.3155) Steps 634(645.97) | Grad Norm 4.2815(5.8494) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0282 | Time 23.0203, Epoch Time 406.4633(409.8988), Bit/dim 3.6743(best: 3.6721), Xent 0.9241, Loss 4.1364, Error 0.3277(best: 0.3132)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1693 | Time 61.2003(62.4853) | Bit/dim 3.6727(3.6739) | Xent 0.9125(0.8858) | Loss 12.4246(9.7406) | Error 0.3257(0.3158) Steps 622(645.25) | Grad Norm 10.4172(5.9864) | Total Time 0.00(0.00)\n",
      "Iter 1694 | Time 63.0901(62.5035) | Bit/dim 3.6756(3.6740) | Xent 0.8838(0.8857) | Loss 9.1889(9.7241) | Error 0.3120(0.3156) Steps 664(645.81) | Grad Norm 9.1556(6.0815) | Total Time 0.00(0.00)\n",
      "Iter 1695 | Time 59.9987(62.4283) | Bit/dim 3.6796(3.6742) | Xent 0.8753(0.8854) | Loss 9.2523(9.7099) | Error 0.3096(0.3155) Steps 646(645.82) | Grad Norm 5.8696(6.0752) | Total Time 0.00(0.00)\n",
      "Iter 1696 | Time 63.2705(62.4536) | Bit/dim 3.6666(3.6739) | Xent 0.8887(0.8855) | Loss 9.1133(9.6920) | Error 0.3147(0.3154) Steps 634(645.46) | Grad Norm 7.8351(6.1280) | Total Time 0.00(0.00)\n",
      "Iter 1697 | Time 57.7134(62.3114) | Bit/dim 3.6612(3.6736) | Xent 0.8681(0.8850) | Loss 9.1269(9.6751) | Error 0.3087(0.3152) Steps 640(645.30) | Grad Norm 4.7812(6.0876) | Total Time 0.00(0.00)\n",
      "Iter 1698 | Time 61.9390(62.3002) | Bit/dim 3.6719(3.6735) | Xent 0.8960(0.8853) | Loss 9.2534(9.6624) | Error 0.3240(0.3155) Steps 616(644.42) | Grad Norm 7.4079(6.1272) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0283 | Time 23.2522, Epoch Time 406.5068(409.7970), Bit/dim 3.6738(best: 3.6721), Xent 0.8854, Loss 4.1165, Error 0.3145(best: 0.3132)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1699 | Time 62.4491(62.3047) | Bit/dim 3.6638(3.6732) | Xent 0.8612(0.8846) | Loss 12.5294(9.7484) | Error 0.3125(0.3154) Steps 610(643.39) | Grad Norm 3.3859(6.0449) | Total Time 0.00(0.00)\n",
      "Iter 1700 | Time 59.3530(62.2161) | Bit/dim 3.6671(3.6730) | Xent 0.9132(0.8854) | Loss 8.7697(9.7191) | Error 0.3254(0.3157) Steps 622(642.75) | Grad Norm 8.6176(6.1221) | Total Time 0.00(0.00)\n",
      "Iter 1701 | Time 59.0699(62.1217) | Bit/dim 3.6658(3.6728) | Xent 0.8716(0.8850) | Loss 9.0126(9.6979) | Error 0.3151(0.3157) Steps 634(642.49) | Grad Norm 5.2487(6.0959) | Total Time 0.00(0.00)\n",
      "Iter 1702 | Time 62.6509(62.1376) | Bit/dim 3.6799(3.6730) | Xent 0.8963(0.8854) | Loss 9.1402(9.6811) | Error 0.3231(0.3159) Steps 670(643.31) | Grad Norm 6.6743(6.1133) | Total Time 0.00(0.00)\n",
      "Iter 1703 | Time 57.8937(62.0103) | Bit/dim 3.6818(3.6733) | Xent 0.8628(0.8847) | Loss 8.9872(9.6603) | Error 0.3076(0.3157) Steps 628(642.85) | Grad Norm 5.5702(6.0970) | Total Time 0.00(0.00)\n",
      "Iter 1704 | Time 62.8303(62.0349) | Bit/dim 3.6751(3.6733) | Xent 0.8729(0.8843) | Loss 9.1781(9.6458) | Error 0.3173(0.3157) Steps 646(642.95) | Grad Norm 7.4813(6.1385) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0284 | Time 23.9456, Epoch Time 404.5587(409.6399), Bit/dim 3.6727(best: 3.6721), Xent 0.8974, Loss 4.1214, Error 0.3184(best: 0.3132)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1705 | Time 56.0761(61.8561) | Bit/dim 3.6596(3.6729) | Xent 0.8567(0.8835) | Loss 12.3298(9.7264) | Error 0.3110(0.3156) Steps 658(643.40) | Grad Norm 4.2783(6.0827) | Total Time 0.00(0.00)\n",
      "Iter 1706 | Time 62.2940(61.8693) | Bit/dim 3.6775(3.6731) | Xent 0.8747(0.8832) | Loss 9.2796(9.7130) | Error 0.3115(0.3155) Steps 628(642.94) | Grad Norm 5.4394(6.0634) | Total Time 0.00(0.00)\n",
      "Iter 1707 | Time 62.4928(61.8880) | Bit/dim 3.6699(3.6730) | Xent 0.8546(0.8824) | Loss 9.3103(9.7009) | Error 0.3031(0.3151) Steps 652(643.21) | Grad Norm 4.7731(6.0247) | Total Time 0.00(0.00)\n",
      "Iter 1708 | Time 59.8491(61.8268) | Bit/dim 3.6632(3.6727) | Xent 0.8823(0.8824) | Loss 9.0512(9.6814) | Error 0.3193(0.3152) Steps 640(643.11) | Grad Norm 4.2430(5.9712) | Total Time 0.00(0.00)\n",
      "Iter 1709 | Time 60.6996(61.7930) | Bit/dim 3.6715(3.6726) | Xent 0.8639(0.8818) | Loss 9.1161(9.6644) | Error 0.3074(0.3150) Steps 634(642.84) | Grad Norm 4.0456(5.9135) | Total Time 0.00(0.00)\n",
      "Iter 1710 | Time 65.2012(61.8952) | Bit/dim 3.6825(3.6729) | Xent 0.8454(0.8807) | Loss 9.1862(9.6501) | Error 0.3029(0.3146) Steps 658(643.29) | Grad Norm 5.7657(5.9090) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0285 | Time 22.8477, Epoch Time 406.0232(409.5314), Bit/dim 3.6716(best: 3.6721), Xent 0.8893, Loss 4.1162, Error 0.3139(best: 0.3132)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1711 | Time 58.8713(61.8045) | Bit/dim 3.6759(3.6730) | Xent 0.8543(0.8799) | Loss 12.2119(9.7269) | Error 0.3020(0.3142) Steps 658(643.73) | Grad Norm 3.7588(5.8445) | Total Time 0.00(0.00)\n",
      "Iter 1712 | Time 58.9419(61.7186) | Bit/dim 3.6699(3.6729) | Xent 0.8787(0.8799) | Loss 9.3343(9.7152) | Error 0.3129(0.3142) Steps 640(643.62) | Grad Norm 4.8754(5.8154) | Total Time 0.00(0.00)\n",
      "Iter 1713 | Time 66.6037(61.8652) | Bit/dim 3.6775(3.6731) | Xent 0.8655(0.8795) | Loss 8.8965(9.6906) | Error 0.3099(0.3141) Steps 634(643.33) | Grad Norm 5.7247(5.8127) | Total Time 0.00(0.00)\n",
      "Iter 1714 | Time 59.4482(61.7927) | Bit/dim 3.6772(3.6732) | Xent 0.8812(0.8795) | Loss 9.2107(9.6762) | Error 0.3149(0.3141) Steps 640(643.23) | Grad Norm 4.8294(5.7832) | Total Time 0.00(0.00)\n",
      "Iter 1715 | Time 60.3774(61.7502) | Bit/dim 3.6637(3.6729) | Xent 0.8905(0.8798) | Loss 9.1247(9.6597) | Error 0.3200(0.3143) Steps 652(643.50) | Grad Norm 7.1226(5.8234) | Total Time 0.00(0.00)\n",
      "Iter 1716 | Time 64.1541(61.8223) | Bit/dim 3.6566(3.6724) | Xent 0.9019(0.8805) | Loss 9.1534(9.6445) | Error 0.3196(0.3144) Steps 676(644.47) | Grad Norm 9.1665(5.9237) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0286 | Time 23.6540, Epoch Time 408.1398(409.4896), Bit/dim 3.6801(best: 3.6716), Xent 0.9015, Loss 4.1308, Error 0.3205(best: 0.3132)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1717 | Time 62.9536(61.8563) | Bit/dim 3.6784(3.6726) | Xent 0.8714(0.8802) | Loss 12.6990(9.7361) | Error 0.3083(0.3142) Steps 634(644.16) | Grad Norm 8.8265(6.0108) | Total Time 0.00(0.00)\n",
      "Iter 1718 | Time 63.5891(61.9083) | Bit/dim 3.6675(3.6724) | Xent 0.8527(0.8794) | Loss 9.0634(9.7159) | Error 0.3125(0.3142) Steps 616(643.31) | Grad Norm 3.9665(5.9495) | Total Time 0.00(0.00)\n",
      "Iter 1719 | Time 64.0706(61.9731) | Bit/dim 3.6747(3.6725) | Xent 0.8693(0.8791) | Loss 9.2583(9.7022) | Error 0.3116(0.3141) Steps 676(644.29) | Grad Norm 7.0158(5.9814) | Total Time 0.00(0.00)\n",
      "Iter 1720 | Time 56.9509(61.8225) | Bit/dim 3.6679(3.6724) | Xent 0.8566(0.8784) | Loss 9.2169(9.6876) | Error 0.3023(0.3138) Steps 634(643.98) | Grad Norm 1.9089(5.8593) | Total Time 0.00(0.00)\n",
      "Iter 1721 | Time 64.1425(61.8921) | Bit/dim 3.6768(3.6725) | Xent 0.8817(0.8785) | Loss 9.2050(9.6732) | Error 0.3159(0.3138) Steps 640(643.86) | Grad Norm 6.0873(5.8661) | Total Time 0.00(0.00)\n",
      "Iter 1722 | Time 62.2133(61.9017) | Bit/dim 3.6660(3.6723) | Xent 0.8650(0.8781) | Loss 9.2040(9.6591) | Error 0.3013(0.3134) Steps 658(644.29) | Grad Norm 2.3231(5.7598) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0287 | Time 23.4064, Epoch Time 413.7786(409.6183), Bit/dim 3.6712(best: 3.6716), Xent 0.8873, Loss 4.1149, Error 0.3157(best: 0.3132)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1723 | Time 63.2697(61.9427) | Bit/dim 3.6727(3.6723) | Xent 0.8578(0.8775) | Loss 12.5951(9.7472) | Error 0.3016(0.3131) Steps 658(644.70) | Grad Norm 4.8899(5.7337) | Total Time 0.00(0.00)\n",
      "Iter 1724 | Time 60.4526(61.8980) | Bit/dim 3.6575(3.6719) | Xent 0.8497(0.8767) | Loss 8.9417(9.7230) | Error 0.3040(0.3128) Steps 652(644.92) | Grad Norm 3.6995(5.6727) | Total Time 0.00(0.00)\n",
      "Iter 1725 | Time 62.7006(61.9221) | Bit/dim 3.6813(3.6722) | Xent 0.8669(0.8764) | Loss 9.2240(9.7080) | Error 0.3117(0.3128) Steps 646(644.95) | Grad Norm 5.9455(5.6809) | Total Time 0.00(0.00)\n",
      "Iter 1726 | Time 58.6199(61.8231) | Bit/dim 3.6613(3.6718) | Xent 0.8467(0.8755) | Loss 8.9304(9.6847) | Error 0.2967(0.3123) Steps 640(644.80) | Grad Norm 3.3480(5.6109) | Total Time 0.00(0.00)\n",
      "Iter 1727 | Time 61.4386(61.8115) | Bit/dim 3.6716(3.6718) | Xent 0.8781(0.8756) | Loss 9.0758(9.6664) | Error 0.3185(0.3125) Steps 640(644.66) | Grad Norm 5.1166(5.5961) | Total Time 0.00(0.00)\n",
      "Iter 1728 | Time 59.0098(61.7275) | Bit/dim 3.6635(3.6716) | Xent 0.9043(0.8764) | Loss 9.2195(9.6530) | Error 0.3189(0.3127) Steps 664(645.24) | Grad Norm 5.1566(5.5829) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0288 | Time 23.7251, Epoch Time 405.3130(409.4891), Bit/dim 3.6703(best: 3.6712), Xent 0.8936, Loss 4.1171, Error 0.3123(best: 0.3132)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1729 | Time 55.3988(61.5376) | Bit/dim 3.6709(3.6716) | Xent 0.8632(0.8760) | Loss 12.5924(9.7412) | Error 0.3115(0.3126) Steps 622(644.54) | Grad Norm 6.7217(5.6170) | Total Time 0.00(0.00)\n",
      "Iter 1730 | Time 59.1067(61.4647) | Bit/dim 3.6642(3.6713) | Xent 0.8510(0.8753) | Loss 9.1432(9.7233) | Error 0.3000(0.3123) Steps 658(644.95) | Grad Norm 3.5154(5.5540) | Total Time 0.00(0.00)\n",
      "Iter 1731 | Time 59.4779(61.4051) | Bit/dim 3.6710(3.6713) | Xent 0.9030(0.8761) | Loss 9.2189(9.7081) | Error 0.3233(0.3126) Steps 640(644.80) | Grad Norm 9.1125(5.6608) | Total Time 0.00(0.00)\n",
      "Iter 1732 | Time 61.0556(61.3946) | Bit/dim 3.6814(3.6716) | Xent 0.8665(0.8758) | Loss 9.0983(9.6898) | Error 0.3125(0.3126) Steps 652(645.01) | Grad Norm 8.2245(5.7377) | Total Time 0.00(0.00)\n",
      "Iter 1733 | Time 60.9063(61.3799) | Bit/dim 3.6649(3.6714) | Xent 0.8759(0.8758) | Loss 9.0958(9.6720) | Error 0.3144(0.3126) Steps 640(644.86) | Grad Norm 6.7597(5.7683) | Total Time 0.00(0.00)\n",
      "Iter 1734 | Time 61.0943(61.3714) | Bit/dim 3.6657(3.6713) | Xent 0.9047(0.8767) | Loss 8.9311(9.6498) | Error 0.3230(0.3130) Steps 658(645.26) | Grad Norm 10.2040(5.9014) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0289 | Time 23.1265, Epoch Time 396.2001(409.0904), Bit/dim 3.6674(best: 3.6703), Xent 0.9107, Loss 4.1228, Error 0.3214(best: 0.3123)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1735 | Time 65.7933(61.5040) | Bit/dim 3.6708(3.6712) | Xent 0.8785(0.8768) | Loss 12.5409(9.7365) | Error 0.3151(0.3130) Steps 658(645.64) | Grad Norm 10.0011(6.0244) | Total Time 0.00(0.00)\n",
      "Iter 1736 | Time 64.3323(61.5889) | Bit/dim 3.6589(3.6709) | Xent 0.8858(0.8770) | Loss 9.3207(9.7241) | Error 0.3160(0.3131) Steps 670(646.37) | Grad Norm 6.3523(6.0342) | Total Time 0.00(0.00)\n",
      "Iter 1737 | Time 62.6642(61.6211) | Bit/dim 3.6769(3.6711) | Xent 0.8837(0.8772) | Loss 9.2072(9.7086) | Error 0.3181(0.3133) Steps 646(646.36) | Grad Norm 10.5614(6.1700) | Total Time 0.00(0.00)\n",
      "Iter 1738 | Time 64.2697(61.7006) | Bit/dim 3.6698(3.6710) | Xent 0.9191(0.8785) | Loss 9.0808(9.6897) | Error 0.3260(0.3136) Steps 628(645.81) | Grad Norm 11.7477(6.3374) | Total Time 0.00(0.00)\n",
      "Iter 1739 | Time 63.1846(61.7451) | Bit/dim 3.6579(3.6706) | Xent 0.8742(0.8784) | Loss 9.1760(9.6743) | Error 0.3180(0.3138) Steps 658(646.17) | Grad Norm 7.4502(6.3708) | Total Time 0.00(0.00)\n",
      "Iter 1740 | Time 62.4744(61.7670) | Bit/dim 3.6738(3.6707) | Xent 0.9109(0.8793) | Loss 9.3436(9.6644) | Error 0.3285(0.3142) Steps 658(646.53) | Grad Norm 9.8076(6.4739) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0290 | Time 23.3490, Epoch Time 422.1252(409.4815), Bit/dim 3.6700(best: 3.6674), Xent 0.8920, Loss 4.1160, Error 0.3150(best: 0.3123)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1741 | Time 60.9282(61.7418) | Bit/dim 3.6731(3.6708) | Xent 0.8959(0.8798) | Loss 12.4749(9.7487) | Error 0.3203(0.3144) Steps 676(647.41) | Grad Norm 6.6089(6.4779) | Total Time 0.00(0.00)\n",
      "Iter 1742 | Time 60.8640(61.7155) | Bit/dim 3.6816(3.6711) | Xent 0.8631(0.8793) | Loss 9.2101(9.7325) | Error 0.3054(0.3141) Steps 646(647.37) | Grad Norm 4.2547(6.4112) | Total Time 0.00(0.00)\n",
      "Iter 1743 | Time 61.8897(61.7207) | Bit/dim 3.6601(3.6708) | Xent 0.8832(0.8794) | Loss 9.0162(9.7111) | Error 0.3167(0.3142) Steps 646(647.33) | Grad Norm 7.4569(6.4426) | Total Time 0.00(0.00)\n",
      "Iter 1744 | Time 59.5619(61.6560) | Bit/dim 3.6706(3.6708) | Xent 0.8628(0.8789) | Loss 8.9817(9.6892) | Error 0.3113(0.3141) Steps 658(647.65) | Grad Norm 6.2050(6.4355) | Total Time 0.00(0.00)\n",
      "Iter 1745 | Time 59.3406(61.5865) | Bit/dim 3.6692(3.6707) | Xent 0.8526(0.8782) | Loss 9.1100(9.6718) | Error 0.3047(0.3138) Steps 646(647.60) | Grad Norm 6.1940(6.4282) | Total Time 0.00(0.00)\n",
      "Iter 1746 | Time 57.5322(61.4649) | Bit/dim 3.6668(3.6706) | Xent 0.9083(0.8791) | Loss 9.0058(9.6518) | Error 0.3213(0.3141) Steps 634(647.19) | Grad Norm 10.9198(6.5630) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0291 | Time 23.5048, Epoch Time 399.7968(409.1909), Bit/dim 3.6689(best: 3.6674), Xent 0.9082, Loss 4.1230, Error 0.3252(best: 0.3123)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1747 | Time 61.9366(61.4790) | Bit/dim 3.6679(3.6705) | Xent 0.8839(0.8792) | Loss 12.1837(9.7278) | Error 0.3210(0.3143) Steps 676(648.06) | Grad Norm 6.8775(6.5724) | Total Time 0.00(0.00)\n",
      "Iter 1748 | Time 62.0012(61.4947) | Bit/dim 3.6728(3.6706) | Xent 0.8647(0.8788) | Loss 9.1532(9.7105) | Error 0.3090(0.3141) Steps 640(647.81) | Grad Norm 4.0065(6.4954) | Total Time 0.00(0.00)\n",
      "Iter 1749 | Time 57.1017(61.3629) | Bit/dim 3.6686(3.6705) | Xent 0.8703(0.8785) | Loss 9.0139(9.6896) | Error 0.3107(0.3140) Steps 640(647.58) | Grad Norm 5.8947(6.4774) | Total Time 0.00(0.00)\n",
      "Iter 1750 | Time 63.6210(61.4306) | Bit/dim 3.6581(3.6702) | Xent 0.8848(0.8787) | Loss 9.0118(9.6693) | Error 0.3166(0.3141) Steps 634(647.17) | Grad Norm 5.3111(6.4424) | Total Time 0.00(0.00)\n",
      "Iter 1751 | Time 62.1665(61.4527) | Bit/dim 3.6660(3.6700) | Xent 0.8680(0.8784) | Loss 9.1261(9.6530) | Error 0.3066(0.3139) Steps 634(646.78) | Grad Norm 3.7904(6.3629) | Total Time 0.00(0.00)\n",
      "Iter 1752 | Time 62.8553(61.4948) | Bit/dim 3.6738(3.6702) | Xent 0.8695(0.8781) | Loss 9.1011(9.6364) | Error 0.3161(0.3139) Steps 688(648.01) | Grad Norm 5.6538(6.3416) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0292 | Time 22.9822, Epoch Time 408.7534(409.1778), Bit/dim 3.6721(best: 3.6674), Xent 0.8944, Loss 4.1193, Error 0.3155(best: 0.3123)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1753 | Time 66.2827(61.6384) | Bit/dim 3.6655(3.6700) | Xent 0.8702(0.8779) | Loss 12.3883(9.7190) | Error 0.3107(0.3138) Steps 616(647.05) | Grad Norm 5.8223(6.3260) | Total Time 0.00(0.00)\n",
      "Iter 1754 | Time 60.6062(61.6075) | Bit/dim 3.6664(3.6699) | Xent 0.8942(0.8784) | Loss 9.1438(9.7017) | Error 0.3151(0.3139) Steps 652(647.20) | Grad Norm 8.0594(6.3780) | Total Time 0.00(0.00)\n",
      "Iter 1755 | Time 63.9020(61.6763) | Bit/dim 3.6672(3.6698) | Xent 0.8753(0.8783) | Loss 9.2008(9.6867) | Error 0.3129(0.3138) Steps 646(647.17) | Grad Norm 8.3699(6.4378) | Total Time 0.00(0.00)\n",
      "Iter 1756 | Time 57.6178(61.5546) | Bit/dim 3.6773(3.6700) | Xent 0.8491(0.8774) | Loss 9.0173(9.6666) | Error 0.2981(0.3134) Steps 646(647.13) | Grad Norm 5.4537(6.4082) | Total Time 0.00(0.00)\n",
      "Iter 1757 | Time 60.9362(61.5360) | Bit/dim 3.6601(3.6697) | Xent 0.8624(0.8770) | Loss 9.1102(9.6499) | Error 0.3050(0.3131) Steps 664(647.64) | Grad Norm 5.8206(6.3906) | Total Time 0.00(0.00)\n",
      "Iter 1758 | Time 60.7812(61.5134) | Bit/dim 3.6781(3.6700) | Xent 0.8561(0.8763) | Loss 8.9884(9.6301) | Error 0.3085(0.3130) Steps 676(648.49) | Grad Norm 5.4512(6.3624) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0293 | Time 23.7222, Epoch Time 410.1748(409.2077), Bit/dim 3.6675(best: 3.6674), Xent 0.8787, Loss 4.1068, Error 0.3136(best: 0.3123)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1759 | Time 62.5429(61.5442) | Bit/dim 3.6692(3.6700) | Xent 0.8606(0.8759) | Loss 11.7910(9.6949) | Error 0.3109(0.3129) Steps 622(647.69) | Grad Norm 2.6832(6.2521) | Total Time 0.00(0.00)\n",
      "Iter 1760 | Time 63.2380(61.5951) | Bit/dim 3.6632(3.6698) | Xent 0.8644(0.8755) | Loss 9.1552(9.6787) | Error 0.3106(0.3128) Steps 640(647.46) | Grad Norm 3.9223(6.1822) | Total Time 0.00(0.00)\n",
      "Iter 1761 | Time 64.6146(61.6856) | Bit/dim 3.6761(3.6700) | Xent 0.8535(0.8748) | Loss 9.1146(9.6618) | Error 0.3029(0.3126) Steps 658(647.78) | Grad Norm 3.2035(6.0928) | Total Time 0.00(0.00)\n",
      "Iter 1762 | Time 59.0025(61.6051) | Bit/dim 3.6632(3.6698) | Xent 0.8701(0.8747) | Loss 8.9908(9.6417) | Error 0.3100(0.3125) Steps 628(647.19) | Grad Norm 3.7822(6.0235) | Total Time 0.00(0.00)\n",
      "Iter 1763 | Time 56.9628(61.4659) | Bit/dim 3.6693(3.6697) | Xent 0.8438(0.8738) | Loss 9.0622(9.6243) | Error 0.3009(0.3121) Steps 634(646.79) | Grad Norm 5.3544(6.0034) | Total Time 0.00(0.00)\n",
      "Iter 1764 | Time 60.5516(61.4384) | Bit/dim 3.6774(3.6700) | Xent 0.8434(0.8729) | Loss 9.0983(9.6085) | Error 0.2986(0.3117) Steps 646(646.77) | Grad Norm 2.6165(5.9018) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0294 | Time 23.3778, Epoch Time 406.3226(409.1212), Bit/dim 3.6685(best: 3.6674), Xent 0.8776, Loss 4.1073, Error 0.3131(best: 0.3123)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1765 | Time 65.7907(61.5690) | Bit/dim 3.6673(3.6699) | Xent 0.8351(0.8717) | Loss 12.6655(9.7002) | Error 0.3027(0.3115) Steps 664(647.28) | Grad Norm 2.9429(5.8130) | Total Time 0.00(0.00)\n",
      "Iter 1766 | Time 64.0514(61.6435) | Bit/dim 3.6655(3.6698) | Xent 0.8648(0.8715) | Loss 8.9922(9.6790) | Error 0.3105(0.3114) Steps 610(646.16) | Grad Norm 4.5972(5.7766) | Total Time 0.00(0.00)\n",
      "Iter 1767 | Time 63.8089(61.7084) | Bit/dim 3.6663(3.6697) | Xent 0.8461(0.8708) | Loss 9.2053(9.6648) | Error 0.3011(0.3111) Steps 658(646.52) | Grad Norm 3.1177(5.6968) | Total Time 0.00(0.00)\n",
      "Iter 1768 | Time 63.8938(61.7740) | Bit/dim 3.6696(3.6697) | Xent 0.8519(0.8702) | Loss 8.9001(9.6418) | Error 0.2953(0.3106) Steps 640(646.32) | Grad Norm 3.6919(5.6366) | Total Time 0.00(0.00)\n",
      "Iter 1769 | Time 68.5767(61.9781) | Bit/dim 3.6657(3.6695) | Xent 0.8511(0.8696) | Loss 8.9977(9.6225) | Error 0.3103(0.3106) Steps 652(646.49) | Grad Norm 4.6200(5.6061) | Total Time 0.00(0.00)\n",
      "Iter 1770 | Time 60.3329(61.9287) | Bit/dim 3.6729(3.6696) | Xent 0.8420(0.8688) | Loss 8.9051(9.6010) | Error 0.2984(0.3103) Steps 622(645.76) | Grad Norm 2.5997(5.5160) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0295 | Time 23.6441, Epoch Time 425.7701(409.6206), Bit/dim 3.6680(best: 3.6674), Xent 0.8783, Loss 4.1072, Error 0.3099(best: 0.3123)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1771 | Time 63.6227(61.9796) | Bit/dim 3.6598(3.6693) | Xent 0.8613(0.8686) | Loss 12.5070(9.6882) | Error 0.3074(0.3102) Steps 634(645.41) | Grad Norm 5.1193(5.5041) | Total Time 0.00(0.00)\n",
      "Iter 1772 | Time 60.4062(61.9324) | Bit/dim 3.6864(3.6699) | Xent 0.8390(0.8677) | Loss 9.1096(9.6708) | Error 0.3021(0.3099) Steps 646(645.42) | Grad Norm 4.2363(5.4660) | Total Time 0.00(0.00)\n",
      "Iter 1773 | Time 60.7528(61.8970) | Bit/dim 3.6648(3.6697) | Xent 0.8496(0.8671) | Loss 9.1837(9.6562) | Error 0.3050(0.3098) Steps 658(645.80) | Grad Norm 5.7060(5.4732) | Total Time 0.00(0.00)\n",
      "Iter 1774 | Time 60.8658(61.8660) | Bit/dim 3.6745(3.6698) | Xent 0.8600(0.8669) | Loss 9.1950(9.6424) | Error 0.3129(0.3099) Steps 634(645.45) | Grad Norm 8.0313(5.5500) | Total Time 0.00(0.00)\n",
      "Iter 1775 | Time 60.1602(61.8149) | Bit/dim 3.6610(3.6696) | Xent 0.8707(0.8670) | Loss 9.1189(9.6267) | Error 0.3113(0.3099) Steps 664(646.00) | Grad Norm 7.5143(5.6089) | Total Time 0.00(0.00)\n",
      "Iter 1776 | Time 63.3855(61.8620) | Bit/dim 3.6650(3.6694) | Xent 0.8570(0.8667) | Loss 9.2438(9.6152) | Error 0.3046(0.3098) Steps 664(646.54) | Grad Norm 4.1264(5.5644) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0296 | Time 22.4672, Epoch Time 407.6361(409.5611), Bit/dim 3.6675(best: 3.6674), Xent 0.8854, Loss 4.1102, Error 0.3156(best: 0.3099)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1777 | Time 60.7143(61.8275) | Bit/dim 3.6733(3.6696) | Xent 0.8392(0.8659) | Loss 12.2091(9.6930) | Error 0.3036(0.3096) Steps 652(646.71) | Grad Norm 5.3477(5.5579) | Total Time 0.00(0.00)\n",
      "Iter 1778 | Time 62.3593(61.8435) | Bit/dim 3.6658(3.6694) | Xent 0.8368(0.8650) | Loss 8.9103(9.6695) | Error 0.2999(0.3093) Steps 652(646.87) | Grad Norm 5.5228(5.5569) | Total Time 0.00(0.00)\n",
      "Iter 1779 | Time 58.3323(61.7382) | Bit/dim 3.6679(3.6694) | Xent 0.8686(0.8651) | Loss 8.9311(9.6474) | Error 0.3155(0.3095) Steps 634(646.48) | Grad Norm 4.8485(5.5356) | Total Time 0.00(0.00)\n",
      "Iter 1780 | Time 64.3391(61.8162) | Bit/dim 3.6691(3.6694) | Xent 0.8548(0.8648) | Loss 8.9991(9.6279) | Error 0.3071(0.3094) Steps 670(647.19) | Grad Norm 2.8920(5.4563) | Total Time 0.00(0.00)\n",
      "Iter 1781 | Time 61.0459(61.7931) | Bit/dim 3.6728(3.6695) | Xent 0.8583(0.8646) | Loss 9.1341(9.6131) | Error 0.2995(0.3091) Steps 628(646.61) | Grad Norm 2.8811(5.3790) | Total Time 0.00(0.00)\n",
      "Iter 1782 | Time 57.1691(61.6544) | Bit/dim 3.6694(3.6695) | Xent 0.8481(0.8641) | Loss 8.8991(9.5917) | Error 0.3057(0.3090) Steps 652(646.77) | Grad Norm 2.6715(5.2978) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0297 | Time 23.1217, Epoch Time 402.9232(409.3620), Bit/dim 3.6679(best: 3.6674), Xent 0.8734, Loss 4.1046, Error 0.3083(best: 0.3099)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1783 | Time 62.2841(61.6733) | Bit/dim 3.6697(3.6695) | Xent 0.8471(0.8636) | Loss 12.5291(9.6798) | Error 0.3074(0.3090) Steps 664(647.29) | Grad Norm 2.7558(5.2216) | Total Time 0.00(0.00)\n",
      "Iter 1784 | Time 58.1085(61.5663) | Bit/dim 3.6640(3.6693) | Xent 0.8541(0.8633) | Loss 9.0244(9.6601) | Error 0.3003(0.3087) Steps 640(647.07) | Grad Norm 5.5845(5.2325) | Total Time 0.00(0.00)\n",
      "Iter 1785 | Time 57.9880(61.4590) | Bit/dim 3.6576(3.6690) | Xent 0.8511(0.8630) | Loss 8.9075(9.6376) | Error 0.3009(0.3085) Steps 628(646.50) | Grad Norm 8.3312(5.3254) | Total Time 0.00(0.00)\n",
      "Iter 1786 | Time 60.5427(61.4315) | Bit/dim 3.6716(3.6691) | Xent 0.8978(0.8640) | Loss 8.8163(9.6129) | Error 0.3221(0.3089) Steps 634(646.12) | Grad Norm 10.2742(5.4739) | Total Time 0.00(0.00)\n",
      "Iter 1787 | Time 65.4478(61.5520) | Bit/dim 3.6765(3.6693) | Xent 0.8961(0.8650) | Loss 9.2665(9.6025) | Error 0.3209(0.3092) Steps 658(646.48) | Grad Norm 9.0690(5.5817) | Total Time 0.00(0.00)\n",
      "Iter 1788 | Time 60.7477(61.5278) | Bit/dim 3.6734(3.6694) | Xent 0.8535(0.8646) | Loss 8.8879(9.5811) | Error 0.3064(0.3091) Steps 628(645.93) | Grad Norm 4.1665(5.5393) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0298 | Time 23.5402, Epoch Time 404.8962(409.2280), Bit/dim 3.6658(best: 3.6674), Xent 0.8859, Loss 4.1088, Error 0.3142(best: 0.3083)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1789 | Time 60.6035(61.5001) | Bit/dim 3.6776(3.6696) | Xent 0.8438(0.8640) | Loss 12.3420(9.6639) | Error 0.3044(0.3090) Steps 664(646.47) | Grad Norm 6.1809(5.5585) | Total Time 0.00(0.00)\n",
      "Iter 1790 | Time 61.9292(61.5130) | Bit/dim 3.6675(3.6696) | Xent 0.8866(0.8647) | Loss 9.2520(9.6516) | Error 0.3166(0.3092) Steps 646(646.45) | Grad Norm 7.6611(5.6216) | Total Time 0.00(0.00)\n",
      "Iter 1791 | Time 61.2284(61.5044) | Bit/dim 3.6634(3.6694) | Xent 0.8479(0.8642) | Loss 9.2383(9.6392) | Error 0.3099(0.3092) Steps 634(646.08) | Grad Norm 2.5411(5.5292) | Total Time 0.00(0.00)\n",
      "Iter 1792 | Time 64.9606(61.6081) | Bit/dim 3.6606(3.6691) | Xent 0.8509(0.8638) | Loss 9.2716(9.6281) | Error 0.3067(0.3092) Steps 628(645.54) | Grad Norm 4.5862(5.5009) | Total Time 0.00(0.00)\n",
      "Iter 1793 | Time 65.8621(61.7357) | Bit/dim 3.6592(3.6688) | Xent 0.8562(0.8636) | Loss 9.2563(9.6170) | Error 0.3026(0.3090) Steps 640(645.37) | Grad Norm 4.2196(5.4625) | Total Time 0.00(0.00)\n",
      "Iter 1794 | Time 63.4658(61.7876) | Bit/dim 3.6669(3.6688) | Xent 0.8808(0.8641) | Loss 8.9714(9.5976) | Error 0.3125(0.3091) Steps 646(645.39) | Grad Norm 4.6138(5.4370) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0299 | Time 23.6852, Epoch Time 418.0160(409.4916), Bit/dim 3.6647(best: 3.6658), Xent 0.8801, Loss 4.1047, Error 0.3097(best: 0.3083)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1795 | Time 63.2941(61.8328) | Bit/dim 3.6750(3.6690) | Xent 0.8467(0.8636) | Loss 12.5953(9.6875) | Error 0.3010(0.3088) Steps 670(646.13) | Grad Norm 5.8047(5.4480) | Total Time 0.00(0.00)\n",
      "Iter 1796 | Time 56.7352(61.6799) | Bit/dim 3.6808(3.6693) | Xent 0.8519(0.8632) | Loss 9.0726(9.6691) | Error 0.3045(0.3087) Steps 676(647.02) | Grad Norm 7.6715(5.5147) | Total Time 0.00(0.00)\n",
      "Iter 1797 | Time 62.9418(61.7178) | Bit/dim 3.6504(3.6688) | Xent 0.8318(0.8623) | Loss 9.1390(9.6532) | Error 0.2976(0.3084) Steps 652(647.17) | Grad Norm 4.2764(5.4776) | Total Time 0.00(0.00)\n",
      "Iter 1798 | Time 58.4346(61.6193) | Bit/dim 3.6599(3.6685) | Xent 0.8465(0.8618) | Loss 9.1124(9.6370) | Error 0.2989(0.3081) Steps 634(646.78) | Grad Norm 5.8861(5.4898) | Total Time 0.00(0.00)\n",
      "Iter 1799 | Time 61.5845(61.6182) | Bit/dim 3.6711(3.6686) | Xent 0.8640(0.8619) | Loss 8.9357(9.6159) | Error 0.3094(0.3081) Steps 670(647.48) | Grad Norm 7.1737(5.5404) | Total Time 0.00(0.00)\n",
      "Iter 1800 | Time 64.6911(61.7104) | Bit/dim 3.6690(3.6686) | Xent 0.8736(0.8622) | Loss 9.1210(9.6011) | Error 0.3033(0.3080) Steps 634(647.07) | Grad Norm 3.9937(5.4940) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0300 | Time 23.0119, Epoch Time 407.2752(409.4251), Bit/dim 3.6643(best: 3.6647), Xent 0.8827, Loss 4.1056, Error 0.3116(best: 0.3083)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1801 | Time 63.7600(61.7719) | Bit/dim 3.6599(3.6683) | Xent 0.8511(0.8619) | Loss 12.4567(9.6868) | Error 0.3001(0.3077) Steps 664(647.58) | Grad Norm 6.0621(5.5110) | Total Time 0.00(0.00)\n",
      "Iter 1802 | Time 62.3801(61.7902) | Bit/dim 3.6572(3.6680) | Xent 0.8545(0.8617) | Loss 9.1548(9.6708) | Error 0.2995(0.3075) Steps 658(647.89) | Grad Norm 4.3383(5.4758) | Total Time 0.00(0.00)\n",
      "Iter 1803 | Time 58.9343(61.7045) | Bit/dim 3.6805(3.6684) | Xent 0.8372(0.8609) | Loss 8.9627(9.6496) | Error 0.3053(0.3074) Steps 640(647.65) | Grad Norm 3.1085(5.4048) | Total Time 0.00(0.00)\n",
      "Iter 1804 | Time 60.1739(61.6586) | Bit/dim 3.6694(3.6684) | Xent 0.8524(0.8607) | Loss 9.0742(9.6323) | Error 0.3074(0.3074) Steps 670(648.33) | Grad Norm 8.0294(5.4835) | Total Time 0.00(0.00)\n",
      "Iter 1805 | Time 61.9557(61.6675) | Bit/dim 3.6704(3.6684) | Xent 0.8686(0.8609) | Loss 9.3010(9.6224) | Error 0.3110(0.3075) Steps 652(648.44) | Grad Norm 10.7587(5.6418) | Total Time 0.00(0.00)\n",
      "Iter 1806 | Time 68.4429(61.8707) | Bit/dim 3.6647(3.6683) | Xent 0.8932(0.8619) | Loss 9.3646(9.6146) | Error 0.3174(0.3078) Steps 646(648.36) | Grad Norm 9.6569(5.7622) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0301 | Time 23.1647, Epoch Time 414.7721(409.5856), Bit/dim 3.6700(best: 3.6643), Xent 0.9121, Loss 4.1260, Error 0.3228(best: 0.3083)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1807 | Time 70.9249(62.1424) | Bit/dim 3.6646(3.6682) | Xent 0.8798(0.8624) | Loss 12.5795(9.7036) | Error 0.3137(0.3080) Steps 628(647.75) | Grad Norm 9.7304(5.8813) | Total Time 0.00(0.00)\n",
      "Iter 1808 | Time 63.8990(62.1951) | Bit/dim 3.6743(3.6684) | Xent 0.9490(0.8650) | Loss 9.2917(9.6912) | Error 0.3334(0.3088) Steps 634(647.34) | Grad Norm 11.9583(6.0636) | Total Time 0.00(0.00)\n",
      "Iter 1809 | Time 64.5021(62.2643) | Bit/dim 3.6634(3.6683) | Xent 0.9359(0.8671) | Loss 8.9475(9.6689) | Error 0.3335(0.3095) Steps 628(646.76) | Grad Norm 8.8413(6.1469) | Total Time 0.00(0.00)\n",
      "Iter 1810 | Time 59.8327(62.1913) | Bit/dim 3.6639(3.6681) | Xent 0.8471(0.8665) | Loss 9.1763(9.6541) | Error 0.3051(0.3094) Steps 646(646.74) | Grad Norm 9.4488(6.2460) | Total Time 0.00(0.00)\n",
      "Iter 1811 | Time 58.6561(62.0853) | Bit/dim 3.6760(3.6684) | Xent 0.8850(0.8671) | Loss 9.2423(9.6418) | Error 0.3110(0.3094) Steps 670(647.43) | Grad Norm 9.3219(6.3383) | Total Time 0.00(0.00)\n",
      "Iter 1812 | Time 60.9300(62.0506) | Bit/dim 3.6652(3.6683) | Xent 0.8626(0.8670) | Loss 9.1260(9.6263) | Error 0.3135(0.3096) Steps 640(647.21) | Grad Norm 4.5603(6.2849) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0302 | Time 23.4206, Epoch Time 417.8202(409.8326), Bit/dim 3.6701(best: 3.6643), Xent 0.9085, Loss 4.1243, Error 0.3214(best: 0.3083)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1813 | Time 64.5283(62.1249) | Bit/dim 3.6696(3.6683) | Xent 0.8946(0.8678) | Loss 12.7624(9.7204) | Error 0.3199(0.3099) Steps 658(647.53) | Grad Norm 12.7305(6.4783) | Total Time 0.00(0.00)\n",
      "Iter 1814 | Time 66.7640(62.2641) | Bit/dim 3.6630(3.6681) | Xent 0.8933(0.8685) | Loss 9.2669(9.7068) | Error 0.3137(0.3100) Steps 676(648.39) | Grad Norm 9.9995(6.5839) | Total Time 0.00(0.00)\n",
      "Iter 1815 | Time 60.2863(62.2048) | Bit/dim 3.6632(3.6680) | Xent 0.8735(0.8687) | Loss 9.1084(9.6888) | Error 0.3045(0.3098) Steps 610(647.24) | Grad Norm 6.9116(6.5938) | Total Time 0.00(0.00)\n",
      "Iter 1816 | Time 58.5277(62.0945) | Bit/dim 3.6708(3.6681) | Xent 0.9130(0.8700) | Loss 9.1945(9.6740) | Error 0.3241(0.3102) Steps 646(647.20) | Grad Norm 13.3438(6.7963) | Total Time 0.00(0.00)\n",
      "Iter 1817 | Time 58.5907(61.9894) | Bit/dim 3.6710(3.6682) | Xent 0.8516(0.8695) | Loss 9.0342(9.6548) | Error 0.3039(0.3101) Steps 652(647.34) | Grad Norm 6.0479(6.7738) | Total Time 0.00(0.00)\n",
      "Iter 1818 | Time 58.9384(61.8978) | Bit/dim 3.6708(3.6682) | Xent 0.8642(0.8693) | Loss 9.3411(9.6454) | Error 0.3051(0.3099) Steps 664(647.84) | Grad Norm 7.5346(6.7966) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0303 | Time 23.4315, Epoch Time 406.9666(409.7466), Bit/dim 3.6701(best: 3.6643), Xent 0.9056, Loss 4.1229, Error 0.3184(best: 0.3083)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1819 | Time 62.7689(61.9240) | Bit/dim 3.6668(3.6682) | Xent 0.8997(0.8702) | Loss 12.6118(9.7344) | Error 0.3217(0.3103) Steps 634(647.43) | Grad Norm 10.4707(6.9069) | Total Time 0.00(0.00)\n",
      "Iter 1820 | Time 58.4654(61.8202) | Bit/dim 3.6711(3.6683) | Xent 0.8589(0.8699) | Loss 9.0542(9.7140) | Error 0.3020(0.3100) Steps 640(647.21) | Grad Norm 7.6413(6.9289) | Total Time 0.00(0.00)\n",
      "Iter 1821 | Time 60.1676(61.7706) | Bit/dim 3.6769(3.6686) | Xent 0.8542(0.8694) | Loss 9.2232(9.6992) | Error 0.3045(0.3098) Steps 664(647.71) | Grad Norm 9.1855(6.9966) | Total Time 0.00(0.00)\n",
      "Iter 1822 | Time 62.9844(61.8070) | Bit/dim 3.6620(3.6684) | Xent 0.8736(0.8695) | Loss 9.2417(9.6855) | Error 0.3163(0.3100) Steps 670(648.38) | Grad Norm 5.2399(6.9439) | Total Time 0.00(0.00)\n",
      "Iter 1823 | Time 60.4395(61.7660) | Bit/dim 3.6698(3.6684) | Xent 0.8909(0.8702) | Loss 9.3017(9.6740) | Error 0.3260(0.3105) Steps 652(648.49) | Grad Norm 7.4977(6.9605) | Total Time 0.00(0.00)\n",
      "Iter 1824 | Time 58.7075(61.6743) | Bit/dim 3.6662(3.6683) | Xent 0.8495(0.8696) | Loss 9.2371(9.6609) | Error 0.3025(0.3103) Steps 628(647.87) | Grad Norm 4.0218(6.8723) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0304 | Time 22.7658, Epoch Time 402.5965(409.5321), Bit/dim 3.6728(best: 3.6643), Xent 0.8826, Loss 4.1142, Error 0.3108(best: 0.3083)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1825 | Time 60.3676(61.6351) | Bit/dim 3.6609(3.6681) | Xent 0.8450(0.8688) | Loss 12.4110(9.7434) | Error 0.2987(0.3099) Steps 628(647.28) | Grad Norm 4.6329(6.8052) | Total Time 0.00(0.00)\n",
      "Iter 1826 | Time 54.9978(61.4359) | Bit/dim 3.6769(3.6684) | Xent 0.8389(0.8679) | Loss 9.1672(9.7261) | Error 0.3033(0.3097) Steps 628(646.70) | Grad Norm 3.0989(6.6940) | Total Time 0.00(0.00)\n",
      "Iter 1827 | Time 65.0080(61.5431) | Bit/dim 3.6747(3.6686) | Xent 0.8638(0.8678) | Loss 8.9248(9.7021) | Error 0.3109(0.3098) Steps 640(646.50) | Grad Norm 4.8771(6.6395) | Total Time 0.00(0.00)\n",
      "Iter 1828 | Time 57.3170(61.4163) | Bit/dim 3.6621(3.6684) | Xent 0.8389(0.8669) | Loss 9.0802(9.6834) | Error 0.3044(0.3096) Steps 634(646.12) | Grad Norm 6.6260(6.6391) | Total Time 0.00(0.00)\n",
      "Iter 1829 | Time 56.5040(61.2689) | Bit/dim 3.6796(3.6687) | Xent 0.8696(0.8670) | Loss 8.9926(9.6627) | Error 0.3096(0.3096) Steps 628(645.58) | Grad Norm 9.4026(6.7220) | Total Time 0.00(0.00)\n",
      "Iter 1830 | Time 55.5088(61.0961) | Bit/dim 3.6613(3.6685) | Xent 0.8419(0.8663) | Loss 9.1668(9.6478) | Error 0.3015(0.3094) Steps 652(645.77) | Grad Norm 2.6374(6.5994) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0305 | Time 23.4900, Epoch Time 389.0025(408.9162), Bit/dim 3.6752(best: 3.6643), Xent 0.8762, Loss 4.1133, Error 0.3100(best: 0.3083)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1831 | Time 56.7263(60.9650) | Bit/dim 3.6735(3.6686) | Xent 0.8526(0.8659) | Loss 12.4772(9.7327) | Error 0.3107(0.3094) Steps 670(646.50) | Grad Norm 8.5036(6.6566) | Total Time 0.00(0.00)\n",
      "Iter 1832 | Time 57.1079(60.8493) | Bit/dim 3.6661(3.6686) | Xent 0.8655(0.8658) | Loss 9.0069(9.7109) | Error 0.3117(0.3095) Steps 652(646.66) | Grad Norm 5.7207(6.6285) | Total Time 0.00(0.00)\n",
      "Iter 1833 | Time 60.4828(60.8383) | Bit/dim 3.6668(3.6685) | Xent 0.8621(0.8657) | Loss 9.0958(9.6925) | Error 0.3014(0.3092) Steps 658(647.00) | Grad Norm 5.1463(6.5840) | Total Time 0.00(0.00)\n",
      "Iter 1834 | Time 65.1440(60.9675) | Bit/dim 3.6725(3.6686) | Xent 0.8454(0.8651) | Loss 9.0526(9.6733) | Error 0.3014(0.3090) Steps 616(646.07) | Grad Norm 4.8686(6.5325) | Total Time 0.00(0.00)\n",
      "Iter 1835 | Time 56.9934(60.8483) | Bit/dim 3.6607(3.6684) | Xent 0.8513(0.8647) | Loss 9.1115(9.6564) | Error 0.3020(0.3088) Steps 628(645.53) | Grad Norm 6.4962(6.5315) | Total Time 0.00(0.00)\n",
      "Iter 1836 | Time 61.5042(60.8680) | Bit/dim 3.6635(3.6682) | Xent 0.8599(0.8646) | Loss 9.0692(9.6388) | Error 0.3100(0.3088) Steps 646(645.54) | Grad Norm 6.3831(6.5270) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0306 | Time 22.8520, Epoch Time 396.8625(408.5546), Bit/dim 3.6752(best: 3.6643), Xent 0.8801, Loss 4.1153, Error 0.3100(best: 0.3083)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1837 | Time 54.2888(60.6706) | Bit/dim 3.6696(3.6683) | Xent 0.8467(0.8640) | Loss 12.1490(9.7141) | Error 0.3014(0.3086) Steps 628(645.02) | Grad Norm 5.3685(6.4923) | Total Time 0.00(0.00)\n",
      "Iter 1838 | Time 57.5024(60.5755) | Bit/dim 3.6672(3.6682) | Xent 0.8308(0.8630) | Loss 9.0611(9.6945) | Error 0.2951(0.3082) Steps 652(645.23) | Grad Norm 5.6649(6.4674) | Total Time 0.00(0.00)\n",
      "Iter 1839 | Time 67.2844(60.7768) | Bit/dim 3.6720(3.6684) | Xent 0.8475(0.8626) | Loss 9.3194(9.6833) | Error 0.3091(0.3082) Steps 712(647.23) | Grad Norm 9.0444(6.5447) | Total Time 0.00(0.00)\n",
      "Iter 1840 | Time 57.9016(60.6905) | Bit/dim 3.6771(3.6686) | Xent 0.9061(0.8639) | Loss 9.1089(9.6660) | Error 0.3201(0.3086) Steps 628(646.65) | Grad Norm 9.9408(6.6466) | Total Time 0.00(0.00)\n",
      "Iter 1841 | Time 59.0681(60.6419) | Bit/dim 3.6755(3.6688) | Xent 0.8449(0.8633) | Loss 9.0743(9.6483) | Error 0.3036(0.3084) Steps 640(646.45) | Grad Norm 6.6345(6.6463) | Total Time 0.00(0.00)\n",
      "Iter 1842 | Time 61.7671(60.6756) | Bit/dim 3.6619(3.6686) | Xent 0.8759(0.8637) | Loss 9.1460(9.6332) | Error 0.3109(0.3085) Steps 640(646.26) | Grad Norm 6.4487(6.6403) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0307 | Time 23.6248, Epoch Time 397.7514(408.2305), Bit/dim 3.6719(best: 3.6643), Xent 0.9091, Loss 4.1264, Error 0.3181(best: 0.3083)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1843 | Time 56.1672(60.5404) | Bit/dim 3.6668(3.6686) | Xent 0.9031(0.8649) | Loss 12.4489(9.7177) | Error 0.3221(0.3089) Steps 640(646.07) | Grad Norm 7.8455(6.6765) | Total Time 0.00(0.00)\n",
      "Iter 1844 | Time 62.2057(60.5903) | Bit/dim 3.6615(3.6684) | Xent 0.8576(0.8646) | Loss 9.2192(9.7027) | Error 0.3043(0.3088) Steps 646(646.07) | Grad Norm 6.2465(6.6636) | Total Time 0.00(0.00)\n",
      "Iter 1845 | Time 61.2363(60.6097) | Bit/dim 3.6662(3.6683) | Xent 0.8869(0.8653) | Loss 9.2773(9.6900) | Error 0.3145(0.3089) Steps 658(646.43) | Grad Norm 8.3266(6.7135) | Total Time 0.00(0.00)\n",
      "Iter 1846 | Time 66.2039(60.7775) | Bit/dim 3.6513(3.6678) | Xent 0.8388(0.8645) | Loss 9.1321(9.6732) | Error 0.3015(0.3087) Steps 622(645.70) | Grad Norm 4.8578(6.6578) | Total Time 0.00(0.00)\n",
      "Iter 1847 | Time 63.8929(60.8710) | Bit/dim 3.6730(3.6679) | Xent 0.8678(0.8646) | Loss 9.2334(9.6600) | Error 0.3115(0.3088) Steps 652(645.88) | Grad Norm 12.5173(6.8336) | Total Time 0.00(0.00)\n",
      "Iter 1848 | Time 60.2582(60.8526) | Bit/dim 3.6788(3.6683) | Xent 0.8353(0.8637) | Loss 9.2547(9.6479) | Error 0.2971(0.3085) Steps 604(644.63) | Grad Norm 7.2812(6.8470) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0308 | Time 22.3409, Epoch Time 408.2740(408.2318), Bit/dim 3.6665(best: 3.6643), Xent 0.8963, Loss 4.1146, Error 0.3154(best: 0.3083)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1849 | Time 57.8906(60.7638) | Bit/dim 3.6608(3.6680) | Xent 0.8658(0.8638) | Loss 12.7191(9.7400) | Error 0.3119(0.3086) Steps 652(644.85) | Grad Norm 11.2548(6.9793) | Total Time 0.00(0.00)\n",
      "Iter 1850 | Time 59.1458(60.7152) | Bit/dim 3.6767(3.6683) | Xent 0.9079(0.8651) | Loss 9.0864(9.7204) | Error 0.3255(0.3091) Steps 652(645.06) | Grad Norm 12.4149(7.1423) | Total Time 0.00(0.00)\n",
      "Iter 1851 | Time 57.4589(60.6175) | Bit/dim 3.6662(3.6682) | Xent 0.8722(0.8653) | Loss 9.0997(9.7018) | Error 0.3100(0.3091) Steps 628(644.55) | Grad Norm 7.4936(7.1529) | Total Time 0.00(0.00)\n",
      "Iter 1852 | Time 56.8612(60.5048) | Bit/dim 3.6748(3.6684) | Xent 0.8911(0.8661) | Loss 9.1003(9.6837) | Error 0.3176(0.3093) Steps 634(644.24) | Grad Norm 11.4158(7.2807) | Total Time 0.00(0.00)\n",
      "Iter 1853 | Time 64.1819(60.6152) | Bit/dim 3.6836(3.6689) | Xent 0.8712(0.8663) | Loss 9.2138(9.6696) | Error 0.3084(0.3093) Steps 670(645.01) | Grad Norm 7.1241(7.2760) | Total Time 0.00(0.00)\n",
      "Iter 1854 | Time 58.9225(60.5644) | Bit/dim 3.6623(3.6687) | Xent 0.9133(0.8677) | Loss 9.3754(9.6608) | Error 0.3326(0.3100) Steps 658(645.40) | Grad Norm 11.4190(7.4003) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0309 | Time 23.5224, Epoch Time 394.0700(407.8070), Bit/dim 3.6758(best: 3.6643), Xent 0.9287, Loss 4.1402, Error 0.3249(best: 0.3083)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1855 | Time 57.4468(60.4708) | Bit/dim 3.6764(3.6689) | Xent 0.8912(0.8684) | Loss 12.4475(9.7444) | Error 0.3200(0.3103) Steps 652(645.60) | Grad Norm 9.9810(7.4778) | Total Time 0.00(0.00)\n",
      "Iter 1856 | Time 63.0317(60.5477) | Bit/dim 3.6738(3.6691) | Xent 0.8800(0.8687) | Loss 9.1881(9.7277) | Error 0.3165(0.3105) Steps 652(645.79) | Grad Norm 8.1733(7.4986) | Total Time 0.00(0.00)\n",
      "Iter 1857 | Time 58.1705(60.4764) | Bit/dim 3.6841(3.6695) | Xent 0.8898(0.8694) | Loss 8.9383(9.7040) | Error 0.3077(0.3104) Steps 616(644.89) | Grad Norm 9.8525(7.5692) | Total Time 0.00(0.00)\n",
      "Iter 1858 | Time 60.3203(60.4717) | Bit/dim 3.6671(3.6694) | Xent 0.8580(0.8690) | Loss 9.2259(9.6897) | Error 0.3065(0.3103) Steps 658(645.29) | Grad Norm 6.6464(7.5416) | Total Time 0.00(0.00)\n",
      "Iter 1859 | Time 62.7413(60.5398) | Bit/dim 3.6758(3.6696) | Xent 0.8976(0.8699) | Loss 9.2749(9.6773) | Error 0.3196(0.3106) Steps 676(646.21) | Grad Norm 10.0755(7.6176) | Total Time 0.00(0.00)\n",
      "Iter 1860 | Time 60.4246(60.5363) | Bit/dim 3.6684(3.6696) | Xent 0.8844(0.8703) | Loss 9.1720(9.6621) | Error 0.3196(0.3109) Steps 628(645.66) | Grad Norm 8.4068(7.6412) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0310 | Time 22.8914, Epoch Time 401.4097(407.6150), Bit/dim 3.6713(best: 3.6643), Xent 0.8995, Loss 4.1210, Error 0.3206(best: 0.3083)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1861 | Time 61.5825(60.5677) | Bit/dim 3.6679(3.6695) | Xent 0.8787(0.8706) | Loss 12.5547(9.7489) | Error 0.3084(0.3108) Steps 694(647.11) | Grad Norm 8.7606(7.6748) | Total Time 0.00(0.00)\n",
      "Iter 1862 | Time 60.3968(60.5626) | Bit/dim 3.6880(3.6701) | Xent 0.8623(0.8703) | Loss 9.2796(9.7348) | Error 0.3066(0.3107) Steps 658(647.44) | Grad Norm 5.1985(7.6005) | Total Time 0.00(0.00)\n",
      "Iter 1863 | Time 61.8160(60.6002) | Bit/dim 3.6717(3.6701) | Xent 0.8703(0.8703) | Loss 9.3036(9.7219) | Error 0.3163(0.3108) Steps 658(647.76) | Grad Norm 9.5039(7.6576) | Total Time 0.00(0.00)\n",
      "Iter 1864 | Time 61.3627(60.6230) | Bit/dim 3.6740(3.6703) | Xent 0.8538(0.8698) | Loss 9.0227(9.7009) | Error 0.3094(0.3108) Steps 634(647.34) | Grad Norm 5.8901(7.6046) | Total Time 0.00(0.00)\n",
      "Iter 1865 | Time 59.8869(60.6010) | Bit/dim 3.6729(3.6703) | Xent 0.8725(0.8699) | Loss 9.0183(9.6804) | Error 0.3136(0.3109) Steps 676(648.20) | Grad Norm 8.8720(7.6426) | Total Time 0.00(0.00)\n",
      "Iter 1866 | Time 61.3150(60.6224) | Bit/dim 3.6776(3.6706) | Xent 0.8589(0.8696) | Loss 9.2123(9.6664) | Error 0.3077(0.3108) Steps 604(646.88) | Grad Norm 7.4406(7.6366) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0311 | Time 22.8423, Epoch Time 405.3304(407.5465), Bit/dim 3.6767(best: 3.6643), Xent 0.8803, Loss 4.1169, Error 0.3103(best: 0.3083)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1867 | Time 64.5364(60.7398) | Bit/dim 3.6847(3.6710) | Xent 0.8556(0.8691) | Loss 12.3979(9.7483) | Error 0.3033(0.3105) Steps 622(646.13) | Grad Norm 6.2836(7.5960) | Total Time 0.00(0.00)\n",
      "Iter 1868 | Time 64.3882(60.8493) | Bit/dim 3.6621(3.6707) | Xent 0.8456(0.8684) | Loss 9.0659(9.7278) | Error 0.3010(0.3103) Steps 670(646.85) | Grad Norm 6.6410(7.5673) | Total Time 0.00(0.00)\n",
      "Iter 1869 | Time 58.6173(60.7823) | Bit/dim 3.6721(3.6708) | Xent 0.8374(0.8675) | Loss 9.2295(9.7129) | Error 0.3019(0.3100) Steps 640(646.64) | Grad Norm 5.1874(7.4959) | Total Time 0.00(0.00)\n",
      "Iter 1870 | Time 61.1454(60.7932) | Bit/dim 3.6679(3.6707) | Xent 0.8694(0.8676) | Loss 9.3255(9.7013) | Error 0.3086(0.3100) Steps 622(645.90) | Grad Norm 4.9305(7.4190) | Total Time 0.00(0.00)\n",
      "Iter 1871 | Time 61.2356(60.8065) | Bit/dim 3.6715(3.6707) | Xent 0.8345(0.8666) | Loss 9.2295(9.6871) | Error 0.2945(0.3095) Steps 658(646.27) | Grad Norm 3.9459(7.3148) | Total Time 0.00(0.00)\n",
      "Iter 1872 | Time 59.1130(60.7557) | Bit/dim 3.6666(3.6706) | Xent 0.8317(0.8655) | Loss 9.1504(9.6710) | Error 0.2959(0.3091) Steps 664(646.80) | Grad Norm 3.3262(7.1951) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0312 | Time 23.9345, Epoch Time 409.2240(407.5968), Bit/dim 3.6680(best: 3.6643), Xent 0.8802, Loss 4.1081, Error 0.3110(best: 0.3083)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1873 | Time 59.8004(60.7270) | Bit/dim 3.6705(3.6706) | Xent 0.8421(0.8648) | Loss 12.6021(9.7589) | Error 0.3007(0.3088) Steps 670(647.49) | Grad Norm 4.5618(7.1161) | Total Time 0.00(0.00)\n",
      "Iter 1874 | Time 58.0718(60.6473) | Bit/dim 3.6685(3.6705) | Xent 0.8351(0.8639) | Loss 9.1251(9.7399) | Error 0.2994(0.3086) Steps 664(647.99) | Grad Norm 4.3718(7.0338) | Total Time 0.00(0.00)\n",
      "Iter 1875 | Time 61.7008(60.6790) | Bit/dim 3.6729(3.6706) | Xent 0.8364(0.8631) | Loss 8.9769(9.7170) | Error 0.3003(0.3083) Steps 640(647.75) | Grad Norm 2.9446(6.9111) | Total Time 0.00(0.00)\n",
      "Iter 1876 | Time 57.0743(60.5708) | Bit/dim 3.6680(3.6705) | Xent 0.8707(0.8633) | Loss 8.9678(9.6946) | Error 0.3113(0.3084) Steps 634(647.34) | Grad Norm 5.6450(6.8731) | Total Time 0.00(0.00)\n",
      "Iter 1877 | Time 59.1249(60.5274) | Bit/dim 3.6637(3.6703) | Xent 0.8336(0.8624) | Loss 9.0474(9.6751) | Error 0.2964(0.3080) Steps 640(647.12) | Grad Norm 1.2229(6.7036) | Total Time 0.00(0.00)\n",
      "Iter 1878 | Time 62.5495(60.5881) | Bit/dim 3.6591(3.6700) | Xent 0.8266(0.8614) | Loss 9.0462(9.6563) | Error 0.2957(0.3077) Steps 598(645.64) | Grad Norm 5.0797(6.6549) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0313 | Time 22.9019, Epoch Time 397.2149(407.2854), Bit/dim 3.6679(best: 3.6643), Xent 0.8709, Loss 4.1033, Error 0.3066(best: 0.3083)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1879 | Time 57.1616(60.4853) | Bit/dim 3.6611(3.6697) | Xent 0.8473(0.8609) | Loss 12.4668(9.7406) | Error 0.3019(0.3075) Steps 634(645.29) | Grad Norm 3.6901(6.5660) | Total Time 0.00(0.00)\n",
      "Iter 1880 | Time 63.8948(60.5876) | Bit/dim 3.6641(3.6695) | Xent 0.8287(0.8600) | Loss 9.0646(9.7203) | Error 0.2919(0.3070) Steps 646(645.31) | Grad Norm 5.8897(6.5457) | Total Time 0.00(0.00)\n",
      "Iter 1881 | Time 57.6633(60.4999) | Bit/dim 3.6660(3.6694) | Xent 0.8417(0.8594) | Loss 9.1723(9.7039) | Error 0.2999(0.3068) Steps 658(645.70) | Grad Norm 4.8288(6.4942) | Total Time 0.00(0.00)\n",
      "Iter 1882 | Time 55.7103(60.3562) | Bit/dim 3.6731(3.6695) | Xent 0.8399(0.8588) | Loss 9.1159(9.6862) | Error 0.2991(0.3066) Steps 628(645.16) | Grad Norm 3.7793(6.4127) | Total Time 0.00(0.00)\n",
      "Iter 1883 | Time 61.6151(60.3939) | Bit/dim 3.6626(3.6693) | Xent 0.8351(0.8581) | Loss 9.0830(9.6681) | Error 0.3016(0.3064) Steps 622(644.47) | Grad Norm 5.2121(6.3767) | Total Time 0.00(0.00)\n",
      "Iter 1884 | Time 62.8112(60.4665) | Bit/dim 3.6734(3.6695) | Xent 0.8478(0.8578) | Loss 9.0023(9.6482) | Error 0.3015(0.3063) Steps 682(645.60) | Grad Norm 4.4625(6.3193) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0314 | Time 23.0213, Epoch Time 398.4505(407.0203), Bit/dim 3.6678(best: 3.6643), Xent 0.8658, Loss 4.1007, Error 0.3056(best: 0.3066)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1885 | Time 58.9930(60.4223) | Bit/dim 3.6591(3.6691) | Xent 0.8402(0.8573) | Loss 12.0322(9.7197) | Error 0.2966(0.3060) Steps 640(645.43) | Grad Norm 3.6410(6.2389) | Total Time 0.00(0.00)\n",
      "Iter 1886 | Time 57.3255(60.3294) | Bit/dim 3.6557(3.6687) | Xent 0.8597(0.8574) | Loss 9.0212(9.6987) | Error 0.3116(0.3062) Steps 640(645.26) | Grad Norm 3.4578(6.1555) | Total Time 0.00(0.00)\n",
      "Iter 1887 | Time 60.6974(60.3404) | Bit/dim 3.6577(3.6684) | Xent 0.8321(0.8566) | Loss 9.0574(9.6795) | Error 0.2984(0.3059) Steps 652(645.47) | Grad Norm 4.1353(6.0949) | Total Time 0.00(0.00)\n",
      "Iter 1888 | Time 63.3475(60.4306) | Bit/dim 3.6684(3.6684) | Xent 0.8360(0.8560) | Loss 9.1106(9.6624) | Error 0.2963(0.3056) Steps 628(644.94) | Grad Norm 2.4784(5.9864) | Total Time 0.00(0.00)\n",
      "Iter 1889 | Time 60.5976(60.4356) | Bit/dim 3.6654(3.6683) | Xent 0.8425(0.8556) | Loss 9.2133(9.6490) | Error 0.3046(0.3056) Steps 646(644.97) | Grad Norm 5.3910(5.9685) | Total Time 0.00(0.00)\n",
      "Iter 1890 | Time 56.9038(60.3297) | Bit/dim 3.6675(3.6683) | Xent 0.8264(0.8547) | Loss 8.9450(9.6278) | Error 0.2971(0.3054) Steps 634(644.65) | Grad Norm 4.4742(5.9237) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0315 | Time 22.7151, Epoch Time 397.1191(406.7233), Bit/dim 3.6644(best: 3.6643), Xent 0.8658, Loss 4.0973, Error 0.3059(best: 0.3056)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1891 | Time 58.7223(60.2814) | Bit/dim 3.6606(3.6681) | Xent 0.8229(0.8538) | Loss 12.4676(9.7130) | Error 0.2985(0.3051) Steps 634(644.33) | Grad Norm 3.4010(5.8480) | Total Time 0.00(0.00)\n",
      "Iter 1892 | Time 64.5787(60.4104) | Bit/dim 3.6690(3.6681) | Xent 0.8465(0.8535) | Loss 9.1853(9.6972) | Error 0.3019(0.3051) Steps 640(644.20) | Grad Norm 3.8924(5.7894) | Total Time 0.00(0.00)\n",
      "Iter 1893 | Time 62.1763(60.4633) | Bit/dim 3.6614(3.6679) | Xent 0.8367(0.8530) | Loss 9.0998(9.6793) | Error 0.2943(0.3047) Steps 670(644.97) | Grad Norm 4.3268(5.7455) | Total Time 0.00(0.00)\n",
      "Iter 1894 | Time 60.3220(60.4591) | Bit/dim 3.6664(3.6678) | Xent 0.8180(0.8520) | Loss 9.2363(9.6660) | Error 0.2950(0.3044) Steps 670(645.72) | Grad Norm 4.8582(5.7189) | Total Time 0.00(0.00)\n",
      "Iter 1895 | Time 61.8810(60.5018) | Bit/dim 3.6562(3.6675) | Xent 0.8400(0.8516) | Loss 8.9375(9.6441) | Error 0.3025(0.3044) Steps 670(646.45) | Grad Norm 5.3655(5.7083) | Total Time 0.00(0.00)\n",
      "Iter 1896 | Time 64.3053(60.6159) | Bit/dim 3.6661(3.6675) | Xent 0.8476(0.8515) | Loss 9.1711(9.6299) | Error 0.3026(0.3043) Steps 646(646.44) | Grad Norm 5.2420(5.6943) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0316 | Time 22.9906, Epoch Time 410.6505(406.8411), Bit/dim 3.6613(best: 3.6643), Xent 0.8684, Loss 4.0955, Error 0.3059(best: 0.3056)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1897 | Time 61.3350(60.6374) | Bit/dim 3.6660(3.6674) | Xent 0.8402(0.8512) | Loss 12.3661(9.7120) | Error 0.3039(0.3043) Steps 640(646.24) | Grad Norm 4.8961(5.6703) | Total Time 0.00(0.00)\n",
      "Iter 1898 | Time 62.7360(60.7004) | Bit/dim 3.6538(3.6670) | Xent 0.8397(0.8508) | Loss 9.0005(9.6907) | Error 0.3029(0.3043) Steps 646(646.24) | Grad Norm 3.0814(5.5927) | Total Time 0.00(0.00)\n",
      "Iter 1899 | Time 59.8141(60.6738) | Bit/dim 3.6619(3.6668) | Xent 0.8156(0.8498) | Loss 9.0789(9.6723) | Error 0.2875(0.3038) Steps 682(647.31) | Grad Norm 4.3700(5.5560) | Total Time 0.00(0.00)\n",
      "Iter 1900 | Time 63.4271(60.7564) | Bit/dim 3.6629(3.6667) | Xent 0.8533(0.8499) | Loss 9.1885(9.6578) | Error 0.3030(0.3037) Steps 616(646.37) | Grad Norm 5.7994(5.5633) | Total Time 0.00(0.00)\n",
      "Iter 1901 | Time 63.7395(60.8459) | Bit/dim 3.6736(3.6669) | Xent 0.8515(0.8499) | Loss 9.1386(9.6422) | Error 0.3057(0.3038) Steps 652(646.54) | Grad Norm 5.5452(5.5627) | Total Time 0.00(0.00)\n",
      "Iter 1902 | Time 60.5620(60.8374) | Bit/dim 3.6648(3.6669) | Xent 0.8681(0.8505) | Loss 9.0338(9.6240) | Error 0.3096(0.3040) Steps 646(646.52) | Grad Norm 8.4368(5.6490) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0317 | Time 23.0625, Epoch Time 410.6442(406.9552), Bit/dim 3.6742(best: 3.6613), Xent 0.8682, Loss 4.1084, Error 0.3061(best: 0.3056)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1903 | Time 62.3437(60.8826) | Bit/dim 3.6616(3.6667) | Xent 0.8360(0.8500) | Loss 12.7085(9.7165) | Error 0.2981(0.3038) Steps 628(645.97) | Grad Norm 6.7698(5.6826) | Total Time 0.00(0.00)\n",
      "Iter 1904 | Time 62.4452(60.9294) | Bit/dim 3.6563(3.6664) | Xent 0.8220(0.8492) | Loss 9.0918(9.6978) | Error 0.2917(0.3034) Steps 634(645.61) | Grad Norm 2.2894(5.5808) | Total Time 0.00(0.00)\n",
      "Iter 1905 | Time 63.9781(61.0209) | Bit/dim 3.6705(3.6665) | Xent 0.8444(0.8490) | Loss 9.1750(9.6821) | Error 0.2991(0.3033) Steps 670(646.34) | Grad Norm 6.0101(5.5937) | Total Time 0.00(0.00)\n",
      "Iter 1906 | Time 60.4277(61.0031) | Bit/dim 3.6641(3.6665) | Xent 0.8314(0.8485) | Loss 9.1626(9.6665) | Error 0.2987(0.3032) Steps 634(645.97) | Grad Norm 2.5968(5.5038) | Total Time 0.00(0.00)\n",
      "Iter 1907 | Time 59.2148(60.9495) | Bit/dim 3.6727(3.6666) | Xent 0.8286(0.8479) | Loss 9.1514(9.6511) | Error 0.2946(0.3029) Steps 646(645.97) | Grad Norm 4.1374(5.4628) | Total Time 0.00(0.00)\n",
      "Iter 1908 | Time 62.0462(60.9824) | Bit/dim 3.6555(3.6663) | Xent 0.8155(0.8469) | Loss 9.2319(9.6385) | Error 0.2855(0.3024) Steps 640(645.79) | Grad Norm 2.3820(5.3704) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0318 | Time 23.0419, Epoch Time 409.5306(407.0325), Bit/dim 3.6645(best: 3.6613), Xent 0.8615, Loss 4.0952, Error 0.3045(best: 0.3056)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1909 | Time 65.7163(61.1244) | Bit/dim 3.6514(3.6659) | Xent 0.8266(0.8463) | Loss 12.3363(9.7194) | Error 0.2959(0.3022) Steps 646(645.80) | Grad Norm 4.8514(5.3548) | Total Time 0.00(0.00)\n",
      "Iter 1910 | Time 58.8552(61.0563) | Bit/dim 3.6742(3.6661) | Xent 0.8391(0.8461) | Loss 9.1905(9.7035) | Error 0.3030(0.3022) Steps 640(645.62) | Grad Norm 8.2584(5.4419) | Total Time 0.00(0.00)\n",
      "Iter 1911 | Time 59.4171(61.0071) | Bit/dim 3.6558(3.6658) | Xent 0.8598(0.8465) | Loss 9.0532(9.6840) | Error 0.3074(0.3024) Steps 616(644.73) | Grad Norm 8.9680(5.5477) | Total Time 0.00(0.00)\n",
      "Iter 1912 | Time 66.6142(61.1753) | Bit/dim 3.6591(3.6656) | Xent 0.8722(0.8473) | Loss 9.0769(9.6658) | Error 0.3119(0.3027) Steps 610(643.69) | Grad Norm 9.2224(5.6579) | Total Time 0.00(0.00)\n",
      "Iter 1913 | Time 60.7299(61.1620) | Bit/dim 3.6592(3.6654) | Xent 0.8666(0.8479) | Loss 9.1141(9.6493) | Error 0.3119(0.3029) Steps 634(643.40) | Grad Norm 8.3871(5.7398) | Total Time 0.00(0.00)\n",
      "Iter 1914 | Time 58.2459(61.0745) | Bit/dim 3.6707(3.6656) | Xent 0.8217(0.8471) | Loss 9.1131(9.6332) | Error 0.2934(0.3027) Steps 658(643.84) | Grad Norm 5.0551(5.7192) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0319 | Time 23.1420, Epoch Time 409.0519(407.0930), Bit/dim 3.6717(best: 3.6613), Xent 0.8823, Loss 4.1128, Error 0.3102(best: 0.3045)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1915 | Time 59.0993(61.0152) | Bit/dim 3.6593(3.6654) | Xent 0.8553(0.8473) | Loss 13.0230(9.7349) | Error 0.3134(0.3030) Steps 652(644.08) | Grad Norm 5.9633(5.7266) | Total Time 0.00(0.00)\n",
      "Iter 1916 | Time 63.1698(61.0799) | Bit/dim 3.6668(3.6654) | Xent 0.8632(0.8478) | Loss 9.2265(9.7196) | Error 0.3085(0.3031) Steps 634(643.78) | Grad Norm 8.7819(5.8182) | Total Time 0.00(0.00)\n",
      "Iter 1917 | Time 63.7309(61.1594) | Bit/dim 3.6526(3.6650) | Xent 0.8176(0.8469) | Loss 8.9844(9.6976) | Error 0.2947(0.3029) Steps 682(644.93) | Grad Norm 6.1782(5.8290) | Total Time 0.00(0.00)\n",
      "Iter 1918 | Time 61.4077(61.1669) | Bit/dim 3.6796(3.6655) | Xent 0.8623(0.8474) | Loss 9.1284(9.6805) | Error 0.3100(0.3031) Steps 646(644.96) | Grad Norm 6.0877(5.8368) | Total Time 0.00(0.00)\n",
      "Iter 1919 | Time 58.3080(61.0811) | Bit/dim 3.6603(3.6653) | Xent 0.8707(0.8481) | Loss 9.0883(9.6627) | Error 0.3105(0.3033) Steps 664(645.53) | Grad Norm 10.2287(5.9685) | Total Time 0.00(0.00)\n",
      "Iter 1920 | Time 65.9158(61.2261) | Bit/dim 3.6697(3.6654) | Xent 0.8389(0.8478) | Loss 9.1529(9.6474) | Error 0.2997(0.3032) Steps 616(644.65) | Grad Norm 7.5169(6.0150) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0320 | Time 23.1427, Epoch Time 411.0178(407.2108), Bit/dim 3.6650(best: 3.6613), Xent 0.8678, Loss 4.0989, Error 0.3098(best: 0.3045)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1921 | Time 60.7294(61.2112) | Bit/dim 3.6604(3.6653) | Xent 0.8415(0.8476) | Loss 12.5520(9.7346) | Error 0.3029(0.3032) Steps 652(644.87) | Grad Norm 3.1262(5.9283) | Total Time 0.00(0.00)\n",
      "Iter 1922 | Time 61.4224(61.2176) | Bit/dim 3.6739(3.6656) | Xent 0.8380(0.8473) | Loss 9.2434(9.7198) | Error 0.2973(0.3030) Steps 628(644.36) | Grad Norm 5.9480(5.9289) | Total Time 0.00(0.00)\n",
      "Iter 1923 | Time 53.8972(60.9979) | Bit/dim 3.6713(3.6657) | Xent 0.8307(0.8468) | Loss 9.1736(9.7035) | Error 0.2995(0.3029) Steps 646(644.41) | Grad Norm 3.8007(5.8651) | Total Time 0.00(0.00)\n",
      "Iter 1924 | Time 61.3534(61.0086) | Bit/dim 3.6603(3.6656) | Xent 0.8368(0.8465) | Loss 8.8334(9.6774) | Error 0.2956(0.3027) Steps 646(644.46) | Grad Norm 5.0599(5.8409) | Total Time 0.00(0.00)\n",
      "Iter 1925 | Time 59.7208(60.9700) | Bit/dim 3.6583(3.6653) | Xent 0.8133(0.8455) | Loss 9.0156(9.6575) | Error 0.2951(0.3025) Steps 646(644.50) | Grad Norm 3.1647(5.7606) | Total Time 0.00(0.00)\n",
      "Iter 1926 | Time 61.7250(60.9926) | Bit/dim 3.6600(3.6652) | Xent 0.8166(0.8447) | Loss 9.1780(9.6431) | Error 0.2924(0.3022) Steps 628(644.01) | Grad Norm 2.6636(5.6677) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0321 | Time 22.6095, Epoch Time 397.4772(406.9188), Bit/dim 3.6629(best: 3.6613), Xent 0.8648, Loss 4.0953, Error 0.3067(best: 0.3045)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1927 | Time 59.0312(60.9338) | Bit/dim 3.6670(3.6652) | Xent 0.8167(0.8438) | Loss 12.1709(9.7189) | Error 0.2885(0.3018) Steps 652(644.25) | Grad Norm 3.7585(5.6104) | Total Time 0.00(0.00)\n",
      "Iter 1928 | Time 60.3722(60.9169) | Bit/dim 3.6675(3.6653) | Xent 0.8212(0.8431) | Loss 9.0629(9.6993) | Error 0.2969(0.3016) Steps 664(644.84) | Grad Norm 2.4857(5.5167) | Total Time 0.00(0.00)\n",
      "Iter 1929 | Time 56.9667(60.7984) | Bit/dim 3.6589(3.6651) | Xent 0.8209(0.8425) | Loss 9.0515(9.6798) | Error 0.2914(0.3013) Steps 634(644.52) | Grad Norm 3.0173(5.4417) | Total Time 0.00(0.00)\n",
      "Iter 1930 | Time 60.8744(60.8007) | Bit/dim 3.6631(3.6651) | Xent 0.8215(0.8418) | Loss 9.1755(9.6647) | Error 0.2960(0.3011) Steps 640(644.38) | Grad Norm 3.5111(5.3838) | Total Time 0.00(0.00)\n",
      "Iter 1931 | Time 60.8239(60.8014) | Bit/dim 3.6619(3.6650) | Xent 0.8204(0.8412) | Loss 8.9050(9.6419) | Error 0.2954(0.3010) Steps 646(644.43) | Grad Norm 4.0433(5.3436) | Total Time 0.00(0.00)\n",
      "Iter 1932 | Time 55.0397(60.6286) | Bit/dim 3.6673(3.6650) | Xent 0.8245(0.8407) | Loss 9.1060(9.6258) | Error 0.2937(0.3008) Steps 610(643.40) | Grad Norm 3.6902(5.2940) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0322 | Time 23.0048, Epoch Time 392.2544(406.4788), Bit/dim 3.6601(best: 3.6613), Xent 0.8590, Loss 4.0896, Error 0.3018(best: 0.3045)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1933 | Time 62.6670(60.6897) | Bit/dim 3.6559(3.6648) | Xent 0.8153(0.8399) | Loss 12.2657(9.7050) | Error 0.2963(0.3006) Steps 616(642.57) | Grad Norm 2.7917(5.2189) | Total Time 0.00(0.00)\n",
      "Iter 1934 | Time 60.6052(60.6872) | Bit/dim 3.6599(3.6646) | Xent 0.8132(0.8391) | Loss 8.9513(9.6824) | Error 0.2923(0.3004) Steps 682(643.76) | Grad Norm 3.8545(5.1780) | Total Time 0.00(0.00)\n",
      "Iter 1935 | Time 62.3418(60.7368) | Bit/dim 3.6698(3.6648) | Xent 0.8270(0.8388) | Loss 9.1827(9.6674) | Error 0.2963(0.3002) Steps 640(643.64) | Grad Norm 5.1842(5.1782) | Total Time 0.00(0.00)\n",
      "Iter 1936 | Time 59.8820(60.7112) | Bit/dim 3.6576(3.6645) | Xent 0.8442(0.8389) | Loss 9.1193(9.6510) | Error 0.3023(0.3003) Steps 658(644.07) | Grad Norm 5.8209(5.1975) | Total Time 0.00(0.00)\n",
      "Iter 1937 | Time 61.8200(60.7444) | Bit/dim 3.6662(3.6646) | Xent 0.8462(0.8392) | Loss 9.0907(9.6342) | Error 0.3045(0.3004) Steps 670(644.85) | Grad Norm 5.8913(5.2183) | Total Time 0.00(0.00)\n",
      "Iter 1938 | Time 59.1811(60.6975) | Bit/dim 3.6553(3.6643) | Xent 0.8275(0.8388) | Loss 8.9483(9.6136) | Error 0.2951(0.3003) Steps 646(644.89) | Grad Norm 5.6209(5.2303) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0323 | Time 23.6174, Epoch Time 406.1096(406.4678), Bit/dim 3.6647(best: 3.6601), Xent 0.8631, Loss 4.0963, Error 0.3033(best: 0.3018)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1939 | Time 66.8361(60.8817) | Bit/dim 3.6643(3.6643) | Xent 0.8142(0.8381) | Loss 12.5772(9.7025) | Error 0.2934(0.3001) Steps 640(644.74) | Grad Norm 6.2805(5.2619) | Total Time 0.00(0.00)\n",
      "Iter 1940 | Time 63.0828(60.9477) | Bit/dim 3.6631(3.6643) | Xent 0.8499(0.8384) | Loss 9.1350(9.6855) | Error 0.3025(0.3001) Steps 658(645.14) | Grad Norm 7.3970(5.3259) | Total Time 0.00(0.00)\n",
      "Iter 1941 | Time 64.8432(61.0646) | Bit/dim 3.6622(3.6642) | Xent 0.8270(0.8381) | Loss 9.0206(9.6655) | Error 0.2940(0.3000) Steps 634(644.80) | Grad Norm 6.7724(5.3693) | Total Time 0.00(0.00)\n",
      "Iter 1942 | Time 63.9231(61.1503) | Bit/dim 3.6548(3.6639) | Xent 0.8278(0.8378) | Loss 9.0365(9.6467) | Error 0.2990(0.2999) Steps 664(645.38) | Grad Norm 3.6076(5.3165) | Total Time 0.00(0.00)\n",
      "Iter 1943 | Time 57.3887(61.0375) | Bit/dim 3.6534(3.6636) | Xent 0.8405(0.8378) | Loss 9.1034(9.6304) | Error 0.3040(0.3001) Steps 652(645.58) | Grad Norm 5.1907(5.3127) | Total Time 0.00(0.00)\n",
      "Iter 1944 | Time 61.1334(61.0404) | Bit/dim 3.6641(3.6636) | Xent 0.8694(0.8388) | Loss 9.0739(9.6137) | Error 0.3105(0.3004) Steps 640(645.41) | Grad Norm 8.4012(5.4053) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0324 | Time 23.5316, Epoch Time 417.0620(406.7856), Bit/dim 3.6646(best: 3.6601), Xent 0.9210, Loss 4.1251, Error 0.3274(best: 0.3018)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1945 | Time 60.8701(61.0353) | Bit/dim 3.6551(3.6634) | Xent 0.9023(0.8407) | Loss 12.7149(9.7067) | Error 0.3279(0.3012) Steps 658(645.79) | Grad Norm 8.3584(5.4939) | Total Time 0.00(0.00)\n",
      "Iter 1946 | Time 66.4586(61.1980) | Bit/dim 3.6518(3.6630) | Xent 0.8570(0.8412) | Loss 9.1620(9.6904) | Error 0.3111(0.3015) Steps 622(645.08) | Grad Norm 9.3558(5.6098) | Total Time 0.00(0.00)\n",
      "Iter 1947 | Time 60.9850(61.1916) | Bit/dim 3.6716(3.6633) | Xent 0.8983(0.8429) | Loss 9.2603(9.6775) | Error 0.3169(0.3019) Steps 652(645.28) | Grad Norm 9.0817(5.7139) | Total Time 0.00(0.00)\n",
      "Iter 1948 | Time 58.9577(61.1246) | Bit/dim 3.6628(3.6633) | Xent 0.8871(0.8442) | Loss 9.1614(9.6620) | Error 0.3183(0.3024) Steps 646(645.30) | Grad Norm 7.4456(5.7659) | Total Time 0.00(0.00)\n",
      "Iter 1949 | Time 61.3177(61.1304) | Bit/dim 3.6716(3.6635) | Xent 0.8676(0.8449) | Loss 9.1293(9.6460) | Error 0.3153(0.3028) Steps 676(646.23) | Grad Norm 9.9368(5.8910) | Total Time 0.00(0.00)\n",
      "Iter 1950 | Time 62.0879(61.1591) | Bit/dim 3.6707(3.6637) | Xent 0.9283(0.8474) | Loss 9.3425(9.6369) | Error 0.3324(0.3037) Steps 688(647.48) | Grad Norm 9.6735(6.0045) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0325 | Time 23.5294, Epoch Time 409.9539(406.8806), Bit/dim 3.6681(best: 3.6601), Xent 0.9139, Loss 4.1250, Error 0.3254(best: 0.3018)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1951 | Time 62.4863(61.1989) | Bit/dim 3.6661(3.6638) | Xent 0.8781(0.8483) | Loss 12.4861(9.7224) | Error 0.3147(0.3040) Steps 628(646.89) | Grad Norm 9.2728(6.1025) | Total Time 0.00(0.00)\n",
      "Iter 1952 | Time 57.5749(61.0902) | Bit/dim 3.6636(3.6638) | Xent 0.9134(0.8503) | Loss 9.1116(9.7041) | Error 0.3235(0.3046) Steps 658(647.23) | Grad Norm 7.4314(6.1424) | Total Time 0.00(0.00)\n",
      "Iter 1953 | Time 57.6918(60.9882) | Bit/dim 3.6648(3.6638) | Xent 0.9037(0.8519) | Loss 9.0990(9.6859) | Error 0.3216(0.3051) Steps 658(647.55) | Grad Norm 10.2995(6.2671) | Total Time 0.00(0.00)\n",
      "Iter 1954 | Time 62.4296(61.0315) | Bit/dim 3.6802(3.6643) | Xent 0.9039(0.8535) | Loss 9.3019(9.6744) | Error 0.3207(0.3056) Steps 688(648.76) | Grad Norm 15.1605(6.5339) | Total Time 0.00(0.00)\n",
      "Iter 1955 | Time 65.2350(61.1576) | Bit/dim 3.6700(3.6645) | Xent 0.8739(0.8541) | Loss 9.1917(9.6599) | Error 0.3163(0.3059) Steps 682(649.76) | Grad Norm 7.7690(6.5710) | Total Time 0.00(0.00)\n",
      "Iter 1956 | Time 59.7606(61.1157) | Bit/dim 3.6673(3.6646) | Xent 0.8470(0.8539) | Loss 9.1354(9.6442) | Error 0.3037(0.3059) Steps 646(649.65) | Grad Norm 9.0807(6.6463) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0326 | Time 23.5017, Epoch Time 404.9721(406.8234), Bit/dim 3.6716(best: 3.6601), Xent 0.8873, Loss 4.1153, Error 0.3133(best: 0.3018)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1957 | Time 62.1223(61.1459) | Bit/dim 3.6779(3.6650) | Xent 0.8810(0.8547) | Loss 12.7648(9.7378) | Error 0.3150(0.3061) Steps 694(650.98) | Grad Norm 9.0182(6.7174) | Total Time 0.00(0.00)\n",
      "Iter 1958 | Time 62.7493(61.1940) | Bit/dim 3.6714(3.6652) | Xent 0.8659(0.8550) | Loss 9.1504(9.7202) | Error 0.3033(0.3060) Steps 664(651.37) | Grad Norm 7.8289(6.7508) | Total Time 0.00(0.00)\n",
      "Iter 1959 | Time 60.0108(61.1585) | Bit/dim 3.6561(3.6649) | Xent 0.8453(0.8547) | Loss 9.2040(9.7047) | Error 0.3055(0.3060) Steps 664(651.75) | Grad Norm 5.8887(6.7249) | Total Time 0.00(0.00)\n",
      "Iter 1960 | Time 62.6105(61.2020) | Bit/dim 3.6743(3.6652) | Xent 0.8394(0.8543) | Loss 9.2053(9.6897) | Error 0.3063(0.3060) Steps 664(652.12) | Grad Norm 10.0095(6.8234) | Total Time 0.00(0.00)\n",
      "Iter 1961 | Time 59.4975(61.1509) | Bit/dim 3.6596(3.6650) | Xent 0.8413(0.8539) | Loss 9.0090(9.6693) | Error 0.2997(0.3058) Steps 652(652.11) | Grad Norm 6.5449(6.8151) | Total Time 0.00(0.00)\n",
      "Iter 1962 | Time 60.1160(61.1198) | Bit/dim 3.6724(3.6652) | Xent 0.8473(0.8537) | Loss 9.0818(9.6516) | Error 0.3004(0.3057) Steps 670(652.65) | Grad Norm 5.2855(6.7692) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0327 | Time 22.5974, Epoch Time 406.6587(406.8184), Bit/dim 3.6734(best: 3.6601), Xent 0.8763, Loss 4.1116, Error 0.3095(best: 0.3018)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1963 | Time 59.2475(61.0637) | Bit/dim 3.6863(3.6659) | Xent 0.8288(0.8529) | Loss 11.6718(9.7123) | Error 0.2946(0.3054) Steps 628(651.91) | Grad Norm 6.7727(6.7693) | Total Time 0.00(0.00)\n",
      "Iter 1964 | Time 59.0670(61.0038) | Bit/dim 3.6567(3.6656) | Xent 0.8137(0.8518) | Loss 9.0347(9.6919) | Error 0.2911(0.3049) Steps 586(649.93) | Grad Norm 3.7036(6.6773) | Total Time 0.00(0.00)\n",
      "Iter 1965 | Time 56.5820(60.8711) | Bit/dim 3.6552(3.6653) | Xent 0.8362(0.8513) | Loss 9.1259(9.6749) | Error 0.2955(0.3046) Steps 646(649.81) | Grad Norm 5.8725(6.6532) | Total Time 0.00(0.00)\n",
      "Iter 1966 | Time 58.1513(60.7895) | Bit/dim 3.6638(3.6652) | Xent 0.8230(0.8504) | Loss 8.9217(9.6523) | Error 0.2961(0.3044) Steps 658(650.06) | Grad Norm 3.3514(6.5541) | Total Time 0.00(0.00)\n",
      "Iter 1967 | Time 57.9444(60.7042) | Bit/dim 3.6790(3.6656) | Xent 0.8562(0.8506) | Loss 9.2480(9.6402) | Error 0.2969(0.3042) Steps 652(650.12) | Grad Norm 6.7079(6.5587) | Total Time 0.00(0.00)\n",
      "Iter 1968 | Time 55.0130(60.5334) | Bit/dim 3.6722(3.6658) | Xent 0.8529(0.8507) | Loss 8.9800(9.6204) | Error 0.3073(0.3043) Steps 628(649.45) | Grad Norm 6.1812(6.5474) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0328 | Time 23.5529, Epoch Time 385.8697(406.1900), Bit/dim 3.6675(best: 3.6601), Xent 0.8718, Loss 4.1034, Error 0.3082(best: 0.3018)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1969 | Time 66.8308(60.7224) | Bit/dim 3.6674(3.6659) | Xent 0.8235(0.8499) | Loss 12.7938(9.7156) | Error 0.2971(0.3040) Steps 604(648.09) | Grad Norm 4.5490(6.4875) | Total Time 0.00(0.00)\n",
      "Iter 1970 | Time 62.0220(60.7613) | Bit/dim 3.6601(3.6657) | Xent 0.8355(0.8494) | Loss 9.1527(9.6987) | Error 0.2974(0.3038) Steps 634(647.67) | Grad Norm 7.9575(6.5316) | Total Time 0.00(0.00)\n",
      "Iter 1971 | Time 59.0738(60.7107) | Bit/dim 3.6585(3.6655) | Xent 0.8468(0.8494) | Loss 8.8893(9.6744) | Error 0.3050(0.3039) Steps 646(647.62) | Grad Norm 6.9061(6.5428) | Total Time 0.00(0.00)\n",
      "Iter 1972 | Time 63.1325(60.7834) | Bit/dim 3.6657(3.6655) | Xent 0.8240(0.8486) | Loss 9.2775(9.6625) | Error 0.2975(0.3037) Steps 640(647.39) | Grad Norm 3.3243(6.4462) | Total Time 0.00(0.00)\n",
      "Iter 1973 | Time 60.5589(60.7766) | Bit/dim 3.6624(3.6654) | Xent 0.8668(0.8491) | Loss 8.9741(9.6419) | Error 0.3149(0.3040) Steps 682(648.43) | Grad Norm 7.7084(6.4841) | Total Time 0.00(0.00)\n",
      "Iter 1974 | Time 51.7725(60.5065) | Bit/dim 3.6627(3.6653) | Xent 0.8248(0.8484) | Loss 8.9585(9.6214) | Error 0.2944(0.3037) Steps 598(646.92) | Grad Norm 3.7353(6.4016) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0329 | Time 22.6418, Epoch Time 402.3668(406.0753), Bit/dim 3.6611(best: 3.6601), Xent 0.8691, Loss 4.0957, Error 0.3063(best: 0.3018)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1975 | Time 64.9170(60.6388) | Bit/dim 3.6647(3.6653) | Xent 0.8467(0.8484) | Loss 12.6745(9.7130) | Error 0.2993(0.3036) Steps 634(646.53) | Grad Norm 4.8977(6.3565) | Total Time 0.00(0.00)\n",
      "Iter 1976 | Time 64.6117(60.7580) | Bit/dim 3.6611(3.6652) | Xent 0.8214(0.8476) | Loss 9.1764(9.6969) | Error 0.2929(0.3033) Steps 634(646.15) | Grad Norm 5.2056(6.3220) | Total Time 0.00(0.00)\n",
      "Iter 1977 | Time 61.4235(60.7780) | Bit/dim 3.6582(3.6650) | Xent 0.8335(0.8471) | Loss 8.9199(9.6736) | Error 0.3044(0.3033) Steps 640(645.97) | Grad Norm 3.9612(6.2512) | Total Time 0.00(0.00)\n",
      "Iter 1978 | Time 62.0248(60.8154) | Bit/dim 3.6631(3.6649) | Xent 0.8084(0.8460) | Loss 9.0596(9.6551) | Error 0.2895(0.3029) Steps 652(646.15) | Grad Norm 8.2267(6.3104) | Total Time 0.00(0.00)\n",
      "Iter 1979 | Time 68.8621(61.0568) | Bit/dim 3.6715(3.6651) | Xent 0.8346(0.8456) | Loss 9.1100(9.6388) | Error 0.2976(0.3027) Steps 670(646.86) | Grad Norm 4.1695(6.2462) | Total Time 0.00(0.00)\n",
      "Iter 1980 | Time 58.0375(60.9662) | Bit/dim 3.6668(3.6652) | Xent 0.8214(0.8449) | Loss 9.1911(9.6254) | Error 0.2957(0.3025) Steps 628(646.30) | Grad Norm 5.0629(6.2107) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0330 | Time 23.5941, Epoch Time 419.8193(406.4876), Bit/dim 3.6646(best: 3.6601), Xent 0.8543, Loss 4.0918, Error 0.2974(best: 0.3018)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1981 | Time 60.6902(60.9579) | Bit/dim 3.6603(3.6650) | Xent 0.8297(0.8444) | Loss 12.7415(9.7188) | Error 0.2984(0.3024) Steps 670(647.01) | Grad Norm 2.8801(6.1108) | Total Time 0.00(0.00)\n",
      "Iter 1982 | Time 58.7741(60.8924) | Bit/dim 3.6674(3.6651) | Xent 0.8165(0.8436) | Loss 8.8145(9.6917) | Error 0.2925(0.3021) Steps 652(647.16) | Grad Norm 4.4299(6.0604) | Total Time 0.00(0.00)\n",
      "Iter 1983 | Time 55.3637(60.7265) | Bit/dim 3.6675(3.6652) | Xent 0.8006(0.8423) | Loss 8.9261(9.6687) | Error 0.2889(0.3017) Steps 622(646.40) | Grad Norm 3.9492(5.9970) | Total Time 0.00(0.00)\n",
      "Iter 1984 | Time 60.9432(60.7330) | Bit/dim 3.6529(3.6648) | Xent 0.8430(0.8423) | Loss 9.0414(9.6499) | Error 0.2967(0.3016) Steps 646(646.39) | Grad Norm 4.7866(5.9607) | Total Time 0.00(0.00)\n",
      "Iter 1985 | Time 61.3409(60.7513) | Bit/dim 3.6617(3.6647) | Xent 0.8245(0.8418) | Loss 9.1582(9.6352) | Error 0.2951(0.3014) Steps 670(647.10) | Grad Norm 3.1361(5.8760) | Total Time 0.00(0.00)\n",
      "Iter 1986 | Time 59.3416(60.7090) | Bit/dim 3.6606(3.6646) | Xent 0.8127(0.8409) | Loss 9.1740(9.6213) | Error 0.2923(0.3011) Steps 658(647.43) | Grad Norm 6.2862(5.8883) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0331 | Time 23.5652, Epoch Time 396.4263(406.1858), Bit/dim 3.6600(best: 3.6601), Xent 0.8661, Loss 4.0931, Error 0.3047(best: 0.2974)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1987 | Time 59.8243(60.6824) | Bit/dim 3.6703(3.6648) | Xent 0.8263(0.8405) | Loss 12.0734(9.6949) | Error 0.2966(0.3010) Steps 646(647.38) | Grad Norm 6.9760(5.9209) | Total Time 0.00(0.00)\n",
      "Iter 1988 | Time 58.4984(60.6169) | Bit/dim 3.6646(3.6648) | Xent 0.8161(0.8398) | Loss 8.9826(9.6735) | Error 0.2877(0.3006) Steps 634(646.98) | Grad Norm 2.0873(5.8059) | Total Time 0.00(0.00)\n",
      "Iter 1989 | Time 60.0430(60.5997) | Bit/dim 3.6511(3.6643) | Xent 0.8346(0.8396) | Loss 9.1172(9.6568) | Error 0.3003(0.3006) Steps 616(646.05) | Grad Norm 6.9260(5.8395) | Total Time 0.00(0.00)\n",
      "Iter 1990 | Time 59.2560(60.5594) | Bit/dim 3.6547(3.6641) | Xent 0.8238(0.8391) | Loss 9.0529(9.6387) | Error 0.3001(0.3005) Steps 640(645.87) | Grad Norm 3.9432(5.7826) | Total Time 0.00(0.00)\n",
      "Iter 1991 | Time 66.5266(60.7384) | Bit/dim 3.6686(3.6642) | Xent 0.8522(0.8395) | Loss 9.1675(9.6246) | Error 0.3020(0.3006) Steps 688(647.14) | Grad Norm 7.1945(5.8250) | Total Time 0.00(0.00)\n",
      "Iter 1992 | Time 61.5601(60.7631) | Bit/dim 3.6599(3.6641) | Xent 0.8298(0.8392) | Loss 9.0548(9.6075) | Error 0.3015(0.3006) Steps 652(647.28) | Grad Norm 9.0984(5.9232) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0332 | Time 23.2988, Epoch Time 404.8624(406.1461), Bit/dim 3.6642(best: 3.6600), Xent 0.8899, Loss 4.1092, Error 0.3177(best: 0.2974)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1993 | Time 60.0490(60.7416) | Bit/dim 3.6636(3.6640) | Xent 0.8560(0.8397) | Loss 12.3518(9.6898) | Error 0.3051(0.3007) Steps 670(647.96) | Grad Norm 11.3823(6.0870) | Total Time 0.00(0.00)\n",
      "Iter 1994 | Time 63.2870(60.8180) | Bit/dim 3.6658(3.6641) | Xent 0.8549(0.8402) | Loss 9.1299(9.6730) | Error 0.3084(0.3010) Steps 670(648.62) | Grad Norm 11.9377(6.2625) | Total Time 0.00(0.00)\n",
      "Iter 1995 | Time 60.1831(60.7990) | Bit/dim 3.6572(3.6639) | Xent 0.8297(0.8399) | Loss 9.0686(9.6549) | Error 0.2970(0.3009) Steps 622(647.83) | Grad Norm 6.4072(6.2668) | Total Time 0.00(0.00)\n",
      "Iter 1996 | Time 67.3338(60.9950) | Bit/dim 3.6537(3.6636) | Xent 0.8546(0.8403) | Loss 9.1451(9.6396) | Error 0.3096(0.3011) Steps 658(648.13) | Grad Norm 9.4657(6.3628) | Total Time 0.00(0.00)\n",
      "Iter 1997 | Time 61.3253(61.0049) | Bit/dim 3.6648(3.6636) | Xent 0.8435(0.8404) | Loss 9.1551(9.6251) | Error 0.3040(0.3012) Steps 664(648.61) | Grad Norm 4.3674(6.3029) | Total Time 0.00(0.00)\n",
      "Iter 1998 | Time 65.6876(61.1454) | Bit/dim 3.6580(3.6635) | Xent 0.8408(0.8404) | Loss 9.2276(9.6131) | Error 0.3036(0.3013) Steps 682(649.61) | Grad Norm 8.2116(6.3602) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0333 | Time 23.4960, Epoch Time 417.3505(406.4822), Bit/dim 3.6640(best: 3.6600), Xent 0.8555, Loss 4.0917, Error 0.3007(best: 0.2974)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1999 | Time 61.8242(61.1658) | Bit/dim 3.6683(3.6636) | Xent 0.8259(0.8400) | Loss 12.5789(9.7021) | Error 0.2940(0.3011) Steps 682(650.58) | Grad Norm 2.8541(6.2550) | Total Time 0.00(0.00)\n",
      "Iter 2000 | Time 60.1201(61.1344) | Bit/dim 3.6428(3.6630) | Xent 0.8559(0.8405) | Loss 9.0884(9.6837) | Error 0.3071(0.3012) Steps 670(651.16) | Grad Norm 9.4572(6.3511) | Total Time 0.00(0.00)\n",
      "Iter 2001 | Time 62.3630(61.1712) | Bit/dim 3.6600(3.6629) | Xent 0.8155(0.8397) | Loss 8.9098(9.6605) | Error 0.2886(0.3009) Steps 652(651.19) | Grad Norm 6.5834(6.3580) | Total Time 0.00(0.00)\n",
      "Iter 2002 | Time 59.4772(61.1204) | Bit/dim 3.6617(3.6629) | Xent 0.8469(0.8399) | Loss 9.1907(9.6464) | Error 0.3047(0.3010) Steps 652(651.21) | Grad Norm 6.3463(6.3577) | Total Time 0.00(0.00)\n",
      "Iter 2003 | Time 58.2445(61.0341) | Bit/dim 3.6572(3.6627) | Xent 0.8016(0.8388) | Loss 8.9707(9.6261) | Error 0.2906(0.3007) Steps 670(651.78) | Grad Norm 3.8360(6.2820) | Total Time 0.00(0.00)\n",
      "Iter 2004 | Time 56.9507(60.9116) | Bit/dim 3.6638(3.6627) | Xent 0.8204(0.8382) | Loss 9.1778(9.6127) | Error 0.2901(0.3004) Steps 658(651.96) | Grad Norm 6.1378(6.2777) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0334 | Time 23.5696, Epoch Time 398.7638(406.2507), Bit/dim 3.6630(best: 3.6600), Xent 0.8569, Loss 4.0915, Error 0.3021(best: 0.2974)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2005 | Time 62.1295(60.9482) | Bit/dim 3.6649(3.6628) | Xent 0.8097(0.8374) | Loss 12.3737(9.6955) | Error 0.2889(0.3000) Steps 670(652.50) | Grad Norm 3.8395(6.2046) | Total Time 0.00(0.00)\n",
      "Iter 2006 | Time 61.2593(60.9575) | Bit/dim 3.6581(3.6626) | Xent 0.8250(0.8370) | Loss 9.0769(9.6769) | Error 0.2954(0.2999) Steps 646(652.31) | Grad Norm 5.3111(6.1778) | Total Time 0.00(0.00)\n",
      "Iter 2007 | Time 61.3946(60.9706) | Bit/dim 3.6574(3.6625) | Xent 0.8218(0.8365) | Loss 9.0659(9.6586) | Error 0.2984(0.2998) Steps 640(651.94) | Grad Norm 5.7144(6.1639) | Total Time 0.00(0.00)\n",
      "Iter 2008 | Time 64.3316(61.0715) | Bit/dim 3.6558(3.6623) | Xent 0.8211(0.8361) | Loss 9.1054(9.6420) | Error 0.2897(0.2995) Steps 688(653.02) | Grad Norm 3.1120(6.0723) | Total Time 0.00(0.00)\n",
      "Iter 2009 | Time 58.7371(61.0014) | Bit/dim 3.6560(3.6621) | Xent 0.8113(0.8353) | Loss 9.0556(9.6244) | Error 0.2924(0.2993) Steps 628(652.27) | Grad Norm 4.3883(6.0218) | Total Time 0.00(0.00)\n",
      "Iter 2010 | Time 63.2835(61.0699) | Bit/dim 3.6549(3.6619) | Xent 0.8542(0.8359) | Loss 9.2260(9.6125) | Error 0.3105(0.2996) Steps 664(652.62) | Grad Norm 4.9714(5.9903) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0335 | Time 23.7981, Epoch Time 410.5123(406.3785), Bit/dim 3.6646(best: 3.6600), Xent 0.8668, Loss 4.0980, Error 0.3040(best: 0.2974)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2011 | Time 60.4025(61.0499) | Bit/dim 3.6669(3.6620) | Xent 0.8335(0.8358) | Loss 12.7944(9.7079) | Error 0.2925(0.2994) Steps 640(652.24) | Grad Norm 4.9893(5.9602) | Total Time 0.00(0.00)\n",
      "Iter 2012 | Time 59.1702(60.9935) | Bit/dim 3.6529(3.6618) | Xent 0.8249(0.8355) | Loss 8.9118(9.6840) | Error 0.3021(0.2995) Steps 676(652.96) | Grad Norm 5.3982(5.9434) | Total Time 0.00(0.00)\n",
      "Iter 2013 | Time 67.2085(61.1799) | Bit/dim 3.6577(3.6616) | Xent 0.8122(0.8348) | Loss 9.2056(9.6697) | Error 0.2870(0.2991) Steps 676(653.65) | Grad Norm 8.6574(6.0248) | Total Time 0.00(0.00)\n",
      "Iter 2014 | Time 65.4118(61.3069) | Bit/dim 3.6542(3.6614) | Xent 0.8340(0.8348) | Loss 9.3205(9.6592) | Error 0.2964(0.2991) Steps 682(654.50) | Grad Norm 7.4742(6.0683) | Total Time 0.00(0.00)\n",
      "Iter 2015 | Time 61.8454(61.3230) | Bit/dim 3.6535(3.6612) | Xent 0.8156(0.8342) | Loss 8.8393(9.6346) | Error 0.2959(0.2990) Steps 622(653.52) | Grad Norm 6.0939(6.0691) | Total Time 0.00(0.00)\n",
      "Iter 2016 | Time 58.6349(61.2424) | Bit/dim 3.6565(3.6610) | Xent 0.8323(0.8341) | Loss 8.9441(9.6139) | Error 0.2964(0.2989) Steps 652(653.48) | Grad Norm 5.6224(6.0557) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0336 | Time 23.8294, Epoch Time 412.6769(406.5675), Bit/dim 3.6615(best: 3.6600), Xent 0.8653, Loss 4.0941, Error 0.3031(best: 0.2974)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2017 | Time 64.5003(61.3401) | Bit/dim 3.6529(3.6608) | Xent 0.8379(0.8343) | Loss 12.3524(9.6961) | Error 0.2946(0.2988) Steps 664(653.79) | Grad Norm 6.5814(6.0714) | Total Time 0.00(0.00)\n",
      "Iter 2018 | Time 61.9680(61.3590) | Bit/dim 3.6671(3.6610) | Xent 0.8409(0.8345) | Loss 9.3158(9.6846) | Error 0.3050(0.2989) Steps 670(654.28) | Grad Norm 4.0617(6.0111) | Total Time 0.00(0.00)\n",
      "Iter 2019 | Time 63.4220(61.4209) | Bit/dim 3.6555(3.6608) | Xent 0.8159(0.8339) | Loss 9.0350(9.6652) | Error 0.2887(0.2986) Steps 622(653.31) | Grad Norm 5.1016(5.9839) | Total Time 0.00(0.00)\n",
      "Iter 2020 | Time 60.5591(61.3950) | Bit/dim 3.6581(3.6607) | Xent 0.8087(0.8331) | Loss 9.0818(9.6477) | Error 0.2857(0.2982) Steps 664(653.63) | Grad Norm 6.1629(5.9892) | Total Time 0.00(0.00)\n",
      "Iter 2021 | Time 57.0122(61.2635) | Bit/dim 3.6703(3.6610) | Xent 0.7998(0.8321) | Loss 9.0929(9.6310) | Error 0.2901(0.2980) Steps 670(654.12) | Grad Norm 2.9534(5.8982) | Total Time 0.00(0.00)\n",
      "Iter 2022 | Time 59.2203(61.2022) | Bit/dim 3.6637(3.6611) | Xent 0.8134(0.8316) | Loss 9.1120(9.6154) | Error 0.2921(0.2978) Steps 646(653.88) | Grad Norm 6.3237(5.9109) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0337 | Time 23.9549, Epoch Time 406.6295(406.5693), Bit/dim 3.6619(best: 3.6600), Xent 0.8591, Loss 4.0915, Error 0.3033(best: 0.2974)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2023 | Time 61.1435(61.2005) | Bit/dim 3.6532(3.6609) | Xent 0.8266(0.8314) | Loss 12.7245(9.7087) | Error 0.2919(0.2976) Steps 700(655.26) | Grad Norm 6.0956(5.9165) | Total Time 0.00(0.00)\n",
      "Iter 2024 | Time 65.3880(61.3261) | Bit/dim 3.6552(3.6607) | Xent 0.8310(0.8314) | Loss 9.0352(9.6885) | Error 0.2985(0.2977) Steps 652(655.16) | Grad Norm 3.8957(5.8558) | Total Time 0.00(0.00)\n",
      "Iter 2025 | Time 58.2209(61.2329) | Bit/dim 3.6591(3.6607) | Xent 0.8216(0.8311) | Loss 9.0698(9.6699) | Error 0.2943(0.2976) Steps 610(653.81) | Grad Norm 8.5295(5.9360) | Total Time 0.00(0.00)\n",
      "Iter 2026 | Time 59.7322(61.1879) | Bit/dim 3.6729(3.6610) | Xent 0.8399(0.8314) | Loss 9.0193(9.6504) | Error 0.3047(0.2978) Steps 628(653.04) | Grad Norm 10.8076(6.0822) | Total Time 0.00(0.00)\n",
      "Iter 2027 | Time 63.2363(61.2494) | Bit/dim 3.6558(3.6609) | Xent 0.7908(0.8302) | Loss 8.9683(9.6300) | Error 0.2795(0.2972) Steps 670(653.54) | Grad Norm 4.2713(6.0279) | Total Time 0.00(0.00)\n",
      "Iter 2028 | Time 63.2746(61.3101) | Bit/dim 3.6718(3.6612) | Xent 0.8016(0.8293) | Loss 8.9755(9.6103) | Error 0.2839(0.2968) Steps 688(654.58) | Grad Norm 5.9464(6.0254) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0338 | Time 23.3092, Epoch Time 410.5298(406.6881), Bit/dim 3.6609(best: 3.6600), Xent 0.8609, Loss 4.0914, Error 0.3023(best: 0.2974)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2029 | Time 62.0874(61.3334) | Bit/dim 3.6460(3.6607) | Xent 0.8047(0.8286) | Loss 12.2162(9.6885) | Error 0.2887(0.2966) Steps 670(655.04) | Grad Norm 6.5161(6.0401) | Total Time 0.00(0.00)\n",
      "Iter 2030 | Time 62.2497(61.3609) | Bit/dim 3.6551(3.6606) | Xent 0.8068(0.8279) | Loss 9.0740(9.6701) | Error 0.2921(0.2965) Steps 616(653.87) | Grad Norm 3.6555(5.9686) | Total Time 0.00(0.00)\n",
      "Iter 2031 | Time 67.2883(61.5388) | Bit/dim 3.6644(3.6607) | Xent 0.8076(0.8273) | Loss 8.8504(9.6455) | Error 0.2924(0.2963) Steps 658(653.99) | Grad Norm 6.1414(5.9738) | Total Time 0.00(0.00)\n",
      "Iter 2032 | Time 61.9704(61.5517) | Bit/dim 3.6630(3.6608) | Xent 0.8391(0.8277) | Loss 9.0949(9.6290) | Error 0.2950(0.2963) Steps 640(653.57) | Grad Norm 6.5634(5.9915) | Total Time 0.00(0.00)\n",
      "Iter 2033 | Time 59.8813(61.5016) | Bit/dim 3.6655(3.6609) | Xent 0.8257(0.8276) | Loss 9.2082(9.6163) | Error 0.2925(0.2962) Steps 634(652.99) | Grad Norm 6.1350(5.9958) | Total Time 0.00(0.00)\n",
      "Iter 2034 | Time 63.2727(61.5547) | Bit/dim 3.6512(3.6606) | Xent 0.8578(0.8285) | Loss 9.0718(9.6000) | Error 0.3075(0.2965) Steps 664(653.32) | Grad Norm 10.4333(6.1289) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0339 | Time 23.3966, Epoch Time 416.2501(406.9750), Bit/dim 3.6717(best: 3.6600), Xent 0.8705, Loss 4.1069, Error 0.3075(best: 0.2974)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2035 | Time 63.0272(61.5989) | Bit/dim 3.6521(3.6603) | Xent 0.8155(0.8281) | Loss 12.4520(9.6856) | Error 0.2921(0.2964) Steps 652(653.28) | Grad Norm 8.0095(6.1853) | Total Time 0.00(0.00)\n",
      "Iter 2036 | Time 62.9379(61.6391) | Bit/dim 3.6647(3.6605) | Xent 0.8000(0.8273) | Loss 9.0100(9.6653) | Error 0.2860(0.2961) Steps 646(653.06) | Grad Norm 2.8645(6.0857) | Total Time 0.00(0.00)\n",
      "Iter 2037 | Time 59.1183(61.5634) | Bit/dim 3.6626(3.6605) | Xent 0.8405(0.8277) | Loss 9.1141(9.6488) | Error 0.3011(0.2962) Steps 628(652.31) | Grad Norm 9.0152(6.1736) | Total Time 0.00(0.00)\n",
      "Iter 2038 | Time 60.4082(61.5288) | Bit/dim 3.6628(3.6606) | Xent 0.8358(0.8279) | Loss 9.0448(9.6306) | Error 0.2996(0.2963) Steps 670(652.84) | Grad Norm 7.1954(6.2042) | Total Time 0.00(0.00)\n",
      "Iter 2039 | Time 60.5252(61.4987) | Bit/dim 3.6658(3.6608) | Xent 0.8585(0.8288) | Loss 9.0196(9.6123) | Error 0.3083(0.2967) Steps 646(652.63) | Grad Norm 7.2264(6.2349) | Total Time 0.00(0.00)\n",
      "Iter 2040 | Time 62.7442(61.5360) | Bit/dim 3.6619(3.6608) | Xent 0.8307(0.8289) | Loss 9.2727(9.6021) | Error 0.2961(0.2967) Steps 706(654.23) | Grad Norm 7.0206(6.2585) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0340 | Time 23.8806, Epoch Time 408.5520(407.0223), Bit/dim 3.6563(best: 3.6600), Xent 0.8701, Loss 4.0914, Error 0.3052(best: 0.2974)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2041 | Time 60.4956(61.5048) | Bit/dim 3.6511(3.6605) | Xent 0.8112(0.8284) | Loss 12.5435(9.6904) | Error 0.2919(0.2965) Steps 676(654.89) | Grad Norm 7.0647(6.2827) | Total Time 0.00(0.00)\n",
      "Iter 2042 | Time 57.4925(61.3845) | Bit/dim 3.6606(3.6605) | Xent 0.8261(0.8283) | Loss 9.0659(9.6716) | Error 0.2963(0.2965) Steps 646(654.62) | Grad Norm 5.3662(6.2552) | Total Time 0.00(0.00)\n",
      "Iter 2043 | Time 60.5795(61.3603) | Bit/dim 3.6743(3.6609) | Xent 0.8063(0.8276) | Loss 9.0500(9.6530) | Error 0.2881(0.2963) Steps 688(655.62) | Grad Norm 4.6008(6.2055) | Total Time 0.00(0.00)\n",
      "Iter 2044 | Time 62.0627(61.3814) | Bit/dim 3.6617(3.6609) | Xent 0.8549(0.8284) | Loss 9.1222(9.6371) | Error 0.3059(0.2966) Steps 640(655.15) | Grad Norm 7.1481(6.2338) | Total Time 0.00(0.00)\n",
      "Iter 2045 | Time 58.4216(61.2926) | Bit/dim 3.6570(3.6608) | Xent 0.8345(0.8286) | Loss 9.0038(9.6181) | Error 0.2980(0.2966) Steps 652(655.06) | Grad Norm 6.9623(6.2557) | Total Time 0.00(0.00)\n",
      "Iter 2046 | Time 64.9218(61.4015) | Bit/dim 3.6513(3.6605) | Xent 0.8055(0.8279) | Loss 9.0570(9.6012) | Error 0.2907(0.2964) Steps 700(656.41) | Grad Norm 5.5228(6.2337) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0341 | Time 23.2724, Epoch Time 403.2530(406.9092), Bit/dim 3.6619(best: 3.6563), Xent 0.8591, Loss 4.0914, Error 0.3042(best: 0.2974)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2047 | Time 63.2616(61.4573) | Bit/dim 3.6649(3.6607) | Xent 0.8092(0.8274) | Loss 12.3791(9.6846) | Error 0.2839(0.2960) Steps 688(657.35) | Grad Norm 4.5902(6.1844) | Total Time 0.00(0.00)\n",
      "Iter 2048 | Time 63.3716(61.5147) | Bit/dim 3.6434(3.6602) | Xent 0.8366(0.8277) | Loss 8.9589(9.6628) | Error 0.3007(0.2962) Steps 652(657.19) | Grad Norm 5.0463(6.1502) | Total Time 0.00(0.00)\n",
      "Iter 2049 | Time 56.2283(61.3561) | Bit/dim 3.6652(3.6603) | Xent 0.7981(0.8268) | Loss 9.0313(9.6438) | Error 0.2896(0.2960) Steps 622(656.14) | Grad Norm 4.0036(6.0858) | Total Time 0.00(0.00)\n",
      "Iter 2050 | Time 62.5186(61.3910) | Bit/dim 3.6542(3.6601) | Xent 0.8240(0.8267) | Loss 9.2203(9.6311) | Error 0.2930(0.2959) Steps 676(656.73) | Grad Norm 4.6190(6.0418) | Total Time 0.00(0.00)\n",
      "Iter 2051 | Time 56.9087(61.2565) | Bit/dim 3.6478(3.6598) | Xent 0.8304(0.8268) | Loss 8.9840(9.6117) | Error 0.2957(0.2959) Steps 622(655.69) | Grad Norm 4.5813(5.9980) | Total Time 0.00(0.00)\n",
      "Iter 2052 | Time 57.7526(61.1514) | Bit/dim 3.6658(3.6599) | Xent 0.8011(0.8260) | Loss 8.9957(9.5932) | Error 0.2911(0.2958) Steps 664(655.94) | Grad Norm 4.7547(5.9607) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0342 | Time 23.4177, Epoch Time 399.5463(406.6883), Bit/dim 3.6633(best: 3.6563), Xent 0.8571, Loss 4.0919, Error 0.3032(best: 0.2974)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2053 | Time 63.1162(61.2103) | Bit/dim 3.6563(3.6598) | Xent 0.7984(0.8252) | Loss 12.7821(9.6889) | Error 0.2877(0.2955) Steps 646(655.64) | Grad Norm 4.6028(5.9200) | Total Time 0.00(0.00)\n",
      "Iter 2054 | Time 63.6133(61.2824) | Bit/dim 3.6513(3.6596) | Xent 0.8138(0.8249) | Loss 9.1416(9.6725) | Error 0.2947(0.2955) Steps 640(655.17) | Grad Norm 3.6792(5.8528) | Total Time 0.00(0.00)\n",
      "Iter 2055 | Time 62.9367(61.3321) | Bit/dim 3.6588(3.6595) | Xent 0.8219(0.8248) | Loss 9.0063(9.6525) | Error 0.2867(0.2952) Steps 640(654.72) | Grad Norm 7.9310(5.9151) | Total Time 0.00(0.00)\n",
      "Iter 2056 | Time 61.3172(61.3316) | Bit/dim 3.6620(3.6596) | Xent 0.8158(0.8245) | Loss 9.1816(9.6384) | Error 0.2885(0.2950) Steps 664(655.00) | Grad Norm 6.1705(5.9228) | Total Time 0.00(0.00)\n",
      "Iter 2057 | Time 63.8756(61.4079) | Bit/dim 3.6596(3.6596) | Xent 0.8242(0.8245) | Loss 9.2800(9.6276) | Error 0.2985(0.2951) Steps 706(656.53) | Grad Norm 6.2134(5.9315) | Total Time 0.00(0.00)\n",
      "Iter 2058 | Time 63.3843(61.4672) | Bit/dim 3.6711(3.6600) | Xent 0.8248(0.8245) | Loss 9.1595(9.6136) | Error 0.2974(0.2952) Steps 670(656.93) | Grad Norm 6.9621(5.9624) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0343 | Time 23.4583, Epoch Time 417.8744(407.0239), Bit/dim 3.6581(best: 3.6563), Xent 0.9113, Loss 4.1138, Error 0.3246(best: 0.2974)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2059 | Time 60.6763(61.4435) | Bit/dim 3.6419(3.6594) | Xent 0.8699(0.8259) | Loss 12.4364(9.6983) | Error 0.3094(0.2956) Steps 646(656.60) | Grad Norm 12.4079(6.1558) | Total Time 0.00(0.00)\n",
      "Iter 2060 | Time 72.6677(61.7802) | Bit/dim 3.6530(3.6592) | Xent 0.8439(0.8264) | Loss 9.3430(9.6876) | Error 0.3031(0.2958) Steps 682(657.37) | Grad Norm 10.8952(6.2979) | Total Time 0.00(0.00)\n",
      "Iter 2061 | Time 61.2211(61.7634) | Bit/dim 3.6542(3.6591) | Xent 0.8413(0.8268) | Loss 8.9536(9.6656) | Error 0.3041(0.2961) Steps 658(657.38) | Grad Norm 7.9401(6.3472) | Total Time 0.00(0.00)\n",
      "Iter 2062 | Time 59.8670(61.7066) | Bit/dim 3.6602(3.6591) | Xent 0.8196(0.8266) | Loss 9.2426(9.6529) | Error 0.2930(0.2960) Steps 646(657.04) | Grad Norm 4.7965(6.3007) | Total Time 0.00(0.00)\n",
      "Iter 2063 | Time 61.9647(61.7143) | Bit/dim 3.6669(3.6593) | Xent 0.8265(0.8266) | Loss 9.2044(9.6395) | Error 0.2989(0.2961) Steps 646(656.71) | Grad Norm 6.6023(6.3097) | Total Time 0.00(0.00)\n",
      "Iter 2064 | Time 57.1495(61.5774) | Bit/dim 3.6598(3.6594) | Xent 0.8393(0.8270) | Loss 9.0114(9.6206) | Error 0.3005(0.2962) Steps 658(656.75) | Grad Norm 7.2759(6.3387) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0344 | Time 23.1181, Epoch Time 412.5282(407.1890), Bit/dim 3.6596(best: 3.6563), Xent 0.8724, Loss 4.0958, Error 0.3071(best: 0.2974)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2065 | Time 64.0000(61.6500) | Bit/dim 3.6674(3.6596) | Xent 0.8235(0.8269) | Loss 12.5963(9.7099) | Error 0.2907(0.2961) Steps 652(656.61) | Grad Norm 6.2108(6.3349) | Total Time 0.00(0.00)\n",
      "Iter 2066 | Time 61.1308(61.6345) | Bit/dim 3.6506(3.6593) | Xent 0.8264(0.8269) | Loss 9.1413(9.6928) | Error 0.2947(0.2960) Steps 658(656.65) | Grad Norm 6.3088(6.3341) | Total Time 0.00(0.00)\n",
      "Iter 2067 | Time 58.0588(61.5272) | Bit/dim 3.6760(3.6598) | Xent 0.8109(0.8264) | Loss 8.8328(9.6670) | Error 0.2857(0.2957) Steps 646(656.33) | Grad Norm 4.6110(6.2824) | Total Time 0.00(0.00)\n",
      "Iter 2068 | Time 62.7987(61.5653) | Bit/dim 3.6594(3.6598) | Xent 0.8486(0.8271) | Loss 9.1220(9.6507) | Error 0.3030(0.2959) Steps 688(657.28) | Grad Norm 10.1941(6.3998) | Total Time 0.00(0.00)\n",
      "Iter 2069 | Time 61.5229(61.5641) | Bit/dim 3.6652(3.6600) | Xent 0.8414(0.8275) | Loss 9.1304(9.6351) | Error 0.3039(0.2962) Steps 640(656.76) | Grad Norm 14.1877(6.6334) | Total Time 0.00(0.00)\n",
      "Iter 2070 | Time 61.4163(61.5596) | Bit/dim 3.6466(3.6596) | Xent 0.8551(0.8283) | Loss 9.1447(9.6204) | Error 0.3045(0.2964) Steps 628(655.90) | Grad Norm 9.5013(6.7194) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0345 | Time 23.8851, Epoch Time 409.0345(407.2444), Bit/dim 3.6592(best: 3.6563), Xent 0.8714, Loss 4.0949, Error 0.3041(best: 0.2974)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2071 | Time 64.1513(61.6374) | Bit/dim 3.6606(3.6596) | Xent 0.8467(0.8289) | Loss 12.7600(9.7145) | Error 0.3036(0.2966) Steps 628(655.06) | Grad Norm 7.9861(6.7574) | Total Time 0.00(0.00)\n",
      "Iter 2072 | Time 61.9254(61.6460) | Bit/dim 3.6467(3.6592) | Xent 0.8436(0.8293) | Loss 9.0490(9.6946) | Error 0.2999(0.2967) Steps 616(653.89) | Grad Norm 9.6396(6.8439) | Total Time 0.00(0.00)\n",
      "Iter 2073 | Time 63.1818(61.6921) | Bit/dim 3.6550(3.6591) | Xent 0.8388(0.8296) | Loss 9.1828(9.6792) | Error 0.3034(0.2969) Steps 670(654.37) | Grad Norm 6.8552(6.8442) | Total Time 0.00(0.00)\n",
      "Iter 2074 | Time 61.2938(61.6801) | Bit/dim 3.6630(3.6592) | Xent 0.8173(0.8292) | Loss 9.0530(9.6604) | Error 0.2889(0.2967) Steps 688(655.38) | Grad Norm 4.0558(6.7606) | Total Time 0.00(0.00)\n",
      "Iter 2075 | Time 60.9440(61.6581) | Bit/dim 3.6705(3.6595) | Xent 0.8262(0.8291) | Loss 9.1255(9.6444) | Error 0.2993(0.2968) Steps 676(656.00) | Grad Norm 7.3718(6.7789) | Total Time 0.00(0.00)\n",
      "Iter 2076 | Time 64.5825(61.7458) | Bit/dim 3.6587(3.6595) | Xent 0.8070(0.8285) | Loss 9.1925(9.6308) | Error 0.2919(0.2966) Steps 682(656.78) | Grad Norm 3.9987(6.6955) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0346 | Time 23.8320, Epoch Time 416.4544(407.5207), Bit/dim 3.6616(best: 3.6563), Xent 0.8555, Loss 4.0894, Error 0.3011(best: 0.2974)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2077 | Time 62.9773(61.7827) | Bit/dim 3.6564(3.6594) | Xent 0.8332(0.8286) | Loss 12.5590(9.7187) | Error 0.3014(0.2968) Steps 646(656.46) | Grad Norm 6.6228(6.6933) | Total Time 0.00(0.00)\n",
      "Iter 2078 | Time 59.7268(61.7211) | Bit/dim 3.6640(3.6596) | Xent 0.8149(0.8282) | Loss 9.2217(9.7038) | Error 0.2869(0.2965) Steps 676(657.04) | Grad Norm 4.1162(6.6160) | Total Time 0.00(0.00)\n",
      "Iter 2079 | Time 64.9900(61.8191) | Bit/dim 3.6674(3.6598) | Xent 0.7941(0.8272) | Loss 9.0949(9.6855) | Error 0.2836(0.2961) Steps 718(658.87) | Grad Norm 3.4395(6.5207) | Total Time 0.00(0.00)\n",
      "Iter 2080 | Time 63.9059(61.8817) | Bit/dim 3.6603(3.6598) | Xent 0.8140(0.8268) | Loss 9.0837(9.6674) | Error 0.2899(0.2959) Steps 658(658.85) | Grad Norm 6.5176(6.5206) | Total Time 0.00(0.00)\n",
      "Iter 2081 | Time 62.0873(61.8879) | Bit/dim 3.6483(3.6595) | Xent 0.8189(0.8266) | Loss 9.0121(9.6478) | Error 0.2936(0.2958) Steps 658(658.82) | Grad Norm 5.9666(6.5040) | Total Time 0.00(0.00)\n",
      "Iter 2082 | Time 65.7620(62.0041) | Bit/dim 3.6627(3.6596) | Xent 0.8164(0.8262) | Loss 9.1605(9.6332) | Error 0.2944(0.2958) Steps 628(657.90) | Grad Norm 5.5359(6.4750) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0347 | Time 23.1432, Epoch Time 418.6334(407.8541), Bit/dim 3.6639(best: 3.6563), Xent 0.8761, Loss 4.1019, Error 0.3095(best: 0.2974)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2083 | Time 58.8791(61.9104) | Bit/dim 3.6621(3.6596) | Xent 0.8316(0.8264) | Loss 12.0469(9.7056) | Error 0.3037(0.2960) Steps 646(657.54) | Grad Norm 9.8329(6.5757) | Total Time 0.00(0.00)\n",
      "Iter 2084 | Time 59.9929(61.8528) | Bit/dim 3.6634(3.6598) | Xent 0.8121(0.8260) | Loss 9.0217(9.6851) | Error 0.2876(0.2958) Steps 670(657.91) | Grad Norm 7.0496(6.5899) | Total Time 0.00(0.00)\n",
      "Iter 2085 | Time 60.7317(61.8192) | Bit/dim 3.6746(3.6602) | Xent 0.8148(0.8256) | Loss 9.1037(9.6676) | Error 0.2921(0.2957) Steps 658(657.92) | Grad Norm 10.3592(6.7030) | Total Time 0.00(0.00)\n",
      "Iter 2086 | Time 63.4456(61.8680) | Bit/dim 3.6655(3.6604) | Xent 0.8304(0.8258) | Loss 9.2648(9.6555) | Error 0.2947(0.2956) Steps 658(657.92) | Grad Norm 9.7432(6.7942) | Total Time 0.00(0.00)\n",
      "Iter 2087 | Time 58.8222(61.7766) | Bit/dim 3.6381(3.6597) | Xent 0.8163(0.8255) | Loss 8.8409(9.6311) | Error 0.2949(0.2956) Steps 634(657.20) | Grad Norm 6.6107(6.7887) | Total Time 0.00(0.00)\n",
      "Iter 2088 | Time 62.5010(61.7984) | Bit/dim 3.6478(3.6593) | Xent 0.8132(0.8251) | Loss 9.0569(9.6139) | Error 0.2875(0.2954) Steps 640(656.68) | Grad Norm 4.7912(6.7288) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0348 | Time 23.6525, Epoch Time 404.0398(407.7397), Bit/dim 3.6650(best: 3.6563), Xent 0.8627, Loss 4.0964, Error 0.3005(best: 0.2974)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2089 | Time 60.0911(61.7471) | Bit/dim 3.6701(3.6597) | Xent 0.8306(0.8253) | Loss 12.0833(9.6880) | Error 0.2916(0.2953) Steps 652(656.54) | Grad Norm 8.4151(6.7794) | Total Time 0.00(0.00)\n",
      "Iter 2090 | Time 67.8933(61.9315) | Bit/dim 3.6734(3.6601) | Xent 0.8162(0.8250) | Loss 9.2679(9.6754) | Error 0.2904(0.2951) Steps 682(657.31) | Grad Norm 7.7709(6.8091) | Total Time 0.00(0.00)\n",
      "Iter 2091 | Time 61.0194(61.9042) | Bit/dim 3.6681(3.6603) | Xent 0.8039(0.8244) | Loss 8.9577(9.6538) | Error 0.2859(0.2948) Steps 664(657.51) | Grad Norm 3.0021(6.6949) | Total Time 0.00(0.00)\n",
      "Iter 2092 | Time 64.8757(61.9933) | Bit/dim 3.6560(3.6602) | Xent 0.8236(0.8244) | Loss 9.2140(9.6406) | Error 0.2934(0.2948) Steps 634(656.80) | Grad Norm 10.3558(6.8047) | Total Time 0.00(0.00)\n",
      "Iter 2093 | Time 60.9029(61.9606) | Bit/dim 3.6505(3.6599) | Xent 0.8171(0.8241) | Loss 8.9770(9.6207) | Error 0.2924(0.2947) Steps 688(657.74) | Grad Norm 6.3082(6.7898) | Total Time 0.00(0.00)\n",
      "Iter 2094 | Time 63.5079(62.0070) | Bit/dim 3.6434(3.6594) | Xent 0.8197(0.8240) | Loss 9.0340(9.6031) | Error 0.3010(0.2949) Steps 652(657.57) | Grad Norm 5.7875(6.7598) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0349 | Time 23.4966, Epoch Time 418.1036(408.0506), Bit/dim 3.6527(best: 3.6563), Xent 0.8529, Loss 4.0792, Error 0.3001(best: 0.2974)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2095 | Time 58.9794(61.9162) | Bit/dim 3.6576(3.6593) | Xent 0.8014(0.8233) | Loss 12.6187(9.6936) | Error 0.2887(0.2947) Steps 652(657.40) | Grad Norm 5.8606(6.7328) | Total Time 0.00(0.00)\n",
      "Iter 2096 | Time 66.8900(62.0654) | Bit/dim 3.6382(3.6587) | Xent 0.7860(0.8222) | Loss 9.0526(9.6744) | Error 0.2776(0.2942) Steps 640(656.88) | Grad Norm 4.1335(6.6548) | Total Time 0.00(0.00)\n",
      "Iter 2097 | Time 57.0956(61.9163) | Bit/dim 3.6613(3.6588) | Xent 0.8166(0.8220) | Loss 9.1231(9.6578) | Error 0.2895(0.2941) Steps 670(657.27) | Grad Norm 5.2372(6.6123) | Total Time 0.00(0.00)\n",
      "Iter 2098 | Time 67.1530(62.0734) | Bit/dim 3.6564(3.6587) | Xent 0.8155(0.8218) | Loss 9.0519(9.6396) | Error 0.2984(0.2942) Steps 664(657.47) | Grad Norm 5.2207(6.5705) | Total Time 0.00(0.00)\n",
      "Iter 2099 | Time 61.3994(62.0532) | Bit/dim 3.6553(3.6586) | Xent 0.8268(0.8220) | Loss 9.0457(9.6218) | Error 0.2937(0.2942) Steps 628(656.59) | Grad Norm 8.0765(6.6157) | Total Time 0.00(0.00)\n",
      "Iter 2100 | Time 60.9493(62.0201) | Bit/dim 3.6807(3.6593) | Xent 0.7923(0.8211) | Loss 9.1177(9.6067) | Error 0.2870(0.2940) Steps 670(656.99) | Grad Norm 5.0932(6.5700) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0350 | Time 23.7786, Epoch Time 412.0493(408.1705), Bit/dim 3.6577(best: 3.6527), Xent 0.8554, Loss 4.0855, Error 0.3008(best: 0.2974)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2101 | Time 61.8939(62.0163) | Bit/dim 3.6534(3.6591) | Xent 0.8182(0.8210) | Loss 12.6543(9.6981) | Error 0.2887(0.2938) Steps 670(657.38) | Grad Norm 5.6369(6.5421) | Total Time 0.00(0.00)\n",
      "Iter 2102 | Time 62.9874(62.0454) | Bit/dim 3.6617(3.6592) | Xent 0.7841(0.8199) | Loss 9.0184(9.6777) | Error 0.2819(0.2935) Steps 682(658.12) | Grad Norm 4.1844(6.4713) | Total Time 0.00(0.00)\n",
      "Iter 2103 | Time 62.5216(62.0597) | Bit/dim 3.6473(3.6588) | Xent 0.8028(0.8194) | Loss 9.1457(9.6618) | Error 0.2817(0.2931) Steps 676(658.66) | Grad Norm 4.4575(6.4109) | Total Time 0.00(0.00)\n",
      "Iter 2104 | Time 61.8585(62.0537) | Bit/dim 3.6498(3.6585) | Xent 0.8106(0.8191) | Loss 9.1631(9.6468) | Error 0.2891(0.2930) Steps 658(658.64) | Grad Norm 4.0136(6.3390) | Total Time 0.00(0.00)\n",
      "Iter 2105 | Time 69.9504(62.2906) | Bit/dim 3.6523(3.6584) | Xent 0.8024(0.8186) | Loss 9.1149(9.6309) | Error 0.2881(0.2928) Steps 634(657.90) | Grad Norm 2.7497(6.2313) | Total Time 0.00(0.00)\n",
      "Iter 2106 | Time 57.6318(62.1508) | Bit/dim 3.6609(3.6584) | Xent 0.7956(0.8179) | Loss 9.0461(9.6133) | Error 0.2835(0.2926) Steps 646(657.54) | Grad Norm 4.0488(6.1658) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0351 | Time 23.3463, Epoch Time 416.6105(408.4237), Bit/dim 3.6547(best: 3.6527), Xent 0.8422, Loss 4.0758, Error 0.2970(best: 0.2974)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2107 | Time 58.0793(62.0287) | Bit/dim 3.6480(3.6581) | Xent 0.8033(0.8175) | Loss 12.5803(9.7023) | Error 0.2834(0.2923) Steps 658(657.55) | Grad Norm 4.8981(6.1278) | Total Time 0.00(0.00)\n",
      "Iter 2108 | Time 58.4328(61.9208) | Bit/dim 3.6549(3.6580) | Xent 0.7865(0.8166) | Loss 9.0622(9.6831) | Error 0.2784(0.2919) Steps 670(657.93) | Grad Norm 4.5307(6.0799) | Total Time 0.00(0.00)\n",
      "Iter 2109 | Time 62.0432(61.9245) | Bit/dim 3.6595(3.6581) | Xent 0.7866(0.8157) | Loss 8.8947(9.6595) | Error 0.2800(0.2915) Steps 664(658.11) | Grad Norm 2.9620(5.9863) | Total Time 0.00(0.00)\n",
      "Iter 2110 | Time 60.9150(61.8942) | Bit/dim 3.6578(3.6581) | Xent 0.8204(0.8158) | Loss 9.1692(9.6448) | Error 0.2923(0.2915) Steps 658(658.11) | Grad Norm 4.3365(5.9369) | Total Time 0.00(0.00)\n",
      "Iter 2111 | Time 65.1990(61.9933) | Bit/dim 3.6658(3.6583) | Xent 0.8226(0.8160) | Loss 9.1235(9.6291) | Error 0.2996(0.2918) Steps 664(658.28) | Grad Norm 7.4498(5.9822) | Total Time 0.00(0.00)\n",
      "Iter 2112 | Time 59.9867(61.9331) | Bit/dim 3.6492(3.6580) | Xent 0.8406(0.8168) | Loss 8.7682(9.6033) | Error 0.2980(0.2920) Steps 658(658.28) | Grad Norm 11.2654(6.1407) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0352 | Time 24.0232, Epoch Time 404.7761(408.3143), Bit/dim 3.6582(best: 3.6527), Xent 0.9107, Loss 4.1135, Error 0.3239(best: 0.2970)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2113 | Time 57.1151(61.7886) | Bit/dim 3.6601(3.6581) | Xent 0.8836(0.8188) | Loss 12.8223(9.6999) | Error 0.3151(0.2927) Steps 646(657.91) | Grad Norm 10.0030(6.2566) | Total Time 0.00(0.00)\n",
      "Iter 2114 | Time 65.3118(61.8943) | Bit/dim 3.6536(3.6579) | Xent 0.8201(0.8188) | Loss 9.0955(9.6817) | Error 0.2884(0.2925) Steps 682(658.63) | Grad Norm 5.8078(6.2431) | Total Time 0.00(0.00)\n",
      "Iter 2115 | Time 65.4416(62.0007) | Bit/dim 3.6526(3.6578) | Xent 0.8135(0.8186) | Loss 9.1644(9.6662) | Error 0.2910(0.2925) Steps 670(658.97) | Grad Norm 3.1835(6.1514) | Total Time 0.00(0.00)\n",
      "Iter 2116 | Time 58.3250(61.8904) | Bit/dim 3.6525(3.6576) | Xent 0.8190(0.8187) | Loss 9.1221(9.6499) | Error 0.2926(0.2925) Steps 676(659.48) | Grad Norm 4.8803(6.1132) | Total Time 0.00(0.00)\n",
      "Iter 2117 | Time 60.9992(61.8637) | Bit/dim 3.6563(3.6576) | Xent 0.8052(0.8182) | Loss 9.0274(9.6312) | Error 0.2865(0.2923) Steps 676(659.98) | Grad Norm 4.9391(6.0780) | Total Time 0.00(0.00)\n",
      "Iter 2118 | Time 57.9714(61.7469) | Bit/dim 3.6570(3.6576) | Xent 0.7852(0.8173) | Loss 9.1467(9.6167) | Error 0.2798(0.2919) Steps 640(659.38) | Grad Norm 3.5144(6.0011) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0353 | Time 23.0633, Epoch Time 404.2794(408.1933), Bit/dim 3.6571(best: 3.6527), Xent 0.8447, Loss 4.0795, Error 0.2992(best: 0.2970)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2119 | Time 60.7032(61.7156) | Bit/dim 3.6479(3.6573) | Xent 0.7893(0.8164) | Loss 12.4025(9.7002) | Error 0.2774(0.2915) Steps 676(659.88) | Grad Norm 3.0742(5.9133) | Total Time 0.00(0.00)\n",
      "Iter 2120 | Time 65.7750(61.8374) | Bit/dim 3.6634(3.6575) | Xent 0.7985(0.8159) | Loss 8.9680(9.6783) | Error 0.2855(0.2913) Steps 676(660.36) | Grad Norm 6.0024(5.9160) | Total Time 0.00(0.00)\n",
      "Iter 2121 | Time 62.3135(61.8517) | Bit/dim 3.6567(3.6574) | Xent 0.8177(0.8159) | Loss 8.9944(9.6578) | Error 0.2936(0.2914) Steps 616(659.03) | Grad Norm 6.5273(5.9343) | Total Time 0.00(0.00)\n",
      "Iter 2122 | Time 67.4656(62.0201) | Bit/dim 3.6639(3.6576) | Xent 0.7901(0.8152) | Loss 9.0917(9.6408) | Error 0.2824(0.2911) Steps 658(659.00) | Grad Norm 5.9636(5.9352) | Total Time 0.00(0.00)\n",
      "Iter 2123 | Time 60.9120(61.9869) | Bit/dim 3.6518(3.6575) | Xent 0.8380(0.8158) | Loss 9.1650(9.6265) | Error 0.3061(0.2916) Steps 652(658.79) | Grad Norm 7.1312(5.9711) | Total Time 0.00(0.00)\n",
      "Iter 2124 | Time 59.4322(61.9102) | Bit/dim 3.6505(3.6572) | Xent 0.8366(0.8165) | Loss 9.2375(9.6148) | Error 0.2966(0.2917) Steps 664(658.95) | Grad Norm 9.3659(6.0729) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0354 | Time 23.6103, Epoch Time 415.9976(408.4274), Bit/dim 3.6595(best: 3.6527), Xent 0.8412, Loss 4.0802, Error 0.2947(best: 0.2970)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2125 | Time 60.4794(61.8673) | Bit/dim 3.6581(3.6573) | Xent 0.8126(0.8164) | Loss 12.4797(9.7008) | Error 0.2901(0.2917) Steps 622(657.84) | Grad Norm 3.4198(5.9933) | Total Time 0.00(0.00)\n",
      "Iter 2126 | Time 61.4248(61.8540) | Bit/dim 3.6483(3.6570) | Xent 0.8147(0.8163) | Loss 9.0952(9.6826) | Error 0.2891(0.2916) Steps 682(658.56) | Grad Norm 6.8492(6.0190) | Total Time 0.00(0.00)\n",
      "Iter 2127 | Time 59.4071(61.7806) | Bit/dim 3.6572(3.6570) | Xent 0.8224(0.8165) | Loss 9.0304(9.6631) | Error 0.2990(0.2918) Steps 664(658.72) | Grad Norm 7.6350(6.0675) | Total Time 0.00(0.00)\n",
      "Iter 2128 | Time 60.3026(61.7363) | Bit/dim 3.6444(3.6566) | Xent 0.8032(0.8161) | Loss 9.0310(9.6441) | Error 0.2893(0.2917) Steps 640(658.16) | Grad Norm 4.4585(6.0192) | Total Time 0.00(0.00)\n",
      "Iter 2129 | Time 65.8400(61.8594) | Bit/dim 3.6503(3.6564) | Xent 0.7817(0.8151) | Loss 9.2179(9.6313) | Error 0.2871(0.2916) Steps 676(658.70) | Grad Norm 3.8295(5.9535) | Total Time 0.00(0.00)\n",
      "Iter 2130 | Time 61.1762(61.8389) | Bit/dim 3.6667(3.6567) | Xent 0.8013(0.8146) | Loss 9.1784(9.6177) | Error 0.2829(0.2913) Steps 658(658.68) | Grad Norm 4.7699(5.9180) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0355 | Time 23.1952, Epoch Time 407.8915(408.4113), Bit/dim 3.6584(best: 3.6527), Xent 0.8722, Loss 4.0945, Error 0.3068(best: 0.2947)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2131 | Time 58.9054(61.7509) | Bit/dim 3.6601(3.6568) | Xent 0.8190(0.8148) | Loss 12.3231(9.6989) | Error 0.2924(0.2914) Steps 646(658.30) | Grad Norm 7.8825(5.9769) | Total Time 0.00(0.00)\n",
      "Iter 2132 | Time 59.6741(61.6886) | Bit/dim 3.6722(3.6573) | Xent 0.7926(0.8141) | Loss 9.1899(9.6836) | Error 0.2833(0.2911) Steps 670(658.65) | Grad Norm 8.4522(6.0512) | Total Time 0.00(0.00)\n",
      "Iter 2133 | Time 63.3875(61.7395) | Bit/dim 3.6599(3.6574) | Xent 0.8077(0.8139) | Loss 9.1252(9.6669) | Error 0.2896(0.2911) Steps 640(658.09) | Grad Norm 4.7095(6.0109) | Total Time 0.00(0.00)\n",
      "Iter 2134 | Time 60.4292(61.7002) | Bit/dim 3.6481(3.6571) | Xent 0.7767(0.8128) | Loss 9.1191(9.6504) | Error 0.2768(0.2906) Steps 652(657.91) | Grad Norm 3.3943(5.9324) | Total Time 0.00(0.00)\n",
      "Iter 2135 | Time 61.2416(61.6865) | Bit/dim 3.6613(3.6572) | Xent 0.7917(0.8122) | Loss 9.0033(9.6310) | Error 0.2855(0.2905) Steps 676(658.45) | Grad Norm 3.0216(5.8451) | Total Time 0.00(0.00)\n",
      "Iter 2136 | Time 60.0692(61.6380) | Bit/dim 3.6414(3.6568) | Xent 0.8115(0.8121) | Loss 9.0608(9.6139) | Error 0.2941(0.2906) Steps 652(658.26) | Grad Norm 4.0130(5.7902) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0356 | Time 23.3777, Epoch Time 403.0266(408.2498), Bit/dim 3.6552(best: 3.6527), Xent 0.8496, Loss 4.0800, Error 0.2979(best: 0.2947)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2137 | Time 59.4028(61.5709) | Bit/dim 3.6553(3.6567) | Xent 0.8032(0.8119) | Loss 12.7683(9.7085) | Error 0.2924(0.2907) Steps 670(658.61) | Grad Norm 5.5116(5.7818) | Total Time 0.00(0.00)\n",
      "Iter 2138 | Time 62.9925(61.6135) | Bit/dim 3.6620(3.6569) | Xent 0.7958(0.8114) | Loss 8.9804(9.6867) | Error 0.2870(0.2905) Steps 640(658.05) | Grad Norm 7.5933(5.8361) | Total Time 0.00(0.00)\n",
      "Iter 2139 | Time 59.0782(61.5375) | Bit/dim 3.6445(3.6565) | Xent 0.8088(0.8113) | Loss 9.0158(9.6666) | Error 0.2917(0.2906) Steps 670(658.41) | Grad Norm 7.2358(5.8781) | Total Time 0.00(0.00)\n",
      "Iter 2140 | Time 63.2459(61.5887) | Bit/dim 3.6519(3.6564) | Xent 0.7994(0.8110) | Loss 8.8926(9.6433) | Error 0.2900(0.2906) Steps 670(658.76) | Grad Norm 3.8160(5.8163) | Total Time 0.00(0.00)\n",
      "Iter 2141 | Time 62.4622(61.6149) | Bit/dim 3.6596(3.6565) | Xent 0.8168(0.8111) | Loss 9.1595(9.6288) | Error 0.2943(0.2907) Steps 670(659.09) | Grad Norm 4.6252(5.7805) | Total Time 0.00(0.00)\n",
      "Iter 2142 | Time 64.5981(61.7044) | Bit/dim 3.6627(3.6566) | Xent 0.8084(0.8111) | Loss 9.2129(9.6163) | Error 0.2917(0.2907) Steps 664(659.24) | Grad Norm 6.2332(5.7941) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0357 | Time 23.4291, Epoch Time 411.9390(408.3604), Bit/dim 3.6574(best: 3.6527), Xent 0.8563, Loss 4.0856, Error 0.3012(best: 0.2947)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2143 | Time 60.2322(61.6603) | Bit/dim 3.6485(3.6564) | Xent 0.8046(0.8109) | Loss 12.3839(9.6994) | Error 0.2914(0.2907) Steps 640(658.66) | Grad Norm 5.5965(5.7882) | Total Time 0.00(0.00)\n",
      "Iter 2144 | Time 63.1513(61.7050) | Bit/dim 3.6624(3.6566) | Xent 0.8025(0.8106) | Loss 9.1868(9.6840) | Error 0.2874(0.2906) Steps 634(657.92) | Grad Norm 4.9258(5.7623) | Total Time 0.00(0.00)\n",
      "Iter 2145 | Time 62.5582(61.7306) | Bit/dim 3.6709(3.6570) | Xent 0.7937(0.8101) | Loss 9.0486(9.6649) | Error 0.2859(0.2905) Steps 640(657.39) | Grad Norm 7.1693(5.8045) | Total Time 0.00(0.00)\n",
      "Iter 2146 | Time 60.6126(61.6971) | Bit/dim 3.6541(3.6569) | Xent 0.8181(0.8103) | Loss 9.1521(9.6496) | Error 0.2906(0.2905) Steps 682(658.12) | Grad Norm 6.1249(5.8141) | Total Time 0.00(0.00)\n",
      "Iter 2147 | Time 63.7847(61.7597) | Bit/dim 3.6482(3.6567) | Xent 0.7802(0.8094) | Loss 9.1582(9.6348) | Error 0.2748(0.2900) Steps 652(657.94) | Grad Norm 2.9763(5.7290) | Total Time 0.00(0.00)\n",
      "Iter 2148 | Time 61.7065(61.7581) | Bit/dim 3.6551(3.6566) | Xent 0.7890(0.8088) | Loss 9.2353(9.6228) | Error 0.2853(0.2899) Steps 688(658.84) | Grad Norm 3.4296(5.6600) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0358 | Time 23.9259, Epoch Time 412.4788(408.4840), Bit/dim 3.6605(best: 3.6527), Xent 0.8504, Loss 4.0857, Error 0.3013(best: 0.2947)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2149 | Time 57.8881(61.6420) | Bit/dim 3.6550(3.6566) | Xent 0.7853(0.8081) | Loss 12.3051(9.7033) | Error 0.2804(0.2896) Steps 640(658.28) | Grad Norm 4.9507(5.6387) | Total Time 0.00(0.00)\n",
      "Iter 2150 | Time 61.0367(61.6238) | Bit/dim 3.6517(3.6564) | Xent 0.8128(0.8083) | Loss 9.1954(9.6881) | Error 0.2885(0.2896) Steps 658(658.27) | Grad Norm 2.8426(5.5549) | Total Time 0.00(0.00)\n",
      "Iter 2151 | Time 64.1861(61.7007) | Bit/dim 3.6668(3.6567) | Xent 0.7641(0.8069) | Loss 9.0894(9.6701) | Error 0.2740(0.2891) Steps 634(657.54) | Grad Norm 5.7367(5.5603) | Total Time 0.00(0.00)\n",
      "Iter 2152 | Time 65.7207(61.8213) | Bit/dim 3.6644(3.6570) | Xent 0.8175(0.8073) | Loss 9.2667(9.6580) | Error 0.2957(0.2893) Steps 670(657.91) | Grad Norm 8.3454(5.6439) | Total Time 0.00(0.00)\n",
      "Iter 2153 | Time 59.6234(61.7554) | Bit/dim 3.6547(3.6569) | Xent 0.7896(0.8067) | Loss 8.9921(9.6380) | Error 0.2781(0.2890) Steps 634(657.20) | Grad Norm 6.6886(5.6752) | Total Time 0.00(0.00)\n",
      "Iter 2154 | Time 62.5292(61.7786) | Bit/dim 3.6383(3.6563) | Xent 0.8044(0.8067) | Loss 8.9446(9.6172) | Error 0.2896(0.2890) Steps 628(656.32) | Grad Norm 2.2510(5.5725) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0359 | Time 24.3225, Epoch Time 411.2202(408.5661), Bit/dim 3.6548(best: 3.6527), Xent 0.8410, Loss 4.0753, Error 0.2935(best: 0.2947)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2155 | Time 58.1153(61.6687) | Bit/dim 3.6585(3.6564) | Xent 0.8030(0.8065) | Loss 12.2641(9.6966) | Error 0.2889(0.2890) Steps 634(655.65) | Grad Norm 7.3403(5.6255) | Total Time 0.00(0.00)\n",
      "Iter 2156 | Time 62.6239(61.6973) | Bit/dim 3.6497(3.6562) | Xent 0.8054(0.8065) | Loss 9.0651(9.6777) | Error 0.2816(0.2888) Steps 676(656.26) | Grad Norm 6.7478(5.6592) | Total Time 0.00(0.00)\n",
      "Iter 2157 | Time 61.0512(61.6780) | Bit/dim 3.6546(3.6562) | Xent 0.8098(0.8066) | Loss 8.8423(9.6526) | Error 0.2851(0.2886) Steps 670(656.67) | Grad Norm 6.7623(5.6923) | Total Time 0.00(0.00)\n",
      "Iter 2158 | Time 63.3569(61.7283) | Bit/dim 3.6543(3.6561) | Xent 0.8899(0.8091) | Loss 9.2381(9.6402) | Error 0.3210(0.2896) Steps 628(655.81) | Grad Norm 12.0170(5.8820) | Total Time 0.00(0.00)\n",
      "Iter 2159 | Time 58.1138(61.6199) | Bit/dim 3.6688(3.6565) | Xent 0.8825(0.8113) | Loss 8.9931(9.6208) | Error 0.3104(0.2902) Steps 682(656.60) | Grad Norm 15.4459(6.1689) | Total Time 0.00(0.00)\n",
      "Iter 2160 | Time 58.3234(61.5210) | Bit/dim 3.6601(3.6566) | Xent 0.8773(0.8133) | Loss 8.8242(9.5969) | Error 0.3155(0.2910) Steps 622(655.56) | Grad Norm 11.7620(6.3367) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0360 | Time 22.5620, Epoch Time 400.2104(408.3154), Bit/dim 3.6599(best: 3.6527), Xent 0.9228, Loss 4.1213, Error 0.3236(best: 0.2935)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2161 | Time 58.4959(61.4302) | Bit/dim 3.6589(3.6567) | Xent 0.8791(0.8153) | Loss 12.7797(9.6924) | Error 0.3166(0.2918) Steps 640(655.09) | Grad Norm 12.9466(6.5350) | Total Time 0.00(0.00)\n",
      "Iter 2162 | Time 66.0196(61.5679) | Bit/dim 3.6502(3.6565) | Xent 0.8698(0.8169) | Loss 8.9621(9.6704) | Error 0.3099(0.2923) Steps 670(655.54) | Grad Norm 11.9378(6.6971) | Total Time 0.00(0.00)\n",
      "Iter 2163 | Time 65.3043(61.6800) | Bit/dim 3.6583(3.6565) | Xent 0.8391(0.8176) | Loss 9.2510(9.6579) | Error 0.3057(0.2927) Steps 634(654.90) | Grad Norm 8.3776(6.7475) | Total Time 0.00(0.00)\n",
      "Iter 2164 | Time 62.9673(61.7186) | Bit/dim 3.6619(3.6567) | Xent 0.8689(0.8191) | Loss 9.2575(9.6458) | Error 0.3099(0.2932) Steps 646(654.63) | Grad Norm 11.6707(6.8952) | Total Time 0.00(0.00)\n",
      "Iter 2165 | Time 57.2850(61.5856) | Bit/dim 3.6814(3.6574) | Xent 0.8079(0.8188) | Loss 8.8255(9.6212) | Error 0.2919(0.2932) Steps 658(654.73) | Grad Norm 9.6138(6.9768) | Total Time 0.00(0.00)\n",
      "Iter 2166 | Time 61.4032(61.5802) | Bit/dim 3.6579(3.6574) | Xent 0.8322(0.8192) | Loss 9.0243(9.6033) | Error 0.2949(0.2932) Steps 628(653.93) | Grad Norm 5.9044(6.9446) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0361 | Time 24.0361, Epoch Time 410.9802(408.3954), Bit/dim 3.6615(best: 3.6527), Xent 0.8675, Loss 4.0953, Error 0.3016(best: 0.2935)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2167 | Time 58.3293(61.4826) | Bit/dim 3.6547(3.6574) | Xent 0.8219(0.8193) | Loss 12.2618(9.6831) | Error 0.2970(0.2933) Steps 646(653.69) | Grad Norm 8.9342(7.0043) | Total Time 0.00(0.00)\n",
      "Iter 2168 | Time 62.6832(61.5186) | Bit/dim 3.6552(3.6573) | Xent 0.8088(0.8189) | Loss 9.0844(9.6651) | Error 0.2990(0.2935) Steps 652(653.64) | Grad Norm 3.9176(6.9117) | Total Time 0.00(0.00)\n",
      "Iter 2169 | Time 64.2458(61.6005) | Bit/dim 3.6720(3.6577) | Xent 0.8328(0.8194) | Loss 9.2183(9.6517) | Error 0.3013(0.2937) Steps 646(653.41) | Grad Norm 6.7569(6.9070) | Total Time 0.00(0.00)\n",
      "Iter 2170 | Time 59.9067(61.5496) | Bit/dim 3.6616(3.6578) | Xent 0.7984(0.8187) | Loss 8.9993(9.6321) | Error 0.2879(0.2936) Steps 664(653.73) | Grad Norm 5.1411(6.8541) | Total Time 0.00(0.00)\n",
      "Iter 2171 | Time 62.4082(61.5754) | Bit/dim 3.6490(3.6576) | Xent 0.8255(0.8189) | Loss 8.8312(9.6081) | Error 0.2957(0.2936) Steps 670(654.22) | Grad Norm 5.4594(6.8122) | Total Time 0.00(0.00)\n",
      "Iter 2172 | Time 62.0686(61.5902) | Bit/dim 3.6698(3.6579) | Xent 0.8097(0.8187) | Loss 9.1603(9.5947) | Error 0.2944(0.2937) Steps 658(654.33) | Grad Norm 4.5777(6.7452) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0362 | Time 24.1452, Epoch Time 410.2815(408.4519), Bit/dim 3.6587(best: 3.6527), Xent 0.8613, Loss 4.0893, Error 0.3040(best: 0.2935)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2173 | Time 63.4887(61.6472) | Bit/dim 3.6586(3.6580) | Xent 0.8211(0.8187) | Loss 12.5606(9.6837) | Error 0.2960(0.2937) Steps 682(655.16) | Grad Norm 7.0391(6.7540) | Total Time 0.00(0.00)\n",
      "Iter 2174 | Time 64.6776(61.7381) | Bit/dim 3.6513(3.6578) | Xent 0.7950(0.8180) | Loss 9.1479(9.6676) | Error 0.2854(0.2935) Steps 694(656.32) | Grad Norm 7.0018(6.7614) | Total Time 0.00(0.00)\n",
      "Iter 2175 | Time 59.6601(61.6757) | Bit/dim 3.6634(3.6579) | Xent 0.8119(0.8178) | Loss 9.0152(9.6480) | Error 0.2884(0.2933) Steps 664(656.55) | Grad Norm 4.3534(6.6892) | Total Time 0.00(0.00)\n",
      "Iter 2176 | Time 58.4633(61.5794) | Bit/dim 3.6630(3.6581) | Xent 0.7967(0.8172) | Loss 9.0191(9.6291) | Error 0.2864(0.2931) Steps 682(657.32) | Grad Norm 6.4277(6.6814) | Total Time 0.00(0.00)\n",
      "Iter 2177 | Time 56.7872(61.4356) | Bit/dim 3.6537(3.6580) | Xent 0.7816(0.8161) | Loss 8.9662(9.6093) | Error 0.2749(0.2926) Steps 646(656.98) | Grad Norm 6.6786(6.6813) | Total Time 0.00(0.00)\n",
      "Iter 2178 | Time 57.9099(61.3298) | Bit/dim 3.6491(3.6577) | Xent 0.8049(0.8158) | Loss 8.8755(9.5872) | Error 0.2847(0.2923) Steps 658(657.01) | Grad Norm 3.8046(6.5950) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0363 | Time 23.1457, Epoch Time 399.9460(408.1968), Bit/dim 3.6656(best: 3.6527), Xent 0.8474, Loss 4.0893, Error 0.3003(best: 0.2935)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2179 | Time 60.0829(61.2924) | Bit/dim 3.6551(3.6576) | Xent 0.8096(0.8156) | Loss 12.1589(9.6644) | Error 0.2826(0.2920) Steps 628(656.14) | Grad Norm 6.6843(6.5976) | Total Time 0.00(0.00)\n",
      "Iter 2180 | Time 57.3602(61.1744) | Bit/dim 3.6575(3.6576) | Xent 0.7702(0.8142) | Loss 8.7356(9.6365) | Error 0.2718(0.2914) Steps 634(655.47) | Grad Norm 3.8004(6.5137) | Total Time 0.00(0.00)\n",
      "Iter 2181 | Time 61.9601(61.1980) | Bit/dim 3.6475(3.6573) | Xent 0.7879(0.8135) | Loss 9.1131(9.6208) | Error 0.2829(0.2912) Steps 676(656.09) | Grad Norm 5.9170(6.4958) | Total Time 0.00(0.00)\n",
      "Iter 2182 | Time 65.2069(61.3183) | Bit/dim 3.6580(3.6573) | Xent 0.8149(0.8135) | Loss 9.2512(9.6097) | Error 0.2909(0.2912) Steps 664(656.33) | Grad Norm 8.5777(6.5583) | Total Time 0.00(0.00)\n",
      "Iter 2183 | Time 66.2299(61.4656) | Bit/dim 3.6574(3.6573) | Xent 0.7758(0.8124) | Loss 9.0975(9.5944) | Error 0.2764(0.2907) Steps 670(656.74) | Grad Norm 3.1826(6.4570) | Total Time 0.00(0.00)\n",
      "Iter 2184 | Time 65.9366(61.5998) | Bit/dim 3.6544(3.6572) | Xent 0.7986(0.8120) | Loss 9.1237(9.5803) | Error 0.2865(0.2906) Steps 640(656.24) | Grad Norm 4.6065(6.4015) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0364 | Time 23.4852, Epoch Time 416.2901(408.4396), Bit/dim 3.6566(best: 3.6527), Xent 0.8300, Loss 4.0716, Error 0.2917(best: 0.2935)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2185 | Time 61.6440(61.6011) | Bit/dim 3.6586(3.6573) | Xent 0.7958(0.8115) | Loss 12.1510(9.6574) | Error 0.2911(0.2906) Steps 658(656.29) | Grad Norm 3.0485(6.3009) | Total Time 0.00(0.00)\n",
      "Iter 2186 | Time 60.9826(61.5825) | Bit/dim 3.6596(3.6574) | Xent 0.7973(0.8110) | Loss 8.9279(9.6355) | Error 0.2821(0.2904) Steps 670(656.70) | Grad Norm 4.3398(6.2421) | Total Time 0.00(0.00)\n",
      "Iter 2187 | Time 60.0377(61.5362) | Bit/dim 3.6543(3.6573) | Xent 0.7794(0.8101) | Loss 8.9948(9.6163) | Error 0.2812(0.2901) Steps 658(656.74) | Grad Norm 4.1865(6.1804) | Total Time 0.00(0.00)\n",
      "Iter 2188 | Time 65.2238(61.6468) | Bit/dim 3.6528(3.6571) | Xent 0.7782(0.8091) | Loss 9.2387(9.6049) | Error 0.2801(0.2898) Steps 694(657.86) | Grad Norm 3.0060(6.0852) | Total Time 0.00(0.00)\n",
      "Iter 2189 | Time 60.2250(61.6042) | Bit/dim 3.6661(3.6574) | Xent 0.7989(0.8088) | Loss 9.1346(9.5908) | Error 0.2855(0.2897) Steps 700(659.12) | Grad Norm 5.8640(6.0785) | Total Time 0.00(0.00)\n",
      "Iter 2190 | Time 61.3601(61.5968) | Bit/dim 3.6495(3.6572) | Xent 0.7881(0.8082) | Loss 8.9098(9.5704) | Error 0.2816(0.2894) Steps 640(658.55) | Grad Norm 4.5711(6.0333) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0365 | Time 23.5909, Epoch Time 409.1039(408.4595), Bit/dim 3.6550(best: 3.6527), Xent 0.8461, Loss 4.0781, Error 0.2960(best: 0.2917)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2191 | Time 62.6695(61.6290) | Bit/dim 3.6530(3.6570) | Xent 0.8176(0.8085) | Loss 12.7082(9.6645) | Error 0.2895(0.2894) Steps 652(658.35) | Grad Norm 6.1259(6.0361) | Total Time 0.00(0.00)\n",
      "Iter 2192 | Time 61.0661(61.6121) | Bit/dim 3.6524(3.6569) | Xent 0.7863(0.8078) | Loss 8.9431(9.6429) | Error 0.2795(0.2891) Steps 664(658.52) | Grad Norm 3.8321(5.9700) | Total Time 0.00(0.00)\n",
      "Iter 2193 | Time 59.6892(61.5544) | Bit/dim 3.6496(3.6567) | Xent 0.7835(0.8071) | Loss 9.0745(9.6258) | Error 0.2825(0.2889) Steps 628(657.61) | Grad Norm 4.7230(5.9326) | Total Time 0.00(0.00)\n",
      "Iter 2194 | Time 63.6332(61.6168) | Bit/dim 3.6560(3.6567) | Xent 0.8056(0.8070) | Loss 9.0928(9.6099) | Error 0.2891(0.2889) Steps 694(658.70) | Grad Norm 7.5907(5.9823) | Total Time 0.00(0.00)\n",
      "Iter 2195 | Time 59.3106(61.5476) | Bit/dim 3.6612(3.6568) | Xent 0.7756(0.8061) | Loss 8.9323(9.5895) | Error 0.2745(0.2885) Steps 670(659.04) | Grad Norm 4.1267(5.9266) | Total Time 0.00(0.00)\n",
      "Iter 2196 | Time 61.3374(61.5413) | Bit/dim 3.6609(3.6569) | Xent 0.7871(0.8055) | Loss 9.1548(9.5765) | Error 0.2785(0.2882) Steps 670(659.36) | Grad Norm 4.4266(5.8816) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0366 | Time 23.8867, Epoch Time 407.5304(408.4316), Bit/dim 3.6562(best: 3.6527), Xent 0.8399, Loss 4.0762, Error 0.2953(best: 0.2917)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2197 | Time 61.2899(61.5338) | Bit/dim 3.6629(3.6571) | Xent 0.7814(0.8048) | Loss 12.4898(9.6639) | Error 0.2798(0.2879) Steps 646(658.96) | Grad Norm 5.6160(5.8737) | Total Time 0.00(0.00)\n",
      "Iter 2198 | Time 61.5835(61.5353) | Bit/dim 3.6407(3.6566) | Xent 0.8101(0.8050) | Loss 8.8962(9.6409) | Error 0.2947(0.2881) Steps 682(659.66) | Grad Norm 4.8221(5.8421) | Total Time 0.00(0.00)\n",
      "Iter 2199 | Time 64.2709(61.6173) | Bit/dim 3.6459(3.6563) | Xent 0.8046(0.8050) | Loss 9.1583(9.6264) | Error 0.2864(0.2881) Steps 706(661.05) | Grad Norm 5.2558(5.8245) | Total Time 0.00(0.00)\n",
      "Iter 2200 | Time 65.0871(61.7214) | Bit/dim 3.6584(3.6563) | Xent 0.7939(0.8046) | Loss 9.1112(9.6109) | Error 0.2784(0.2878) Steps 664(661.13) | Grad Norm 6.0656(5.8318) | Total Time 0.00(0.00)\n",
      "Iter 2201 | Time 63.7153(61.7812) | Bit/dim 3.6547(3.6563) | Xent 0.7973(0.8044) | Loss 9.0004(9.5926) | Error 0.2823(0.2876) Steps 682(661.76) | Grad Norm 6.4675(5.8508) | Total Time 0.00(0.00)\n",
      "Iter 2202 | Time 62.5917(61.8056) | Bit/dim 3.6607(3.6564) | Xent 0.8071(0.8045) | Loss 9.0143(9.5753) | Error 0.2904(0.2877) Steps 682(662.37) | Grad Norm 6.5556(5.8720) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0367 | Time 24.1224, Epoch Time 418.9727(408.7479), Bit/dim 3.6564(best: 3.6527), Xent 0.8686, Loss 4.0907, Error 0.3057(best: 0.2917)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2203 | Time 59.1028(61.7245) | Bit/dim 3.6557(3.6564) | Xent 0.8508(0.8059) | Loss 12.5456(9.6644) | Error 0.3050(0.2882) Steps 658(662.24) | Grad Norm 8.3420(5.9461) | Total Time 0.00(0.00)\n",
      "Iter 2204 | Time 62.2437(61.7401) | Bit/dim 3.6543(3.6563) | Xent 0.8473(0.8071) | Loss 9.1766(9.6497) | Error 0.3051(0.2887) Steps 700(663.37) | Grad Norm 9.9184(6.0653) | Total Time 0.00(0.00)\n",
      "Iter 2205 | Time 62.1464(61.7522) | Bit/dim 3.6543(3.6563) | Xent 0.8798(0.8093) | Loss 9.0075(9.6305) | Error 0.3173(0.2896) Steps 646(662.85) | Grad Norm 16.4049(6.3754) | Total Time 0.00(0.00)\n",
      "Iter 2206 | Time 60.5921(61.7174) | Bit/dim 3.6587(3.6564) | Xent 0.8734(0.8112) | Loss 9.3175(9.6211) | Error 0.3109(0.2902) Steps 676(663.24) | Grad Norm 15.1674(6.6392) | Total Time 0.00(0.00)\n",
      "Iter 2207 | Time 61.1500(61.7004) | Bit/dim 3.6634(3.6566) | Xent 0.8108(0.8112) | Loss 8.9929(9.6022) | Error 0.2863(0.2901) Steps 670(663.45) | Grad Norm 9.5761(6.7273) | Total Time 0.00(0.00)\n",
      "Iter 2208 | Time 63.3006(61.7484) | Bit/dim 3.6700(3.6570) | Xent 0.8791(0.8132) | Loss 9.2987(9.5931) | Error 0.3126(0.2908) Steps 682(664.00) | Grad Norm 15.2713(6.9836) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0368 | Time 24.0105, Epoch Time 408.2726(408.7336), Bit/dim 3.6582(best: 3.6527), Xent 0.8409, Loss 4.0787, Error 0.2969(best: 0.2917)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2209 | Time 62.7370(61.7781) | Bit/dim 3.6652(3.6572) | Xent 0.8136(0.8133) | Loss 12.8700(9.6914) | Error 0.2884(0.2907) Steps 658(663.82) | Grad Norm 5.0112(6.9245) | Total Time 0.00(0.00)\n",
      "Iter 2210 | Time 64.6278(61.8636) | Bit/dim 3.6674(3.6575) | Xent 0.8371(0.8140) | Loss 9.1384(9.6748) | Error 0.3005(0.2910) Steps 688(664.55) | Grad Norm 9.9355(7.0148) | Total Time 0.00(0.00)\n",
      "Iter 2211 | Time 66.5894(62.0053) | Bit/dim 3.6496(3.6573) | Xent 0.8150(0.8140) | Loss 9.0957(9.6575) | Error 0.2867(0.2909) Steps 664(664.53) | Grad Norm 8.4227(7.0570) | Total Time 0.00(0.00)\n",
      "Iter 2212 | Time 60.8381(61.9703) | Bit/dim 3.6514(3.6571) | Xent 0.8370(0.8147) | Loss 8.7361(9.6298) | Error 0.3041(0.2913) Steps 646(663.97) | Grad Norm 12.5056(7.2205) | Total Time 0.00(0.00)\n",
      "Iter 2213 | Time 60.2454(61.9186) | Bit/dim 3.6570(3.6571) | Xent 0.7936(0.8141) | Loss 9.0537(9.6125) | Error 0.2819(0.2910) Steps 658(663.80) | Grad Norm 6.3437(7.1942) | Total Time 0.00(0.00)\n",
      "Iter 2214 | Time 63.7196(61.9726) | Bit/dim 3.6509(3.6569) | Xent 0.8104(0.8140) | Loss 9.2387(9.6013) | Error 0.2907(0.2910) Steps 652(663.44) | Grad Norm 7.2543(7.1960) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0369 | Time 23.8183, Epoch Time 418.3689(409.0227), Bit/dim 3.6530(best: 3.6527), Xent 0.8622, Loss 4.0841, Error 0.3051(best: 0.2917)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2215 | Time 63.4920(62.0182) | Bit/dim 3.6538(3.6568) | Xent 0.8136(0.8139) | Loss 12.2619(9.6811) | Error 0.2846(0.2908) Steps 658(663.28) | Grad Norm 6.7991(7.1841) | Total Time 0.00(0.00)\n",
      "Iter 2216 | Time 61.1682(61.9927) | Bit/dim 3.6604(3.6569) | Xent 0.8163(0.8140) | Loss 9.0567(9.6624) | Error 0.2885(0.2907) Steps 658(663.12) | Grad Norm 7.8061(7.2027) | Total Time 0.00(0.00)\n",
      "Iter 2217 | Time 61.4605(61.9767) | Bit/dim 3.6501(3.6567) | Xent 0.8004(0.8136) | Loss 9.1044(9.6457) | Error 0.2861(0.2906) Steps 694(664.05) | Grad Norm 6.4392(7.1798) | Total Time 0.00(0.00)\n",
      "Iter 2218 | Time 61.2529(61.9550) | Bit/dim 3.6608(3.6568) | Xent 0.8396(0.8144) | Loss 9.1113(9.6296) | Error 0.2983(0.2908) Steps 670(664.23) | Grad Norm 10.5121(7.2798) | Total Time 0.00(0.00)\n",
      "Iter 2219 | Time 61.2819(61.9348) | Bit/dim 3.6585(3.6569) | Xent 0.8178(0.8145) | Loss 9.1896(9.6164) | Error 0.2935(0.2909) Steps 676(664.58) | Grad Norm 7.8440(7.2967) | Total Time 0.00(0.00)\n",
      "Iter 2220 | Time 60.4004(61.8888) | Bit/dim 3.6442(3.6565) | Xent 0.8094(0.8143) | Loss 9.0556(9.5996) | Error 0.2911(0.2909) Steps 676(664.92) | Grad Norm 6.4088(7.2701) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0370 | Time 24.1881, Epoch Time 409.4051(409.0341), Bit/dim 3.6569(best: 3.6527), Xent 0.8599, Loss 4.0869, Error 0.3052(best: 0.2917)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2221 | Time 59.5373(61.8182) | Bit/dim 3.6689(3.6569) | Xent 0.8010(0.8139) | Loss 12.4963(9.6865) | Error 0.2875(0.2908) Steps 688(665.61) | Grad Norm 7.2870(7.2706) | Total Time 0.00(0.00)\n",
      "Iter 2222 | Time 60.9129(61.7911) | Bit/dim 3.6511(3.6567) | Xent 0.8032(0.8136) | Loss 9.0079(9.6662) | Error 0.2910(0.2908) Steps 688(666.29) | Grad Norm 9.5442(7.3388) | Total Time 0.00(0.00)\n",
      "Iter 2223 | Time 66.1890(61.9230) | Bit/dim 3.6514(3.6566) | Xent 0.8050(0.8134) | Loss 9.0846(9.6487) | Error 0.2939(0.2909) Steps 670(666.40) | Grad Norm 8.2247(7.3654) | Total Time 0.00(0.00)\n",
      "Iter 2224 | Time 63.9459(61.9837) | Bit/dim 3.6569(3.6566) | Xent 0.8141(0.8134) | Loss 8.7762(9.6225) | Error 0.2936(0.2910) Steps 634(665.42) | Grad Norm 8.5269(7.4002) | Total Time 0.00(0.00)\n",
      "Iter 2225 | Time 61.7292(61.9761) | Bit/dim 3.6546(3.6565) | Xent 0.8092(0.8133) | Loss 9.0062(9.6040) | Error 0.2899(0.2910) Steps 676(665.74) | Grad Norm 6.4458(7.3716) | Total Time 0.00(0.00)\n",
      "Iter 2226 | Time 63.3574(62.0175) | Bit/dim 3.6585(3.6566) | Xent 0.8198(0.8134) | Loss 9.1602(9.5907) | Error 0.2900(0.2909) Steps 634(664.79) | Grad Norm 7.6031(7.3785) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0371 | Time 23.8173, Epoch Time 415.6090(409.2314), Bit/dim 3.6547(best: 3.6527), Xent 0.8499, Loss 4.0796, Error 0.3027(best: 0.2917)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2227 | Time 57.6927(61.8878) | Bit/dim 3.6575(3.6566) | Xent 0.8380(0.8142) | Loss 12.6748(9.6833) | Error 0.2993(0.2912) Steps 682(665.31) | Grad Norm 5.6531(7.3268) | Total Time 0.00(0.00)\n",
      "Iter 2228 | Time 61.8185(61.8857) | Bit/dim 3.6501(3.6564) | Xent 0.7886(0.8134) | Loss 9.0274(9.6636) | Error 0.2816(0.2909) Steps 646(664.73) | Grad Norm 7.4976(7.3319) | Total Time 0.00(0.00)\n",
      "Iter 2229 | Time 64.2544(61.9567) | Bit/dim 3.6464(3.6561) | Xent 0.7711(0.8121) | Loss 9.0813(9.6461) | Error 0.2765(0.2905) Steps 676(665.06) | Grad Norm 6.1276(7.2958) | Total Time 0.00(0.00)\n",
      "Iter 2230 | Time 56.5886(61.7957) | Bit/dim 3.6588(3.6562) | Xent 0.7791(0.8112) | Loss 9.0997(9.6297) | Error 0.2796(0.2901) Steps 664(665.03) | Grad Norm 6.0609(7.2587) | Total Time 0.00(0.00)\n",
      "Iter 2231 | Time 62.8338(61.8268) | Bit/dim 3.6557(3.6562) | Xent 0.8180(0.8114) | Loss 9.0498(9.6123) | Error 0.2946(0.2903) Steps 646(664.46) | Grad Norm 6.9392(7.2491) | Total Time 0.00(0.00)\n",
      "Iter 2232 | Time 61.7044(61.8232) | Bit/dim 3.6525(3.6561) | Xent 0.7908(0.8107) | Loss 9.1170(9.5975) | Error 0.2827(0.2900) Steps 664(664.45) | Grad Norm 4.8992(7.1786) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0372 | Time 23.0601, Epoch Time 403.8376(409.0696), Bit/dim 3.6544(best: 3.6527), Xent 0.8395, Loss 4.0742, Error 0.2942(best: 0.2917)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2233 | Time 66.0707(61.9506) | Bit/dim 3.6454(3.6557) | Xent 0.7760(0.8097) | Loss 12.4069(9.6817) | Error 0.2816(0.2898) Steps 628(663.35) | Grad Norm 4.3506(7.0938) | Total Time 0.00(0.00)\n",
      "Iter 2234 | Time 56.5698(61.7892) | Bit/dim 3.6551(3.6557) | Xent 0.8010(0.8094) | Loss 9.0609(9.6631) | Error 0.2884(0.2897) Steps 670(663.55) | Grad Norm 7.3302(7.1009) | Total Time 0.00(0.00)\n",
      "Iter 2235 | Time 61.5781(61.7828) | Bit/dim 3.6555(3.6557) | Xent 0.8441(0.8105) | Loss 9.1046(9.6464) | Error 0.3047(0.2902) Steps 670(663.75) | Grad Norm 10.2094(7.1941) | Total Time 0.00(0.00)\n",
      "Iter 2236 | Time 60.4305(61.7423) | Bit/dim 3.6578(3.6558) | Xent 0.8227(0.8108) | Loss 9.0661(9.6290) | Error 0.2919(0.2902) Steps 646(663.22) | Grad Norm 9.6629(7.2682) | Total Time 0.00(0.00)\n",
      "Iter 2237 | Time 59.7798(61.6834) | Bit/dim 3.6567(3.6558) | Xent 0.7880(0.8102) | Loss 8.7753(9.6033) | Error 0.2834(0.2900) Steps 670(663.42) | Grad Norm 6.4222(7.2428) | Total Time 0.00(0.00)\n",
      "Iter 2238 | Time 63.6007(61.7409) | Bit/dim 3.6543(3.6558) | Xent 0.7953(0.8097) | Loss 9.0912(9.5880) | Error 0.2760(0.2896) Steps 640(662.72) | Grad Norm 8.8525(7.2911) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0373 | Time 23.5859, Epoch Time 407.4296(409.0204), Bit/dim 3.6581(best: 3.6527), Xent 0.8830, Loss 4.0996, Error 0.3099(best: 0.2917)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2239 | Time 69.5026(61.9738) | Bit/dim 3.6536(3.6557) | Xent 0.8383(0.8106) | Loss 12.8023(9.6844) | Error 0.3009(0.2900) Steps 628(661.67) | Grad Norm 10.7744(7.3956) | Total Time 0.00(0.00)\n",
      "Iter 2240 | Time 59.9745(61.9138) | Bit/dim 3.6671(3.6560) | Xent 0.8249(0.8110) | Loss 9.1476(9.6683) | Error 0.2965(0.2902) Steps 664(661.74) | Grad Norm 10.0011(7.4738) | Total Time 0.00(0.00)\n",
      "Iter 2241 | Time 58.2616(61.8042) | Bit/dim 3.6537(3.6560) | Xent 0.7830(0.8102) | Loss 9.0205(9.6489) | Error 0.2782(0.2898) Steps 688(662.53) | Grad Norm 5.5401(7.4158) | Total Time 0.00(0.00)\n",
      "Iter 2242 | Time 61.9465(61.8085) | Bit/dim 3.6550(3.6559) | Xent 0.7997(0.8098) | Loss 8.9867(9.6290) | Error 0.2890(0.2898) Steps 682(663.12) | Grad Norm 6.4667(7.3873) | Total Time 0.00(0.00)\n",
      "Iter 2243 | Time 64.7709(61.8974) | Bit/dim 3.6408(3.6555) | Xent 0.7938(0.8094) | Loss 9.1110(9.6135) | Error 0.2812(0.2895) Steps 676(663.50) | Grad Norm 5.2426(7.3230) | Total Time 0.00(0.00)\n",
      "Iter 2244 | Time 58.7093(61.8017) | Bit/dim 3.6548(3.6555) | Xent 0.7794(0.8085) | Loss 9.0275(9.5959) | Error 0.2749(0.2891) Steps 646(662.98) | Grad Norm 6.7163(7.3048) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0374 | Time 23.4040, Epoch Time 412.9700(409.1388), Bit/dim 3.6593(best: 3.6527), Xent 0.8380, Loss 4.0783, Error 0.2918(best: 0.2917)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2245 | Time 61.4944(61.7925) | Bit/dim 3.6490(3.6553) | Xent 0.7757(0.8075) | Loss 12.5011(9.6830) | Error 0.2764(0.2887) Steps 640(662.29) | Grad Norm 5.5902(7.2533) | Total Time 0.00(0.00)\n",
      "Iter 2246 | Time 63.8458(61.8541) | Bit/dim 3.6643(3.6555) | Xent 0.7943(0.8071) | Loss 9.3150(9.6720) | Error 0.2791(0.2884) Steps 646(661.80) | Grad Norm 8.5620(7.2926) | Total Time 0.00(0.00)\n",
      "Iter 2247 | Time 60.7076(61.8197) | Bit/dim 3.6488(3.6553) | Xent 0.7943(0.8067) | Loss 9.0289(9.6527) | Error 0.2855(0.2883) Steps 682(662.41) | Grad Norm 6.1395(7.2580) | Total Time 0.00(0.00)\n",
      "Iter 2248 | Time 62.1041(61.8282) | Bit/dim 3.6415(3.6549) | Xent 0.7955(0.8064) | Loss 9.1005(9.6361) | Error 0.2885(0.2883) Steps 670(662.63) | Grad Norm 4.6795(7.1806) | Total Time 0.00(0.00)\n",
      "Iter 2249 | Time 68.6450(62.0327) | Bit/dim 3.6545(3.6549) | Xent 0.7901(0.8059) | Loss 9.0642(9.6190) | Error 0.2804(0.2881) Steps 652(662.31) | Grad Norm 4.9214(7.1129) | Total Time 0.00(0.00)\n",
      "Iter 2250 | Time 59.1597(61.9466) | Bit/dim 3.6579(3.6550) | Xent 0.7976(0.8056) | Loss 9.2294(9.6073) | Error 0.2857(0.2880) Steps 688(663.08) | Grad Norm 6.0434(7.0808) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0375 | Time 23.7024, Epoch Time 415.5725(409.3319), Bit/dim 3.6574(best: 3.6527), Xent 0.8386, Loss 4.0767, Error 0.3001(best: 0.2917)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2251 | Time 56.6068(61.7864) | Bit/dim 3.6572(3.6551) | Xent 0.7792(0.8048) | Loss 12.5373(9.6952) | Error 0.2789(0.2877) Steps 652(662.75) | Grad Norm 3.9870(6.9880) | Total Time 0.00(0.00)\n",
      "Iter 2252 | Time 65.8463(61.9082) | Bit/dim 3.6484(3.6549) | Xent 0.7953(0.8046) | Loss 9.2839(9.6829) | Error 0.2800(0.2875) Steps 640(662.07) | Grad Norm 5.4498(6.9418) | Total Time 0.00(0.00)\n",
      "Iter 2253 | Time 64.1009(61.9739) | Bit/dim 3.6462(3.6546) | Xent 0.7712(0.8036) | Loss 9.1240(9.6661) | Error 0.2754(0.2871) Steps 664(662.13) | Grad Norm 5.2192(6.8901) | Total Time 0.00(0.00)\n",
      "Iter 2254 | Time 57.7740(61.8479) | Bit/dim 3.6639(3.6549) | Xent 0.7985(0.8034) | Loss 8.8668(9.6421) | Error 0.2830(0.2870) Steps 670(662.36) | Grad Norm 5.0752(6.8357) | Total Time 0.00(0.00)\n",
      "Iter 2255 | Time 58.1203(61.7361) | Bit/dim 3.6551(3.6549) | Xent 0.7733(0.8025) | Loss 9.0398(9.6240) | Error 0.2760(0.2867) Steps 670(662.59) | Grad Norm 4.5439(6.7669) | Total Time 0.00(0.00)\n",
      "Iter 2256 | Time 60.0220(61.6847) | Bit/dim 3.6549(3.6549) | Xent 0.7697(0.8015) | Loss 8.9943(9.6052) | Error 0.2768(0.2864) Steps 652(662.28) | Grad Norm 3.6722(6.6741) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0376 | Time 23.3068, Epoch Time 401.9587(409.1107), Bit/dim 3.6508(best: 3.6527), Xent 0.8360, Loss 4.0688, Error 0.2958(best: 0.2917)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2257 | Time 60.9666(61.6631) | Bit/dim 3.6417(3.6545) | Xent 0.7721(0.8006) | Loss 12.5998(9.6950) | Error 0.2676(0.2858) Steps 670(662.51) | Grad Norm 4.1116(6.5972) | Total Time 0.00(0.00)\n",
      "Iter 2258 | Time 63.9424(61.7315) | Bit/dim 3.6473(3.6543) | Xent 0.7864(0.8002) | Loss 8.8644(9.6701) | Error 0.2851(0.2858) Steps 688(663.27) | Grad Norm 5.3876(6.5609) | Total Time 0.00(0.00)\n",
      "Iter 2259 | Time 64.3928(61.8114) | Bit/dim 3.6624(3.6545) | Xent 0.7963(0.8001) | Loss 9.1238(9.6537) | Error 0.2846(0.2858) Steps 658(663.11) | Grad Norm 5.5874(6.5317) | Total Time 0.00(0.00)\n",
      "Iter 2260 | Time 64.4715(61.8912) | Bit/dim 3.6580(3.6546) | Xent 0.7890(0.7998) | Loss 9.0268(9.6349) | Error 0.2855(0.2858) Steps 658(662.96) | Grad Norm 6.7380(6.5379) | Total Time 0.00(0.00)\n",
      "Iter 2261 | Time 58.9730(61.8036) | Bit/dim 3.6452(3.6543) | Xent 0.7780(0.7991) | Loss 9.1444(9.6202) | Error 0.2802(0.2856) Steps 664(662.99) | Grad Norm 7.0800(6.5542) | Total Time 0.00(0.00)\n",
      "Iter 2262 | Time 61.4886(61.7942) | Bit/dim 3.6473(3.6541) | Xent 0.8021(0.7992) | Loss 9.2082(9.6078) | Error 0.2861(0.2856) Steps 688(663.74) | Grad Norm 8.7423(6.6198) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0377 | Time 23.7243, Epoch Time 414.3431(409.2676), Bit/dim 3.6590(best: 3.6508), Xent 0.8503, Loss 4.0841, Error 0.2958(best: 0.2917)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2263 | Time 65.3039(61.8995) | Bit/dim 3.6475(3.6539) | Xent 0.7966(0.7991) | Loss 12.4570(9.6933) | Error 0.2849(0.2856) Steps 658(663.57) | Grad Norm 9.5265(6.7070) | Total Time 0.00(0.00)\n",
      "Iter 2264 | Time 61.8164(61.8970) | Bit/dim 3.6568(3.6540) | Xent 0.7962(0.7990) | Loss 9.1412(9.6767) | Error 0.2854(0.2856) Steps 694(664.48) | Grad Norm 7.4172(6.7283) | Total Time 0.00(0.00)\n",
      "Iter 2265 | Time 61.7639(61.8930) | Bit/dim 3.6441(3.6537) | Xent 0.8054(0.7992) | Loss 9.0007(9.6564) | Error 0.2877(0.2857) Steps 628(663.39) | Grad Norm 5.5665(6.6935) | Total Time 0.00(0.00)\n",
      "Iter 2266 | Time 61.6084(61.8844) | Bit/dim 3.6628(3.6540) | Xent 0.7953(0.7991) | Loss 9.0142(9.6372) | Error 0.2866(0.2857) Steps 664(663.41) | Grad Norm 7.9687(6.7317) | Total Time 0.00(0.00)\n",
      "Iter 2267 | Time 63.5005(61.9329) | Bit/dim 3.6475(3.6538) | Xent 0.7828(0.7986) | Loss 9.0534(9.6197) | Error 0.2850(0.2857) Steps 664(663.42) | Grad Norm 5.1856(6.6853) | Total Time 0.00(0.00)\n",
      "Iter 2268 | Time 60.1753(61.8802) | Bit/dim 3.6511(3.6537) | Xent 0.8021(0.7987) | Loss 9.1692(9.6061) | Error 0.2840(0.2856) Steps 664(663.44) | Grad Norm 6.0794(6.6672) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0378 | Time 23.4360, Epoch Time 413.6714(409.3998), Bit/dim 3.6533(best: 3.6508), Xent 0.8390, Loss 4.0728, Error 0.2922(best: 0.2917)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2269 | Time 57.6773(61.7541) | Bit/dim 3.6581(3.6539) | Xent 0.7877(0.7984) | Loss 12.5959(9.6958) | Error 0.2796(0.2854) Steps 664(663.46) | Grad Norm 6.0103(6.6475) | Total Time 0.00(0.00)\n",
      "Iter 2270 | Time 61.3392(61.7417) | Bit/dim 3.6399(3.6534) | Xent 0.7919(0.7982) | Loss 9.0087(9.6752) | Error 0.2853(0.2854) Steps 676(663.83) | Grad Norm 4.7419(6.5903) | Total Time 0.00(0.00)\n",
      "Iter 2271 | Time 63.4310(61.7923) | Bit/dim 3.6608(3.6537) | Xent 0.7769(0.7975) | Loss 9.2073(9.6612) | Error 0.2769(0.2852) Steps 706(665.10) | Grad Norm 5.7185(6.5641) | Total Time 0.00(0.00)\n",
      "Iter 2272 | Time 65.5410(61.9048) | Bit/dim 3.6520(3.6536) | Xent 0.8013(0.7977) | Loss 9.0486(9.6428) | Error 0.2880(0.2853) Steps 688(665.79) | Grad Norm 7.8531(6.6028) | Total Time 0.00(0.00)\n",
      "Iter 2273 | Time 58.9959(61.8175) | Bit/dim 3.6507(3.6535) | Xent 0.7745(0.7970) | Loss 8.9865(9.6231) | Error 0.2796(0.2851) Steps 664(665.73) | Grad Norm 4.8417(6.5500) | Total Time 0.00(0.00)\n",
      "Iter 2274 | Time 59.2056(61.7392) | Bit/dim 3.6519(3.6535) | Xent 0.7754(0.7963) | Loss 8.8995(9.6014) | Error 0.2754(0.2848) Steps 652(665.32) | Grad Norm 3.8020(6.4675) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0379 | Time 24.4041, Epoch Time 406.8114(409.3221), Bit/dim 3.6494(best: 3.6508), Xent 0.8391, Loss 4.0690, Error 0.2921(best: 0.2917)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2275 | Time 63.1354(61.7811) | Bit/dim 3.6566(3.6536) | Xent 0.7945(0.7963) | Loss 12.9622(9.7022) | Error 0.2851(0.2848) Steps 646(664.74) | Grad Norm 4.8159(6.4180) | Total Time 0.00(0.00)\n",
      "Iter 2276 | Time 58.3380(61.6778) | Bit/dim 3.6531(3.6535) | Xent 0.7753(0.7956) | Loss 9.1458(9.6855) | Error 0.2836(0.2848) Steps 652(664.36) | Grad Norm 5.7061(6.3966) | Total Time 0.00(0.00)\n",
      "Iter 2277 | Time 59.4354(61.6105) | Bit/dim 3.6456(3.6533) | Xent 0.7812(0.7952) | Loss 8.9286(9.6628) | Error 0.2811(0.2847) Steps 658(664.17) | Grad Norm 3.3454(6.3051) | Total Time 0.00(0.00)\n",
      "Iter 2278 | Time 65.1892(61.7179) | Bit/dim 3.6482(3.6532) | Xent 0.7731(0.7945) | Loss 9.0643(9.6449) | Error 0.2708(0.2842) Steps 640(663.44) | Grad Norm 6.9227(6.3236) | Total Time 0.00(0.00)\n",
      "Iter 2279 | Time 62.5422(61.7426) | Bit/dim 3.6387(3.6527) | Xent 0.7999(0.7947) | Loss 8.9770(9.6248) | Error 0.2913(0.2845) Steps 682(664.00) | Grad Norm 5.2742(6.2921) | Total Time 0.00(0.00)\n",
      "Iter 2280 | Time 63.9554(61.8090) | Bit/dim 3.6599(3.6529) | Xent 0.7723(0.7940) | Loss 9.2189(9.6127) | Error 0.2731(0.2841) Steps 646(663.46) | Grad Norm 6.2516(6.2909) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0380 | Time 24.1566, Epoch Time 412.7026(409.4235), Bit/dim 3.6539(best: 3.6494), Xent 0.8483, Loss 4.0780, Error 0.2955(best: 0.2917)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2281 | Time 61.9211(61.8123) | Bit/dim 3.6529(3.6529) | Xent 0.8005(0.7942) | Loss 12.3796(9.6957) | Error 0.2883(0.2842) Steps 688(664.20) | Grad Norm 10.5069(6.4174) | Total Time 0.00(0.00)\n",
      "Iter 2282 | Time 59.6688(61.7480) | Bit/dim 3.6484(3.6528) | Xent 0.8449(0.7957) | Loss 9.0339(9.6758) | Error 0.3001(0.2847) Steps 652(663.83) | Grad Norm 12.3942(6.5967) | Total Time 0.00(0.00)\n",
      "Iter 2283 | Time 61.1213(61.7292) | Bit/dim 3.6549(3.6529) | Xent 0.8278(0.7967) | Loss 9.1559(9.6602) | Error 0.2997(0.2852) Steps 670(664.02) | Grad Norm 11.1110(6.7321) | Total Time 0.00(0.00)\n",
      "Iter 2284 | Time 62.1520(61.7419) | Bit/dim 3.6499(3.6528) | Xent 0.7880(0.7965) | Loss 9.0026(9.6405) | Error 0.2893(0.2853) Steps 676(664.37) | Grad Norm 4.0560(6.6518) | Total Time 0.00(0.00)\n",
      "Iter 2285 | Time 67.2987(61.9086) | Bit/dim 3.6484(3.6526) | Xent 0.7763(0.7958) | Loss 9.0746(9.6235) | Error 0.2808(0.2852) Steps 670(664.54) | Grad Norm 6.8656(6.6583) | Total Time 0.00(0.00)\n",
      "Iter 2286 | Time 61.6752(61.9016) | Bit/dim 3.6531(3.6527) | Xent 0.8052(0.7961) | Loss 9.1498(9.6093) | Error 0.2863(0.2852) Steps 700(665.61) | Grad Norm 6.1707(6.6436) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0381 | Time 23.6933, Epoch Time 413.7856(409.5544), Bit/dim 3.6492(best: 3.6494), Xent 0.8333, Loss 4.0659, Error 0.2937(best: 0.2917)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2287 | Time 65.1558(61.9992) | Bit/dim 3.6492(3.6526) | Xent 0.7625(0.7951) | Loss 12.6270(9.6998) | Error 0.2740(0.2848) Steps 706(666.82) | Grad Norm 6.1090(6.6276) | Total Time 0.00(0.00)\n",
      "Iter 2288 | Time 61.0181(61.9698) | Bit/dim 3.6577(3.6527) | Xent 0.8166(0.7958) | Loss 8.9461(9.6772) | Error 0.2913(0.2850) Steps 664(666.73) | Grad Norm 12.0539(6.7904) | Total Time 0.00(0.00)\n",
      "Iter 2289 | Time 60.5637(61.9276) | Bit/dim 3.6505(3.6526) | Xent 0.8045(0.7960) | Loss 8.9923(9.6567) | Error 0.2886(0.2851) Steps 682(667.19) | Grad Norm 6.9342(6.7947) | Total Time 0.00(0.00)\n",
      "Iter 2290 | Time 60.9979(61.8997) | Bit/dim 3.6501(3.6526) | Xent 0.7914(0.7959) | Loss 9.1582(9.6417) | Error 0.2856(0.2852) Steps 682(667.64) | Grad Norm 7.7438(6.8232) | Total Time 0.00(0.00)\n",
      "Iter 2291 | Time 60.2823(61.8512) | Bit/dim 3.6641(3.6529) | Xent 0.8120(0.7964) | Loss 9.2561(9.6302) | Error 0.2869(0.2852) Steps 646(666.99) | Grad Norm 11.3806(6.9599) | Total Time 0.00(0.00)\n",
      "Iter 2292 | Time 66.9500(62.0042) | Bit/dim 3.6417(3.6526) | Xent 0.7804(0.7959) | Loss 9.1286(9.6151) | Error 0.2817(0.2851) Steps 658(666.72) | Grad Norm 3.2863(6.8497) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0382 | Time 23.3054, Epoch Time 414.3829(409.6992), Bit/dim 3.6547(best: 3.6492), Xent 0.8525, Loss 4.0810, Error 0.2967(best: 0.2917)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2293 | Time 62.1844(62.0096) | Bit/dim 3.6615(3.6528) | Xent 0.7970(0.7959) | Loss 12.3323(9.6966) | Error 0.2910(0.2853) Steps 652(666.28) | Grad Norm 10.0546(6.9458) | Total Time 0.00(0.00)\n",
      "Iter 2294 | Time 60.2051(61.9554) | Bit/dim 3.6461(3.6526) | Xent 0.8136(0.7965) | Loss 8.9700(9.6748) | Error 0.2967(0.2856) Steps 646(665.67) | Grad Norm 8.3476(6.9879) | Total Time 0.00(0.00)\n",
      "Iter 2295 | Time 62.6377(61.9759) | Bit/dim 3.6508(3.6526) | Xent 0.8268(0.7974) | Loss 8.8963(9.6515) | Error 0.3003(0.2861) Steps 652(665.26) | Grad Norm 8.7973(7.0422) | Total Time 0.00(0.00)\n",
      "Iter 2296 | Time 59.7003(61.9076) | Bit/dim 3.6534(3.6526) | Xent 0.8388(0.7986) | Loss 9.1184(9.6355) | Error 0.3020(0.2865) Steps 676(665.58) | Grad Norm 14.4609(7.2647) | Total Time 0.00(0.00)\n",
      "Iter 2297 | Time 57.5623(61.7773) | Bit/dim 3.6503(3.6525) | Xent 0.8721(0.8008) | Loss 9.0781(9.6187) | Error 0.3099(0.2872) Steps 646(664.99) | Grad Norm 17.6209(7.5754) | Total Time 0.00(0.00)\n",
      "Iter 2298 | Time 62.2148(61.7904) | Bit/dim 3.6621(3.6528) | Xent 0.8249(0.8015) | Loss 9.1602(9.6050) | Error 0.2891(0.2873) Steps 718(666.58) | Grad Norm 7.7297(7.5800) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0383 | Time 23.5843, Epoch Time 403.9447(409.5266), Bit/dim 3.6659(best: 3.6492), Xent 0.9051, Loss 4.1184, Error 0.3179(best: 0.2917)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2299 | Time 62.3804(61.8081) | Bit/dim 3.6568(3.6529) | Xent 0.8323(0.8025) | Loss 12.4330(9.6898) | Error 0.3031(0.2878) Steps 652(666.15) | Grad Norm 15.8017(7.8267) | Total Time 0.00(0.00)\n",
      "Iter 2300 | Time 62.5143(61.8293) | Bit/dim 3.6548(3.6530) | Xent 0.7970(0.8023) | Loss 9.1971(9.6751) | Error 0.2840(0.2877) Steps 700(667.16) | Grad Norm 7.6219(7.8206) | Total Time 0.00(0.00)\n",
      "Iter 2301 | Time 61.3768(61.8157) | Bit/dim 3.6491(3.6529) | Xent 0.8594(0.8040) | Loss 9.1100(9.6581) | Error 0.3086(0.2883) Steps 658(666.89) | Grad Norm 12.4764(7.9602) | Total Time 0.00(0.00)\n",
      "Iter 2302 | Time 64.6128(61.8996) | Bit/dim 3.6501(3.6528) | Xent 0.8392(0.8051) | Loss 9.0060(9.6385) | Error 0.2957(0.2885) Steps 664(666.80) | Grad Norm 6.4376(7.9145) | Total Time 0.00(0.00)\n",
      "Iter 2303 | Time 60.9921(61.8724) | Bit/dim 3.6675(3.6532) | Xent 0.8341(0.8059) | Loss 9.1383(9.6235) | Error 0.3003(0.2889) Steps 694(667.62) | Grad Norm 9.9814(7.9766) | Total Time 0.00(0.00)\n",
      "Iter 2304 | Time 62.3894(61.8879) | Bit/dim 3.6598(3.6534) | Xent 0.8110(0.8061) | Loss 9.1258(9.6086) | Error 0.2917(0.2890) Steps 652(667.15) | Grad Norm 7.0424(7.9485) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0384 | Time 23.9372, Epoch Time 414.3645(409.6717), Bit/dim 3.6572(best: 3.6492), Xent 0.8566, Loss 4.0855, Error 0.3031(best: 0.2917)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2305 | Time 60.2429(61.8386) | Bit/dim 3.6592(3.6536) | Xent 0.8258(0.8067) | Loss 12.5029(9.6954) | Error 0.2954(0.2891) Steps 664(667.05) | Grad Norm 8.5955(7.9679) | Total Time 0.00(0.00)\n",
      "Iter 2306 | Time 63.1459(61.8778) | Bit/dim 3.6486(3.6535) | Xent 0.8143(0.8069) | Loss 9.1442(9.6789) | Error 0.2907(0.2892) Steps 694(667.86) | Grad Norm 7.6929(7.9597) | Total Time 0.00(0.00)\n",
      "Iter 2307 | Time 60.3333(61.8315) | Bit/dim 3.6731(3.6540) | Xent 0.7861(0.8063) | Loss 9.0945(9.6614) | Error 0.2823(0.2890) Steps 670(667.93) | Grad Norm 9.5521(8.0075) | Total Time 0.00(0.00)\n",
      "Iter 2308 | Time 59.1755(61.7518) | Bit/dim 3.6518(3.6540) | Xent 0.7980(0.8060) | Loss 9.0741(9.6437) | Error 0.2826(0.2888) Steps 706(669.07) | Grad Norm 5.8586(7.9430) | Total Time 0.00(0.00)\n",
      "Iter 2309 | Time 61.8313(61.7542) | Bit/dim 3.6583(3.6541) | Xent 0.7813(0.8053) | Loss 9.1548(9.6291) | Error 0.2792(0.2885) Steps 670(669.10) | Grad Norm 6.6621(7.9046) | Total Time 0.00(0.00)\n",
      "Iter 2310 | Time 58.2335(61.6485) | Bit/dim 3.6661(3.6545) | Xent 0.7928(0.8049) | Loss 8.9838(9.6097) | Error 0.2899(0.2886) Steps 670(669.12) | Grad Norm 6.8741(7.8737) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0385 | Time 24.5165, Epoch Time 403.2988(409.4805), Bit/dim 3.6581(best: 3.6492), Xent 0.8337, Loss 4.0750, Error 0.2952(best: 0.2917)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2311 | Time 59.8476(61.5945) | Bit/dim 3.6657(3.6548) | Xent 0.7750(0.8040) | Loss 12.5889(9.6991) | Error 0.2776(0.2882) Steps 682(669.51) | Grad Norm 3.2551(7.7351) | Total Time 0.00(0.00)\n",
      "Iter 2312 | Time 67.2732(61.7649) | Bit/dim 3.6647(3.6551) | Xent 0.7835(0.8034) | Loss 9.2024(9.6842) | Error 0.2815(0.2880) Steps 706(670.60) | Grad Norm 7.6269(7.7319) | Total Time 0.00(0.00)\n",
      "Iter 2313 | Time 58.4288(61.6648) | Bit/dim 3.6561(3.6551) | Xent 0.7718(0.8025) | Loss 9.1621(9.6685) | Error 0.2739(0.2876) Steps 682(670.95) | Grad Norm 3.8157(7.6144) | Total Time 0.00(0.00)\n",
      "Iter 2314 | Time 66.8818(61.8213) | Bit/dim 3.6536(3.6551) | Xent 0.7937(0.8022) | Loss 9.1761(9.6538) | Error 0.2817(0.2874) Steps 676(671.10) | Grad Norm 5.4962(7.5508) | Total Time 0.00(0.00)\n",
      "Iter 2315 | Time 62.7376(61.8488) | Bit/dim 3.6489(3.6549) | Xent 0.7618(0.8010) | Loss 9.1216(9.6378) | Error 0.2726(0.2870) Steps 682(671.42) | Grad Norm 3.5367(7.4304) | Total Time 0.00(0.00)\n",
      "Iter 2316 | Time 61.2549(61.8310) | Bit/dim 3.6573(3.6550) | Xent 0.7881(0.8006) | Loss 9.0289(9.6195) | Error 0.2841(0.2869) Steps 676(671.56) | Grad Norm 4.1276(7.3313) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0386 | Time 23.5519, Epoch Time 415.9189(409.6737), Bit/dim 3.6582(best: 3.6492), Xent 0.8247, Loss 4.0706, Error 0.2920(best: 0.2917)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2317 | Time 63.6939(61.8869) | Bit/dim 3.6508(3.6548) | Xent 0.7658(0.7996) | Loss 12.9117(9.7183) | Error 0.2750(0.2865) Steps 664(671.33) | Grad Norm 2.9956(7.2012) | Total Time 0.00(0.00)\n",
      "Iter 2318 | Time 64.0040(61.9504) | Bit/dim 3.6578(3.6549) | Xent 0.7661(0.7985) | Loss 9.1534(9.7013) | Error 0.2798(0.2863) Steps 694(672.01) | Grad Norm 3.1974(7.0811) | Total Time 0.00(0.00)\n",
      "Iter 2319 | Time 61.8769(61.9482) | Bit/dim 3.6570(3.6550) | Xent 0.7682(0.7976) | Loss 8.8800(9.6767) | Error 0.2711(0.2859) Steps 628(670.69) | Grad Norm 5.8555(7.0444) | Total Time 0.00(0.00)\n",
      "Iter 2320 | Time 61.7389(61.9419) | Bit/dim 3.6570(3.6551) | Xent 0.7842(0.7972) | Loss 8.9846(9.6559) | Error 0.2786(0.2857) Steps 658(670.31) | Grad Norm 4.4984(6.9680) | Total Time 0.00(0.00)\n",
      "Iter 2321 | Time 58.9823(61.8531) | Bit/dim 3.6580(3.6551) | Xent 0.7834(0.7968) | Loss 8.9722(9.6354) | Error 0.2809(0.2855) Steps 670(670.30) | Grad Norm 5.5599(6.9257) | Total Time 0.00(0.00)\n",
      "Iter 2322 | Time 62.1805(61.8629) | Bit/dim 3.6504(3.6550) | Xent 0.7494(0.7954) | Loss 8.9564(9.6151) | Error 0.2711(0.2851) Steps 694(671.02) | Grad Norm 3.7498(6.8305) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0387 | Time 23.9881, Epoch Time 412.3873(409.7551), Bit/dim 3.6536(best: 3.6492), Xent 0.8247, Loss 4.0660, Error 0.2903(best: 0.2917)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2323 | Time 61.9548(61.8657) | Bit/dim 3.6529(3.6549) | Xent 0.7766(0.7948) | Loss 12.7393(9.7088) | Error 0.2740(0.2848) Steps 700(671.88) | Grad Norm 5.6264(6.7943) | Total Time 0.00(0.00)\n",
      "Iter 2324 | Time 59.3246(61.7895) | Bit/dim 3.6554(3.6550) | Xent 0.7731(0.7942) | Loss 9.1925(9.6933) | Error 0.2805(0.2846) Steps 664(671.65) | Grad Norm 5.0033(6.7406) | Total Time 0.00(0.00)\n",
      "Iter 2325 | Time 60.6818(61.7562) | Bit/dim 3.6584(3.6551) | Xent 0.7904(0.7941) | Loss 8.8956(9.6694) | Error 0.2847(0.2846) Steps 676(671.78) | Grad Norm 5.5991(6.7064) | Total Time 0.00(0.00)\n",
      "Iter 2326 | Time 58.8045(61.6677) | Bit/dim 3.6521(3.6550) | Xent 0.7602(0.7931) | Loss 9.0431(9.6506) | Error 0.2676(0.2841) Steps 670(671.73) | Grad Norm 5.8910(6.6819) | Total Time 0.00(0.00)\n",
      "Iter 2327 | Time 58.0414(61.5589) | Bit/dim 3.6520(3.6549) | Xent 0.7987(0.7932) | Loss 9.1503(9.6356) | Error 0.2881(0.2842) Steps 658(671.31) | Grad Norm 7.9465(6.7198) | Total Time 0.00(0.00)\n",
      "Iter 2328 | Time 58.2298(61.4590) | Bit/dim 3.6610(3.6551) | Xent 0.7655(0.7924) | Loss 8.9955(9.6164) | Error 0.2694(0.2838) Steps 658(670.91) | Grad Norm 4.1792(6.6436) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0388 | Time 24.5431, Epoch Time 397.5847(409.3900), Bit/dim 3.6553(best: 3.6492), Xent 0.8268, Loss 4.0687, Error 0.2893(best: 0.2903)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2329 | Time 61.0830(61.4477) | Bit/dim 3.6552(3.6551) | Xent 0.7773(0.7919) | Loss 12.4929(9.7027) | Error 0.2752(0.2835) Steps 682(671.25) | Grad Norm 5.6849(6.6149) | Total Time 0.00(0.00)\n",
      "Iter 2330 | Time 58.5536(61.3609) | Bit/dim 3.6558(3.6551) | Xent 0.7558(0.7908) | Loss 9.0307(9.6825) | Error 0.2672(0.2830) Steps 658(670.85) | Grad Norm 4.1942(6.5422) | Total Time 0.00(0.00)\n",
      "Iter 2331 | Time 58.7402(61.2823) | Bit/dim 3.6649(3.6554) | Xent 0.7762(0.7904) | Loss 8.8571(9.6577) | Error 0.2812(0.2830) Steps 694(671.54) | Grad Norm 5.4341(6.5090) | Total Time 0.00(0.00)\n",
      "Iter 2332 | Time 60.3536(61.2544) | Bit/dim 3.6359(3.6548) | Xent 0.7783(0.7900) | Loss 9.0030(9.6381) | Error 0.2798(0.2829) Steps 688(672.04) | Grad Norm 5.1270(6.4675) | Total Time 0.00(0.00)\n",
      "Iter 2333 | Time 60.0158(61.2173) | Bit/dim 3.6616(3.6550) | Xent 0.7764(0.7896) | Loss 9.0594(9.6207) | Error 0.2776(0.2827) Steps 700(672.88) | Grad Norm 5.5138(6.4389) | Total Time 0.00(0.00)\n",
      "Iter 2334 | Time 64.3381(61.3109) | Bit/dim 3.6379(3.6545) | Xent 0.7758(0.7892) | Loss 9.0069(9.6023) | Error 0.2799(0.2827) Steps 724(674.41) | Grad Norm 4.9410(6.3940) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0389 | Time 24.1786, Epoch Time 403.5125(409.2137), Bit/dim 3.6578(best: 3.6492), Xent 0.8466, Loss 4.0811, Error 0.2977(best: 0.2893)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2335 | Time 58.6260(61.2303) | Bit/dim 3.6509(3.6544) | Xent 0.8014(0.7896) | Loss 12.5324(9.6902) | Error 0.2873(0.2828) Steps 664(674.10) | Grad Norm 9.4656(6.4861) | Total Time 0.00(0.00)\n",
      "Iter 2336 | Time 62.5262(61.2692) | Bit/dim 3.6540(3.6544) | Xent 0.8014(0.7899) | Loss 9.0232(9.6702) | Error 0.2801(0.2827) Steps 694(674.69) | Grad Norm 11.5978(6.6395) | Total Time 0.00(0.00)\n",
      "Iter 2337 | Time 62.2193(61.2977) | Bit/dim 3.6430(3.6540) | Xent 0.8384(0.7914) | Loss 9.2264(9.6569) | Error 0.3021(0.2833) Steps 694(675.27) | Grad Norm 10.1285(6.7442) | Total Time 0.00(0.00)\n",
      "Iter 2338 | Time 59.7079(61.2500) | Bit/dim 3.6505(3.6539) | Xent 0.7874(0.7913) | Loss 9.1865(9.6428) | Error 0.2833(0.2833) Steps 682(675.48) | Grad Norm 7.5676(6.7689) | Total Time 0.00(0.00)\n",
      "Iter 2339 | Time 60.5674(61.2295) | Bit/dim 3.6388(3.6535) | Xent 0.8290(0.7924) | Loss 9.0482(9.6249) | Error 0.2981(0.2837) Steps 688(675.85) | Grad Norm 10.8946(6.8926) | Total Time 0.00(0.00)\n",
      "Iter 2340 | Time 64.2449(61.3200) | Bit/dim 3.6647(3.6538) | Xent 0.8308(0.7936) | Loss 8.9311(9.6041) | Error 0.2941(0.2840) Steps 700(676.58) | Grad Norm 7.7485(6.9183) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0390 | Time 24.3362, Epoch Time 408.5301(409.1932), Bit/dim 3.6527(best: 3.6492), Xent 0.8332, Loss 4.0693, Error 0.2949(best: 0.2893)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2341 | Time 64.1065(61.4036) | Bit/dim 3.6445(3.6535) | Xent 0.7816(0.7932) | Loss 12.6054(9.6942) | Error 0.2839(0.2840) Steps 706(677.46) | Grad Norm 6.2476(6.8982) | Total Time 0.00(0.00)\n",
      "Iter 2342 | Time 63.2411(61.4587) | Bit/dim 3.6402(3.6531) | Xent 0.8024(0.7935) | Loss 9.1671(9.6784) | Error 0.2907(0.2842) Steps 670(677.23) | Grad Norm 5.7753(6.8645) | Total Time 0.00(0.00)\n",
      "Iter 2343 | Time 58.7060(61.3761) | Bit/dim 3.6624(3.6534) | Xent 0.8230(0.7944) | Loss 8.7934(9.6518) | Error 0.2904(0.2844) Steps 682(677.38) | Grad Norm 9.5987(6.9465) | Total Time 0.00(0.00)\n",
      "Iter 2344 | Time 64.2023(61.4609) | Bit/dim 3.6673(3.6538) | Xent 0.8169(0.7950) | Loss 9.0232(9.6330) | Error 0.2893(0.2846) Steps 658(676.80) | Grad Norm 10.1821(7.0436) | Total Time 0.00(0.00)\n",
      "Iter 2345 | Time 59.2856(61.3957) | Bit/dim 3.6475(3.6536) | Xent 0.7649(0.7941) | Loss 9.0265(9.6148) | Error 0.2698(0.2841) Steps 694(677.31) | Grad Norm 4.1210(6.9559) | Total Time 0.00(0.00)\n",
      "Iter 2346 | Time 62.2527(61.4214) | Bit/dim 3.6481(3.6535) | Xent 0.7969(0.7942) | Loss 9.0598(9.5981) | Error 0.2864(0.2842) Steps 664(676.91) | Grad Norm 8.2246(6.9940) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0391 | Time 23.8638, Epoch Time 411.9864(409.2770), Bit/dim 3.6522(best: 3.6492), Xent 0.8397, Loss 4.0720, Error 0.2936(best: 0.2893)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2347 | Time 60.5990(61.3967) | Bit/dim 3.6499(3.6534) | Xent 0.7915(0.7941) | Loss 12.7879(9.6938) | Error 0.2851(0.2842) Steps 670(676.71) | Grad Norm 5.5982(6.9521) | Total Time 0.00(0.00)\n",
      "Iter 2348 | Time 59.4012(61.3368) | Bit/dim 3.6476(3.6532) | Xent 0.7835(0.7938) | Loss 8.9911(9.6727) | Error 0.2806(0.2841) Steps 670(676.50) | Grad Norm 8.7214(7.0052) | Total Time 0.00(0.00)\n",
      "Iter 2349 | Time 63.2070(61.3929) | Bit/dim 3.6534(3.6532) | Xent 0.7866(0.7936) | Loss 9.0323(9.6535) | Error 0.2819(0.2840) Steps 658(675.95) | Grad Norm 6.5722(6.9922) | Total Time 0.00(0.00)\n",
      "Iter 2350 | Time 65.5549(61.5178) | Bit/dim 3.6492(3.6531) | Xent 0.7942(0.7936) | Loss 9.1887(9.6396) | Error 0.2821(0.2840) Steps 664(675.59) | Grad Norm 7.6801(7.0128) | Total Time 0.00(0.00)\n",
      "Iter 2351 | Time 61.7966(61.5262) | Bit/dim 3.6532(3.6531) | Xent 0.7719(0.7930) | Loss 8.9960(9.6203) | Error 0.2806(0.2839) Steps 676(675.60) | Grad Norm 4.2857(6.9310) | Total Time 0.00(0.00)\n",
      "Iter 2352 | Time 59.5252(61.4661) | Bit/dim 3.6622(3.6534) | Xent 0.7618(0.7920) | Loss 8.8784(9.5980) | Error 0.2724(0.2835) Steps 694(676.16) | Grad Norm 6.8194(6.9277) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0392 | Time 24.0780, Epoch Time 410.0378(409.2998), Bit/dim 3.6540(best: 3.6492), Xent 0.8319, Loss 4.0700, Error 0.2927(best: 0.2893)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2353 | Time 60.5156(61.4376) | Bit/dim 3.6522(3.6533) | Xent 0.7689(0.7913) | Loss 12.6560(9.6897) | Error 0.2781(0.2834) Steps 688(676.51) | Grad Norm 4.5867(6.8574) | Total Time 0.00(0.00)\n",
      "Iter 2354 | Time 63.6485(61.5039) | Bit/dim 3.6498(3.6532) | Xent 0.7873(0.7912) | Loss 9.1031(9.6721) | Error 0.2874(0.2835) Steps 688(676.86) | Grad Norm 6.9798(6.8611) | Total Time 0.00(0.00)\n",
      "Iter 2355 | Time 64.6302(61.5977) | Bit/dim 3.6499(3.6531) | Xent 0.8029(0.7916) | Loss 9.1914(9.6577) | Error 0.2880(0.2836) Steps 664(676.47) | Grad Norm 7.9221(6.8929) | Total Time 0.00(0.00)\n",
      "Iter 2356 | Time 58.5682(61.5069) | Bit/dim 3.6559(3.6532) | Xent 0.8087(0.7921) | Loss 8.9411(9.6362) | Error 0.2909(0.2839) Steps 628(675.02) | Grad Norm 8.7766(6.9494) | Total Time 0.00(0.00)\n",
      "Iter 2357 | Time 60.1681(61.4667) | Bit/dim 3.6489(3.6531) | Xent 0.7914(0.7921) | Loss 9.1798(9.6225) | Error 0.2791(0.2837) Steps 694(675.59) | Grad Norm 7.2165(6.9575) | Total Time 0.00(0.00)\n",
      "Iter 2358 | Time 61.2183(61.4592) | Bit/dim 3.6511(3.6530) | Xent 0.7961(0.7922) | Loss 8.9208(9.6015) | Error 0.2850(0.2837) Steps 718(676.86) | Grad Norm 5.8525(6.9243) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0393 | Time 23.5420, Epoch Time 408.5733(409.2780), Bit/dim 3.6499(best: 3.6492), Xent 0.8490, Loss 4.0744, Error 0.2984(best: 0.2893)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2359 | Time 61.3466(61.4559) | Bit/dim 3.6542(3.6530) | Xent 0.7922(0.7922) | Loss 12.8447(9.6988) | Error 0.2873(0.2839) Steps 658(676.29) | Grad Norm 8.5991(6.9746) | Total Time 0.00(0.00)\n",
      "Iter 2360 | Time 56.1256(61.2959) | Bit/dim 3.6627(3.6533) | Xent 0.8169(0.7929) | Loss 8.8591(9.6736) | Error 0.2909(0.2841) Steps 670(676.10) | Grad Norm 9.3625(7.0462) | Total Time 0.00(0.00)\n",
      "Iter 2361 | Time 60.7359(61.2791) | Bit/dim 3.6527(3.6533) | Xent 0.7819(0.7926) | Loss 8.7988(9.6473) | Error 0.2772(0.2839) Steps 652(675.38) | Grad Norm 7.6428(7.0641) | Total Time 0.00(0.00)\n",
      "Iter 2362 | Time 59.6996(61.2318) | Bit/dim 3.6464(3.6531) | Xent 0.7776(0.7921) | Loss 8.8919(9.6247) | Error 0.2701(0.2834) Steps 682(675.58) | Grad Norm 7.0276(7.0630) | Total Time 0.00(0.00)\n",
      "Iter 2363 | Time 58.6727(61.1550) | Bit/dim 3.6648(3.6535) | Xent 0.7864(0.7920) | Loss 9.0160(9.6064) | Error 0.2811(0.2834) Steps 658(675.05) | Grad Norm 8.8642(7.1170) | Total Time 0.00(0.00)\n",
      "Iter 2364 | Time 63.4948(61.2252) | Bit/dim 3.6433(3.6532) | Xent 0.7751(0.7915) | Loss 8.9576(9.5870) | Error 0.2817(0.2833) Steps 658(674.54) | Grad Norm 3.9189(7.0211) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0394 | Time 24.0744, Epoch Time 399.9218(408.9973), Bit/dim 3.6488(best: 3.6492), Xent 0.8399, Loss 4.0688, Error 0.2959(best: 0.2893)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2365 | Time 68.0494(61.4299) | Bit/dim 3.6591(3.6533) | Xent 0.7926(0.7915) | Loss 12.7028(9.6804) | Error 0.2800(0.2832) Steps 664(674.22) | Grad Norm 7.6813(7.0409) | Total Time 0.00(0.00)\n",
      "Iter 2366 | Time 67.6581(61.6168) | Bit/dim 3.6475(3.6532) | Xent 0.7875(0.7914) | Loss 9.2007(9.6660) | Error 0.2840(0.2833) Steps 670(674.10) | Grad Norm 5.8855(7.0062) | Total Time 0.00(0.00)\n",
      "Iter 2367 | Time 63.6120(61.6766) | Bit/dim 3.6448(3.6529) | Xent 0.7749(0.7909) | Loss 9.2282(9.6529) | Error 0.2740(0.2830) Steps 676(674.15) | Grad Norm 4.8857(6.9426) | Total Time 0.00(0.00)\n",
      "Iter 2368 | Time 61.9709(61.6854) | Bit/dim 3.6453(3.6527) | Xent 0.7733(0.7904) | Loss 8.9896(9.6330) | Error 0.2801(0.2829) Steps 694(674.75) | Grad Norm 6.9449(6.9427) | Total Time 0.00(0.00)\n",
      "Iter 2369 | Time 60.1608(61.6397) | Bit/dim 3.6357(3.6522) | Xent 0.7546(0.7893) | Loss 9.2171(9.6205) | Error 0.2801(0.2828) Steps 664(674.43) | Grad Norm 4.7696(6.8775) | Total Time 0.00(0.00)\n",
      "Iter 2370 | Time 61.0413(61.6218) | Bit/dim 3.6538(3.6522) | Xent 0.7380(0.7878) | Loss 8.9260(9.5997) | Error 0.2660(0.2823) Steps 670(674.29) | Grad Norm 3.1736(6.7664) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0395 | Time 24.2236, Epoch Time 424.1949(409.4532), Bit/dim 3.6530(best: 3.6488), Xent 0.8400, Loss 4.0730, Error 0.2957(best: 0.2893)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2371 | Time 65.2943(61.7319) | Bit/dim 3.6517(3.6522) | Xent 0.7587(0.7869) | Loss 12.6279(9.6905) | Error 0.2751(0.2821) Steps 712(675.43) | Grad Norm 7.5807(6.7908) | Total Time 0.00(0.00)\n",
      "Iter 2372 | Time 61.8739(61.7362) | Bit/dim 3.6611(3.6525) | Xent 0.7723(0.7864) | Loss 9.0206(9.6704) | Error 0.2760(0.2819) Steps 652(674.72) | Grad Norm 5.4938(6.7519) | Total Time 0.00(0.00)\n",
      "Iter 2373 | Time 66.0359(61.8652) | Bit/dim 3.6626(3.6528) | Xent 0.7679(0.7859) | Loss 9.0083(9.6506) | Error 0.2710(0.2816) Steps 646(673.86) | Grad Norm 5.1159(6.7028) | Total Time 0.00(0.00)\n",
      "Iter 2374 | Time 62.1792(61.8746) | Bit/dim 3.6437(3.6525) | Xent 0.7748(0.7856) | Loss 9.0272(9.6319) | Error 0.2760(0.2814) Steps 694(674.46) | Grad Norm 6.1966(6.6876) | Total Time 0.00(0.00)\n",
      "Iter 2375 | Time 62.7907(61.9021) | Bit/dim 3.6500(3.6524) | Xent 0.7872(0.7856) | Loss 9.0127(9.6133) | Error 0.2790(0.2813) Steps 670(674.33) | Grad Norm 7.6998(6.7180) | Total Time 0.00(0.00)\n",
      "Iter 2376 | Time 59.8589(61.8408) | Bit/dim 3.6502(3.6524) | Xent 0.7736(0.7852) | Loss 9.0084(9.5952) | Error 0.2775(0.2812) Steps 676(674.38) | Grad Norm 4.8971(6.6634) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0396 | Time 24.1853, Epoch Time 418.9599(409.7384), Bit/dim 3.6474(best: 3.6488), Xent 0.8274, Loss 4.0611, Error 0.2910(best: 0.2893)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2377 | Time 62.6433(61.8649) | Bit/dim 3.6445(3.6521) | Xent 0.7733(0.7849) | Loss 12.6825(9.6878) | Error 0.2734(0.2810) Steps 688(674.79) | Grad Norm 4.1365(6.5876) | Total Time 0.00(0.00)\n",
      "Iter 2378 | Time 62.9870(61.8985) | Bit/dim 3.6441(3.6519) | Xent 0.7815(0.7848) | Loss 8.8859(9.6637) | Error 0.2750(0.2808) Steps 664(674.47) | Grad Norm 6.3944(6.5818) | Total Time 0.00(0.00)\n",
      "Iter 2379 | Time 62.3835(61.9131) | Bit/dim 3.6643(3.6523) | Xent 0.7596(0.7840) | Loss 9.1857(9.6494) | Error 0.2765(0.2807) Steps 688(674.87) | Grad Norm 4.7115(6.5257) | Total Time 0.00(0.00)\n",
      "Iter 2380 | Time 64.5947(61.9935) | Bit/dim 3.6461(3.6521) | Xent 0.7764(0.7838) | Loss 9.1741(9.6351) | Error 0.2831(0.2808) Steps 700(675.63) | Grad Norm 6.5919(6.5276) | Total Time 0.00(0.00)\n",
      "Iter 2381 | Time 63.1468(62.0281) | Bit/dim 3.6548(3.6522) | Xent 0.7586(0.7830) | Loss 9.1239(9.6198) | Error 0.2760(0.2806) Steps 718(676.90) | Grad Norm 6.6863(6.5324) | Total Time 0.00(0.00)\n",
      "Iter 2382 | Time 63.7114(62.0786) | Bit/dim 3.6517(3.6521) | Xent 0.7768(0.7829) | Loss 9.0913(9.6039) | Error 0.2728(0.2804) Steps 658(676.33) | Grad Norm 5.4877(6.5011) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0397 | Time 24.1207, Epoch Time 419.8716(410.0424), Bit/dim 3.6504(best: 3.6474), Xent 0.8278, Loss 4.0642, Error 0.2920(best: 0.2893)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2383 | Time 59.6185(62.0048) | Bit/dim 3.6397(3.6518) | Xent 0.7789(0.7827) | Loss 12.7831(9.6993) | Error 0.2790(0.2803) Steps 670(676.14) | Grad Norm 4.7558(6.4487) | Total Time 0.00(0.00)\n",
      "Iter 2384 | Time 62.6762(62.0250) | Bit/dim 3.6409(3.6514) | Xent 0.7890(0.7829) | Loss 9.1121(9.6817) | Error 0.2837(0.2804) Steps 670(675.96) | Grad Norm 7.3609(6.4761) | Total Time 0.00(0.00)\n",
      "Iter 2385 | Time 63.9240(62.0819) | Bit/dim 3.6489(3.6514) | Xent 0.7560(0.7821) | Loss 9.1744(9.6665) | Error 0.2659(0.2800) Steps 670(675.78) | Grad Norm 6.6926(6.4826) | Total Time 0.00(0.00)\n",
      "Iter 2386 | Time 60.6602(62.0393) | Bit/dim 3.6436(3.6511) | Xent 0.7682(0.7817) | Loss 9.1510(9.6510) | Error 0.2778(0.2799) Steps 676(675.78) | Grad Norm 5.2903(6.4468) | Total Time 0.00(0.00)\n",
      "Iter 2387 | Time 63.1129(62.0715) | Bit/dim 3.6579(3.6513) | Xent 0.7471(0.7807) | Loss 9.0147(9.6319) | Error 0.2658(0.2795) Steps 706(676.69) | Grad Norm 6.1896(6.4391) | Total Time 0.00(0.00)\n",
      "Iter 2388 | Time 59.6546(61.9990) | Bit/dim 3.6643(3.6517) | Xent 0.7576(0.7800) | Loss 8.9016(9.6100) | Error 0.2694(0.2792) Steps 682(676.85) | Grad Norm 5.0596(6.3977) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0398 | Time 24.0649, Epoch Time 410.0412(410.0424), Bit/dim 3.6530(best: 3.6474), Xent 0.8274, Loss 4.0667, Error 0.2905(best: 0.2893)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2389 | Time 55.8851(61.8156) | Bit/dim 3.6410(3.6514) | Xent 0.7684(0.7796) | Loss 12.3300(9.6916) | Error 0.2779(0.2792) Steps 670(676.64) | Grad Norm 4.9319(6.3537) | Total Time 0.00(0.00)\n",
      "Iter 2390 | Time 58.4369(61.7142) | Bit/dim 3.6566(3.6516) | Xent 0.7647(0.7792) | Loss 8.7793(9.6642) | Error 0.2729(0.2790) Steps 682(676.80) | Grad Norm 3.5875(6.2707) | Total Time 0.00(0.00)\n",
      "Iter 2391 | Time 62.0081(61.7230) | Bit/dim 3.6516(3.6516) | Xent 0.7675(0.7788) | Loss 9.1030(9.6474) | Error 0.2668(0.2786) Steps 700(677.50) | Grad Norm 5.7387(6.2548) | Total Time 0.00(0.00)\n",
      "Iter 2392 | Time 59.0318(61.6423) | Bit/dim 3.6605(3.6518) | Xent 0.7452(0.7778) | Loss 8.8073(9.6222) | Error 0.2634(0.2781) Steps 688(677.82) | Grad Norm 4.6600(6.2069) | Total Time 0.00(0.00)\n",
      "Iter 2393 | Time 59.3415(61.5733) | Bit/dim 3.6469(3.6517) | Xent 0.7751(0.7777) | Loss 9.0906(9.6062) | Error 0.2734(0.2780) Steps 694(678.30) | Grad Norm 5.4002(6.1827) | Total Time 0.00(0.00)\n",
      "Iter 2394 | Time 62.9568(61.6148) | Bit/dim 3.6575(3.6519) | Xent 0.7740(0.7776) | Loss 8.9651(9.5870) | Error 0.2739(0.2779) Steps 670(678.05) | Grad Norm 4.8767(6.1436) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0399 | Time 24.4631, Epoch Time 398.3763(409.6924), Bit/dim 3.6491(best: 3.6474), Xent 0.8526, Loss 4.0754, Error 0.2961(best: 0.2893)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2395 | Time 61.3672(61.6073) | Bit/dim 3.6562(3.6520) | Xent 0.8235(0.7790) | Loss 12.7943(9.6832) | Error 0.2993(0.2785) Steps 664(677.63) | Grad Norm 9.8058(6.2534) | Total Time 0.00(0.00)\n",
      "Iter 2396 | Time 57.1339(61.4731) | Bit/dim 3.6562(3.6521) | Xent 0.7898(0.7793) | Loss 9.2324(9.6697) | Error 0.2827(0.2787) Steps 688(677.94) | Grad Norm 11.2208(6.4024) | Total Time 0.00(0.00)\n",
      "Iter 2397 | Time 67.4378(61.6521) | Bit/dim 3.6358(3.6516) | Xent 0.7340(0.7780) | Loss 9.0309(9.6505) | Error 0.2619(0.2781) Steps 646(676.98) | Grad Norm 5.6186(6.3789) | Total Time 0.00(0.00)\n",
      "Iter 2398 | Time 61.0783(61.6349) | Bit/dim 3.6682(3.6521) | Xent 0.7623(0.7775) | Loss 9.1406(9.6352) | Error 0.2670(0.2778) Steps 670(676.77) | Grad Norm 6.7589(6.3903) | Total Time 0.00(0.00)\n",
      "Iter 2399 | Time 60.2491(61.5933) | Bit/dim 3.6556(3.6522) | Xent 0.7752(0.7774) | Loss 8.6971(9.6071) | Error 0.2804(0.2779) Steps 682(676.93) | Grad Norm 5.6388(6.3678) | Total Time 0.00(0.00)\n",
      "Iter 2400 | Time 58.9326(61.5135) | Bit/dim 3.6509(3.6522) | Xent 0.7921(0.7779) | Loss 9.0453(9.5902) | Error 0.2867(0.2782) Steps 688(677.26) | Grad Norm 9.8904(6.4735) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0400 | Time 24.0781, Epoch Time 406.5802(409.5990), Bit/dim 3.6493(best: 3.6474), Xent 0.8261, Loss 4.0624, Error 0.2906(best: 0.2893)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2401 | Time 59.5654(61.4550) | Bit/dim 3.6499(3.6521) | Xent 0.7614(0.7774) | Loss 12.2418(9.6698) | Error 0.2655(0.2778) Steps 682(677.40) | Grad Norm 6.0394(6.4604) | Total Time 0.00(0.00)\n",
      "Iter 2402 | Time 64.9545(61.5600) | Bit/dim 3.6608(3.6524) | Xent 0.7814(0.7775) | Loss 9.0254(9.6505) | Error 0.2785(0.2778) Steps 706(678.26) | Grad Norm 8.3174(6.5161) | Total Time 0.00(0.00)\n",
      "Iter 2403 | Time 58.6418(61.4725) | Bit/dim 3.6620(3.6527) | Xent 0.7819(0.7776) | Loss 9.1139(9.6344) | Error 0.2835(0.2780) Steps 664(677.83) | Grad Norm 5.3102(6.4800) | Total Time 0.00(0.00)\n",
      "Iter 2404 | Time 58.8456(61.3937) | Bit/dim 3.6603(3.6529) | Xent 0.7804(0.7777) | Loss 9.1324(9.6193) | Error 0.2736(0.2778) Steps 664(677.42) | Grad Norm 7.4233(6.5083) | Total Time 0.00(0.00)\n",
      "Iter 2405 | Time 63.6697(61.4619) | Bit/dim 3.6439(3.6526) | Xent 0.7778(0.7777) | Loss 8.9290(9.5986) | Error 0.2810(0.2779) Steps 706(678.28) | Grad Norm 5.5372(6.4791) | Total Time 0.00(0.00)\n",
      "Iter 2406 | Time 60.4355(61.4312) | Bit/dim 3.6574(3.6528) | Xent 0.7826(0.7779) | Loss 8.9642(9.5796) | Error 0.2812(0.2780) Steps 676(678.21) | Grad Norm 10.2597(6.5926) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0401 | Time 24.3073, Epoch Time 406.6473(409.5105), Bit/dim 3.6572(best: 3.6474), Xent 0.8438, Loss 4.0791, Error 0.2971(best: 0.2893)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2407 | Time 64.3395(61.5184) | Bit/dim 3.6467(3.6526) | Xent 0.7729(0.7777) | Loss 12.7128(9.6736) | Error 0.2686(0.2778) Steps 718(679.40) | Grad Norm 7.5707(6.6219) | Total Time 0.00(0.00)\n",
      "Iter 2408 | Time 61.5103(61.5182) | Bit/dim 3.6492(3.6525) | Xent 0.8532(0.7800) | Loss 9.1274(9.6572) | Error 0.3033(0.2785) Steps 688(679.66) | Grad Norm 13.8463(6.8386) | Total Time 0.00(0.00)\n",
      "Iter 2409 | Time 64.7858(61.6162) | Bit/dim 3.6561(3.6526) | Xent 0.8933(0.7834) | Loss 9.0715(9.6396) | Error 0.3191(0.2797) Steps 712(680.63) | Grad Norm 17.4562(7.1572) | Total Time 0.00(0.00)\n",
      "Iter 2410 | Time 64.8335(61.7127) | Bit/dim 3.6706(3.6531) | Xent 0.9438(0.7882) | Loss 9.2319(9.6274) | Error 0.3433(0.2816) Steps 676(680.49) | Grad Norm 19.5497(7.5289) | Total Time 0.00(0.00)\n",
      "Iter 2411 | Time 65.1871(61.8169) | Bit/dim 3.6539(3.6532) | Xent 0.8314(0.7895) | Loss 9.1310(9.6125) | Error 0.2989(0.2822) Steps 700(681.08) | Grad Norm 12.5469(7.6795) | Total Time 0.00(0.00)\n",
      "Iter 2412 | Time 63.1248(61.8562) | Bit/dim 3.6740(3.6538) | Xent 0.8964(0.7927) | Loss 9.2692(9.6022) | Error 0.3179(0.2832) Steps 658(680.38) | Grad Norm 17.7535(7.9817) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0402 | Time 23.7134, Epoch Time 423.6703(409.9353), Bit/dim 3.6640(best: 3.6474), Xent 0.9550, Loss 4.1415, Error 0.3339(best: 0.2893)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2413 | Time 62.4378(61.8736) | Bit/dim 3.6719(3.6543) | Xent 0.9084(0.7962) | Loss 13.2870(9.7127) | Error 0.3211(0.2844) Steps 676(680.25) | Grad Norm 17.1891(8.2579) | Total Time 0.00(0.00)\n",
      "Iter 2414 | Time 62.5203(61.8930) | Bit/dim 3.6690(3.6548) | Xent 0.8711(0.7984) | Loss 9.0610(9.6932) | Error 0.3105(0.2851) Steps 670(679.95) | Grad Norm 9.3490(8.2906) | Total Time 0.00(0.00)\n",
      "Iter 2415 | Time 69.9996(62.1362) | Bit/dim 3.6445(3.6545) | Xent 0.8043(0.7986) | Loss 9.2297(9.6793) | Error 0.2866(0.2852) Steps 724(681.27) | Grad Norm 8.9029(8.3090) | Total Time 0.00(0.00)\n",
      "Iter 2416 | Time 61.7659(62.1251) | Bit/dim 3.6784(3.6552) | Xent 0.8653(0.8006) | Loss 9.1965(9.6648) | Error 0.3131(0.2860) Steps 688(681.47) | Grad Norm 11.9853(8.4193) | Total Time 0.00(0.00)\n",
      "Iter 2417 | Time 61.8273(62.1162) | Bit/dim 3.6558(3.6552) | Xent 0.8116(0.8009) | Loss 9.0806(9.6473) | Error 0.2910(0.2862) Steps 688(681.67) | Grad Norm 7.5721(8.3939) | Total Time 0.00(0.00)\n",
      "Iter 2418 | Time 60.6960(62.0736) | Bit/dim 3.6675(3.6556) | Xent 0.8414(0.8021) | Loss 9.0307(9.6288) | Error 0.2971(0.2865) Steps 676(681.50) | Grad Norm 14.6409(8.5813) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0403 | Time 23.5506, Epoch Time 419.0822(410.2097), Bit/dim 3.6724(best: 3.6474), Xent 0.8999, Loss 4.1224, Error 0.3189(best: 0.2893)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2419 | Time 60.3217(62.0210) | Bit/dim 3.6692(3.6560) | Xent 0.8717(0.8042) | Loss 12.5768(9.7172) | Error 0.3066(0.2871) Steps 682(681.51) | Grad Norm 15.4355(8.7869) | Total Time 0.00(0.00)\n",
      "Iter 2420 | Time 61.1288(61.9943) | Bit/dim 3.6592(3.6561) | Xent 0.8899(0.8068) | Loss 9.1463(9.7001) | Error 0.3166(0.2880) Steps 688(681.71) | Grad Norm 10.3354(8.8334) | Total Time 0.00(0.00)\n",
      "Iter 2421 | Time 59.9944(61.9343) | Bit/dim 3.6685(3.6564) | Xent 0.7849(0.8061) | Loss 9.1197(9.6827) | Error 0.2794(0.2877) Steps 700(682.25) | Grad Norm 5.6749(8.7386) | Total Time 0.00(0.00)\n",
      "Iter 2422 | Time 65.0127(62.0266) | Bit/dim 3.6692(3.6568) | Xent 0.7989(0.8059) | Loss 8.9935(9.6620) | Error 0.2854(0.2877) Steps 694(682.61) | Grad Norm 8.2313(8.7234) | Total Time 0.00(0.00)\n",
      "Iter 2423 | Time 64.9448(62.1142) | Bit/dim 3.6720(3.6573) | Xent 0.7768(0.8050) | Loss 9.1430(9.6464) | Error 0.2770(0.2873) Steps 676(682.41) | Grad Norm 4.3171(8.5912) | Total Time 0.00(0.00)\n",
      "Iter 2424 | Time 67.6468(62.2801) | Bit/dim 3.6614(3.6574) | Xent 0.8294(0.8058) | Loss 9.0391(9.6282) | Error 0.2934(0.2875) Steps 718(683.48) | Grad Norm 9.5345(8.6195) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0404 | Time 24.4848, Epoch Time 419.9624(410.5023), Bit/dim 3.6663(best: 3.6474), Xent 0.8282, Loss 4.0805, Error 0.2930(best: 0.2893)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2425 | Time 58.8197(62.1763) | Bit/dim 3.6571(3.6574) | Xent 0.7689(0.8047) | Loss 12.6642(9.7193) | Error 0.2791(0.2873) Steps 694(683.79) | Grad Norm 4.1091(8.4842) | Total Time 0.00(0.00)\n",
      "Iter 2426 | Time 63.7147(62.2225) | Bit/dim 3.6563(3.6574) | Xent 0.7978(0.8045) | Loss 9.1148(9.7011) | Error 0.2854(0.2872) Steps 670(683.38) | Grad Norm 6.7721(8.4328) | Total Time 0.00(0.00)\n",
      "Iter 2427 | Time 67.3532(62.3764) | Bit/dim 3.6577(3.6574) | Xent 0.7801(0.8037) | Loss 8.8491(9.6756) | Error 0.2790(0.2870) Steps 688(683.52) | Grad Norm 6.3788(8.3712) | Total Time 0.00(0.00)\n",
      "Iter 2428 | Time 67.1102(62.5184) | Bit/dim 3.6551(3.6573) | Xent 0.7938(0.8034) | Loss 8.9306(9.6532) | Error 0.2786(0.2867) Steps 712(684.37) | Grad Norm 5.7656(8.2931) | Total Time 0.00(0.00)\n",
      "Iter 2429 | Time 66.8021(62.6469) | Bit/dim 3.6648(3.6575) | Xent 0.7687(0.8024) | Loss 9.0860(9.6362) | Error 0.2796(0.2865) Steps 694(684.66) | Grad Norm 4.3231(8.1740) | Total Time 0.00(0.00)\n",
      "Iter 2430 | Time 62.4649(62.6415) | Bit/dim 3.6574(3.6575) | Xent 0.7889(0.8020) | Loss 8.8270(9.6119) | Error 0.2819(0.2864) Steps 694(684.94) | Grad Norm 5.7540(8.1014) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0405 | Time 23.9302, Epoch Time 426.2726(410.9754), Bit/dim 3.6634(best: 3.6474), Xent 0.8242, Loss 4.0754, Error 0.2884(best: 0.2893)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2431 | Time 62.2099(62.6285) | Bit/dim 3.6683(3.6578) | Xent 0.7629(0.8008) | Loss 12.6342(9.7026) | Error 0.2740(0.2860) Steps 688(685.03) | Grad Norm 4.6352(7.9974) | Total Time 0.00(0.00)\n",
      "Iter 2432 | Time 62.3013(62.6187) | Bit/dim 3.6375(3.6572) | Xent 0.7663(0.7998) | Loss 9.1896(9.6872) | Error 0.2728(0.2856) Steps 700(685.48) | Grad Norm 3.2279(7.8543) | Total Time 0.00(0.00)\n",
      "Iter 2433 | Time 61.2651(62.5781) | Bit/dim 3.6609(3.6573) | Xent 0.7747(0.7990) | Loss 8.7816(9.6601) | Error 0.2719(0.2852) Steps 688(685.56) | Grad Norm 3.5552(7.7253) | Total Time 0.00(0.00)\n",
      "Iter 2434 | Time 61.5891(62.5484) | Bit/dim 3.6619(3.6575) | Xent 0.7505(0.7976) | Loss 8.9976(9.6402) | Error 0.2690(0.2847) Steps 694(685.81) | Grad Norm 3.4889(7.5982) | Total Time 0.00(0.00)\n",
      "Iter 2435 | Time 65.9682(62.6510) | Bit/dim 3.6665(3.6578) | Xent 0.7638(0.7966) | Loss 9.0957(9.6238) | Error 0.2754(0.2844) Steps 694(686.06) | Grad Norm 2.6628(7.4502) | Total Time 0.00(0.00)\n",
      "Iter 2436 | Time 64.6002(62.7095) | Bit/dim 3.6677(3.6581) | Xent 0.7800(0.7961) | Loss 8.7083(9.5964) | Error 0.2814(0.2843) Steps 700(686.47) | Grad Norm 3.4815(7.3311) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0406 | Time 24.1879, Epoch Time 418.2334(411.1931), Bit/dim 3.6856(best: 3.6474), Xent 0.8271, Loss 4.0992, Error 0.2899(best: 0.2884)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2437 | Time 61.3445(62.6685) | Bit/dim 3.6819(3.6588) | Xent 0.7737(0.7954) | Loss 12.6211(9.6871) | Error 0.2771(0.2841) Steps 658(685.62) | Grad Norm 7.1008(7.3242) | Total Time 0.00(0.00)\n",
      "Iter 2438 | Time 65.8006(62.7625) | Bit/dim 3.7160(3.6605) | Xent 0.8133(0.7959) | Loss 9.1177(9.6700) | Error 0.2899(0.2843) Steps 676(685.33) | Grad Norm 5.9529(7.2831) | Total Time 0.00(0.00)\n",
      "Iter 2439 | Time 68.6753(62.9399) | Bit/dim 3.7421(3.6629) | Xent 0.7631(0.7949) | Loss 9.2275(9.6568) | Error 0.2768(0.2841) Steps 706(685.95) | Grad Norm 5.6282(7.2334) | Total Time 0.00(0.00)\n",
      "Iter 2440 | Time 75.1827(63.3072) | Bit/dim 3.7885(3.6667) | Xent 0.7796(0.7945) | Loss 9.3334(9.6471) | Error 0.2835(0.2840) Steps 688(686.01) | Grad Norm 4.4292(7.1493) | Total Time 0.00(0.00)\n",
      "Iter 2441 | Time 82.7746(63.8912) | Bit/dim 3.7744(3.6699) | Xent 0.7554(0.7933) | Loss 9.4817(9.6421) | Error 0.2694(0.2836) Steps 670(685.53) | Grad Norm 4.6137(7.0732) | Total Time 0.00(0.00)\n",
      "Iter 2442 | Time 68.3350(64.0245) | Bit/dim 3.7679(3.6729) | Xent 0.7868(0.7931) | Loss 9.3043(9.6320) | Error 0.2789(0.2835) Steps 730(686.87) | Grad Norm 4.6982(7.0020) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0407 | Time 24.6072, Epoch Time 462.7038(412.7384), Bit/dim 3.7564(best: 3.6474), Xent 0.8293, Loss 4.1711, Error 0.2920(best: 0.2884)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2443 | Time 69.6180(64.1923) | Bit/dim 3.7507(3.6752) | Xent 0.7736(0.7925) | Loss 13.0074(9.7332) | Error 0.2755(0.2832) Steps 700(687.26) | Grad Norm 4.1271(6.9157) | Total Time 0.00(0.00)\n",
      "Iter 2444 | Time 69.6029(64.3546) | Bit/dim 3.7541(3.6776) | Xent 0.7712(0.7919) | Loss 9.1596(9.7160) | Error 0.2745(0.2830) Steps 742(688.90) | Grad Norm 4.1404(6.8325) | Total Time 0.00(0.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_rl.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 8000 --test_batch_size 5000 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_rl_stdlearnscale_30_run1 --resume ../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_rl_stdlearnscale_30_run1/epoch_400_checkpt.pth --seed 1 --conditional True --controlled_tol True --train_mode semisup --lr 0.0001 --warmup_iters 1000 --atol 1e-4  --rtol 1e-4 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5 --scale_fac 1.0 --gate cnn2 --scale_std 30.0 --max_grad_norm 20.0\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
