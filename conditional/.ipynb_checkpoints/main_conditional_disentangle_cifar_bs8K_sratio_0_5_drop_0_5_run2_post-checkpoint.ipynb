{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_cifar.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "if args.controlled_tol:\n",
      "    import lib.odenvp_conditional_tol as odenvp\n",
      "else:\n",
      "    import lib.odenvp_conditional as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            loss.backward()\n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=0.0001, autoencode=False, batch_norm=False, batch_size=8000, batch_size_schedule='', begin_epoch=1, condition_ratio=0.5, conditional=True, controlled_tol=True, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=1, lr=0.0001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume='../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_run1/epoch_400_checkpt.pth', rtol=0.0001, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_run1', seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=5000, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000.0, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1414198\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc.py:286: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "Iter 2401 | Time 110.1974(57.8078) | Bit/dim 3.6719(3.6607) | Xent 0.8723(0.8958) | Loss 4.1081(4.1086) | Error 0.3131(0.3215) Steps 568(572.88) | Grad Norm 3.3875(5.0355) | Total Time 14.00(14.00)\n",
      "Iter 2402 | Time 57.2417(57.7909) | Bit/dim 3.6603(3.6607) | Xent 0.8736(0.8951) | Loss 4.0970(4.1082) | Error 0.3077(0.3211) Steps 586(573.27) | Grad Norm 3.1472(4.9789) | Total Time 14.00(14.00)\n",
      "Iter 2403 | Time 53.8240(57.6719) | Bit/dim 3.6609(3.6607) | Xent 0.8676(0.8943) | Loss 4.0947(4.1078) | Error 0.3114(0.3208) Steps 580(573.47) | Grad Norm 2.3042(4.8986) | Total Time 14.00(14.00)\n",
      "Iter 2404 | Time 56.6306(57.6406) | Bit/dim 3.6509(3.6604) | Xent 0.8702(0.8936) | Loss 4.0860(4.1072) | Error 0.3035(0.3203) Steps 580(573.67) | Grad Norm 1.6923(4.8024) | Total Time 14.00(14.00)\n",
      "Iter 2405 | Time 53.9262(57.5292) | Bit/dim 3.6545(3.6602) | Xent 0.8594(0.8926) | Loss 4.0842(4.1065) | Error 0.3085(0.3199) Steps 580(573.86) | Grad Norm 1.3937(4.7002) | Total Time 14.00(14.00)\n",
      "Iter 2406 | Time 59.4252(57.5861) | Bit/dim 3.6487(3.6599) | Xent 0.8649(0.8917) | Loss 4.0812(4.1057) | Error 0.3110(0.3196) Steps 574(573.86) | Grad Norm 1.6278(4.6080) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0401 | Time 37.4990, Epoch Time 444.8395(373.7716), Bit/dim 3.6566(best: inf), Xent 0.8795, Loss 4.0964, Error 0.3123(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2407 | Time 71.9756(58.0178) | Bit/dim 3.6608(3.6599) | Xent 0.8853(0.8915) | Loss 4.1034(4.1057) | Error 0.3195(0.3196) Steps 574(573.87) | Grad Norm 2.1701(4.5349) | Total Time 14.00(14.00)\n",
      "Iter 2408 | Time 60.9054(58.1044) | Bit/dim 3.6540(3.6597) | Xent 0.8491(0.8903) | Loss 4.0786(4.1049) | Error 0.3027(0.3191) Steps 580(574.05) | Grad Norm 2.0419(4.4601) | Total Time 14.00(14.00)\n",
      "Iter 2409 | Time 58.8210(58.1259) | Bit/dim 3.6546(3.6596) | Xent 0.8589(0.8893) | Loss 4.0840(4.1042) | Error 0.3087(0.3188) Steps 574(574.05) | Grad Norm 2.0653(4.3882) | Total Time 14.00(14.00)\n",
      "Iter 2410 | Time 55.3695(58.0432) | Bit/dim 3.6610(3.6596) | Xent 0.8709(0.8888) | Loss 4.0965(4.1040) | Error 0.3191(0.3188) Steps 574(574.05) | Grad Norm 1.8889(4.3132) | Total Time 14.00(14.00)\n",
      "Iter 2411 | Time 53.0271(57.8927) | Bit/dim 3.6484(3.6593) | Xent 0.8505(0.8876) | Loss 4.0737(4.1031) | Error 0.3010(0.3183) Steps 574(574.05) | Grad Norm 1.2960(4.2227) | Total Time 14.00(14.00)\n",
      "Iter 2412 | Time 57.3265(57.8757) | Bit/dim 3.6479(3.6589) | Xent 0.8451(0.8864) | Loss 4.0704(4.1021) | Error 0.2941(0.3176) Steps 580(574.23) | Grad Norm 1.2747(4.1343) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0402 | Time 22.5138, Epoch Time 395.6206(374.4271), Bit/dim 3.6529(best: 3.6566), Xent 0.8720, Loss 4.0889, Error 0.3016(best: 0.3123)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2413 | Time 53.1265(57.7332) | Bit/dim 3.6627(3.6590) | Xent 0.8549(0.8854) | Loss 4.0901(4.1017) | Error 0.3040(0.3172) Steps 580(574.40) | Grad Norm 1.4330(4.0533) | Total Time 14.00(14.00)\n",
      "Iter 2414 | Time 56.2575(57.6890) | Bit/dim 3.6541(3.6589) | Xent 0.8605(0.8847) | Loss 4.0843(4.1012) | Error 0.3045(0.3168) Steps 580(574.57) | Grad Norm 1.6018(3.9797) | Total Time 14.00(14.00)\n",
      "Iter 2415 | Time 54.6616(57.5982) | Bit/dim 3.6425(3.6584) | Xent 0.8566(0.8838) | Loss 4.0708(4.1003) | Error 0.3070(0.3165) Steps 574(574.55) | Grad Norm 1.4699(3.9044) | Total Time 14.00(14.00)\n",
      "Iter 2416 | Time 52.2832(57.4387) | Bit/dim 3.6453(3.6580) | Xent 0.8731(0.8835) | Loss 4.0818(4.0998) | Error 0.3091(0.3163) Steps 568(574.35) | Grad Norm 1.3412(3.8275) | Total Time 14.00(14.00)\n",
      "Iter 2417 | Time 54.7839(57.3591) | Bit/dim 3.6477(3.6577) | Xent 0.8421(0.8823) | Loss 4.0687(4.0988) | Error 0.3019(0.3158) Steps 574(574.34) | Grad Norm 1.2096(3.7490) | Total Time 14.00(14.00)\n",
      "Iter 2418 | Time 59.4137(57.4207) | Bit/dim 3.6612(3.6578) | Xent 0.8424(0.8811) | Loss 4.0824(4.0983) | Error 0.3037(0.3155) Steps 574(574.33) | Grad Norm 0.9070(3.6637) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0403 | Time 22.1811, Epoch Time 368.5146(374.2497), Bit/dim 3.6530(best: 3.6529), Xent 0.8653, Loss 4.0857, Error 0.3037(best: 0.3016)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2419 | Time 53.4988(57.3030) | Bit/dim 3.6455(3.6574) | Xent 0.8325(0.8796) | Loss 4.0617(4.0972) | Error 0.2971(0.3149) Steps 574(574.32) | Grad Norm 1.0932(3.5866) | Total Time 14.00(14.00)\n",
      "Iter 2420 | Time 59.1390(57.3581) | Bit/dim 3.6600(3.6575) | Xent 0.8491(0.8787) | Loss 4.0845(4.0969) | Error 0.3024(0.3146) Steps 586(574.67) | Grad Norm 1.1624(3.5139) | Total Time 14.00(14.00)\n",
      "Iter 2421 | Time 53.7339(57.2494) | Bit/dim 3.6623(3.6577) | Xent 0.8588(0.8781) | Loss 4.0917(4.0967) | Error 0.3049(0.3143) Steps 574(574.65) | Grad Norm 0.9627(3.4373) | Total Time 14.00(14.00)\n",
      "Iter 2422 | Time 55.3907(57.1936) | Bit/dim 3.6385(3.6571) | Xent 0.8493(0.8772) | Loss 4.0632(4.0957) | Error 0.3016(0.3139) Steps 574(574.63) | Grad Norm 1.3089(3.3735) | Total Time 14.00(14.00)\n",
      "Iter 2423 | Time 56.5352(57.1739) | Bit/dim 3.6552(3.6570) | Xent 0.8558(0.8766) | Loss 4.0831(4.0953) | Error 0.3114(0.3138) Steps 574(574.61) | Grad Norm 1.1181(3.3058) | Total Time 14.00(14.00)\n",
      "Iter 2424 | Time 53.8764(57.0750) | Bit/dim 3.6505(3.6568) | Xent 0.8436(0.8756) | Loss 4.0723(4.0946) | Error 0.3047(0.3135) Steps 574(574.60) | Grad Norm 1.0486(3.2381) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0404 | Time 22.2531, Epoch Time 370.1708(374.1273), Bit/dim 3.6538(best: 3.6529), Xent 0.8628, Loss 4.0852, Error 0.3002(best: 0.3016)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2425 | Time 56.6409(57.0619) | Bit/dim 3.6461(3.6565) | Xent 0.8447(0.8747) | Loss 4.0685(4.0938) | Error 0.3023(0.3132) Steps 574(574.58) | Grad Norm 1.2708(3.1791) | Total Time 14.00(14.00)\n",
      "Iter 2426 | Time 54.9834(56.9996) | Bit/dim 3.6466(3.6562) | Xent 0.8483(0.8739) | Loss 4.0708(4.0931) | Error 0.3006(0.3128) Steps 574(574.56) | Grad Norm 1.2434(3.1210) | Total Time 14.00(14.00)\n",
      "Iter 2427 | Time 55.5795(56.9570) | Bit/dim 3.6580(3.6563) | Xent 0.8381(0.8728) | Loss 4.0771(4.0927) | Error 0.3004(0.3124) Steps 574(574.54) | Grad Norm 0.8804(3.0538) | Total Time 14.00(14.00)\n",
      "Iter 2428 | Time 54.7988(56.8922) | Bit/dim 3.6564(3.6563) | Xent 0.8553(0.8723) | Loss 4.0840(4.0924) | Error 0.3087(0.3123) Steps 574(574.53) | Grad Norm 0.7448(2.9845) | Total Time 14.00(14.00)\n",
      "Iter 2429 | Time 54.4222(56.8181) | Bit/dim 3.6522(3.6561) | Xent 0.8368(0.8712) | Loss 4.0706(4.0918) | Error 0.3009(0.3120) Steps 574(574.51) | Grad Norm 0.6890(2.9157) | Total Time 14.00(14.00)\n",
      "Iter 2430 | Time 53.3252(56.7133) | Bit/dim 3.6482(3.6559) | Xent 0.8354(0.8701) | Loss 4.0660(4.0910) | Error 0.3024(0.3117) Steps 568(574.32) | Grad Norm 0.7005(2.8492) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0405 | Time 22.1186, Epoch Time 368.1272(373.9473), Bit/dim 3.6534(best: 3.6529), Xent 0.8601, Loss 4.0835, Error 0.3030(best: 0.3002)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2431 | Time 53.6817(56.6224) | Bit/dim 3.6505(3.6557) | Xent 0.8338(0.8691) | Loss 4.0674(4.0903) | Error 0.2979(0.3113) Steps 574(574.31) | Grad Norm 0.4538(2.7773) | Total Time 14.00(14.00)\n",
      "Iter 2432 | Time 54.8469(56.5691) | Bit/dim 3.6524(3.6556) | Xent 0.8172(0.8675) | Loss 4.0610(4.0894) | Error 0.2999(0.3109) Steps 568(574.12) | Grad Norm 0.7893(2.7177) | Total Time 14.00(14.00)\n",
      "Iter 2433 | Time 54.4765(56.5063) | Bit/dim 3.6462(3.6554) | Xent 0.8227(0.8662) | Loss 4.0576(4.0884) | Error 0.2906(0.3103) Steps 574(574.11) | Grad Norm 0.6178(2.6547) | Total Time 14.00(14.00)\n",
      "Iter 2434 | Time 53.9521(56.4297) | Bit/dim 3.6522(3.6553) | Xent 0.8614(0.8660) | Loss 4.0829(4.0883) | Error 0.3070(0.3102) Steps 574(574.11) | Grad Norm 0.7264(2.5969) | Total Time 14.00(14.00)\n",
      "Iter 2435 | Time 55.9381(56.4150) | Bit/dim 3.6629(3.6555) | Xent 0.8419(0.8653) | Loss 4.0839(4.0881) | Error 0.3049(0.3101) Steps 574(574.11) | Grad Norm 0.5696(2.5360) | Total Time 14.00(14.00)\n",
      "Iter 2436 | Time 55.7306(56.3944) | Bit/dim 3.6465(3.6552) | Xent 0.8540(0.8649) | Loss 4.0735(4.0877) | Error 0.3075(0.3100) Steps 568(573.92) | Grad Norm 0.4944(2.4748) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0406 | Time 21.7519, Epoch Time 365.9161(373.7064), Bit/dim 3.6526(best: 3.6529), Xent 0.8608, Loss 4.0830, Error 0.3035(best: 0.3002)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2437 | Time 56.0447(56.3839) | Bit/dim 3.6525(3.6551) | Xent 0.8346(0.8640) | Loss 4.0698(4.0872) | Error 0.2997(0.3097) Steps 580(574.11) | Grad Norm 0.6237(2.4193) | Total Time 14.00(14.00)\n",
      "Iter 2438 | Time 55.0450(56.3438) | Bit/dim 3.6489(3.6550) | Xent 0.8361(0.8632) | Loss 4.0669(4.0866) | Error 0.2976(0.3093) Steps 568(573.92) | Grad Norm 0.5093(2.3620) | Total Time 14.00(14.00)\n",
      "Iter 2439 | Time 58.5030(56.4086) | Bit/dim 3.6614(3.6551) | Xent 0.8217(0.8620) | Loss 4.0722(4.0861) | Error 0.3014(0.3091) Steps 580(574.11) | Grad Norm 0.5437(2.3074) | Total Time 14.00(14.00)\n",
      "Iter 2440 | Time 54.2982(56.3452) | Bit/dim 3.6313(3.6544) | Xent 0.8426(0.8614) | Loss 4.0526(4.0851) | Error 0.3037(0.3089) Steps 568(573.92) | Grad Norm 0.6941(2.2590) | Total Time 14.00(14.00)\n",
      "Iter 2441 | Time 57.6579(56.3846) | Bit/dim 3.6548(3.6544) | Xent 0.8383(0.8607) | Loss 4.0740(4.0848) | Error 0.3000(0.3087) Steps 586(574.28) | Grad Norm 0.6299(2.2101) | Total Time 14.00(14.00)\n",
      "Iter 2442 | Time 59.6274(56.4819) | Bit/dim 3.6570(3.6545) | Xent 0.8307(0.8598) | Loss 4.0723(4.0844) | Error 0.2941(0.3082) Steps 580(574.46) | Grad Norm 0.4574(2.1576) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0407 | Time 22.0406, Epoch Time 378.8823(373.8617), Bit/dim 3.6521(best: 3.6526), Xent 0.8579, Loss 4.0810, Error 0.2998(best: 0.3002)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2443 | Time 56.5311(56.4834) | Bit/dim 3.6458(3.6543) | Xent 0.8362(0.8591) | Loss 4.0639(4.0838) | Error 0.3020(0.3080) Steps 568(574.26) | Grad Norm 0.5793(2.1102) | Total Time 14.00(14.00)\n",
      "Iter 2444 | Time 55.1261(56.4427) | Bit/dim 3.6455(3.6540) | Xent 0.8428(0.8586) | Loss 4.0669(4.0833) | Error 0.2971(0.3077) Steps 574(574.25) | Grad Norm 0.4241(2.0596) | Total Time 14.00(14.00)\n",
      "Iter 2445 | Time 56.6604(56.4492) | Bit/dim 3.6563(3.6541) | Xent 0.8445(0.8582) | Loss 4.0785(4.0831) | Error 0.3096(0.3078) Steps 568(574.07) | Grad Norm 0.7294(2.0197) | Total Time 14.00(14.00)\n",
      "Iter 2446 | Time 56.0437(56.4370) | Bit/dim 3.6550(3.6541) | Xent 0.8328(0.8574) | Loss 4.0714(4.0828) | Error 0.2980(0.3075) Steps 574(574.07) | Grad Norm 0.5631(1.9760) | Total Time 14.00(14.00)\n",
      "Iter 2447 | Time 52.7864(56.3275) | Bit/dim 3.6506(3.6540) | Xent 0.8406(0.8569) | Loss 4.0709(4.0824) | Error 0.3034(0.3074) Steps 568(573.88) | Grad Norm 0.4972(1.9317) | Total Time 14.00(14.00)\n",
      "Iter 2448 | Time 54.4267(56.2705) | Bit/dim 3.6546(3.6540) | Xent 0.8417(0.8564) | Loss 4.0755(4.0822) | Error 0.2953(0.3070) Steps 568(573.71) | Grad Norm 0.5776(1.8910) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0408 | Time 21.8726, Epoch Time 369.0579(373.7176), Bit/dim 3.6535(best: 3.6521), Xent 0.8582, Loss 4.0825, Error 0.3033(best: 0.2998)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2449 | Time 56.0605(56.2642) | Bit/dim 3.6429(3.6537) | Xent 0.8396(0.8559) | Loss 4.0628(4.0816) | Error 0.3011(0.3068) Steps 574(573.72) | Grad Norm 0.4023(1.8464) | Total Time 14.00(14.00)\n",
      "Iter 2450 | Time 51.8808(56.1327) | Bit/dim 3.6563(3.6538) | Xent 0.8323(0.8552) | Loss 4.0725(4.0814) | Error 0.3036(0.3067) Steps 568(573.54) | Grad Norm 0.6492(1.8105) | Total Time 14.00(14.00)\n",
      "Iter 2451 | Time 52.8889(56.0354) | Bit/dim 3.6488(3.6536) | Xent 0.8306(0.8545) | Loss 4.0641(4.0809) | Error 0.2923(0.3063) Steps 568(573.38) | Grad Norm 0.5179(1.7717) | Total Time 14.00(14.00)\n",
      "Iter 2452 | Time 57.1627(56.0692) | Bit/dim 3.6524(3.6536) | Xent 0.8252(0.8536) | Loss 4.0650(4.0804) | Error 0.3003(0.3061) Steps 568(573.22) | Grad Norm 0.4710(1.7327) | Total Time 14.00(14.00)\n",
      "Iter 2453 | Time 56.9609(56.0959) | Bit/dim 3.6596(3.6538) | Xent 0.8387(0.8532) | Loss 4.0790(4.0803) | Error 0.3087(0.3062) Steps 568(573.06) | Grad Norm 0.8595(1.7065) | Total Time 14.00(14.00)\n",
      "Iter 2454 | Time 55.6092(56.0813) | Bit/dim 3.6522(3.6537) | Xent 0.8346(0.8526) | Loss 4.0694(4.0800) | Error 0.2981(0.3059) Steps 568(572.91) | Grad Norm 0.5088(1.6705) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0409 | Time 21.9628, Epoch Time 367.7927(373.5398), Bit/dim 3.6526(best: 3.6521), Xent 0.8580, Loss 4.0816, Error 0.3020(best: 0.2998)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2455 | Time 57.6818(56.1294) | Bit/dim 3.6539(3.6537) | Xent 0.8535(0.8526) | Loss 4.0806(4.0800) | Error 0.3100(0.3061) Steps 580(573.12) | Grad Norm 0.7408(1.6426) | Total Time 14.00(14.00)\n",
      "Iter 2456 | Time 59.8163(56.2400) | Bit/dim 3.6606(3.6539) | Xent 0.8508(0.8526) | Loss 4.0860(4.0802) | Error 0.3017(0.3059) Steps 568(572.97) | Grad Norm 0.5451(1.6097) | Total Time 14.00(14.00)\n",
      "Iter 2457 | Time 52.8486(56.1382) | Bit/dim 3.6440(3.6536) | Xent 0.8192(0.8516) | Loss 4.0536(4.0794) | Error 0.2925(0.3055) Steps 568(572.82) | Grad Norm 0.7224(1.5831) | Total Time 14.00(14.00)\n",
      "Iter 2458 | Time 57.7549(56.1867) | Bit/dim 3.6543(3.6536) | Xent 0.8358(0.8511) | Loss 4.0722(4.0792) | Error 0.2971(0.3053) Steps 580(573.03) | Grad Norm 0.9607(1.5644) | Total Time 14.00(14.00)\n",
      "Iter 2459 | Time 54.4974(56.1360) | Bit/dim 3.6457(3.6534) | Xent 0.8397(0.8508) | Loss 4.0655(4.0788) | Error 0.2947(0.3050) Steps 568(572.88) | Grad Norm 0.5625(1.5344) | Total Time 14.00(14.00)\n",
      "Iter 2460 | Time 57.6469(56.1814) | Bit/dim 3.6500(3.6533) | Xent 0.8281(0.8501) | Loss 4.0641(4.0783) | Error 0.2954(0.3047) Steps 568(572.74) | Grad Norm 0.5232(1.5040) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0410 | Time 22.2257, Epoch Time 378.1879(373.6793), Bit/dim 3.6519(best: 3.6521), Xent 0.8570, Loss 4.0804, Error 0.3016(best: 0.2998)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2461 | Time 55.6265(56.1647) | Bit/dim 3.6385(3.6529) | Xent 0.8340(0.8496) | Loss 4.0556(4.0777) | Error 0.2993(0.3045) Steps 568(572.59) | Grad Norm 0.6137(1.4773) | Total Time 14.00(14.00)\n",
      "Iter 2462 | Time 55.3719(56.1409) | Bit/dim 3.6566(3.6530) | Xent 0.8413(0.8493) | Loss 4.0772(4.0776) | Error 0.2977(0.3043) Steps 568(572.46) | Grad Norm 0.7530(1.4556) | Total Time 14.00(14.00)\n",
      "Iter 2463 | Time 59.7821(56.2502) | Bit/dim 3.6649(3.6533) | Xent 0.8203(0.8485) | Loss 4.0751(4.0776) | Error 0.2936(0.3040) Steps 580(572.68) | Grad Norm 0.6011(1.4300) | Total Time 14.00(14.00)\n",
      "Iter 2464 | Time 53.4962(56.1676) | Bit/dim 3.6475(3.6532) | Xent 0.8408(0.8482) | Loss 4.0679(4.0773) | Error 0.3035(0.3040) Steps 568(572.54) | Grad Norm 0.7827(1.4105) | Total Time 14.00(14.00)\n",
      "Iter 2465 | Time 55.1797(56.1379) | Bit/dim 3.6551(3.6532) | Xent 0.8347(0.8478) | Loss 4.0725(4.0771) | Error 0.3000(0.3039) Steps 574(572.59) | Grad Norm 0.7071(1.3894) | Total Time 14.00(14.00)\n",
      "Iter 2466 | Time 56.4949(56.1486) | Bit/dim 3.6415(3.6529) | Xent 0.8256(0.8472) | Loss 4.0542(4.0764) | Error 0.2911(0.3035) Steps 568(572.45) | Grad Norm 0.4974(1.3627) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0411 | Time 22.3065, Epoch Time 374.0001(373.6889), Bit/dim 3.6525(best: 3.6519), Xent 0.8585, Loss 4.0818, Error 0.3038(best: 0.2998)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2467 | Time 56.1404(56.1484) | Bit/dim 3.6342(3.6523) | Xent 0.8365(0.8469) | Loss 4.0525(4.0757) | Error 0.3001(0.3034) Steps 568(572.31) | Grad Norm 0.6151(1.3403) | Total Time 14.00(14.00)\n",
      "Iter 2468 | Time 56.3588(56.1547) | Bit/dim 3.6516(3.6523) | Xent 0.8319(0.8464) | Loss 4.0676(4.0755) | Error 0.2974(0.3032) Steps 568(572.19) | Grad Norm 0.5988(1.3180) | Total Time 14.00(14.00)\n",
      "Iter 2469 | Time 53.0956(56.0629) | Bit/dim 3.6439(3.6520) | Xent 0.8365(0.8461) | Loss 4.0622(4.0751) | Error 0.2987(0.3031) Steps 568(572.06) | Grad Norm 0.5219(1.2941) | Total Time 14.00(14.00)\n",
      "Iter 2470 | Time 55.4511(56.0446) | Bit/dim 3.6555(3.6521) | Xent 0.8461(0.8461) | Loss 4.0785(4.0752) | Error 0.3003(0.3030) Steps 568(571.94) | Grad Norm 0.8833(1.2818) | Total Time 14.00(14.00)\n",
      "Iter 2471 | Time 54.3665(55.9942) | Bit/dim 3.6594(3.6523) | Xent 0.8290(0.8456) | Loss 4.0739(4.0751) | Error 0.2949(0.3027) Steps 568(571.82) | Grad Norm 0.6214(1.2620) | Total Time 14.00(14.00)\n",
      "Iter 2472 | Time 58.4153(56.0669) | Bit/dim 3.6606(3.6526) | Xent 0.8472(0.8456) | Loss 4.0842(4.0754) | Error 0.3061(0.3028) Steps 580(572.07) | Grad Norm 0.5999(1.2421) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0412 | Time 22.2394, Epoch Time 371.6797(373.6286), Bit/dim 3.6516(best: 3.6519), Xent 0.8568, Loss 4.0800, Error 0.3023(best: 0.2998)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2473 | Time 55.3533(56.0454) | Bit/dim 3.6529(3.6526) | Xent 0.8313(0.8452) | Loss 4.0685(4.0752) | Error 0.2975(0.3027) Steps 568(571.94) | Grad Norm 0.6674(1.2249) | Total Time 14.00(14.00)\n",
      "Iter 2474 | Time 54.2660(55.9921) | Bit/dim 3.6493(3.6525) | Xent 0.8145(0.8443) | Loss 4.0566(4.0746) | Error 0.2913(0.3023) Steps 568(571.83) | Grad Norm 0.5178(1.2037) | Total Time 14.00(14.00)\n",
      "Iter 2475 | Time 56.3993(56.0043) | Bit/dim 3.6555(3.6526) | Xent 0.8356(0.8440) | Loss 4.0733(4.0746) | Error 0.2981(0.3022) Steps 568(571.71) | Grad Norm 0.4230(1.1802) | Total Time 14.00(14.00)\n",
      "Iter 2476 | Time 56.7363(56.0262) | Bit/dim 3.6511(3.6526) | Xent 0.8446(0.8440) | Loss 4.0734(4.0746) | Error 0.2986(0.3021) Steps 568(571.60) | Grad Norm 0.4434(1.1581) | Total Time 14.00(14.00)\n",
      "Iter 2477 | Time 55.0612(55.9973) | Bit/dim 3.6450(3.6523) | Xent 0.8361(0.8438) | Loss 4.0630(4.0742) | Error 0.2990(0.3020) Steps 568(571.49) | Grad Norm 0.6771(1.1437) | Total Time 14.00(14.00)\n",
      "Iter 2478 | Time 56.4464(56.0108) | Bit/dim 3.6552(3.6524) | Xent 0.8547(0.8441) | Loss 4.0825(4.0745) | Error 0.3031(0.3020) Steps 568(571.39) | Grad Norm 0.5172(1.1249) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0413 | Time 22.0389, Epoch Time 371.7609(373.5726), Bit/dim 3.6530(best: 3.6516), Xent 0.8579, Loss 4.0819, Error 0.3030(best: 0.2998)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2479 | Time 56.7409(56.0327) | Bit/dim 3.6557(3.6525) | Xent 0.8372(0.8439) | Loss 4.0743(4.0745) | Error 0.3007(0.3020) Steps 568(571.28) | Grad Norm 0.5947(1.1090) | Total Time 14.00(14.00)\n",
      "Iter 2480 | Time 57.0857(56.0643) | Bit/dim 3.6415(3.6522) | Xent 0.8183(0.8432) | Loss 4.0507(4.0738) | Error 0.2953(0.3018) Steps 568(571.19) | Grad Norm 0.4340(1.0888) | Total Time 14.00(14.00)\n",
      "Iter 2481 | Time 56.8042(56.0865) | Bit/dim 3.6547(3.6523) | Xent 0.8589(0.8436) | Loss 4.0842(4.0741) | Error 0.3064(0.3019) Steps 568(571.09) | Grad Norm 0.4641(1.0700) | Total Time 14.00(14.00)\n",
      "Iter 2482 | Time 57.2848(56.1224) | Bit/dim 3.6402(3.6519) | Xent 0.8340(0.8433) | Loss 4.0572(4.0736) | Error 0.2996(0.3019) Steps 568(571.00) | Grad Norm 0.4766(1.0522) | Total Time 14.00(14.00)\n",
      "Iter 2483 | Time 56.6453(56.1381) | Bit/dim 3.6559(3.6520) | Xent 0.8218(0.8427) | Loss 4.0668(4.0734) | Error 0.2980(0.3017) Steps 586(571.45) | Grad Norm 0.7327(1.0426) | Total Time 14.00(14.00)\n",
      "Iter 2484 | Time 57.2776(56.1723) | Bit/dim 3.6508(3.6520) | Xent 0.8434(0.8427) | Loss 4.0725(4.0733) | Error 0.2961(0.3016) Steps 586(571.88) | Grad Norm 0.4042(1.0235) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0414 | Time 22.1400, Epoch Time 379.7058(373.7566), Bit/dim 3.6521(best: 3.6516), Xent 0.8583, Loss 4.0813, Error 0.3039(best: 0.2998)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2485 | Time 57.5345(56.2131) | Bit/dim 3.6495(3.6519) | Xent 0.8447(0.8428) | Loss 4.0719(4.0733) | Error 0.3063(0.3017) Steps 580(572.13) | Grad Norm 0.5390(1.0089) | Total Time 14.00(14.00)\n",
      "Iter 2486 | Time 58.5237(56.2825) | Bit/dim 3.6586(3.6521) | Xent 0.8355(0.8426) | Loss 4.0764(4.0734) | Error 0.3011(0.3017) Steps 568(572.00) | Grad Norm 0.4418(0.9919) | Total Time 14.00(14.00)\n",
      "Iter 2487 | Time 57.2832(56.3125) | Bit/dim 3.6516(3.6521) | Xent 0.8623(0.8432) | Loss 4.0827(4.0737) | Error 0.3066(0.3019) Steps 574(572.06) | Grad Norm 0.8071(0.9864) | Total Time 14.00(14.00)\n",
      "Iter 2488 | Time 54.4025(56.2552) | Bit/dim 3.6484(3.6520) | Xent 0.8433(0.8432) | Loss 4.0701(4.0736) | Error 0.3021(0.3019) Steps 568(571.94) | Grad Norm 0.4143(0.9692) | Total Time 14.00(14.00)\n",
      "Iter 2489 | Time 59.0470(56.3389) | Bit/dim 3.6554(3.6521) | Xent 0.8228(0.8425) | Loss 4.0668(4.0734) | Error 0.2993(0.3018) Steps 568(571.82) | Grad Norm 0.6359(0.9592) | Total Time 14.00(14.00)\n",
      "Iter 2490 | Time 52.6545(56.2284) | Bit/dim 3.6434(3.6518) | Xent 0.8422(0.8425) | Loss 4.0644(4.0731) | Error 0.2980(0.3017) Steps 568(571.71) | Grad Norm 0.7201(0.9521) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0415 | Time 22.3155, Epoch Time 377.2239(373.8606), Bit/dim 3.6517(best: 3.6516), Xent 0.8555, Loss 4.0795, Error 0.3023(best: 0.2998)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2491 | Time 56.0923(56.2243) | Bit/dim 3.6471(3.6517) | Xent 0.8292(0.8421) | Loss 4.0617(4.0727) | Error 0.2987(0.3016) Steps 568(571.60) | Grad Norm 0.5590(0.9403) | Total Time 14.00(14.00)\n",
      "Iter 2492 | Time 59.2917(56.3163) | Bit/dim 3.6463(3.6515) | Xent 0.8366(0.8420) | Loss 4.0646(4.0725) | Error 0.3067(0.3017) Steps 580(571.85) | Grad Norm 0.5174(0.9276) | Total Time 14.00(14.00)\n",
      "Iter 2493 | Time 58.5191(56.3824) | Bit/dim 3.6501(3.6515) | Xent 0.8302(0.8416) | Loss 4.0652(4.0723) | Error 0.2959(0.3016) Steps 568(571.73) | Grad Norm 0.5417(0.9160) | Total Time 14.00(14.00)\n",
      "Iter 2494 | Time 57.4059(56.4131) | Bit/dim 3.6675(3.6520) | Xent 0.8400(0.8416) | Loss 4.0875(4.0727) | Error 0.2961(0.3014) Steps 568(571.62) | Grad Norm 0.4779(0.9029) | Total Time 14.00(14.00)\n",
      "Iter 2495 | Time 54.3979(56.3527) | Bit/dim 3.6406(3.6516) | Xent 0.8395(0.8415) | Loss 4.0603(4.0724) | Error 0.3046(0.3015) Steps 568(571.51) | Grad Norm 0.7488(0.8982) | Total Time 14.00(14.00)\n",
      "Iter 2496 | Time 56.9261(56.3699) | Bit/dim 3.6441(3.6514) | Xent 0.8293(0.8411) | Loss 4.0588(4.0720) | Error 0.2944(0.3013) Steps 580(571.77) | Grad Norm 0.5025(0.8864) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0416 | Time 22.4302, Epoch Time 381.0302(374.0757), Bit/dim 3.6525(best: 3.6516), Xent 0.8569, Loss 4.0810, Error 0.2995(best: 0.2998)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2497 | Time 57.1596(56.3936) | Bit/dim 3.6510(3.6514) | Xent 0.8420(0.8412) | Loss 4.0720(4.0720) | Error 0.3067(0.3014) Steps 580(572.02) | Grad Norm 0.9249(0.8875) | Total Time 14.00(14.00)\n",
      "Iter 2498 | Time 58.9168(56.4693) | Bit/dim 3.6421(3.6511) | Xent 0.8412(0.8412) | Loss 4.0627(4.0717) | Error 0.3006(0.3014) Steps 580(572.25) | Grad Norm 0.6274(0.8797) | Total Time 14.00(14.00)\n",
      "Iter 2499 | Time 56.0835(56.4577) | Bit/dim 3.6551(3.6512) | Xent 0.8230(0.8406) | Loss 4.0666(4.0715) | Error 0.2920(0.3011) Steps 568(572.13) | Grad Norm 0.9948(0.8832) | Total Time 14.00(14.00)\n",
      "Iter 2500 | Time 57.9585(56.5027) | Bit/dim 3.6472(3.6511) | Xent 0.8043(0.8395) | Loss 4.0494(4.0709) | Error 0.2820(0.3006) Steps 568(572.00) | Grad Norm 0.6444(0.8760) | Total Time 14.00(14.00)\n",
      "Iter 2501 | Time 57.2004(56.5236) | Bit/dim 3.6604(3.6514) | Xent 0.8474(0.8398) | Loss 4.0841(4.0713) | Error 0.3027(0.3006) Steps 568(571.88) | Grad Norm 0.7471(0.8721) | Total Time 14.00(14.00)\n",
      "Iter 2502 | Time 55.1494(56.4824) | Bit/dim 3.6487(3.6513) | Xent 0.8170(0.8391) | Loss 4.0572(4.0708) | Error 0.2945(0.3004) Steps 568(571.77) | Grad Norm 0.6686(0.8660) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0417 | Time 22.0053, Epoch Time 379.9961(374.2533), Bit/dim 3.6512(best: 3.6516), Xent 0.8569, Loss 4.0797, Error 0.3022(best: 0.2995)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2503 | Time 57.6006(56.5160) | Bit/dim 3.6490(3.6512) | Xent 0.8436(0.8392) | Loss 4.0708(4.0708) | Error 0.3029(0.3005) Steps 568(571.65) | Grad Norm 0.3816(0.8515) | Total Time 14.00(14.00)\n",
      "Iter 2504 | Time 58.1071(56.5637) | Bit/dim 3.6438(3.6510) | Xent 0.8332(0.8390) | Loss 4.0605(4.0705) | Error 0.2973(0.3004) Steps 568(571.54) | Grad Norm 0.7738(0.8492) | Total Time 14.00(14.00)\n",
      "Iter 2505 | Time 56.6597(56.5666) | Bit/dim 3.6524(3.6511) | Xent 0.8281(0.8387) | Loss 4.0665(4.0704) | Error 0.2960(0.3003) Steps 580(571.80) | Grad Norm 0.6859(0.8443) | Total Time 14.00(14.00)\n",
      "Iter 2506 | Time 57.2378(56.5867) | Bit/dim 3.6458(3.6509) | Xent 0.8176(0.8381) | Loss 4.0546(4.0699) | Error 0.2977(0.3002) Steps 568(571.68) | Grad Norm 0.7439(0.8413) | Total Time 14.00(14.00)\n",
      "Iter 2507 | Time 59.0495(56.6606) | Bit/dim 3.6529(3.6510) | Xent 0.8347(0.8380) | Loss 4.0703(4.0699) | Error 0.2980(0.3001) Steps 580(571.93) | Grad Norm 0.6241(0.8347) | Total Time 14.00(14.00)\n",
      "Iter 2508 | Time 56.5111(56.6561) | Bit/dim 3.6597(3.6512) | Xent 0.8438(0.8382) | Loss 4.0816(4.0703) | Error 0.3031(0.3002) Steps 568(571.82) | Grad Norm 0.7033(0.8308) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0418 | Time 22.2716, Epoch Time 383.2737(374.5239), Bit/dim 3.6511(best: 3.6512), Xent 0.8566, Loss 4.0794, Error 0.3002(best: 0.2995)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2509 | Time 55.8799(56.6328) | Bit/dim 3.6623(3.6516) | Xent 0.8459(0.8384) | Loss 4.0852(4.0707) | Error 0.3069(0.3004) Steps 568(571.70) | Grad Norm 0.5987(0.8238) | Total Time 14.00(14.00)\n",
      "Iter 2510 | Time 56.2367(56.6209) | Bit/dim 3.6363(3.6511) | Xent 0.8283(0.8381) | Loss 4.0504(4.0701) | Error 0.2981(0.3004) Steps 574(571.77) | Grad Norm 0.9103(0.8264) | Total Time 14.00(14.00)\n",
      "Iter 2511 | Time 56.3498(56.6128) | Bit/dim 3.6402(3.6508) | Xent 0.8389(0.8381) | Loss 4.0596(4.0698) | Error 0.2991(0.3003) Steps 568(571.66) | Grad Norm 0.3900(0.8133) | Total Time 14.00(14.00)\n",
      "Iter 2512 | Time 57.4264(56.6372) | Bit/dim 3.6482(3.6507) | Xent 0.8197(0.8376) | Loss 4.0581(4.0695) | Error 0.2930(0.3001) Steps 568(571.55) | Grad Norm 0.6688(0.8090) | Total Time 14.00(14.00)\n",
      "Iter 2513 | Time 59.2942(56.7169) | Bit/dim 3.6659(3.6511) | Xent 0.8420(0.8377) | Loss 4.0869(4.0700) | Error 0.2986(0.3001) Steps 568(571.44) | Grad Norm 0.4776(0.7991) | Total Time 14.00(14.00)\n",
      "Iter 2514 | Time 56.0887(56.6981) | Bit/dim 3.6510(3.6511) | Xent 0.8291(0.8374) | Loss 4.0655(4.0699) | Error 0.3001(0.3001) Steps 568(571.34) | Grad Norm 0.8181(0.7996) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0419 | Time 22.0265, Epoch Time 378.8841(374.6547), Bit/dim 3.6523(best: 3.6511), Xent 0.8564, Loss 4.0805, Error 0.3010(best: 0.2995)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2515 | Time 57.0123(56.7075) | Bit/dim 3.6580(3.6513) | Xent 0.8444(0.8376) | Loss 4.0802(4.0702) | Error 0.3004(0.3001) Steps 580(571.60) | Grad Norm 0.5957(0.7935) | Total Time 14.00(14.00)\n",
      "Iter 2516 | Time 56.0634(56.6882) | Bit/dim 3.6499(3.6513) | Xent 0.8242(0.8372) | Loss 4.0620(4.0699) | Error 0.2947(0.2999) Steps 568(571.49) | Grad Norm 0.4022(0.7818) | Total Time 14.00(14.00)\n",
      "Iter 2517 | Time 55.7103(56.6588) | Bit/dim 3.6476(3.6512) | Xent 0.8154(0.8366) | Loss 4.0553(4.0695) | Error 0.2933(0.2997) Steps 580(571.74) | Grad Norm 0.5824(0.7758) | Total Time 14.00(14.00)\n",
      "Iter 2518 | Time 53.3055(56.5582) | Bit/dim 3.6444(3.6510) | Xent 0.8290(0.8364) | Loss 4.0589(4.0692) | Error 0.3006(0.2997) Steps 568(571.63) | Grad Norm 0.4863(0.7671) | Total Time 14.00(14.00)\n",
      "Iter 2519 | Time 56.7522(56.5641) | Bit/dim 3.6532(3.6511) | Xent 0.8327(0.8362) | Loss 4.0695(4.0692) | Error 0.2969(0.2997) Steps 568(571.52) | Grad Norm 0.6814(0.7645) | Total Time 14.00(14.00)\n",
      "Iter 2520 | Time 59.2962(56.6460) | Bit/dim 3.6503(3.6510) | Xent 0.8465(0.8366) | Loss 4.0735(4.0693) | Error 0.3064(0.2999) Steps 580(571.78) | Grad Norm 0.6796(0.7620) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0420 | Time 22.1913, Epoch Time 376.0746(374.6973), Bit/dim 3.6515(best: 3.6511), Xent 0.8584, Loss 4.0807, Error 0.3035(best: 0.2995)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2521 | Time 57.5546(56.6733) | Bit/dim 3.6593(3.6513) | Xent 0.8295(0.8363) | Loss 4.0741(4.0694) | Error 0.3016(0.2999) Steps 568(571.66) | Grad Norm 0.5291(0.7550) | Total Time 14.00(14.00)\n",
      "Iter 2522 | Time 54.6172(56.6116) | Bit/dim 3.6431(3.6510) | Xent 0.8401(0.8365) | Loss 4.0631(4.0693) | Error 0.3019(0.3000) Steps 568(571.55) | Grad Norm 0.4035(0.7445) | Total Time 14.00(14.00)\n",
      "Iter 2523 | Time 58.0223(56.6539) | Bit/dim 3.6498(3.6510) | Xent 0.8239(0.8361) | Loss 4.0618(4.0690) | Error 0.2897(0.2997) Steps 568(571.45) | Grad Norm 0.6876(0.7428) | Total Time 14.00(14.00)\n",
      "Iter 2524 | Time 57.8923(56.6911) | Bit/dim 3.6508(3.6510) | Xent 0.8220(0.8357) | Loss 4.0618(4.0688) | Error 0.2914(0.2994) Steps 568(571.34) | Grad Norm 0.9342(0.7485) | Total Time 14.00(14.00)\n",
      "Iter 2525 | Time 59.1299(56.7642) | Bit/dim 3.6512(3.6510) | Xent 0.8444(0.8359) | Loss 4.0734(4.0690) | Error 0.2996(0.2994) Steps 580(571.60) | Grad Norm 0.7133(0.7474) | Total Time 14.00(14.00)\n",
      "Iter 2526 | Time 57.2859(56.7799) | Bit/dim 3.6454(3.6508) | Xent 0.8237(0.8356) | Loss 4.0573(4.0686) | Error 0.2953(0.2993) Steps 568(571.50) | Grad Norm 0.3895(0.7367) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0421 | Time 22.2225, Epoch Time 382.5269(374.9322), Bit/dim 3.6518(best: 3.6511), Xent 0.8542, Loss 4.0789, Error 0.3039(best: 0.2995)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2527 | Time 55.5364(56.7426) | Bit/dim 3.6576(3.6510) | Xent 0.8270(0.8353) | Loss 4.0711(4.0687) | Error 0.2999(0.2993) Steps 568(571.39) | Grad Norm 0.7069(0.7358) | Total Time 14.00(14.00)\n",
      "Iter 2528 | Time 56.4540(56.7339) | Bit/dim 3.6533(3.6511) | Xent 0.8356(0.8353) | Loss 4.0711(4.0688) | Error 0.3023(0.2994) Steps 568(571.29) | Grad Norm 0.9751(0.7430) | Total Time 14.00(14.00)\n",
      "Iter 2529 | Time 57.6396(56.7611) | Bit/dim 3.6437(3.6509) | Xent 0.8314(0.8352) | Loss 4.0594(4.0685) | Error 0.2965(0.2993) Steps 586(571.73) | Grad Norm 0.6282(0.7395) | Total Time 14.00(14.00)\n",
      "Iter 2530 | Time 60.1540(56.8629) | Bit/dim 3.6570(3.6511) | Xent 0.8463(0.8355) | Loss 4.0802(4.0688) | Error 0.3020(0.2994) Steps 580(571.98) | Grad Norm 0.5924(0.7351) | Total Time 14.00(14.00)\n",
      "Iter 2531 | Time 56.9191(56.8646) | Bit/dim 3.6486(3.6510) | Xent 0.8255(0.8352) | Loss 4.0613(4.0686) | Error 0.2939(0.2992) Steps 568(571.86) | Grad Norm 0.3552(0.7237) | Total Time 14.00(14.00)\n",
      "Iter 2532 | Time 57.4173(56.8811) | Bit/dim 3.6480(3.6509) | Xent 0.8371(0.8353) | Loss 4.0665(4.0685) | Error 0.2996(0.2992) Steps 574(571.92) | Grad Norm 0.5262(0.7178) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0422 | Time 22.5246, Epoch Time 382.1517(375.1488), Bit/dim 3.6515(best: 3.6511), Xent 0.8565, Loss 4.0798, Error 0.3046(best: 0.2995)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2533 | Time 57.4193(56.8973) | Bit/dim 3.6550(3.6510) | Xent 0.8348(0.8353) | Loss 4.0724(4.0687) | Error 0.3034(0.2994) Steps 568(571.81) | Grad Norm 0.6312(0.7152) | Total Time 14.00(14.00)\n",
      "Iter 2534 | Time 55.9450(56.8687) | Bit/dim 3.6526(3.6511) | Xent 0.8350(0.8353) | Loss 4.0701(4.0687) | Error 0.3000(0.2994) Steps 568(571.69) | Grad Norm 0.6414(0.7130) | Total Time 14.00(14.00)\n",
      "Iter 2535 | Time 54.9576(56.8114) | Bit/dim 3.6547(3.6512) | Xent 0.8435(0.8355) | Loss 4.0765(4.0689) | Error 0.2949(0.2992) Steps 568(571.58) | Grad Norm 0.7377(0.7137) | Total Time 14.00(14.00)\n",
      "Iter 2536 | Time 55.6252(56.7758) | Bit/dim 3.6530(3.6512) | Xent 0.8253(0.8352) | Loss 4.0656(4.0688) | Error 0.2976(0.2992) Steps 580(571.83) | Grad Norm 0.5142(0.7077) | Total Time 14.00(14.00)\n",
      "Iter 2537 | Time 58.1393(56.8167) | Bit/dim 3.6451(3.6510) | Xent 0.8015(0.8342) | Loss 4.0458(4.0681) | Error 0.2860(0.2988) Steps 568(571.72) | Grad Norm 0.5380(0.7027) | Total Time 14.00(14.00)\n",
      "Iter 2538 | Time 59.7888(56.9059) | Bit/dim 3.6426(3.6508) | Xent 0.8348(0.8342) | Loss 4.0600(4.0679) | Error 0.2996(0.2988) Steps 580(571.97) | Grad Norm 0.4481(0.6950) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0423 | Time 22.2100, Epoch Time 379.9341(375.2923), Bit/dim 3.6526(best: 3.6511), Xent 0.8556, Loss 4.0804, Error 0.3016(best: 0.2995)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2539 | Time 55.5163(56.8642) | Bit/dim 3.6390(3.6504) | Xent 0.8238(0.8339) | Loss 4.0509(4.0674) | Error 0.2957(0.2987) Steps 568(571.85) | Grad Norm 0.4891(0.6888) | Total Time 14.00(14.00)\n",
      "Iter 2540 | Time 57.6679(56.8883) | Bit/dim 3.6434(3.6502) | Xent 0.8390(0.8340) | Loss 4.0629(4.0673) | Error 0.2954(0.2986) Steps 580(572.09) | Grad Norm 0.4785(0.6825) | Total Time 14.00(14.00)\n",
      "Iter 2541 | Time 57.4005(56.9037) | Bit/dim 3.6450(3.6501) | Xent 0.8237(0.8337) | Loss 4.0569(4.0669) | Error 0.2914(0.2984) Steps 580(572.33) | Grad Norm 0.4900(0.6768) | Total Time 14.00(14.00)\n",
      "Iter 2542 | Time 57.2762(56.9148) | Bit/dim 3.6605(3.6504) | Xent 0.8424(0.8340) | Loss 4.0817(4.0674) | Error 0.2986(0.2984) Steps 568(572.20) | Grad Norm 0.5606(0.6733) | Total Time 14.00(14.00)\n",
      "Iter 2543 | Time 55.2200(56.8640) | Bit/dim 3.6504(3.6504) | Xent 0.8138(0.8334) | Loss 4.0573(4.0671) | Error 0.2939(0.2983) Steps 568(572.07) | Grad Norm 0.5817(0.6705) | Total Time 14.00(14.00)\n",
      "Iter 2544 | Time 59.1347(56.9321) | Bit/dim 3.6564(3.6506) | Xent 0.8327(0.8334) | Loss 4.0727(4.0672) | Error 0.3031(0.2984) Steps 574(572.13) | Grad Norm 1.0117(0.6808) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0424 | Time 22.2314, Epoch Time 380.2318(375.4405), Bit/dim 3.6520(best: 3.6511), Xent 0.8561, Loss 4.0801, Error 0.3010(best: 0.2995)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2545 | Time 56.0556(56.9058) | Bit/dim 3.6526(3.6506) | Xent 0.8183(0.8329) | Loss 4.0618(4.0671) | Error 0.2854(0.2980) Steps 568(572.01) | Grad Norm 0.6542(0.6800) | Total Time 14.00(14.00)\n",
      "Iter 2546 | Time 58.9263(56.9664) | Bit/dim 3.6463(3.6505) | Xent 0.8402(0.8331) | Loss 4.0664(4.0671) | Error 0.3029(0.2982) Steps 580(572.25) | Grad Norm 0.3714(0.6707) | Total Time 14.00(14.00)\n",
      "Iter 2547 | Time 56.5593(56.9542) | Bit/dim 3.6497(3.6505) | Xent 0.8367(0.8332) | Loss 4.0681(4.0671) | Error 0.3005(0.2983) Steps 586(572.66) | Grad Norm 0.5195(0.6662) | Total Time 14.00(14.00)\n",
      "Iter 2548 | Time 57.2897(56.9643) | Bit/dim 3.6507(3.6505) | Xent 0.8227(0.8329) | Loss 4.0620(4.0669) | Error 0.2936(0.2981) Steps 580(572.88) | Grad Norm 0.6326(0.6652) | Total Time 14.00(14.00)\n",
      "Iter 2549 | Time 57.5288(56.9812) | Bit/dim 3.6515(3.6505) | Xent 0.8300(0.8328) | Loss 4.0665(4.0669) | Error 0.2943(0.2980) Steps 568(572.73) | Grad Norm 0.6258(0.6640) | Total Time 14.00(14.00)\n",
      "Iter 2550 | Time 52.2015(56.8378) | Bit/dim 3.6548(3.6506) | Xent 0.8164(0.8323) | Loss 4.0630(4.0668) | Error 0.2986(0.2980) Steps 568(572.59) | Grad Norm 0.4218(0.6567) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0425 | Time 22.3047, Epoch Time 376.5266(375.4731), Bit/dim 3.6526(best: 3.6511), Xent 0.8547, Loss 4.0800, Error 0.3017(best: 0.2995)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2551 | Time 57.8139(56.8671) | Bit/dim 3.6441(3.6504) | Xent 0.8206(0.8320) | Loss 4.0544(4.0664) | Error 0.2961(0.2980) Steps 580(572.81) | Grad Norm 0.5476(0.6534) | Total Time 14.00(14.00)\n",
      "Iter 2552 | Time 58.3565(56.9118) | Bit/dim 3.6414(3.6502) | Xent 0.8283(0.8319) | Loss 4.0555(4.0661) | Error 0.2957(0.2979) Steps 580(573.03) | Grad Norm 0.4676(0.6479) | Total Time 14.00(14.00)\n",
      "Iter 2553 | Time 52.0945(56.7673) | Bit/dim 3.6615(3.6505) | Xent 0.8305(0.8318) | Loss 4.0767(4.0664) | Error 0.2974(0.2979) Steps 568(572.88) | Grad Norm 0.8310(0.6534) | Total Time 14.00(14.00)\n",
      "Iter 2554 | Time 58.1439(56.8086) | Bit/dim 3.6504(3.6505) | Xent 0.8227(0.8316) | Loss 4.0617(4.0663) | Error 0.2989(0.2979) Steps 580(573.09) | Grad Norm 0.5466(0.6502) | Total Time 14.00(14.00)\n",
      "Iter 2555 | Time 58.2728(56.8525) | Bit/dim 3.6406(3.6502) | Xent 0.8188(0.8312) | Loss 4.0500(4.0658) | Error 0.2945(0.2978) Steps 580(573.30) | Grad Norm 0.5527(0.6472) | Total Time 14.00(14.00)\n",
      "Iter 2556 | Time 57.6311(56.8759) | Bit/dim 3.6656(3.6507) | Xent 0.8550(0.8319) | Loss 4.0931(4.0666) | Error 0.3040(0.2980) Steps 574(573.32) | Grad Norm 0.5311(0.6438) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0426 | Time 21.8725, Epoch Time 379.6373(375.5980), Bit/dim 3.6521(best: 3.6511), Xent 0.8560, Loss 4.0801, Error 0.3036(best: 0.2995)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2557 | Time 55.6144(56.8380) | Bit/dim 3.6534(3.6508) | Xent 0.8284(0.8318) | Loss 4.0676(4.0666) | Error 0.2975(0.2980) Steps 568(573.16) | Grad Norm 0.5866(0.6420) | Total Time 14.00(14.00)\n",
      "Iter 2558 | Time 56.7373(56.8350) | Bit/dim 3.6598(3.6510) | Xent 0.8417(0.8321) | Loss 4.0806(4.0671) | Error 0.2991(0.2980) Steps 580(573.37) | Grad Norm 0.5286(0.6386) | Total Time 14.00(14.00)\n",
      "Iter 2559 | Time 56.8004(56.8340) | Bit/dim 3.6381(3.6506) | Xent 0.8484(0.8326) | Loss 4.0623(4.0669) | Error 0.3030(0.2982) Steps 568(573.21) | Grad Norm 0.4979(0.6344) | Total Time 14.00(14.00)\n",
      "Iter 2560 | Time 58.2900(56.8776) | Bit/dim 3.6512(3.6507) | Xent 0.8203(0.8322) | Loss 4.0613(4.0668) | Error 0.2913(0.2980) Steps 580(573.41) | Grad Norm 0.4483(0.6288) | Total Time 14.00(14.00)\n",
      "Iter 2561 | Time 57.9823(56.9108) | Bit/dim 3.6548(3.6508) | Xent 0.8260(0.8320) | Loss 4.0678(4.0668) | Error 0.2994(0.2980) Steps 580(573.61) | Grad Norm 0.9516(0.6385) | Total Time 14.00(14.00)\n",
      "Iter 2562 | Time 57.0588(56.9152) | Bit/dim 3.6410(3.6505) | Xent 0.8233(0.8318) | Loss 4.0527(4.0664) | Error 0.2891(0.2977) Steps 580(573.80) | Grad Norm 0.5962(0.6372) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0427 | Time 22.1605, Epoch Time 380.1664(375.7351), Bit/dim 3.6523(best: 3.6511), Xent 0.8550, Loss 4.0798, Error 0.3029(best: 0.2995)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2563 | Time 58.3771(56.9591) | Bit/dim 3.6543(3.6506) | Xent 0.8172(0.8313) | Loss 4.0629(4.0663) | Error 0.2940(0.2976) Steps 568(573.62) | Grad Norm 0.6694(0.6382) | Total Time 14.00(14.00)\n",
      "Iter 2564 | Time 56.0322(56.9313) | Bit/dim 3.6568(3.6508) | Xent 0.8481(0.8318) | Loss 4.0809(4.0667) | Error 0.3017(0.2977) Steps 568(573.46) | Grad Norm 0.4620(0.6329) | Total Time 14.00(14.00)\n",
      "Iter 2565 | Time 55.5542(56.8900) | Bit/dim 3.6362(3.6503) | Xent 0.8330(0.8319) | Loss 4.0527(4.0663) | Error 0.3010(0.2978) Steps 574(573.47) | Grad Norm 0.5223(0.6296) | Total Time 14.00(14.00)\n",
      "Iter 2566 | Time 57.6241(56.9120) | Bit/dim 3.6451(3.6502) | Xent 0.8389(0.8321) | Loss 4.0645(4.0662) | Error 0.3014(0.2979) Steps 568(573.31) | Grad Norm 0.9439(0.6390) | Total Time 14.00(14.00)\n",
      "Iter 2567 | Time 55.2734(56.8628) | Bit/dim 3.6554(3.6503) | Xent 0.8249(0.8319) | Loss 4.0679(4.0663) | Error 0.2927(0.2978) Steps 568(573.15) | Grad Norm 1.2356(0.6569) | Total Time 14.00(14.00)\n",
      "Iter 2568 | Time 58.3807(56.9084) | Bit/dim 3.6477(3.6503) | Xent 0.8240(0.8316) | Loss 4.0597(4.0661) | Error 0.3004(0.2979) Steps 580(573.35) | Grad Norm 0.8267(0.6620) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0428 | Time 22.2139, Epoch Time 379.2085(375.8393), Bit/dim 3.6514(best: 3.6511), Xent 0.8542, Loss 4.0785, Error 0.3020(best: 0.2995)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2569 | Time 55.8672(56.8771) | Bit/dim 3.6555(3.6504) | Xent 0.8609(0.8325) | Loss 4.0860(4.0667) | Error 0.3093(0.2982) Steps 568(573.19) | Grad Norm 0.5860(0.6597) | Total Time 14.00(14.00)\n",
      "Iter 2570 | Time 57.7177(56.9023) | Bit/dim 3.6481(3.6504) | Xent 0.8192(0.8321) | Loss 4.0577(4.0664) | Error 0.2935(0.2981) Steps 568(573.04) | Grad Norm 0.6793(0.6603) | Total Time 14.00(14.00)\n",
      "Iter 2571 | Time 55.9428(56.8735) | Bit/dim 3.6523(3.6504) | Xent 0.8202(0.8317) | Loss 4.0624(4.0663) | Error 0.2921(0.2979) Steps 568(572.89) | Grad Norm 0.7312(0.6625) | Total Time 14.00(14.00)\n",
      "Iter 2572 | Time 56.9813(56.8768) | Bit/dim 3.6410(3.6501) | Xent 0.8308(0.8317) | Loss 4.0564(4.0660) | Error 0.2973(0.2979) Steps 580(573.10) | Grad Norm 0.6308(0.6615) | Total Time 14.00(14.00)\n",
      "Iter 2573 | Time 59.2692(56.9486) | Bit/dim 3.6473(3.6500) | Xent 0.8281(0.8316) | Loss 4.0614(4.0659) | Error 0.2963(0.2978) Steps 568(572.95) | Grad Norm 0.8660(0.6676) | Total Time 14.00(14.00)\n",
      "Iter 2574 | Time 58.3151(56.9896) | Bit/dim 3.6553(3.6502) | Xent 0.8403(0.8319) | Loss 4.0755(4.0661) | Error 0.2997(0.2979) Steps 568(572.80) | Grad Norm 0.4851(0.6622) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0429 | Time 22.4592, Epoch Time 382.6143(376.0425), Bit/dim 3.6522(best: 3.6511), Xent 0.8565, Loss 4.0804, Error 0.3037(best: 0.2995)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2575 | Time 54.7910(56.9236) | Bit/dim 3.6550(3.6504) | Xent 0.8466(0.8323) | Loss 4.0784(4.0665) | Error 0.3053(0.2981) Steps 568(572.65) | Grad Norm 0.8357(0.6674) | Total Time 14.00(14.00)\n",
      "Iter 2576 | Time 59.4160(56.9984) | Bit/dim 3.6513(3.6504) | Xent 0.8180(0.8319) | Loss 4.0603(4.0663) | Error 0.2976(0.2981) Steps 580(572.88) | Grad Norm 0.4573(0.6611) | Total Time 14.00(14.00)\n",
      "Iter 2577 | Time 57.0198(56.9990) | Bit/dim 3.6380(3.6500) | Xent 0.8098(0.8312) | Loss 4.0429(4.0656) | Error 0.2905(0.2979) Steps 568(572.73) | Grad Norm 0.7255(0.6630) | Total Time 14.00(14.00)\n",
      "Iter 2578 | Time 56.6662(56.9890) | Bit/dim 3.6580(3.6502) | Xent 0.8358(0.8314) | Loss 4.0759(4.0659) | Error 0.2940(0.2977) Steps 580(572.95) | Grad Norm 0.4885(0.6578) | Total Time 14.00(14.00)\n",
      "Iter 2579 | Time 58.8555(57.0450) | Bit/dim 3.6539(3.6504) | Xent 0.8321(0.8314) | Loss 4.0700(4.0661) | Error 0.3016(0.2979) Steps 580(573.16) | Grad Norm 0.4373(0.6512) | Total Time 14.00(14.00)\n",
      "Iter 2580 | Time 57.8862(57.0703) | Bit/dim 3.6449(3.6502) | Xent 0.8195(0.8310) | Loss 4.0547(4.0657) | Error 0.2977(0.2979) Steps 568(573.00) | Grad Norm 0.5170(0.6471) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0430 | Time 22.3204, Epoch Time 382.9222(376.2489), Bit/dim 3.6512(best: 3.6511), Xent 0.8544, Loss 4.0784, Error 0.3020(best: 0.2995)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2581 | Time 57.5765(57.0854) | Bit/dim 3.6573(3.6504) | Xent 0.8216(0.8307) | Loss 4.0681(4.0658) | Error 0.2977(0.2979) Steps 580(573.21) | Grad Norm 0.5290(0.6436) | Total Time 14.00(14.00)\n",
      "Iter 2582 | Time 57.3908(57.0946) | Bit/dim 3.6477(3.6503) | Xent 0.8293(0.8307) | Loss 4.0623(4.0657) | Error 0.2993(0.2979) Steps 568(573.06) | Grad Norm 0.6100(0.6426) | Total Time 14.00(14.00)\n",
      "Iter 2583 | Time 57.9981(57.1217) | Bit/dim 3.6373(3.6499) | Xent 0.8280(0.8306) | Loss 4.0513(4.0652) | Error 0.2959(0.2978) Steps 580(573.27) | Grad Norm 0.5179(0.6388) | Total Time 14.00(14.00)\n",
      "Iter 2584 | Time 56.7834(57.1116) | Bit/dim 3.6547(3.6501) | Xent 0.8233(0.8304) | Loss 4.0664(4.0653) | Error 0.3006(0.2979) Steps 568(573.11) | Grad Norm 0.5734(0.6369) | Total Time 14.00(14.00)\n",
      "Iter 2585 | Time 55.7174(57.0697) | Bit/dim 3.6523(3.6501) | Xent 0.8183(0.8300) | Loss 4.0615(4.0652) | Error 0.2950(0.2978) Steps 568(572.95) | Grad Norm 0.5100(0.6331) | Total Time 14.00(14.00)\n",
      "Iter 2586 | Time 58.5191(57.1132) | Bit/dim 3.6520(3.6502) | Xent 0.8287(0.8300) | Loss 4.0663(4.0652) | Error 0.2993(0.2979) Steps 568(572.81) | Grad Norm 0.3759(0.6254) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0431 | Time 22.0486, Epoch Time 381.6588(376.4112), Bit/dim 3.6506(best: 3.6511), Xent 0.8542, Loss 4.0777, Error 0.3020(best: 0.2995)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2587 | Time 57.5787(57.1272) | Bit/dim 3.6487(3.6502) | Xent 0.8287(0.8300) | Loss 4.0631(4.0651) | Error 0.3045(0.2981) Steps 580(573.02) | Grad Norm 0.7322(0.6286) | Total Time 14.00(14.00)\n",
      "Iter 2588 | Time 57.0908(57.1261) | Bit/dim 3.6605(3.6505) | Xent 0.8348(0.8301) | Loss 4.0779(4.0655) | Error 0.2989(0.2981) Steps 568(572.87) | Grad Norm 0.6645(0.6296) | Total Time 14.00(14.00)\n",
      "Iter 2589 | Time 57.4695(57.1364) | Bit/dim 3.6517(3.6505) | Xent 0.8109(0.8295) | Loss 4.0571(4.0653) | Error 0.2911(0.2979) Steps 580(573.08) | Grad Norm 0.5900(0.6284) | Total Time 14.00(14.00)\n",
      "Iter 2590 | Time 57.6561(57.1520) | Bit/dim 3.6500(3.6505) | Xent 0.8372(0.8298) | Loss 4.0686(4.0654) | Error 0.2990(0.2979) Steps 568(572.93) | Grad Norm 0.4893(0.6243) | Total Time 14.00(14.00)\n",
      "Iter 2591 | Time 55.5646(57.1044) | Bit/dim 3.6400(3.6502) | Xent 0.8188(0.8294) | Loss 4.0494(4.0649) | Error 0.3019(0.2980) Steps 580(573.14) | Grad Norm 0.6658(0.6255) | Total Time 14.00(14.00)\n",
      "Iter 2592 | Time 56.6427(57.0905) | Bit/dim 3.6497(3.6502) | Xent 0.8602(0.8304) | Loss 4.0798(4.0653) | Error 0.3055(0.2983) Steps 580(573.35) | Grad Norm 0.6719(0.6269) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0432 | Time 22.4589, Epoch Time 380.5356(376.5350), Bit/dim 3.6518(best: 3.6506), Xent 0.8555, Loss 4.0795, Error 0.3031(best: 0.2995)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2593 | Time 58.1578(57.1225) | Bit/dim 3.6519(3.6502) | Xent 0.8287(0.8303) | Loss 4.0662(4.0654) | Error 0.2945(0.2982) Steps 568(573.19) | Grad Norm 0.5480(0.6245) | Total Time 14.00(14.00)\n",
      "Iter 2594 | Time 56.0229(57.0895) | Bit/dim 3.6503(3.6502) | Xent 0.8174(0.8299) | Loss 4.0590(4.0652) | Error 0.2886(0.2979) Steps 568(573.03) | Grad Norm 0.5120(0.6212) | Total Time 14.00(14.00)\n",
      "Iter 2595 | Time 56.5013(57.0719) | Bit/dim 3.6516(3.6503) | Xent 0.8366(0.8301) | Loss 4.0699(4.0653) | Error 0.3003(0.2979) Steps 568(572.88) | Grad Norm 0.5862(0.6201) | Total Time 14.00(14.00)\n",
      "Iter 2596 | Time 56.4560(57.0534) | Bit/dim 3.6520(3.6503) | Xent 0.8263(0.8300) | Loss 4.0652(4.0653) | Error 0.3013(0.2980) Steps 568(572.74) | Grad Norm 0.5440(0.6178) | Total Time 14.00(14.00)\n",
      "Iter 2597 | Time 57.8793(57.0782) | Bit/dim 3.6528(3.6504) | Xent 0.8243(0.8298) | Loss 4.0650(4.0653) | Error 0.2891(0.2978) Steps 568(572.59) | Grad Norm 0.9169(0.6268) | Total Time 14.00(14.00)\n",
      "Iter 2598 | Time 55.9354(57.0439) | Bit/dim 3.6409(3.6501) | Xent 0.8347(0.8300) | Loss 4.0582(4.0651) | Error 0.3006(0.2979) Steps 574(572.64) | Grad Norm 0.4950(0.6228) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0433 | Time 22.1989, Epoch Time 378.8964(376.6058), Bit/dim 3.6515(best: 3.6506), Xent 0.8552, Loss 4.0791, Error 0.2988(best: 0.2995)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2599 | Time 59.8353(57.1277) | Bit/dim 3.6577(3.6503) | Xent 0.8333(0.8301) | Loss 4.0744(4.0654) | Error 0.2960(0.2978) Steps 574(572.68) | Grad Norm 0.7677(0.6272) | Total Time 14.00(14.00)\n",
      "Iter 2600 | Time 58.8629(57.1797) | Bit/dim 3.6533(3.6504) | Xent 0.8197(0.8298) | Loss 4.0631(4.0653) | Error 0.2991(0.2978) Steps 574(572.72) | Grad Norm 0.4925(0.6232) | Total Time 14.00(14.00)\n",
      "Iter 2601 | Time 58.1026(57.2074) | Bit/dim 3.6500(3.6504) | Xent 0.8189(0.8294) | Loss 4.0594(4.0651) | Error 0.2915(0.2976) Steps 580(572.94) | Grad Norm 0.9457(0.6328) | Total Time 14.00(14.00)\n",
      "Iter 2602 | Time 59.1025(57.2643) | Bit/dim 3.6475(3.6503) | Xent 0.8392(0.8297) | Loss 4.0672(4.0652) | Error 0.2947(0.2976) Steps 574(572.97) | Grad Norm 0.6404(0.6331) | Total Time 14.00(14.00)\n",
      "Iter 2603 | Time 56.6782(57.2467) | Bit/dim 3.6478(3.6502) | Xent 0.8230(0.8295) | Loss 4.0593(4.0650) | Error 0.2959(0.2975) Steps 568(572.82) | Grad Norm 1.0508(0.6456) | Total Time 14.00(14.00)\n",
      "Iter 2604 | Time 59.5822(57.3167) | Bit/dim 3.6458(3.6501) | Xent 0.8212(0.8293) | Loss 4.0564(4.0648) | Error 0.2933(0.2974) Steps 568(572.67) | Grad Norm 0.5892(0.6439) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0434 | Time 22.3786, Epoch Time 390.3697(377.0187), Bit/dim 3.6513(best: 3.6506), Xent 0.8570, Loss 4.0798, Error 0.3009(best: 0.2988)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2605 | Time 56.1204(57.2808) | Bit/dim 3.6488(3.6501) | Xent 0.8203(0.8290) | Loss 4.0589(4.0646) | Error 0.2953(0.2973) Steps 568(572.53) | Grad Norm 0.7434(0.6469) | Total Time 14.00(14.00)\n",
      "Iter 2606 | Time 56.7954(57.2663) | Bit/dim 3.6524(3.6501) | Xent 0.8236(0.8288) | Loss 4.0641(4.0646) | Error 0.2974(0.2973) Steps 568(572.40) | Grad Norm 1.2229(0.6642) | Total Time 14.00(14.00)\n",
      "Iter 2607 | Time 56.2562(57.2360) | Bit/dim 3.6433(3.6499) | Xent 0.8230(0.8287) | Loss 4.0548(4.0643) | Error 0.2917(0.2972) Steps 568(572.27) | Grad Norm 0.5574(0.6610) | Total Time 14.00(14.00)\n",
      "Iter 2608 | Time 55.3451(57.1793) | Bit/dim 3.6533(3.6500) | Xent 0.8214(0.8285) | Loss 4.0640(4.0643) | Error 0.2951(0.2971) Steps 568(572.14) | Grad Norm 0.9878(0.6708) | Total Time 14.00(14.00)\n",
      "Iter 2609 | Time 57.7707(57.1970) | Bit/dim 3.6557(3.6502) | Xent 0.8358(0.8287) | Loss 4.0736(4.0645) | Error 0.2945(0.2970) Steps 568(572.01) | Grad Norm 0.7351(0.6727) | Total Time 14.00(14.00)\n",
      "Iter 2610 | Time 58.8600(57.2469) | Bit/dim 3.6398(3.6499) | Xent 0.8378(0.8289) | Loss 4.0587(4.0644) | Error 0.2953(0.2970) Steps 568(571.89) | Grad Norm 0.3345(0.6626) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0435 | Time 22.1181, Epoch Time 378.8402(377.0734), Bit/dim 3.6516(best: 3.6506), Xent 0.8540, Loss 4.0786, Error 0.3027(best: 0.2988)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2611 | Time 57.3012(57.2485) | Bit/dim 3.6404(3.6496) | Xent 0.8304(0.8290) | Loss 4.0556(4.0641) | Error 0.2965(0.2969) Steps 568(571.78) | Grad Norm 0.9941(0.6725) | Total Time 14.00(14.00)\n",
      "Iter 2612 | Time 56.3061(57.2202) | Bit/dim 3.6455(3.6495) | Xent 0.8205(0.8287) | Loss 4.0558(4.0639) | Error 0.2925(0.2968) Steps 568(571.66) | Grad Norm 1.1155(0.6858) | Total Time 14.00(14.00)\n",
      "Iter 2613 | Time 56.0127(57.1840) | Bit/dim 3.6557(3.6497) | Xent 0.8284(0.8287) | Loss 4.0699(4.0640) | Error 0.3015(0.2970) Steps 568(571.55) | Grad Norm 0.5963(0.6831) | Total Time 14.00(14.00)\n",
      "Iter 2614 | Time 60.5194(57.2841) | Bit/dim 3.6573(3.6499) | Xent 0.8156(0.8283) | Loss 4.0651(4.0641) | Error 0.2907(0.2968) Steps 568(571.45) | Grad Norm 0.4887(0.6773) | Total Time 14.00(14.00)\n",
      "Iter 2615 | Time 59.8898(57.3623) | Bit/dim 3.6495(3.6499) | Xent 0.8324(0.8285) | Loss 4.0657(4.0641) | Error 0.2954(0.2967) Steps 574(571.52) | Grad Norm 0.8103(0.6813) | Total Time 14.00(14.00)\n",
      "Iter 2616 | Time 57.1840(57.3569) | Bit/dim 3.6505(3.6499) | Xent 0.8280(0.8284) | Loss 4.0645(4.0641) | Error 0.2960(0.2967) Steps 580(571.78) | Grad Norm 0.7348(0.6829) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0436 | Time 22.2883, Epoch Time 384.9699(377.3103), Bit/dim 3.6511(best: 3.6506), Xent 0.8543, Loss 4.0783, Error 0.3049(best: 0.2988)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2617 | Time 56.1042(57.3193) | Bit/dim 3.6605(3.6502) | Xent 0.8152(0.8280) | Loss 4.0681(4.0643) | Error 0.2944(0.2966) Steps 574(571.84) | Grad Norm 0.7225(0.6841) | Total Time 14.00(14.00)\n",
      "Iter 2618 | Time 55.9403(57.2780) | Bit/dim 3.6529(3.6503) | Xent 0.8294(0.8281) | Loss 4.0676(4.0644) | Error 0.2963(0.2966) Steps 568(571.73) | Grad Norm 0.5282(0.6794) | Total Time 14.00(14.00)\n",
      "Iter 2619 | Time 56.9793(57.2690) | Bit/dim 3.6423(3.6501) | Xent 0.8388(0.8284) | Loss 4.0617(4.0643) | Error 0.3015(0.2968) Steps 580(571.98) | Grad Norm 0.5892(0.6767) | Total Time 14.00(14.00)\n",
      "Iter 2620 | Time 57.6671(57.2809) | Bit/dim 3.6452(3.6499) | Xent 0.8362(0.8286) | Loss 4.0633(4.0642) | Error 0.2991(0.2968) Steps 586(572.40) | Grad Norm 0.4377(0.6695) | Total Time 14.00(14.00)\n",
      "Iter 2621 | Time 58.9585(57.3313) | Bit/dim 3.6517(3.6500) | Xent 0.8294(0.8287) | Loss 4.0664(4.0643) | Error 0.3003(0.2969) Steps 568(572.27) | Grad Norm 0.5226(0.6651) | Total Time 14.00(14.00)\n",
      "Iter 2622 | Time 56.1301(57.2952) | Bit/dim 3.6453(3.6498) | Xent 0.8254(0.8286) | Loss 4.0580(4.0641) | Error 0.2946(0.2969) Steps 568(572.14) | Grad Norm 0.5682(0.6622) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0437 | Time 22.2803, Epoch Time 379.5130(377.3763), Bit/dim 3.6507(best: 3.6506), Xent 0.8544, Loss 4.0779, Error 0.3001(best: 0.2988)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2623 | Time 57.3986(57.2983) | Bit/dim 3.6485(3.6498) | Xent 0.8410(0.8289) | Loss 4.0690(4.0643) | Error 0.3009(0.2970) Steps 568(572.01) | Grad Norm 0.5131(0.6577) | Total Time 14.00(14.00)\n",
      "Iter 2624 | Time 56.3442(57.2697) | Bit/dim 3.6522(3.6499) | Xent 0.8264(0.8289) | Loss 4.0654(4.0643) | Error 0.2931(0.2969) Steps 568(571.89) | Grad Norm 0.4536(0.6516) | Total Time 14.00(14.00)\n",
      "Iter 2625 | Time 54.1606(57.1764) | Bit/dim 3.6404(3.6496) | Xent 0.8409(0.8292) | Loss 4.0609(4.0642) | Error 0.3055(0.2971) Steps 568(571.78) | Grad Norm 0.5813(0.6495) | Total Time 14.00(14.00)\n",
      "Iter 2626 | Time 57.3844(57.1827) | Bit/dim 3.6550(3.6497) | Xent 0.8270(0.8292) | Loss 4.0685(4.0643) | Error 0.2936(0.2970) Steps 580(572.02) | Grad Norm 0.4142(0.6424) | Total Time 14.00(14.00)\n",
      "Iter 2627 | Time 56.8372(57.1723) | Bit/dim 3.6499(3.6498) | Xent 0.8300(0.8292) | Loss 4.0649(4.0643) | Error 0.2959(0.2970) Steps 580(572.26) | Grad Norm 0.4407(0.6364) | Total Time 14.00(14.00)\n",
      "Iter 2628 | Time 58.6073(57.2154) | Bit/dim 3.6529(3.6498) | Xent 0.8286(0.8292) | Loss 4.0672(4.0644) | Error 0.2927(0.2969) Steps 580(572.49) | Grad Norm 0.4984(0.6322) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0438 | Time 22.1753, Epoch Time 378.4951(377.4099), Bit/dim 3.6506(best: 3.6506), Xent 0.8529, Loss 4.0770, Error 0.2999(best: 0.2988)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2629 | Time 56.3983(57.1908) | Bit/dim 3.6440(3.6497) | Xent 0.8266(0.8291) | Loss 4.0572(4.0642) | Error 0.2959(0.2968) Steps 568(572.36) | Grad Norm 0.3313(0.6232) | Total Time 14.00(14.00)\n",
      "Iter 2630 | Time 56.1480(57.1596) | Bit/dim 3.6548(3.6498) | Xent 0.8152(0.8287) | Loss 4.0624(4.0642) | Error 0.2907(0.2967) Steps 568(572.23) | Grad Norm 0.7944(0.6283) | Total Time 14.00(14.00)\n",
      "Iter 2631 | Time 56.6103(57.1431) | Bit/dim 3.6510(3.6499) | Xent 0.8398(0.8290) | Loss 4.0709(4.0644) | Error 0.3023(0.2968) Steps 568(572.10) | Grad Norm 0.8258(0.6343) | Total Time 14.00(14.00)\n",
      "Iter 2632 | Time 56.8753(57.1350) | Bit/dim 3.6484(3.6498) | Xent 0.8262(0.8289) | Loss 4.0615(4.0643) | Error 0.2971(0.2968) Steps 568(571.98) | Grad Norm 0.6624(0.6351) | Total Time 14.00(14.00)\n",
      "Iter 2633 | Time 60.8224(57.2457) | Bit/dim 3.6491(3.6498) | Xent 0.8269(0.8289) | Loss 4.0625(4.0642) | Error 0.2994(0.2969) Steps 580(572.22) | Grad Norm 0.6230(0.6347) | Total Time 14.00(14.00)\n",
      "Iter 2634 | Time 57.0120(57.2387) | Bit/dim 3.6612(3.6501) | Xent 0.8270(0.8288) | Loss 4.0747(4.0645) | Error 0.2986(0.2970) Steps 568(572.09) | Grad Norm 0.4640(0.6296) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0439 | Time 22.3635, Epoch Time 381.8960(377.5445), Bit/dim 3.6521(best: 3.6506), Xent 0.8538, Loss 4.0790, Error 0.2996(best: 0.2988)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2635 | Time 58.8373(57.2866) | Bit/dim 3.6536(3.6502) | Xent 0.8338(0.8290) | Loss 4.0704(4.0647) | Error 0.2931(0.2968) Steps 568(571.97) | Grad Norm 0.8215(0.6354) | Total Time 14.00(14.00)\n",
      "Iter 2636 | Time 56.3764(57.2593) | Bit/dim 3.6494(3.6502) | Xent 0.8231(0.8288) | Loss 4.0609(4.0646) | Error 0.2934(0.2967) Steps 580(572.21) | Grad Norm 0.4583(0.6301) | Total Time 14.00(14.00)\n",
      "Iter 2637 | Time 55.6044(57.2097) | Bit/dim 3.6432(3.6500) | Xent 0.8255(0.8287) | Loss 4.0560(4.0643) | Error 0.2901(0.2965) Steps 568(572.08) | Grad Norm 0.8087(0.6354) | Total Time 14.00(14.00)\n",
      "Iter 2638 | Time 56.5215(57.1890) | Bit/dim 3.6505(3.6500) | Xent 0.8268(0.8286) | Loss 4.0639(4.0643) | Error 0.2977(0.2966) Steps 580(572.32) | Grad Norm 0.9799(0.6458) | Total Time 14.00(14.00)\n",
      "Iter 2639 | Time 56.5034(57.1684) | Bit/dim 3.6525(3.6501) | Xent 0.8274(0.8286) | Loss 4.0662(4.0644) | Error 0.2927(0.2965) Steps 568(572.19) | Grad Norm 0.4213(0.6390) | Total Time 14.00(14.00)\n",
      "Iter 2640 | Time 57.7081(57.1846) | Bit/dim 3.6486(3.6500) | Xent 0.8183(0.8283) | Loss 4.0577(4.0642) | Error 0.2926(0.2963) Steps 574(572.25) | Grad Norm 0.7485(0.6423) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0440 | Time 22.2919, Epoch Time 379.4027(377.6002), Bit/dim 3.6504(best: 3.6506), Xent 0.8534, Loss 4.0771, Error 0.3002(best: 0.2988)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2641 | Time 58.5346(57.2251) | Bit/dim 3.6495(3.6500) | Xent 0.8445(0.8288) | Loss 4.0718(4.0644) | Error 0.3053(0.2966) Steps 568(572.12) | Grad Norm 0.8170(0.6475) | Total Time 14.00(14.00)\n",
      "Iter 2642 | Time 58.7802(57.2718) | Bit/dim 3.6495(3.6500) | Xent 0.8395(0.8291) | Loss 4.0693(4.0646) | Error 0.2983(0.2967) Steps 568(572.00) | Grad Norm 0.7062(0.6493) | Total Time 14.00(14.00)\n",
      "Iter 2643 | Time 58.2707(57.3018) | Bit/dim 3.6442(3.6498) | Xent 0.8294(0.8291) | Loss 4.0589(4.0644) | Error 0.2965(0.2967) Steps 568(571.88) | Grad Norm 0.4912(0.6446) | Total Time 14.00(14.00)\n",
      "Iter 2644 | Time 57.2151(57.2992) | Bit/dim 3.6487(3.6498) | Xent 0.8194(0.8288) | Loss 4.0584(4.0642) | Error 0.2986(0.2967) Steps 568(571.76) | Grad Norm 0.6128(0.6436) | Total Time 14.00(14.00)\n",
      "Iter 2645 | Time 56.4857(57.2748) | Bit/dim 3.6531(3.6499) | Xent 0.8226(0.8286) | Loss 4.0644(4.0642) | Error 0.2986(0.2968) Steps 568(571.65) | Grad Norm 0.4942(0.6391) | Total Time 14.00(14.00)\n",
      "Iter 2646 | Time 55.2269(57.2133) | Bit/dim 3.6501(3.6499) | Xent 0.8348(0.8288) | Loss 4.0674(4.0643) | Error 0.2975(0.2968) Steps 568(571.54) | Grad Norm 0.6731(0.6401) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0441 | Time 21.9936, Epoch Time 382.1449(377.7366), Bit/dim 3.6523(best: 3.6504), Xent 0.8542, Loss 4.0795, Error 0.3000(best: 0.2988)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2647 | Time 55.5025(57.1620) | Bit/dim 3.6507(3.6499) | Xent 0.8174(0.8285) | Loss 4.0594(4.0642) | Error 0.2934(0.2967) Steps 580(571.79) | Grad Norm 0.5122(0.6363) | Total Time 14.00(14.00)\n",
      "Iter 2648 | Time 57.2510(57.1647) | Bit/dim 3.6640(3.6504) | Xent 0.8354(0.8287) | Loss 4.0816(4.0647) | Error 0.2983(0.2967) Steps 580(572.04) | Grad Norm 0.4509(0.6307) | Total Time 14.00(14.00)\n",
      "Iter 2649 | Time 57.5659(57.1767) | Bit/dim 3.6447(3.6502) | Xent 0.8296(0.8287) | Loss 4.0596(4.0645) | Error 0.3033(0.2969) Steps 568(571.92) | Grad Norm 0.8623(0.6377) | Total Time 14.00(14.00)\n",
      "Iter 2650 | Time 55.6173(57.1299) | Bit/dim 3.6434(3.6500) | Xent 0.8294(0.8287) | Loss 4.0581(4.0643) | Error 0.2985(0.2970) Steps 568(571.80) | Grad Norm 0.5698(0.6357) | Total Time 14.00(14.00)\n",
      "Iter 2651 | Time 58.3620(57.1669) | Bit/dim 3.6470(3.6499) | Xent 0.8217(0.8285) | Loss 4.0579(4.0641) | Error 0.2933(0.2969) Steps 568(571.69) | Grad Norm 0.4507(0.6301) | Total Time 14.00(14.00)\n",
      "Iter 2652 | Time 57.1121(57.1652) | Bit/dim 3.6468(3.6498) | Xent 0.8454(0.8290) | Loss 4.0695(4.0643) | Error 0.3045(0.2971) Steps 568(571.57) | Grad Norm 0.7663(0.6342) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0442 | Time 22.2959, Epoch Time 379.5987(377.7924), Bit/dim 3.6508(best: 3.6504), Xent 0.8544, Loss 4.0780, Error 0.3004(best: 0.2988)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2653 | Time 56.2544(57.1379) | Bit/dim 3.6600(3.6501) | Xent 0.8369(0.8292) | Loss 4.0785(4.0647) | Error 0.3020(0.2972) Steps 580(571.83) | Grad Norm 0.4519(0.6287) | Total Time 14.00(14.00)\n",
      "Iter 2654 | Time 55.6569(57.0935) | Bit/dim 3.6509(3.6501) | Xent 0.8205(0.8290) | Loss 4.0611(4.0646) | Error 0.2933(0.2971) Steps 568(571.71) | Grad Norm 0.7770(0.6332) | Total Time 14.00(14.00)\n",
      "Iter 2655 | Time 57.4021(57.1027) | Bit/dim 3.6499(3.6501) | Xent 0.8348(0.8292) | Loss 4.0673(4.0647) | Error 0.2960(0.2971) Steps 568(571.60) | Grad Norm 0.5466(0.6306) | Total Time 14.00(14.00)\n",
      "Iter 2656 | Time 58.0045(57.1298) | Bit/dim 3.6395(3.6498) | Xent 0.8205(0.8289) | Loss 4.0497(4.0643) | Error 0.2991(0.2972) Steps 580(571.85) | Grad Norm 0.5773(0.6290) | Total Time 14.00(14.00)\n",
      "Iter 2657 | Time 57.2773(57.1342) | Bit/dim 3.6512(3.6498) | Xent 0.8337(0.8290) | Loss 4.0680(4.0644) | Error 0.3056(0.2974) Steps 568(571.74) | Grad Norm 0.4184(0.6227) | Total Time 14.00(14.00)\n",
      "Iter 2658 | Time 58.6876(57.1808) | Bit/dim 3.6488(3.6498) | Xent 0.8320(0.8291) | Loss 4.0648(4.0644) | Error 0.3009(0.2975) Steps 568(571.63) | Grad Norm 0.8542(0.6296) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0443 | Time 22.0209, Epoch Time 380.9743(377.8879), Bit/dim 3.6511(best: 3.6504), Xent 0.8540, Loss 4.0781, Error 0.3024(best: 0.2988)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2659 | Time 54.8181(57.1099) | Bit/dim 3.6511(3.6499) | Xent 0.8392(0.8294) | Loss 4.0707(4.0646) | Error 0.3057(0.2978) Steps 568(571.52) | Grad Norm 1.1995(0.6467) | Total Time 14.00(14.00)\n",
      "Iter 2660 | Time 58.3024(57.1457) | Bit/dim 3.6519(3.6499) | Xent 0.8343(0.8296) | Loss 4.0691(4.0647) | Error 0.3003(0.2978) Steps 580(571.77) | Grad Norm 0.6626(0.6472) | Total Time 14.00(14.00)\n",
      "Iter 2661 | Time 58.2555(57.1790) | Bit/dim 3.6546(3.6501) | Xent 0.8204(0.8293) | Loss 4.0648(4.0647) | Error 0.2903(0.2976) Steps 580(572.02) | Grad Norm 0.9672(0.6568) | Total Time 14.00(14.00)\n",
      "Iter 2662 | Time 57.5203(57.1892) | Bit/dim 3.6489(3.6500) | Xent 0.8358(0.8295) | Loss 4.0668(4.0648) | Error 0.2974(0.2976) Steps 580(572.26) | Grad Norm 0.7030(0.6582) | Total Time 14.00(14.00)\n",
      "Iter 2663 | Time 57.2180(57.1901) | Bit/dim 3.6395(3.6497) | Xent 0.8323(0.8296) | Loss 4.0557(4.0645) | Error 0.2954(0.2975) Steps 568(572.13) | Grad Norm 0.4547(0.6521) | Total Time 14.00(14.00)\n",
      "Iter 2664 | Time 55.6018(57.1425) | Bit/dim 3.6557(3.6499) | Xent 0.8308(0.8296) | Loss 4.0711(4.0647) | Error 0.2963(0.2975) Steps 568(572.01) | Grad Norm 1.0863(0.6651) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0444 | Time 22.2606, Epoch Time 380.1201(377.9549), Bit/dim 3.6502(best: 3.6504), Xent 0.8546, Loss 4.0775, Error 0.2998(best: 0.2988)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2665 | Time 57.5845(57.1557) | Bit/dim 3.6516(3.6499) | Xent 0.8167(0.8292) | Loss 4.0599(4.0646) | Error 0.2963(0.2975) Steps 568(571.89) | Grad Norm 1.0910(0.6779) | Total Time 14.00(14.00)\n",
      "Iter 2666 | Time 58.3071(57.1903) | Bit/dim 3.6576(3.6502) | Xent 0.8421(0.8296) | Loss 4.0786(4.0650) | Error 0.3023(0.2976) Steps 580(572.13) | Grad Norm 0.7615(0.6804) | Total Time 14.00(14.00)\n",
      "Iter 2667 | Time 57.5776(57.2019) | Bit/dim 3.6507(3.6502) | Xent 0.8221(0.8294) | Loss 4.0618(4.0649) | Error 0.3017(0.2977) Steps 568(572.01) | Grad Norm 0.6816(0.6804) | Total Time 14.00(14.00)\n",
      "Iter 2668 | Time 57.5494(57.2123) | Bit/dim 3.6429(3.6500) | Xent 0.8091(0.8288) | Loss 4.0474(4.0644) | Error 0.2826(0.2973) Steps 580(572.25) | Grad Norm 0.7310(0.6819) | Total Time 14.00(14.00)\n",
      "Iter 2669 | Time 55.6914(57.1667) | Bit/dim 3.6549(3.6501) | Xent 0.8628(0.8298) | Loss 4.0863(4.0650) | Error 0.3111(0.2977) Steps 568(572.12) | Grad Norm 1.1320(0.6954) | Total Time 14.00(14.00)\n",
      "Iter 2670 | Time 57.2942(57.1705) | Bit/dim 3.6449(3.6500) | Xent 0.8218(0.8296) | Loss 4.0558(4.0647) | Error 0.2909(0.2975) Steps 580(572.35) | Grad Norm 0.7883(0.6982) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0445 | Time 22.3796, Epoch Time 382.2928(378.0850), Bit/dim 3.6509(best: 3.6502), Xent 0.8545, Loss 4.0781, Error 0.3027(best: 0.2988)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2671 | Time 59.2268(57.2322) | Bit/dim 3.6438(3.6498) | Xent 0.8334(0.8297) | Loss 4.0605(4.0646) | Error 0.3027(0.2976) Steps 568(572.22) | Grad Norm 0.4026(0.6893) | Total Time 14.00(14.00)\n",
      "Iter 2672 | Time 56.4224(57.2079) | Bit/dim 3.6480(3.6497) | Xent 0.8137(0.8292) | Loss 4.0548(4.0643) | Error 0.2925(0.2975) Steps 568(572.10) | Grad Norm 0.5508(0.6852) | Total Time 14.00(14.00)\n",
      "Iter 2673 | Time 56.6830(57.1921) | Bit/dim 3.6504(3.6497) | Xent 0.8388(0.8295) | Loss 4.0698(4.0645) | Error 0.3004(0.2976) Steps 568(571.97) | Grad Norm 1.0692(0.6967) | Total Time 14.00(14.00)\n",
      "Iter 2674 | Time 54.3366(57.1065) | Bit/dim 3.6556(3.6499) | Xent 0.8382(0.8297) | Loss 4.0747(4.0648) | Error 0.2996(0.2976) Steps 568(571.85) | Grad Norm 1.4299(0.7187) | Total Time 14.00(14.00)\n",
      "Iter 2675 | Time 55.4496(57.0568) | Bit/dim 3.6481(3.6499) | Xent 0.8385(0.8300) | Loss 4.0673(4.0649) | Error 0.3041(0.2978) Steps 568(571.74) | Grad Norm 0.7345(0.7192) | Total Time 14.00(14.00)\n",
      "Iter 2676 | Time 51.7675(56.8981) | Bit/dim 3.6547(3.6500) | Xent 0.8135(0.8295) | Loss 4.0614(4.0648) | Error 0.2910(0.2976) Steps 568(571.63) | Grad Norm 0.8099(0.7219) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0446 | Time 22.3267, Epoch Time 371.7481(377.8949), Bit/dim 3.6516(best: 3.6502), Xent 0.8558, Loss 4.0795, Error 0.3009(best: 0.2988)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2677 | Time 58.6184(56.9497) | Bit/dim 3.6620(3.6504) | Xent 0.8461(0.8300) | Loss 4.0851(4.0654) | Error 0.3053(0.2979) Steps 580(571.88) | Grad Norm 1.0614(0.7321) | Total Time 14.00(14.00)\n",
      "Iter 2678 | Time 58.5005(56.9962) | Bit/dim 3.6449(3.6502) | Xent 0.8288(0.8300) | Loss 4.0593(4.0652) | Error 0.2991(0.2979) Steps 580(572.12) | Grad Norm 1.0219(0.7408) | Total Time 14.00(14.00)\n",
      "Iter 2679 | Time 56.6349(56.9854) | Bit/dim 3.6528(3.6503) | Xent 0.8308(0.8300) | Loss 4.0682(4.0653) | Error 0.2959(0.2978) Steps 580(572.36) | Grad Norm 0.9786(0.7479) | Total Time 14.00(14.00)\n",
      "Iter 2680 | Time 57.6936(57.0066) | Bit/dim 3.6489(3.6502) | Xent 0.8438(0.8304) | Loss 4.0709(4.0654) | Error 0.3031(0.2980) Steps 568(572.23) | Grad Norm 0.8818(0.7519) | Total Time 14.00(14.00)\n",
      "Iter 2681 | Time 58.3439(57.0468) | Bit/dim 3.6452(3.6501) | Xent 0.8083(0.8298) | Loss 4.0493(4.0650) | Error 0.2915(0.2978) Steps 568(572.10) | Grad Norm 0.8478(0.7548) | Total Time 14.00(14.00)\n",
      "Iter 2682 | Time 57.0978(57.0483) | Bit/dim 3.6452(3.6499) | Xent 0.8273(0.8297) | Loss 4.0588(4.0648) | Error 0.2969(0.2978) Steps 568(571.98) | Grad Norm 0.8907(0.7589) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0447 | Time 22.2361, Epoch Time 384.6681(378.0981), Bit/dim 3.6507(best: 3.6502), Xent 0.8531, Loss 4.0773, Error 0.3028(best: 0.2988)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2683 | Time 56.5692(57.0339) | Bit/dim 3.6473(3.6499) | Xent 0.8295(0.8297) | Loss 4.0621(4.0647) | Error 0.2950(0.2977) Steps 568(571.86) | Grad Norm 0.8406(0.7613) | Total Time 14.00(14.00)\n",
      "Iter 2684 | Time 58.1019(57.0660) | Bit/dim 3.6420(3.6496) | Xent 0.8185(0.8293) | Loss 4.0512(4.0643) | Error 0.2930(0.2975) Steps 580(572.10) | Grad Norm 0.8534(0.7641) | Total Time 14.00(14.00)\n",
      "Iter 2685 | Time 57.8553(57.0896) | Bit/dim 3.6537(3.6497) | Xent 0.8425(0.8297) | Loss 4.0750(4.0646) | Error 0.3060(0.2978) Steps 592(572.70) | Grad Norm 1.0189(0.7717) | Total Time 14.00(14.00)\n",
      "Iter 2686 | Time 56.4252(57.0697) | Bit/dim 3.6543(3.6499) | Xent 0.8325(0.8298) | Loss 4.0705(4.0648) | Error 0.3011(0.2979) Steps 568(572.56) | Grad Norm 0.8787(0.7750) | Total Time 14.00(14.00)\n",
      "Iter 2687 | Time 56.2057(57.0438) | Bit/dim 3.6435(3.6497) | Xent 0.8385(0.8301) | Loss 4.0627(4.0647) | Error 0.3061(0.2981) Steps 568(572.42) | Grad Norm 0.6422(0.7710) | Total Time 14.00(14.00)\n",
      "Iter 2688 | Time 56.8667(57.0385) | Bit/dim 3.6504(3.6497) | Xent 0.8188(0.8297) | Loss 4.0598(4.0646) | Error 0.2959(0.2981) Steps 574(572.47) | Grad Norm 1.2929(0.7866) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0448 | Time 22.5276, Epoch Time 380.0478(378.1566), Bit/dim 3.6497(best: 3.6502), Xent 0.8537, Loss 4.0766, Error 0.3016(best: 0.2988)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2689 | Time 56.8525(57.0329) | Bit/dim 3.6509(3.6497) | Xent 0.8179(0.8294) | Loss 4.0599(4.0644) | Error 0.2955(0.2980) Steps 568(572.33) | Grad Norm 1.0192(0.7936) | Total Time 14.00(14.00)\n",
      "Iter 2690 | Time 54.4886(56.9566) | Bit/dim 3.6482(3.6497) | Xent 0.8341(0.8295) | Loss 4.0652(4.0645) | Error 0.3039(0.2982) Steps 568(572.20) | Grad Norm 0.8717(0.7959) | Total Time 14.00(14.00)\n",
      "Iter 2691 | Time 60.7753(57.0711) | Bit/dim 3.6490(3.6497) | Xent 0.8268(0.8294) | Loss 4.0624(4.0644) | Error 0.3054(0.2984) Steps 580(572.44) | Grad Norm 0.7684(0.7951) | Total Time 14.00(14.00)\n",
      "Iter 2692 | Time 59.8714(57.1551) | Bit/dim 3.6538(3.6498) | Xent 0.8119(0.8289) | Loss 4.0598(4.0643) | Error 0.2896(0.2981) Steps 580(572.67) | Grad Norm 1.2925(0.8100) | Total Time 14.00(14.00)\n",
      "Iter 2693 | Time 56.4770(57.1348) | Bit/dim 3.6476(3.6497) | Xent 0.8449(0.8294) | Loss 4.0700(4.0644) | Error 0.2987(0.2981) Steps 568(572.53) | Grad Norm 1.5022(0.8308) | Total Time 14.00(14.00)\n",
      "Iter 2694 | Time 56.9720(57.1299) | Bit/dim 3.6484(3.6497) | Xent 0.8495(0.8300) | Loss 4.0731(4.0647) | Error 0.3103(0.2985) Steps 568(572.39) | Grad Norm 0.9843(0.8354) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0449 | Time 22.3043, Epoch Time 383.4596(378.3157), Bit/dim 3.6507(best: 3.6497), Xent 0.8523, Loss 4.0768, Error 0.3022(best: 0.2988)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2695 | Time 53.6364(57.0251) | Bit/dim 3.6530(3.6498) | Xent 0.8220(0.8298) | Loss 4.0640(4.0647) | Error 0.2969(0.2985) Steps 568(572.26) | Grad Norm 0.6860(0.8309) | Total Time 14.00(14.00)\n",
      "Iter 2696 | Time 55.3682(56.9754) | Bit/dim 3.6416(3.6495) | Xent 0.8424(0.8301) | Loss 4.0628(4.0646) | Error 0.3023(0.2986) Steps 568(572.13) | Grad Norm 1.0051(0.8361) | Total Time 14.00(14.00)\n",
      "Iter 2697 | Time 58.0963(57.0090) | Bit/dim 3.6587(3.6498) | Xent 0.8136(0.8296) | Loss 4.0655(4.0646) | Error 0.2930(0.2984) Steps 580(572.37) | Grad Norm 1.4771(0.8554) | Total Time 14.00(14.00)\n",
      "Iter 2698 | Time 58.0164(57.0392) | Bit/dim 3.6485(3.6498) | Xent 0.8326(0.8297) | Loss 4.0648(4.0646) | Error 0.2999(0.2985) Steps 580(572.60) | Grad Norm 1.3199(0.8693) | Total Time 14.00(14.00)\n",
      "Iter 2699 | Time 55.1937(56.9839) | Bit/dim 3.6416(3.6495) | Xent 0.8416(0.8301) | Loss 4.0624(4.0646) | Error 0.2991(0.2985) Steps 568(572.46) | Grad Norm 0.4753(0.8575) | Total Time 14.00(14.00)\n",
      "Iter 2700 | Time 57.6789(57.0047) | Bit/dim 3.6548(3.6497) | Xent 0.8207(0.8298) | Loss 4.0651(4.0646) | Error 0.2906(0.2982) Steps 568(572.32) | Grad Norm 0.5775(0.8491) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0450 | Time 22.4348, Epoch Time 376.2473(378.2536), Bit/dim 3.6515(best: 3.6497), Xent 0.8531, Loss 4.0781, Error 0.3018(best: 0.2988)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2701 | Time 58.0568(57.0363) | Bit/dim 3.6538(3.6498) | Xent 0.8440(0.8302) | Loss 4.0758(4.0649) | Error 0.3011(0.2983) Steps 568(572.19) | Grad Norm 0.9363(0.8517) | Total Time 14.00(14.00)\n",
      "Iter 2702 | Time 55.8915(57.0019) | Bit/dim 3.6483(3.6498) | Xent 0.8378(0.8305) | Loss 4.0672(4.0650) | Error 0.2999(0.2984) Steps 568(572.07) | Grad Norm 1.0608(0.8580) | Total Time 14.00(14.00)\n",
      "Iter 2703 | Time 55.1803(56.9473) | Bit/dim 3.6552(3.6499) | Xent 0.8283(0.8304) | Loss 4.0693(4.0651) | Error 0.2987(0.2984) Steps 568(571.95) | Grad Norm 0.6361(0.8513) | Total Time 14.00(14.00)\n",
      "Iter 2704 | Time 57.5539(56.9655) | Bit/dim 3.6426(3.6497) | Xent 0.8222(0.8301) | Loss 4.0537(4.0648) | Error 0.2976(0.2984) Steps 568(571.83) | Grad Norm 0.4834(0.8403) | Total Time 14.00(14.00)\n",
      "Iter 2705 | Time 58.0119(56.9969) | Bit/dim 3.6414(3.6495) | Xent 0.8448(0.8306) | Loss 4.0638(4.0648) | Error 0.3090(0.2987) Steps 580(572.07) | Grad Norm 0.8606(0.8409) | Total Time 14.00(14.00)\n",
      "Iter 2706 | Time 56.1195(56.9706) | Bit/dim 3.6566(3.6497) | Xent 0.8176(0.8302) | Loss 4.0654(4.0648) | Error 0.2935(0.2985) Steps 568(571.95) | Grad Norm 1.0067(0.8459) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0451 | Time 22.1534, Epoch Time 378.4893(378.2607), Bit/dim 3.6505(best: 3.6497), Xent 0.8515, Loss 4.0762, Error 0.3008(best: 0.2988)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2707 | Time 58.5049(57.0166) | Bit/dim 3.6556(3.6499) | Xent 0.8207(0.8299) | Loss 4.0660(4.0648) | Error 0.2929(0.2984) Steps 580(572.19) | Grad Norm 0.5230(0.8362) | Total Time 14.00(14.00)\n",
      "Iter 2708 | Time 56.3898(56.9978) | Bit/dim 3.6460(3.6497) | Xent 0.8426(0.8303) | Loss 4.0673(4.0649) | Error 0.3033(0.2985) Steps 580(572.43) | Grad Norm 0.6722(0.8313) | Total Time 14.00(14.00)\n",
      "Iter 2709 | Time 57.3653(57.0088) | Bit/dim 3.6504(3.6498) | Xent 0.8238(0.8301) | Loss 4.0622(4.0648) | Error 0.2970(0.2985) Steps 568(572.29) | Grad Norm 1.0920(0.8391) | Total Time 14.00(14.00)\n",
      "Iter 2710 | Time 55.0968(56.9515) | Bit/dim 3.6451(3.6496) | Xent 0.8197(0.8298) | Loss 4.0549(4.0645) | Error 0.2936(0.2983) Steps 568(572.16) | Grad Norm 0.6097(0.8322) | Total Time 14.00(14.00)\n",
      "Iter 2711 | Time 57.8517(56.9785) | Bit/dim 3.6417(3.6494) | Xent 0.8266(0.8297) | Loss 4.0549(4.0642) | Error 0.3011(0.2984) Steps 586(572.58) | Grad Norm 0.6551(0.8269) | Total Time 14.00(14.00)\n",
      "Iter 2712 | Time 56.5349(56.9652) | Bit/dim 3.6562(3.6496) | Xent 0.8193(0.8294) | Loss 4.0658(4.0643) | Error 0.2890(0.2981) Steps 568(572.44) | Grad Norm 1.1432(0.8364) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0452 | Time 22.4484, Epoch Time 380.0308(378.3138), Bit/dim 3.6502(best: 3.6497), Xent 0.8532, Loss 4.0768, Error 0.2996(best: 0.2988)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2713 | Time 58.5260(57.0120) | Bit/dim 3.6458(3.6495) | Xent 0.8393(0.8297) | Loss 4.0655(4.0643) | Error 0.3035(0.2983) Steps 568(572.31) | Grad Norm 1.1632(0.8462) | Total Time 14.00(14.00)\n",
      "Iter 2714 | Time 56.8802(57.0080) | Bit/dim 3.6630(3.6499) | Xent 0.8140(0.8292) | Loss 4.0699(4.0645) | Error 0.2893(0.2980) Steps 568(572.18) | Grad Norm 1.0061(0.8510) | Total Time 14.00(14.00)\n",
      "Iter 2715 | Time 55.8704(56.9739) | Bit/dim 3.6460(3.6498) | Xent 0.8211(0.8290) | Loss 4.0566(4.0642) | Error 0.2954(0.2979) Steps 568(572.05) | Grad Norm 0.4988(0.8404) | Total Time 14.00(14.00)\n",
      "Iter 2716 | Time 56.6187(56.9632) | Bit/dim 3.6500(3.6498) | Xent 0.8376(0.8292) | Loss 4.0688(4.0644) | Error 0.2959(0.2979) Steps 574(572.11) | Grad Norm 1.2527(0.8528) | Total Time 14.00(14.00)\n",
      "Iter 2717 | Time 57.2531(56.9719) | Bit/dim 3.6455(3.6496) | Xent 0.8222(0.8290) | Loss 4.0565(4.0641) | Error 0.3014(0.2980) Steps 568(571.99) | Grad Norm 1.3579(0.8679) | Total Time 14.00(14.00)\n",
      "Iter 2718 | Time 56.8938(56.9696) | Bit/dim 3.6448(3.6495) | Xent 0.8214(0.8288) | Loss 4.0555(4.0639) | Error 0.2945(0.2979) Steps 568(571.87) | Grad Norm 0.7010(0.8629) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0453 | Time 22.1456, Epoch Time 379.8496(378.3599), Bit/dim 3.6504(best: 3.6497), Xent 0.8521, Loss 4.0764, Error 0.3021(best: 0.2988)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2719 | Time 53.2605(56.8583) | Bit/dim 3.6553(3.6497) | Xent 0.8403(0.8291) | Loss 4.0755(4.0642) | Error 0.3033(0.2980) Steps 568(571.75) | Grad Norm 0.7488(0.8595) | Total Time 14.00(14.00)\n",
      "Iter 2720 | Time 57.5559(56.8792) | Bit/dim 3.6575(3.6499) | Xent 0.8385(0.8294) | Loss 4.0768(4.0646) | Error 0.2981(0.2980) Steps 568(571.64) | Grad Norm 0.9102(0.8610) | Total Time 14.00(14.00)\n",
      "Iter 2721 | Time 56.6082(56.8711) | Bit/dim 3.6444(3.6497) | Xent 0.8122(0.8289) | Loss 4.0505(4.0642) | Error 0.2949(0.2979) Steps 568(571.53) | Grad Norm 0.7145(0.8566) | Total Time 14.00(14.00)\n",
      "Iter 2722 | Time 56.4498(56.8585) | Bit/dim 3.6433(3.6495) | Xent 0.8223(0.8287) | Loss 4.0544(4.0639) | Error 0.2941(0.2978) Steps 580(571.79) | Grad Norm 0.6289(0.8498) | Total Time 14.00(14.00)\n",
      "Iter 2723 | Time 57.7061(56.8839) | Bit/dim 3.6476(3.6495) | Xent 0.8339(0.8289) | Loss 4.0645(4.0639) | Error 0.3000(0.2979) Steps 586(572.21) | Grad Norm 1.0325(0.8553) | Total Time 14.00(14.00)\n",
      "Iter 2724 | Time 58.1481(56.9218) | Bit/dim 3.6477(3.6494) | Xent 0.8243(0.8287) | Loss 4.0599(4.0638) | Error 0.3013(0.2980) Steps 580(572.45) | Grad Norm 0.4923(0.8444) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0454 | Time 22.3220, Epoch Time 377.8349(378.3441), Bit/dim 3.6515(best: 3.6497), Xent 0.8531, Loss 4.0781, Error 0.2997(best: 0.2988)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2725 | Time 57.0470(56.9256) | Bit/dim 3.6485(3.6494) | Xent 0.8319(0.8288) | Loss 4.0644(4.0638) | Error 0.2950(0.2979) Steps 574(572.49) | Grad Norm 0.5430(0.8354) | Total Time 14.00(14.00)\n",
      "Iter 2726 | Time 57.8681(56.9539) | Bit/dim 3.6419(3.6492) | Xent 0.8315(0.8289) | Loss 4.0577(4.0636) | Error 0.2937(0.2978) Steps 568(572.36) | Grad Norm 0.7627(0.8332) | Total Time 14.00(14.00)\n",
      "Iter 2727 | Time 56.4453(56.9386) | Bit/dim 3.6554(3.6494) | Xent 0.8286(0.8289) | Loss 4.0697(4.0638) | Error 0.2991(0.2978) Steps 568(572.23) | Grad Norm 0.6622(0.8280) | Total Time 14.00(14.00)\n",
      "Iter 2728 | Time 56.2307(56.9174) | Bit/dim 3.6522(3.6494) | Xent 0.8073(0.8282) | Loss 4.0558(4.0636) | Error 0.2877(0.2975) Steps 568(572.10) | Grad Norm 0.5934(0.8210) | Total Time 14.00(14.00)\n",
      "Iter 2729 | Time 61.4363(57.0529) | Bit/dim 3.6494(3.6494) | Xent 0.8287(0.8282) | Loss 4.0638(4.0636) | Error 0.2946(0.2974) Steps 580(572.34) | Grad Norm 0.8891(0.8230) | Total Time 14.00(14.00)\n",
      "Iter 2730 | Time 57.7648(57.0743) | Bit/dim 3.6493(3.6494) | Xent 0.8271(0.8282) | Loss 4.0629(4.0636) | Error 0.3014(0.2975) Steps 580(572.57) | Grad Norm 0.4849(0.8129) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0455 | Time 22.4787, Epoch Time 384.8318(378.5387), Bit/dim 3.6510(best: 3.6497), Xent 0.8542, Loss 4.0781, Error 0.3013(best: 0.2988)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2731 | Time 59.8712(57.1582) | Bit/dim 3.6504(3.6495) | Xent 0.8292(0.8282) | Loss 4.0650(4.0636) | Error 0.2975(0.2975) Steps 568(572.43) | Grad Norm 0.9308(0.8164) | Total Time 14.00(14.00)\n",
      "Iter 2732 | Time 60.0050(57.2436) | Bit/dim 3.6497(3.6495) | Xent 0.8377(0.8285) | Loss 4.0686(4.0637) | Error 0.2964(0.2975) Steps 580(572.66) | Grad Norm 1.1431(0.8262) | Total Time 14.00(14.00)\n",
      "Iter 2733 | Time 53.8975(57.1432) | Bit/dim 3.6419(3.6493) | Xent 0.8344(0.8287) | Loss 4.0591(4.0636) | Error 0.2939(0.2974) Steps 574(572.70) | Grad Norm 0.9543(0.8301) | Total Time 14.00(14.00)\n",
      "Iter 2734 | Time 53.9273(57.0467) | Bit/dim 3.6499(3.6493) | Xent 0.8406(0.8291) | Loss 4.0701(4.0638) | Error 0.2987(0.2974) Steps 568(572.56) | Grad Norm 1.0843(0.8377) | Total Time 14.00(14.00)\n",
      "Iter 2735 | Time 56.3563(57.0260) | Bit/dim 3.6532(3.6494) | Xent 0.8237(0.8289) | Loss 4.0651(4.0638) | Error 0.2920(0.2973) Steps 574(572.60) | Grad Norm 0.8265(0.8374) | Total Time 14.00(14.00)\n",
      "Iter 2736 | Time 55.9100(56.9925) | Bit/dim 3.6469(3.6493) | Xent 0.8214(0.8287) | Loss 4.0576(4.0637) | Error 0.2985(0.2973) Steps 568(572.46) | Grad Norm 1.4133(0.8546) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0456 | Time 22.1698, Epoch Time 377.7357(378.5146), Bit/dim 3.6512(best: 3.6497), Xent 0.8538, Loss 4.0781, Error 0.3025(best: 0.2988)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2737 | Time 54.6734(56.9230) | Bit/dim 3.6496(3.6493) | Xent 0.8527(0.8294) | Loss 4.0760(4.0640) | Error 0.3054(0.2976) Steps 574(572.51) | Grad Norm 1.2338(0.8660) | Total Time 14.00(14.00)\n",
      "Iter 2738 | Time 56.9180(56.9228) | Bit/dim 3.6500(3.6493) | Xent 0.8271(0.8293) | Loss 4.0635(4.0640) | Error 0.3007(0.2976) Steps 568(572.37) | Grad Norm 0.6493(0.8595) | Total Time 14.00(14.00)\n",
      "Iter 2739 | Time 56.1663(56.9001) | Bit/dim 3.6532(3.6495) | Xent 0.8243(0.8292) | Loss 4.0653(4.0640) | Error 0.2935(0.2975) Steps 568(572.24) | Grad Norm 0.8377(0.8589) | Total Time 14.00(14.00)\n",
      "Iter 2740 | Time 57.4290(56.9160) | Bit/dim 3.6425(3.6493) | Xent 0.8114(0.8286) | Loss 4.0482(4.0636) | Error 0.2951(0.2975) Steps 568(572.11) | Grad Norm 0.9438(0.8614) | Total Time 14.00(14.00)\n",
      "Iter 2741 | Time 55.5523(56.8751) | Bit/dim 3.6401(3.6490) | Xent 0.8166(0.8283) | Loss 4.0484(4.0631) | Error 0.2917(0.2973) Steps 568(571.99) | Grad Norm 0.4592(0.8493) | Total Time 14.00(14.00)\n",
      "Iter 2742 | Time 57.6131(56.8972) | Bit/dim 3.6596(3.6493) | Xent 0.8359(0.8285) | Loss 4.0776(4.0635) | Error 0.3013(0.2974) Steps 580(572.23) | Grad Norm 0.6296(0.8428) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0457 | Time 22.3659, Epoch Time 376.5391(378.4554), Bit/dim 3.6503(best: 3.6497), Xent 0.8528, Loss 4.0767, Error 0.3017(best: 0.2988)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2743 | Time 55.4749(56.8546) | Bit/dim 3.6524(3.6494) | Xent 0.8308(0.8286) | Loss 4.0678(4.0637) | Error 0.2935(0.2973) Steps 580(572.46) | Grad Norm 0.5131(0.8329) | Total Time 14.00(14.00)\n",
      "Iter 2744 | Time 57.2566(56.8666) | Bit/dim 3.6507(3.6494) | Xent 0.8376(0.8288) | Loss 4.0695(4.0639) | Error 0.3056(0.2975) Steps 568(572.33) | Grad Norm 0.5116(0.8232) | Total Time 14.00(14.00)\n",
      "Iter 2745 | Time 56.6906(56.8613) | Bit/dim 3.6514(3.6495) | Xent 0.8271(0.8288) | Loss 4.0649(4.0639) | Error 0.2961(0.2975) Steps 568(572.20) | Grad Norm 0.5812(0.8160) | Total Time 14.00(14.00)\n",
      "Iter 2746 | Time 55.4157(56.8180) | Bit/dim 3.6527(3.6496) | Xent 0.8262(0.8287) | Loss 4.0658(4.0639) | Error 0.2964(0.2975) Steps 568(572.07) | Grad Norm 0.6189(0.8101) | Total Time 14.00(14.00)\n",
      "Iter 2747 | Time 56.5552(56.8101) | Bit/dim 3.6513(3.6496) | Xent 0.8170(0.8284) | Loss 4.0598(4.0638) | Error 0.2934(0.2973) Steps 568(571.95) | Grad Norm 0.7061(0.8069) | Total Time 14.00(14.00)\n",
      "Iter 2748 | Time 58.9829(56.8753) | Bit/dim 3.6325(3.6491) | Xent 0.8269(0.8283) | Loss 4.0460(4.0633) | Error 0.2989(0.2974) Steps 580(572.19) | Grad Norm 0.5476(0.7992) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0458 | Time 22.4741, Epoch Time 378.6055(378.4599), Bit/dim 3.6505(best: 3.6497), Xent 0.8526, Loss 4.0768, Error 0.3023(best: 0.2988)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2749 | Time 56.2434(56.8563) | Bit/dim 3.6536(3.6493) | Xent 0.8143(0.8279) | Loss 4.0607(4.0632) | Error 0.2931(0.2973) Steps 568(572.07) | Grad Norm 0.4798(0.7896) | Total Time 14.00(14.00)\n",
      "Iter 2750 | Time 57.2569(56.8683) | Bit/dim 3.6610(3.6496) | Xent 0.8273(0.8279) | Loss 4.0747(4.0636) | Error 0.2979(0.2973) Steps 574(572.13) | Grad Norm 0.6877(0.7865) | Total Time 14.00(14.00)\n",
      "Iter 2751 | Time 56.6009(56.8603) | Bit/dim 3.6293(3.6490) | Xent 0.8393(0.8282) | Loss 4.0489(4.0631) | Error 0.2994(0.2973) Steps 580(572.36) | Grad Norm 0.8212(0.7876) | Total Time 14.00(14.00)\n",
      "Iter 2752 | Time 56.6964(56.8554) | Bit/dim 3.6441(3.6489) | Xent 0.8295(0.8283) | Loss 4.0589(4.0630) | Error 0.2986(0.2974) Steps 568(572.23) | Grad Norm 0.5221(0.7796) | Total Time 14.00(14.00)\n",
      "Iter 2753 | Time 56.9950(56.8596) | Bit/dim 3.6455(3.6487) | Xent 0.8257(0.8282) | Loss 4.0583(4.0628) | Error 0.2951(0.2973) Steps 586(572.64) | Grad Norm 0.5716(0.7734) | Total Time 14.00(14.00)\n",
      "Iter 2754 | Time 59.2758(56.9321) | Bit/dim 3.6576(3.6490) | Xent 0.8336(0.8284) | Loss 4.0744(4.0632) | Error 0.3000(0.2974) Steps 580(572.86) | Grad Norm 0.7980(0.7741) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0459 | Time 22.2010, Epoch Time 380.7065(378.5273), Bit/dim 3.6521(best: 3.6497), Xent 0.8498, Loss 4.0770, Error 0.3012(best: 0.2988)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2755 | Time 55.3571(56.8848) | Bit/dim 3.6472(3.6490) | Xent 0.8315(0.8285) | Loss 4.0630(4.0632) | Error 0.2970(0.2974) Steps 568(572.72) | Grad Norm 0.6672(0.7709) | Total Time 14.00(14.00)\n",
      "Iter 2756 | Time 54.9644(56.8272) | Bit/dim 3.6440(3.6488) | Xent 0.8062(0.8278) | Loss 4.0471(4.0627) | Error 0.2903(0.2972) Steps 568(572.58) | Grad Norm 0.5217(0.7634) | Total Time 14.00(14.00)\n",
      "Iter 2757 | Time 59.2456(56.8998) | Bit/dim 3.6424(3.6486) | Xent 0.8090(0.8272) | Loss 4.0469(4.0622) | Error 0.2930(0.2970) Steps 580(572.80) | Grad Norm 0.6083(0.7588) | Total Time 14.00(14.00)\n",
      "Iter 2758 | Time 58.9229(56.9604) | Bit/dim 3.6496(3.6487) | Xent 0.8259(0.8272) | Loss 4.0625(4.0622) | Error 0.2944(0.2970) Steps 586(573.20) | Grad Norm 0.5556(0.7527) | Total Time 14.00(14.00)\n",
      "Iter 2759 | Time 57.5701(56.9787) | Bit/dim 3.6613(3.6490) | Xent 0.8290(0.8272) | Loss 4.0758(4.0626) | Error 0.2981(0.2970) Steps 568(573.04) | Grad Norm 0.5406(0.7463) | Total Time 14.00(14.00)\n",
      "Iter 2760 | Time 57.5210(56.9950) | Bit/dim 3.6496(3.6490) | Xent 0.8426(0.8277) | Loss 4.0709(4.0629) | Error 0.2997(0.2971) Steps 574(573.07) | Grad Norm 0.7107(0.7452) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0460 | Time 22.3181, Epoch Time 381.3390(378.6116), Bit/dim 3.6506(best: 3.6497), Xent 0.8520, Loss 4.0766, Error 0.3025(best: 0.2988)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2761 | Time 54.2893(56.9138) | Bit/dim 3.6548(3.6492) | Xent 0.8303(0.8278) | Loss 4.0699(4.0631) | Error 0.2991(0.2971) Steps 568(572.92) | Grad Norm 0.6013(0.7409) | Total Time 14.00(14.00)\n",
      "Iter 2762 | Time 54.5235(56.8421) | Bit/dim 3.6445(3.6491) | Xent 0.8292(0.8278) | Loss 4.0592(4.0630) | Error 0.2939(0.2970) Steps 586(573.31) | Grad Norm 0.4632(0.7326) | Total Time 14.00(14.00)\n",
      "Iter 2765 | Time 56.5500(56.9756) | Bit/dim 3.6450(3.6493) | Xent 0.8185(0.8278) | Loss 4.0543(4.0632) | Error 0.2980(0.2971) Steps 568(573.36) | Grad Norm 0.7121(0.7310) | Total Time 14.00(14.00)\n",
      "Iter 2766 | Time 60.1389(57.0705) | Bit/dim 3.6472(3.6493) | Xent 0.8387(0.8281) | Loss 4.0666(4.0633) | Error 0.3025(0.2973) Steps 580(573.56) | Grad Norm 0.9441(0.7374) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0461 | Time 22.2538, Epoch Time 382.5972(378.7312), Bit/dim 3.6502(best: 3.6497), Xent 0.8511, Loss 4.0758, Error 0.2998(best: 0.2988)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2767 | Time 54.6185(56.9969) | Bit/dim 3.6486(3.6492) | Xent 0.8177(0.8278) | Loss 4.0575(4.0632) | Error 0.2950(0.2972) Steps 580(573.76) | Grad Norm 0.5730(0.7325) | Total Time 14.00(14.00)\n",
      "Iter 2768 | Time 56.4019(56.9791) | Bit/dim 3.6518(3.6493) | Xent 0.8159(0.8275) | Loss 4.0598(4.0631) | Error 0.2931(0.2971) Steps 586(574.12) | Grad Norm 0.8030(0.7346) | Total Time 14.00(14.00)\n",
      "Iter 2769 | Time 60.3125(57.0791) | Bit/dim 3.6480(3.6493) | Xent 0.8481(0.8281) | Loss 4.0721(4.0633) | Error 0.2964(0.2971) Steps 592(574.66) | Grad Norm 0.6970(0.7335) | Total Time 14.00(14.00)\n",
      "Iter 2770 | Time 57.2834(57.0852) | Bit/dim 3.6495(3.6493) | Xent 0.8160(0.8277) | Loss 4.0575(4.0632) | Error 0.2957(0.2970) Steps 574(574.64) | Grad Norm 0.8554(0.7371) | Total Time 14.00(14.00)\n",
      "Iter 2771 | Time 59.4437(57.1560) | Bit/dim 3.6424(3.6491) | Xent 0.8310(0.8278) | Loss 4.0579(4.0630) | Error 0.2980(0.2971) Steps 568(574.44) | Grad Norm 0.9874(0.7446) | Total Time 14.00(14.00)\n",
      "Iter 2772 | Time 57.0906(57.1540) | Bit/dim 3.6523(3.6492) | Xent 0.8266(0.8278) | Loss 4.0656(4.0631) | Error 0.2955(0.2970) Steps 580(574.61) | Grad Norm 0.6416(0.7416) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0462 | Time 22.3129, Epoch Time 383.4898(378.8740), Bit/dim 3.6498(best: 3.6497), Xent 0.8525, Loss 4.0760, Error 0.3003(best: 0.2988)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2773 | Time 57.8288(57.1743) | Bit/dim 3.6496(3.6492) | Xent 0.8143(0.8274) | Loss 4.0568(4.0629) | Error 0.2960(0.2970) Steps 580(574.77) | Grad Norm 0.5441(0.7356) | Total Time 14.00(14.00)\n",
      "Iter 2774 | Time 58.5961(57.2169) | Bit/dim 3.6559(3.6494) | Xent 0.8236(0.8273) | Loss 4.0677(4.0630) | Error 0.2907(0.2968) Steps 586(575.11) | Grad Norm 0.8903(0.7403) | Total Time 14.00(14.00)\n",
      "Iter 2775 | Time 59.6885(57.2911) | Bit/dim 3.6511(3.6494) | Xent 0.8421(0.8277) | Loss 4.0722(4.0633) | Error 0.2975(0.2968) Steps 592(575.61) | Grad Norm 1.0567(0.7498) | Total Time 14.00(14.00)\n",
      "Iter 2776 | Time 55.5509(57.2389) | Bit/dim 3.6526(3.6495) | Xent 0.8269(0.8277) | Loss 4.0660(4.0634) | Error 0.3016(0.2970) Steps 568(575.38) | Grad Norm 0.9544(0.7559) | Total Time 14.00(14.00)\n",
      "Iter 2777 | Time 58.9467(57.2901) | Bit/dim 3.6392(3.6492) | Xent 0.8181(0.8274) | Loss 4.0482(4.0629) | Error 0.2913(0.2968) Steps 568(575.16) | Grad Norm 0.5644(0.7502) | Total Time 14.00(14.00)\n",
      "Iter 2778 | Time 59.1411(57.3456) | Bit/dim 3.6425(3.6490) | Xent 0.8179(0.8271) | Loss 4.0514(4.0626) | Error 0.2993(0.2969) Steps 580(575.31) | Grad Norm 1.0232(0.7583) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0463 | Time 22.3303, Epoch Time 387.7053(379.1389), Bit/dim 3.6506(best: 3.6497), Xent 0.8516, Loss 4.0764, Error 0.3009(best: 0.2988)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2779 | Time 52.6970(57.2062) | Bit/dim 3.6476(3.6490) | Xent 0.8368(0.8274) | Loss 4.0661(4.0627) | Error 0.2935(0.2968) Steps 574(575.27) | Grad Norm 1.1509(0.7701) | Total Time 14.00(14.00)\n",
      "Iter 2780 | Time 57.6784(57.2203) | Bit/dim 3.6530(3.6491) | Xent 0.8283(0.8274) | Loss 4.0672(4.0628) | Error 0.2999(0.2969) Steps 568(575.05) | Grad Norm 0.5119(0.7624) | Total Time 14.00(14.00)\n",
      "Iter 2781 | Time 60.5245(57.3195) | Bit/dim 3.6438(3.6489) | Xent 0.8149(0.8271) | Loss 4.0513(4.0625) | Error 0.2940(0.2968) Steps 580(575.20) | Grad Norm 0.6228(0.7582) | Total Time 14.00(14.00)\n",
      "Iter 2782 | Time 59.1765(57.3752) | Bit/dim 3.6432(3.6488) | Xent 0.8316(0.8272) | Loss 4.0590(4.0624) | Error 0.2964(0.2968) Steps 574(575.16) | Grad Norm 1.3728(0.7766) | Total Time 14.00(14.00)\n",
      "Iter 2783 | Time 58.5949(57.4118) | Bit/dim 3.6482(3.6488) | Xent 0.8336(0.8274) | Loss 4.0650(4.0625) | Error 0.3033(0.2970) Steps 574(575.13) | Grad Norm 1.3612(0.7942) | Total Time 14.00(14.00)\n",
      "Iter 2784 | Time 57.5037(57.4145) | Bit/dim 3.6594(3.6491) | Xent 0.8230(0.8273) | Loss 4.0709(4.0627) | Error 0.2919(0.2968) Steps 574(575.09) | Grad Norm 0.6428(0.7896) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0464 | Time 22.6181, Epoch Time 384.5839(379.3022), Bit/dim 3.6508(best: 3.6497), Xent 0.8522, Loss 4.0769, Error 0.2990(best: 0.2988)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2785 | Time 59.1298(57.4660) | Bit/dim 3.6571(3.6493) | Xent 0.8205(0.8271) | Loss 4.0673(4.0628) | Error 0.2957(0.2968) Steps 568(574.88) | Grad Norm 0.5519(0.7825) | Total Time 14.00(14.00)\n",
      "Iter 2786 | Time 52.7760(57.3253) | Bit/dim 3.6457(3.6492) | Xent 0.8360(0.8273) | Loss 4.0638(4.0629) | Error 0.2991(0.2968) Steps 568(574.68) | Grad Norm 1.0324(0.7900) | Total Time 14.00(14.00)\n",
      "Iter 2787 | Time 58.6699(57.3656) | Bit/dim 3.6514(3.6493) | Xent 0.8265(0.8273) | Loss 4.0647(4.0629) | Error 0.2999(0.2969) Steps 568(574.48) | Grad Norm 1.5025(0.8114) | Total Time 14.00(14.00)\n",
      "Iter 2788 | Time 54.5709(57.2818) | Bit/dim 3.6459(3.6492) | Xent 0.8230(0.8272) | Loss 4.0574(4.0628) | Error 0.3005(0.2970) Steps 580(574.64) | Grad Norm 0.6094(0.8053) | Total Time 14.00(14.00)\n",
      "Iter 2789 | Time 56.5120(57.2587) | Bit/dim 3.6425(3.6490) | Xent 0.8328(0.8273) | Loss 4.0589(4.0626) | Error 0.3027(0.2972) Steps 574(574.62) | Grad Norm 0.8229(0.8058) | Total Time 14.00(14.00)\n",
      "Iter 2790 | Time 54.9122(57.1883) | Bit/dim 3.6525(3.6491) | Xent 0.8089(0.8268) | Loss 4.0569(4.0625) | Error 0.2875(0.2969) Steps 568(574.42) | Grad Norm 1.0005(0.8117) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0465 | Time 22.1021, Epoch Time 374.6826(379.1637), Bit/dim 3.6504(best: 3.6497), Xent 0.8529, Loss 4.0768, Error 0.3020(best: 0.2988)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2791 | Time 55.4085(57.1349) | Bit/dim 3.6438(3.6489) | Xent 0.8101(0.8263) | Loss 4.0489(4.0621) | Error 0.2881(0.2967) Steps 580(574.59) | Grad Norm 1.1497(0.8218) | Total Time 14.00(14.00)\n",
      "Iter 2792 | Time 58.8139(57.1853) | Bit/dim 3.6564(3.6491) | Xent 0.8240(0.8262) | Loss 4.0684(4.0623) | Error 0.2927(0.2965) Steps 580(574.75) | Grad Norm 0.6973(0.8181) | Total Time 14.00(14.00)\n",
      "Iter 2793 | Time 58.2274(57.2165) | Bit/dim 3.6527(3.6493) | Xent 0.8212(0.8261) | Loss 4.0633(4.0623) | Error 0.2983(0.2966) Steps 568(574.55) | Grad Norm 0.4625(0.8074) | Total Time 14.00(14.00)\n",
      "Iter 2794 | Time 57.0678(57.2121) | Bit/dim 3.6514(3.6493) | Xent 0.8377(0.8264) | Loss 4.0703(4.0625) | Error 0.3066(0.2969) Steps 568(574.35) | Grad Norm 1.1251(0.8169) | Total Time 14.00(14.00)\n",
      "Iter 2795 | Time 57.4823(57.2202) | Bit/dim 3.6464(3.6492) | Xent 0.8272(0.8264) | Loss 4.0600(4.0625) | Error 0.2966(0.2969) Steps 568(574.16) | Grad Norm 1.0771(0.8248) | Total Time 14.00(14.00)\n",
      "Iter 2796 | Time 58.4221(57.2562) | Bit/dim 3.6442(3.6491) | Xent 0.8227(0.8263) | Loss 4.0555(4.0622) | Error 0.2993(0.2970) Steps 574(574.16) | Grad Norm 1.0297(0.8309) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0466 | Time 22.4168, Epoch Time 383.5965(379.2966), Bit/dim 3.6506(best: 3.6497), Xent 0.8518, Loss 4.0765, Error 0.3018(best: 0.2988)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2797 | Time 55.5927(57.2063) | Bit/dim 3.6418(3.6489) | Xent 0.8171(0.8260) | Loss 4.0503(4.0619) | Error 0.2917(0.2968) Steps 568(573.97) | Grad Norm 0.8725(0.8322) | Total Time 14.00(14.00)\n",
      "Iter 2798 | Time 57.5454(57.2165) | Bit/dim 3.6622(3.6493) | Xent 0.8404(0.8265) | Loss 4.0824(4.0625) | Error 0.2939(0.2967) Steps 580(574.15) | Grad Norm 0.6157(0.8257) | Total Time 14.00(14.00)\n",
      "Iter 2799 | Time 58.2924(57.2488) | Bit/dim 3.6523(3.6494) | Xent 0.8282(0.8265) | Loss 4.0663(4.0626) | Error 0.2991(0.2968) Steps 568(573.97) | Grad Norm 0.8338(0.8259) | Total Time 14.00(14.00)\n",
      "Iter 2800 | Time 58.5097(57.2866) | Bit/dim 3.6472(3.6493) | Xent 0.8350(0.8268) | Loss 4.0648(4.0627) | Error 0.2980(0.2968) Steps 568(573.79) | Grad Norm 1.3911(0.8429) | Total Time 14.00(14.00)\n",
      "Iter 2801 | Time 56.4033(57.2601) | Bit/dim 3.6466(3.6492) | Xent 0.8197(0.8266) | Loss 4.0564(4.0625) | Error 0.2939(0.2967) Steps 574(573.80) | Grad Norm 1.0442(0.8489) | Total Time 14.00(14.00)\n",
      "Iter 2802 | Time 57.9920(57.2821) | Bit/dim 3.6416(3.6490) | Xent 0.8048(0.8259) | Loss 4.0440(4.0619) | Error 0.2863(0.2964) Steps 574(573.80) | Grad Norm 0.7219(0.8451) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0467 | Time 22.2019, Epoch Time 382.2540(379.3854), Bit/dim 3.6504(best: 3.6497), Xent 0.8516, Loss 4.0762, Error 0.3032(best: 0.2988)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2803 | Time 54.0560(57.1853) | Bit/dim 3.6514(3.6491) | Xent 0.8106(0.8255) | Loss 4.0567(4.0618) | Error 0.2893(0.2962) Steps 574(573.81) | Grad Norm 1.8063(0.8739) | Total Time 14.00(14.00)\n",
      "Iter 2804 | Time 59.0744(57.2420) | Bit/dim 3.6527(3.6492) | Xent 0.8363(0.8258) | Loss 4.0708(4.0621) | Error 0.2991(0.2963) Steps 574(573.81) | Grad Norm 0.9272(0.8755) | Total Time 14.00(14.00)\n",
      "Iter 2805 | Time 56.8493(57.2302) | Bit/dim 3.6493(3.6492) | Xent 0.8187(0.8256) | Loss 4.0587(4.0620) | Error 0.2917(0.2962) Steps 568(573.64) | Grad Norm 1.0826(0.8817) | Total Time 14.00(14.00)\n",
      "Iter 2806 | Time 57.7044(57.2444) | Bit/dim 3.6400(3.6489) | Xent 0.8155(0.8253) | Loss 4.0478(4.0615) | Error 0.2885(0.2959) Steps 568(573.47) | Grad Norm 1.2334(0.8923) | Total Time 14.00(14.00)\n",
      "Iter 2807 | Time 55.8993(57.2041) | Bit/dim 3.6430(3.6487) | Xent 0.8252(0.8253) | Loss 4.0556(4.0613) | Error 0.2957(0.2959) Steps 568(573.31) | Grad Norm 1.4366(0.9086) | Total Time 14.00(14.00)\n",
      "Iter 2808 | Time 57.5602(57.2147) | Bit/dim 3.6606(3.6491) | Xent 0.8352(0.8256) | Loss 4.0782(4.0619) | Error 0.2984(0.2960) Steps 580(573.51) | Grad Norm 1.4634(0.9253) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0468 | Time 22.5516, Epoch Time 379.4249(379.3866), Bit/dim 3.6502(best: 3.6497), Xent 0.8506, Loss 4.0755, Error 0.2990(best: 0.2988)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2809 | Time 57.7727(57.2315) | Bit/dim 3.6545(3.6492) | Xent 0.8151(0.8252) | Loss 4.0621(4.0619) | Error 0.2897(0.2958) Steps 568(573.34) | Grad Norm 0.5466(0.9139) | Total Time 14.00(14.00)\n",
      "Iter 2810 | Time 55.7773(57.1878) | Bit/dim 3.6432(3.6491) | Xent 0.8287(0.8254) | Loss 4.0575(4.0617) | Error 0.3016(0.2960) Steps 568(573.18) | Grad Norm 0.9563(0.9152) | Total Time 14.00(14.00)\n",
      "Iter 2811 | Time 57.3510(57.1927) | Bit/dim 3.6469(3.6490) | Xent 0.8507(0.8261) | Loss 4.0722(4.0620) | Error 0.3024(0.2962) Steps 580(573.39) | Grad Norm 1.2221(0.9244) | Total Time 14.00(14.00)\n",
      "Iter 2812 | Time 53.3917(57.0787) | Bit/dim 3.6520(3.6491) | Xent 0.8555(0.8270) | Loss 4.0797(4.0626) | Error 0.3074(0.2965) Steps 568(573.22) | Grad Norm 0.9982(0.9266) | Total Time 14.00(14.00)\n",
      "Iter 2813 | Time 56.8226(57.0710) | Bit/dim 3.6414(3.6488) | Xent 0.8214(0.8268) | Loss 4.0521(4.0623) | Error 0.3037(0.2967) Steps 568(573.07) | Grad Norm 0.5696(0.9159) | Total Time 14.00(14.00)\n",
      "Iter 2814 | Time 56.4944(57.0537) | Bit/dim 3.6536(3.6490) | Xent 0.8115(0.8264) | Loss 4.0594(4.0622) | Error 0.2933(0.2966) Steps 574(573.10) | Grad Norm 0.6976(0.9093) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0469 | Time 22.2679, Epoch Time 375.5504(379.2715), Bit/dim 3.6506(best: 3.6497), Xent 0.8518, Loss 4.0765, Error 0.3002(best: 0.2988)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2815 | Time 58.6239(57.1008) | Bit/dim 3.6542(3.6491) | Xent 0.8173(0.8261) | Loss 4.0629(4.0622) | Error 0.2944(0.2966) Steps 592(573.66) | Grad Norm 1.4428(0.9253) | Total Time 14.00(14.00)\n",
      "Iter 2816 | Time 55.5880(57.0554) | Bit/dim 3.6348(3.6487) | Xent 0.8139(0.8257) | Loss 4.0417(4.0616) | Error 0.2879(0.2963) Steps 568(573.49) | Grad Norm 1.2144(0.9340) | Total Time 14.00(14.00)\n",
      "Iter 2817 | Time 56.2367(57.0309) | Bit/dim 3.6511(3.6488) | Xent 0.8201(0.8256) | Loss 4.0611(4.0616) | Error 0.2966(0.2963) Steps 580(573.69) | Grad Norm 0.4513(0.9195) | Total Time 14.00(14.00)\n",
      "Iter 2818 | Time 56.8852(57.0265) | Bit/dim 3.6548(3.6490) | Xent 0.8162(0.8253) | Loss 4.0629(4.0616) | Error 0.2895(0.2961) Steps 568(573.52) | Grad Norm 0.9742(0.9212) | Total Time 14.00(14.00)\n",
      "Iter 2819 | Time 55.8024(56.9898) | Bit/dim 3.6541(3.6491) | Xent 0.8360(0.8256) | Loss 4.0721(4.0619) | Error 0.2990(0.2962) Steps 580(573.71) | Grad Norm 0.9681(0.9226) | Total Time 14.00(14.00)\n",
      "Iter 2820 | Time 57.3647(57.0010) | Bit/dim 3.6475(3.6491) | Xent 0.8281(0.8257) | Loss 4.0615(4.0619) | Error 0.2987(0.2963) Steps 592(574.26) | Grad Norm 0.8075(0.9191) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0470 | Time 22.4444, Epoch Time 378.5409(379.2496), Bit/dim 3.6502(best: 3.6497), Xent 0.8495, Loss 4.0750, Error 0.2976(best: 0.2988)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2821 | Time 57.8731(57.0272) | Bit/dim 3.6522(3.6492) | Xent 0.8262(0.8257) | Loss 4.0653(4.0620) | Error 0.2936(0.2962) Steps 574(574.25) | Grad Norm 0.3945(0.9034) | Total Time 14.00(14.00)\n",
      "Iter 2822 | Time 57.4006(57.0384) | Bit/dim 3.6599(3.6495) | Xent 0.8099(0.8252) | Loss 4.0649(4.0621) | Error 0.2964(0.2962) Steps 580(574.43) | Grad Norm 0.7717(0.8994) | Total Time 14.00(14.00)\n",
      "Iter 2823 | Time 56.8547(57.0329) | Bit/dim 3.6502(3.6495) | Xent 0.8434(0.8258) | Loss 4.0719(4.0624) | Error 0.2961(0.2962) Steps 568(574.23) | Grad Norm 0.7249(0.8942) | Total Time 14.00(14.00)\n",
      "Iter 2824 | Time 60.7429(57.1442) | Bit/dim 3.6395(3.6492) | Xent 0.8426(0.8263) | Loss 4.0608(4.0623) | Error 0.3035(0.2964) Steps 586(574.59) | Grad Norm 0.6203(0.8860) | Total Time 14.00(14.00)\n",
      "Iter 2825 | Time 59.4963(57.2148) | Bit/dim 3.6545(3.6494) | Xent 0.8140(0.8259) | Loss 4.0614(4.0623) | Error 0.2900(0.2962) Steps 580(574.75) | Grad Norm 0.4728(0.8736) | Total Time 14.00(14.00)\n",
      "Iter 2826 | Time 55.2270(57.1551) | Bit/dim 3.6375(3.6490) | Xent 0.8222(0.8258) | Loss 4.0486(4.0619) | Error 0.2967(0.2962) Steps 586(575.09) | Grad Norm 0.7388(0.8695) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0471 | Time 22.4659, Epoch Time 385.4340(379.4351), Bit/dim 3.6503(best: 3.6497), Xent 0.8512, Loss 4.0759, Error 0.3003(best: 0.2976)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2827 | Time 56.7444(57.1428) | Bit/dim 3.6525(3.6491) | Xent 0.8256(0.8258) | Loss 4.0653(4.0620) | Error 0.2893(0.2960) Steps 568(574.87) | Grad Norm 0.4600(0.8573) | Total Time 14.00(14.00)\n",
      "Iter 2828 | Time 55.3203(57.0881) | Bit/dim 3.6524(3.6492) | Xent 0.8300(0.8259) | Loss 4.0674(4.0622) | Error 0.2970(0.2961) Steps 568(574.67) | Grad Norm 0.5958(0.8494) | Total Time 14.00(14.00)\n",
      "Iter 2829 | Time 59.0916(57.1482) | Bit/dim 3.6469(3.6491) | Xent 0.8277(0.8260) | Loss 4.0608(4.0621) | Error 0.2909(0.2959) Steps 580(574.83) | Grad Norm 0.3710(0.8351) | Total Time 14.00(14.00)\n",
      "Iter 2830 | Time 57.3895(57.1555) | Bit/dim 3.6416(3.6489) | Xent 0.8391(0.8264) | Loss 4.0611(4.0621) | Error 0.3017(0.2961) Steps 580(574.98) | Grad Norm 0.6351(0.8291) | Total Time 14.00(14.00)\n",
      "Iter 2831 | Time 55.6264(57.1096) | Bit/dim 3.6504(3.6490) | Xent 0.8181(0.8261) | Loss 4.0595(4.0620) | Error 0.2917(0.2959) Steps 568(574.77) | Grad Norm 0.5202(0.8198) | Total Time 14.00(14.00)\n",
      "Iter 2832 | Time 57.3151(57.1158) | Bit/dim 3.6489(3.6490) | Xent 0.8045(0.8255) | Loss 4.0512(4.0617) | Error 0.2931(0.2959) Steps 574(574.75) | Grad Norm 0.8902(0.8219) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0472 | Time 22.2740, Epoch Time 379.4472(379.4355), Bit/dim 3.6501(best: 3.6497), Xent 0.8504, Loss 4.0753, Error 0.2985(best: 0.2976)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2833 | Time 57.4538(57.1259) | Bit/dim 3.6476(3.6489) | Xent 0.8285(0.8256) | Loss 4.0618(4.0617) | Error 0.2971(0.2959) Steps 568(574.55) | Grad Norm 0.4351(0.8103) | Total Time 14.00(14.00)\n",
      "Iter 2834 | Time 55.4292(57.0750) | Bit/dim 3.6482(3.6489) | Xent 0.8272(0.8256) | Loss 4.0618(4.0617) | Error 0.2977(0.2960) Steps 574(574.53) | Grad Norm 0.4801(0.8004) | Total Time 14.00(14.00)\n",
      "Iter 2835 | Time 55.7711(57.0359) | Bit/dim 3.6485(3.6489) | Xent 0.8201(0.8254) | Loss 4.0585(4.0616) | Error 0.2944(0.2959) Steps 568(574.33) | Grad Norm 0.8036(0.8005) | Total Time 14.00(14.00)\n",
      "Iter 2836 | Time 57.4671(57.0488) | Bit/dim 3.6464(3.6488) | Xent 0.8060(0.8249) | Loss 4.0495(4.0612) | Error 0.2874(0.2956) Steps 568(574.14) | Grad Norm 0.5022(0.7915) | Total Time 14.00(14.00)\n",
      "Iter 2837 | Time 57.2972(57.0563) | Bit/dim 3.6510(3.6489) | Xent 0.8214(0.8248) | Loss 4.0617(4.0613) | Error 0.2971(0.2957) Steps 568(573.96) | Grad Norm 0.7376(0.7899) | Total Time 14.00(14.00)\n",
      "Iter 2838 | Time 55.9119(57.0219) | Bit/dim 3.6462(3.6488) | Xent 0.8149(0.8245) | Loss 4.0537(4.0610) | Error 0.2947(0.2957) Steps 568(573.78) | Grad Norm 0.5373(0.7823) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0473 | Time 22.1437, Epoch Time 377.2801(379.3708), Bit/dim 3.6504(best: 3.6497), Xent 0.8521, Loss 4.0765, Error 0.3014(best: 0.2976)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2839 | Time 57.8755(57.0475) | Bit/dim 3.6523(3.6489) | Xent 0.8259(0.8245) | Loss 4.0652(4.0612) | Error 0.2987(0.2958) Steps 568(573.61) | Grad Norm 0.9013(0.7859) | Total Time 14.00(14.00)\n",
      "Iter 2840 | Time 56.9776(57.0454) | Bit/dim 3.6374(3.6486) | Xent 0.8398(0.8250) | Loss 4.0572(4.0610) | Error 0.3076(0.2961) Steps 568(573.44) | Grad Norm 0.5612(0.7792) | Total Time 14.00(14.00)\n",
      "Iter 2841 | Time 57.7156(57.0656) | Bit/dim 3.6561(3.6488) | Xent 0.8120(0.8246) | Loss 4.0621(4.0611) | Error 0.2920(0.2960) Steps 580(573.64) | Grad Norm 0.5865(0.7734) | Total Time 14.00(14.00)\n",
      "Iter 2842 | Time 56.4785(57.0479) | Bit/dim 3.6409(3.6485) | Xent 0.8300(0.8247) | Loss 4.0559(4.0609) | Error 0.2991(0.2961) Steps 574(573.65) | Grad Norm 0.5624(0.7671) | Total Time 14.00(14.00)\n",
      "Iter 2843 | Time 55.9541(57.0151) | Bit/dim 3.6602(3.6489) | Xent 0.8257(0.8248) | Loss 4.0730(4.0613) | Error 0.2891(0.2959) Steps 568(573.48) | Grad Norm 1.3242(0.7838) | Total Time 14.00(14.00)\n",
      "Iter 2844 | Time 58.5749(57.0619) | Bit/dim 3.6476(3.6489) | Xent 0.8277(0.8248) | Loss 4.0615(4.0613) | Error 0.2987(0.2960) Steps 568(573.31) | Grad Norm 0.8089(0.7845) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0474 | Time 22.4742, Epoch Time 381.7446(379.4420), Bit/dim 3.6491(best: 3.6497), Xent 0.8511, Loss 4.0747, Error 0.2970(best: 0.2976)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2845 | Time 56.5911(57.0478) | Bit/dim 3.6411(3.6486) | Xent 0.8201(0.8247) | Loss 4.0512(4.0610) | Error 0.2964(0.2960) Steps 568(573.15) | Grad Norm 1.0333(0.7920) | Total Time 14.00(14.00)\n",
      "Iter 2846 | Time 53.7953(56.9502) | Bit/dim 3.6566(3.6489) | Xent 0.8249(0.8247) | Loss 4.0691(4.0612) | Error 0.2955(0.2960) Steps 574(573.18) | Grad Norm 0.5649(0.7852) | Total Time 14.00(14.00)\n",
      "Iter 2847 | Time 58.5275(56.9975) | Bit/dim 3.6614(3.6492) | Xent 0.8181(0.8245) | Loss 4.0705(4.0615) | Error 0.2889(0.2957) Steps 586(573.56) | Grad Norm 0.6885(0.7823) | Total Time 14.00(14.00)\n",
      "Iter 2848 | Time 56.8936(56.9944) | Bit/dim 3.6435(3.6491) | Xent 0.8354(0.8248) | Loss 4.0612(4.0615) | Error 0.3013(0.2959) Steps 568(573.40) | Grad Norm 1.0350(0.7899) | Total Time 14.00(14.00)\n",
      "Iter 2849 | Time 59.1030(57.0577) | Bit/dim 3.6502(3.6491) | Xent 0.8238(0.8248) | Loss 4.0621(4.0615) | Error 0.2917(0.2958) Steps 568(573.24) | Grad Norm 0.8152(0.7906) | Total Time 14.00(14.00)\n",
      "Iter 2850 | Time 58.3375(57.0961) | Bit/dim 3.6426(3.6489) | Xent 0.8234(0.8248) | Loss 4.0543(4.0613) | Error 0.2994(0.2959) Steps 574(573.26) | Grad Norm 0.9554(0.7956) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0475 | Time 22.1655, Epoch Time 380.7932(379.4825), Bit/dim 3.6503(best: 3.6491), Xent 0.8516, Loss 4.0761, Error 0.2980(best: 0.2970)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2851 | Time 55.6615(57.0530) | Bit/dim 3.6496(3.6489) | Xent 0.8174(0.8245) | Loss 4.0582(4.0612) | Error 0.2973(0.2959) Steps 568(573.10) | Grad Norm 0.7856(0.7953) | Total Time 14.00(14.00)\n",
      "Iter 2852 | Time 57.2737(57.0597) | Bit/dim 3.6469(3.6489) | Xent 0.8126(0.8242) | Loss 4.0532(4.0610) | Error 0.2921(0.2958) Steps 580(573.31) | Grad Norm 1.4592(0.8152) | Total Time 14.00(14.00)\n",
      "Iter 2853 | Time 54.0400(56.9691) | Bit/dim 3.6444(3.6487) | Xent 0.8269(0.8243) | Loss 4.0579(4.0609) | Error 0.2965(0.2958) Steps 574(573.33) | Grad Norm 0.6613(0.8106) | Total Time 14.00(14.00)\n",
      "Iter 2854 | Time 57.7481(56.9924) | Bit/dim 3.6433(3.6486) | Xent 0.8344(0.8246) | Loss 4.0605(4.0609) | Error 0.3041(0.2961) Steps 574(573.35) | Grad Norm 0.8746(0.8125) | Total Time 14.00(14.00)\n",
      "Iter 2855 | Time 57.5317(57.0086) | Bit/dim 3.6482(3.6486) | Xent 0.8275(0.8247) | Loss 4.0620(4.0609) | Error 0.3007(0.2962) Steps 580(573.55) | Grad Norm 0.7870(0.8117) | Total Time 14.00(14.00)\n",
      "Iter 2856 | Time 56.0821(56.9808) | Bit/dim 3.6587(3.6489) | Xent 0.8262(0.8247) | Loss 4.0718(4.0612) | Error 0.2937(0.2962) Steps 568(573.38) | Grad Norm 0.5235(0.8031) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0476 | Time 22.5025, Epoch Time 376.6002(379.3961), Bit/dim 3.6503(best: 3.6491), Xent 0.8496, Loss 4.0751, Error 0.2987(best: 0.2970)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2857 | Time 56.2559(56.9591) | Bit/dim 3.6477(3.6488) | Xent 0.8258(0.8247) | Loss 4.0606(4.0612) | Error 0.2953(0.2961) Steps 580(573.58) | Grad Norm 0.5958(0.7969) | Total Time 14.00(14.00)\n",
      "Iter 2858 | Time 56.6484(56.9498) | Bit/dim 3.6568(3.6491) | Xent 0.8166(0.8245) | Loss 4.0651(4.0613) | Error 0.2927(0.2960) Steps 574(573.59) | Grad Norm 1.0022(0.8030) | Total Time 14.00(14.00)\n",
      "Iter 2859 | Time 55.3959(56.9031) | Bit/dim 3.6469(3.6490) | Xent 0.8345(0.8248) | Loss 4.0641(4.0614) | Error 0.3031(0.2962) Steps 568(573.43) | Grad Norm 0.6718(0.7991) | Total Time 14.00(14.00)\n",
      "Iter 2860 | Time 55.6613(56.8659) | Bit/dim 3.6374(3.6487) | Xent 0.8414(0.8253) | Loss 4.0581(4.0613) | Error 0.2971(0.2963) Steps 586(573.80) | Grad Norm 0.8496(0.8006) | Total Time 14.00(14.00)\n",
      "Iter 2861 | Time 59.1409(56.9341) | Bit/dim 3.6537(3.6488) | Xent 0.8232(0.8252) | Loss 4.0653(4.0614) | Error 0.2950(0.2962) Steps 586(574.17) | Grad Norm 0.9151(0.8040) | Total Time 14.00(14.00)\n",
      "Iter 2862 | Time 56.7887(56.9298) | Bit/dim 3.6478(3.6488) | Xent 0.8205(0.8251) | Loss 4.0581(4.0613) | Error 0.2949(0.2962) Steps 568(573.98) | Grad Norm 0.7733(0.8031) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0477 | Time 22.6442, Epoch Time 378.3698(379.3653), Bit/dim 3.6491(best: 3.6491), Xent 0.8486, Loss 4.0734, Error 0.2990(best: 0.2970)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2863 | Time 56.6242(56.9206) | Bit/dim 3.6381(3.6485) | Xent 0.8303(0.8252) | Loss 4.0533(4.0611) | Error 0.2939(0.2961) Steps 568(573.80) | Grad Norm 0.8077(0.8032) | Total Time 14.00(14.00)\n",
      "Iter 2864 | Time 58.0692(56.9551) | Bit/dim 3.6526(3.6486) | Xent 0.8215(0.8251) | Loss 4.0634(4.0611) | Error 0.2917(0.2960) Steps 574(573.81) | Grad Norm 0.9234(0.8068) | Total Time 14.00(14.00)\n",
      "Iter 2865 | Time 58.2128(56.9928) | Bit/dim 3.6578(3.6489) | Xent 0.8150(0.8248) | Loss 4.0653(4.0613) | Error 0.2939(0.2959) Steps 580(574.00) | Grad Norm 0.5313(0.7986) | Total Time 14.00(14.00)\n",
      "Iter 2866 | Time 55.6615(56.9529) | Bit/dim 3.6410(3.6486) | Xent 0.8310(0.8250) | Loss 4.0565(4.0611) | Error 0.2981(0.2960) Steps 568(573.82) | Grad Norm 0.7174(0.7961) | Total Time 14.00(14.00)\n",
      "Iter 2867 | Time 55.1968(56.9002) | Bit/dim 3.6434(3.6485) | Xent 0.8194(0.8248) | Loss 4.0531(4.0609) | Error 0.2915(0.2959) Steps 586(574.18) | Grad Norm 1.1045(0.8054) | Total Time 14.00(14.00)\n",
      "Iter 2868 | Time 56.5975(56.8911) | Bit/dim 3.6560(3.6487) | Xent 0.8486(0.8256) | Loss 4.0803(4.0615) | Error 0.3021(0.2960) Steps 574(574.18) | Grad Norm 0.5216(0.7969) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0478 | Time 22.5846, Epoch Time 378.9402(379.3525), Bit/dim 3.6499(best: 3.6491), Xent 0.8500, Loss 4.0749, Error 0.3003(best: 0.2970)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2869 | Time 56.5962(56.8822) | Bit/dim 3.6429(3.6485) | Xent 0.8398(0.8260) | Loss 4.0628(4.0615) | Error 0.2999(0.2962) Steps 568(573.99) | Grad Norm 1.2159(0.8095) | Total Time 14.00(14.00)\n",
      "Iter 2870 | Time 55.5390(56.8420) | Bit/dim 3.6501(3.6486) | Xent 0.8163(0.8257) | Loss 4.0583(4.0614) | Error 0.2935(0.2961) Steps 574(573.99) | Grad Norm 1.1469(0.8196) | Total Time 14.00(14.00)\n",
      "Iter 2871 | Time 59.7664(56.9297) | Bit/dim 3.6360(3.6482) | Xent 0.8219(0.8256) | Loss 4.0470(4.0610) | Error 0.2946(0.2960) Steps 580(574.17) | Grad Norm 0.5133(0.8104) | Total Time 14.00(14.00)\n",
      "Iter 2872 | Time 59.7747(57.0150) | Bit/dim 3.6448(3.6481) | Xent 0.8277(0.8256) | Loss 4.0587(4.0609) | Error 0.2976(0.2961) Steps 586(574.53) | Grad Norm 1.0025(0.8162) | Total Time 14.00(14.00)\n",
      "Iter 2873 | Time 58.1177(57.0481) | Bit/dim 3.6586(3.6484) | Xent 0.8167(0.8254) | Loss 4.0670(4.0611) | Error 0.2941(0.2960) Steps 580(574.69) | Grad Norm 0.6262(0.8105) | Total Time 14.00(14.00)\n",
      "Iter 2874 | Time 56.7563(57.0394) | Bit/dim 3.6585(3.6487) | Xent 0.8301(0.8255) | Loss 4.0735(4.0615) | Error 0.2936(0.2960) Steps 586(575.03) | Grad Norm 1.0209(0.8168) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0479 | Time 22.2004, Epoch Time 384.8594(379.5177), Bit/dim 3.6503(best: 3.6491), Xent 0.8519, Loss 4.0763, Error 0.3034(best: 0.2970)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2875 | Time 57.5604(57.0550) | Bit/dim 3.6499(3.6487) | Xent 0.8264(0.8255) | Loss 4.0631(4.0615) | Error 0.2981(0.2960) Steps 568(574.82) | Grad Norm 0.5228(0.8079) | Total Time 14.00(14.00)\n",
      "Iter 2876 | Time 57.9636(57.0822) | Bit/dim 3.6441(3.6486) | Xent 0.8202(0.8254) | Loss 4.0542(4.0613) | Error 0.2927(0.2959) Steps 580(574.97) | Grad Norm 1.0089(0.8140) | Total Time 14.00(14.00)\n",
      "Iter 2877 | Time 55.9367(57.0479) | Bit/dim 3.6472(3.6486) | Xent 0.8316(0.8256) | Loss 4.0630(4.0613) | Error 0.2966(0.2959) Steps 574(574.94) | Grad Norm 0.9088(0.8168) | Total Time 14.00(14.00)\n",
      "Iter 2878 | Time 55.2847(56.9950) | Bit/dim 3.6610(3.6489) | Xent 0.8295(0.8257) | Loss 4.0758(4.0618) | Error 0.2949(0.2959) Steps 568(574.74) | Grad Norm 0.7709(0.8154) | Total Time 14.00(14.00)\n",
      "Iter 2879 | Time 58.0740(57.0274) | Bit/dim 3.6468(3.6489) | Xent 0.8161(0.8254) | Loss 4.0549(4.0616) | Error 0.2940(0.2959) Steps 568(574.53) | Grad Norm 0.9951(0.8208) | Total Time 14.00(14.00)\n",
      "Iter 2880 | Time 57.7088(57.0478) | Bit/dim 3.6455(3.6488) | Xent 0.8185(0.8252) | Loss 4.0547(4.0614) | Error 0.2903(0.2957) Steps 568(574.34) | Grad Norm 1.2501(0.8337) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0480 | Time 22.3071, Epoch Time 380.4707(379.5463), Bit/dim 3.6519(best: 3.6491), Xent 0.8502, Loss 4.0770, Error 0.2971(best: 0.2970)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2881 | Time 56.5442(57.0327) | Bit/dim 3.6540(3.6489) | Xent 0.8036(0.8245) | Loss 4.0558(4.0612) | Error 0.2850(0.2954) Steps 574(574.33) | Grad Norm 0.6908(0.8294) | Total Time 14.00(14.00)\n",
      "Iter 2882 | Time 56.7598(57.0245) | Bit/dim 3.6600(3.6493) | Xent 0.8319(0.8248) | Loss 4.0760(4.0616) | Error 0.2981(0.2954) Steps 586(574.68) | Grad Norm 0.4485(0.8180) | Total Time 14.00(14.00)\n",
      "Iter 2883 | Time 58.9300(57.0817) | Bit/dim 3.6341(3.6488) | Xent 0.8204(0.8246) | Loss 4.0443(4.0611) | Error 0.2951(0.2954) Steps 574(574.66) | Grad Norm 0.8824(0.8199) | Total Time 14.00(14.00)\n",
      "Iter 2884 | Time 56.4313(57.0622) | Bit/dim 3.6487(3.6488) | Xent 0.8255(0.8247) | Loss 4.0615(4.0611) | Error 0.2947(0.2954) Steps 574(574.64) | Grad Norm 0.6371(0.8144) | Total Time 14.00(14.00)\n",
      "Iter 2885 | Time 57.2980(57.0692) | Bit/dim 3.6445(3.6487) | Xent 0.8230(0.8246) | Loss 4.0560(4.0610) | Error 0.2929(0.2953) Steps 574(574.62) | Grad Norm 1.3377(0.8301) | Total Time 14.00(14.00)\n",
      "Iter 2886 | Time 58.0023(57.0972) | Bit/dim 3.6479(3.6486) | Xent 0.8281(0.8247) | Loss 4.0620(4.0610) | Error 0.2969(0.2954) Steps 568(574.42) | Grad Norm 0.5259(0.8210) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0481 | Time 22.3799, Epoch Time 381.8716(379.6161), Bit/dim 3.6505(best: 3.6491), Xent 0.8516, Loss 4.0763, Error 0.3016(best: 0.2970)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2887 | Time 54.5730(57.0215) | Bit/dim 3.6480(3.6486) | Xent 0.8328(0.8250) | Loss 4.0644(4.0611) | Error 0.2989(0.2955) Steps 580(574.59) | Grad Norm 0.6755(0.8167) | Total Time 14.00(14.00)\n",
      "Iter 2888 | Time 59.4332(57.0939) | Bit/dim 3.6544(3.6488) | Xent 0.8097(0.8245) | Loss 4.0593(4.0611) | Error 0.2957(0.2955) Steps 592(575.11) | Grad Norm 0.5648(0.8091) | Total Time 14.00(14.00)\n",
      "Iter 2889 | Time 57.6273(57.1099) | Bit/dim 3.6410(3.6486) | Xent 0.8292(0.8246) | Loss 4.0556(4.0609) | Error 0.3016(0.2957) Steps 568(574.90) | Grad Norm 0.6618(0.8047) | Total Time 14.00(14.00)\n",
      "Iter 2890 | Time 56.1432(57.0809) | Bit/dim 3.6543(3.6487) | Xent 0.8085(0.8242) | Loss 4.0585(4.0608) | Error 0.2871(0.2954) Steps 568(574.69) | Grad Norm 0.7827(0.8040) | Total Time 14.00(14.00)\n",
      "Iter 2891 | Time 59.9296(57.1663) | Bit/dim 3.6487(3.6487) | Xent 0.8387(0.8246) | Loss 4.0681(4.0610) | Error 0.3041(0.2957) Steps 568(574.49) | Grad Norm 0.4547(0.7935) | Total Time 14.00(14.00)\n",
      "Iter 2892 | Time 57.8640(57.1872) | Bit/dim 3.6456(3.6486) | Xent 0.8239(0.8246) | Loss 4.0576(4.0609) | Error 0.2955(0.2957) Steps 574(574.47) | Grad Norm 1.1167(0.8032) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0482 | Time 22.2882, Epoch Time 383.2526(379.7252), Bit/dim 3.6502(best: 3.6491), Xent 0.8490, Loss 4.0747, Error 0.3015(best: 0.2970)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2893 | Time 57.2318(57.1886) | Bit/dim 3.6485(3.6486) | Xent 0.8448(0.8252) | Loss 4.0709(4.0612) | Error 0.3046(0.2959) Steps 574(574.46) | Grad Norm 0.5160(0.7946) | Total Time 14.00(14.00)\n",
      "Iter 2894 | Time 60.0156(57.2734) | Bit/dim 3.6541(3.6488) | Xent 0.8169(0.8249) | Loss 4.0625(4.0613) | Error 0.2876(0.2957) Steps 592(574.99) | Grad Norm 0.8258(0.7955) | Total Time 14.00(14.00)\n",
      "Iter 2895 | Time 55.2166(57.2117) | Bit/dim 3.6481(3.6488) | Xent 0.8239(0.8249) | Loss 4.0601(4.0612) | Error 0.2934(0.2956) Steps 574(574.96) | Grad Norm 0.7241(0.7934) | Total Time 14.00(14.00)\n",
      "Iter 2896 | Time 57.8009(57.2294) | Bit/dim 3.6445(3.6487) | Xent 0.8131(0.8245) | Loss 4.0510(4.0609) | Error 0.2899(0.2955) Steps 568(574.75) | Grad Norm 0.6664(0.7896) | Total Time 14.00(14.00)\n",
      "Iter 2897 | Time 56.5059(57.2077) | Bit/dim 3.6423(3.6485) | Xent 0.8118(0.8242) | Loss 4.0482(4.0605) | Error 0.2901(0.2953) Steps 568(574.55) | Grad Norm 0.8177(0.7904) | Total Time 14.00(14.00)\n",
      "Iter 2898 | Time 53.8649(57.1074) | Bit/dim 3.6554(3.6487) | Xent 0.8125(0.8238) | Loss 4.0616(4.0606) | Error 0.2887(0.2951) Steps 574(574.53) | Grad Norm 0.7859(0.7903) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0483 | Time 22.3856, Epoch Time 378.6675(379.6934), Bit/dim 3.6500(best: 3.6491), Xent 0.8510, Loss 4.0755, Error 0.3017(best: 0.2970)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2899 | Time 57.7347(57.1262) | Bit/dim 3.6528(3.6488) | Xent 0.8318(0.8241) | Loss 4.0687(4.0608) | Error 0.2943(0.2951) Steps 568(574.33) | Grad Norm 0.8058(0.7908) | Total Time 14.00(14.00)\n",
      "Iter 2900 | Time 59.3191(57.1920) | Bit/dim 3.6428(3.6486) | Xent 0.8140(0.8237) | Loss 4.0497(4.0605) | Error 0.2899(0.2949) Steps 568(574.14) | Grad Norm 0.8522(0.7926) | Total Time 14.00(14.00)\n",
      "Iter 2901 | Time 55.1557(57.1309) | Bit/dim 3.6379(3.6483) | Xent 0.8317(0.8240) | Loss 4.0538(4.0603) | Error 0.2933(0.2949) Steps 568(573.96) | Grad Norm 0.6091(0.7871) | Total Time 14.00(14.00)\n",
      "Iter 2902 | Time 57.1848(57.1325) | Bit/dim 3.6482(3.6483) | Xent 0.8223(0.8239) | Loss 4.0593(4.0603) | Error 0.2961(0.2949) Steps 592(574.50) | Grad Norm 0.6816(0.7839) | Total Time 14.00(14.00)\n",
      "Iter 2903 | Time 55.7417(57.0908) | Bit/dim 3.6581(3.6486) | Xent 0.8090(0.8235) | Loss 4.0626(4.0603) | Error 0.2870(0.2947) Steps 568(574.31) | Grad Norm 1.0672(0.7924) | Total Time 14.00(14.00)\n",
      "Iter 2904 | Time 59.0131(57.1485) | Bit/dim 3.6523(3.6487) | Xent 0.8333(0.8238) | Loss 4.0690(4.0606) | Error 0.2993(0.2948) Steps 574(574.30) | Grad Norm 1.1645(0.8036) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0484 | Time 22.3495, Epoch Time 381.8184(379.7572), Bit/dim 3.6487(best: 3.6491), Xent 0.8508, Loss 4.0741, Error 0.2999(best: 0.2970)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2905 | Time 56.4293(57.1269) | Bit/dim 3.6530(3.6488) | Xent 0.8377(0.8242) | Loss 4.0718(4.0609) | Error 0.3003(0.2950) Steps 580(574.47) | Grad Norm 0.6247(0.7982) | Total Time 14.00(14.00)\n",
      "Iter 2906 | Time 58.7771(57.1764) | Bit/dim 3.6418(3.6486) | Xent 0.8252(0.8242) | Loss 4.0544(4.0607) | Error 0.2969(0.2950) Steps 586(574.81) | Grad Norm 0.7817(0.7977) | Total Time 14.00(14.00)\n",
      "Iter 2907 | Time 56.4939(57.1559) | Bit/dim 3.6498(3.6486) | Xent 0.8216(0.8242) | Loss 4.0606(4.0607) | Error 0.2957(0.2950) Steps 568(574.61) | Grad Norm 0.6958(0.7947) | Total Time 14.00(14.00)\n",
      "Iter 2908 | Time 56.9460(57.1496) | Bit/dim 3.6492(3.6487) | Xent 0.8159(0.8239) | Loss 4.0571(4.0606) | Error 0.2883(0.2948) Steps 568(574.41) | Grad Norm 0.7870(0.7944) | Total Time 14.00(14.00)\n",
      "Iter 2909 | Time 57.4711(57.1593) | Bit/dim 3.6492(3.6487) | Xent 0.8125(0.8236) | Loss 4.0555(4.0605) | Error 0.2946(0.2948) Steps 574(574.40) | Grad Norm 0.6282(0.7895) | Total Time 14.00(14.00)\n",
      "Iter 2910 | Time 57.9016(57.1815) | Bit/dim 3.6445(3.6486) | Xent 0.8168(0.8234) | Loss 4.0529(4.0602) | Error 0.2891(0.2947) Steps 586(574.75) | Grad Norm 1.1182(0.7993) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0485 | Time 22.2344, Epoch Time 382.0429(379.8258), Bit/dim 3.6488(best: 3.6487), Xent 0.8513, Loss 4.0744, Error 0.3008(best: 0.2970)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2911 | Time 56.8196(57.1707) | Bit/dim 3.6491(3.6486) | Xent 0.8276(0.8235) | Loss 4.0629(4.0603) | Error 0.3029(0.2949) Steps 574(574.72) | Grad Norm 0.6429(0.7946) | Total Time 14.00(14.00)\n",
      "Iter 2912 | Time 56.9947(57.1654) | Bit/dim 3.6359(3.6482) | Xent 0.8255(0.8235) | Loss 4.0486(4.0600) | Error 0.3021(0.2951) Steps 574(574.70) | Grad Norm 0.8990(0.7978) | Total Time 14.00(14.00)\n",
      "Iter 2913 | Time 57.4096(57.1727) | Bit/dim 3.6557(3.6484) | Xent 0.8272(0.8237) | Loss 4.0693(4.0602) | Error 0.2946(0.2951) Steps 568(574.50) | Grad Norm 1.0470(0.8052) | Total Time 14.00(14.00)\n",
      "Iter 2914 | Time 59.1584(57.2323) | Bit/dim 3.6516(3.6485) | Xent 0.8145(0.8234) | Loss 4.0588(4.0602) | Error 0.2907(0.2950) Steps 568(574.31) | Grad Norm 0.7125(0.8025) | Total Time 14.00(14.00)\n",
      "Iter 2915 | Time 59.7797(57.3087) | Bit/dim 3.6498(3.6485) | Xent 0.8341(0.8237) | Loss 4.0668(4.0604) | Error 0.3016(0.2952) Steps 568(574.12) | Grad Norm 0.6758(0.7987) | Total Time 14.00(14.00)\n",
      "Iter 2916 | Time 56.3264(57.2792) | Bit/dim 3.6437(3.6484) | Xent 0.8215(0.8236) | Loss 4.0544(4.0602) | Error 0.2981(0.2953) Steps 574(574.11) | Grad Norm 0.8385(0.7998) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0486 | Time 22.4640, Epoch Time 385.4651(379.9949), Bit/dim 3.6497(best: 3.6487), Xent 0.8522, Loss 4.0757, Error 0.3007(best: 0.2970)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2917 | Time 56.4085(57.2531) | Bit/dim 3.6411(3.6482) | Xent 0.8216(0.8236) | Loss 4.0519(4.0600) | Error 0.2961(0.2953) Steps 568(573.93) | Grad Norm 0.7152(0.7973) | Total Time 14.00(14.00)\n",
      "Iter 2918 | Time 55.5359(57.2016) | Bit/dim 3.6539(3.6484) | Xent 0.8236(0.8236) | Loss 4.0657(4.0601) | Error 0.2901(0.2951) Steps 574(573.93) | Grad Norm 0.6605(0.7932) | Total Time 14.00(14.00)\n",
      "Iter 2919 | Time 57.9677(57.2246) | Bit/dim 3.6474(3.6483) | Xent 0.8049(0.8230) | Loss 4.0498(4.0598) | Error 0.2916(0.2950) Steps 586(574.29) | Grad Norm 0.6108(0.7877) | Total Time 14.00(14.00)\n",
      "Iter 2920 | Time 56.6251(57.2066) | Bit/dim 3.6475(3.6483) | Xent 0.8211(0.8230) | Loss 4.0581(4.0598) | Error 0.2956(0.2951) Steps 586(574.65) | Grad Norm 0.9397(0.7923) | Total Time 14.00(14.00)\n",
      "Iter 2921 | Time 59.1277(57.2642) | Bit/dim 3.6424(3.6481) | Xent 0.8216(0.8229) | Loss 4.0532(4.0596) | Error 0.2937(0.2950) Steps 580(574.81) | Grad Norm 0.5984(0.7865) | Total Time 14.00(14.00)\n",
      "Iter 2922 | Time 56.9409(57.2545) | Bit/dim 3.6584(3.6484) | Xent 0.8121(0.8226) | Loss 4.0645(4.0597) | Error 0.2941(0.2950) Steps 592(575.32) | Grad Norm 0.4989(0.7779) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0487 | Time 22.7546, Epoch Time 381.1750(380.0303), Bit/dim 3.6502(best: 3.6487), Xent 0.8482, Loss 4.0743, Error 0.3035(best: 0.2970)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2923 | Time 58.2202(57.2835) | Bit/dim 3.6476(3.6484) | Xent 0.8153(0.8224) | Loss 4.0553(4.0596) | Error 0.2940(0.2950) Steps 568(575.10) | Grad Norm 0.5277(0.7703) | Total Time 14.00(14.00)\n",
      "Iter 2924 | Time 56.1966(57.2509) | Bit/dim 3.6457(3.6483) | Xent 0.8278(0.8225) | Loss 4.0595(4.0596) | Error 0.2953(0.2950) Steps 574(575.07) | Grad Norm 0.4961(0.7621) | Total Time 14.00(14.00)\n",
      "Iter 2925 | Time 57.6138(57.2618) | Bit/dim 3.6499(3.6484) | Xent 0.8164(0.8224) | Loss 4.0581(4.0595) | Error 0.2950(0.2950) Steps 574(575.04) | Grad Norm 0.7207(0.7609) | Total Time 14.00(14.00)\n",
      "Iter 2926 | Time 54.8200(57.1885) | Bit/dim 3.6483(3.6484) | Xent 0.8222(0.8224) | Loss 4.0594(4.0595) | Error 0.2921(0.2949) Steps 568(574.83) | Grad Norm 0.4774(0.7524) | Total Time 14.00(14.00)\n",
      "Iter 2927 | Time 58.8319(57.2378) | Bit/dim 3.6448(3.6483) | Xent 0.8266(0.8225) | Loss 4.0582(4.0595) | Error 0.2957(0.2949) Steps 586(575.16) | Grad Norm 0.9617(0.7587) | Total Time 14.00(14.00)\n",
      "Iter 2928 | Time 55.3038(57.1798) | Bit/dim 3.6515(3.6484) | Xent 0.8151(0.8223) | Loss 4.0590(4.0595) | Error 0.2880(0.2947) Steps 586(575.49) | Grad Norm 0.7935(0.7597) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0488 | Time 21.9586, Epoch Time 378.4050(379.9816), Bit/dim 3.6493(best: 3.6487), Xent 0.8479, Loss 4.0733, Error 0.3015(best: 0.2970)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2929 | Time 57.5578(57.1911) | Bit/dim 3.6407(3.6481) | Xent 0.8170(0.8221) | Loss 4.0493(4.0592) | Error 0.2883(0.2945) Steps 568(575.26) | Grad Norm 0.6172(0.7554) | Total Time 14.00(14.00)\n",
      "Iter 2930 | Time 59.1841(57.2509) | Bit/dim 3.6424(3.6480) | Xent 0.8218(0.8221) | Loss 4.0533(4.0590) | Error 0.2933(0.2945) Steps 580(575.40) | Grad Norm 0.4882(0.7474) | Total Time 14.00(14.00)\n",
      "Iter 2931 | Time 57.1974(57.2493) | Bit/dim 3.6590(3.6483) | Xent 0.8246(0.8222) | Loss 4.0713(4.0594) | Error 0.2906(0.2944) Steps 568(575.18) | Grad Norm 0.9687(0.7540) | Total Time 14.00(14.00)\n",
      "Iter 2932 | Time 58.4712(57.2860) | Bit/dim 3.6556(3.6485) | Xent 0.8229(0.8222) | Loss 4.0671(4.0596) | Error 0.2929(0.2943) Steps 580(575.33) | Grad Norm 0.9210(0.7591) | Total Time 14.00(14.00)\n",
      "Iter 2933 | Time 55.5140(57.2328) | Bit/dim 3.6337(3.6481) | Xent 0.8327(0.8225) | Loss 4.0501(4.0593) | Error 0.3034(0.2946) Steps 568(575.11) | Grad Norm 0.5988(0.7542) | Total Time 14.00(14.00)\n",
      "Iter 2934 | Time 56.7399(57.2180) | Bit/dim 3.6535(3.6482) | Xent 0.8150(0.8223) | Loss 4.0610(4.0594) | Error 0.2896(0.2944) Steps 574(575.07) | Grad Norm 0.7930(0.7554) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0489 | Time 22.3182, Epoch Time 382.8011(380.0662), Bit/dim 3.6498(best: 3.6487), Xent 0.8507, Loss 4.0752, Error 0.2995(best: 0.2970)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2935 | Time 55.1969(57.1574) | Bit/dim 3.6389(3.6480) | Xent 0.8218(0.8223) | Loss 4.0498(4.0591) | Error 0.2956(0.2945) Steps 580(575.22) | Grad Norm 0.9499(0.7612) | Total Time 14.00(14.00)\n",
      "Iter 2936 | Time 58.0262(57.1835) | Bit/dim 3.6441(3.6478) | Xent 0.8191(0.8222) | Loss 4.0537(4.0589) | Error 0.2940(0.2945) Steps 580(575.36) | Grad Norm 1.5441(0.7847) | Total Time 14.00(14.00)\n",
      "Iter 2937 | Time 58.5600(57.2248) | Bit/dim 3.6492(3.6479) | Xent 0.8247(0.8222) | Loss 4.0615(4.0590) | Error 0.2910(0.2944) Steps 586(575.68) | Grad Norm 0.5248(0.7769) | Total Time 14.00(14.00)\n",
      "Iter 2952 | Time 58.4345(57.3824) | Bit/dim 3.6427(3.6481) | Xent 0.8081(0.8228) | Loss 4.0468(4.0595) | Error 0.2916(0.2948) Steps 586(577.10) | Grad Norm 1.0765(0.8716) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0492 | Time 22.5619, Epoch Time 388.6934(380.3658), Bit/dim 3.6494(best: 3.6487), Xent 0.8488, Loss 4.0738, Error 0.3000(best: 0.2970)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2953 | Time 57.4884(57.3855) | Bit/dim 3.6401(3.6479) | Xent 0.8093(0.8224) | Loss 4.0447(4.0591) | Error 0.2885(0.2946) Steps 574(577.00) | Grad Norm 1.2128(0.8818) | Total Time 14.00(14.00)\n",
      "Iter 2954 | Time 55.6214(57.3326) | Bit/dim 3.6482(3.6479) | Xent 0.8245(0.8224) | Loss 4.0605(4.0591) | Error 0.2937(0.2946) Steps 580(577.09) | Grad Norm 1.2888(0.8940) | Total Time 14.00(14.00)\n",
      "Iter 2955 | Time 58.2198(57.3592) | Bit/dim 3.6585(3.6482) | Xent 0.8110(0.8221) | Loss 4.0640(4.0593) | Error 0.2961(0.2947) Steps 592(577.54) | Grad Norm 1.0624(0.8991) | Total Time 14.00(14.00)\n",
      "Iter 2956 | Time 59.8463(57.4338) | Bit/dim 3.6527(3.6484) | Xent 0.8319(0.8224) | Loss 4.0686(4.0596) | Error 0.2969(0.2947) Steps 580(577.61) | Grad Norm 0.4733(0.8863) | Total Time 14.00(14.00)\n",
      "Iter 2957 | Time 57.3898(57.4325) | Bit/dim 3.6432(3.6482) | Xent 0.8257(0.8225) | Loss 4.0560(4.0594) | Error 0.2947(0.2947) Steps 568(577.32) | Grad Norm 0.6898(0.8804) | Total Time 14.00(14.00)\n",
      "Iter 2958 | Time 56.7624(57.4124) | Bit/dim 3.6453(3.6481) | Xent 0.8392(0.8230) | Loss 4.0649(4.0596) | Error 0.3019(0.2949) Steps 586(577.59) | Grad Norm 1.3936(0.8958) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0493 | Time 22.3807, Epoch Time 383.0482(380.4462), Bit/dim 3.6488(best: 3.6487), Xent 0.8511, Loss 4.0744, Error 0.3006(best: 0.2970)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2959 | Time 57.4276(57.4129) | Bit/dim 3.6376(3.6478) | Xent 0.8173(0.8228) | Loss 4.0462(4.0592) | Error 0.2939(0.2949) Steps 574(577.48) | Grad Norm 0.6651(0.8889) | Total Time 14.00(14.00)\n",
      "Iter 2960 | Time 57.3656(57.4115) | Bit/dim 3.6491(3.6478) | Xent 0.8251(0.8229) | Loss 4.0617(4.0593) | Error 0.2920(0.2948) Steps 574(577.37) | Grad Norm 0.7095(0.8835) | Total Time 14.00(14.00)\n",
      "Iter 2961 | Time 52.1124(57.2525) | Bit/dim 3.6349(3.6475) | Xent 0.8034(0.8223) | Loss 4.0366(4.0586) | Error 0.2877(0.2946) Steps 574(577.27) | Grad Norm 1.1043(0.8901) | Total Time 14.00(14.00)\n",
      "Iter 2962 | Time 56.5881(57.2325) | Bit/dim 3.6465(3.6474) | Xent 0.8452(0.8230) | Loss 4.0692(4.0589) | Error 0.3045(0.2949) Steps 574(577.17) | Grad Norm 0.9675(0.8925) | Total Time 14.00(14.00)\n",
      "Iter 2963 | Time 56.0263(57.1964) | Bit/dim 3.6625(3.6479) | Xent 0.8210(0.8229) | Loss 4.0729(4.0593) | Error 0.2909(0.2948) Steps 568(576.90) | Grad Norm 0.8910(0.8924) | Total Time 14.00(14.00)\n",
      "Iter 2964 | Time 55.7381(57.1526) | Bit/dim 3.6501(3.6480) | Xent 0.8189(0.8228) | Loss 4.0595(4.0593) | Error 0.2919(0.2947) Steps 574(576.81) | Grad Norm 1.1081(0.8989) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0494 | Time 22.2318, Epoch Time 373.0138(380.2233), Bit/dim 3.6490(best: 3.6487), Xent 0.8500, Loss 4.0740, Error 0.3011(best: 0.2970)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2965 | Time 58.2512(57.1856) | Bit/dim 3.6512(3.6480) | Xent 0.8124(0.8225) | Loss 4.0573(4.0593) | Error 0.2853(0.2944) Steps 574(576.73) | Grad Norm 0.9530(0.9005) | Total Time 14.00(14.00)\n",
      "Iter 2966 | Time 57.1836(57.1855) | Bit/dim 3.6408(3.6478) | Xent 0.8296(0.8227) | Loss 4.0556(4.0592) | Error 0.2964(0.2945) Steps 574(576.65) | Grad Norm 1.0649(0.9054) | Total Time 14.00(14.00)\n",
      "Iter 2967 | Time 54.5467(57.1063) | Bit/dim 3.6465(3.6478) | Xent 0.8198(0.8226) | Loss 4.0565(4.0591) | Error 0.2933(0.2944) Steps 562(576.21) | Grad Norm 0.9676(0.9073) | Total Time 14.00(14.00)\n",
      "Iter 2968 | Time 59.6948(57.1840) | Bit/dim 3.6536(3.6480) | Xent 0.8286(0.8228) | Loss 4.0679(4.0594) | Error 0.3003(0.2946) Steps 568(575.96) | Grad Norm 0.9806(0.9095) | Total Time 14.00(14.00)\n",
      "Iter 2969 | Time 57.3848(57.1900) | Bit/dim 3.6473(3.6479) | Xent 0.8194(0.8227) | Loss 4.0570(4.0593) | Error 0.2956(0.2946) Steps 580(576.08) | Grad Norm 0.5801(0.8996) | Total Time 14.00(14.00)\n",
      "Iter 2970 | Time 58.1667(57.2193) | Bit/dim 3.6483(3.6480) | Xent 0.8111(0.8223) | Loss 4.0538(4.0591) | Error 0.2926(0.2946) Steps 586(576.38) | Grad Norm 0.7510(0.8952) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0495 | Time 22.3215, Epoch Time 382.9324(380.3045), Bit/dim 3.6483(best: 3.6487), Xent 0.8503, Loss 4.0734, Error 0.3003(best: 0.2970)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2971 | Time 57.6284(57.2316) | Bit/dim 3.6480(3.6480) | Xent 0.8350(0.8227) | Loss 4.0655(4.0593) | Error 0.3046(0.2949) Steps 586(576.67) | Grad Norm 0.9104(0.8956) | Total Time 14.00(14.00)\n",
      "Iter 2972 | Time 55.0688(57.1667) | Bit/dim 3.6493(3.6480) | Xent 0.8211(0.8227) | Loss 4.0599(4.0593) | Error 0.2947(0.2949) Steps 574(576.59) | Grad Norm 0.5332(0.8847) | Total Time 14.00(14.00)\n",
      "Iter 2973 | Time 56.8903(57.1584) | Bit/dim 3.6480(3.6480) | Xent 0.8041(0.8221) | Loss 4.0500(4.0590) | Error 0.2859(0.2946) Steps 580(576.69) | Grad Norm 0.5625(0.8751) | Total Time 14.00(14.00)\n",
      "Iter 2974 | Time 57.9567(57.1824) | Bit/dim 3.6403(3.6478) | Xent 0.8275(0.8223) | Loss 4.0540(4.0589) | Error 0.2974(0.2947) Steps 580(576.79) | Grad Norm 1.0652(0.8808) | Total Time 14.00(14.00)\n",
      "Iter 2975 | Time 57.9651(57.2059) | Bit/dim 3.6558(3.6480) | Xent 0.8308(0.8225) | Loss 4.0712(4.0593) | Error 0.2957(0.2947) Steps 574(576.71) | Grad Norm 0.8210(0.8790) | Total Time 14.00(14.00)\n",
      "Iter 2976 | Time 56.9611(57.1985) | Bit/dim 3.6430(3.6479) | Xent 0.8193(0.8224) | Loss 4.0527(4.0591) | Error 0.2929(0.2947) Steps 568(576.44) | Grad Norm 0.9458(0.8810) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0496 | Time 22.5685, Epoch Time 380.9350(380.3234), Bit/dim 3.6501(best: 3.6483), Xent 0.8481, Loss 4.0742, Error 0.3010(best: 0.2970)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2977 | Time 55.2117(57.1389) | Bit/dim 3.6600(3.6482) | Xent 0.8245(0.8225) | Loss 4.0723(4.0595) | Error 0.3039(0.2949) Steps 574(576.37) | Grad Norm 0.5882(0.8722) | Total Time 14.00(14.00)\n",
      "Iter 2978 | Time 60.0344(57.2258) | Bit/dim 3.6421(3.6480) | Xent 0.8145(0.8223) | Loss 4.0494(4.0592) | Error 0.2934(0.2949) Steps 592(576.84) | Grad Norm 1.0020(0.8761) | Total Time 14.00(14.00)\n",
      "Iter 2979 | Time 56.2080(57.1952) | Bit/dim 3.6390(3.6478) | Xent 0.8125(0.8220) | Loss 4.0452(4.0587) | Error 0.2935(0.2949) Steps 568(576.57) | Grad Norm 0.9442(0.8781) | Total Time 14.00(14.00)\n",
      "Iter 2980 | Time 58.3907(57.2311) | Bit/dim 3.6489(3.6478) | Xent 0.8183(0.8219) | Loss 4.0580(4.0587) | Error 0.2939(0.2948) Steps 574(576.50) | Grad Norm 0.8240(0.8765) | Total Time 14.00(14.00)\n",
      "Iter 2981 | Time 55.1608(57.1690) | Bit/dim 3.6497(3.6479) | Xent 0.8139(0.8216) | Loss 4.0566(4.0587) | Error 0.2910(0.2947) Steps 574(576.42) | Grad Norm 0.9291(0.8781) | Total Time 14.00(14.00)\n",
      "Iter 2982 | Time 57.9478(57.1924) | Bit/dim 3.6520(3.6480) | Xent 0.8194(0.8215) | Loss 4.0617(4.0588) | Error 0.2980(0.2948) Steps 574(576.35) | Grad Norm 0.7739(0.8750) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0497 | Time 22.4919, Epoch Time 381.2224(380.3504), Bit/dim 3.6494(best: 3.6483), Xent 0.8505, Loss 4.0746, Error 0.3005(best: 0.2970)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2983 | Time 57.3353(57.1966) | Bit/dim 3.6514(3.6481) | Xent 0.8324(0.8219) | Loss 4.0675(4.0590) | Error 0.3016(0.2950) Steps 580(576.46) | Grad Norm 0.9166(0.8762) | Total Time 14.00(14.00)\n",
      "Iter 2984 | Time 58.4610(57.2346) | Bit/dim 3.6559(3.6483) | Xent 0.8335(0.8222) | Loss 4.0726(4.0594) | Error 0.2967(0.2951) Steps 574(576.39) | Grad Norm 1.5187(0.8955) | Total Time 14.00(14.00)\n",
      "Iter 2985 | Time 56.7484(57.2200) | Bit/dim 3.6385(3.6480) | Xent 0.8084(0.8218) | Loss 4.0427(4.0589) | Error 0.2941(0.2950) Steps 574(576.31) | Grad Norm 0.8281(0.8935) | Total Time 14.00(14.00)\n",
      "Iter 2986 | Time 57.5553(57.2301) | Bit/dim 3.6474(3.6480) | Xent 0.8412(0.8224) | Loss 4.0680(4.0592) | Error 0.3021(0.2952) Steps 574(576.24) | Grad Norm 0.5723(0.8838) | Total Time 14.00(14.00)\n",
      "Iter 2987 | Time 57.9372(57.2513) | Bit/dim 3.6484(3.6480) | Xent 0.8152(0.8222) | Loss 4.0561(4.0591) | Error 0.2927(0.2952) Steps 574(576.18) | Grad Norm 1.0837(0.8898) | Total Time 14.00(14.00)\n",
      "Iter 2988 | Time 59.2087(57.3100) | Bit/dim 3.6457(3.6479) | Xent 0.8154(0.8220) | Loss 4.0534(4.0589) | Error 0.2956(0.2952) Steps 568(575.93) | Grad Norm 0.8187(0.8877) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0498 | Time 22.5218, Epoch Time 385.9391(380.5181), Bit/dim 3.6497(best: 3.6483), Xent 0.8483, Loss 4.0738, Error 0.3008(best: 0.2970)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2989 | Time 57.4627(57.3146) | Bit/dim 3.6417(3.6478) | Xent 0.8104(0.8216) | Loss 4.0468(4.0586) | Error 0.2943(0.2952) Steps 574(575.87) | Grad Norm 0.9045(0.8882) | Total Time 14.00(14.00)\n",
      "Iter 2990 | Time 61.7188(57.4467) | Bit/dim 3.6431(3.6476) | Xent 0.8213(0.8216) | Loss 4.0538(4.0584) | Error 0.2931(0.2951) Steps 586(576.18) | Grad Norm 1.0064(0.8917) | Total Time 14.00(14.00)\n",
      "Iter 2991 | Time 57.2788(57.4417) | Bit/dim 3.6618(3.6480) | Xent 0.8156(0.8214) | Loss 4.0695(4.0588) | Error 0.2943(0.2951) Steps 580(576.29) | Grad Norm 0.9066(0.8922) | Total Time 14.00(14.00)\n",
      "Iter 2992 | Time 56.4112(57.4107) | Bit/dim 3.6482(3.6480) | Xent 0.8510(0.8223) | Loss 4.0737(4.0592) | Error 0.3100(0.2955) Steps 568(576.04) | Grad Norm 0.9354(0.8935) | Total Time 14.00(14.00)\n",
      "Iter 2993 | Time 57.5820(57.4159) | Bit/dim 3.6406(3.6478) | Xent 0.7978(0.8216) | Loss 4.0395(4.0586) | Error 0.2857(0.2952) Steps 580(576.16) | Grad Norm 0.8828(0.8932) | Total Time 14.00(14.00)\n",
      "Iter 2994 | Time 56.7861(57.3970) | Bit/dim 3.6511(3.6479) | Xent 0.8372(0.8221) | Loss 4.0697(4.0589) | Error 0.2973(0.2953) Steps 568(575.92) | Grad Norm 0.7565(0.8891) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0499 | Time 22.7157, Epoch Time 385.8971(380.6794), Bit/dim 3.6495(best: 3.6483), Xent 0.8491, Loss 4.0740, Error 0.3021(best: 0.2970)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2995 | Time 59.2324(57.4521) | Bit/dim 3.6492(3.6480) | Xent 0.8221(0.8221) | Loss 4.0602(4.0590) | Error 0.2870(0.2950) Steps 568(575.68) | Grad Norm 1.2256(0.8992) | Total Time 14.00(14.00)\n",
      "Iter 2996 | Time 56.1815(57.4139) | Bit/dim 3.6476(3.6479) | Xent 0.8056(0.8216) | Loss 4.0503(4.0587) | Error 0.2887(0.2949) Steps 574(575.63) | Grad Norm 0.9886(0.9018) | Total Time 14.00(14.00)\n",
      "Iter 2997 | Time 56.5898(57.3892) | Bit/dim 3.6399(3.6477) | Xent 0.8133(0.8213) | Loss 4.0466(4.0584) | Error 0.2911(0.2947) Steps 580(575.76) | Grad Norm 0.7368(0.8969) | Total Time 14.00(14.00)\n",
      "Iter 2998 | Time 58.3951(57.4194) | Bit/dim 3.6493(3.6478) | Xent 0.8192(0.8212) | Loss 4.0589(4.0584) | Error 0.3006(0.2949) Steps 568(575.53) | Grad Norm 1.3070(0.9092) | Total Time 14.00(14.00)\n",
      "Iter 2999 | Time 56.1250(57.3806) | Bit/dim 3.6533(3.6479) | Xent 0.8392(0.8218) | Loss 4.0729(4.0588) | Error 0.2981(0.2950) Steps 568(575.30) | Grad Norm 0.9805(0.9113) | Total Time 14.00(14.00)\n",
      "Iter 3000 | Time 57.9014(57.3962) | Bit/dim 3.6456(3.6478) | Xent 0.8246(0.8219) | Loss 4.0579(4.0588) | Error 0.2974(0.2951) Steps 568(575.08) | Grad Norm 0.6359(0.9031) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0500 | Time 22.4215, Epoch Time 382.6474(380.7385), Bit/dim 3.6490(best: 3.6483), Xent 0.8503, Loss 4.0741, Error 0.3029(best: 0.2970)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3001 | Time 59.4077(57.4565) | Bit/dim 3.6454(3.6478) | Xent 0.8296(0.8221) | Loss 4.0602(4.0588) | Error 0.2939(0.2950) Steps 592(575.59) | Grad Norm 1.1441(0.9103) | Total Time 14.00(14.00)\n",
      "Iter 3002 | Time 54.5246(57.3686) | Bit/dim 3.6446(3.6477) | Xent 0.8185(0.8220) | Loss 4.0538(4.0587) | Error 0.2913(0.2949) Steps 568(575.36) | Grad Norm 1.3660(0.9240) | Total Time 14.00(14.00)\n",
      "Iter 3003 | Time 54.9047(57.2947) | Bit/dim 3.6449(3.6476) | Xent 0.8240(0.8221) | Loss 4.0569(4.0586) | Error 0.2966(0.2950) Steps 586(575.68) | Grad Norm 1.0551(0.9279) | Total Time 14.00(14.00)\n",
      "Iter 3004 | Time 54.9424(57.2241) | Bit/dim 3.6642(3.6481) | Xent 0.8186(0.8219) | Loss 4.0735(4.0591) | Error 0.3006(0.2952) Steps 574(575.63) | Grad Norm 1.3659(0.9411) | Total Time 14.00(14.00)\n",
      "Iter 3005 | Time 60.1514(57.3119) | Bit/dim 3.6342(3.6477) | Xent 0.8184(0.8218) | Loss 4.0435(4.0586) | Error 0.2925(0.2951) Steps 586(575.94) | Grad Norm 1.6470(0.9622) | Total Time 14.00(14.00)\n",
      "Iter 3006 | Time 55.9172(57.2701) | Bit/dim 3.6477(3.6477) | Xent 0.8240(0.8219) | Loss 4.0597(4.0586) | Error 0.2939(0.2950) Steps 580(576.06) | Grad Norm 0.9940(0.9632) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0501 | Time 22.7255, Epoch Time 378.2625(380.6642), Bit/dim 3.6491(best: 3.6483), Xent 0.8498, Loss 4.0740, Error 0.2997(best: 0.2970)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3007 | Time 57.2763(57.2702) | Bit/dim 3.6522(3.6478) | Xent 0.8398(0.8224) | Loss 4.0721(4.0590) | Error 0.3019(0.2952) Steps 574(576.00) | Grad Norm 0.8874(0.9609) | Total Time 14.00(14.00)\n",
      "Iter 3008 | Time 54.9653(57.2011) | Bit/dim 3.6437(3.6477) | Xent 0.8145(0.8222) | Loss 4.0509(4.0588) | Error 0.2915(0.2951) Steps 580(576.12) | Grad Norm 0.9549(0.9607) | Total Time 14.00(14.00)\n",
      "Iter 3009 | Time 57.2472(57.2025) | Bit/dim 3.6462(3.6476) | Xent 0.7993(0.8215) | Loss 4.0458(4.0584) | Error 0.2857(0.2948) Steps 574(576.06) | Grad Norm 1.1795(0.9673) | Total Time 14.00(14.00)\n",
      "Iter 3010 | Time 54.8577(57.1321) | Bit/dim 3.6488(3.6477) | Xent 0.8132(0.8213) | Loss 4.0554(4.0583) | Error 0.2914(0.2947) Steps 580(576.18) | Grad Norm 1.0856(0.9708) | Total Time 14.00(14.00)\n",
      "Iter 3011 | Time 56.3515(57.1087) | Bit/dim 3.6447(3.6476) | Xent 0.8075(0.8209) | Loss 4.0484(4.0580) | Error 0.2867(0.2945) Steps 568(575.93) | Grad Norm 1.2382(0.9789) | Total Time 14.00(14.00)\n",
      "Iter 3012 | Time 57.4221(57.1181) | Bit/dim 3.6490(3.6476) | Xent 0.8344(0.8213) | Loss 4.0662(4.0583) | Error 0.3033(0.2948) Steps 574(575.87) | Grad Norm 1.5075(0.9947) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0502 | Time 22.4270, Epoch Time 376.2799(380.5327), Bit/dim 3.6494(best: 3.6483), Xent 0.8523, Loss 4.0755, Error 0.2990(best: 0.2970)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3013 | Time 57.2120(57.1209) | Bit/dim 3.6470(3.6476) | Xent 0.8384(0.8218) | Loss 4.0662(4.0585) | Error 0.3001(0.2949) Steps 574(575.82) | Grad Norm 1.1351(0.9989) | Total Time 14.00(14.00)\n",
      "Iter 3014 | Time 56.3640(57.0982) | Bit/dim 3.6564(3.6479) | Xent 0.8178(0.8217) | Loss 4.0653(4.0587) | Error 0.2924(0.2949) Steps 574(575.76) | Grad Norm 1.5897(1.0167) | Total Time 14.00(14.00)\n",
      "Iter 3015 | Time 58.7681(57.1483) | Bit/dim 3.6381(3.6476) | Xent 0.8338(0.8220) | Loss 4.0550(4.0586) | Error 0.2993(0.2950) Steps 568(575.53) | Grad Norm 1.4918(1.0309) | Total Time 14.00(14.00)\n",
      "Iter 3016 | Time 54.4631(57.0678) | Bit/dim 3.6469(3.6476) | Xent 0.7996(0.8213) | Loss 4.0467(4.0582) | Error 0.2856(0.2947) Steps 574(575.48) | Grad Norm 1.0128(1.0304) | Total Time 14.00(14.00)\n",
      "Iter 3017 | Time 57.7416(57.0880) | Bit/dim 3.6540(3.6478) | Xent 0.8330(0.8217) | Loss 4.0705(4.0586) | Error 0.3000(0.2949) Steps 574(575.44) | Grad Norm 1.0296(1.0303) | Total Time 14.00(14.00)\n",
      "Iter 3018 | Time 57.7436(57.1076) | Bit/dim 3.6391(3.6475) | Xent 0.8008(0.8211) | Loss 4.0395(4.0580) | Error 0.2867(0.2946) Steps 586(575.76) | Grad Norm 1.1100(1.0327) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0503 | Time 22.3262, Epoch Time 380.3040(380.5258), Bit/dim 3.6486(best: 3.6483), Xent 0.8482, Loss 4.0728, Error 0.2981(best: 0.2970)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3019 | Time 57.1356(57.1085) | Bit/dim 3.6504(3.6476) | Xent 0.8109(0.8208) | Loss 4.0559(4.0580) | Error 0.2906(0.2945) Steps 586(576.06) | Grad Norm 0.9793(1.0311) | Total Time 14.00(14.00)\n",
      "Iter 3020 | Time 57.3406(57.1155) | Bit/dim 3.6508(3.6477) | Xent 0.8251(0.8209) | Loss 4.0634(4.0581) | Error 0.2967(0.2946) Steps 574(576.00) | Grad Norm 1.1621(1.0351) | Total Time 14.00(14.00)\n",
      "Iter 3021 | Time 57.4285(57.1248) | Bit/dim 3.6484(3.6477) | Xent 0.8244(0.8210) | Loss 4.0606(4.0582) | Error 0.2953(0.2946) Steps 574(575.94) | Grad Norm 1.4156(1.0465) | Total Time 14.00(14.00)\n",
      "Iter 3022 | Time 54.2847(57.0396) | Bit/dim 3.6384(3.6474) | Xent 0.8248(0.8211) | Loss 4.0508(4.0580) | Error 0.2989(0.2947) Steps 580(576.06) | Grad Norm 1.5801(1.0625) | Total Time 14.00(14.00)\n",
      "Iter 3023 | Time 56.0494(57.0099) | Bit/dim 3.6446(3.6473) | Xent 0.8185(0.8210) | Loss 4.0539(4.0579) | Error 0.2916(0.2946) Steps 574(576.00) | Grad Norm 0.5513(1.0471) | Total Time 14.00(14.00)\n",
      "Iter 3024 | Time 55.0297(56.9505) | Bit/dim 3.6446(3.6473) | Xent 0.8114(0.8207) | Loss 4.0503(4.0576) | Error 0.2895(0.2945) Steps 568(575.76) | Grad Norm 0.9179(1.0433) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0504 | Time 22.5250, Epoch Time 376.7132(380.4114), Bit/dim 3.6498(best: 3.6483), Xent 0.8490, Loss 4.0743, Error 0.3003(best: 0.2970)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3025 | Time 58.9714(57.0112) | Bit/dim 3.6400(3.6470) | Xent 0.8154(0.8206) | Loss 4.0477(4.0573) | Error 0.2910(0.2944) Steps 592(576.25) | Grad Norm 1.2784(1.0503) | Total Time 14.00(14.00)\n",
      "Iter 3026 | Time 56.5137(56.9962) | Bit/dim 3.6448(3.6470) | Xent 0.8195(0.8206) | Loss 4.0545(4.0572) | Error 0.2911(0.2943) Steps 568(576.00) | Grad Norm 0.7076(1.0400) | Total Time 14.00(14.00)\n",
      "Iter 3027 | Time 57.4847(57.0109) | Bit/dim 3.6434(3.6469) | Xent 0.8040(0.8201) | Loss 4.0454(4.0569) | Error 0.2900(0.2941) Steps 580(576.12) | Grad Norm 0.8285(1.0337) | Total Time 14.00(14.00)\n",
      "Iter 3028 | Time 57.3849(57.0221) | Bit/dim 3.6526(3.6470) | Xent 0.8113(0.8198) | Loss 4.0583(4.0569) | Error 0.2934(0.2941) Steps 568(575.88) | Grad Norm 1.3562(1.0434) | Total Time 14.00(14.00)\n",
      "Iter 3029 | Time 57.3246(57.0312) | Bit/dim 3.6585(3.6474) | Xent 0.8305(0.8201) | Loss 4.0737(4.0574) | Error 0.3021(0.2944) Steps 574(575.82) | Grad Norm 1.4118(1.0544) | Total Time 14.00(14.00)\n",
      "Iter 3030 | Time 57.7739(57.0535) | Bit/dim 3.6424(3.6472) | Xent 0.8233(0.8202) | Loss 4.0540(4.0573) | Error 0.2981(0.2945) Steps 580(575.95) | Grad Norm 0.7736(1.0460) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0505 | Time 22.5732, Epoch Time 383.5697(380.5062), Bit/dim 3.6490(best: 3.6483), Xent 0.8498, Loss 4.0739, Error 0.2975(best: 0.2970)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3031 | Time 58.5253(57.0976) | Bit/dim 3.6401(3.6470) | Xent 0.8156(0.8201) | Loss 4.0479(4.0571) | Error 0.2945(0.2945) Steps 586(576.25) | Grad Norm 1.1860(1.0502) | Total Time 14.00(14.00)\n",
      "Iter 3032 | Time 58.1547(57.1293) | Bit/dim 3.6606(3.6474) | Xent 0.8083(0.8197) | Loss 4.0648(4.0573) | Error 0.2914(0.2944) Steps 592(576.72) | Grad Norm 1.3383(1.0588) | Total Time 14.00(14.00)\n",
      "Iter 3033 | Time 56.8333(57.1204) | Bit/dim 3.6518(3.6476) | Xent 0.8132(0.8195) | Loss 4.0584(4.0573) | Error 0.2903(0.2943) Steps 598(577.36) | Grad Norm 1.4125(1.0694) | Total Time 14.00(14.00)\n",
      "Iter 3034 | Time 56.9227(57.1145) | Bit/dim 3.6439(3.6474) | Xent 0.8264(0.8197) | Loss 4.0571(4.0573) | Error 0.2943(0.2943) Steps 592(577.80) | Grad Norm 0.9704(1.0665) | Total Time 14.00(14.00)\n",
      "Iter 3035 | Time 55.9517(57.0796) | Bit/dim 3.6480(3.6475) | Xent 0.7945(0.8190) | Loss 4.0452(4.0570) | Error 0.2845(0.2940) Steps 568(577.50) | Grad Norm 1.3295(1.0744) | Total Time 14.00(14.00)\n",
      "Iter 3036 | Time 56.8524(57.0728) | Bit/dim 3.6451(3.6474) | Xent 0.8290(0.8193) | Loss 4.0596(4.0570) | Error 0.2929(0.2939) Steps 574(577.40) | Grad Norm 0.9661(1.0711) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0506 | Time 22.0169, Epoch Time 380.6988(380.5120), Bit/dim 3.6486(best: 3.6483), Xent 0.8488, Loss 4.0730, Error 0.3025(best: 0.2970)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3037 | Time 56.7611(57.0635) | Bit/dim 3.6482(3.6474) | Xent 0.8156(0.8192) | Loss 4.0560(4.0570) | Error 0.2866(0.2937) Steps 568(577.12) | Grad Norm 0.9104(1.0663) | Total Time 14.00(14.00)\n",
      "Iter 3038 | Time 58.0898(57.0942) | Bit/dim 3.6513(3.6475) | Xent 0.8066(0.8188) | Loss 4.0546(4.0569) | Error 0.2933(0.2937) Steps 574(577.02) | Grad Norm 0.8876(1.0609) | Total Time 14.00(14.00)\n",
      "Iter 3039 | Time 55.4943(57.0462) | Bit/dim 3.6412(3.6473) | Xent 0.8266(0.8190) | Loss 4.0545(4.0569) | Error 0.2994(0.2939) Steps 574(576.93) | Grad Norm 1.7080(1.0803) | Total Time 14.00(14.00)\n",
      "Iter 3040 | Time 54.9215(56.9825) | Bit/dim 3.6350(3.6470) | Xent 0.8344(0.8195) | Loss 4.0521(4.0567) | Error 0.2969(0.2940) Steps 574(576.85) | Grad Norm 0.5860(1.0655) | Total Time 14.00(14.00)\n",
      "Iter 3041 | Time 58.4477(57.0265) | Bit/dim 3.6511(3.6471) | Xent 0.8237(0.8196) | Loss 4.0629(4.0569) | Error 0.2895(0.2938) Steps 592(577.30) | Grad Norm 1.1534(1.0681) | Total Time 14.00(14.00)\n",
      "Iter 3042 | Time 56.9385(57.0238) | Bit/dim 3.6531(3.6473) | Xent 0.8226(0.8197) | Loss 4.0644(4.0571) | Error 0.2919(0.2938) Steps 580(577.38) | Grad Norm 1.3458(1.0765) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0507 | Time 22.4440, Epoch Time 379.0288(380.4675), Bit/dim 3.6494(best: 3.6483), Xent 0.8491, Loss 4.0739, Error 0.2990(best: 0.2970)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3043 | Time 57.7629(57.0460) | Bit/dim 3.6504(3.6474) | Xent 0.8317(0.8201) | Loss 4.0662(4.0574) | Error 0.2957(0.2938) Steps 580(577.46) | Grad Norm 0.9894(1.0739) | Total Time 14.00(14.00)\n",
      "Iter 3044 | Time 58.4108(57.0869) | Bit/dim 3.6515(3.6475) | Xent 0.8228(0.8201) | Loss 4.0629(4.0576) | Error 0.2917(0.2938) Steps 580(577.54) | Grad Norm 0.7201(1.0633) | Total Time 14.00(14.00)\n",
      "Iter 3045 | Time 58.5759(57.1316) | Bit/dim 3.6462(3.6475) | Xent 0.8172(0.8201) | Loss 4.0548(4.0575) | Error 0.2911(0.2937) Steps 580(577.61) | Grad Norm 1.3994(1.0733) | Total Time 14.00(14.00)\n",
      "Iter 3046 | Time 57.5590(57.1444) | Bit/dim 3.6351(3.6471) | Xent 0.8155(0.8199) | Loss 4.0428(4.0570) | Error 0.2929(0.2937) Steps 568(577.32) | Grad Norm 1.6821(1.0916) | Total Time 14.00(14.00)\n",
      "Iter 3047 | Time 56.7798(57.1335) | Bit/dim 3.6491(3.6471) | Xent 0.8246(0.8201) | Loss 4.0614(4.0572) | Error 0.3005(0.2939) Steps 568(577.04) | Grad Norm 0.5690(1.0759) | Total Time 14.00(14.00)\n",
      "Iter 3048 | Time 56.0420(57.1007) | Bit/dim 3.6536(3.6473) | Xent 0.8171(0.8200) | Loss 4.0621(4.0573) | Error 0.2947(0.2939) Steps 574(576.95) | Grad Norm 1.0395(1.0748) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0508 | Time 22.2624, Epoch Time 383.0334(380.5444), Bit/dim 3.6481(best: 3.6483), Xent 0.8478, Loss 4.0720, Error 0.3002(best: 0.2970)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3049 | Time 57.8373(57.1228) | Bit/dim 3.6373(3.6470) | Xent 0.8306(0.8203) | Loss 4.0526(4.0572) | Error 0.2939(0.2939) Steps 574(576.86) | Grad Norm 1.1196(1.0762) | Total Time 14.00(14.00)\n",
      "Iter 3050 | Time 58.0596(57.1509) | Bit/dim 3.6388(3.6468) | Xent 0.8188(0.8202) | Loss 4.0482(4.0569) | Error 0.2894(0.2938) Steps 580(576.96) | Grad Norm 0.5703(1.0610) | Total Time 14.00(14.00)\n",
      "Iter 3051 | Time 58.0730(57.1786) | Bit/dim 3.6516(3.6469) | Xent 0.8146(0.8201) | Loss 4.0588(4.0570) | Error 0.2944(0.2938) Steps 586(577.23) | Grad Norm 0.7232(1.0509) | Total Time 14.00(14.00)\n",
      "Iter 3052 | Time 55.7921(57.1370) | Bit/dim 3.6531(3.6471) | Xent 0.8127(0.8199) | Loss 4.0595(4.0570) | Error 0.2914(0.2937) Steps 586(577.49) | Grad Norm 0.6709(1.0395) | Total Time 14.00(14.00)\n",
      "Iter 3053 | Time 58.4521(57.1765) | Bit/dim 3.6529(3.6473) | Xent 0.8103(0.8196) | Loss 4.0580(4.0571) | Error 0.2939(0.2937) Steps 574(577.39) | Grad Norm 0.7583(1.0310) | Total Time 14.00(14.00)\n",
      "Iter 3054 | Time 57.7356(57.1932) | Bit/dim 3.6451(3.6472) | Xent 0.8202(0.8196) | Loss 4.0552(4.0570) | Error 0.2919(0.2937) Steps 568(577.10) | Grad Norm 0.5759(1.0174) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0509 | Time 22.4518, Epoch Time 384.1826(380.6536), Bit/dim 3.6482(best: 3.6481), Xent 0.8471, Loss 4.0717, Error 0.2995(best: 0.2970)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3055 | Time 59.9542(57.2761) | Bit/dim 3.6526(3.6474) | Xent 0.8158(0.8195) | Loss 4.0604(4.0571) | Error 0.2901(0.2935) Steps 568(576.83) | Grad Norm 0.6519(1.0064) | Total Time 14.00(14.00)\n",
      "Iter 3056 | Time 55.5252(57.2235) | Bit/dim 3.6468(3.6474) | Xent 0.8175(0.8194) | Loss 4.0555(4.0571) | Error 0.2851(0.2933) Steps 574(576.75) | Grad Norm 0.5819(0.9937) | Total Time 14.00(14.00)\n",
      "Iter 3057 | Time 57.5366(57.2329) | Bit/dim 3.6487(3.6474) | Xent 0.8145(0.8193) | Loss 4.0560(4.0570) | Error 0.2929(0.2933) Steps 580(576.84) | Grad Norm 0.6513(0.9834) | Total Time 14.00(14.00)\n",
      "Iter 3058 | Time 57.4473(57.2394) | Bit/dim 3.6512(3.6475) | Xent 0.8254(0.8194) | Loss 4.0639(4.0572) | Error 0.2981(0.2934) Steps 580(576.94) | Grad Norm 0.7360(0.9760) | Total Time 14.00(14.00)\n",
      "Iter 3059 | Time 56.1105(57.2055) | Bit/dim 3.6360(3.6472) | Xent 0.8282(0.8197) | Loss 4.0501(4.0570) | Error 0.2953(0.2935) Steps 568(576.67) | Grad Norm 0.7445(0.9690) | Total Time 14.00(14.00)\n",
      "Iter 3060 | Time 52.9272(57.0772) | Bit/dim 3.6443(3.6471) | Xent 0.8230(0.8198) | Loss 4.0557(4.0570) | Error 0.2966(0.2936) Steps 574(576.59) | Grad Norm 0.6395(0.9592) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0510 | Time 22.6596, Epoch Time 377.8975(380.5709), Bit/dim 3.6481(best: 3.6481), Xent 0.8490, Loss 4.0726, Error 0.3008(best: 0.2970)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3061 | Time 57.7441(57.0972) | Bit/dim 3.6539(3.6473) | Xent 0.8170(0.8197) | Loss 4.0624(4.0572) | Error 0.2913(0.2935) Steps 586(576.87) | Grad Norm 1.2651(0.9683) | Total Time 14.00(14.00)\n",
      "Iter 3062 | Time 57.3278(57.1041) | Bit/dim 3.6395(3.6471) | Xent 0.8137(0.8195) | Loss 4.0463(4.0568) | Error 0.2941(0.2935) Steps 568(576.61) | Grad Norm 0.8243(0.9640) | Total Time 14.00(14.00)\n",
      "Iter 3063 | Time 58.0224(57.1316) | Bit/dim 3.6532(3.6472) | Xent 0.8153(0.8194) | Loss 4.0609(4.0570) | Error 0.2895(0.2934) Steps 574(576.53) | Grad Norm 1.2574(0.9728) | Total Time 14.00(14.00)\n",
      "Iter 3064 | Time 55.8997(57.0947) | Bit/dim 3.6469(3.6472) | Xent 0.8228(0.8195) | Loss 4.0583(4.0570) | Error 0.2927(0.2934) Steps 586(576.81) | Grad Norm 0.8277(0.9685) | Total Time 14.00(14.00)\n",
      "Iter 3065 | Time 57.4855(57.1064) | Bit/dim 3.6474(3.6472) | Xent 0.8144(0.8194) | Loss 4.0546(4.0569) | Error 0.2931(0.2934) Steps 586(577.09) | Grad Norm 0.5361(0.9555) | Total Time 14.00(14.00)\n",
      "Iter 3066 | Time 57.7815(57.1266) | Bit/dim 3.6383(3.6470) | Xent 0.8055(0.8189) | Loss 4.0411(4.0564) | Error 0.2821(0.2930) Steps 574(577.00) | Grad Norm 0.6904(0.9475) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0511 | Time 22.5543, Epoch Time 382.6547(380.6334), Bit/dim 3.6482(best: 3.6481), Xent 0.8497, Loss 4.0731, Error 0.3002(best: 0.2970)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3067 | Time 58.1405(57.1571) | Bit/dim 3.6371(3.6467) | Xent 0.8184(0.8189) | Loss 4.0463(4.0561) | Error 0.2953(0.2931) Steps 592(577.45) | Grad Norm 1.2287(0.9560) | Total Time 14.00(14.00)\n",
      "Iter 3068 | Time 58.0041(57.1825) | Bit/dim 3.6461(3.6467) | Xent 0.8140(0.8188) | Loss 4.0531(4.0560) | Error 0.2915(0.2931) Steps 586(577.70) | Grad Norm 0.7701(0.9504) | Total Time 14.00(14.00)\n",
      "Iter 3069 | Time 58.9649(57.2359) | Bit/dim 3.6454(3.6466) | Xent 0.8160(0.8187) | Loss 4.0533(4.0560) | Error 0.2855(0.2928) Steps 580(577.77) | Grad Norm 1.5671(0.9689) | Total Time 14.00(14.00)\n",
      "Iter 3070 | Time 60.5549(57.3355) | Bit/dim 3.6585(3.6470) | Xent 0.8083(0.8184) | Loss 4.0627(4.0562) | Error 0.2837(0.2926) Steps 586(578.02) | Grad Norm 0.8773(0.9661) | Total Time 14.00(14.00)\n",
      "Iter 3071 | Time 57.1320(57.3294) | Bit/dim 3.6408(3.6468) | Xent 0.8329(0.8188) | Loss 4.0572(4.0562) | Error 0.3021(0.2928) Steps 574(577.90) | Grad Norm 0.5738(0.9544) | Total Time 14.00(14.00)\n",
      "Iter 3072 | Time 58.3525(57.3601) | Bit/dim 3.6592(3.6472) | Xent 0.8280(0.8191) | Loss 4.0732(4.0567) | Error 0.3013(0.2931) Steps 574(577.78) | Grad Norm 1.0362(0.9568) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0512 | Time 22.3636, Epoch Time 389.5378(380.9005), Bit/dim 3.6484(best: 3.6481), Xent 0.8481, Loss 4.0724, Error 0.2993(best: 0.2970)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3073 | Time 58.1082(57.3825) | Bit/dim 3.6413(3.6470) | Xent 0.8170(0.8190) | Loss 4.0498(4.0565) | Error 0.2881(0.2929) Steps 598(578.39) | Grad Norm 0.9771(0.9574) | Total Time 14.00(14.00)\n",
      "Iter 3074 | Time 58.5495(57.4176) | Bit/dim 3.6427(3.6469) | Xent 0.8150(0.8189) | Loss 4.0502(4.0563) | Error 0.2923(0.2929) Steps 574(578.26) | Grad Norm 1.0620(0.9606) | Total Time 14.00(14.00)\n",
      "Iter 3075 | Time 57.8029(57.4291) | Bit/dim 3.6492(3.6469) | Xent 0.8329(0.8193) | Loss 4.0656(4.0566) | Error 0.2980(0.2931) Steps 580(578.31) | Grad Norm 1.0193(0.9623) | Total Time 14.00(14.00)\n",
      "Iter 3076 | Time 56.2356(57.3933) | Bit/dim 3.6498(3.6470) | Xent 0.8322(0.8197) | Loss 4.0659(4.0569) | Error 0.3010(0.2933) Steps 580(578.36) | Grad Norm 1.1738(0.9687) | Total Time 14.00(14.00)\n",
      "Iter 3077 | Time 59.1920(57.4473) | Bit/dim 3.6501(3.6471) | Xent 0.8162(0.8196) | Loss 4.0582(4.0569) | Error 0.2951(0.2934) Steps 574(578.23) | Grad Norm 1.0472(0.9710) | Total Time 14.00(14.00)\n",
      "Iter 3078 | Time 58.6227(57.4825) | Bit/dim 3.6455(3.6471) | Xent 0.8133(0.8194) | Loss 4.0522(4.0568) | Error 0.2926(0.2933) Steps 580(578.28) | Grad Norm 0.7630(0.9648) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0513 | Time 22.4749, Epoch Time 386.7836(381.0770), Bit/dim 3.6497(best: 3.6481), Xent 0.8479, Loss 4.0737, Error 0.2994(best: 0.2970)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3079 | Time 56.5598(57.4549) | Bit/dim 3.6427(3.6469) | Xent 0.8196(0.8194) | Loss 4.0526(4.0566) | Error 0.2975(0.2935) Steps 568(577.97) | Grad Norm 0.9169(0.9634) | Total Time 14.00(14.00)\n",
      "Iter 3080 | Time 54.7116(57.3726) | Bit/dim 3.6431(3.6468) | Xent 0.8082(0.8191) | Loss 4.0472(4.0564) | Error 0.2961(0.2936) Steps 586(578.21) | Grad Norm 1.0648(0.9664) | Total Time 14.00(14.00)\n",
      "Iter 3081 | Time 56.7783(57.3547) | Bit/dim 3.6511(3.6469) | Xent 0.8344(0.8196) | Loss 4.0682(4.0567) | Error 0.2995(0.2937) Steps 586(578.45) | Grad Norm 0.7007(0.9584) | Total Time 14.00(14.00)\n",
      "Iter 3082 | Time 56.2752(57.3223) | Bit/dim 3.6382(3.6467) | Xent 0.8100(0.8193) | Loss 4.0431(4.0563) | Error 0.2854(0.2935) Steps 568(578.13) | Grad Norm 1.0624(0.9615) | Total Time 14.00(14.00)\n",
      "Iter 3083 | Time 59.6628(57.3926) | Bit/dim 3.6572(3.6470) | Xent 0.8128(0.8191) | Loss 4.0636(4.0565) | Error 0.2920(0.2934) Steps 574(578.01) | Grad Norm 1.0361(0.9638) | Total Time 14.00(14.00)\n",
      "Iter 3084 | Time 56.2032(57.3569) | Bit/dim 3.6534(3.6472) | Xent 0.8073(0.8187) | Loss 4.0571(4.0565) | Error 0.2927(0.2934) Steps 586(578.25) | Grad Norm 0.7564(0.9576) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0514 | Time 22.6321, Epoch Time 378.6474(381.0041), Bit/dim 3.6487(best: 3.6481), Xent 0.8480, Loss 4.0727, Error 0.3027(best: 0.2970)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3085 | Time 57.2351(57.3532) | Bit/dim 3.6534(3.6474) | Xent 0.8055(0.8183) | Loss 4.0561(4.0565) | Error 0.2876(0.2932) Steps 580(578.30) | Grad Norm 1.2451(0.9662) | Total Time 14.00(14.00)\n",
      "Iter 3086 | Time 58.8277(57.3975) | Bit/dim 3.6510(3.6475) | Xent 0.8233(0.8185) | Loss 4.0626(4.0567) | Error 0.2941(0.2933) Steps 586(578.53) | Grad Norm 1.0240(0.9679) | Total Time 14.00(14.00)\n",
      "Iter 3087 | Time 57.7621(57.4084) | Bit/dim 3.6431(3.6473) | Xent 0.8289(0.8188) | Loss 4.0575(4.0567) | Error 0.2994(0.2935) Steps 574(578.40) | Grad Norm 0.8215(0.9635) | Total Time 14.00(14.00)\n",
      "Iter 3088 | Time 56.7055(57.3873) | Bit/dim 3.6474(3.6473) | Xent 0.8155(0.8187) | Loss 4.0552(4.0567) | Error 0.2917(0.2934) Steps 586(578.62) | Grad Norm 0.8302(0.9595) | Total Time 14.00(14.00)\n",
      "Iter 3089 | Time 57.2908(57.3844) | Bit/dim 3.6467(3.6473) | Xent 0.8221(0.8188) | Loss 4.0577(4.0567) | Error 0.2930(0.2934) Steps 574(578.49) | Grad Norm 1.0336(0.9618) | Total Time 14.00(14.00)\n",
      "Iter 3090 | Time 57.6634(57.3928) | Bit/dim 3.6386(3.6471) | Xent 0.8125(0.8186) | Loss 4.0448(4.0564) | Error 0.2900(0.2933) Steps 574(578.35) | Grad Norm 0.6710(0.9530) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0515 | Time 22.2792, Epoch Time 383.6515(381.0836), Bit/dim 3.6498(best: 3.6481), Xent 0.8482, Loss 4.0739, Error 0.3012(best: 0.2970)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3091 | Time 52.4406(57.2442) | Bit/dim 3.6433(3.6470) | Xent 0.8236(0.8188) | Loss 4.0552(4.0563) | Error 0.2934(0.2933) Steps 574(578.22) | Grad Norm 0.7879(0.9481) | Total Time 14.00(14.00)\n",
      "Iter 3092 | Time 58.9601(57.2957) | Bit/dim 3.6415(3.6468) | Xent 0.8134(0.8186) | Loss 4.0482(4.0561) | Error 0.2903(0.2932) Steps 592(578.63) | Grad Norm 1.3293(0.9595) | Total Time 14.00(14.00)\n",
      "Iter 3093 | Time 56.8761(57.2831) | Bit/dim 3.6415(3.6466) | Xent 0.8065(0.8182) | Loss 4.0448(4.0557) | Error 0.2921(0.2932) Steps 574(578.50) | Grad Norm 1.0563(0.9624) | Total Time 14.00(14.00)\n",
      "Iter 3094 | Time 59.0314(57.3356) | Bit/dim 3.6459(3.6466) | Xent 0.8122(0.8180) | Loss 4.0520(4.0556) | Error 0.2935(0.2932) Steps 586(578.72) | Grad Norm 0.5182(0.9491) | Total Time 14.00(14.00)\n",
      "Iter 3095 | Time 58.0845(57.3580) | Bit/dim 3.6506(3.6467) | Xent 0.8195(0.8181) | Loss 4.0604(4.0558) | Error 0.2980(0.2933) Steps 592(579.12) | Grad Norm 1.3071(0.9598) | Total Time 14.00(14.00)\n",
      "Iter 3096 | Time 58.3703(57.3884) | Bit/dim 3.6580(3.6471) | Xent 0.8418(0.8188) | Loss 4.0789(4.0565) | Error 0.3024(0.2936) Steps 580(579.15) | Grad Norm 1.4183(0.9736) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0516 | Time 22.2374, Epoch Time 381.9594(381.1098), Bit/dim 3.6484(best: 3.6481), Xent 0.8477, Loss 4.0722, Error 0.3021(best: 0.2970)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3097 | Time 58.1994(57.4127) | Bit/dim 3.6406(3.6469) | Xent 0.8202(0.8188) | Loss 4.0507(4.0563) | Error 0.2973(0.2937) Steps 580(579.17) | Grad Norm 0.7638(0.9673) | Total Time 14.00(14.00)\n",
      "Iter 3099 | Time 52.3926(57.3153) | Bit/dim 3.6474(3.6469) | Xent 0.8246(0.8191) | Loss 4.0597(4.0564) | Error 0.2933(0.2938) Steps 574(579.04) | Grad Norm 1.0488(0.9647) | Total Time 14.00(14.00)\n",
      "Iter 3100 | Time 57.6460(57.3252) | Bit/dim 3.6523(3.6471) | Xent 0.8129(0.8189) | Loss 4.0587(4.0565) | Error 0.2915(0.2938) Steps 586(579.25) | Grad Norm 0.7906(0.9595) | Total Time 14.00(14.00)\n",
      "Iter 3101 | Time 58.1846(57.3510) | Bit/dim 3.6440(3.6470) | Xent 0.8352(0.8194) | Loss 4.0616(4.0567) | Error 0.3021(0.2940) Steps 574(579.09) | Grad Norm 1.0791(0.9630) | Total Time 14.00(14.00)\n",
      "Iter 3102 | Time 57.6700(57.3606) | Bit/dim 3.6495(3.6471) | Xent 0.8105(0.8191) | Loss 4.0548(4.0566) | Error 0.2919(0.2939) Steps 574(578.94) | Grad Norm 1.0236(0.9649) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0517 | Time 22.4944, Epoch Time 381.5781(381.1239), Bit/dim 3.6488(best: 3.6481), Xent 0.8468, Loss 4.0722, Error 0.3002(best: 0.2970)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3103 | Time 57.1019(57.3528) | Bit/dim 3.6486(3.6471) | Xent 0.8219(0.8192) | Loss 4.0595(4.0567) | Error 0.2916(0.2939) Steps 586(579.15) | Grad Norm 1.4126(0.9783) | Total Time 14.00(14.00)\n",
      "Iter 3104 | Time 58.4482(57.3857) | Bit/dim 3.6349(3.6467) | Xent 0.8216(0.8193) | Loss 4.0457(4.0564) | Error 0.2934(0.2939) Steps 568(578.82) | Grad Norm 1.1419(0.9832) | Total Time 14.00(14.00)\n",
      "Iter 3105 | Time 57.2709(57.3822) | Bit/dim 3.6521(3.6469) | Xent 0.8210(0.8193) | Loss 4.0626(4.0565) | Error 0.2944(0.2939) Steps 580(578.85) | Grad Norm 1.0955(0.9866) | Total Time 14.00(14.00)\n",
      "Iter 3106 | Time 55.9584(57.3395) | Bit/dim 3.6458(3.6469) | Xent 0.8169(0.8192) | Loss 4.0543(4.0565) | Error 0.2919(0.2938) Steps 586(579.07) | Grad Norm 1.7312(1.0089) | Total Time 14.00(14.00)\n",
      "Iter 3107 | Time 58.1814(57.3648) | Bit/dim 3.6449(3.6468) | Xent 0.8164(0.8192) | Loss 4.0532(4.0564) | Error 0.2941(0.2938) Steps 568(578.73) | Grad Norm 0.8823(1.0051) | Total Time 14.00(14.00)\n",
      "Iter 3108 | Time 58.2528(57.3914) | Bit/dim 3.6530(3.6470) | Xent 0.8109(0.8189) | Loss 4.0585(4.0564) | Error 0.2913(0.2937) Steps 574(578.59) | Grad Norm 1.5785(1.0223) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0518 | Time 22.5542, Epoch Time 383.6977(381.2011), Bit/dim 3.6483(best: 3.6481), Xent 0.8466, Loss 4.0716, Error 0.3004(best: 0.2970)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3109 | Time 55.3285(57.3295) | Bit/dim 3.6407(3.6468) | Xent 0.8088(0.8186) | Loss 4.0451(4.0561) | Error 0.2951(0.2938) Steps 574(578.45) | Grad Norm 0.5924(1.0094) | Total Time 14.00(14.00)\n",
      "Iter 3110 | Time 56.9488(57.3181) | Bit/dim 3.6401(3.6466) | Xent 0.8225(0.8187) | Loss 4.0513(4.0560) | Error 0.2910(0.2937) Steps 574(578.32) | Grad Norm 0.6354(0.9982) | Total Time 14.00(14.00)\n",
      "Iter 3111 | Time 59.8604(57.3944) | Bit/dim 3.6447(3.6465) | Xent 0.8302(0.8191) | Loss 4.0598(4.0561) | Error 0.3000(0.2939) Steps 580(578.37) | Grad Norm 0.8943(0.9951) | Total Time 14.00(14.00)\n",
      "Iter 3112 | Time 58.8179(57.4371) | Bit/dim 3.6531(3.6467) | Xent 0.8223(0.8192) | Loss 4.0643(4.0563) | Error 0.2940(0.2939) Steps 580(578.42) | Grad Norm 0.8354(0.9903) | Total Time 14.00(14.00)\n",
      "Iter 3113 | Time 52.9455(57.3023) | Bit/dim 3.6464(3.6467) | Xent 0.8036(0.8187) | Loss 4.0482(4.0561) | Error 0.2880(0.2937) Steps 574(578.29) | Grad Norm 1.2102(0.9969) | Total Time 14.00(14.00)\n",
      "Iter 3114 | Time 58.3761(57.3346) | Bit/dim 3.6571(3.6470) | Xent 0.8218(0.8188) | Loss 4.0680(4.0564) | Error 0.2966(0.2938) Steps 568(577.98) | Grad Norm 0.8244(0.9917) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0519 | Time 22.5657, Epoch Time 380.6673(381.1851), Bit/dim 3.6478(best: 3.6481), Xent 0.8460, Loss 4.0708, Error 0.2992(best: 0.2970)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3115 | Time 56.1947(57.3004) | Bit/dim 3.6450(3.6470) | Xent 0.8151(0.8187) | Loss 4.0525(4.0563) | Error 0.2945(0.2938) Steps 580(578.04) | Grad Norm 0.8383(0.9871) | Total Time 14.00(14.00)\n",
      "Iter 3116 | Time 56.8132(57.2857) | Bit/dim 3.6460(3.6469) | Xent 0.8207(0.8187) | Loss 4.0563(4.0563) | Error 0.2911(0.2937) Steps 580(578.10) | Grad Norm 1.5425(1.0038) | Total Time 14.00(14.00)\n",
      "Iter 3117 | Time 55.7707(57.2403) | Bit/dim 3.6466(3.6469) | Xent 0.8111(0.8185) | Loss 4.0522(4.0562) | Error 0.2929(0.2937) Steps 574(577.98) | Grad Norm 0.7609(0.9965) | Total Time 14.00(14.00)\n",
      "Iter 3118 | Time 55.4123(57.1855) | Bit/dim 3.6652(3.6475) | Xent 0.8105(0.8183) | Loss 4.0705(4.0566) | Error 0.2880(0.2935) Steps 574(577.86) | Grad Norm 0.9776(0.9959) | Total Time 14.00(14.00)\n",
      "Iter 3119 | Time 57.7250(57.2016) | Bit/dim 3.6378(3.6472) | Xent 0.8148(0.8182) | Loss 4.0452(4.0563) | Error 0.2897(0.2934) Steps 574(577.74) | Grad Norm 0.8525(0.9916) | Total Time 14.00(14.00)\n",
      "Iter 3120 | Time 58.9595(57.2544) | Bit/dim 3.6396(3.6470) | Xent 0.7999(0.8176) | Loss 4.0396(4.0558) | Error 0.2874(0.2933) Steps 592(578.17) | Grad Norm 1.1952(0.9977) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0520 | Time 22.1406, Epoch Time 378.8648(381.1155), Bit/dim 3.6484(best: 3.6478), Xent 0.8482, Loss 4.0725, Error 0.3007(best: 0.2970)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3121 | Time 57.1249(57.2505) | Bit/dim 3.6408(3.6468) | Xent 0.8153(0.8175) | Loss 4.0485(4.0556) | Error 0.2961(0.2933) Steps 586(578.40) | Grad Norm 1.4078(1.0100) | Total Time 14.00(14.00)\n",
      "Iter 3122 | Time 58.2307(57.2799) | Bit/dim 3.6378(3.6465) | Xent 0.8238(0.8177) | Loss 4.0497(4.0554) | Error 0.2894(0.2932) Steps 586(578.63) | Grad Norm 0.7055(1.0009) | Total Time 14.00(14.00)\n",
      "Iter 3123 | Time 57.9260(57.2993) | Bit/dim 3.6493(3.6466) | Xent 0.7902(0.8169) | Loss 4.0444(4.0551) | Error 0.2870(0.2930) Steps 592(579.03) | Grad Norm 0.9341(0.9989) | Total Time 14.00(14.00)\n",
      "Iter 3124 | Time 57.9517(57.3189) | Bit/dim 3.6539(3.6468) | Xent 0.8405(0.8176) | Loss 4.0741(4.0556) | Error 0.3010(0.2933) Steps 586(579.24) | Grad Norm 1.6954(1.0198) | Total Time 14.00(14.00)\n",
      "Iter 3125 | Time 57.4342(57.3223) | Bit/dim 3.6500(3.6469) | Xent 0.8224(0.8178) | Loss 4.0612(4.0558) | Error 0.2976(0.2934) Steps 586(579.44) | Grad Norm 1.2652(1.0271) | Total Time 14.00(14.00)\n",
      "Iter 3126 | Time 59.4814(57.3871) | Bit/dim 3.6461(3.6469) | Xent 0.8128(0.8176) | Loss 4.0525(4.0557) | Error 0.2897(0.2933) Steps 586(579.64) | Grad Norm 0.5111(1.0117) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0521 | Time 22.6334, Epoch Time 386.7271(381.2838), Bit/dim 3.6474(best: 3.6478), Xent 0.8468, Loss 4.0708, Error 0.2997(best: 0.2970)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3127 | Time 56.7399(57.3677) | Bit/dim 3.6539(3.6471) | Xent 0.8187(0.8176) | Loss 4.0633(4.0559) | Error 0.2965(0.2934) Steps 586(579.83) | Grad Norm 1.0943(1.0141) | Total Time 14.00(14.00)\n",
      "Iter 3128 | Time 58.5939(57.4045) | Bit/dim 3.6376(3.6468) | Xent 0.8083(0.8174) | Loss 4.0418(4.0555) | Error 0.2864(0.2932) Steps 580(579.84) | Grad Norm 1.8801(1.0401) | Total Time 14.00(14.00)\n",
      "Iter 3129 | Time 59.8589(57.4781) | Bit/dim 3.6477(3.6468) | Xent 0.8271(0.8177) | Loss 4.0613(4.0557) | Error 0.2969(0.2933) Steps 574(579.66) | Grad Norm 0.5859(1.0265) | Total Time 14.00(14.00)\n",
      "Iter 3130 | Time 57.7415(57.4860) | Bit/dim 3.6485(3.6469) | Xent 0.8275(0.8180) | Loss 4.0622(4.0559) | Error 0.2937(0.2933) Steps 568(579.31) | Grad Norm 1.0362(1.0268) | Total Time 14.00(14.00)\n",
      "Iter 3131 | Time 55.2475(57.4188) | Bit/dim 3.6428(3.6468) | Xent 0.8200(0.8180) | Loss 4.0528(4.0558) | Error 0.2996(0.2935) Steps 574(579.15) | Grad Norm 1.3596(1.0368) | Total Time 14.00(14.00)\n",
      "Iter 3132 | Time 54.3413(57.3265) | Bit/dim 3.6475(3.6468) | Xent 0.8084(0.8177) | Loss 4.0517(4.0557) | Error 0.2894(0.2934) Steps 574(579.00) | Grad Norm 0.7425(1.0279) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0522 | Time 22.5280, Epoch Time 380.6835(381.2658), Bit/dim 3.6490(best: 3.6474), Xent 0.8447, Loss 4.0713, Error 0.2977(best: 0.2970)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3133 | Time 59.7965(57.4006) | Bit/dim 3.6509(3.6469) | Xent 0.8100(0.8175) | Loss 4.0558(4.0557) | Error 0.2936(0.2934) Steps 586(579.21) | Grad Norm 1.0596(1.0289) | Total Time 14.00(14.00)\n",
      "Iter 3134 | Time 58.6554(57.4383) | Bit/dim 3.6472(3.6469) | Xent 0.8133(0.8174) | Loss 4.0539(4.0556) | Error 0.2990(0.2935) Steps 580(579.23) | Grad Norm 1.5664(1.0450) | Total Time 14.00(14.00)\n",
      "Iter 3135 | Time 58.5126(57.4705) | Bit/dim 3.6528(3.6471) | Xent 0.8112(0.8172) | Loss 4.0584(4.0557) | Error 0.2891(0.2934) Steps 574(579.07) | Grad Norm 0.8256(1.0384) | Total Time 14.00(14.00)\n",
      "Iter 3136 | Time 57.6890(57.4770) | Bit/dim 3.6413(3.6469) | Xent 0.8178(0.8172) | Loss 4.0502(4.0555) | Error 0.2880(0.2933) Steps 580(579.10) | Grad Norm 0.6342(1.0263) | Total Time 14.00(14.00)\n",
      "Iter 3137 | Time 56.7865(57.4563) | Bit/dim 3.6445(3.6468) | Xent 0.8309(0.8176) | Loss 4.0599(4.0557) | Error 0.2996(0.2934) Steps 592(579.49) | Grad Norm 0.9849(1.0251) | Total Time 14.00(14.00)\n",
      "Iter 3138 | Time 58.8473(57.4981) | Bit/dim 3.6423(3.6467) | Xent 0.7972(0.8170) | Loss 4.0409(4.0552) | Error 0.2856(0.2932) Steps 586(579.68) | Grad Norm 1.1824(1.0298) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0523 | Time 22.5820, Epoch Time 388.6382(381.4870), Bit/dim 3.6498(best: 3.6474), Xent 0.8452, Loss 4.0723, Error 0.2979(best: 0.2970)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3139 | Time 59.2638(57.5510) | Bit/dim 3.6625(3.6472) | Xent 0.8145(0.8169) | Loss 4.0698(4.0556) | Error 0.2933(0.2932) Steps 574(579.51) | Grad Norm 0.8984(1.0258) | Total Time 14.00(14.00)\n",
      "Iter 3140 | Time 58.4935(57.5793) | Bit/dim 3.6420(3.6470) | Xent 0.8107(0.8167) | Loss 4.0474(4.0554) | Error 0.2937(0.2932) Steps 580(579.53) | Grad Norm 1.1620(1.0299) | Total Time 14.00(14.00)\n",
      "Iter 3141 | Time 60.3710(57.6631) | Bit/dim 3.6413(3.6469) | Xent 0.8046(0.8164) | Loss 4.0436(4.0550) | Error 0.2887(0.2931) Steps 598(580.08) | Grad Norm 1.0239(1.0297) | Total Time 14.00(14.00)\n",
      "Iter 3142 | Time 55.8645(57.6091) | Bit/dim 3.6464(3.6468) | Xent 0.8280(0.8167) | Loss 4.0604(4.0552) | Error 0.2946(0.2931) Steps 574(579.90) | Grad Norm 1.0935(1.0317) | Total Time 14.00(14.00)\n",
      "Iter 3143 | Time 57.8147(57.6153) | Bit/dim 3.6453(3.6468) | Xent 0.8267(0.8170) | Loss 4.0586(4.0553) | Error 0.2991(0.2933) Steps 568(579.54) | Grad Norm 1.1534(1.0353) | Total Time 14.00(14.00)\n",
      "Iter 3144 | Time 57.3957(57.6087) | Bit/dim 3.6496(3.6469) | Xent 0.8024(0.8166) | Loss 4.0507(4.0552) | Error 0.2863(0.2931) Steps 580(579.56) | Grad Norm 1.2203(1.0409) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0524 | Time 22.5527, Epoch Time 387.4116(381.6647), Bit/dim 3.6483(best: 3.6474), Xent 0.8467, Loss 4.0716, Error 0.2993(best: 0.2970)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3145 | Time 58.1479(57.6249) | Bit/dim 3.6565(3.6472) | Xent 0.8190(0.8167) | Loss 4.0660(4.0555) | Error 0.2913(0.2930) Steps 574(579.39) | Grad Norm 0.8673(1.0357) | Total Time 14.00(14.00)\n",
      "Iter 3146 | Time 57.7536(57.6287) | Bit/dim 3.6487(3.6472) | Xent 0.8135(0.8166) | Loss 4.0555(4.0555) | Error 0.2896(0.2929) Steps 568(579.05) | Grad Norm 0.8145(1.0290) | Total Time 14.00(14.00)\n",
      "Iter 3147 | Time 56.6015(57.5979) | Bit/dim 3.6343(3.6468) | Xent 0.8095(0.8164) | Loss 4.0391(4.0550) | Error 0.2887(0.2928) Steps 574(578.90) | Grad Norm 1.5585(1.0449) | Total Time 14.00(14.00)\n",
      "Iter 3148 | Time 58.1168(57.6135) | Bit/dim 3.6496(3.6469) | Xent 0.8078(0.8161) | Loss 4.0535(4.0550) | Error 0.2936(0.2928) Steps 580(578.93) | Grad Norm 1.1498(1.0481) | Total Time 14.00(14.00)\n",
      "Iter 3149 | Time 58.0350(57.6261) | Bit/dim 3.6458(3.6469) | Xent 0.8285(0.8165) | Loss 4.0601(4.0551) | Error 0.2944(0.2929) Steps 574(578.78) | Grad Norm 1.2558(1.0543) | Total Time 14.00(14.00)\n",
      "Iter 3150 | Time 58.2677(57.6454) | Bit/dim 3.6443(3.6468) | Xent 0.8138(0.8164) | Loss 4.0512(4.0550) | Error 0.2923(0.2929) Steps 568(578.46) | Grad Norm 1.1256(1.0564) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0525 | Time 22.3369, Epoch Time 384.7780(381.7581), Bit/dim 3.6478(best: 3.6474), Xent 0.8466, Loss 4.0711, Error 0.2992(best: 0.2970)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3151 | Time 57.3530(57.6366) | Bit/dim 3.6522(3.6470) | Xent 0.7872(0.8155) | Loss 4.0458(4.0547) | Error 0.2786(0.2924) Steps 574(578.32) | Grad Norm 0.9727(1.0539) | Total Time 14.00(14.00)\n",
      "Iter 3152 | Time 57.5992(57.6355) | Bit/dim 3.6435(3.6469) | Xent 0.8178(0.8156) | Loss 4.0524(4.0547) | Error 0.2903(0.2924) Steps 586(578.56) | Grad Norm 1.5771(1.0696) | Total Time 14.00(14.00)\n",
      "Iter 3153 | Time 58.5011(57.6614) | Bit/dim 3.6523(3.6470) | Xent 0.8108(0.8154) | Loss 4.0577(4.0547) | Error 0.2896(0.2923) Steps 574(578.42) | Grad Norm 0.6970(1.0584) | Total Time 14.00(14.00)\n",
      "Iter 3154 | Time 57.1670(57.6466) | Bit/dim 3.6327(3.6466) | Xent 0.8336(0.8160) | Loss 4.0495(4.0546) | Error 0.3011(0.2926) Steps 574(578.29) | Grad Norm 0.6649(1.0466) | Total Time 14.00(14.00)\n",
      "Iter 3155 | Time 55.9200(57.5948) | Bit/dim 3.6430(3.6465) | Xent 0.8248(0.8162) | Loss 4.0554(4.0546) | Error 0.3009(0.2928) Steps 574(578.16) | Grad Norm 1.2034(1.0513) | Total Time 14.00(14.00)\n",
      "Iter 3156 | Time 57.8688(57.6030) | Bit/dim 3.6578(3.6468) | Xent 0.8202(0.8164) | Loss 4.0679(4.0550) | Error 0.2967(0.2929) Steps 586(578.39) | Grad Norm 1.0545(1.0514) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0526 | Time 22.9411, Epoch Time 383.0539(381.7970), Bit/dim 3.6488(best: 3.6474), Xent 0.8460, Loss 4.0718, Error 0.2981(best: 0.2970)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3157 | Time 58.1450(57.6193) | Bit/dim 3.6463(3.6468) | Xent 0.8081(0.8161) | Loss 4.0503(4.0549) | Error 0.2927(0.2929) Steps 568(578.08) | Grad Norm 0.5199(1.0355) | Total Time 14.00(14.00)\n",
      "Iter 3158 | Time 56.8319(57.5957) | Bit/dim 3.6389(3.6466) | Xent 0.8270(0.8164) | Loss 4.0524(4.0548) | Error 0.2907(0.2929) Steps 580(578.14) | Grad Norm 0.7886(1.0281) | Total Time 14.00(14.00)\n",
      "Iter 3159 | Time 57.2069(57.5840) | Bit/dim 3.6397(3.6464) | Xent 0.8131(0.8163) | Loss 4.0462(4.0545) | Error 0.2946(0.2929) Steps 568(577.83) | Grad Norm 1.4450(1.0406) | Total Time 14.00(14.00)\n",
      "Iter 3160 | Time 57.5448(57.5828) | Bit/dim 3.6620(3.6468) | Xent 0.8119(0.8162) | Loss 4.0679(4.0549) | Error 0.2909(0.2928) Steps 580(577.90) | Grad Norm 0.7844(1.0329) | Total Time 14.00(14.00)\n",
      "Iter 3161 | Time 58.1552(57.6000) | Bit/dim 3.6396(3.6466) | Xent 0.8070(0.8159) | Loss 4.0431(4.0546) | Error 0.2937(0.2929) Steps 598(578.50) | Grad Norm 0.7488(1.0244) | Total Time 14.00(14.00)\n",
      "Iter 3162 | Time 56.8542(57.5776) | Bit/dim 3.6561(3.6469) | Xent 0.8077(0.8157) | Loss 4.0599(4.0547) | Error 0.2937(0.2929) Steps 568(578.19) | Grad Norm 0.6540(1.0133) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0527 | Time 22.4956, Epoch Time 382.9754(381.8324), Bit/dim 3.6480(best: 3.6474), Xent 0.8466, Loss 4.0713, Error 0.2984(best: 0.2970)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3163 | Time 55.3769(57.5116) | Bit/dim 3.6461(3.6469) | Xent 0.8258(0.8160) | Loss 4.0590(4.0549) | Error 0.2933(0.2929) Steps 580(578.24) | Grad Norm 0.8692(1.0089) | Total Time 14.00(14.00)\n",
      "Iter 3164 | Time 58.8307(57.5512) | Bit/dim 3.6341(3.6465) | Xent 0.7982(0.8155) | Loss 4.0332(4.0542) | Error 0.2871(0.2927) Steps 574(578.11) | Grad Norm 0.9742(1.0079) | Total Time 14.00(14.00)\n",
      "Iter 3165 | Time 60.2324(57.6316) | Bit/dim 3.6486(3.6466) | Xent 0.8198(0.8156) | Loss 4.0585(4.0544) | Error 0.2926(0.2927) Steps 598(578.71) | Grad Norm 0.6890(0.9983) | Total Time 14.00(14.00)\n",
      "Iter 3166 | Time 56.7461(57.6050) | Bit/dim 3.6497(3.6467) | Xent 0.8282(0.8160) | Loss 4.0637(4.0546) | Error 0.2983(0.2929) Steps 586(578.93) | Grad Norm 1.0180(0.9989) | Total Time 14.00(14.00)\n",
      "Iter 3167 | Time 58.3363(57.6270) | Bit/dim 3.6427(3.6465) | Xent 0.8052(0.8156) | Loss 4.0453(4.0544) | Error 0.2873(0.2927) Steps 574(578.78) | Grad Norm 0.7228(0.9906) | Total Time 14.00(14.00)\n",
      "Iter 3168 | Time 56.7902(57.6019) | Bit/dim 3.6579(3.6469) | Xent 0.8163(0.8157) | Loss 4.0660(4.0547) | Error 0.2929(0.2927) Steps 592(579.18) | Grad Norm 1.0234(0.9916) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0528 | Time 22.6228, Epoch Time 384.5335(381.9134), Bit/dim 3.6486(best: 3.6474), Xent 0.8468, Loss 4.0720, Error 0.3007(best: 0.2970)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3169 | Time 56.6455(57.5732) | Bit/dim 3.6572(3.6472) | Xent 0.8209(0.8158) | Loss 4.0677(4.0551) | Error 0.2940(0.2928) Steps 574(579.02) | Grad Norm 0.8292(0.9867) | Total Time 14.00(14.00)\n",
      "Iter 3170 | Time 57.6725(57.5762) | Bit/dim 3.6444(3.6471) | Xent 0.8089(0.8156) | Loss 4.0489(4.0549) | Error 0.2925(0.2928) Steps 586(579.23) | Grad Norm 0.7999(0.9811) | Total Time 14.00(14.00)\n",
      "Iter 3171 | Time 56.9818(57.5583) | Bit/dim 3.6418(3.6469) | Xent 0.8036(0.8152) | Loss 4.0436(4.0546) | Error 0.2854(0.2925) Steps 574(579.08) | Grad Norm 1.1850(0.9873) | Total Time 14.00(14.00)\n",
      "Iter 3172 | Time 54.2062(57.4578) | Bit/dim 3.6495(3.6470) | Xent 0.7895(0.8145) | Loss 4.0443(4.0543) | Error 0.2799(0.2922) Steps 574(578.92) | Grad Norm 0.4349(0.9707) | Total Time 14.00(14.00)\n",
      "Iter 3173 | Time 58.8035(57.4981) | Bit/dim 3.6415(3.6469) | Xent 0.8156(0.8145) | Loss 4.0493(4.0541) | Error 0.2906(0.2921) Steps 574(578.78) | Grad Norm 0.7979(0.9655) | Total Time 14.00(14.00)\n",
      "Iter 3174 | Time 57.8183(57.5077) | Bit/dim 3.6464(3.6468) | Xent 0.8330(0.8151) | Loss 4.0629(4.0544) | Error 0.2966(0.2923) Steps 580(578.81) | Grad Norm 0.7445(0.9589) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0529 | Time 22.5487, Epoch Time 380.5868(381.8736), Bit/dim 3.6489(best: 3.6474), Xent 0.8463, Loss 4.0721, Error 0.3026(best: 0.2970)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3175 | Time 56.3785(57.4739) | Bit/dim 3.6435(3.6467) | Xent 0.8221(0.8153) | Loss 4.0546(4.0544) | Error 0.2939(0.2923) Steps 586(579.03) | Grad Norm 0.9113(0.9574) | Total Time 14.00(14.00)\n",
      "Iter 3176 | Time 52.1417(57.3139) | Bit/dim 3.6392(3.6465) | Xent 0.8326(0.8158) | Loss 4.0555(4.0544) | Error 0.2984(0.2925) Steps 580(579.06) | Grad Norm 0.8740(0.9549) | Total Time 14.00(14.00)\n",
      "Iter 3177 | Time 58.7896(57.3582) | Bit/dim 3.6461(3.6465) | Xent 0.8212(0.8160) | Loss 4.0567(4.0545) | Error 0.2910(0.2924) Steps 586(579.27) | Grad Norm 0.4226(0.9390) | Total Time 14.00(14.00)\n",
      "Iter 3178 | Time 58.8090(57.4017) | Bit/dim 3.6530(3.6467) | Xent 0.7905(0.8152) | Loss 4.0483(4.0543) | Error 0.2870(0.2923) Steps 580(579.29) | Grad Norm 0.9069(0.9380) | Total Time 14.00(14.00)\n",
      "Iter 3179 | Time 56.5049(57.3748) | Bit/dim 3.6462(3.6467) | Xent 0.8202(0.8153) | Loss 4.0563(4.0544) | Error 0.2925(0.2923) Steps 586(579.49) | Grad Norm 0.9779(0.9392) | Total Time 14.00(14.00)\n",
      "Iter 3180 | Time 58.5058(57.4087) | Bit/dim 3.6494(3.6468) | Xent 0.8051(0.8150) | Loss 4.0520(4.0543) | Error 0.2844(0.2920) Steps 574(579.32) | Grad Norm 0.5804(0.9284) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0530 | Time 22.2894, Epoch Time 379.1857(381.7930), Bit/dim 3.6480(best: 3.6474), Xent 0.8454, Loss 4.0707, Error 0.2984(best: 0.2970)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3181 | Time 55.4230(57.3491) | Bit/dim 3.6427(3.6466) | Xent 0.8166(0.8151) | Loss 4.0510(4.0542) | Error 0.2904(0.2920) Steps 580(579.34) | Grad Norm 0.5992(0.9186) | Total Time 14.00(14.00)\n",
      "Iter 3182 | Time 58.6652(57.3886) | Bit/dim 3.6489(3.6467) | Xent 0.8211(0.8153) | Loss 4.0594(4.0543) | Error 0.2960(0.2921) Steps 574(579.18) | Grad Norm 0.8972(0.9179) | Total Time 14.00(14.00)\n",
      "Iter 3183 | Time 54.2967(57.2959) | Bit/dim 3.6536(3.6469) | Xent 0.8220(0.8155) | Loss 4.0646(4.0546) | Error 0.2961(0.2922) Steps 574(579.03) | Grad Norm 0.4370(0.9035) | Total Time 14.00(14.00)\n",
      "Iter 3184 | Time 57.5983(57.3049) | Bit/dim 3.6431(3.6468) | Xent 0.8074(0.8152) | Loss 4.0468(4.0544) | Error 0.2886(0.2921) Steps 586(579.24) | Grad Norm 0.7394(0.8986) | Total Time 14.00(14.00)\n",
      "Iter 3185 | Time 59.7472(57.3782) | Bit/dim 3.6442(3.6467) | Xent 0.7991(0.8147) | Loss 4.0437(4.0541) | Error 0.2826(0.2918) Steps 574(579.08) | Grad Norm 0.7412(0.8938) | Total Time 14.00(14.00)\n",
      "Iter 3186 | Time 57.0133(57.3673) | Bit/dim 3.6474(3.6467) | Xent 0.8243(0.8150) | Loss 4.0596(4.0543) | Error 0.2925(0.2919) Steps 598(579.65) | Grad Norm 0.4252(0.8798) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0531 | Time 22.4931, Epoch Time 380.9964(381.7691), Bit/dim 3.6478(best: 3.6474), Xent 0.8430, Loss 4.0693, Error 0.2993(best: 0.2970)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3187 | Time 56.1447(57.3306) | Bit/dim 3.6472(3.6468) | Xent 0.7944(0.8144) | Loss 4.0444(4.0540) | Error 0.2844(0.2916) Steps 586(579.84) | Grad Norm 0.7353(0.8755) | Total Time 14.00(14.00)\n",
      "Iter 3188 | Time 57.1465(57.3251) | Bit/dim 3.6499(3.6469) | Xent 0.8182(0.8145) | Loss 4.0590(4.0541) | Error 0.2909(0.2916) Steps 568(579.48) | Grad Norm 0.5193(0.8648) | Total Time 14.00(14.00)\n",
      "Iter 3189 | Time 60.3486(57.4158) | Bit/dim 3.6527(3.6470) | Xent 0.8375(0.8152) | Loss 4.0714(4.0546) | Error 0.3016(0.2919) Steps 580(579.50) | Grad Norm 0.8752(0.8651) | Total Time 14.00(14.00)\n",
      "Iter 3190 | Time 57.4602(57.4171) | Bit/dim 3.6389(3.6468) | Xent 0.8181(0.8153) | Loss 4.0480(4.0544) | Error 0.2921(0.2919) Steps 574(579.33) | Grad Norm 0.6354(0.8582) | Total Time 14.00(14.00)\n",
      "Iter 3191 | Time 60.1868(57.5002) | Bit/dim 3.6398(3.6466) | Xent 0.7912(0.8146) | Loss 4.0354(4.0539) | Error 0.2856(0.2917) Steps 598(579.89) | Grad Norm 0.7033(0.8535) | Total Time 14.00(14.00)\n",
      "Iter 3192 | Time 56.8643(57.4811) | Bit/dim 3.6460(3.6466) | Xent 0.8238(0.8149) | Loss 4.0579(4.0540) | Error 0.2905(0.2917) Steps 580(579.90) | Grad Norm 0.6384(0.8471) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0532 | Time 22.7084, Epoch Time 386.7382(381.9181), Bit/dim 3.6487(best: 3.6474), Xent 0.8453, Loss 4.0714, Error 0.2984(best: 0.2970)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3193 | Time 56.8883(57.4633) | Bit/dim 3.6349(3.6462) | Xent 0.8207(0.8150) | Loss 4.0453(4.0537) | Error 0.2929(0.2917) Steps 568(579.54) | Grad Norm 0.7087(0.8429) | Total Time 14.00(14.00)\n",
      "Iter 3194 | Time 55.6133(57.4078) | Bit/dim 3.6482(3.6463) | Xent 0.7959(0.8145) | Loss 4.0462(4.0535) | Error 0.2817(0.2914) Steps 574(579.37) | Grad Norm 0.4871(0.8323) | Total Time 14.00(14.00)\n",
      "Iter 3195 | Time 57.4823(57.4101) | Bit/dim 3.6435(3.6462) | Xent 0.8236(0.8147) | Loss 4.0553(4.0535) | Error 0.2960(0.2916) Steps 568(579.03) | Grad Norm 1.1626(0.8422) | Total Time 14.00(14.00)\n",
      "Iter 3196 | Time 55.6884(57.3584) | Bit/dim 3.6585(3.6466) | Xent 0.8030(0.8144) | Loss 4.0600(4.0537) | Error 0.2906(0.2915) Steps 586(579.24) | Grad Norm 0.7239(0.8386) | Total Time 14.00(14.00)\n",
      "Iter 3197 | Time 56.8046(57.3418) | Bit/dim 3.6488(3.6466) | Xent 0.8137(0.8144) | Loss 4.0556(4.0538) | Error 0.2896(0.2915) Steps 592(579.62) | Grad Norm 0.8208(0.8381) | Total Time 14.00(14.00)\n",
      "Iter 3198 | Time 58.5745(57.3788) | Bit/dim 3.6456(3.6466) | Xent 0.8204(0.8145) | Loss 4.0558(4.0539) | Error 0.3026(0.2918) Steps 574(579.46) | Grad Norm 0.8573(0.8387) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0533 | Time 22.2646, Epoch Time 378.9649(381.8295), Bit/dim 3.6482(best: 3.6474), Xent 0.8442, Loss 4.0702, Error 0.2977(best: 0.2970)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3199 | Time 58.8264(57.4222) | Bit/dim 3.6437(3.6465) | Xent 0.8155(0.8146) | Loss 4.0514(4.0538) | Error 0.2920(0.2918) Steps 586(579.65) | Grad Norm 0.6197(0.8321) | Total Time 14.00(14.00)\n",
      "Iter 3200 | Time 56.1613(57.3844) | Bit/dim 3.6605(3.6469) | Xent 0.8047(0.8143) | Loss 4.0628(4.0541) | Error 0.2861(0.2917) Steps 586(579.84) | Grad Norm 0.6590(0.8269) | Total Time 14.00(14.00)\n",
      "Iter 3201 | Time 56.8987(57.3698) | Bit/dim 3.6434(3.6468) | Xent 0.7995(0.8138) | Loss 4.0431(4.0537) | Error 0.2899(0.2916) Steps 574(579.67) | Grad Norm 0.7585(0.8249) | Total Time 14.00(14.00)\n",
      "Iter 3202 | Time 58.0583(57.3905) | Bit/dim 3.6347(3.6464) | Xent 0.8113(0.8138) | Loss 4.0404(4.0533) | Error 0.2946(0.2917) Steps 574(579.50) | Grad Norm 1.1902(0.8358) | Total Time 14.00(14.00)\n",
      "Iter 3217 | Time 58.0242(57.5420) | Bit/dim 3.6543(3.6467) | Xent 0.8257(0.8148) | Loss 4.0671(4.0541) | Error 0.3004(0.2925) Steps 592(579.48) | Grad Norm 0.3947(0.8839) | Total Time 14.00(14.00)\n",
      "Iter 3218 | Time 56.6130(57.5141) | Bit/dim 3.6436(3.6466) | Xent 0.8207(0.8150) | Loss 4.0540(4.0541) | Error 0.2910(0.2925) Steps 568(579.13) | Grad Norm 1.3000(0.8964) | Total Time 14.00(14.00)\n",
      "Iter 3219 | Time 55.8893(57.4653) | Bit/dim 3.6405(3.6464) | Xent 0.8189(0.8151) | Loss 4.0499(4.0540) | Error 0.2936(0.2925) Steps 574(578.98) | Grad Norm 0.9191(0.8971) | Total Time 14.00(14.00)\n",
      "Iter 3220 | Time 54.9584(57.3901) | Bit/dim 3.6510(3.6466) | Xent 0.8040(0.8148) | Loss 4.0530(4.0539) | Error 0.2867(0.2923) Steps 586(579.19) | Grad Norm 0.7078(0.8914) | Total Time 14.00(14.00)\n",
      "Iter 3221 | Time 58.5528(57.4250) | Bit/dim 3.6456(3.6465) | Xent 0.8151(0.8148) | Loss 4.0531(4.0539) | Error 0.2920(0.2923) Steps 574(579.03) | Grad Norm 0.7162(0.8861) | Total Time 14.00(14.00)\n",
      "Iter 3222 | Time 59.0344(57.4733) | Bit/dim 3.6480(3.6466) | Xent 0.8185(0.8149) | Loss 4.0572(4.0540) | Error 0.2896(0.2922) Steps 586(579.24) | Grad Norm 1.5182(0.9051) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0537 | Time 22.3406, Epoch Time 380.6764(381.9979), Bit/dim 3.6480(best: 3.6474), Xent 0.8454, Loss 4.0707, Error 0.2977(best: 0.2970)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3223 | Time 57.3629(57.4700) | Bit/dim 3.6533(3.6468) | Xent 0.8037(0.8146) | Loss 4.0551(4.0541) | Error 0.2920(0.2922) Steps 574(579.08) | Grad Norm 0.8144(0.9024) | Total Time 14.00(14.00)\n",
      "Iter 3224 | Time 59.5872(57.5335) | Bit/dim 3.6477(3.6468) | Xent 0.8185(0.8147) | Loss 4.0570(4.0541) | Error 0.2914(0.2922) Steps 586(579.29) | Grad Norm 0.5842(0.8928) | Total Time 14.00(14.00)\n",
      "Iter 3225 | Time 56.7879(57.5111) | Bit/dim 3.6465(3.6468) | Xent 0.8085(0.8145) | Loss 4.0507(4.0540) | Error 0.2939(0.2922) Steps 586(579.49) | Grad Norm 1.3913(0.9078) | Total Time 14.00(14.00)\n",
      "Iter 3226 | Time 55.5018(57.4509) | Bit/dim 3.6417(3.6466) | Xent 0.8010(0.8141) | Loss 4.0423(4.0537) | Error 0.2943(0.2923) Steps 574(579.33) | Grad Norm 1.6494(0.9300) | Total Time 14.00(14.00)\n",
      "Iter 3227 | Time 57.8915(57.4641) | Bit/dim 3.6460(3.6466) | Xent 0.8271(0.8145) | Loss 4.0595(4.0539) | Error 0.2950(0.2924) Steps 574(579.17) | Grad Norm 0.5613(0.9190) | Total Time 14.00(14.00)\n",
      "Iter 3228 | Time 56.8659(57.4461) | Bit/dim 3.6372(3.6463) | Xent 0.8142(0.8145) | Loss 4.0443(4.0536) | Error 0.2959(0.2925) Steps 580(579.19) | Grad Norm 1.5027(0.9365) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0538 | Time 22.2758, Epoch Time 381.9195(381.9955), Bit/dim 3.6474(best: 3.6474), Xent 0.8456, Loss 4.0702, Error 0.3024(best: 0.2970)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3229 | Time 57.7871(57.4564) | Bit/dim 3.6496(3.6464) | Xent 0.8094(0.8143) | Loss 4.0543(4.0536) | Error 0.2934(0.2925) Steps 574(579.04) | Grad Norm 1.4100(0.9507) | Total Time 14.00(14.00)\n",
      "Iter 3230 | Time 59.8484(57.5281) | Bit/dim 3.6533(3.6466) | Xent 0.8213(0.8145) | Loss 4.0640(4.0539) | Error 0.2931(0.2925) Steps 574(578.89) | Grad Norm 0.8282(0.9470) | Total Time 14.00(14.00)\n",
      "Iter 3231 | Time 51.5028(57.3474) | Bit/dim 3.6440(3.6466) | Xent 0.8082(0.8143) | Loss 4.0481(4.0537) | Error 0.2859(0.2923) Steps 574(578.74) | Grad Norm 1.0112(0.9489) | Total Time 14.00(14.00)\n",
      "Iter 3232 | Time 56.7171(57.3284) | Bit/dim 3.6458(3.6465) | Xent 0.8183(0.8145) | Loss 4.0550(4.0538) | Error 0.2959(0.2924) Steps 574(578.60) | Grad Norm 1.3186(0.9600) | Total Time 14.00(14.00)\n",
      "Iter 3233 | Time 58.2775(57.3569) | Bit/dim 3.6379(3.6463) | Xent 0.8115(0.8144) | Loss 4.0437(4.0535) | Error 0.2939(0.2925) Steps 580(578.64) | Grad Norm 1.2846(0.9698) | Total Time 14.00(14.00)\n",
      "Iter 3234 | Time 57.6340(57.3652) | Bit/dim 3.6434(3.6462) | Xent 0.8157(0.8144) | Loss 4.0513(4.0534) | Error 0.2893(0.2924) Steps 586(578.86) | Grad Norm 0.7986(0.9646) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0539 | Time 22.2833, Epoch Time 379.9308(381.9336), Bit/dim 3.6480(best: 3.6474), Xent 0.8440, Loss 4.0700, Error 0.3000(best: 0.2970)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3235 | Time 57.7329(57.3763) | Bit/dim 3.6459(3.6462) | Xent 0.8195(0.8146) | Loss 4.0556(4.0535) | Error 0.2923(0.2924) Steps 586(579.07) | Grad Norm 0.5609(0.9525) | Total Time 14.00(14.00)\n",
      "Iter 3236 | Time 57.4718(57.3791) | Bit/dim 3.6425(3.6461) | Xent 0.7930(0.8139) | Loss 4.0390(4.0530) | Error 0.2877(0.2922) Steps 586(579.28) | Grad Norm 1.0467(0.9553) | Total Time 14.00(14.00)\n",
      "Iter 3237 | Time 57.3130(57.3771) | Bit/dim 3.6454(3.6461) | Xent 0.8312(0.8144) | Loss 4.0610(4.0533) | Error 0.3014(0.2925) Steps 574(579.12) | Grad Norm 1.2645(0.9646) | Total Time 14.00(14.00)\n",
      "Iter 3238 | Time 55.9078(57.3331) | Bit/dim 3.6538(3.6463) | Xent 0.8157(0.8145) | Loss 4.0617(4.0535) | Error 0.2947(0.2926) Steps 574(578.97) | Grad Norm 0.6709(0.9558) | Total Time 14.00(14.00)\n",
      "Iter 3239 | Time 55.6718(57.2832) | Bit/dim 3.6476(3.6463) | Xent 0.8122(0.8144) | Loss 4.0537(4.0535) | Error 0.2951(0.2927) Steps 598(579.54) | Grad Norm 1.1376(0.9613) | Total Time 14.00(14.00)\n",
      "Iter 3240 | Time 56.6027(57.2628) | Bit/dim 3.6426(3.6462) | Xent 0.8236(0.8147) | Loss 4.0544(4.0536) | Error 0.2979(0.2928) Steps 568(579.19) | Grad Norm 1.4802(0.9768) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0540 | Time 22.9775, Epoch Time 379.3503(381.8561), Bit/dim 3.6472(best: 3.6474), Xent 0.8438, Loss 4.0691, Error 0.2992(best: 0.2970)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3241 | Time 53.5467(57.1513) | Bit/dim 3.6428(3.6461) | Xent 0.8139(0.8147) | Loss 4.0498(4.0534) | Error 0.2885(0.2927) Steps 574(579.04) | Grad Norm 0.5342(0.9635) | Total Time 14.00(14.00)\n",
      "Iter 3242 | Time 57.2701(57.1549) | Bit/dim 3.6412(3.6460) | Xent 0.8065(0.8144) | Loss 4.0445(4.0532) | Error 0.2921(0.2927) Steps 574(578.89) | Grad Norm 0.7092(0.9559) | Total Time 14.00(14.00)\n",
      "Iter 3243 | Time 60.6832(57.2607) | Bit/dim 3.6379(3.6457) | Xent 0.8047(0.8141) | Loss 4.0402(4.0528) | Error 0.2814(0.2923) Steps 592(579.28) | Grad Norm 1.0092(0.9575) | Total Time 14.00(14.00)\n",
      "Iter 3244 | Time 58.5420(57.2992) | Bit/dim 3.6507(3.6459) | Xent 0.8170(0.8142) | Loss 4.0592(4.0530) | Error 0.2905(0.2923) Steps 586(579.48) | Grad Norm 0.6845(0.9493) | Total Time 14.00(14.00)\n",
      "Iter 3245 | Time 58.7999(57.3442) | Bit/dim 3.6534(3.6461) | Xent 0.8020(0.8138) | Loss 4.0543(4.0530) | Error 0.2895(0.2922) Steps 574(579.32) | Grad Norm 0.8463(0.9462) | Total Time 14.00(14.00)\n",
      "Iter 3246 | Time 56.9569(57.3326) | Bit/dim 3.6462(3.6461) | Xent 0.8328(0.8144) | Loss 4.0626(4.0533) | Error 0.3027(0.2925) Steps 586(579.52) | Grad Norm 0.6541(0.9375) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0541 | Time 22.8623, Epoch Time 384.2136(381.9268), Bit/dim 3.6481(best: 3.6472), Xent 0.8431, Loss 4.0697, Error 0.3002(best: 0.2970)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3247 | Time 56.4178(57.3051) | Bit/dim 3.6471(3.6461) | Xent 0.8080(0.8142) | Loss 4.0511(4.0532) | Error 0.2891(0.2924) Steps 586(579.71) | Grad Norm 1.0783(0.9417) | Total Time 14.00(14.00)\n",
      "Iter 3248 | Time 59.3119(57.3653) | Bit/dim 3.6453(3.6461) | Xent 0.8160(0.8143) | Loss 4.0533(4.0532) | Error 0.2913(0.2924) Steps 586(579.90) | Grad Norm 0.9476(0.9419) | Total Time 14.00(14.00)\n",
      "Iter 3249 | Time 57.2199(57.3610) | Bit/dim 3.6441(3.6461) | Xent 0.8167(0.8143) | Loss 4.0525(4.0532) | Error 0.2955(0.2925) Steps 580(579.90) | Grad Norm 0.5045(0.9287) | Total Time 14.00(14.00)\n",
      "Iter 3250 | Time 58.4248(57.3929) | Bit/dim 3.6445(3.6460) | Xent 0.8232(0.8146) | Loss 4.0561(4.0533) | Error 0.2931(0.2925) Steps 580(579.91) | Grad Norm 0.6319(0.9198) | Total Time 14.00(14.00)\n",
      "Iter 3251 | Time 55.6315(57.3400) | Bit/dim 3.6508(3.6461) | Xent 0.8044(0.8143) | Loss 4.0530(4.0533) | Error 0.2853(0.2923) Steps 574(579.73) | Grad Norm 0.5035(0.9074) | Total Time 14.00(14.00)\n",
      "Iter 3252 | Time 58.8057(57.3840) | Bit/dim 3.6436(3.6461) | Xent 0.8062(0.8141) | Loss 4.0467(4.0531) | Error 0.2916(0.2923) Steps 574(579.56) | Grad Norm 0.6212(0.8988) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0542 | Time 22.6069, Epoch Time 384.3096(381.9983), Bit/dim 3.6477(best: 3.6472), Xent 0.8444, Loss 4.0699, Error 0.2982(best: 0.2970)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3253 | Time 57.5877(57.3901) | Bit/dim 3.6493(3.6462) | Xent 0.7973(0.8136) | Loss 4.0480(4.0529) | Error 0.2924(0.2923) Steps 586(579.75) | Grad Norm 0.7526(0.8944) | Total Time 14.00(14.00)\n",
      "Iter 3254 | Time 54.6731(57.3086) | Bit/dim 3.6440(3.6461) | Xent 0.8279(0.8140) | Loss 4.0580(4.0531) | Error 0.2946(0.2923) Steps 580(579.76) | Grad Norm 0.8403(0.8928) | Total Time 14.00(14.00)\n",
      "Iter 3255 | Time 57.1176(57.3029) | Bit/dim 3.6422(3.6460) | Xent 0.8090(0.8138) | Loss 4.0467(4.0529) | Error 0.2883(0.2922) Steps 592(580.13) | Grad Norm 1.3607(0.9068) | Total Time 14.00(14.00)\n",
      "Iter 3256 | Time 56.2836(57.2723) | Bit/dim 3.6476(3.6460) | Xent 0.8131(0.8138) | Loss 4.0541(4.0529) | Error 0.2936(0.2922) Steps 568(579.76) | Grad Norm 1.1830(0.9151) | Total Time 14.00(14.00)\n",
      "Iter 3257 | Time 59.2674(57.3322) | Bit/dim 3.6454(3.6460) | Xent 0.8166(0.8139) | Loss 4.0536(4.0530) | Error 0.2926(0.2923) Steps 574(579.59) | Grad Norm 0.5478(0.9041) | Total Time 14.00(14.00)\n",
      "Iter 3258 | Time 56.5045(57.3073) | Bit/dim 3.6532(3.6462) | Xent 0.8137(0.8139) | Loss 4.0600(4.0532) | Error 0.2940(0.2923) Steps 580(579.60) | Grad Norm 1.2617(0.9148) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0543 | Time 22.4633, Epoch Time 379.2292(381.9152), Bit/dim 3.6481(best: 3.6472), Xent 0.8456, Loss 4.0709, Error 0.2965(best: 0.2970)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3259 | Time 57.9483(57.3266) | Bit/dim 3.6409(3.6461) | Xent 0.8051(0.8136) | Loss 4.0434(4.0529) | Error 0.2904(0.2923) Steps 580(579.61) | Grad Norm 1.3522(0.9279) | Total Time 14.00(14.00)\n",
      "Iter 3260 | Time 57.0965(57.3197) | Bit/dim 3.6460(3.6461) | Xent 0.8170(0.8137) | Loss 4.0545(4.0529) | Error 0.2919(0.2922) Steps 586(579.81) | Grad Norm 0.6933(0.9209) | Total Time 14.00(14.00)\n",
      "Iter 3261 | Time 56.4403(57.2933) | Bit/dim 3.6476(3.6461) | Xent 0.8163(0.8138) | Loss 4.0558(4.0530) | Error 0.2941(0.2923) Steps 586(579.99) | Grad Norm 0.8432(0.9185) | Total Time 14.00(14.00)\n",
      "Iter 3262 | Time 58.0107(57.3148) | Bit/dim 3.6519(3.6463) | Xent 0.8158(0.8139) | Loss 4.0598(4.0532) | Error 0.2861(0.2921) Steps 574(579.81) | Grad Norm 1.5750(0.9382) | Total Time 14.00(14.00)\n",
      "Iter 3263 | Time 57.5369(57.3215) | Bit/dim 3.6395(3.6461) | Xent 0.8361(0.8145) | Loss 4.0575(4.0533) | Error 0.3046(0.2925) Steps 598(580.36) | Grad Norm 0.8104(0.9344) | Total Time 14.00(14.00)\n",
      "Iter 3264 | Time 52.9917(57.1916) | Bit/dim 3.6503(3.6462) | Xent 0.8160(0.8146) | Loss 4.0583(4.0535) | Error 0.2930(0.2925) Steps 574(580.17) | Grad Norm 0.5300(0.9223) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0544 | Time 22.6923, Epoch Time 378.0528(381.7993), Bit/dim 3.6473(best: 3.6472), Xent 0.8442, Loss 4.0694, Error 0.2995(best: 0.2965)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3265 | Time 57.2501(57.1933) | Bit/dim 3.6442(3.6462) | Xent 0.8164(0.8146) | Loss 4.0524(4.0535) | Error 0.2947(0.2926) Steps 586(580.34) | Grad Norm 1.3524(0.9352) | Total Time 14.00(14.00)\n",
      "Iter 3266 | Time 58.4447(57.2309) | Bit/dim 3.6425(3.6460) | Xent 0.8109(0.8145) | Loss 4.0480(4.0533) | Error 0.2944(0.2926) Steps 586(580.51) | Grad Norm 0.8807(0.9335) | Total Time 14.00(14.00)\n",
      "Iter 3267 | Time 57.0917(57.2267) | Bit/dim 3.6449(3.6460) | Xent 0.8163(0.8146) | Loss 4.0531(4.0533) | Error 0.2911(0.2926) Steps 592(580.86) | Grad Norm 0.6929(0.9263) | Total Time 14.00(14.00)\n",
      "Iter 3268 | Time 57.7167(57.2414) | Bit/dim 3.6453(3.6460) | Xent 0.8366(0.8152) | Loss 4.0636(4.0536) | Error 0.3019(0.2929) Steps 580(580.83) | Grad Norm 1.6452(0.9479) | Total Time 14.00(14.00)\n",
      "Iter 3269 | Time 57.8358(57.2592) | Bit/dim 3.6463(3.6460) | Xent 0.8133(0.8152) | Loss 4.0529(4.0536) | Error 0.2914(0.2928) Steps 574(580.63) | Grad Norm 1.0079(0.9497) | Total Time 14.00(14.00)\n",
      "Iter 3270 | Time 56.2641(57.2294) | Bit/dim 3.6498(3.6461) | Xent 0.8088(0.8150) | Loss 4.0542(4.0536) | Error 0.2874(0.2927) Steps 586(580.79) | Grad Norm 0.4892(0.9359) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0545 | Time 22.7684, Epoch Time 382.9701(381.8345), Bit/dim 3.6480(best: 3.6472), Xent 0.8440, Loss 4.0700, Error 0.2996(best: 0.2965)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3271 | Time 59.7459(57.3049) | Bit/dim 3.6436(3.6460) | Xent 0.8049(0.8147) | Loss 4.0461(4.0534) | Error 0.2890(0.2925) Steps 586(580.94) | Grad Norm 1.9356(0.9659) | Total Time 14.00(14.00)\n",
      "Iter 3272 | Time 56.9199(57.2933) | Bit/dim 3.6548(3.6463) | Xent 0.8272(0.8151) | Loss 4.0685(4.0538) | Error 0.2965(0.2927) Steps 574(580.73) | Grad Norm 0.9108(0.9642) | Total Time 14.00(14.00)\n",
      "Iter 3273 | Time 57.5688(57.3016) | Bit/dim 3.6576(3.6466) | Xent 0.8012(0.8146) | Loss 4.0582(4.0540) | Error 0.2847(0.2924) Steps 580(580.71) | Grad Norm 0.4824(0.9498) | Total Time 14.00(14.00)\n",
      "Iter 3274 | Time 57.7070(57.3137) | Bit/dim 3.6380(3.6464) | Xent 0.8233(0.8149) | Loss 4.0497(4.0538) | Error 0.2957(0.2925) Steps 568(580.33) | Grad Norm 1.0423(0.9525) | Total Time 14.00(14.00)\n",
      "Iter 3275 | Time 58.7571(57.3570) | Bit/dim 3.6314(3.6459) | Xent 0.7973(0.8144) | Loss 4.0301(4.0531) | Error 0.2804(0.2922) Steps 574(580.14) | Grad Norm 1.2281(0.9608) | Total Time 14.00(14.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_cifar.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 8000 --test_batch_size 5000 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_run1_post --load_dir ../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_run1 --seed 1 --conditional True --controlled_tol True --train_mode semisup --lr 0.001 --warmup_iters 1000 --atol 1e-4  --rtol 1e-4 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
