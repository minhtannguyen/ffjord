{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_drop_cifar.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "if args.controlled_tol:\n",
      "    import lib.odenvp_conditional_tol as odenvp\n",
      "else:\n",
      "    import lib.odenvp_conditional as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz = modules.GaussianDiag.logp(mean, logs, z).view(-1,1)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        z = model.module.dropout(z)\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(z)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol},\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z = modules.GaussianDiag.sample(mean, logs)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            loss.backward()\n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=0.0001, autoencode=False, batch_norm=False, batch_size=8000, batch_size_schedule='', begin_epoch=1, conditional=True, controlled_tol=True, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=1, lr=0.01, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume='../experiments_published/cnf_conditional_cifar10_8K_drop_0_5_run1/epoch_32_checkpt.pth', rtol=0.0001, save='../experiments_published/cnf_conditional_cifar10_8K_drop_0_5_run1', seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=5000, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000.0, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=6144, bias=True)\n",
      "  (project_class): LinearZeros(in_features=3072, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1469494\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc.py:286: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "Iter 0193 | Time 107.9612(47.6826) | Bit/dim 4.7752(5.1180) | Xent 1.7812(1.8988) | Loss 5.6658(6.0674) | Error 0.6162(0.6561) Steps 556(528.83) | Grad Norm 4.1194(5.8523) | Total Time 14.00(14.00)\n",
      "Iter 0194 | Time 48.7073(47.7133) | Bit/dim 4.7588(5.1072) | Xent 1.7710(1.8950) | Loss 5.6442(6.0547) | Error 0.6184(0.6549) Steps 550(529.47) | Grad Norm 3.9526(5.7953) | Total Time 14.00(14.00)\n",
      "Iter 0195 | Time 48.1457(47.7263) | Bit/dim 4.7514(5.0965) | Xent 1.7574(1.8909) | Loss 5.6301(6.0419) | Error 0.6113(0.6536) Steps 550(530.08) | Grad Norm 6.0466(5.8028) | Total Time 14.00(14.00)\n",
      "Iter 0196 | Time 54.8239(47.9392) | Bit/dim 4.7503(5.0861) | Xent 1.7723(1.8873) | Loss 5.6365(6.0298) | Error 0.6162(0.6525) Steps 556(530.86) | Grad Norm 6.8971(5.8356) | Total Time 14.00(14.00)\n",
      "Iter 0197 | Time 49.9644(48.0000) | Bit/dim 4.7644(5.0765) | Xent 1.7811(1.8841) | Loss 5.6549(6.0185) | Error 0.6222(0.6516) Steps 562(531.79) | Grad Norm 7.8685(5.8966) | Total Time 14.00(14.00)\n",
      "Iter 0198 | Time 53.0790(48.1524) | Bit/dim 4.7540(5.0668) | Xent 1.7798(1.8810) | Loss 5.6439(6.0073) | Error 0.6118(0.6504) Steps 568(532.88) | Grad Norm 7.5895(5.9474) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0033 | Time 36.8537, Epoch Time 415.8377(336.1736), Bit/dim 4.7618(best: inf), Xent 1.7537, Loss 5.6386, Error 0.5978(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0199 | Time 62.7556(48.5905) | Bit/dim 4.7601(5.0576) | Xent 1.7566(1.8772) | Loss 5.6384(5.9962) | Error 0.6086(0.6491) Steps 556(533.57) | Grad Norm 5.9469(5.9474) | Total Time 14.00(14.00)\n",
      "Iter 0200 | Time 53.7372(48.7449) | Bit/dim 4.7146(5.0473) | Xent 1.7337(1.8729) | Loss 5.5815(5.9838) | Error 0.5979(0.6476) Steps 562(534.43) | Grad Norm 3.9050(5.8861) | Total Time 14.00(14.00)\n",
      "Iter 0201 | Time 53.8407(48.8977) | Bit/dim 4.7279(5.0377) | Xent 1.7774(1.8701) | Loss 5.6166(5.9728) | Error 0.6250(0.6469) Steps 592(536.15) | Grad Norm 9.7221(6.0012) | Total Time 14.00(14.00)\n",
      "Iter 0202 | Time 50.5229(48.9465) | Bit/dim 4.7878(5.0302) | Xent 1.8696(1.8701) | Loss 5.7226(5.9653) | Error 0.6584(0.6473) Steps 586(537.65) | Grad Norm 18.7525(6.3837) | Total Time 14.00(14.00)\n",
      "Iter 0203 | Time 50.3691(48.9892) | Bit/dim 4.8489(5.0248) | Xent 1.8821(1.8704) | Loss 5.7899(5.9600) | Error 0.6549(0.6475) Steps 532(537.48) | Grad Norm 20.0241(6.7930) | Total Time 14.00(14.00)\n",
      "Iter 0204 | Time 53.4076(49.1217) | Bit/dim 4.7736(5.0172) | Xent 1.9180(1.8718) | Loss 5.7326(5.9532) | Error 0.6619(0.6479) Steps 562(538.21) | Grad Norm 20.7403(7.2114) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0034 | Time 22.6094, Epoch Time 363.1815(336.9839), Bit/dim 5.0553(best: 4.7618), Xent 2.0594, Loss 6.0851, Error 0.7242(best: 0.5978)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0205 | Time 52.3787(49.2194) | Bit/dim 5.0568(5.0184) | Xent 2.0550(1.8773) | Loss 6.0843(5.9571) | Error 0.7251(0.6503) Steps 586(539.65) | Grad Norm 14.5101(7.4303) | Total Time 14.00(14.00)\n",
      "Iter 0206 | Time 49.4528(49.2264) | Bit/dim 5.0500(5.0194) | Xent 1.9276(1.8789) | Loss 6.0139(5.9588) | Error 0.6773(0.6511) Steps 604(541.58) | Grad Norm 9.7725(7.5006) | Total Time 14.00(14.00)\n",
      "Iter 0207 | Time 56.1820(49.4351) | Bit/dim 4.8033(5.0129) | Xent 1.9335(1.8805) | Loss 5.7700(5.9531) | Error 0.6894(0.6522) Steps 640(544.53) | Grad Norm 4.9782(7.4249) | Total Time 14.00(14.00)\n",
      "Iter 0208 | Time 54.4521(49.5856) | Bit/dim 4.9161(5.0100) | Xent 1.9403(1.8823) | Loss 5.8863(5.9511) | Error 0.6774(0.6530) Steps 634(547.22) | Grad Norm 10.3656(7.5132) | Total Time 14.00(14.00)\n",
      "Iter 0209 | Time 54.0682(49.7201) | Bit/dim 4.8307(5.0046) | Xent 1.9689(1.8849) | Loss 5.8151(5.9471) | Error 0.6947(0.6542) Steps 634(549.82) | Grad Norm 6.4967(7.4827) | Total Time 14.00(14.00)\n",
      "Iter 0210 | Time 51.1603(49.7633) | Bit/dim 4.9094(5.0018) | Xent 1.8847(1.8849) | Loss 5.8518(5.9442) | Error 0.6655(0.6546) Steps 616(551.80) | Grad Norm 4.8925(7.4050) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0035 | Time 23.9372, Epoch Time 357.3629(337.5952), Bit/dim 4.8633(best: 4.7618), Xent 1.9049, Loss 5.8158, Error 0.6760(best: 0.5978)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0211 | Time 51.6624(49.8203) | Bit/dim 4.8692(4.9978) | Xent 1.9068(1.8855) | Loss 5.8226(5.9406) | Error 0.6800(0.6553) Steps 622(553.91) | Grad Norm 5.6753(7.3531) | Total Time 14.00(14.00)\n",
      "Iter 0212 | Time 58.0367(50.0668) | Bit/dim 4.7799(4.9912) | Xent 1.8670(1.8850) | Loss 5.7134(5.9337) | Error 0.6499(0.6552) Steps 640(556.49) | Grad Norm 3.3927(7.2343) | Total Time 14.00(14.00)\n",
      "Iter 0213 | Time 52.8329(50.1497) | Bit/dim 4.7591(4.9843) | Xent 1.9095(1.8857) | Loss 5.7139(5.9271) | Error 0.6761(0.6558) Steps 604(557.92) | Grad Norm 4.0230(7.1379) | Total Time 14.00(14.00)\n",
      "Iter 0214 | Time 54.9898(50.2950) | Bit/dim 4.7814(4.9782) | Xent 1.9043(1.8863) | Loss 5.7336(5.9213) | Error 0.6606(0.6559) Steps 628(560.02) | Grad Norm 3.5179(7.0293) | Total Time 14.00(14.00)\n",
      "Iter 0215 | Time 50.7059(50.3073) | Bit/dim 4.7725(4.9720) | Xent 1.8682(1.8857) | Loss 5.7066(5.9149) | Error 0.6564(0.6559) Steps 604(561.34) | Grad Norm 3.3897(6.9201) | Total Time 14.00(14.00)\n",
      "Iter 0216 | Time 54.8072(50.4423) | Bit/dim 4.7794(4.9662) | Xent 1.8471(1.8846) | Loss 5.7029(5.9085) | Error 0.6464(0.6557) Steps 610(562.80) | Grad Norm 3.1495(6.8070) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0036 | Time 24.3034, Epoch Time 363.2231(338.3641), Bit/dim 4.7518(best: 4.7618), Xent 1.8188, Loss 5.6612, Error 0.6212(best: 0.5978)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0217 | Time 54.1046(50.5521) | Bit/dim 4.7452(4.9596) | Xent 1.8059(1.8822) | Loss 5.6481(5.9007) | Error 0.6228(0.6547) Steps 622(564.58) | Grad Norm 3.2058(6.6990) | Total Time 14.00(14.00)\n",
      "Iter 0218 | Time 53.5939(50.6434) | Bit/dim 4.7289(4.9527) | Xent 1.8282(1.8806) | Loss 5.6430(5.8930) | Error 0.6384(0.6542) Steps 628(566.48) | Grad Norm 2.2361(6.5651) | Total Time 14.00(14.00)\n",
      "Iter 0219 | Time 52.5792(50.7015) | Bit/dim 4.7282(4.9460) | Xent 1.8233(1.8789) | Loss 5.6399(5.8854) | Error 0.6375(0.6537) Steps 610(567.78) | Grad Norm 3.5892(6.4758) | Total Time 14.00(14.00)\n",
      "Iter 0220 | Time 53.6249(50.7892) | Bit/dim 4.7114(4.9389) | Xent 1.8040(1.8766) | Loss 5.6134(5.8772) | Error 0.6327(0.6531) Steps 622(569.41) | Grad Norm 2.7260(6.3633) | Total Time 14.00(14.00)\n",
      "Iter 0221 | Time 50.8109(50.7898) | Bit/dim 4.6869(4.9314) | Xent 1.7813(1.8738) | Loss 5.5775(5.8682) | Error 0.6210(0.6521) Steps 604(570.45) | Grad Norm 2.4519(6.2460) | Total Time 14.00(14.00)\n",
      "Iter 0222 | Time 53.9135(50.8835) | Bit/dim 4.6714(4.9236) | Xent 1.7911(1.8713) | Loss 5.5670(5.8592) | Error 0.6244(0.6513) Steps 604(571.45) | Grad Norm 2.7481(6.1410) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0037 | Time 23.4031, Epoch Time 357.7408(338.9454), Bit/dim 4.6633(best: 4.7518), Xent 1.7737, Loss 5.5501, Error 0.6092(best: 0.5978)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0223 | Time 52.5902(50.9347) | Bit/dim 4.6710(4.9160) | Xent 1.7783(1.8685) | Loss 5.5601(5.8502) | Error 0.6179(0.6503) Steps 610(572.61) | Grad Norm 2.9048(6.0439) | Total Time 14.00(14.00)\n",
      "Iter 0224 | Time 54.5103(51.0420) | Bit/dim 4.6622(4.9084) | Xent 1.7683(1.8655) | Loss 5.5463(5.8411) | Error 0.6138(0.6492) Steps 586(573.01) | Grad Norm 2.9995(5.9526) | Total Time 14.00(14.00)\n",
      "Iter 0225 | Time 51.0067(51.0409) | Bit/dim 4.6525(4.9007) | Xent 1.7305(1.8614) | Loss 5.5177(5.8314) | Error 0.6058(0.6479) Steps 592(573.58) | Grad Norm 2.0377(5.8352) | Total Time 14.00(14.00)\n",
      "Iter 0226 | Time 50.6445(51.0291) | Bit/dim 4.6375(4.8928) | Xent 1.7539(1.8582) | Loss 5.5144(5.8219) | Error 0.6104(0.6467) Steps 586(573.96) | Grad Norm 2.1808(5.7255) | Total Time 14.00(14.00)\n",
      "Iter 0227 | Time 54.2975(51.1271) | Bit/dim 4.6204(4.8846) | Xent 1.7295(1.8544) | Loss 5.4852(5.8118) | Error 0.5989(0.6453) Steps 592(574.50) | Grad Norm 3.2744(5.6520) | Total Time 14.00(14.00)\n",
      "Iter 0228 | Time 58.7677(51.3563) | Bit/dim 4.6135(4.8765) | Xent 1.7432(1.8510) | Loss 5.4851(5.8020) | Error 0.6006(0.6440) Steps 562(574.12) | Grad Norm 2.8911(5.5692) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0038 | Time 22.8317, Epoch Time 360.4818(339.5915), Bit/dim 4.6169(best: 4.6633), Xent 1.7180, Loss 5.4759, Error 0.5931(best: 0.5978)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0229 | Time 53.5524(51.4222) | Bit/dim 4.6242(4.8689) | Xent 1.7451(1.8478) | Loss 5.4968(5.7928) | Error 0.6056(0.6428) Steps 586(574.48) | Grad Norm 1.8394(5.4573) | Total Time 14.00(14.00)\n",
      "Iter 0230 | Time 51.7861(51.4331) | Bit/dim 4.6043(4.8610) | Xent 1.7150(1.8439) | Loss 5.4618(5.7829) | Error 0.5950(0.6414) Steps 586(574.82) | Grad Norm 2.3829(5.3650) | Total Time 14.00(14.00)\n",
      "Iter 0231 | Time 50.1960(51.3960) | Bit/dim 4.5939(4.8530) | Xent 1.6938(1.8394) | Loss 5.4408(5.7727) | Error 0.5925(0.6399) Steps 586(575.16) | Grad Norm 3.0708(5.2962) | Total Time 14.00(14.00)\n",
      "Iter 0232 | Time 54.0402(51.4753) | Bit/dim 4.5911(4.8451) | Xent 1.7038(1.8353) | Loss 5.4430(5.7628) | Error 0.5940(0.6385) Steps 598(575.84) | Grad Norm 7.4087(5.3596) | Total Time 14.00(14.00)\n",
      "Iter 0233 | Time 50.8647(51.4570) | Bit/dim 4.5973(4.8377) | Xent 1.8379(1.8354) | Loss 5.5163(5.7554) | Error 0.6411(0.6386) Steps 568(575.61) | Grad Norm 18.5720(5.7560) | Total Time 14.00(14.00)\n",
      "Iter 0234 | Time 55.5751(51.5806) | Bit/dim 4.7152(4.8340) | Xent 2.2361(1.8474) | Loss 5.8333(5.7577) | Error 0.7320(0.6414) Steps 610(576.64) | Grad Norm 29.0215(6.4539) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0039 | Time 23.8080, Epoch Time 355.6664(340.0737), Bit/dim 4.7545(best: 4.6169), Xent 1.9945, Loss 5.7518, Error 0.7153(best: 0.5931)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0235 | Time 54.6765(51.6734) | Bit/dim 4.7526(4.8316) | Xent 1.9726(1.8511) | Loss 5.7389(5.7571) | Error 0.6999(0.6432) Steps 640(578.54) | Grad Norm 12.2077(6.6265) | Total Time 14.00(14.00)\n",
      "Iter 0236 | Time 55.5394(51.7894) | Bit/dim 4.6689(4.8267) | Xent 1.9493(1.8541) | Loss 5.6436(5.7537) | Error 0.6874(0.6445) Steps 640(580.39) | Grad Norm 14.4046(6.8599) | Total Time 14.00(14.00)\n",
      "Iter 0237 | Time 55.5959(51.9036) | Bit/dim 4.6665(4.8219) | Xent 1.8604(1.8543) | Loss 5.5966(5.7490) | Error 0.6542(0.6448) Steps 634(581.99) | Grad Norm 7.3252(6.8738) | Total Time 14.00(14.00)\n",
      "Iter 0238 | Time 52.7620(51.9294) | Bit/dim 4.7385(4.8194) | Xent 1.9127(1.8560) | Loss 5.6949(5.7474) | Error 0.6800(0.6458) Steps 646(583.91) | Grad Norm 9.3441(6.9480) | Total Time 14.00(14.00)\n",
      "Iter 0239 | Time 51.8352(51.9265) | Bit/dim 4.7293(4.8167) | Xent 1.8470(1.8558) | Loss 5.6528(5.7446) | Error 0.6470(0.6459) Steps 646(585.78) | Grad Norm 6.5164(6.9350) | Total Time 14.00(14.00)\n",
      "Iter 0240 | Time 55.3554(52.0294) | Bit/dim 4.6703(4.8123) | Xent 1.8448(1.8554) | Loss 5.5927(5.7400) | Error 0.6522(0.6461) Steps 682(588.66) | Grad Norm 5.5761(6.8942) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0040 | Time 24.7142, Epoch Time 366.3313(340.8614), Bit/dim 4.6204(best: 4.6169), Xent 1.8259, Loss 5.5333, Error 0.6381(best: 0.5931)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0241 | Time 56.9182(52.1761) | Bit/dim 4.6128(4.8063) | Xent 1.8484(1.8552) | Loss 5.5370(5.7339) | Error 0.6486(0.6461) Steps 682(591.46) | Grad Norm 4.7252(6.8292) | Total Time 14.00(14.00)\n",
      "Iter 0242 | Time 53.4556(52.2145) | Bit/dim 4.6338(4.8011) | Xent 1.8578(1.8553) | Loss 5.5627(5.7288) | Error 0.6519(0.6463) Steps 628(592.56) | Grad Norm 4.5890(6.7620) | Total Time 14.00(14.00)\n",
      "Iter 0243 | Time 54.5134(52.2834) | Bit/dim 4.6529(4.7967) | Xent 1.8279(1.8545) | Loss 5.5668(5.7239) | Error 0.6407(0.6461) Steps 628(593.62) | Grad Norm 4.4324(6.6921) | Total Time 14.00(14.00)\n",
      "Iter 0244 | Time 57.1057(52.4281) | Bit/dim 4.5993(4.7908) | Xent 1.8121(1.8532) | Loss 5.5053(5.7174) | Error 0.6412(0.6460) Steps 634(594.83) | Grad Norm 3.8739(6.6075) | Total Time 14.00(14.00)\n",
      "Iter 0245 | Time 53.9984(52.4752) | Bit/dim 4.6069(4.7852) | Xent 1.8553(1.8533) | Loss 5.5346(5.7119) | Error 0.6507(0.6461) Steps 664(596.91) | Grad Norm 6.5769(6.6066) | Total Time 14.00(14.00)\n",
      "Iter 0246 | Time 51.9138(52.4584) | Bit/dim 4.5743(4.7789) | Xent 1.7960(1.8515) | Loss 5.4723(5.7047) | Error 0.6355(0.6458) Steps 622(597.66) | Grad Norm 2.3507(6.4789) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0041 | Time 23.3879, Epoch Time 367.1655(341.6506), Bit/dim 4.6000(best: 4.6169), Xent 1.7888, Loss 5.4944, Error 0.6245(best: 0.5931)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0247 | Time 54.4619(52.5185) | Bit/dim 4.5993(4.7735) | Xent 1.7937(1.8498) | Loss 5.4962(5.6984) | Error 0.6269(0.6453) Steps 622(598.39) | Grad Norm 4.0219(6.4052) | Total Time 14.00(14.00)\n",
      "Iter 0248 | Time 50.4614(52.4568) | Bit/dim 4.5703(4.7674) | Xent 1.7891(1.8480) | Loss 5.4648(5.6914) | Error 0.6199(0.6445) Steps 622(599.10) | Grad Norm 3.3714(6.3142) | Total Time 14.00(14.00)\n",
      "Iter 0249 | Time 53.3884(52.4847) | Bit/dim 4.5617(4.7613) | Xent 1.7812(1.8460) | Loss 5.4523(5.6842) | Error 0.6344(0.6442) Steps 610(599.43) | Grad Norm 4.0662(6.2468) | Total Time 14.00(14.00)\n",
      "Iter 0250 | Time 53.2676(52.5082) | Bit/dim 4.5438(4.7547) | Xent 1.7609(1.8434) | Loss 5.4243(5.6764) | Error 0.6160(0.6433) Steps 622(600.10) | Grad Norm 2.8890(6.1460) | Total Time 14.00(14.00)\n",
      "Iter 0251 | Time 57.0239(52.6437) | Bit/dim 4.5701(4.7492) | Xent 1.7459(1.8405) | Loss 5.4431(5.6694) | Error 0.6071(0.6423) Steps 622(600.76) | Grad Norm 3.3634(6.0626) | Total Time 14.00(14.00)\n",
      "Iter 0252 | Time 53.0830(52.6568) | Bit/dim 4.5267(4.7425) | Xent 1.7788(1.8387) | Loss 5.4161(5.6618) | Error 0.6270(0.6418) Steps 604(600.86) | Grad Norm 2.8031(5.9648) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0042 | Time 23.4710, Epoch Time 360.9371(342.2292), Bit/dim 4.5318(best: 4.6000), Xent 1.7291, Loss 5.3963, Error 0.6094(best: 0.5931)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0253 | Time 51.0965(52.6100) | Bit/dim 4.5321(4.7362) | Xent 1.7791(1.8369) | Loss 5.4216(5.6546) | Error 0.6295(0.6414) Steps 610(601.13) | Grad Norm 3.6585(5.8956) | Total Time 14.00(14.00)\n",
      "Iter 0254 | Time 52.1461(52.5961) | Bit/dim 4.5174(4.7296) | Xent 1.7644(1.8347) | Loss 5.3996(5.6470) | Error 0.6254(0.6410) Steps 634(602.12) | Grad Norm 4.8671(5.8647) | Total Time 14.00(14.00)\n",
      "Iter 0255 | Time 51.8377(52.5734) | Bit/dim 4.5252(4.7235) | Xent 1.7806(1.8331) | Loss 5.4155(5.6400) | Error 0.6305(0.6406) Steps 622(602.71) | Grad Norm 8.1402(5.9330) | Total Time 14.00(14.00)\n",
      "Iter 0256 | Time 58.8717(52.7623) | Bit/dim 4.5682(4.7188) | Xent 1.9069(1.8353) | Loss 5.5216(5.6365) | Error 0.6607(0.6412) Steps 688(605.27) | Grad Norm 8.2867(6.0036) | Total Time 14.00(14.00)\n",
      "Iter 0257 | Time 56.4705(52.8736) | Bit/dim 4.5119(4.7126) | Xent 1.7542(1.8329) | Loss 5.3890(5.6291) | Error 0.6196(0.6406) Steps 694(607.94) | Grad Norm 4.1241(5.9472) | Total Time 14.00(14.00)\n",
      "Iter 0258 | Time 55.5412(52.9536) | Bit/dim 4.5285(4.7071) | Xent 1.7735(1.8311) | Loss 5.4153(5.6227) | Error 0.6151(0.6398) Steps 628(608.54) | Grad Norm 6.1528(5.9534) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0043 | Time 23.9236, Epoch Time 365.6837(342.9328), Bit/dim 4.5340(best: 4.5318), Xent 1.7496, Loss 5.4088, Error 0.6232(best: 0.5931)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0259 | Time 57.3422(53.0852) | Bit/dim 4.5369(4.7020) | Xent 1.7447(1.8285) | Loss 5.4093(5.6163) | Error 0.6154(0.6391) Steps 622(608.94) | Grad Norm 5.7439(5.9471) | Total Time 14.00(14.00)\n",
      "Iter 0260 | Time 58.2044(53.2388) | Bit/dim 4.4939(4.6958) | Xent 1.7904(1.8273) | Loss 5.3891(5.6094) | Error 0.6312(0.6389) Steps 628(609.51) | Grad Norm 8.2595(6.0165) | Total Time 14.00(14.00)\n",
      "Iter 0261 | Time 55.7943(53.3155) | Bit/dim 4.5859(4.6925) | Xent 1.8058(1.8267) | Loss 5.4888(5.6058) | Error 0.6406(0.6389) Steps 682(611.69) | Grad Norm 11.2892(6.1747) | Total Time 14.00(14.00)\n",
      "Iter 0262 | Time 55.3836(53.3775) | Bit/dim 4.5152(4.6872) | Xent 1.7717(1.8250) | Loss 5.4010(5.5997) | Error 0.6261(0.6385) Steps 640(612.54) | Grad Norm 8.9655(6.2584) | Total Time 14.00(14.00)\n",
      "Iter 0263 | Time 54.1116(53.3996) | Bit/dim 4.5139(4.6820) | Xent 1.7167(1.8218) | Loss 5.3723(5.5929) | Error 0.6106(0.6377) Steps 634(613.18) | Grad Norm 4.7829(6.2141) | Total Time 14.00(14.00)\n",
      "Iter 0264 | Time 53.5266(53.4034) | Bit/dim 4.4905(4.6762) | Xent 1.6978(1.8181) | Loss 5.3394(5.5852) | Error 0.5960(0.6364) Steps 634(613.81) | Grad Norm 3.7976(6.1416) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0044 | Time 23.2641, Epoch Time 373.1421(343.8391), Bit/dim 4.5078(best: 4.5318), Xent 1.6857, Loss 5.3506, Error 0.6035(best: 0.5931)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0265 | Time 54.7490(53.4437) | Bit/dim 4.5057(4.6711) | Xent 1.7112(1.8149) | Loss 5.3613(5.5785) | Error 0.6049(0.6355) Steps 622(614.05) | Grad Norm 5.1741(6.1126) | Total Time 14.00(14.00)\n",
      "Iter 0266 | Time 61.0338(53.6714) | Bit/dim 4.4919(4.6657) | Xent 1.6905(1.8111) | Loss 5.3372(5.5713) | Error 0.5955(0.6343) Steps 634(614.65) | Grad Norm 5.5911(6.0969) | Total Time 14.00(14.00)\n",
      "Iter 0267 | Time 56.0670(53.7433) | Bit/dim 4.4827(4.6602) | Xent 1.6845(1.8073) | Loss 5.3249(5.5639) | Error 0.5881(0.6329) Steps 664(616.13) | Grad Norm 3.6512(6.0236) | Total Time 14.00(14.00)\n",
      "Iter 0268 | Time 54.8496(53.7765) | Bit/dim 4.4654(4.6544) | Xent 1.6881(1.8038) | Loss 5.3094(5.5563) | Error 0.5959(0.6318) Steps 676(617.93) | Grad Norm 3.7643(5.9558) | Total Time 14.00(14.00)\n",
      "Iter 0269 | Time 56.8481(53.8686) | Bit/dim 4.4608(4.6486) | Xent 1.7018(1.8007) | Loss 5.3117(5.5489) | Error 0.6038(0.6310) Steps 652(618.95) | Grad Norm 6.3993(5.9691) | Total Time 14.00(14.00)\n",
      "Iter 0270 | Time 56.7651(53.9555) | Bit/dim 4.4601(4.6429) | Xent 1.6728(1.7969) | Loss 5.2965(5.5414) | Error 0.5988(0.6300) Steps 676(620.66) | Grad Norm 5.5622(5.9569) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0045 | Time 24.2736, Epoch Time 380.2969(344.9328), Bit/dim 4.4467(best: 4.5078), Xent 1.6359, Loss 5.2646, Error 0.5781(best: 0.5931)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0271 | Time 56.4822(54.0313) | Bit/dim 4.4514(4.6372) | Xent 1.6392(1.7921) | Loss 5.2710(5.5332) | Error 0.5813(0.6285) Steps 640(621.24) | Grad Norm 3.4872(5.8828) | Total Time 14.00(14.00)\n",
      "Iter 0272 | Time 59.3087(54.1897) | Bit/dim 4.4297(4.6310) | Xent 1.6505(1.7879) | Loss 5.2550(5.5249) | Error 0.5800(0.6271) Steps 640(621.80) | Grad Norm 4.9998(5.8563) | Total Time 14.00(14.00)\n",
      "Iter 0273 | Time 61.8911(54.4207) | Bit/dim 4.4525(4.6256) | Xent 1.6825(1.7847) | Loss 5.2938(5.5180) | Error 0.5864(0.6259) Steps 658(622.89) | Grad Norm 9.4828(5.9651) | Total Time 14.00(14.00)\n",
      "Iter 0274 | Time 54.6448(54.4274) | Bit/dim 4.4258(4.6196) | Xent 1.6831(1.7817) | Loss 5.2673(5.5104) | Error 0.5996(0.6251) Steps 634(623.22) | Grad Norm 7.1589(6.0009) | Total Time 14.00(14.00)\n",
      "Iter 0275 | Time 57.2755(54.5129) | Bit/dim 4.4227(4.6137) | Xent 1.6563(1.7779) | Loss 5.2508(5.5027) | Error 0.5869(0.6239) Steps 670(624.63) | Grad Norm 4.9131(5.9683) | Total Time 14.00(14.00)\n",
      "Iter 0276 | Time 60.8187(54.7020) | Bit/dim 4.4278(4.6081) | Xent 1.6682(1.7746) | Loss 5.2619(5.4954) | Error 0.5931(0.6230) Steps 640(625.09) | Grad Norm 8.0550(6.0309) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0046 | Time 24.1253, Epoch Time 390.0047(346.2850), Bit/dim 4.4210(best: 4.4467), Xent 1.6249, Loss 5.2335, Error 0.5798(best: 0.5781)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0277 | Time 59.6676(54.8510) | Bit/dim 4.4151(4.6023) | Xent 1.6454(1.7707) | Loss 5.2378(5.4877) | Error 0.5829(0.6218) Steps 658(626.07) | Grad Norm 6.5047(6.0451) | Total Time 14.00(14.00)\n",
      "Iter 0278 | Time 58.3176(54.9550) | Bit/dim 4.4282(4.5971) | Xent 1.6272(1.7664) | Loss 5.2418(5.4803) | Error 0.5759(0.6204) Steps 658(627.03) | Grad Norm 5.2025(6.0198) | Total Time 14.00(14.00)\n",
      "Iter 0279 | Time 55.4871(54.9710) | Bit/dim 4.3837(4.5907) | Xent 1.6541(1.7631) | Loss 5.2107(5.4722) | Error 0.5882(0.6194) Steps 658(627.96) | Grad Norm 5.9205(6.0168) | Total Time 14.00(14.00)\n",
      "Iter 0280 | Time 54.5807(54.9593) | Bit/dim 4.3907(4.5847) | Xent 1.6420(1.7594) | Loss 5.2117(5.4644) | Error 0.5862(0.6185) Steps 634(628.14) | Grad Norm 5.3192(5.9959) | Total Time 14.00(14.00)\n",
      "Iter 0281 | Time 58.5156(55.0659) | Bit/dim 4.4231(4.5799) | Xent 1.6173(1.7552) | Loss 5.2317(5.4574) | Error 0.5606(0.6167) Steps 658(629.04) | Grad Norm 5.0754(5.9683) | Total Time 14.00(14.00)\n",
      "Iter 0282 | Time 57.1212(55.1276) | Bit/dim 4.3702(4.5736) | Xent 1.6246(1.7513) | Loss 5.1825(5.4492) | Error 0.5699(0.6153) Steps 634(629.19) | Grad Norm 4.2377(5.9164) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0047 | Time 23.8080, Epoch Time 383.2803(347.3948), Bit/dim 4.3805(best: 4.4210), Xent 1.5876, Loss 5.1743, Error 0.5609(best: 0.5781)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0283 | Time 53.7737(55.0870) | Bit/dim 4.3828(4.5678) | Xent 1.6184(1.7473) | Loss 5.1921(5.4415) | Error 0.5717(0.6140) Steps 652(629.87) | Grad Norm 5.3699(5.9000) | Total Time 14.00(14.00)\n",
      "Iter 0284 | Time 59.3956(55.2162) | Bit/dim 4.4163(4.5633) | Xent 1.6046(1.7430) | Loss 5.2186(5.4348) | Error 0.5696(0.6127) Steps 658(630.72) | Grad Norm 7.3539(5.9436) | Total Time 14.00(14.00)\n",
      "Iter 0285 | Time 56.0754(55.2420) | Bit/dim 4.3617(4.5573) | Xent 1.6350(1.7397) | Loss 5.1792(5.4271) | Error 0.5836(0.6118) Steps 658(631.53) | Grad Norm 7.4901(5.9900) | Total Time 14.00(14.00)\n",
      "Iter 0286 | Time 52.7161(55.1662) | Bit/dim 4.3744(4.5518) | Xent 1.6523(1.7371) | Loss 5.2005(5.4203) | Error 0.5869(0.6111) Steps 616(631.07) | Grad Norm 8.7254(6.0721) | Total Time 14.00(14.00)\n",
      "Iter 0287 | Time 58.2912(55.2600) | Bit/dim 4.3947(4.5471) | Xent 1.7147(1.7365) | Loss 5.2520(5.4153) | Error 0.6048(0.6109) Steps 682(632.60) | Grad Norm 12.7268(6.2717) | Total Time 14.00(14.00)\n",
      "Iter 0288 | Time 58.0652(55.3441) | Bit/dim 4.3680(4.5417) | Xent 1.7204(1.7360) | Loss 5.2282(5.4097) | Error 0.6055(0.6107) Steps 700(634.62) | Grad Norm 11.9224(6.4412) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0048 | Time 24.2697, Epoch Time 378.2002(348.3190), Bit/dim 4.3628(best: 4.3805), Xent 1.6004, Loss 5.1630, Error 0.5650(best: 0.5609)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0289 | Time 55.2159(55.3403) | Bit/dim 4.3534(4.5360) | Xent 1.6360(1.7330) | Loss 5.1714(5.4025) | Error 0.5842(0.6099) Steps 646(634.96) | Grad Norm 5.4800(6.4124) | Total Time 14.00(14.00)\n",
      "Iter 0290 | Time 57.9279(55.4179) | Bit/dim 4.3525(4.5305) | Xent 1.6668(1.7310) | Loss 5.1860(5.3960) | Error 0.5820(0.6091) Steps 640(635.11) | Grad Norm 8.9973(6.4899) | Total Time 14.00(14.00)\n",
      "Iter 0291 | Time 55.1927(55.4112) | Bit/dim 4.4104(4.5269) | Xent 1.6379(1.7282) | Loss 5.2294(5.3910) | Error 0.5733(0.6080) Steps 628(634.90) | Grad Norm 8.9582(6.5640) | Total Time 14.00(14.00)\n",
      "Iter 0292 | Time 55.9939(55.4287) | Bit/dim 4.3819(4.5226) | Xent 1.7845(1.7299) | Loss 5.2741(5.3875) | Error 0.6259(0.6085) Steps 628(634.69) | Grad Norm 16.0302(6.8480) | Total Time 14.00(14.00)\n",
      "Iter 0293 | Time 59.4969(55.5507) | Bit/dim 4.6119(4.5253) | Xent 1.9578(1.7367) | Loss 5.5908(5.3936) | Error 0.6721(0.6104) Steps 700(636.65) | Grad Norm 15.4371(7.1056) | Total Time 14.00(14.00)\n",
      "Iter 0294 | Time 58.7948(55.6480) | Bit/dim 4.4579(4.5232) | Xent 1.8581(1.7404) | Loss 5.3870(5.3934) | Error 0.6532(0.6117) Steps 706(638.73) | Grad Norm 9.4584(7.1762) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0049 | Time 25.1492, Epoch Time 383.7582(349.3822), Bit/dim 4.4673(best: 4.3628), Xent 1.7744, Loss 5.3546, Error 0.6337(best: 0.5609)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0295 | Time 60.2989(55.7875) | Bit/dim 4.4608(4.5214) | Xent 1.8141(1.7426) | Loss 5.3678(5.3926) | Error 0.6390(0.6125) Steps 718(641.11) | Grad Norm 11.0756(7.2932) | Total Time 14.00(14.00)\n",
      "Iter 0296 | Time 56.2066(55.8001) | Bit/dim 4.4343(4.5188) | Xent 1.7546(1.7429) | Loss 5.3116(5.3902) | Error 0.6275(0.6130) Steps 700(642.87) | Grad Norm 4.2381(7.2016) | Total Time 14.00(14.00)\n",
      "Iter 0297 | Time 57.2147(55.8426) | Bit/dim 4.4619(4.5170) | Xent 1.7872(1.7443) | Loss 5.3555(5.3892) | Error 0.6281(0.6134) Steps 676(643.87) | Grad Norm 10.1460(7.2899) | Total Time 14.00(14.00)\n",
      "Iter 0298 | Time 56.1853(55.8528) | Bit/dim 4.4505(4.5150) | Xent 1.7365(1.7440) | Loss 5.3187(5.3871) | Error 0.6149(0.6135) Steps 670(644.65) | Grad Norm 6.5103(7.2665) | Total Time 14.00(14.00)\n",
      "Iter 0299 | Time 57.1239(55.8910) | Bit/dim 4.4110(4.5119) | Xent 1.6842(1.7422) | Loss 5.2531(5.3830) | Error 0.6079(0.6133) Steps 676(645.59) | Grad Norm 3.2994(7.1475) | Total Time 14.00(14.00)\n",
      "Iter 0300 | Time 58.5505(55.9708) | Bit/dim 4.4265(4.5094) | Xent 1.7339(1.7420) | Loss 5.2935(5.3804) | Error 0.6215(0.6136) Steps 658(645.97) | Grad Norm 10.5583(7.2498) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0050 | Time 24.2175, Epoch Time 385.3358(350.4608), Bit/dim 4.3959(best: 4.3628), Xent 1.7303, Loss 5.2611, Error 0.6094(best: 0.5609)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0301 | Time 58.2309(56.0386) | Bit/dim 4.4079(4.5063) | Xent 1.7365(1.7418) | Loss 5.2762(5.3772) | Error 0.6118(0.6135) Steps 646(645.97) | Grad Norm 9.3752(7.3136) | Total Time 14.00(14.00)\n",
      "Iter 0302 | Time 56.5643(56.0543) | Bit/dim 4.3775(4.5025) | Xent 1.6462(1.7390) | Loss 5.2006(5.3719) | Error 0.5845(0.6126) Steps 676(646.87) | Grad Norm 2.1624(7.1590) | Total Time 14.00(14.00)\n",
      "Iter 0303 | Time 61.3751(56.2140) | Bit/dim 4.4034(4.4995) | Xent 1.7253(1.7385) | Loss 5.2661(5.3688) | Error 0.6066(0.6125) Steps 628(646.30) | Grad Norm 8.9926(7.2140) | Total Time 14.00(14.00)\n",
      "Iter 0304 | Time 54.2169(56.1540) | Bit/dim 4.3692(4.4956) | Xent 1.7109(1.7377) | Loss 5.2246(5.3644) | Error 0.6015(0.6121) Steps 646(646.29) | Grad Norm 5.7767(7.1709) | Total Time 14.00(14.00)\n",
      "Iter 0305 | Time 55.8588(56.1452) | Bit/dim 4.3477(4.4911) | Xent 1.6843(1.7361) | Loss 5.1899(5.3592) | Error 0.5965(0.6117) Steps 616(645.38) | Grad Norm 4.6309(7.0947) | Total Time 14.00(14.00)\n",
      "Iter 0306 | Time 52.5979(56.0388) | Bit/dim 4.3580(4.4871) | Xent 1.6785(1.7344) | Loss 5.1972(5.3543) | Error 0.5941(0.6111) Steps 616(644.50) | Grad Norm 6.4060(7.0741) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0051 | Time 23.5676, Epoch Time 377.9506(351.2855), Bit/dim 4.3366(best: 4.3628), Xent 1.6170, Loss 5.1451, Error 0.5704(best: 0.5609)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0307 | Time 54.8963(56.0045) | Bit/dim 4.3382(4.4827) | Xent 1.6468(1.7318) | Loss 5.1616(5.3486) | Error 0.5786(0.6102) Steps 622(643.83) | Grad Norm 2.5437(6.9382) | Total Time 14.00(14.00)\n",
      "Iter 0308 | Time 57.8285(56.0592) | Bit/dim 4.3326(4.4782) | Xent 1.6446(1.7291) | Loss 5.1549(5.3427) | Error 0.5839(0.6094) Steps 634(643.53) | Grad Norm 6.0409(6.9112) | Total Time 14.00(14.00)\n",
      "Iter 0309 | Time 56.0928(56.0602) | Bit/dim 4.3212(4.4735) | Xent 1.6124(1.7256) | Loss 5.1274(5.3363) | Error 0.5742(0.6083) Steps 640(643.43) | Grad Norm 2.3920(6.7757) | Total Time 14.00(14.00)\n",
      "Iter 0310 | Time 55.8108(56.0527) | Bit/dim 4.3164(4.4688) | Xent 1.6341(1.7229) | Loss 5.1334(5.3302) | Error 0.5802(0.6075) Steps 640(643.32) | Grad Norm 5.6261(6.7412) | Total Time 14.00(14.00)\n",
      "Iter 0311 | Time 53.5632(55.9780) | Bit/dim 4.3260(4.4645) | Xent 1.5972(1.7191) | Loss 5.1246(5.3240) | Error 0.5694(0.6063) Steps 640(643.22) | Grad Norm 4.8202(6.6835) | Total Time 14.00(14.00)\n",
      "Iter 0312 | Time 57.8592(56.0345) | Bit/dim 4.3018(4.4596) | Xent 1.5954(1.7154) | Loss 5.0995(5.3173) | Error 0.5671(0.6052) Steps 652(643.49) | Grad Norm 3.8762(6.5993) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0052 | Time 23.6603, Epoch Time 375.5615(352.0137), Bit/dim 4.2984(best: 4.3366), Xent 1.5742, Loss 5.0855, Error 0.5581(best: 0.5609)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0313 | Time 53.8245(55.9682) | Bit/dim 4.3033(4.4549) | Xent 1.6065(1.7121) | Loss 5.1065(5.3110) | Error 0.5702(0.6041) Steps 646(643.56) | Grad Norm 4.4524(6.5349) | Total Time 14.00(14.00)\n",
      "Iter 0314 | Time 54.6224(55.9278) | Bit/dim 4.2953(4.4501) | Xent 1.5639(1.7077) | Loss 5.0772(5.3040) | Error 0.5466(0.6024) Steps 652(643.82) | Grad Norm 2.2374(6.4060) | Total Time 14.00(14.00)\n",
      "Iter 0315 | Time 56.9674(55.9590) | Bit/dim 4.2845(4.4451) | Xent 1.5678(1.7035) | Loss 5.0684(5.2969) | Error 0.5540(0.6009) Steps 652(644.06) | Grad Norm 4.3971(6.3457) | Total Time 14.00(14.00)\n",
      "Iter 0316 | Time 60.5459(56.0966) | Bit/dim 4.2900(4.4405) | Xent 1.5890(1.7001) | Loss 5.0845(5.2905) | Error 0.5641(0.5998) Steps 682(645.20) | Grad Norm 5.5918(6.3231) | Total Time 14.00(14.00)\n",
      "Iter 0317 | Time 55.8596(56.0895) | Bit/dim 4.2697(4.4354) | Xent 1.5708(1.6962) | Loss 5.0551(5.2835) | Error 0.5563(0.5985) Steps 640(645.04) | Grad Norm 3.1282(6.2273) | Total Time 14.00(14.00)\n",
      "Iter 0318 | Time 55.8127(56.0812) | Bit/dim 4.2730(4.4305) | Xent 1.5399(1.6915) | Loss 5.0430(5.2762) | Error 0.5473(0.5970) Steps 640(644.89) | Grad Norm 2.1431(6.1047) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0053 | Time 24.7604, Epoch Time 378.1538(352.7979), Bit/dim 4.2687(best: 4.2984), Xent 1.5422, Loss 5.0398, Error 0.5522(best: 0.5581)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0319 | Time 59.4474(56.1822) | Bit/dim 4.2698(4.4257) | Xent 1.5757(1.6880) | Loss 5.0576(5.2697) | Error 0.5604(0.5959) Steps 640(644.75) | Grad Norm 5.3123(6.0810) | Total Time 14.00(14.00)\n",
      "Iter 0320 | Time 60.0056(56.2969) | Bit/dim 4.2725(4.4211) | Xent 1.5784(1.6847) | Loss 5.0617(5.2634) | Error 0.5636(0.5949) Steps 694(646.22) | Grad Norm 6.8898(6.1052) | Total Time 14.00(14.00)\n",
      "Iter 0321 | Time 56.7969(56.3119) | Bit/dim 4.2707(4.4166) | Xent 1.5755(1.6815) | Loss 5.0585(5.2573) | Error 0.5509(0.5936) Steps 664(646.76) | Grad Norm 6.7855(6.1256) | Total Time 14.00(14.00)\n",
      "Iter 0322 | Time 56.8251(56.3273) | Bit/dim 4.3095(4.4134) | Xent 1.6168(1.6795) | Loss 5.1179(5.2531) | Error 0.5699(0.5929) Steps 682(647.81) | Grad Norm 8.6274(6.2007) | Total Time 14.00(14.00)\n",
      "Iter 0323 | Time 59.9971(56.4374) | Bit/dim 4.2516(4.4085) | Xent 1.6040(1.6773) | Loss 5.0536(5.2471) | Error 0.5666(0.5921) Steps 652(647.94) | Grad Norm 11.0834(6.3472) | Total Time 14.00(14.00)\n",
      "Iter 0324 | Time 57.8738(56.4805) | Bit/dim 4.2574(4.4040) | Xent 1.5990(1.6749) | Loss 5.0569(5.2414) | Error 0.5696(0.5914) Steps 688(649.14) | Grad Norm 10.7679(6.4798) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0054 | Time 24.8617, Epoch Time 391.7115(353.9654), Bit/dim 4.2759(best: 4.2687), Xent 1.5461, Loss 5.0489, Error 0.5433(best: 0.5522)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0325 | Time 55.7803(56.4595) | Bit/dim 4.2731(4.4000) | Xent 1.5628(1.6715) | Loss 5.0545(5.2358) | Error 0.5561(0.5904) Steps 688(650.31) | Grad Norm 4.3709(6.4165) | Total Time 14.00(14.00)\n",
      "Iter 0326 | Time 61.8537(56.6213) | Bit/dim 4.2263(4.3948) | Xent 1.5855(1.6690) | Loss 5.0191(5.2293) | Error 0.5661(0.5896) Steps 706(651.98) | Grad Norm 6.5910(6.4218) | Total Time 14.00(14.00)\n",
      "Iter 0327 | Time 56.3329(56.6126) | Bit/dim 4.2453(4.3903) | Xent 1.5839(1.6664) | Loss 5.0373(5.2235) | Error 0.5620(0.5888) Steps 664(652.34) | Grad Norm 7.6182(6.4577) | Total Time 14.00(14.00)\n",
      "Iter 0328 | Time 55.8958(56.5911) | Bit/dim 4.2330(4.3856) | Xent 1.5221(1.6621) | Loss 4.9940(5.2167) | Error 0.5464(0.5875) Steps 628(651.61) | Grad Norm 2.8679(6.3500) | Total Time 14.00(14.00)\n",
      "Iter 0329 | Time 56.0366(56.5745) | Bit/dim 4.2400(4.3813) | Xent 1.5857(1.6598) | Loss 5.0329(5.2111) | Error 0.5667(0.5869) Steps 646(651.44) | Grad Norm 8.2855(6.4080) | Total Time 14.00(14.00)\n",
      "Iter 0330 | Time 58.4135(56.6297) | Bit/dim 4.2154(4.3763) | Xent 1.5759(1.6573) | Loss 5.0033(5.2049) | Error 0.5671(0.5863) Steps 646(651.28) | Grad Norm 5.9337(6.3938) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0055 | Time 24.3928, Epoch Time 384.4018(354.8784), Bit/dim 4.2305(best: 4.2687), Xent 1.5719, Loss 5.0164, Error 0.5610(best: 0.5433)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0331 | Time 56.8480(56.6362) | Bit/dim 4.2324(4.3720) | Xent 1.5923(1.6553) | Loss 5.0285(5.1996) | Error 0.5661(0.5857) Steps 628(650.58) | Grad Norm 8.5513(6.4585) | Total Time 14.00(14.00)\n",
      "Iter 0332 | Time 59.5238(56.7228) | Bit/dim 4.2286(4.3677) | Xent 1.5928(1.6534) | Loss 5.0250(5.1944) | Error 0.5659(0.5851) Steps 640(650.26) | Grad Norm 9.2455(6.5421) | Total Time 14.00(14.00)\n",
      "Iter 0333 | Time 56.9011(56.7282) | Bit/dim 4.2211(4.3633) | Xent 1.5923(1.6516) | Loss 5.0173(5.1891) | Error 0.5583(0.5843) Steps 670(650.85) | Grad Norm 9.8664(6.6419) | Total Time 14.00(14.00)\n",
      "Iter 0334 | Time 56.5356(56.7224) | Bit/dim 4.2201(4.3590) | Xent 1.5539(1.6487) | Loss 4.9970(5.1833) | Error 0.5534(0.5834) Steps 682(651.79) | Grad Norm 4.0970(6.5655) | Total Time 14.00(14.00)\n",
      "Iter 0335 | Time 60.8410(56.8460) | Bit/dim 4.2140(4.3546) | Xent 1.5443(1.6455) | Loss 4.9862(5.1774) | Error 0.5416(0.5821) Steps 664(652.15) | Grad Norm 6.9700(6.5776) | Total Time 14.00(14.00)\n",
      "Iter 0336 | Time 60.5677(56.9576) | Bit/dim 4.2089(4.3502) | Xent 1.5976(1.6441) | Loss 5.0077(5.1723) | Error 0.5601(0.5815) Steps 664(652.51) | Grad Norm 8.3662(6.6313) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0056 | Time 24.8446, Epoch Time 391.8006(355.9861), Bit/dim 4.1985(best: 4.2305), Xent 1.5443, Loss 4.9706, Error 0.5523(best: 0.5433)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0337 | Time 58.4987(57.0039) | Bit/dim 4.1923(4.3455) | Xent 1.5584(1.6415) | Loss 4.9715(5.1663) | Error 0.5577(0.5808) Steps 658(652.67) | Grad Norm 6.5236(6.6281) | Total Time 14.00(14.00)\n",
      "Iter 0338 | Time 57.1427(57.0080) | Bit/dim 4.2091(4.3414) | Xent 1.5517(1.6388) | Loss 4.9850(5.1608) | Error 0.5537(0.5799) Steps 652(652.65) | Grad Norm 6.2510(6.6168) | Total Time 14.00(14.00)\n",
      "Iter 0339 | Time 58.0484(57.0392) | Bit/dim 4.3369(4.3413) | Xent 1.5723(1.6368) | Loss 5.1230(5.1597) | Error 0.5580(0.5793) Steps 706(654.25) | Grad Norm 5.6206(6.5869) | Total Time 14.00(14.00)\n",
      "Iter 0340 | Time 58.7020(57.0891) | Bit/dim 4.1990(4.3370) | Xent 1.5490(1.6342) | Loss 4.9735(5.1541) | Error 0.5573(0.5786) Steps 646(654.01) | Grad Norm 4.7527(6.5319) | Total Time 14.00(14.00)\n",
      "Iter 0341 | Time 61.8321(57.2314) | Bit/dim 4.2607(4.3347) | Xent 1.6467(1.6346) | Loss 5.0840(5.1520) | Error 0.5709(0.5784) Steps 652(653.95) | Grad Norm 12.4539(6.7095) | Total Time 14.00(14.00)\n",
      "Iter 0342 | Time 54.3701(57.1456) | Bit/dim 4.3618(4.3355) | Xent 1.6760(1.6358) | Loss 5.1998(5.1535) | Error 0.5914(0.5788) Steps 664(654.25) | Grad Norm 13.3357(6.9083) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0057 | Time 24.1034, Epoch Time 388.8074(356.9708), Bit/dim 4.3136(best: 4.1985), Xent 1.6510, Loss 5.1391, Error 0.6010(best: 0.5433)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0343 | Time 59.8194(57.2258) | Bit/dim 4.3173(4.3350) | Xent 1.6505(1.6363) | Loss 5.1425(5.1531) | Error 0.5915(0.5792) Steps 682(655.08) | Grad Norm 8.6191(6.9596) | Total Time 14.00(14.00)\n",
      "Iter 0344 | Time 57.1616(57.2239) | Bit/dim 4.2511(4.3325) | Xent 1.5891(1.6349) | Loss 5.0456(5.1499) | Error 0.5557(0.5785) Steps 640(654.63) | Grad Norm 7.7064(6.9820) | Total Time 14.00(14.00)\n",
      "Iter 0345 | Time 55.5902(57.1748) | Bit/dim 4.2798(4.3309) | Xent 1.6636(1.6357) | Loss 5.1116(5.1488) | Error 0.5865(0.5787) Steps 646(654.37) | Grad Norm 10.9671(7.1016) | Total Time 14.00(14.00)\n",
      "Iter 0346 | Time 53.2297(57.0565) | Bit/dim 4.2738(4.3292) | Xent 1.6102(1.6349) | Loss 5.0789(5.1467) | Error 0.5716(0.5785) Steps 646(654.12) | Grad Norm 6.3268(7.0783) | Total Time 14.00(14.00)\n",
      "Iter 0347 | Time 55.3702(57.0059) | Bit/dim 4.2661(4.3273) | Xent 1.5735(1.6331) | Loss 5.0528(5.1438) | Error 0.5631(0.5780) Steps 616(652.97) | Grad Norm 3.6455(6.9753) | Total Time 14.00(14.00)\n",
      "Iter 0348 | Time 58.0160(57.0362) | Bit/dim 4.2618(4.3253) | Xent 1.5844(1.6316) | Loss 5.0540(5.1411) | Error 0.5641(0.5776) Steps 640(652.59) | Grad Norm 5.6111(6.9344) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0058 | Time 23.1507, Epoch Time 377.7912(357.5954), Bit/dim 4.2242(best: 4.1985), Xent 1.5651, Loss 5.0068, Error 0.5556(best: 0.5433)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0349 | Time 55.6449(56.9945) | Bit/dim 4.2146(4.3220) | Xent 1.5849(1.6302) | Loss 5.0070(5.1371) | Error 0.5676(0.5773) Steps 616(651.49) | Grad Norm 4.4028(6.8585) | Total Time 14.00(14.00)\n",
      "Iter 0350 | Time 56.7493(56.9871) | Bit/dim 4.2320(4.3193) | Xent 1.5882(1.6290) | Loss 5.0261(5.1338) | Error 0.5745(0.5772) Steps 628(650.78) | Grad Norm 5.9245(6.8305) | Total Time 14.00(14.00)\n",
      "Iter 0351 | Time 56.3564(56.9682) | Bit/dim 4.2271(4.3165) | Xent 1.5627(1.6270) | Loss 5.0085(5.1300) | Error 0.5515(0.5765) Steps 598(649.20) | Grad Norm 4.8386(6.7707) | Total Time 14.00(14.00)\n",
      "Iter 0352 | Time 53.7920(56.8729) | Bit/dim 4.2143(4.3135) | Xent 1.5199(1.6238) | Loss 4.9743(5.1254) | Error 0.5451(0.5755) Steps 592(647.48) | Grad Norm 3.3597(6.6684) | Total Time 14.00(14.00)\n",
      "Iter 0353 | Time 57.1065(56.8799) | Bit/dim 4.1972(4.3100) | Xent 1.5210(1.6207) | Loss 4.9577(5.1203) | Error 0.5444(0.5746) Steps 622(646.72) | Grad Norm 4.8116(6.6127) | Total Time 14.00(14.00)\n",
      "Iter 0354 | Time 58.4955(56.9284) | Bit/dim 4.1948(4.3065) | Xent 1.5623(1.6189) | Loss 4.9760(5.1160) | Error 0.5550(0.5740) Steps 622(645.98) | Grad Norm 3.2213(6.5109) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0059 | Time 23.7184, Epoch Time 377.8014(358.2015), Bit/dim 4.1889(best: 4.1985), Xent 1.5013, Loss 4.9395, Error 0.5343(best: 0.5433)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0355 | Time 57.0770(56.9328) | Bit/dim 4.1938(4.3031) | Xent 1.5459(1.6168) | Loss 4.9668(5.1115) | Error 0.5490(0.5732) Steps 634(645.62) | Grad Norm 5.0817(6.4680) | Total Time 14.00(14.00)\n",
      "Iter 0356 | Time 56.5928(56.9226) | Bit/dim 4.1858(4.2996) | Xent 1.5486(1.6147) | Loss 4.9600(5.1070) | Error 0.5430(0.5723) Steps 634(645.27) | Grad Norm 6.2948(6.4628) | Total Time 14.00(14.00)\n",
      "Iter 0357 | Time 56.7281(56.9168) | Bit/dim 4.1682(4.2957) | Xent 1.4866(1.6109) | Loss 4.9115(5.1011) | Error 0.5302(0.5711) Steps 628(644.75) | Grad Norm 3.0512(6.3605) | Total Time 14.00(14.00)\n",
      "Iter 0358 | Time 54.5920(56.8471) | Bit/dim 4.1711(4.2919) | Xent 1.4933(1.6073) | Loss 4.9178(5.0956) | Error 0.5449(0.5703) Steps 616(643.89) | Grad Norm 3.2741(6.2679) | Total Time 14.00(14.00)\n",
      "Iter 0359 | Time 60.8318(56.9666) | Bit/dim 4.1564(4.2879) | Xent 1.5065(1.6043) | Loss 4.9096(5.0900) | Error 0.5359(0.5693) Steps 628(643.41) | Grad Norm 5.6530(6.2495) | Total Time 14.00(14.00)\n",
      "Iter 0360 | Time 59.6277(57.0464) | Bit/dim 4.1634(4.2841) | Xent 1.5278(1.6020) | Loss 4.9273(5.0852) | Error 0.5424(0.5685) Steps 640(643.31) | Grad Norm 6.5319(6.2579) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0060 | Time 23.1164, Epoch Time 384.6581(358.9952), Bit/dim 4.1613(best: 4.1889), Xent 1.5114, Loss 4.9170, Error 0.5413(best: 0.5343)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0361 | Time 60.2996(57.1440) | Bit/dim 4.1596(4.2804) | Xent 1.5333(1.6000) | Loss 4.9262(5.0804) | Error 0.5553(0.5681) Steps 622(642.67) | Grad Norm 7.9810(6.3096) | Total Time 14.00(14.00)\n",
      "Iter 0362 | Time 58.9346(57.1977) | Bit/dim 4.1692(4.2771) | Xent 1.5870(1.5996) | Loss 4.9627(5.0769) | Error 0.5671(0.5680) Steps 646(642.77) | Grad Norm 11.6679(6.4704) | Total Time 14.00(14.00)\n",
      "Iter 0363 | Time 59.5963(57.2697) | Bit/dim 4.1682(4.2738) | Xent 1.5613(1.5984) | Loss 4.9488(5.0730) | Error 0.5575(0.5677) Steps 652(643.05) | Grad Norm 9.5067(6.5615) | Total Time 14.00(14.00)\n",
      "Iter 0364 | Time 54.9920(57.2014) | Bit/dim 4.1639(4.2705) | Xent 1.5348(1.5965) | Loss 4.9313(5.0688) | Error 0.5494(0.5672) Steps 634(642.78) | Grad Norm 6.6386(6.5638) | Total Time 14.00(14.00)\n",
      "Iter 0365 | Time 58.7362(57.2474) | Bit/dim 4.1666(4.2674) | Xent 1.6102(1.5969) | Loss 4.9717(5.0659) | Error 0.5737(0.5674) Steps 652(643.05) | Grad Norm 8.9864(6.6365) | Total Time 14.00(14.00)\n",
      "Iter 0366 | Time 57.7160(57.2615) | Bit/dim 4.1468(4.2638) | Xent 1.5275(1.5948) | Loss 4.9106(5.0612) | Error 0.5497(0.5668) Steps 640(642.96) | Grad Norm 6.3456(6.6277) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0061 | Time 23.3269, Epoch Time 389.3848(359.9069), Bit/dim 4.1436(best: 4.1613), Xent 1.5072, Loss 4.8972, Error 0.5414(best: 0.5343)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0367 | Time 57.5001(57.2686) | Bit/dim 4.1487(4.2603) | Xent 1.5248(1.5927) | Loss 4.9111(5.0567) | Error 0.5525(0.5664) Steps 640(642.87) | Grad Norm 4.2872(6.5575) | Total Time 14.00(14.00)\n",
      "Iter 0368 | Time 57.8994(57.2876) | Bit/dim 4.1327(4.2565) | Xent 1.5310(1.5909) | Loss 4.8982(5.0519) | Error 0.5469(0.5658) Steps 646(642.97) | Grad Norm 7.5188(6.5864) | Total Time 14.00(14.00)\n",
      "Iter 0369 | Time 56.0461(57.2503) | Bit/dim 4.1612(4.2536) | Xent 1.5548(1.5898) | Loss 4.9385(5.0485) | Error 0.5516(0.5654) Steps 628(642.52) | Grad Norm 8.6415(6.6480) | Total Time 14.00(14.00)\n",
      "Iter 0370 | Time 53.9081(57.1500) | Bit/dim 4.1428(4.2503) | Xent 1.5063(1.5873) | Loss 4.8960(5.0440) | Error 0.5406(0.5646) Steps 634(642.26) | Grad Norm 6.6256(6.6473) | Total Time 14.00(14.00)\n",
      "Iter 0371 | Time 57.5380(57.1617) | Bit/dim 4.1221(4.2465) | Xent 1.4973(1.5846) | Loss 4.8708(5.0388) | Error 0.5301(0.5636) Steps 652(642.55) | Grad Norm 2.4434(6.5212) | Total Time 14.00(14.00)\n",
      "Iter 0372 | Time 59.5368(57.2329) | Bit/dim 4.1252(4.2428) | Xent 1.4953(1.5819) | Loss 4.8729(5.0338) | Error 0.5400(0.5629) Steps 646(642.66) | Grad Norm 4.7826(6.4691) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0062 | Time 23.7918, Epoch Time 381.6548(360.5594), Bit/dim 4.1187(best: 4.1436), Xent 1.4896, Loss 4.8635, Error 0.5358(best: 0.5343)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0373 | Time 59.9254(57.3137) | Bit/dim 4.1170(4.2390) | Xent 1.5196(1.5800) | Loss 4.8768(5.0291) | Error 0.5366(0.5621) Steps 652(642.94) | Grad Norm 6.0810(6.4574) | Total Time 14.00(14.00)\n",
      "Iter 0374 | Time 60.5329(57.4103) | Bit/dim 4.1159(4.2354) | Xent 1.4843(1.5772) | Loss 4.8580(5.0239) | Error 0.5300(0.5612) Steps 634(642.67) | Grad Norm 6.3232(6.4534) | Total Time 14.00(14.00)\n",
      "Iter 0375 | Time 62.6090(57.5662) | Bit/dim 4.1179(4.2318) | Xent 1.4559(1.5735) | Loss 4.8458(5.0186) | Error 0.5210(0.5599) Steps 640(642.59) | Grad Norm 2.6568(6.3395) | Total Time 14.00(14.00)\n",
      "Iter 0376 | Time 59.0128(57.6096) | Bit/dim 4.1100(4.2282) | Xent 1.4736(1.5705) | Loss 4.8469(5.0134) | Error 0.5231(0.5588) Steps 628(642.15) | Grad Norm 3.3257(6.2491) | Total Time 14.00(14.00)\n",
      "Iter 0377 | Time 62.6879(57.7620) | Bit/dim 4.0963(4.2242) | Xent 1.4362(1.5665) | Loss 4.8144(5.0075) | Error 0.5151(0.5575) Steps 628(641.73) | Grad Norm 2.2109(6.1279) | Total Time 14.00(14.00)\n",
      "Iter 0378 | Time 61.2782(57.8675) | Bit/dim 4.1114(4.2208) | Xent 1.4349(1.5626) | Loss 4.8288(5.0021) | Error 0.5114(0.5561) Steps 652(642.04) | Grad Norm 2.1362(6.0082) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0063 | Time 23.5613, Epoch Time 405.2368(361.8997), Bit/dim 4.0960(best: 4.1187), Xent 1.4244, Loss 4.8083, Error 0.5115(best: 0.5343)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0379 | Time 59.5636(57.9184) | Bit/dim 4.0907(4.2169) | Xent 1.4239(1.5584) | Loss 4.8026(4.9961) | Error 0.5139(0.5549) Steps 646(642.15) | Grad Norm 2.4807(5.9024) | Total Time 14.00(14.00)\n",
      "Iter 0380 | Time 62.3748(58.0521) | Bit/dim 4.0879(4.2131) | Xent 1.4709(1.5558) | Loss 4.8233(4.9909) | Error 0.5317(0.5542) Steps 616(641.37) | Grad Norm 3.0251(5.8160) | Total Time 14.00(14.00)\n",
      "Iter 0381 | Time 60.1157(58.1140) | Bit/dim 4.1026(4.2097) | Xent 1.4462(1.5525) | Loss 4.8257(4.9860) | Error 0.5162(0.5530) Steps 622(640.79) | Grad Norm 5.4887(5.8062) | Total Time 14.00(14.00)\n",
      "Iter 0382 | Time 56.4259(58.0633) | Bit/dim 4.1588(4.2082) | Xent 1.5771(1.5532) | Loss 4.9473(4.9848) | Error 0.5647(0.5534) Steps 640(640.76) | Grad Norm 10.6314(5.9510) | Total Time 14.00(14.00)\n",
      "Iter 0383 | Time 60.9847(58.1510) | Bit/dim 4.1277(4.2058) | Xent 1.8818(1.5631) | Loss 5.0686(4.9873) | Error 0.6149(0.5552) Steps 652(641.10) | Grad Norm 22.3606(6.4433) | Total Time 14.00(14.00)\n",
      "Iter 0384 | Time 55.3472(58.0669) | Bit/dim 4.2253(4.2064) | Xent 2.1573(1.5809) | Loss 5.3040(4.9968) | Error 0.6924(0.5594) Steps 664(641.79) | Grad Norm 18.4398(6.8032) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0064 | Time 24.5519, Epoch Time 395.2002(362.8987), Bit/dim 4.2581(best: 4.0960), Xent 1.9651, Loss 5.2407, Error 0.6806(best: 0.5115)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0385 | Time 58.4169(58.0774) | Bit/dim 4.2667(4.2082) | Xent 1.9827(1.5930) | Loss 5.2581(5.0047) | Error 0.6770(0.5629) Steps 670(642.64) | Grad Norm 6.1377(6.7832) | Total Time 14.00(14.00)\n",
      "Iter 0386 | Time 60.8916(58.1618) | Bit/dim 4.2484(4.2094) | Xent 1.9334(1.6032) | Loss 5.2151(5.0110) | Error 0.6775(0.5663) Steps 694(644.18) | Grad Norm 7.1828(6.7952) | Total Time 14.00(14.00)\n",
      "Iter 0387 | Time 64.9580(58.3657) | Bit/dim 4.2893(4.2118) | Xent 1.8783(1.6114) | Loss 5.2284(5.0175) | Error 0.6541(0.5690) Steps 682(645.31) | Grad Norm 8.0565(6.8330) | Total Time 14.00(14.00)\n",
      "Iter 0388 | Time 61.2878(58.4533) | Bit/dim 4.4244(4.2182) | Xent 1.8120(1.6174) | Loss 5.3305(5.0269) | Error 0.6369(0.5710) Steps 730(647.85) | Grad Norm 6.3846(6.8196) | Total Time 14.00(14.00)\n",
      "Iter 0389 | Time 61.9091(58.5570) | Bit/dim 4.3392(4.2218) | Xent 1.7978(1.6229) | Loss 5.2381(5.0332) | Error 0.6356(0.5729) Steps 730(650.32) | Grad Norm 4.8840(6.7615) | Total Time 14.00(14.00)\n",
      "Iter 0390 | Time 58.6671(58.5603) | Bit/dim 4.2706(4.2233) | Xent 1.7874(1.6278) | Loss 5.1643(5.0372) | Error 0.6242(0.5745) Steps 724(652.53) | Grad Norm 5.8230(6.7333) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0065 | Time 26.3382, Epoch Time 408.0966(364.2546), Bit/dim 4.3093(best: 4.0960), Xent 1.7564, Loss 5.1875, Error 0.6169(best: 0.5115)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0391 | Time 64.6375(58.7426) | Bit/dim 4.3103(4.2259) | Xent 1.8039(1.6331) | Loss 5.2122(5.0424) | Error 0.6315(0.5762) Steps 718(654.49) | Grad Norm 10.5390(6.8475) | Total Time 14.00(14.00)\n",
      "Iter 0392 | Time 63.4725(58.8845) | Bit/dim 4.2654(4.2271) | Xent 1.7925(1.6379) | Loss 5.1616(5.0460) | Error 0.6360(0.5780) Steps 754(657.48) | Grad Norm 5.1528(6.7967) | Total Time 14.00(14.00)\n",
      "Iter 0393 | Time 63.5552(59.0246) | Bit/dim 4.2638(4.2282) | Xent 1.7244(1.6405) | Loss 5.1260(5.0484) | Error 0.6144(0.5791) Steps 730(659.65) | Grad Norm 6.3294(6.7827) | Total Time 14.00(14.00)\n",
      "Iter 0394 | Time 68.4240(59.3066) | Bit/dim 4.2056(4.2275) | Xent 1.7945(1.6451) | Loss 5.1028(5.0500) | Error 0.6422(0.5810) Steps 772(663.02) | Grad Norm 5.5973(6.7471) | Total Time 14.00(14.00)\n",
      "Iter 0395 | Time 62.3531(59.3980) | Bit/dim 4.2078(4.2269) | Xent 1.7147(1.6472) | Loss 5.0652(5.0505) | Error 0.6065(0.5817) Steps 736(665.21) | Grad Norm 3.2303(6.6416) | Total Time 14.00(14.00)\n",
      "Iter 0396 | Time 62.9303(59.5040) | Bit/dim 4.1965(4.2260) | Xent 1.6966(1.6486) | Loss 5.0448(5.0503) | Error 0.6019(0.5823) Steps 724(666.98) | Grad Norm 3.6008(6.5504) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0066 | Time 25.4705, Epoch Time 426.1473(366.1114), Bit/dim 4.1956(best: 4.0960), Xent 1.6785, Loss 5.0348, Error 0.5933(best: 0.5115)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0397 | Time 66.3973(59.7108) | Bit/dim 4.1979(4.2251) | Xent 1.6971(1.6501) | Loss 5.0464(5.0502) | Error 0.6049(0.5830) Steps 742(669.23) | Grad Norm 4.7592(6.4966) | Total Time 14.00(14.00)\n",
      "Iter 0398 | Time 66.4242(59.9122) | Bit/dim 4.1951(4.2242) | Xent 1.7121(1.6520) | Loss 5.0511(5.0502) | Error 0.6008(0.5835) Steps 706(670.33) | Grad Norm 3.8287(6.4166) | Total Time 14.00(14.00)\n",
      "Iter 0399 | Time 64.6871(60.0554) | Bit/dim 4.1634(4.2224) | Xent 1.6374(1.6515) | Loss 4.9821(5.0482) | Error 0.5880(0.5837) Steps 742(672.48) | Grad Norm 2.3782(6.2954) | Total Time 14.00(14.00)\n",
      "Iter 0400 | Time 65.8604(60.2296) | Bit/dim 4.1524(4.2203) | Xent 1.6414(1.6512) | Loss 4.9731(5.0459) | Error 0.5774(0.5835) Steps 736(674.38) | Grad Norm 2.6552(6.1862) | Total Time 14.00(14.00)\n",
      "Iter 0401 | Time 66.1587(60.4075) | Bit/dim 4.1449(4.2181) | Xent 1.6539(1.6513) | Loss 4.9719(5.0437) | Error 0.5802(0.5834) Steps 724(675.87) | Grad Norm 3.3144(6.1001) | Total Time 14.00(14.00)\n",
      "Iter 0402 | Time 63.2509(60.4928) | Bit/dim 4.1287(4.2154) | Xent 1.6245(1.6505) | Loss 4.9409(5.0406) | Error 0.5731(0.5831) Steps 730(677.50) | Grad Norm 3.2119(6.0134) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0067 | Time 25.8507, Epoch Time 434.3698(368.1592), Bit/dim 4.1428(best: 4.0960), Xent 1.5913, Loss 4.9384, Error 0.5738(best: 0.5115)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0403 | Time 67.6918(60.7087) | Bit/dim 4.1414(4.2132) | Xent 1.6115(1.6493) | Loss 4.9472(5.0378) | Error 0.5826(0.5831) Steps 730(679.07) | Grad Norm 6.1539(6.0176) | Total Time 14.00(14.00)\n",
      "Iter 0404 | Time 66.1869(60.8731) | Bit/dim 4.1248(4.2105) | Xent 1.6396(1.6490) | Loss 4.9446(5.0350) | Error 0.5824(0.5830) Steps 724(680.42) | Grad Norm 8.1032(6.0802) | Total Time 14.00(14.00)\n",
      "Iter 0405 | Time 63.7848(60.9604) | Bit/dim 4.1263(4.2080) | Xent 1.6460(1.6489) | Loss 4.9493(5.0325) | Error 0.5844(0.5831) Steps 718(681.55) | Grad Norm 7.1572(6.1125) | Total Time 14.00(14.00)\n",
      "Iter 0406 | Time 64.4432(61.0649) | Bit/dim 4.1217(4.2054) | Xent 1.7717(1.6526) | Loss 5.0076(5.0317) | Error 0.6031(0.5837) Steps 682(681.56) | Grad Norm 11.0514(6.2607) | Total Time 14.00(14.00)\n",
      "Iter 0407 | Time 67.4624(61.2568) | Bit/dim 4.1826(4.2047) | Xent 1.8623(1.6589) | Loss 5.1137(5.0342) | Error 0.6439(0.5855) Steps 730(683.01) | Grad Norm 7.1576(6.2876) | Total Time 14.00(14.00)\n",
      "Iter 0408 | Time 66.1263(61.4029) | Bit/dim 4.1475(4.2030) | Xent 1.6631(1.6590) | Loss 4.9790(5.0325) | Error 0.5988(0.5859) Steps 766(685.50) | Grad Norm 4.8230(6.2437) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0068 | Time 27.0679, Epoch Time 438.4973(370.2693), Bit/dim 4.1636(best: 4.0960), Xent 1.6173, Loss 4.9722, Error 0.5799(best: 0.5115)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0409 | Time 64.4365(61.4939) | Bit/dim 4.1620(4.2018) | Xent 1.6463(1.6587) | Loss 4.9852(5.0311) | Error 0.5813(0.5858) Steps 760(687.74) | Grad Norm 4.5897(6.1940) | Total Time 14.00(14.00)\n",
      "Iter 0410 | Time 60.8798(61.4755) | Bit/dim 4.1207(4.1993) | Xent 1.6408(1.6581) | Loss 4.9411(5.0284) | Error 0.5799(0.5856) Steps 742(689.37) | Grad Norm 3.6219(6.1169) | Total Time 14.00(14.00)\n",
      "Iter 0411 | Time 65.1413(61.5855) | Bit/dim 4.1201(4.1970) | Xent 1.5966(1.6563) | Loss 4.9184(5.0251) | Error 0.5613(0.5848) Steps 748(691.13) | Grad Norm 2.5984(6.0113) | Total Time 14.00(14.00)\n",
      "Iter 0412 | Time 66.6318(61.7369) | Bit/dim 4.1104(4.1944) | Xent 1.6136(1.6550) | Loss 4.9172(5.0219) | Error 0.5670(0.5843) Steps 718(691.93) | Grad Norm 3.6654(5.9409) | Total Time 14.00(14.00)\n",
      "Iter 0413 | Time 62.2563(61.7524) | Bit/dim 4.1219(4.1922) | Xent 1.5768(1.6527) | Loss 4.9104(5.0185) | Error 0.5549(0.5834) Steps 730(693.07) | Grad Norm 2.6230(5.8414) | Total Time 14.00(14.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_drop_cifar.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 8000 --test_batch_size 5000 --save ../experiments_published/cnf_conditional_cifar10_8K_drop_0_5_run1 --resume ../experiments_published/cnf_conditional_cifar10_8K_drop_0_5_run1/epoch_32_checkpt.pth --seed 1 --conditional True --controlled_tol True --train_mode semisup --lr 0.01 --warmup_iters 1000 --atol 1e-4  --rtol 1e-4 --weight_y 0.5 --dropout_rate 0.5\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
