{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_rl.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "GATES = [\"cnn1\", \"cnn2\", \"rnn\"]\n",
      "\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--gate', type=str, default='cnn1', choices=GATES)\n",
      "parser.add_argument('--scale', type=float, default=1.0)\n",
      "parser.add_argument('--scale_fac', type=float, default=1.0)\n",
      "parser.add_argument('--scale_std', type=float, default=1.0)\n",
      "parser.add_argument('--eta', default=0.1, type=float,\n",
      "                        help='tuning parameter that allows us to trade-off the competing goals of' \n",
      "                                'minimizing the prediction loss and maximizing the gate rewards ')\n",
      "parser.add_argument('--rl-weight', default=0.01, type=float,\n",
      "                        help='rl weight')\n",
      "\n",
      "parser.add_argument('--gamma', default=0.99, type=float,\n",
      "                        help='discount factor, default: (0.99)')\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "import lib.odenvp_conditional_rl as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol, \"scale\": args.scale, \"scale_fac\": args.scale_fac, \"scale_std\": args.scale_std, \"gate\": args.gate},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            # re-weight the gate rewards\n",
      "            normalized_eta = args.eta / len(logp_actions)\n",
      "            \n",
      "            # collect cumulative future rewards\n",
      "            R = - loss\n",
      "            cum_rewards = []\n",
      "            for r in nfe[::-1]:\n",
      "                R = -normalized_eta * r.view(-1,1) + args.gamma * R\n",
      "                cum_rewards.insert(0,R)\n",
      "            \n",
      "            # apply REINFORCE\n",
      "            rl_loss = 0\n",
      "            for lpa, r in zip(logp_actions, cum_rewards):\n",
      "                rl_loss = rl_loss - lpa.view(-1,1) * args.rl_weight * r\n",
      "                \n",
      "            loss = loss + rl_loss.mean()\n",
      "            \n",
      "            loss.backward()\n",
      "            \n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "            \n",
      "            for idx in range(len(model.module.transforms)):\n",
      "                for layer in model.module.transforms[idx].chain:\n",
      "                    if hasattr(layer, 'atol'):\n",
      "                        layer.odefunc.after_odeint()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'train': atol[tol_indx].mean()}, itr)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'train': rtol[tol_indx].mean()}, itr)\n",
      "                    \n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples, atol, rtol, logp_actions, nfe = model(fixed_z, reverse=True)\n",
      "            generated_samples = generated_samples.view(-1, *data_shape)\n",
      "            for tol_indx in range(len(atol)):\n",
      "                writer.add_scalars('atol_gen_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                writer.add_scalars('rtol_gen_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=0.0001, autoencode=False, batch_norm=False, batch_size=8000, batch_size_schedule='', begin_epoch=1, condition_ratio=0.5, conditional=True, controlled_tol=True, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, eta=0.1, gamma=0.99, gate='cnn2', imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=1, lr=0.01, max_grad_norm=20.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume='../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_rl_stdlearnscale_6_run1/epoch_77_checkpt.pth', rl_weight=0.01, rtol=0.0001, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_rl_stdlearnscale_6_run1', scale=1.0, scale_fac=1.0, scale_std=6.0, seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=5000, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000.0, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1450886\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc_rl.py:291: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Iter 0463 | Time 119.2157(66.4029) | Bit/dim 4.0090(4.1099) | Xent 1.5316(1.6433) | Loss 13.7034(12.2816) | Error 0.5493(0.5854) Steps 0(0.00) | Grad Norm 3.5231(8.1028) | Total Time 0.00(0.00)\n",
      "Iter 0464 | Time 66.4414(66.4040) | Bit/dim 4.0124(4.1070) | Xent 1.5358(1.6401) | Loss 11.3463(12.2535) | Error 0.5500(0.5844) Steps 0(0.00) | Grad Norm 4.6961(8.0006) | Total Time 0.00(0.00)\n",
      "Iter 0465 | Time 79.1729(66.7871) | Bit/dim 3.9845(4.1033) | Xent 1.5482(1.6373) | Loss 11.4168(12.2284) | Error 0.5576(0.5836) Steps 0(0.00) | Grad Norm 7.0056(7.9707) | Total Time 0.00(0.00)\n",
      "Iter 0466 | Time 60.7953(66.6073) | Bit/dim 4.0016(4.1003) | Xent 1.5579(1.6350) | Loss 11.0475(12.1930) | Error 0.5560(0.5827) Steps 0(0.00) | Grad Norm 7.1975(7.9475) | Total Time 0.00(0.00)\n",
      "Iter 0467 | Time 58.6792(66.3695) | Bit/dim 3.9876(4.0969) | Xent 1.5451(1.6323) | Loss 11.1572(12.1619) | Error 0.5513(0.5818) Steps 0(0.00) | Grad Norm 7.0124(7.9195) | Total Time 0.00(0.00)\n",
      "Iter 0468 | Time 77.7071(66.7096) | Bit/dim 3.9953(4.0938) | Xent 1.5373(1.6294) | Loss 11.3833(12.1385) | Error 0.5485(0.5808) Steps 0(0.00) | Grad Norm 5.1757(7.8371) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0078 | Time 46.0855, Epoch Time 518.6095(414.0955), Bit/dim 3.9837(best: inf), Xent 1.4736, Loss 4.7205, Error 0.5250(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0469 | Time 78.5353(67.0644) | Bit/dim 3.9918(4.0908) | Xent 1.5217(1.6262) | Loss 15.3263(12.2342) | Error 0.5511(0.5799) Steps 0(0.00) | Grad Norm 3.7428(7.7143) | Total Time 0.00(0.00)\n",
      "Iter 0470 | Time 67.7642(67.0854) | Bit/dim 3.9832(4.0875) | Xent 1.5138(1.6228) | Loss 11.2830(12.2056) | Error 0.5393(0.5787) Steps 0(0.00) | Grad Norm 1.9629(7.5418) | Total Time 0.00(0.00)\n",
      "Iter 0471 | Time 67.9849(67.1124) | Bit/dim 3.9840(4.0844) | Xent 1.5234(1.6198) | Loss 11.0311(12.1704) | Error 0.5428(0.5776) Steps 0(0.00) | Grad Norm 5.4019(7.4776) | Total Time 0.00(0.00)\n",
      "Iter 0472 | Time 64.4164(67.0315) | Bit/dim 3.9842(4.0814) | Xent 1.5538(1.6179) | Loss 11.1393(12.1395) | Error 0.5575(0.5770) Steps 0(0.00) | Grad Norm 8.8309(7.5182) | Total Time 0.00(0.00)\n",
      "Iter 0473 | Time 65.4784(66.9849) | Bit/dim 3.9939(4.0788) | Xent 1.5780(1.6167) | Loss 11.3287(12.1152) | Error 0.5680(0.5767) Steps 0(0.00) | Grad Norm 11.5264(7.6384) | Total Time 0.00(0.00)\n",
      "Iter 0474 | Time 61.9202(66.8330) | Bit/dim 3.9791(4.0758) | Xent 1.5942(1.6160) | Loss 11.2794(12.0901) | Error 0.5707(0.5766) Steps 0(0.00) | Grad Norm 12.5876(7.7869) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0079 | Time 26.3863, Epoch Time 448.3394(415.1229), Bit/dim 4.0165(best: 3.9837), Xent 1.6733, Loss 4.8532, Error 0.5872(best: 0.5250)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0475 | Time 64.0034(66.7481) | Bit/dim 4.0052(4.0737) | Xent 1.7396(1.6197) | Loss 15.2193(12.1840) | Error 0.6002(0.5773) Steps 0(0.00) | Grad Norm 27.5603(8.3801) | Total Time 0.00(0.00)\n",
      "Iter 0476 | Time 61.8724(66.6018) | Bit/dim 4.0582(4.0732) | Xent 1.6687(1.6212) | Loss 11.3938(12.1603) | Error 0.5862(0.5775) Steps 0(0.00) | Grad Norm 15.6001(8.5967) | Total Time 0.00(0.00)\n",
      "Iter 0477 | Time 61.8630(66.4596) | Bit/dim 4.0158(4.0715) | Xent 1.5573(1.6192) | Loss 10.9914(12.1252) | Error 0.5574(0.5769) Steps 0(0.00) | Grad Norm 6.1982(8.5247) | Total Time 0.00(0.00)\n",
      "Iter 0478 | Time 68.2593(66.5136) | Bit/dim 4.0413(4.0706) | Xent 1.5377(1.6168) | Loss 11.3932(12.1032) | Error 0.5524(0.5762) Steps 0(0.00) | Grad Norm 8.0186(8.5096) | Total Time 0.00(0.00)\n",
      "Iter 0479 | Time 63.3845(66.4198) | Bit/dim 4.0529(4.0701) | Xent 1.5807(1.6157) | Loss 11.4027(12.0822) | Error 0.5699(0.5760) Steps 0(0.00) | Grad Norm 8.6488(8.5137) | Total Time 0.00(0.00)\n",
      "Iter 0480 | Time 66.8757(66.4334) | Bit/dim 4.0221(4.0686) | Xent 1.5704(1.6144) | Loss 11.4805(12.0642) | Error 0.5671(0.5757) Steps 0(0.00) | Grad Norm 9.5743(8.5456) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0080 | Time 25.6100, Epoch Time 428.2126(415.5156), Bit/dim 3.9980(best: 3.9837), Xent 1.5390, Loss 4.7675, Error 0.5492(best: 0.5250)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0481 | Time 67.7159(66.4719) | Bit/dim 3.9955(4.0664) | Xent 1.5968(1.6138) | Loss 15.7488(12.1747) | Error 0.5657(0.5754) Steps 0(0.00) | Grad Norm 11.3670(8.6302) | Total Time 0.00(0.00)\n",
      "Iter 0482 | Time 60.2134(66.2841) | Bit/dim 4.0404(4.0657) | Xent 1.6821(1.6159) | Loss 11.4869(12.1541) | Error 0.5970(0.5761) Steps 0(0.00) | Grad Norm 21.6232(9.0200) | Total Time 0.00(0.00)\n",
      "Iter 0483 | Time 69.2904(66.3743) | Bit/dim 4.0480(4.0651) | Xent 1.7225(1.6191) | Loss 11.3201(12.1290) | Error 0.6039(0.5769) Steps 0(0.00) | Grad Norm 20.4418(9.3626) | Total Time 0.00(0.00)\n",
      "Iter 0484 | Time 61.6415(66.2324) | Bit/dim 4.0232(4.0639) | Xent 1.5355(1.6166) | Loss 11.4040(12.1073) | Error 0.5425(0.5759) Steps 0(0.00) | Grad Norm 4.5888(9.2194) | Total Time 0.00(0.00)\n",
      "Iter 0485 | Time 63.7228(66.1571) | Bit/dim 4.0480(4.0634) | Xent 1.6305(1.6170) | Loss 11.2494(12.0816) | Error 0.5822(0.5761) Steps 0(0.00) | Grad Norm 10.4229(9.2555) | Total Time 0.00(0.00)\n",
      "Iter 0486 | Time 63.4725(66.0765) | Bit/dim 4.0094(4.0618) | Xent 1.5777(1.6158) | Loss 11.4348(12.0622) | Error 0.5630(0.5757) Steps 0(0.00) | Grad Norm 6.9128(9.1852) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0081 | Time 25.8223, Epoch Time 428.0650(415.8920), Bit/dim 4.0178(best: 3.9837), Xent 1.5367, Loss 4.7862, Error 0.5417(best: 0.5250)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0487 | Time 59.9357(65.8923) | Bit/dim 4.0132(4.0603) | Xent 1.5932(1.6151) | Loss 15.9240(12.1780) | Error 0.5662(0.5754) Steps 0(0.00) | Grad Norm 8.3250(9.1594) | Total Time 0.00(0.00)\n",
      "Iter 0488 | Time 68.4634(65.9694) | Bit/dim 4.0117(4.0588) | Xent 1.5596(1.6135) | Loss 11.4974(12.1576) | Error 0.5631(0.5750) Steps 0(0.00) | Grad Norm 5.2806(9.0431) | Total Time 0.00(0.00)\n",
      "Iter 0489 | Time 66.4507(65.9839) | Bit/dim 4.0178(4.0576) | Xent 1.5442(1.6114) | Loss 11.4016(12.1349) | Error 0.5526(0.5744) Steps 0(0.00) | Grad Norm 5.8782(8.9481) | Total Time 0.00(0.00)\n",
      "Iter 0490 | Time 63.7397(65.9166) | Bit/dim 4.0050(4.0560) | Xent 1.5340(1.6091) | Loss 11.3786(12.1122) | Error 0.5491(0.5736) Steps 0(0.00) | Grad Norm 4.5243(8.8154) | Total Time 0.00(0.00)\n",
      "Iter 0491 | Time 63.4423(65.8423) | Bit/dim 4.0114(4.0547) | Xent 1.5192(1.6064) | Loss 11.3921(12.0906) | Error 0.5393(0.5726) Steps 0(0.00) | Grad Norm 8.0631(8.7928) | Total Time 0.00(0.00)\n",
      "Iter 0492 | Time 58.0282(65.6079) | Bit/dim 3.9912(4.0528) | Xent 1.5125(1.6036) | Loss 11.2540(12.0655) | Error 0.5425(0.5717) Steps 0(0.00) | Grad Norm 4.7543(8.6717) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0082 | Time 25.3499, Epoch Time 421.1901(416.0510), Bit/dim 3.9880(best: 3.9837), Xent 1.4660, Loss 4.7210, Error 0.5228(best: 0.5250)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0493 | Time 62.5643(65.5166) | Bit/dim 3.9799(4.0506) | Xent 1.5109(1.6008) | Loss 14.8451(12.1489) | Error 0.5426(0.5708) Steps 0(0.00) | Grad Norm 4.9962(8.5614) | Total Time 0.00(0.00)\n",
      "Iter 0494 | Time 65.6026(65.5192) | Bit/dim 3.9926(4.0489) | Xent 1.5169(1.5983) | Loss 11.2188(12.1210) | Error 0.5491(0.5702) Steps 0(0.00) | Grad Norm 6.0256(8.4853) | Total Time 0.00(0.00)\n",
      "Iter 0495 | Time 59.0031(65.3237) | Bit/dim 3.9787(4.0468) | Xent 1.5068(1.5955) | Loss 10.9939(12.0872) | Error 0.5490(0.5695) Steps 0(0.00) | Grad Norm 7.1418(8.4450) | Total Time 0.00(0.00)\n",
      "Iter 0496 | Time 61.0369(65.1951) | Bit/dim 3.9867(4.0450) | Xent 1.5629(1.5945) | Loss 11.2099(12.0609) | Error 0.5591(0.5692) Steps 0(0.00) | Grad Norm 15.5310(8.6576) | Total Time 0.00(0.00)\n",
      "Iter 0497 | Time 69.9435(65.3375) | Bit/dim 3.9927(4.0434) | Xent 1.6853(1.5973) | Loss 11.4868(12.0436) | Error 0.6073(0.5703) Steps 0(0.00) | Grad Norm 20.1812(9.0033) | Total Time 0.00(0.00)\n",
      "Iter 0498 | Time 64.2403(65.3046) | Bit/dim 4.0074(4.0423) | Xent 1.5654(1.5963) | Loss 11.2892(12.0210) | Error 0.5670(0.5702) Steps 0(0.00) | Grad Norm 13.3953(9.1351) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0083 | Time 25.6239, Epoch Time 424.1779(416.2948), Bit/dim 3.9796(best: 3.9837), Xent 1.4654, Loss 4.7123, Error 0.5228(best: 0.5228)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0499 | Time 60.7545(65.1681) | Bit/dim 3.9687(4.0401) | Xent 1.5146(1.5938) | Loss 14.4706(12.0945) | Error 0.5446(0.5695) Steps 0(0.00) | Grad Norm 3.7698(8.9741) | Total Time 0.00(0.00)\n",
      "Iter 0500 | Time 65.4486(65.1765) | Bit/dim 3.9654(4.0379) | Xent 1.5335(1.5920) | Loss 11.0988(12.0646) | Error 0.5500(0.5689) Steps 0(0.00) | Grad Norm 8.7274(8.9667) | Total Time 0.00(0.00)\n",
      "Iter 0501 | Time 64.3523(65.1518) | Bit/dim 3.9743(4.0360) | Xent 1.4953(1.5891) | Loss 10.8493(12.0282) | Error 0.5337(0.5678) Steps 0(0.00) | Grad Norm 3.4178(8.8003) | Total Time 0.00(0.00)\n",
      "Iter 0502 | Time 62.9609(65.0861) | Bit/dim 3.9725(4.0341) | Xent 1.5025(1.5865) | Loss 11.1285(12.0012) | Error 0.5417(0.5671) Steps 0(0.00) | Grad Norm 5.8233(8.7110) | Total Time 0.00(0.00)\n",
      "Iter 0503 | Time 61.3844(64.9750) | Bit/dim 3.9707(4.0322) | Xent 1.5104(1.5843) | Loss 11.2889(11.9798) | Error 0.5445(0.5664) Steps 0(0.00) | Grad Norm 3.7617(8.5625) | Total Time 0.00(0.00)\n",
      "Iter 0504 | Time 62.6687(64.9058) | Bit/dim 3.9511(4.0297) | Xent 1.4932(1.5815) | Loss 11.0820(11.9529) | Error 0.5396(0.5656) Steps 0(0.00) | Grad Norm 4.6704(8.4457) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0084 | Time 25.8626, Epoch Time 419.3070(416.3852), Bit/dim 3.9546(best: 3.9796), Xent 1.4414, Loss 4.6753, Error 0.5246(best: 0.5228)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0505 | Time 65.3892(64.9203) | Bit/dim 3.9604(4.0276) | Xent 1.4987(1.5790) | Loss 14.9344(12.0423) | Error 0.5359(0.5647) Steps 0(0.00) | Grad Norm 6.1104(8.3757) | Total Time 0.00(0.00)\n",
      "Iter 0506 | Time 64.2334(64.8997) | Bit/dim 3.9571(4.0255) | Xent 1.4523(1.5752) | Loss 11.1088(12.0143) | Error 0.5256(0.5635) Steps 0(0.00) | Grad Norm 5.0440(8.2757) | Total Time 0.00(0.00)\n",
      "Iter 0507 | Time 64.0073(64.8730) | Bit/dim 3.9551(4.0234) | Xent 1.4641(1.5719) | Loss 11.3275(11.9937) | Error 0.5259(0.5624) Steps 0(0.00) | Grad Norm 3.0749(8.1197) | Total Time 0.00(0.00)\n",
      "Iter 0508 | Time 76.3040(65.2159) | Bit/dim 3.9567(4.0214) | Xent 1.4644(1.5687) | Loss 11.1541(11.9685) | Error 0.5310(0.5614) Steps 0(0.00) | Grad Norm 3.7113(7.9874) | Total Time 0.00(0.00)\n",
      "Iter 0509 | Time 63.9204(65.1770) | Bit/dim 3.9499(4.0193) | Xent 1.4753(1.5659) | Loss 11.0787(11.9418) | Error 0.5376(0.5607) Steps 0(0.00) | Grad Norm 5.5200(7.9134) | Total Time 0.00(0.00)\n",
      "Iter 0510 | Time 66.6104(65.2200) | Bit/dim 3.9372(4.0168) | Xent 1.4757(1.5632) | Loss 11.1948(11.9194) | Error 0.5315(0.5599) Steps 0(0.00) | Grad Norm 4.6278(7.8148) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0085 | Time 25.3225, Epoch Time 441.7902(417.1473), Bit/dim 3.9386(best: 3.9546), Xent 1.4151, Loss 4.6461, Error 0.5073(best: 0.5228)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0511 | Time 60.8722(65.0896) | Bit/dim 3.9399(4.0145) | Xent 1.4667(1.5603) | Loss 14.6413(12.0011) | Error 0.5274(0.5589) Steps 0(0.00) | Grad Norm 5.1771(7.7357) | Total Time 0.00(0.00)\n",
      "Iter 0512 | Time 58.8571(64.9026) | Bit/dim 3.9284(4.0119) | Xent 1.4535(1.5571) | Loss 11.0066(11.9712) | Error 0.5251(0.5579) Steps 0(0.00) | Grad Norm 4.6833(7.6441) | Total Time 0.00(0.00)\n",
      "Iter 0513 | Time 71.0927(65.0883) | Bit/dim 3.9317(4.0095) | Xent 1.4488(1.5538) | Loss 11.0020(11.9422) | Error 0.5210(0.5568) Steps 0(0.00) | Grad Norm 3.4184(7.5174) | Total Time 0.00(0.00)\n",
      "Iter 0514 | Time 63.0963(65.0286) | Bit/dim 3.9282(4.0071) | Xent 1.4305(1.5501) | Loss 10.4276(11.8967) | Error 0.5215(0.5557) Steps 0(0.00) | Grad Norm 3.2199(7.3884) | Total Time 0.00(0.00)\n",
      "Iter 0515 | Time 64.4777(65.0120) | Bit/dim 3.9255(4.0046) | Xent 1.4367(1.5467) | Loss 10.9332(11.8678) | Error 0.5148(0.5545) Steps 0(0.00) | Grad Norm 5.0051(7.3169) | Total Time 0.00(0.00)\n",
      "Iter 0516 | Time 60.6409(64.8809) | Bit/dim 3.9404(4.0027) | Xent 1.4507(1.5438) | Loss 10.8807(11.8382) | Error 0.5216(0.5535) Steps 0(0.00) | Grad Norm 9.7693(7.3905) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0086 | Time 25.2846, Epoch Time 420.3176(417.2424), Bit/dim 3.9406(best: 3.9386), Xent 1.5288, Loss 4.7050, Error 0.5527(best: 0.5073)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0517 | Time 63.5815(64.8419) | Bit/dim 3.9459(4.0010) | Xent 1.5641(1.5444) | Loss 14.8257(11.9278) | Error 0.5660(0.5539) Steps 0(0.00) | Grad Norm 19.1277(7.7426) | Total Time 0.00(0.00)\n",
      "Iter 0518 | Time 61.0455(64.7280) | Bit/dim 3.9680(4.0000) | Xent 1.9020(1.5552) | Loss 11.7176(11.9215) | Error 0.6307(0.5562) Steps 0(0.00) | Grad Norm 33.0717(8.5025) | Total Time 0.00(0.00)\n",
      "Iter 0519 | Time 67.2100(64.8025) | Bit/dim 3.9916(3.9997) | Xent 1.8429(1.5638) | Loss 11.3315(11.9038) | Error 0.6495(0.5590) Steps 0(0.00) | Grad Norm 23.1816(8.9429) | Total Time 0.00(0.00)\n",
      "Iter 0520 | Time 67.2239(64.8751) | Bit/dim 4.0142(4.0002) | Xent 1.7152(1.5683) | Loss 11.4337(11.8897) | Error 0.6230(0.5609) Steps 0(0.00) | Grad Norm 14.1255(9.0983) | Total Time 0.00(0.00)\n",
      "Iter 0521 | Time 64.3521(64.8594) | Bit/dim 4.0423(4.0014) | Xent 1.7730(1.5745) | Loss 11.6534(11.8826) | Error 0.6360(0.5631) Steps 0(0.00) | Grad Norm 18.5026(9.3805) | Total Time 0.00(0.00)\n",
      "Iter 0522 | Time 61.9688(64.7727) | Bit/dim 4.1082(4.0046) | Xent 1.6816(1.5777) | Loss 11.6202(11.8748) | Error 0.6011(0.5643) Steps 0(0.00) | Grad Norm 16.4597(9.5929) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0087 | Time 26.3819, Epoch Time 427.6847(417.5557), Bit/dim 4.1011(best: 3.9386), Xent 1.6222, Loss 4.9122, Error 0.6010(best: 0.5073)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0523 | Time 58.0985(64.5725) | Bit/dim 4.0980(4.0074) | Xent 1.6603(1.5802) | Loss 15.5434(11.9848) | Error 0.6039(0.5655) Steps 0(0.00) | Grad Norm 11.1237(9.6388) | Total Time 0.00(0.00)\n",
      "Iter 0524 | Time 59.2557(64.4130) | Bit/dim 4.1118(4.0106) | Xent 1.6322(1.5817) | Loss 11.4335(11.9683) | Error 0.5909(0.5662) Steps 0(0.00) | Grad Norm 9.2388(9.6268) | Total Time 0.00(0.00)\n",
      "Iter 0525 | Time 62.6446(64.3599) | Bit/dim 4.0646(4.0122) | Xent 1.5924(1.5821) | Loss 11.1999(11.9452) | Error 0.5744(0.5665) Steps 0(0.00) | Grad Norm 5.9386(9.5161) | Total Time 0.00(0.00)\n",
      "Iter 0526 | Time 69.2399(64.5063) | Bit/dim 4.0935(4.0146) | Xent 1.5848(1.5821) | Loss 11.6862(11.9375) | Error 0.5774(0.5668) Steps 0(0.00) | Grad Norm 8.7941(9.4945) | Total Time 0.00(0.00)\n",
      "Iter 0527 | Time 68.1421(64.6154) | Bit/dim 4.0607(4.0160) | Xent 1.6114(1.5830) | Loss 11.5722(11.9265) | Error 0.5762(0.5671) Steps 0(0.00) | Grad Norm 10.2372(9.5168) | Total Time 0.00(0.00)\n",
      "Iter 0528 | Time 64.3943(64.6088) | Bit/dim 4.0669(4.0175) | Xent 1.5382(1.5817) | Loss 11.4462(11.9121) | Error 0.5616(0.5669) Steps 0(0.00) | Grad Norm 10.1304(9.5352) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0088 | Time 26.1697, Epoch Time 423.9481(417.7475), Bit/dim 4.0560(best: 3.9386), Xent 1.4930, Loss 4.8025, Error 0.5393(best: 0.5073)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0529 | Time 67.6608(64.7003) | Bit/dim 4.0590(4.0188) | Xent 1.5591(1.5810) | Loss 16.0549(12.0364) | Error 0.5604(0.5667) Steps 0(0.00) | Grad Norm 8.3067(9.4983) | Total Time 0.00(0.00)\n",
      "Iter 0530 | Time 65.1206(64.7129) | Bit/dim 4.0290(4.0191) | Xent 1.5458(1.5799) | Loss 11.5725(12.0225) | Error 0.5574(0.5664) Steps 0(0.00) | Grad Norm 4.3203(9.3430) | Total Time 0.00(0.00)\n",
      "Iter 0531 | Time 69.0498(64.8431) | Bit/dim 4.0022(4.0186) | Xent 1.5634(1.5794) | Loss 11.4197(12.0044) | Error 0.5682(0.5665) Steps 0(0.00) | Grad Norm 6.8033(9.2668) | Total Time 0.00(0.00)\n",
      "Iter 0532 | Time 67.1975(64.9137) | Bit/dim 4.0017(4.0181) | Xent 1.5324(1.5780) | Loss 11.5338(11.9903) | Error 0.5531(0.5661) Steps 0(0.00) | Grad Norm 6.6781(9.1891) | Total Time 0.00(0.00)\n",
      "Iter 0533 | Time 64.5641(64.9032) | Bit/dim 3.9913(4.0173) | Xent 1.4950(1.5755) | Loss 11.2697(11.9686) | Error 0.5388(0.5653) Steps 0(0.00) | Grad Norm 4.9466(9.0618) | Total Time 0.00(0.00)\n",
      "Iter 0534 | Time 70.9525(65.0847) | Bit/dim 4.0030(4.0168) | Xent 1.4961(1.5732) | Loss 11.3710(11.9507) | Error 0.5434(0.5646) Steps 0(0.00) | Grad Norm 3.8252(8.9047) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0089 | Time 25.8754, Epoch Time 446.5283(418.6109), Bit/dim 3.9723(best: 3.9386), Xent 1.4429, Loss 4.6938, Error 0.5177(best: 0.5073)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0535 | Time 67.2667(65.1501) | Bit/dim 3.9745(4.0156) | Xent 1.4872(1.5706) | Loss 14.7285(12.0340) | Error 0.5373(0.5638) Steps 0(0.00) | Grad Norm 6.4606(8.8314) | Total Time 0.00(0.00)\n",
      "Iter 0536 | Time 69.1097(65.2689) | Bit/dim 3.9685(4.0142) | Xent 1.4868(1.5681) | Loss 11.3141(12.0124) | Error 0.5391(0.5631) Steps 0(0.00) | Grad Norm 9.4013(8.8485) | Total Time 0.00(0.00)\n",
      "Iter 0537 | Time 66.2071(65.2971) | Bit/dim 3.9538(4.0124) | Xent 1.4919(1.5658) | Loss 10.7887(11.9757) | Error 0.5363(0.5623) Steps 0(0.00) | Grad Norm 7.8966(8.8200) | Total Time 0.00(0.00)\n",
      "Iter 0538 | Time 68.6270(65.3970) | Bit/dim 3.9678(4.0110) | Xent 1.4963(1.5637) | Loss 11.2082(11.9527) | Error 0.5391(0.5616) Steps 0(0.00) | Grad Norm 5.0107(8.7057) | Total Time 0.00(0.00)\n",
      "Iter 0539 | Time 70.3573(65.5458) | Bit/dim 3.9493(4.0092) | Xent 1.4887(1.5614) | Loss 11.1028(11.9272) | Error 0.5425(0.5610) Steps 0(0.00) | Grad Norm 5.0594(8.5963) | Total Time 0.00(0.00)\n",
      "Iter 0540 | Time 69.3846(65.6609) | Bit/dim 3.9288(4.0068) | Xent 1.4649(1.5586) | Loss 11.1372(11.9035) | Error 0.5240(0.5599) Steps 0(0.00) | Grad Norm 4.5739(8.4756) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0090 | Time 25.7469, Epoch Time 453.0129(419.6429), Bit/dim 3.9387(best: 3.9386), Xent 1.4021, Loss 4.6397, Error 0.5032(best: 0.5073)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0541 | Time 68.1596(65.7359) | Bit/dim 3.9456(4.0049) | Xent 1.4409(1.5550) | Loss 15.2882(12.0051) | Error 0.5160(0.5586) Steps 0(0.00) | Grad Norm 2.6898(8.3020) | Total Time 0.00(0.00)\n",
      "Iter 0542 | Time 64.2204(65.6904) | Bit/dim 3.9518(4.0033) | Xent 1.4616(1.5522) | Loss 11.3326(11.9849) | Error 0.5201(0.5574) Steps 0(0.00) | Grad Norm 3.8138(8.1674) | Total Time 0.00(0.00)\n",
      "Iter 0543 | Time 69.3834(65.8012) | Bit/dim 3.9308(4.0012) | Xent 1.4402(1.5489) | Loss 11.0770(11.9576) | Error 0.5141(0.5561) Steps 0(0.00) | Grad Norm 6.0057(8.1025) | Total Time 0.00(0.00)\n",
      "Iter 0544 | Time 69.9287(65.9250) | Bit/dim 3.9328(3.9991) | Xent 1.4672(1.5464) | Loss 11.1335(11.9329) | Error 0.5260(0.5552) Steps 0(0.00) | Grad Norm 8.5757(8.1167) | Total Time 0.00(0.00)\n",
      "Iter 0545 | Time 61.3688(65.7884) | Bit/dim 3.9208(3.9968) | Xent 1.4718(1.5442) | Loss 11.0125(11.9053) | Error 0.5260(0.5543) Steps 0(0.00) | Grad Norm 11.0196(8.2038) | Total Time 0.00(0.00)\n",
      "Iter 0546 | Time 72.3124(65.9841) | Bit/dim 3.9159(3.9943) | Xent 1.5336(1.5438) | Loss 11.1787(11.8835) | Error 0.5533(0.5543) Steps 0(0.00) | Grad Norm 13.5624(8.3646) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0091 | Time 25.0860, Epoch Time 446.5000(420.4487), Bit/dim 3.9304(best: 3.9386), Xent 1.5469, Loss 4.7038, Error 0.5568(best: 0.5032)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0547 | Time 63.8295(65.9194) | Bit/dim 3.9271(3.9923) | Xent 1.5829(1.5450) | Loss 15.0414(11.9782) | Error 0.5651(0.5546) Steps 0(0.00) | Grad Norm 16.2884(8.6023) | Total Time 0.00(0.00)\n",
      "Iter 0548 | Time 66.8164(65.9464) | Bit/dim 3.9294(3.9904) | Xent 1.5471(1.5451) | Loss 11.1588(11.9537) | Error 0.5510(0.5545) Steps 0(0.00) | Grad Norm 15.5237(8.8099) | Total Time 0.00(0.00)\n",
      "Iter 0549 | Time 69.4252(66.0507) | Bit/dim 3.9333(3.9887) | Xent 1.4816(1.5432) | Loss 11.3143(11.9345) | Error 0.5308(0.5538) Steps 0(0.00) | Grad Norm 7.1839(8.7612) | Total Time 0.00(0.00)\n",
      "Iter 0550 | Time 63.1072(65.9624) | Bit/dim 3.9071(3.9863) | Xent 1.4892(1.5416) | Loss 11.0577(11.9082) | Error 0.5340(0.5532) Steps 0(0.00) | Grad Norm 6.3233(8.6880) | Total Time 0.00(0.00)\n",
      "Iter 0551 | Time 66.2232(65.9702) | Bit/dim 3.9149(3.9841) | Xent 1.4684(1.5394) | Loss 10.9754(11.8802) | Error 0.5300(0.5525) Steps 0(0.00) | Grad Norm 7.8982(8.6643) | Total Time 0.00(0.00)\n",
      "Iter 0552 | Time 59.5535(65.7777) | Bit/dim 3.9171(3.9821) | Xent 1.4522(1.5367) | Loss 10.9362(11.8519) | Error 0.5157(0.5514) Steps 0(0.00) | Grad Norm 6.7783(8.6078) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0092 | Time 24.9727, Epoch Time 430.1179(420.7387), Bit/dim 3.9103(best: 3.9304), Xent 1.4258, Loss 4.6232, Error 0.5104(best: 0.5032)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0553 | Time 58.5993(65.5624) | Bit/dim 3.9129(3.9800) | Xent 1.4799(1.5350) | Loss 14.3807(11.9277) | Error 0.5315(0.5508) Steps 0(0.00) | Grad Norm 8.3612(8.6004) | Total Time 0.00(0.00)\n",
      "Iter 0554 | Time 67.8044(65.6296) | Bit/dim 3.9066(3.9778) | Xent 1.4420(1.5323) | Loss 10.7621(11.8928) | Error 0.5212(0.5499) Steps 0(0.00) | Grad Norm 7.9650(8.5813) | Total Time 0.00(0.00)\n",
      "Iter 0555 | Time 64.9338(65.6088) | Bit/dim 3.9134(3.9759) | Xent 1.4638(1.5302) | Loss 11.1324(11.8700) | Error 0.5312(0.5494) Steps 0(0.00) | Grad Norm 6.6149(8.5223) | Total Time 0.00(0.00)\n",
      "Iter 0556 | Time 62.8383(65.5257) | Bit/dim 3.9005(3.9736) | Xent 1.4236(1.5270) | Loss 10.8967(11.8408) | Error 0.5125(0.5483) Steps 0(0.00) | Grad Norm 4.1496(8.3911) | Total Time 0.00(0.00)\n",
      "Iter 0557 | Time 59.4545(65.3435) | Bit/dim 3.9053(3.9716) | Xent 1.4149(1.5236) | Loss 10.9655(11.8145) | Error 0.5094(0.5471) Steps 0(0.00) | Grad Norm 5.3022(8.2985) | Total Time 0.00(0.00)\n",
      "Iter 0558 | Time 78.0524(65.7248) | Bit/dim 3.8855(3.9690) | Xent 1.4131(1.5203) | Loss 10.9982(11.7900) | Error 0.5159(0.5462) Steps 0(0.00) | Grad Norm 4.9953(8.1994) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0093 | Time 25.1604, Epoch Time 433.2327(421.1136), Bit/dim 3.8984(best: 3.9103), Xent 1.3731, Loss 4.5850, Error 0.4988(best: 0.5032)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0559 | Time 68.5879(65.8107) | Bit/dim 3.9059(3.9671) | Xent 1.4010(1.5167) | Loss 14.9862(11.8859) | Error 0.5096(0.5451) Steps 0(0.00) | Grad Norm 4.2506(8.0809) | Total Time 0.00(0.00)\n",
      "Iter 0560 | Time 68.6221(65.8950) | Bit/dim 3.9021(3.9652) | Xent 1.3885(1.5129) | Loss 10.7452(11.8517) | Error 0.4990(0.5437) Steps 0(0.00) | Grad Norm 4.7474(7.9809) | Total Time 0.00(0.00)\n",
      "Iter 0561 | Time 73.2660(66.1162) | Bit/dim 3.9152(3.9637) | Xent 1.3924(1.5093) | Loss 10.9109(11.8235) | Error 0.5029(0.5425) Steps 0(0.00) | Grad Norm 4.6914(7.8822) | Total Time 0.00(0.00)\n",
      "Iter 0562 | Time 63.0423(66.0239) | Bit/dim 3.8839(3.9613) | Xent 1.3947(1.5058) | Loss 10.8349(11.7938) | Error 0.5081(0.5414) Steps 0(0.00) | Grad Norm 4.4169(7.7782) | Total Time 0.00(0.00)\n",
      "Iter 0563 | Time 65.7020(66.0143) | Bit/dim 3.8812(3.9589) | Xent 1.4183(1.5032) | Loss 10.9738(11.7692) | Error 0.5139(0.5406) Steps 0(0.00) | Grad Norm 4.7620(7.6878) | Total Time 0.00(0.00)\n",
      "Iter 0564 | Time 62.7153(65.9153) | Bit/dim 3.8957(3.9570) | Xent 1.4043(1.5002) | Loss 10.6427(11.7354) | Error 0.5177(0.5399) Steps 0(0.00) | Grad Norm 4.9959(7.6070) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0094 | Time 25.6473, Epoch Time 443.6221(421.7888), Bit/dim 3.8865(best: 3.8984), Xent 1.3673, Loss 4.5701, Error 0.4989(best: 0.4988)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0565 | Time 63.6043(65.8460) | Bit/dim 3.8980(3.9552) | Xent 1.4222(1.4979) | Loss 14.7635(11.8262) | Error 0.5154(0.5392) Steps 0(0.00) | Grad Norm 9.2154(7.6553) | Total Time 0.00(0.00)\n",
      "Iter 0566 | Time 59.0376(65.6417) | Bit/dim 3.8950(3.9534) | Xent 1.4576(1.4967) | Loss 10.8476(11.7969) | Error 0.5151(0.5385) Steps 0(0.00) | Grad Norm 15.3390(7.8858) | Total Time 0.00(0.00)\n",
      "Iter 0567 | Time 67.7393(65.7047) | Bit/dim 3.8964(3.9517) | Xent 1.7829(1.5053) | Loss 11.2639(11.7809) | Error 0.6030(0.5404) Steps 0(0.00) | Grad Norm 23.3738(8.3504) | Total Time 0.00(0.00)\n",
      "Iter 0568 | Time 71.3521(65.8741) | Bit/dim 3.9600(3.9519) | Xent 2.4769(1.5344) | Loss 12.2196(11.7941) | Error 0.7243(0.5459) Steps 0(0.00) | Grad Norm 60.5976(9.9178) | Total Time 0.00(0.00)\n",
      "Iter 0569 | Time 64.3410(65.8281) | Bit/dim 4.0197(3.9540) | Xent 1.9301(1.5463) | Loss 11.7399(11.7924) | Error 0.6747(0.5498) Steps 0(0.00) | Grad Norm 21.3419(10.2605) | Total Time 0.00(0.00)\n",
      "Iter 0570 | Time 61.4289(65.6961) | Bit/dim 4.0266(3.9562) | Xent 2.0324(1.5609) | Loss 11.9343(11.7967) | Error 0.7024(0.5544) Steps 0(0.00) | Grad Norm 17.9095(10.4900) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0095 | Time 26.9039, Epoch Time 430.3859(422.0467), Bit/dim 4.3003(best: 3.8865), Xent 2.7868, Loss 5.6937, Error 0.7235(best: 0.4988)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0571 | Time 72.4638(65.8991) | Bit/dim 4.2995(3.9665) | Xent 2.9069(1.6013) | Loss 18.2993(11.9918) | Error 0.7245(0.5595) Steps 0(0.00) | Grad Norm 48.0983(11.6183) | Total Time 0.00(0.00)\n",
      "Iter 0572 | Time 56.9241(65.6299) | Bit/dim 4.3952(3.9793) | Xent 1.9856(1.6128) | Loss 12.1836(11.9975) | Error 0.6432(0.5620) Steps 0(0.00) | Grad Norm 29.7464(12.1621) | Total Time 0.00(0.00)\n",
      "Iter 0573 | Time 62.1816(65.5264) | Bit/dim 4.3388(3.9901) | Xent 1.7552(1.6171) | Loss 12.4547(12.0112) | Error 0.6156(0.5636) Steps 0(0.00) | Grad Norm 9.5059(12.0824) | Total Time 0.00(0.00)\n",
      "Iter 0574 | Time 67.1419(65.5749) | Bit/dim 4.2398(3.9976) | Xent 1.7496(1.6210) | Loss 12.3205(12.0205) | Error 0.6196(0.5653) Steps 0(0.00) | Grad Norm 12.5370(12.0961) | Total Time 0.00(0.00)\n",
      "Iter 0575 | Time 68.9650(65.6766) | Bit/dim 4.1544(4.0023) | Xent 1.7187(1.6240) | Loss 11.8307(12.0148) | Error 0.6135(0.5667) Steps 0(0.00) | Grad Norm 6.7446(11.9355) | Total Time 0.00(0.00)\n",
      "Iter 0576 | Time 64.0058(65.6265) | Bit/dim 4.2215(4.0089) | Xent 1.7132(1.6266) | Loss 12.1027(12.0175) | Error 0.6062(0.5679) Steps 0(0.00) | Grad Norm 7.4375(11.8006) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0096 | Time 27.1112, Epoch Time 434.5742(422.4225), Bit/dim 4.1652(best: 3.8865), Xent 1.6679, Loss 4.9992, Error 0.5785(best: 0.4988)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0577 | Time 70.4648(65.7716) | Bit/dim 4.1635(4.0135) | Xent 1.7168(1.6294) | Loss 16.6844(12.1575) | Error 0.5990(0.5688) Steps 0(0.00) | Grad Norm 6.7519(11.6491) | Total Time 0.00(0.00)\n",
      "Iter 0578 | Time 66.8039(65.8026) | Bit/dim 4.1107(4.0164) | Xent 1.6970(1.6314) | Loss 11.7143(12.1442) | Error 0.6048(0.5699) Steps 0(0.00) | Grad Norm 5.9628(11.4785) | Total Time 0.00(0.00)\n",
      "Iter 0579 | Time 62.1918(65.6943) | Bit/dim 4.0845(4.0185) | Xent 1.6905(1.6332) | Loss 11.6854(12.1304) | Error 0.6021(0.5709) Steps 0(0.00) | Grad Norm 5.3004(11.2932) | Total Time 0.00(0.00)\n",
      "Iter 0580 | Time 64.6974(65.6644) | Bit/dim 4.0831(4.0204) | Xent 1.6902(1.6349) | Loss 11.6141(12.1149) | Error 0.5948(0.5716) Steps 0(0.00) | Grad Norm 6.4431(11.1477) | Total Time 0.00(0.00)\n",
      "Iter 0581 | Time 68.1318(65.7384) | Bit/dim 4.0648(4.0217) | Xent 1.6474(1.6352) | Loss 11.7220(12.1031) | Error 0.5737(0.5717) Steps 0(0.00) | Grad Norm 4.7961(10.9571) | Total Time 0.00(0.00)\n",
      "Iter 0582 | Time 67.1257(65.7800) | Bit/dim 4.0445(4.0224) | Xent 1.6375(1.6353) | Loss 11.7032(12.0911) | Error 0.5915(0.5722) Steps 0(0.00) | Grad Norm 4.9553(10.7771) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0097 | Time 26.9579, Epoch Time 442.6922(423.0306), Bit/dim 4.0279(best: 3.8865), Xent 1.5858, Loss 4.8208, Error 0.5676(best: 0.4988)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0583 | Time 72.4176(65.9791) | Bit/dim 4.0229(4.0224) | Xent 1.6240(1.6350) | Loss 15.7425(12.2007) | Error 0.5775(0.5724) Steps 0(0.00) | Grad Norm 6.1714(10.6389) | Total Time 0.00(0.00)\n",
      "Iter 0584 | Time 68.0167(66.0403) | Bit/dim 4.0283(4.0226) | Xent 1.6403(1.6351) | Loss 11.6234(12.1834) | Error 0.5862(0.5728) Steps 0(0.00) | Grad Norm 7.2910(10.5385) | Total Time 0.00(0.00)\n",
      "Iter 0585 | Time 64.3641(65.9900) | Bit/dim 4.0222(4.0226) | Xent 1.5892(1.6338) | Loss 11.4773(12.1622) | Error 0.5660(0.5726) Steps 0(0.00) | Grad Norm 5.1424(10.3766) | Total Time 0.00(0.00)\n",
      "Iter 0586 | Time 70.3637(66.1212) | Bit/dim 3.9972(4.0218) | Xent 1.5840(1.6323) | Loss 11.3821(12.1388) | Error 0.5707(0.5726) Steps 0(0.00) | Grad Norm 7.2257(10.2821) | Total Time 0.00(0.00)\n",
      "Iter 0587 | Time 64.1303(66.0615) | Bit/dim 3.9771(4.0205) | Xent 1.6215(1.6319) | Loss 10.9262(12.1024) | Error 0.5780(0.5727) Steps 0(0.00) | Grad Norm 9.7896(10.2673) | Total Time 0.00(0.00)\n",
      "Iter 0588 | Time 64.6129(66.0180) | Bit/dim 3.9780(4.0192) | Xent 1.5876(1.6306) | Loss 11.3730(12.0805) | Error 0.5655(0.5725) Steps 0(0.00) | Grad Norm 10.9807(10.2887) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0098 | Time 26.6742, Epoch Time 447.2218(423.7564), Bit/dim 3.9687(best: 3.8865), Xent 1.5416, Loss 4.7395, Error 0.5479(best: 0.4988)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0589 | Time 62.7189(65.9190) | Bit/dim 3.9699(4.0177) | Xent 1.5965(1.6296) | Loss 15.5631(12.1850) | Error 0.5733(0.5725) Steps 0(0.00) | Grad Norm 10.1656(10.2850) | Total Time 0.00(0.00)\n",
      "Iter 0590 | Time 58.2310(65.6884) | Bit/dim 3.9631(4.0161) | Xent 1.5595(1.6275) | Loss 11.2632(12.1573) | Error 0.5584(0.5721) Steps 0(0.00) | Grad Norm 8.6807(10.2369) | Total Time 0.00(0.00)\n",
      "Iter 0591 | Time 69.4000(65.7997) | Bit/dim 3.9677(4.0147) | Xent 1.5454(1.6250) | Loss 11.3989(12.1346) | Error 0.5573(0.5717) Steps 0(0.00) | Grad Norm 4.1599(10.0546) | Total Time 0.00(0.00)\n",
      "Iter 0592 | Time 66.1328(65.8097) | Bit/dim 3.9339(4.0122) | Xent 1.5236(1.6220) | Loss 11.0952(12.1034) | Error 0.5425(0.5708) Steps 0(0.00) | Grad Norm 3.4079(9.8552) | Total Time 0.00(0.00)\n",
      "Iter 0593 | Time 67.0584(65.8472) | Bit/dim 3.9415(4.0101) | Xent 1.5318(1.6193) | Loss 11.4315(12.0832) | Error 0.5468(0.5701) Steps 0(0.00) | Grad Norm 6.2743(9.7477) | Total Time 0.00(0.00)\n",
      "Iter 0594 | Time 71.6592(66.0216) | Bit/dim 3.9231(4.0075) | Xent 1.5434(1.6170) | Loss 11.3808(12.0622) | Error 0.5549(0.5696) Steps 0(0.00) | Grad Norm 8.0400(9.6965) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0099 | Time 27.8425, Epoch Time 439.2912(424.2224), Bit/dim 3.9279(best: 3.8865), Xent 1.4830, Loss 4.6694, Error 0.5272(best: 0.4988)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0595 | Time 80.5558(66.4576) | Bit/dim 3.9267(4.0051) | Xent 1.5328(1.6145) | Loss 15.6469(12.1697) | Error 0.5426(0.5688) Steps 0(0.00) | Grad Norm 8.6949(9.6665) | Total Time 0.00(0.00)\n",
      "Iter 0596 | Time 62.6739(66.3441) | Bit/dim 3.9345(4.0030) | Xent 1.5781(1.6134) | Loss 11.2121(12.1410) | Error 0.5653(0.5687) Steps 0(0.00) | Grad Norm 12.5589(9.7532) | Total Time 0.00(0.00)\n",
      "Iter 0597 | Time 64.4242(66.2865) | Bit/dim 3.9587(4.0016) | Xent 1.6312(1.6139) | Loss 11.3958(12.1186) | Error 0.5830(0.5691) Steps 0(0.00) | Grad Norm 19.3698(10.0417) | Total Time 0.00(0.00)\n",
      "Iter 0598 | Time 66.9283(66.3057) | Bit/dim 3.9380(3.9997) | Xent 1.6187(1.6141) | Loss 11.2499(12.0926) | Error 0.5860(0.5696) Steps 0(0.00) | Grad Norm 14.1532(10.1651) | Total Time 0.00(0.00)\n",
      "Iter 0599 | Time 69.1519(66.3911) | Bit/dim 3.9252(3.9975) | Xent 1.6021(1.6137) | Loss 11.1210(12.0634) | Error 0.5687(0.5696) Steps 0(0.00) | Grad Norm 10.5197(10.1757) | Total Time 0.00(0.00)\n",
      "Iter 0600 | Time 67.6208(66.4280) | Bit/dim 3.9432(3.9959) | Xent 1.8892(1.6220) | Loss 11.5651(12.0485) | Error 0.6564(0.5722) Steps 0(0.00) | Grad Norm 35.9669(10.9494) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0100 | Time 26.6942, Epoch Time 454.5389(425.1319), Bit/dim 3.9596(best: 3.8865), Xent 1.6728, Loss 4.7960, Error 0.5826(best: 0.4988)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0601 | Time 68.5335(66.4912) | Bit/dim 3.9605(3.9948) | Xent 1.6905(1.6240) | Loss 15.9860(12.1666) | Error 0.5945(0.5729) Steps 0(0.00) | Grad Norm 17.5107(11.1463) | Total Time 0.00(0.00)\n",
      "Iter 0602 | Time 68.9132(66.5638) | Bit/dim 3.9268(3.9928) | Xent 1.5173(1.6208) | Loss 11.2838(12.1401) | Error 0.5445(0.5720) Steps 0(0.00) | Grad Norm 3.4639(10.9158) | Total Time 0.00(0.00)\n",
      "Iter 0603 | Time 63.7977(66.4808) | Bit/dim 3.9572(3.9917) | Xent 1.6321(1.6212) | Loss 11.0848(12.1085) | Error 0.5759(0.5721) Steps 0(0.00) | Grad Norm 15.0825(11.0408) | Total Time 0.00(0.00)\n",
      "Iter 0604 | Time 69.6611(66.5763) | Bit/dim 3.9635(3.9908) | Xent 1.6939(1.6233) | Loss 10.9801(12.0746) | Error 0.5993(0.5730) Steps 0(0.00) | Grad Norm 13.2899(11.1083) | Total Time 0.00(0.00)\n",
      "Iter 0605 | Time 64.5848(66.5165) | Bit/dim 3.9461(3.9895) | Xent 1.6326(1.6236) | Loss 11.4089(12.0546) | Error 0.5890(0.5734) Steps 0(0.00) | Grad Norm 8.4199(11.0276) | Total Time 0.00(0.00)\n",
      "Iter 0606 | Time 61.0442(66.3523) | Bit/dim 3.9848(3.9894) | Xent 1.5482(1.6214) | Loss 11.2054(12.0292) | Error 0.5540(0.5728) Steps 0(0.00) | Grad Norm 7.0598(10.9086) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0101 | Time 28.0730, Epoch Time 440.9721(425.6071), Bit/dim 3.9693(best: 3.8865), Xent 1.4867, Loss 4.7127, Error 0.5312(best: 0.4988)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0607 | Time 64.6163(66.3003) | Bit/dim 3.9791(3.9891) | Xent 1.5353(1.6188) | Loss 15.5330(12.1343) | Error 0.5510(0.5722) Steps 0(0.00) | Grad Norm 5.6892(10.7520) | Total Time 0.00(0.00)\n",
      "Iter 0608 | Time 65.9814(66.2907) | Bit/dim 3.9624(3.9883) | Xent 1.5543(1.6168) | Loss 11.0523(12.1018) | Error 0.5533(0.5716) Steps 0(0.00) | Grad Norm 8.1139(10.6729) | Total Time 0.00(0.00)\n",
      "Iter 0609 | Time 63.2004(66.1980) | Bit/dim 3.9315(3.9866) | Xent 1.5016(1.6134) | Loss 11.0109(12.0691) | Error 0.5304(0.5704) Steps 0(0.00) | Grad Norm 5.4956(10.5175) | Total Time 0.00(0.00)\n",
      "Iter 0610 | Time 63.4619(66.1159) | Bit/dim 3.9380(3.9851) | Xent 1.5151(1.6104) | Loss 11.2717(12.0452) | Error 0.5413(0.5695) Steps 0(0.00) | Grad Norm 4.6519(10.3416) | Total Time 0.00(0.00)\n",
      "Iter 0611 | Time 74.9584(66.3812) | Bit/dim 3.9292(3.9834) | Xent 1.4944(1.6069) | Loss 11.1900(12.0195) | Error 0.5353(0.5685) Steps 0(0.00) | Grad Norm 6.4505(10.2248) | Total Time 0.00(0.00)\n",
      "Iter 0612 | Time 71.6709(66.5399) | Bit/dim 3.9291(3.9818) | Xent 1.4873(1.6034) | Loss 11.1200(11.9925) | Error 0.5353(0.5675) Steps 0(0.00) | Grad Norm 5.8892(10.0948) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0102 | Time 26.8072, Epoch Time 446.8285(426.2438), Bit/dim 3.9085(best: 3.8865), Xent 1.4299, Loss 4.6234, Error 0.5169(best: 0.4988)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0613 | Time 65.9481(66.5221) | Bit/dim 3.9044(3.9795) | Xent 1.4965(1.6002) | Loss 15.9769(12.1121) | Error 0.5356(0.5665) Steps 0(0.00) | Grad Norm 5.2413(9.9492) | Total Time 0.00(0.00)\n",
      "Iter 0614 | Time 64.7137(66.4679) | Bit/dim 3.9086(3.9773) | Xent 1.4841(1.5967) | Loss 10.8595(12.0745) | Error 0.5441(0.5659) Steps 0(0.00) | Grad Norm 3.6725(9.7609) | Total Time 0.00(0.00)\n",
      "Iter 0615 | Time 67.9964(66.5137) | Bit/dim 3.9037(3.9751) | Xent 1.4440(1.5921) | Loss 10.9705(12.0414) | Error 0.5139(0.5643) Steps 0(0.00) | Grad Norm 3.4745(9.5723) | Total Time 0.00(0.00)\n",
      "Iter 0616 | Time 72.2279(66.6851) | Bit/dim 3.9005(3.9729) | Xent 1.4782(1.5887) | Loss 10.7718(12.0033) | Error 0.5336(0.5634) Steps 0(0.00) | Grad Norm 4.0040(9.4052) | Total Time 0.00(0.00)\n",
      "Iter 0617 | Time 74.1267(66.9084) | Bit/dim 3.9000(3.9707) | Xent 1.4717(1.5852) | Loss 10.7249(11.9649) | Error 0.5246(0.5622) Steps 0(0.00) | Grad Norm 6.9181(9.3306) | Total Time 0.00(0.00)\n",
      "Iter 0618 | Time 65.9140(66.8786) | Bit/dim 3.8892(3.9683) | Xent 1.4626(1.5815) | Loss 10.9066(11.9332) | Error 0.5191(0.5609) Steps 0(0.00) | Grad Norm 8.8533(9.3163) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0103 | Time 25.1532, Epoch Time 452.0463(427.0178), Bit/dim 3.9010(best: 3.8865), Xent 1.4489, Loss 4.6255, Error 0.5176(best: 0.4988)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0619 | Time 64.1034(66.7953) | Bit/dim 3.9051(3.9664) | Xent 1.4961(1.5789) | Loss 14.9504(12.0237) | Error 0.5244(0.5598) Steps 0(0.00) | Grad Norm 12.4763(9.4111) | Total Time 0.00(0.00)\n",
      "Iter 0620 | Time 60.8471(66.6169) | Bit/dim 3.9194(3.9650) | Xent 1.5857(1.5791) | Loss 11.1654(11.9979) | Error 0.5574(0.5598) Steps 0(0.00) | Grad Norm 15.7417(9.6010) | Total Time 0.00(0.00)\n",
      "Iter 0621 | Time 62.8926(66.5051) | Bit/dim 3.8847(3.9625) | Xent 1.4859(1.5763) | Loss 10.9604(11.9668) | Error 0.5316(0.5589) Steps 0(0.00) | Grad Norm 10.3690(9.6241) | Total Time 0.00(0.00)\n",
      "Iter 0622 | Time 68.6063(66.5682) | Bit/dim 3.8858(3.9602) | Xent 1.4489(1.5725) | Loss 10.7815(11.9313) | Error 0.5252(0.5579) Steps 0(0.00) | Grad Norm 5.0014(9.4854) | Total Time 0.00(0.00)\n",
      "Iter 0623 | Time 79.6204(66.9597) | Bit/dim 3.8796(3.9578) | Xent 1.5006(1.5704) | Loss 11.1844(11.9088) | Error 0.5316(0.5571) Steps 0(0.00) | Grad Norm 9.5022(9.4859) | Total Time 0.00(0.00)\n",
      "Iter 0624 | Time 65.5524(66.9175) | Bit/dim 3.8549(3.9547) | Xent 1.4681(1.5673) | Loss 10.6054(11.8697) | Error 0.5272(0.5562) Steps 0(0.00) | Grad Norm 5.5071(9.3665) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0104 | Time 26.0681, Epoch Time 443.9781(427.5266), Bit/dim 3.8808(best: 3.8865), Xent 1.4031, Loss 4.5823, Error 0.5011(best: 0.4988)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0625 | Time 68.3071(66.9592) | Bit/dim 3.8718(3.9523) | Xent 1.4576(1.5640) | Loss 15.5592(11.9804) | Error 0.5234(0.5552) Steps 0(0.00) | Grad Norm 7.2372(9.3026) | Total Time 0.00(0.00)\n",
      "Iter 0626 | Time 67.6984(66.9814) | Bit/dim 3.8701(3.9498) | Xent 1.4643(1.5610) | Loss 10.9723(11.9502) | Error 0.5125(0.5539) Steps 0(0.00) | Grad Norm 8.2730(9.2717) | Total Time 0.00(0.00)\n",
      "Iter 0627 | Time 61.3055(66.8111) | Bit/dim 3.8685(3.9473) | Xent 1.4471(1.5576) | Loss 10.7485(11.9141) | Error 0.5092(0.5526) Steps 0(0.00) | Grad Norm 4.8220(9.1383) | Total Time 0.00(0.00)\n",
      "Iter 0628 | Time 71.4843(66.9513) | Bit/dim 3.8742(3.9452) | Xent 1.4326(1.5538) | Loss 10.8032(11.8808) | Error 0.5099(0.5513) Steps 0(0.00) | Grad Norm 6.6182(9.0627) | Total Time 0.00(0.00)\n",
      "Iter 0629 | Time 67.1250(66.9565) | Bit/dim 3.8624(3.9427) | Xent 1.4466(1.5506) | Loss 10.9997(11.8544) | Error 0.5165(0.5503) Steps 0(0.00) | Grad Norm 6.9191(8.9983) | Total Time 0.00(0.00)\n",
      "Iter 0630 | Time 65.4457(66.9112) | Bit/dim 3.8462(3.9398) | Xent 1.4122(1.5465) | Loss 10.7156(11.8202) | Error 0.5079(0.5490) Steps 0(0.00) | Grad Norm 3.0562(8.8201) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0105 | Time 24.8629, Epoch Time 443.1145(427.9943), Bit/dim 3.8593(best: 3.8808), Xent 1.3819, Loss 4.5502, Error 0.4945(best: 0.4988)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0631 | Time 66.8085(66.9081) | Bit/dim 3.8659(3.9376) | Xent 1.4442(1.5434) | Loss 15.4378(11.9287) | Error 0.5182(0.5481) Steps 0(0.00) | Grad Norm 7.0565(8.7672) | Total Time 0.00(0.00)\n",
      "Iter 0632 | Time 62.9034(66.7880) | Bit/dim 3.8498(3.9349) | Xent 1.4279(1.5399) | Loss 10.7647(11.8938) | Error 0.5104(0.5470) Steps 0(0.00) | Grad Norm 8.6941(8.7650) | Total Time 0.00(0.00)\n",
      "Iter 0633 | Time 69.2228(66.8610) | Bit/dim 3.8408(3.9321) | Xent 1.4142(1.5362) | Loss 10.7481(11.8594) | Error 0.5019(0.5456) Steps 0(0.00) | Grad Norm 5.8908(8.6788) | Total Time 0.00(0.00)\n",
      "Iter 0634 | Time 65.8993(66.8322) | Bit/dim 3.8597(3.9299) | Xent 1.4023(1.5321) | Loss 11.0050(11.8338) | Error 0.5104(0.5445) Steps 0(0.00) | Grad Norm 4.5511(8.5549) | Total Time 0.00(0.00)\n",
      "Iter 0635 | Time 73.8708(67.0433) | Bit/dim 3.8508(3.9276) | Xent 1.3953(1.5280) | Loss 10.6788(11.7992) | Error 0.5040(0.5433) Steps 0(0.00) | Grad Norm 5.0186(8.4488) | Total Time 0.00(0.00)\n",
      "Iter 0636 | Time 73.9951(67.2519) | Bit/dim 3.8509(3.9253) | Xent 1.3803(1.5236) | Loss 10.8078(11.7694) | Error 0.4899(0.5417) Steps 0(0.00) | Grad Norm 3.0520(8.2869) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0106 | Time 26.9958, Epoch Time 455.5528(428.8210), Bit/dim 3.8448(best: 3.8593), Xent 1.3261, Loss 4.5079, Error 0.4705(best: 0.4945)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0637 | Time 68.8811(67.3007) | Bit/dim 3.8446(3.9228) | Xent 1.3827(1.5194) | Loss 15.0436(11.8676) | Error 0.4941(0.5403) Steps 0(0.00) | Grad Norm 2.3658(8.1093) | Total Time 0.00(0.00)\n",
      "Iter 0638 | Time 67.1579(67.2965) | Bit/dim 3.8414(3.9204) | Xent 1.3700(1.5149) | Loss 10.7981(11.8356) | Error 0.4911(0.5388) Steps 0(0.00) | Grad Norm 3.1048(7.9592) | Total Time 0.00(0.00)\n",
      "Iter 0639 | Time 69.2389(67.3547) | Bit/dim 3.8402(3.9180) | Xent 1.3666(1.5104) | Loss 10.7947(11.8043) | Error 0.4919(0.5374) Steps 0(0.00) | Grad Norm 3.9902(7.8401) | Total Time 0.00(0.00)\n",
      "Iter 0640 | Time 67.6198(67.3627) | Bit/dim 3.8460(3.9158) | Xent 1.3789(1.5065) | Loss 10.6674(11.7702) | Error 0.4916(0.5360) Steps 0(0.00) | Grad Norm 5.5452(7.7712) | Total Time 0.00(0.00)\n",
      "Iter 0641 | Time 71.7899(67.4955) | Bit/dim 3.8357(3.9134) | Xent 1.4543(1.5049) | Loss 10.6497(11.7366) | Error 0.5115(0.5353) Steps 0(0.00) | Grad Norm 11.7287(7.8900) | Total Time 0.00(0.00)\n",
      "Iter 0642 | Time 67.7288(67.5025) | Bit/dim 3.8741(3.9122) | Xent 1.6037(1.5079) | Loss 10.9165(11.7120) | Error 0.5609(0.5361) Steps 0(0.00) | Grad Norm 20.1830(8.2588) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0107 | Time 25.3103, Epoch Time 453.6824(429.5669), Bit/dim 3.9046(best: 3.8448), Xent 1.5467, Loss 4.6780, Error 0.5461(best: 0.4705)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0643 | Time 66.0019(67.4575) | Bit/dim 3.8977(3.9118) | Xent 1.5956(1.5105) | Loss 15.9831(11.8401) | Error 0.5573(0.5367) Steps 0(0.00) | Grad Norm 15.6270(8.4798) | Total Time 0.00(0.00)\n",
      "Iter 0644 | Time 68.8455(67.4991) | Bit/dim 3.8865(3.9111) | Xent 1.4718(1.5094) | Loss 11.1568(11.8196) | Error 0.5326(0.5366) Steps 0(0.00) | Grad Norm 10.4609(8.5392) | Total Time 0.00(0.00)\n",
      "Iter 0645 | Time 68.6157(67.5326) | Bit/dim 3.8928(3.9105) | Xent 1.5963(1.5120) | Loss 11.0727(11.7972) | Error 0.5624(0.5374) Steps 0(0.00) | Grad Norm 12.3687(8.6541) | Total Time 0.00(0.00)\n",
      "Iter 0646 | Time 65.4993(67.4716) | Bit/dim 3.9002(3.9102) | Xent 1.5052(1.5118) | Loss 11.1481(11.7778) | Error 0.5445(0.5376) Steps 0(0.00) | Grad Norm 7.8461(8.6299) | Total Time 0.00(0.00)\n",
      "Iter 0647 | Time 69.5366(67.5336) | Bit/dim 3.9048(3.9100) | Xent 1.4900(1.5111) | Loss 11.1453(11.7588) | Error 0.5354(0.5375) Steps 0(0.00) | Grad Norm 6.9546(8.5796) | Total Time 0.00(0.00)\n",
      "Iter 0648 | Time 64.0901(67.4303) | Bit/dim 3.9287(3.9106) | Xent 1.4534(1.5094) | Loss 11.1948(11.7419) | Error 0.5230(0.5371) Steps 0(0.00) | Grad Norm 5.4425(8.4855) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0108 | Time 26.0423, Epoch Time 444.5196(430.0155), Bit/dim 3.9123(best: 3.8448), Xent 1.3670, Loss 4.5958, Error 0.4872(best: 0.4705)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0649 | Time 70.5577(67.5241) | Bit/dim 3.9197(3.9109) | Xent 1.4370(1.5072) | Loss 15.6710(11.8597) | Error 0.5176(0.5365) Steps 0(0.00) | Grad Norm 5.0578(8.3827) | Total Time 0.00(0.00)\n",
      "Iter 0650 | Time 71.2532(67.6360) | Bit/dim 3.9066(3.9107) | Xent 1.3816(1.5034) | Loss 10.9777(11.8333) | Error 0.4945(0.5352) Steps 0(0.00) | Grad Norm 3.9056(8.2484) | Total Time 0.00(0.00)\n",
      "Iter 0651 | Time 67.9556(67.6456) | Bit/dim 3.9021(3.9105) | Xent 1.3747(1.4996) | Loss 11.1093(11.8116) | Error 0.4964(0.5341) Steps 0(0.00) | Grad Norm 3.8852(8.1175) | Total Time 0.00(0.00)\n",
      "Iter 0652 | Time 77.8475(67.9516) | Bit/dim 3.8809(3.9096) | Xent 1.3764(1.4959) | Loss 10.7089(11.7785) | Error 0.4840(0.5326) Steps 0(0.00) | Grad Norm 3.5671(7.9810) | Total Time 0.00(0.00)\n",
      "Iter 0653 | Time 77.6812(68.2435) | Bit/dim 3.8940(3.9091) | Xent 1.4133(1.4934) | Loss 10.8540(11.7507) | Error 0.5079(0.5318) Steps 0(0.00) | Grad Norm 5.1246(7.8953) | Total Time 0.00(0.00)\n",
      "Iter 0654 | Time 70.4779(68.3105) | Bit/dim 3.8842(3.9084) | Xent 1.3898(1.4903) | Loss 11.0731(11.7304) | Error 0.4970(0.5308) Steps 0(0.00) | Grad Norm 9.4320(7.9414) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0109 | Time 27.7012, Epoch Time 479.4422(431.4983), Bit/dim 3.9054(best: 3.8448), Xent 1.4372, Loss 4.6241, Error 0.5058(best: 0.4705)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0655 | Time 69.7836(68.3547) | Bit/dim 3.8991(3.9081) | Xent 1.5034(1.4907) | Loss 15.7474(11.8509) | Error 0.5276(0.5307) Steps 0(0.00) | Grad Norm 13.8985(8.1201) | Total Time 0.00(0.00)\n",
      "Iter 0656 | Time 68.4754(68.3583) | Bit/dim 3.8859(3.9074) | Xent 1.5279(1.4918) | Loss 11.2376(11.8325) | Error 0.5468(0.5312) Steps 0(0.00) | Grad Norm 16.8708(8.3826) | Total Time 0.00(0.00)\n",
      "Iter 0657 | Time 65.5057(68.2728) | Bit/dim 3.8853(3.9068) | Xent 1.4859(1.4916) | Loss 11.0703(11.8097) | Error 0.5404(0.5314) Steps 0(0.00) | Grad Norm 12.2358(8.4982) | Total Time 0.00(0.00)\n",
      "Iter 0658 | Time 62.4851(68.0991) | Bit/dim 3.8637(3.9055) | Xent 1.4300(1.4898) | Loss 10.9606(11.7842) | Error 0.5135(0.5309) Steps 0(0.00) | Grad Norm 4.7768(8.3866) | Total Time 0.00(0.00)\n",
      "Iter 0659 | Time 66.6168(68.0547) | Bit/dim 3.8912(3.9050) | Xent 1.4804(1.4895) | Loss 10.9445(11.7590) | Error 0.5308(0.5309) Steps 0(0.00) | Grad Norm 10.3874(8.4466) | Total Time 0.00(0.00)\n",
      "Iter 0660 | Time 71.7409(68.1652) | Bit/dim 3.8646(3.9038) | Xent 1.4384(1.4880) | Loss 11.0977(11.7392) | Error 0.5155(0.5304) Steps 0(0.00) | Grad Norm 7.8681(8.4292) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0110 | Time 26.1170, Epoch Time 446.8904(431.9600), Bit/dim 3.8781(best: 3.8448), Xent 1.3620, Loss 4.5591, Error 0.4898(best: 0.4705)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0661 | Time 65.9950(68.1001) | Bit/dim 3.8751(3.9030) | Xent 1.4075(1.4856) | Loss 15.1780(11.8423) | Error 0.5000(0.5295) Steps 0(0.00) | Grad Norm 6.5103(8.3717) | Total Time 0.00(0.00)\n",
      "Iter 0662 | Time 71.0978(68.1901) | Bit/dim 3.8609(3.9017) | Xent 1.3914(1.4827) | Loss 10.7341(11.8091) | Error 0.4948(0.5285) Steps 0(0.00) | Grad Norm 3.9426(8.2388) | Total Time 0.00(0.00)\n",
      "Iter 0663 | Time 65.0192(68.0949) | Bit/dim 3.8734(3.9009) | Xent 1.3888(1.4799) | Loss 10.9179(11.7823) | Error 0.4992(0.5276) Steps 0(0.00) | Grad Norm 5.1136(8.1450) | Total Time 0.00(0.00)\n",
      "Iter 0664 | Time 68.0322(68.0931) | Bit/dim 3.8615(3.8997) | Xent 1.3782(1.4769) | Loss 11.0677(11.7609) | Error 0.4930(0.5266) Steps 0(0.00) | Grad Norm 4.9938(8.0505) | Total Time 0.00(0.00)\n",
      "Iter 0665 | Time 67.5767(68.0776) | Bit/dim 3.8752(3.8989) | Xent 1.3546(1.4732) | Loss 10.7356(11.7301) | Error 0.4844(0.5253) Steps 0(0.00) | Grad Norm 5.2826(7.9675) | Total Time 0.00(0.00)\n",
      "Iter 0666 | Time 65.3580(67.9960) | Bit/dim 3.8511(3.8975) | Xent 1.3649(1.4699) | Loss 10.8868(11.7048) | Error 0.4874(0.5242) Steps 0(0.00) | Grad Norm 5.0107(7.8788) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0111 | Time 24.8148, Epoch Time 444.0717(432.3234), Bit/dim 3.8498(best: 3.8448), Xent 1.3019, Loss 4.5008, Error 0.4651(best: 0.4705)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0667 | Time 69.0269(68.0269) | Bit/dim 3.8437(3.8959) | Xent 1.3318(1.4658) | Loss 15.0530(11.8053) | Error 0.4752(0.5227) Steps 0(0.00) | Grad Norm 3.6511(7.7519) | Total Time 0.00(0.00)\n",
      "Iter 0668 | Time 69.1608(68.0609) | Bit/dim 3.8390(3.8942) | Xent 1.3487(1.4623) | Loss 10.9627(11.7800) | Error 0.4761(0.5213) Steps 0(0.00) | Grad Norm 3.2530(7.6170) | Total Time 0.00(0.00)\n",
      "Iter 0669 | Time 67.6607(68.0489) | Bit/dim 3.8293(3.8922) | Xent 1.3276(1.4582) | Loss 10.7584(11.7494) | Error 0.4688(0.5197) Steps 0(0.00) | Grad Norm 3.5645(7.4954) | Total Time 0.00(0.00)\n",
      "Iter 0670 | Time 65.9802(67.9869) | Bit/dim 3.8496(3.8910) | Xent 1.3375(1.4546) | Loss 10.7429(11.7192) | Error 0.4754(0.5184) Steps 0(0.00) | Grad Norm 3.3576(7.3713) | Total Time 0.00(0.00)\n",
      "Iter 0671 | Time 63.5644(67.8542) | Bit/dim 3.8407(3.8895) | Xent 1.3324(1.4510) | Loss 10.5614(11.6844) | Error 0.4809(0.5173) Steps 0(0.00) | Grad Norm 4.1046(7.2733) | Total Time 0.00(0.00)\n",
      "Iter 0672 | Time 66.0156(67.7990) | Bit/dim 3.8301(3.8877) | Xent 1.3242(1.4472) | Loss 10.6563(11.6536) | Error 0.4664(0.5157) Steps 0(0.00) | Grad Norm 2.8232(7.1398) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0112 | Time 25.5356, Epoch Time 442.8156(432.6381), Bit/dim 3.8295(best: 3.8448), Xent 1.2734, Loss 4.4662, Error 0.4553(best: 0.4651)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0673 | Time 63.3598(67.6659) | Bit/dim 3.8200(3.8856) | Xent 1.3228(1.4434) | Loss 14.9766(11.7533) | Error 0.4718(0.5144) Steps 0(0.00) | Grad Norm 3.2148(7.0220) | Total Time 0.00(0.00)\n",
      "Iter 0674 | Time 67.9733(67.6751) | Bit/dim 3.8367(3.8842) | Xent 1.3343(1.4402) | Loss 10.7070(11.7219) | Error 0.4771(0.5133) Steps 0(0.00) | Grad Norm 4.6503(6.9509) | Total Time 0.00(0.00)\n",
      "Iter 0675 | Time 66.1924(67.6306) | Bit/dim 3.8279(3.8825) | Xent 1.3297(1.4368) | Loss 10.5701(11.6873) | Error 0.4754(0.5122) Steps 0(0.00) | Grad Norm 5.8047(6.9165) | Total Time 0.00(0.00)\n",
      "Iter 0676 | Time 71.7140(67.7531) | Bit/dim 3.8393(3.8812) | Xent 1.3635(1.4346) | Loss 10.9498(11.6652) | Error 0.4875(0.5114) Steps 0(0.00) | Grad Norm 8.5139(6.9644) | Total Time 0.00(0.00)\n",
      "Iter 0677 | Time 65.5741(67.6877) | Bit/dim 3.8565(3.8805) | Xent 1.5702(1.4387) | Loss 11.0215(11.6459) | Error 0.5554(0.5127) Steps 0(0.00) | Grad Norm 14.0122(7.1758) | Total Time 0.00(0.00)\n",
      "Iter 0678 | Time 59.9030(67.4542) | Bit/dim 3.8869(3.8806) | Xent 1.7778(1.4489) | Loss 11.2519(11.6341) | Error 0.5867(0.5150) Steps 0(0.00) | Grad Norm 35.4944(8.0254) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0113 | Time 26.4600, Epoch Time 436.9557(432.7677), Bit/dim 3.8989(best: 3.8295), Xent 1.4474, Loss 4.6226, Error 0.5115(best: 0.4553)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0679 | Time 71.4704(67.5747) | Bit/dim 3.9029(3.8813) | Xent 1.5104(1.4507) | Loss 15.0090(11.7353) | Error 0.5373(0.5156) Steps 0(0.00) | Grad Norm 12.4939(8.1594) | Total Time 0.00(0.00)\n",
      "Iter 0680 | Time 70.9654(67.6764) | Bit/dim 3.8664(3.8809) | Xent 1.4667(1.4512) | Loss 10.7366(11.7054) | Error 0.5297(0.5161) Steps 0(0.00) | Grad Norm 8.8537(8.1803) | Total Time 0.00(0.00)\n",
      "Iter 0681 | Time 66.1284(67.6300) | Bit/dim 3.8943(3.8813) | Xent 1.4066(1.4499) | Loss 11.1173(11.6877) | Error 0.5104(0.5159) Steps 0(0.00) | Grad Norm 6.6836(8.1354) | Total Time 0.00(0.00)\n",
      "Iter 0682 | Time 73.6896(67.8117) | Bit/dim 3.8836(3.8813) | Xent 1.4290(1.4492) | Loss 11.0664(11.6691) | Error 0.5160(0.5159) Steps 0(0.00) | Grad Norm 7.8568(8.1270) | Total Time 0.00(0.00)\n",
      "Iter 0683 | Time 70.1745(67.8826) | Bit/dim 3.8914(3.8816) | Xent 1.3912(1.4475) | Loss 11.0631(11.6509) | Error 0.4988(0.5154) Steps 0(0.00) | Grad Norm 7.6023(8.1113) | Total Time 0.00(0.00)\n",
      "Iter 0684 | Time 57.3357(67.5662) | Bit/dim 3.8774(3.8815) | Xent 1.4388(1.4472) | Loss 10.7013(11.6224) | Error 0.5182(0.5155) Steps 0(0.00) | Grad Norm 8.1106(8.1112) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0114 | Time 25.5438, Epoch Time 451.0273(433.3155), Bit/dim 3.8709(best: 3.8295), Xent 1.3222, Loss 4.5320, Error 0.4754(best: 0.4553)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0685 | Time 64.9166(67.4867) | Bit/dim 3.8759(3.8813) | Xent 1.3703(1.4449) | Loss 14.5493(11.7102) | Error 0.4895(0.5147) Steps 0(0.00) | Grad Norm 3.8961(7.9848) | Total Time 0.00(0.00)\n",
      "Iter 0686 | Time 64.0323(67.3831) | Bit/dim 3.8600(3.8807) | Xent 1.3506(1.4421) | Loss 10.6886(11.6796) | Error 0.4932(0.5140) Steps 0(0.00) | Grad Norm 5.5808(7.9127) | Total Time 0.00(0.00)\n",
      "Iter 0687 | Time 64.4375(67.2947) | Bit/dim 3.8480(3.8797) | Xent 1.4096(1.4411) | Loss 10.6243(11.6479) | Error 0.5075(0.5138) Steps 0(0.00) | Grad Norm 7.4023(7.8974) | Total Time 0.00(0.00)\n",
      "Iter 0688 | Time 66.3496(67.2664) | Bit/dim 3.8632(3.8792) | Xent 1.3419(1.4381) | Loss 10.8675(11.6245) | Error 0.4830(0.5129) Steps 0(0.00) | Grad Norm 5.6646(7.8304) | Total Time 0.00(0.00)\n",
      "Iter 0689 | Time 72.4987(67.4233) | Bit/dim 3.8579(3.8786) | Xent 1.3296(1.4349) | Loss 10.8377(11.6009) | Error 0.4758(0.5118) Steps 0(0.00) | Grad Norm 3.8834(7.7120) | Total Time 0.00(0.00)\n",
      "Iter 0690 | Time 63.8181(67.3152) | Bit/dim 3.8520(3.8778) | Xent 1.3525(1.4324) | Loss 10.8325(11.5778) | Error 0.4870(0.5111) Steps 0(0.00) | Grad Norm 4.0685(7.6027) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0115 | Time 26.4888, Epoch Time 438.3982(433.4679), Bit/dim 3.8411(best: 3.8295), Xent 1.3104, Loss 4.4963, Error 0.4686(best: 0.4553)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0691 | Time 69.1980(67.3717) | Bit/dim 3.8371(3.8766) | Xent 1.3545(1.4301) | Loss 14.8182(11.6751) | Error 0.4841(0.5102) Steps 0(0.00) | Grad Norm 6.3855(7.5662) | Total Time 0.00(0.00)\n",
      "Iter 0692 | Time 71.6766(67.5008) | Bit/dim 3.8322(3.8752) | Xent 1.3329(1.4272) | Loss 10.6997(11.6458) | Error 0.4764(0.5092) Steps 0(0.00) | Grad Norm 4.7291(7.4810) | Total Time 0.00(0.00)\n",
      "Iter 0693 | Time 66.8508(67.4813) | Bit/dim 3.8427(3.8743) | Xent 1.3440(1.4247) | Loss 10.8081(11.6207) | Error 0.4804(0.5084) Steps 0(0.00) | Grad Norm 3.5045(7.3617) | Total Time 0.00(0.00)\n",
      "Iter 0694 | Time 66.4801(67.4513) | Bit/dim 3.8226(3.8727) | Xent 1.3422(1.4222) | Loss 10.7766(11.5953) | Error 0.4838(0.5076) Steps 0(0.00) | Grad Norm 6.5688(7.3380) | Total Time 0.00(0.00)\n",
      "Iter 0695 | Time 64.4521(67.3613) | Bit/dim 3.8272(3.8713) | Xent 1.3322(1.4195) | Loss 10.7580(11.5702) | Error 0.4772(0.5067) Steps 0(0.00) | Grad Norm 4.6920(7.2586) | Total Time 0.00(0.00)\n",
      "Iter 0696 | Time 67.6323(67.3694) | Bit/dim 3.8274(3.8700) | Xent 1.3180(1.4164) | Loss 10.6918(11.5439) | Error 0.4685(0.5056) Steps 0(0.00) | Grad Norm 2.4392(7.1140) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0116 | Time 25.5869, Epoch Time 447.6354(433.8930), Bit/dim 3.8256(best: 3.8295), Xent 1.2665, Loss 4.4589, Error 0.4470(best: 0.4553)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0697 | Time 70.0099(67.4487) | Bit/dim 3.8201(3.8685) | Xent 1.3249(1.4137) | Loss 14.9337(11.6456) | Error 0.4735(0.5046) Steps 0(0.00) | Grad Norm 4.4925(7.0354) | Total Time 0.00(0.00)\n",
      "Iter 0698 | Time 63.7125(67.3366) | Bit/dim 3.8285(3.8673) | Xent 1.3280(1.4111) | Loss 10.8092(11.6205) | Error 0.4799(0.5039) Steps 0(0.00) | Grad Norm 6.0899(7.0070) | Total Time 0.00(0.00)\n",
      "Iter 0699 | Time 62.6336(67.1955) | Bit/dim 3.8181(3.8659) | Xent 1.3205(1.4084) | Loss 10.7798(11.5953) | Error 0.4738(0.5030) Steps 0(0.00) | Grad Norm 6.3939(6.9886) | Total Time 0.00(0.00)\n",
      "Iter 0700 | Time 66.9845(67.1891) | Bit/dim 3.8208(3.8645) | Xent 1.3201(1.4058) | Loss 10.8121(11.5718) | Error 0.4738(0.5021) Steps 0(0.00) | Grad Norm 6.0338(6.9599) | Total Time 0.00(0.00)\n",
      "Iter 0701 | Time 62.4803(67.0479) | Bit/dim 3.8073(3.8628) | Xent 1.2908(1.4023) | Loss 10.1755(11.5299) | Error 0.4627(0.5009) Steps 0(0.00) | Grad Norm 5.7630(6.9240) | Total Time 0.00(0.00)\n",
      "Iter 0702 | Time 67.1727(67.0516) | Bit/dim 3.8181(3.8614) | Xent 1.3211(1.3999) | Loss 10.3496(11.4945) | Error 0.4692(0.5000) Steps 0(0.00) | Grad Norm 6.8547(6.9220) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0117 | Time 25.6496, Epoch Time 434.3427(433.9065), Bit/dim 3.8228(best: 3.8256), Xent 1.3440, Loss 4.4948, Error 0.4882(best: 0.4470)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0703 | Time 59.9543(66.8387) | Bit/dim 3.8245(3.8603) | Xent 1.3925(1.3997) | Loss 14.2735(11.5778) | Error 0.4964(0.4998) Steps 0(0.00) | Grad Norm 7.5840(6.9418) | Total Time 0.00(0.00)\n",
      "Iter 0704 | Time 70.3382(66.9437) | Bit/dim 3.8108(3.8589) | Xent 1.2936(1.3965) | Loss 10.4503(11.5440) | Error 0.4617(0.4987) Steps 0(0.00) | Grad Norm 6.4283(6.9264) | Total Time 0.00(0.00)\n",
      "Iter 0705 | Time 71.2676(67.0734) | Bit/dim 3.8258(3.8579) | Xent 1.3750(1.3958) | Loss 10.7565(11.5204) | Error 0.4826(0.4982) Steps 0(0.00) | Grad Norm 10.8623(7.0445) | Total Time 0.00(0.00)\n",
      "Iter 0706 | Time 57.3968(66.7831) | Bit/dim 3.8798(3.8585) | Xent 1.4872(1.3986) | Loss 10.6863(11.4954) | Error 0.5359(0.4994) Steps 0(0.00) | Grad Norm 9.2379(7.1103) | Total Time 0.00(0.00)\n",
      "Iter 0707 | Time 65.4451(66.7430) | Bit/dim 3.8559(3.8584) | Xent 1.6049(1.4048) | Loss 11.0697(11.4826) | Error 0.5473(0.5008) Steps 0(0.00) | Grad Norm 21.0477(7.5284) | Total Time 0.00(0.00)\n",
      "Iter 0708 | Time 67.3642(66.7616) | Bit/dim 3.9037(3.8598) | Xent 2.0342(1.4236) | Loss 11.5231(11.4838) | Error 0.6578(0.5055) Steps 0(0.00) | Grad Norm 30.7697(8.2257) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0118 | Time 26.5315, Epoch Time 434.3548(433.9199), Bit/dim 3.9451(best: 3.8228), Xent 1.8739, Loss 4.8820, Error 0.6184(best: 0.4470)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0709 | Time 63.7835(66.6723) | Bit/dim 3.9470(3.8624) | Xent 1.9438(1.4392) | Loss 15.9201(11.6169) | Error 0.6231(0.5090) Steps 0(0.00) | Grad Norm 19.0811(8.5513) | Total Time 0.00(0.00)\n",
      "Iter 0710 | Time 69.8758(66.7684) | Bit/dim 4.0277(3.8674) | Xent 1.7583(1.4488) | Loss 11.5957(11.6163) | Error 0.6226(0.5124) Steps 0(0.00) | Grad Norm 18.8996(8.8618) | Total Time 0.00(0.00)\n",
      "Iter 0711 | Time 73.7118(66.9767) | Bit/dim 4.0072(3.8716) | Xent 2.1822(1.4708) | Loss 11.7892(11.6214) | Error 0.7361(0.5191) Steps 0(0.00) | Grad Norm 18.9022(9.1630) | Total Time 0.00(0.00)\n",
      "Iter 0712 | Time 71.1334(67.1014) | Bit/dim 4.0507(3.8769) | Xent 1.9991(1.4867) | Loss 11.6996(11.6238) | Error 0.6731(0.5238) Steps 0(0.00) | Grad Norm 25.0507(9.6396) | Total Time 0.00(0.00)\n",
      "Iter 0713 | Time 64.2799(67.0167) | Bit/dim 4.1118(3.8840) | Xent 1.8582(1.4978) | Loss 11.8287(11.6299) | Error 0.6754(0.5283) Steps 0(0.00) | Grad Norm 13.9087(9.7677) | Total Time 0.00(0.00)\n",
      "Iter 0714 | Time 73.9794(67.2256) | Bit/dim 4.1030(3.8906) | Xent 1.6929(1.5037) | Loss 11.6667(11.6310) | Error 0.6098(0.5308) Steps 0(0.00) | Grad Norm 7.8108(9.7090) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0119 | Time 27.7108, Epoch Time 460.3767(434.7136), Bit/dim 4.1087(best: 3.8228), Xent 1.6631, Loss 4.9403, Error 0.5997(best: 0.4470)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0715 | Time 68.3778(67.2602) | Bit/dim 4.1056(3.8970) | Xent 1.7476(1.5110) | Loss 16.5681(11.7792) | Error 0.6210(0.5335) Steps 0(0.00) | Grad Norm 9.8503(9.7132) | Total Time 0.00(0.00)\n",
      "Iter 0716 | Time 63.8624(67.1582) | Bit/dim 4.0839(3.9026) | Xent 1.5903(1.5134) | Loss 11.4800(11.7702) | Error 0.5696(0.5345) Steps 0(0.00) | Grad Norm 4.5152(9.5573) | Total Time 0.00(0.00)\n",
      "Iter 0717 | Time 71.9159(67.3010) | Bit/dim 4.0880(3.9082) | Xent 1.5858(1.5155) | Loss 11.4808(11.7615) | Error 0.5584(0.5353) Steps 0(0.00) | Grad Norm 4.5691(9.4076) | Total Time 0.00(0.00)\n",
      "Iter 0718 | Time 68.2066(67.3281) | Bit/dim 4.0421(3.9122) | Xent 1.6207(1.5187) | Loss 11.3735(11.7499) | Error 0.5713(0.5363) Steps 0(0.00) | Grad Norm 4.1123(9.2488) | Total Time 0.00(0.00)\n",
      "Iter 0719 | Time 66.7096(67.3096) | Bit/dim 4.0283(3.9157) | Xent 1.5795(1.5205) | Loss 11.1347(11.7314) | Error 0.5621(0.5371) Steps 0(0.00) | Grad Norm 4.3142(9.1007) | Total Time 0.00(0.00)\n",
      "Iter 0720 | Time 70.7204(67.4119) | Bit/dim 4.0149(3.9187) | Xent 1.5291(1.5208) | Loss 11.5804(11.7269) | Error 0.5453(0.5374) Steps 0(0.00) | Grad Norm 3.4824(8.9322) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0120 | Time 28.4068, Epoch Time 454.1678(435.2972), Bit/dim 3.9927(best: 3.8228), Xent 1.4882, Loss 4.7368, Error 0.5258(best: 0.4470)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0721 | Time 64.8704(67.3357) | Bit/dim 3.9800(3.9205) | Xent 1.5362(1.5212) | Loss 16.5490(11.8715) | Error 0.5389(0.5374) Steps 0(0.00) | Grad Norm 3.9433(8.7825) | Total Time 0.00(0.00)\n",
      "Iter 0722 | Time 70.0313(67.4165) | Bit/dim 3.9831(3.9224) | Xent 1.5518(1.5222) | Loss 11.3781(11.8567) | Error 0.5523(0.5379) Steps 0(0.00) | Grad Norm 5.3523(8.6796) | Total Time 0.00(0.00)\n",
      "Iter 0723 | Time 64.8871(67.3407) | Bit/dim 3.9599(3.9235) | Xent 1.5213(1.5221) | Loss 11.1903(11.8367) | Error 0.5403(0.5379) Steps 0(0.00) | Grad Norm 5.0616(8.5711) | Total Time 0.00(0.00)\n",
      "Iter 0724 | Time 72.4218(67.4931) | Bit/dim 3.9431(3.9241) | Xent 1.5076(1.5217) | Loss 11.4036(11.8237) | Error 0.5421(0.5380) Steps 0(0.00) | Grad Norm 5.7601(8.4867) | Total Time 0.00(0.00)\n",
      "Iter 0725 | Time 71.3378(67.6084) | Bit/dim 3.9304(3.9243) | Xent 1.4776(1.5204) | Loss 11.2677(11.8071) | Error 0.5286(0.5378) Steps 0(0.00) | Grad Norm 3.7674(8.3452) | Total Time 0.00(0.00)\n",
      "Iter 0726 | Time 69.0560(67.6519) | Bit/dim 3.9200(3.9241) | Xent 1.4881(1.5194) | Loss 11.0079(11.7831) | Error 0.5334(0.5376) Steps 0(0.00) | Grad Norm 4.0851(8.2174) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0121 | Time 27.4316, Epoch Time 455.7869(435.9119), Bit/dim 3.9130(best: 3.8228), Xent 1.4483, Loss 4.6371, Error 0.5134(best: 0.4470)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0727 | Time 67.7619(67.6552) | Bit/dim 3.9190(3.9240) | Xent 1.5013(1.5189) | Loss 15.7471(11.9020) | Error 0.5328(0.5375) Steps 0(0.00) | Grad Norm 7.3026(8.1899) | Total Time 0.00(0.00)\n",
      "Iter 0728 | Time 71.4657(67.7695) | Bit/dim 3.8994(3.9233) | Xent 1.4658(1.5173) | Loss 11.3275(11.8848) | Error 0.5196(0.5370) Steps 0(0.00) | Grad Norm 5.8359(8.1193) | Total Time 0.00(0.00)\n",
      "Iter 0729 | Time 68.3573(67.7871) | Bit/dim 3.8940(3.9224) | Xent 1.4526(1.5153) | Loss 11.1706(11.8634) | Error 0.5255(0.5366) Steps 0(0.00) | Grad Norm 4.5188(8.0113) | Total Time 0.00(0.00)\n",
      "Iter 0730 | Time 64.7494(67.6960) | Bit/dim 3.8881(3.9213) | Xent 1.4660(1.5138) | Loss 11.0314(11.8384) | Error 0.5292(0.5364) Steps 0(0.00) | Grad Norm 3.1549(7.8656) | Total Time 0.00(0.00)\n",
      "Iter 0731 | Time 70.6208(67.7837) | Bit/dim 3.8816(3.9202) | Xent 1.4444(1.5118) | Loss 11.1763(11.8185) | Error 0.5139(0.5357) Steps 0(0.00) | Grad Norm 3.7591(7.7424) | Total Time 0.00(0.00)\n",
      "Iter 0732 | Time 65.0074(67.7004) | Bit/dim 3.8797(3.9189) | Xent 1.4355(1.5095) | Loss 11.0636(11.7959) | Error 0.5177(0.5352) Steps 0(0.00) | Grad Norm 3.9166(7.6276) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0122 | Time 28.8808, Epoch Time 452.6823(436.4150), Bit/dim 3.8706(best: 3.8228), Xent 1.3770, Loss 4.5591, Error 0.4891(best: 0.4470)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0733 | Time 74.9957(67.9193) | Bit/dim 3.8808(3.9178) | Xent 1.4076(1.5064) | Loss 16.4055(11.9342) | Error 0.5069(0.5343) Steps 0(0.00) | Grad Norm 3.7025(7.5099) | Total Time 0.00(0.00)\n",
      "Iter 0734 | Time 69.1973(67.9576) | Bit/dim 3.8681(3.9163) | Xent 1.4079(1.5035) | Loss 11.0889(11.9088) | Error 0.5080(0.5335) Steps 0(0.00) | Grad Norm 4.3772(7.4159) | Total Time 0.00(0.00)\n",
      "Iter 0735 | Time 72.3280(68.0887) | Bit/dim 3.8666(3.9148) | Xent 1.4285(1.5012) | Loss 10.8306(11.8765) | Error 0.5155(0.5330) Steps 0(0.00) | Grad Norm 8.4562(7.4471) | Total Time 0.00(0.00)\n",
      "Iter 0736 | Time 72.4061(68.2183) | Bit/dim 3.8861(3.9140) | Xent 1.5401(1.5024) | Loss 11.1426(11.8544) | Error 0.5439(0.5333) Steps 0(0.00) | Grad Norm 14.8551(7.6693) | Total Time 0.00(0.00)\n",
      "Iter 0737 | Time 69.7282(68.2636) | Bit/dim 3.8755(3.9128) | Xent 1.6455(1.5067) | Loss 11.2825(11.8373) | Error 0.5787(0.5347) Steps 0(0.00) | Grad Norm 24.6047(8.1774) | Total Time 0.00(0.00)\n",
      "Iter 0738 | Time 71.7418(68.3679) | Bit/dim 3.8935(3.9122) | Xent 1.6722(1.5116) | Loss 11.4680(11.8262) | Error 0.5776(0.5360) Steps 0(0.00) | Grad Norm 22.0957(8.5949) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0123 | Time 28.7059, Epoch Time 475.5475(437.5890), Bit/dim 3.9415(best: 3.8228), Xent 1.5264, Loss 4.7047, Error 0.5431(best: 0.4470)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0739 | Time 70.2964(68.4258) | Bit/dim 3.9341(3.9129) | Xent 1.5789(1.5137) | Loss 16.1771(11.9567) | Error 0.5548(0.5365) Steps 0(0.00) | Grad Norm 13.7478(8.7495) | Total Time 0.00(0.00)\n",
      "Iter 0740 | Time 72.0494(68.5345) | Bit/dim 3.8966(3.9124) | Xent 1.4810(1.5127) | Loss 10.9499(11.9265) | Error 0.5236(0.5361) Steps 0(0.00) | Grad Norm 9.6932(8.7778) | Total Time 0.00(0.00)\n",
      "Iter 0741 | Time 83.5991(68.9864) | Bit/dim 3.9178(3.9126) | Xent 1.4460(1.5107) | Loss 11.1345(11.9028) | Error 0.5192(0.5356) Steps 0(0.00) | Grad Norm 6.5693(8.7116) | Total Time 0.00(0.00)\n",
      "Iter 0742 | Time 63.0589(68.8086) | Bit/dim 3.8835(3.9117) | Xent 1.4920(1.5101) | Loss 11.0454(11.8771) | Error 0.5348(0.5356) Steps 0(0.00) | Grad Norm 6.5236(8.6459) | Total Time 0.00(0.00)\n",
      "Iter 0743 | Time 75.2536(69.0019) | Bit/dim 3.8935(3.9111) | Xent 1.4182(1.5074) | Loss 11.0475(11.8522) | Error 0.5068(0.5347) Steps 0(0.00) | Grad Norm 4.3841(8.5181) | Total Time 0.00(0.00)\n",
      "Iter 0744 | Time 69.0723(69.0040) | Bit/dim 3.8775(3.9101) | Xent 1.4218(1.5048) | Loss 11.1342(11.8306) | Error 0.5088(0.5340) Steps 0(0.00) | Grad Norm 4.5423(8.3988) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0124 | Time 27.8151, Epoch Time 477.3999(438.7833), Bit/dim 3.8824(best: 3.8228), Xent 1.3722, Loss 4.5685, Error 0.4897(best: 0.4470)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0745 | Time 80.0094(69.3342) | Bit/dim 3.8837(3.9093) | Xent 1.4394(1.5028) | Loss 15.4907(11.9404) | Error 0.5186(0.5335) Steps 0(0.00) | Grad Norm 4.2342(8.2739) | Total Time 0.00(0.00)\n",
      "Iter 0746 | Time 87.6851(69.8847) | Bit/dim 3.8632(3.9079) | Xent 1.4019(1.4998) | Loss 10.8478(11.9076) | Error 0.4980(0.5324) Steps 0(0.00) | Grad Norm 3.7689(8.1387) | Total Time 0.00(0.00)\n",
      "Iter 0747 | Time 72.1113(69.9515) | Bit/dim 3.8543(3.9063) | Xent 1.3815(1.4963) | Loss 10.8755(11.8767) | Error 0.4885(0.5311) Steps 0(0.00) | Grad Norm 3.7694(8.0076) | Total Time 0.00(0.00)\n",
      "Iter 0748 | Time 71.3577(69.9937) | Bit/dim 3.8514(3.9047) | Xent 1.3753(1.4926) | Loss 10.7398(11.8426) | Error 0.4900(0.5299) Steps 0(0.00) | Grad Norm 3.8960(7.8843) | Total Time 0.00(0.00)\n",
      "Iter 0749 | Time 66.2852(69.8825) | Bit/dim 3.8594(3.9033) | Xent 1.3659(1.4888) | Loss 10.8257(11.8121) | Error 0.4832(0.5285) Steps 0(0.00) | Grad Norm 3.3534(7.7484) | Total Time 0.00(0.00)\n",
      "Iter 0750 | Time 63.2080(69.6822) | Bit/dim 3.8425(3.9015) | Xent 1.3513(1.4847) | Loss 10.4689(11.7718) | Error 0.4764(0.5269) Steps 0(0.00) | Grad Norm 4.2198(7.6425) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0125 | Time 26.2605, Epoch Time 482.8352(440.1049), Bit/dim 3.8460(best: 3.8228), Xent 1.3025, Loss 4.4972, Error 0.4637(best: 0.4470)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0751 | Time 79.9558(69.9904) | Bit/dim 3.8381(3.8996) | Xent 1.3625(1.4810) | Loss 15.3295(11.8785) | Error 0.4804(0.5255) Steps 0(0.00) | Grad Norm 3.0396(7.5044) | Total Time 0.00(0.00)\n",
      "Iter 0752 | Time 67.6566(69.9204) | Bit/dim 3.8346(3.8977) | Xent 1.3632(1.4775) | Loss 10.8641(11.8481) | Error 0.4865(0.5244) Steps 0(0.00) | Grad Norm 3.6382(7.3884) | Total Time 0.00(0.00)\n",
      "Iter 0753 | Time 68.1531(69.8674) | Bit/dim 3.8409(3.8960) | Xent 1.3290(1.4730) | Loss 10.6249(11.8114) | Error 0.4705(0.5227) Steps 0(0.00) | Grad Norm 2.7475(7.2492) | Total Time 0.00(0.00)\n",
      "Iter 0754 | Time 65.5723(69.7385) | Bit/dim 3.8404(3.8943) | Xent 1.3310(1.4688) | Loss 10.6177(11.7756) | Error 0.4701(0.5212) Steps 0(0.00) | Grad Norm 3.1181(7.1253) | Total Time 0.00(0.00)\n",
      "Iter 0755 | Time 81.1071(70.0796) | Bit/dim 3.8163(3.8919) | Xent 1.3178(1.4643) | Loss 10.6846(11.7428) | Error 0.4683(0.5196) Steps 0(0.00) | Grad Norm 3.1701(7.0066) | Total Time 0.00(0.00)\n",
      "Iter 0756 | Time 67.2273(69.9940) | Bit/dim 3.8214(3.8898) | Xent 1.3119(1.4597) | Loss 10.4885(11.7052) | Error 0.4650(0.5179) Steps 0(0.00) | Grad Norm 3.9748(6.9157) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0126 | Time 26.7410, Epoch Time 472.5370(441.0779), Bit/dim 3.8170(best: 3.8228), Xent 1.2795, Loss 4.4567, Error 0.4566(best: 0.4470)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0757 | Time 79.3930(70.2760) | Bit/dim 3.8166(3.8876) | Xent 1.3139(1.4553) | Loss 15.1366(11.8082) | Error 0.4710(0.5165) Steps 0(0.00) | Grad Norm 3.2955(6.8071) | Total Time 0.00(0.00)\n",
      "Iter 0758 | Time 67.2475(70.1851) | Bit/dim 3.8155(3.8855) | Xent 1.2917(1.4504) | Loss 10.7164(11.7754) | Error 0.4607(0.5149) Steps 0(0.00) | Grad Norm 2.4608(6.6767) | Total Time 0.00(0.00)\n",
      "Iter 0759 | Time 69.6451(70.1689) | Bit/dim 3.7979(3.8828) | Xent 1.3180(1.4464) | Loss 10.7872(11.7458) | Error 0.4709(0.5135) Steps 0(0.00) | Grad Norm 2.5957(6.5542) | Total Time 0.00(0.00)\n",
      "Iter 0760 | Time 67.5748(70.0911) | Bit/dim 3.8065(3.8806) | Xent 1.3105(1.4423) | Loss 10.4949(11.7082) | Error 0.4636(0.5120) Steps 0(0.00) | Grad Norm 1.9545(6.4163) | Total Time 0.00(0.00)\n",
      "Iter 0761 | Time 68.5452(70.0447) | Bit/dim 3.8066(3.8783) | Xent 1.3081(1.4383) | Loss 10.4625(11.6709) | Error 0.4639(0.5106) Steps 0(0.00) | Grad Norm 2.7688(6.3068) | Total Time 0.00(0.00)\n",
      "Iter 0762 | Time 72.8883(70.1300) | Bit/dim 3.8083(3.8762) | Xent 1.3353(1.4352) | Loss 10.6669(11.6407) | Error 0.4745(0.5095) Steps 0(0.00) | Grad Norm 5.2455(6.2750) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0127 | Time 25.4721, Epoch Time 466.6585(441.8453), Bit/dim 3.8090(best: 3.8170), Xent 1.2689, Loss 4.4434, Error 0.4505(best: 0.4470)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0763 | Time 68.7116(70.0875) | Bit/dim 3.8051(3.8741) | Xent 1.3238(1.4319) | Loss 14.5733(11.7287) | Error 0.4771(0.5085) Steps 0(0.00) | Grad Norm 6.2927(6.2755) | Total Time 0.00(0.00)\n",
      "Iter 0764 | Time 74.5561(70.2216) | Bit/dim 3.8023(3.8719) | Xent 1.3110(1.4283) | Loss 10.6710(11.6970) | Error 0.4716(0.5074) Steps 0(0.00) | Grad Norm 7.6903(6.3180) | Total Time 0.00(0.00)\n",
      "Iter 0765 | Time 77.0491(70.4264) | Bit/dim 3.7984(3.8697) | Xent 1.3743(1.4266) | Loss 10.7359(11.6682) | Error 0.4842(0.5067) Steps 0(0.00) | Grad Norm 10.8107(6.4527) | Total Time 0.00(0.00)\n",
      "Iter 0766 | Time 65.3691(70.2747) | Bit/dim 3.8148(3.8681) | Xent 1.4207(1.4265) | Loss 10.7257(11.6399) | Error 0.5081(0.5068) Steps 0(0.00) | Grad Norm 13.6285(6.6680) | Total Time 0.00(0.00)\n",
      "Iter 0767 | Time 68.1399(70.2106) | Bit/dim 3.8374(3.8672) | Xent 1.4249(1.4264) | Loss 10.9300(11.6186) | Error 0.5079(0.5068) Steps 0(0.00) | Grad Norm 15.5759(6.9353) | Total Time 0.00(0.00)\n",
      "Iter 0768 | Time 67.8109(70.1386) | Bit/dim 3.8676(3.8672) | Xent 1.4438(1.4269) | Loss 10.8722(11.5962) | Error 0.5236(0.5073) Steps 0(0.00) | Grad Norm 11.9668(7.0862) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0128 | Time 26.7651, Epoch Time 464.4158(442.5224), Bit/dim 3.8140(best: 3.8090), Xent 1.5321, Loss 4.5800, Error 0.5351(best: 0.4470)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0769 | Time 62.7142(69.9159) | Bit/dim 3.8232(3.8659) | Xent 1.5930(1.4319) | Loss 15.0360(11.6994) | Error 0.5549(0.5087) Steps 0(0.00) | Grad Norm 15.5462(7.3400) | Total Time 0.00(0.00)\n",
      "Iter 0770 | Time 70.3857(69.9300) | Bit/dim 3.8407(3.8651) | Xent 1.7995(1.4430) | Loss 11.3306(11.6883) | Error 0.6142(0.5119) Steps 0(0.00) | Grad Norm 22.7624(7.8027) | Total Time 0.00(0.00)\n",
      "Iter 0771 | Time 63.0164(69.7226) | Bit/dim 3.8536(3.8648) | Xent 1.6479(1.4491) | Loss 10.9503(11.6662) | Error 0.5953(0.5144) Steps 0(0.00) | Grad Norm 13.2981(7.9675) | Total Time 0.00(0.00)\n",
      "Iter 0772 | Time 78.2280(69.9777) | Bit/dim 3.8854(3.8654) | Xent 1.4270(1.4484) | Loss 11.0636(11.6481) | Error 0.5188(0.5145) Steps 0(0.00) | Grad Norm 6.0410(7.9097) | Total Time 0.00(0.00)\n",
      "Iter 0773 | Time 68.1419(69.9227) | Bit/dim 3.8908(3.8661) | Xent 1.4519(1.4485) | Loss 11.0281(11.6295) | Error 0.5296(0.5150) Steps 0(0.00) | Grad Norm 6.5135(7.8679) | Total Time 0.00(0.00)\n",
      "Iter 0774 | Time 69.0709(69.8971) | Bit/dim 3.8619(3.8660) | Xent 1.4101(1.4474) | Loss 10.7686(11.6037) | Error 0.5109(0.5149) Steps 0(0.00) | Grad Norm 4.9715(7.7810) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0129 | Time 25.7473, Epoch Time 453.2646(442.8447), Bit/dim 3.8576(best: 3.8090), Xent 1.3534, Loss 4.5343, Error 0.4937(best: 0.4470)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0775 | Time 60.9656(69.6292) | Bit/dim 3.8560(3.8657) | Xent 1.4133(1.4464) | Loss 15.0090(11.7058) | Error 0.5110(0.5148) Steps 0(0.00) | Grad Norm 4.6357(7.6866) | Total Time 0.00(0.00)\n",
      "Iter 0776 | Time 65.7196(69.5119) | Bit/dim 3.8683(3.8658) | Xent 1.3764(1.4443) | Loss 10.6183(11.6732) | Error 0.4938(0.5141) Steps 0(0.00) | Grad Norm 4.1021(7.5791) | Total Time 0.00(0.00)\n",
      "Iter 0777 | Time 64.0915(69.3493) | Bit/dim 3.8775(3.8661) | Xent 1.3833(1.4424) | Loss 10.9199(11.6506) | Error 0.5014(0.5137) Steps 0(0.00) | Grad Norm 3.6572(7.4614) | Total Time 0.00(0.00)\n",
      "Iter 0778 | Time 62.4355(69.1419) | Bit/dim 3.8404(3.8654) | Xent 1.3535(1.4398) | Loss 10.8880(11.6277) | Error 0.4899(0.5130) Steps 0(0.00) | Grad Norm 3.4431(7.3409) | Total Time 0.00(0.00)\n",
      "Iter 0779 | Time 64.4537(69.0012) | Bit/dim 3.8472(3.8648) | Xent 1.3442(1.4369) | Loss 10.9272(11.6067) | Error 0.4791(0.5120) Steps 0(0.00) | Grad Norm 2.5691(7.1977) | Total Time 0.00(0.00)\n",
      "Iter 0780 | Time 70.4737(69.0454) | Bit/dim 3.8535(3.8645) | Xent 1.3620(1.4347) | Loss 10.6094(11.5768) | Error 0.4894(0.5113) Steps 0(0.00) | Grad Norm 3.1376(7.0759) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0130 | Time 25.5059, Epoch Time 429.7052(442.4505), Bit/dim 3.8220(best: 3.8090), Xent 1.3061, Loss 4.4751, Error 0.4608(best: 0.4470)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0781 | Time 70.0188(69.0746) | Bit/dim 3.8381(3.8637) | Xent 1.3288(1.4315) | Loss 14.9272(11.6773) | Error 0.4766(0.5103) Steps 0(0.00) | Grad Norm 2.7903(6.9473) | Total Time 0.00(0.00)\n",
      "Iter 0782 | Time 72.3345(69.1724) | Bit/dim 3.8136(3.8622) | Xent 1.3370(1.4286) | Loss 10.7044(11.6481) | Error 0.4875(0.5096) Steps 0(0.00) | Grad Norm 2.3995(6.8109) | Total Time 0.00(0.00)\n",
      "Iter 0783 | Time 67.1802(69.1126) | Bit/dim 3.8219(3.8610) | Xent 1.3498(1.4263) | Loss 10.8558(11.6244) | Error 0.4800(0.5087) Steps 0(0.00) | Grad Norm 2.9577(6.6953) | Total Time 0.00(0.00)\n",
      "Iter 0784 | Time 68.0909(69.0820) | Bit/dim 3.8163(3.8596) | Xent 1.3369(1.4236) | Loss 10.7440(11.5979) | Error 0.4834(0.5080) Steps 0(0.00) | Grad Norm 2.9631(6.5833) | Total Time 0.00(0.00)\n",
      "Iter 0785 | Time 60.5687(68.8266) | Bit/dim 3.8193(3.8584) | Xent 1.3191(1.4205) | Loss 10.4944(11.5648) | Error 0.4768(0.5070) Steps 0(0.00) | Grad Norm 2.4857(6.4604) | Total Time 0.00(0.00)\n",
      "Iter 0786 | Time 61.6207(68.6104) | Bit/dim 3.8028(3.8568) | Xent 1.2981(1.4168) | Loss 10.5621(11.5348) | Error 0.4684(0.5059) Steps 0(0.00) | Grad Norm 2.4382(6.3397) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0131 | Time 25.9167, Epoch Time 441.6636(442.4269), Bit/dim 3.8056(best: 3.8090), Xent 1.2623, Loss 4.4368, Error 0.4497(best: 0.4470)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0787 | Time 60.7742(68.3753) | Bit/dim 3.8023(3.8551) | Xent 1.2911(1.4130) | Loss 15.3458(11.6491) | Error 0.4583(0.5044) Steps 0(0.00) | Grad Norm 2.3713(6.2207) | Total Time 0.00(0.00)\n",
      "Iter 0788 | Time 60.5367(68.1401) | Bit/dim 3.8039(3.8536) | Xent 1.3264(1.4104) | Loss 10.2070(11.6058) | Error 0.4736(0.5035) Steps 0(0.00) | Grad Norm 2.2249(6.1008) | Total Time 0.00(0.00)\n",
      "Iter 0789 | Time 63.1757(67.9912) | Bit/dim 3.7951(3.8518) | Xent 1.3044(1.4072) | Loss 10.4720(11.5718) | Error 0.4675(0.5024) Steps 0(0.00) | Grad Norm 2.7977(6.0017) | Total Time 0.00(0.00)\n",
      "Iter 0790 | Time 67.7457(67.9838) | Bit/dim 3.8011(3.8503) | Xent 1.2893(1.4037) | Loss 10.6250(11.5434) | Error 0.4675(0.5014) Steps 0(0.00) | Grad Norm 2.3551(5.8923) | Total Time 0.00(0.00)\n",
      "Iter 0791 | Time 64.3397(67.8745) | Bit/dim 3.7865(3.8484) | Xent 1.2976(1.4005) | Loss 10.6068(11.5153) | Error 0.4650(0.5003) Steps 0(0.00) | Grad Norm 2.1697(5.7806) | Total Time 0.00(0.00)\n",
      "Iter 0792 | Time 67.1683(67.8533) | Bit/dim 3.7798(3.8463) | Xent 1.2881(1.3972) | Loss 10.4547(11.4835) | Error 0.4650(0.4992) Steps 0(0.00) | Grad Norm 3.0798(5.6996) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0132 | Time 25.6636, Epoch Time 430.7048(442.0752), Bit/dim 3.7955(best: 3.8056), Xent 1.2465, Loss 4.4188, Error 0.4453(best: 0.4470)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0793 | Time 67.2122(67.8341) | Bit/dim 3.7914(3.8447) | Xent 1.2853(1.3938) | Loss 14.6396(11.5782) | Error 0.4615(0.4981) Steps 0(0.00) | Grad Norm 5.0990(5.6816) | Total Time 0.00(0.00)\n",
      "Iter 0794 | Time 63.9709(67.7182) | Bit/dim 3.8003(3.8434) | Xent 1.2647(1.3899) | Loss 10.6193(11.5494) | Error 0.4470(0.4966) Steps 0(0.00) | Grad Norm 5.4680(5.6752) | Total Time 0.00(0.00)\n",
      "Iter 0795 | Time 63.6133(67.5951) | Bit/dim 3.7827(3.8415) | Xent 1.3056(1.3874) | Loss 10.4931(11.5177) | Error 0.4677(0.4957) Steps 0(0.00) | Grad Norm 7.1288(5.7188) | Total Time 0.00(0.00)\n",
      "Iter 0796 | Time 62.3250(67.4370) | Bit/dim 3.8000(3.8403) | Xent 1.3621(1.3866) | Loss 10.4745(11.4864) | Error 0.4795(0.4952) Steps 0(0.00) | Grad Norm 10.4324(5.8602) | Total Time 0.00(0.00)\n",
      "Iter 0797 | Time 64.7619(67.3567) | Bit/dim 3.7876(3.8387) | Xent 1.4200(1.3876) | Loss 10.6891(11.4625) | Error 0.5002(0.4954) Steps 0(0.00) | Grad Norm 15.2877(6.1430) | Total Time 0.00(0.00)\n",
      "Iter 0798 | Time 65.9500(67.3145) | Bit/dim 3.8121(3.8379) | Xent 1.4342(1.3890) | Loss 10.7243(11.4404) | Error 0.5044(0.4956) Steps 0(0.00) | Grad Norm 12.3571(6.3295) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0133 | Time 26.0673, Epoch Time 429.9338(441.7110), Bit/dim 3.8033(best: 3.7955), Xent 1.2947, Loss 4.4507, Error 0.4665(best: 0.4453)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0799 | Time 67.7142(67.3265) | Bit/dim 3.7925(3.8366) | Xent 1.3399(1.3876) | Loss 13.9614(11.5160) | Error 0.4810(0.4952) Steps 0(0.00) | Grad Norm 5.1286(6.2934) | Total Time 0.00(0.00)\n",
      "Iter 0800 | Time 68.4359(67.3598) | Bit/dim 3.8158(3.8359) | Xent 1.3547(1.3866) | Loss 10.6928(11.4913) | Error 0.4795(0.4947) Steps 0(0.00) | Grad Norm 7.6115(6.3330) | Total Time 0.00(0.00)\n",
      "Iter 0801 | Time 62.1539(67.2036) | Bit/dim 3.7935(3.8347) | Xent 1.3297(1.3849) | Loss 10.5258(11.4623) | Error 0.4788(0.4942) Steps 0(0.00) | Grad Norm 4.1802(6.2684) | Total Time 0.00(0.00)\n",
      "Iter 0802 | Time 64.5764(67.1248) | Bit/dim 3.8053(3.8338) | Xent 1.3151(1.3828) | Loss 10.8619(11.4443) | Error 0.4664(0.4934) Steps 0(0.00) | Grad Norm 3.8947(6.1972) | Total Time 0.00(0.00)\n",
      "Iter 0803 | Time 75.9070(67.3882) | Bit/dim 3.7988(3.8327) | Xent 1.3101(1.3806) | Loss 10.9357(11.4291) | Error 0.4734(0.4928) Steps 0(0.00) | Grad Norm 3.1540(6.1059) | Total Time 0.00(0.00)\n",
      "Iter 0804 | Time 69.7874(67.4602) | Bit/dim 3.7885(3.8314) | Xent 1.3047(1.3783) | Loss 10.0066(11.3864) | Error 0.4766(0.4923) Steps 0(0.00) | Grad Norm 4.2014(6.0487) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0134 | Time 24.4977, Epoch Time 449.2311(441.9366), Bit/dim 3.7987(best: 3.7955), Xent 1.2489, Loss 4.4232, Error 0.4489(best: 0.4453)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0805 | Time 65.6819(67.4069) | Bit/dim 3.8064(3.8307) | Xent 1.2930(1.3758) | Loss 15.1356(11.4989) | Error 0.4611(0.4914) Steps 0(0.00) | Grad Norm 4.3120(5.9966) | Total Time 0.00(0.00)\n",
      "Iter 0806 | Time 64.8272(67.3295) | Bit/dim 3.7986(3.8297) | Xent 1.2550(1.3721) | Loss 10.5744(11.4711) | Error 0.4417(0.4899) Steps 0(0.00) | Grad Norm 3.8550(5.9324) | Total Time 0.00(0.00)\n",
      "Iter 0807 | Time 62.2519(67.1772) | Bit/dim 3.7841(3.8283) | Xent 1.2705(1.3691) | Loss 10.6277(11.4458) | Error 0.4583(0.4889) Steps 0(0.00) | Grad Norm 3.6129(5.8628) | Total Time 0.00(0.00)\n",
      "Iter 0808 | Time 66.9511(67.1704) | Bit/dim 3.7803(3.8269) | Xent 1.2448(1.3654) | Loss 10.6956(11.4233) | Error 0.4416(0.4875) Steps 0(0.00) | Grad Norm 3.5752(5.7942) | Total Time 0.00(0.00)\n",
      "Iter 0809 | Time 65.6511(67.1248) | Bit/dim 3.7827(3.8256) | Xent 1.2734(1.3626) | Loss 10.5508(11.3971) | Error 0.4577(0.4866) Steps 0(0.00) | Grad Norm 2.2992(5.6893) | Total Time 0.00(0.00)\n",
      "Iter 0810 | Time 68.8405(67.1763) | Bit/dim 3.7772(3.8241) | Xent 1.2510(1.3592) | Loss 10.7108(11.3765) | Error 0.4446(0.4854) Steps 0(0.00) | Grad Norm 2.6415(5.5979) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0135 | Time 26.1444, Epoch Time 436.3625(441.7693), Bit/dim 3.7788(best: 3.7955), Xent 1.2072, Loss 4.3824, Error 0.4346(best: 0.4453)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0811 | Time 62.7667(67.0440) | Bit/dim 3.7776(3.8227) | Xent 1.2618(1.3563) | Loss 14.9731(11.4844) | Error 0.4526(0.4844) Steps 0(0.00) | Grad Norm 2.4944(5.5048) | Total Time 0.00(0.00)\n",
      "Iter 0812 | Time 62.0830(66.8951) | Bit/dim 3.7732(3.8212) | Xent 1.2564(1.3533) | Loss 10.5443(11.4562) | Error 0.4497(0.4834) Steps 0(0.00) | Grad Norm 2.9136(5.4271) | Total Time 0.00(0.00)\n",
      "Iter 0813 | Time 66.5726(66.8855) | Bit/dim 3.7871(3.8202) | Xent 1.2333(1.3497) | Loss 10.4690(11.4266) | Error 0.4469(0.4823) Steps 0(0.00) | Grad Norm 2.6231(5.3429) | Total Time 0.00(0.00)\n",
      "Iter 0814 | Time 74.3884(67.1106) | Bit/dim 3.7714(3.8187) | Xent 1.2207(1.3459) | Loss 10.5157(11.3993) | Error 0.4436(0.4811) Steps 0(0.00) | Grad Norm 2.6422(5.2619) | Total Time 0.00(0.00)\n",
      "Iter 0815 | Time 68.8211(67.1619) | Bit/dim 3.7801(3.8176) | Xent 1.2421(1.3427) | Loss 10.3954(11.3692) | Error 0.4401(0.4799) Steps 0(0.00) | Grad Norm 4.2427(5.2313) | Total Time 0.00(0.00)\n",
      "Iter 0816 | Time 74.6824(67.3875) | Bit/dim 3.7747(3.8163) | Xent 1.2511(1.3400) | Loss 10.4009(11.3401) | Error 0.4467(0.4789) Steps 0(0.00) | Grad Norm 6.8699(5.2805) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0136 | Time 24.7338, Epoch Time 450.0897(442.0190), Bit/dim 3.7766(best: 3.7788), Xent 1.2661, Loss 4.4096, Error 0.4561(best: 0.4346)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0817 | Time 66.1351(67.3499) | Bit/dim 3.7926(3.8156) | Xent 1.3292(1.3397) | Loss 15.0791(11.4523) | Error 0.4734(0.4787) Steps 0(0.00) | Grad Norm 11.6123(5.4704) | Total Time 0.00(0.00)\n",
      "Iter 0818 | Time 70.4366(67.4425) | Bit/dim 3.8239(3.8158) | Xent 1.5204(1.3451) | Loss 11.1159(11.4422) | Error 0.5174(0.4799) Steps 0(0.00) | Grad Norm 14.5344(5.7424) | Total Time 0.00(0.00)\n",
      "Iter 0819 | Time 65.9328(67.3972) | Bit/dim 3.7762(3.8146) | Xent 1.3233(1.3444) | Loss 10.4460(11.4123) | Error 0.4794(0.4799) Steps 0(0.00) | Grad Norm 5.8517(5.7456) | Total Time 0.00(0.00)\n",
      "Iter 0820 | Time 66.8814(67.3818) | Bit/dim 3.7900(3.8139) | Xent 1.4075(1.3463) | Loss 10.5243(11.3857) | Error 0.5142(0.4809) Steps 0(0.00) | Grad Norm 7.1720(5.7884) | Total Time 0.00(0.00)\n",
      "Iter 0821 | Time 67.7737(67.3935) | Bit/dim 3.7886(3.8131) | Xent 1.3345(1.3460) | Loss 10.4200(11.3567) | Error 0.4829(0.4809) Steps 0(0.00) | Grad Norm 4.4889(5.7495) | Total Time 0.00(0.00)\n",
      "Iter 0822 | Time 73.4955(67.5766) | Bit/dim 3.7990(3.8127) | Xent 1.3554(1.3463) | Loss 10.5930(11.3338) | Error 0.4885(0.4812) Steps 0(0.00) | Grad Norm 7.6522(5.8065) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0137 | Time 26.1570, Epoch Time 452.8468(442.3438), Bit/dim 3.7929(best: 3.7766), Xent 1.3208, Loss 4.4533, Error 0.4762(best: 0.4346)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0823 | Time 61.4125(67.3916) | Bit/dim 3.7983(3.8123) | Xent 1.3705(1.3470) | Loss 15.1708(11.4489) | Error 0.4930(0.4815) Steps 0(0.00) | Grad Norm 12.0050(5.9925) | Total Time 0.00(0.00)\n",
      "Iter 0824 | Time 69.8254(67.4647) | Bit/dim 3.8039(3.8120) | Xent 1.4077(1.3488) | Loss 10.6149(11.4239) | Error 0.4971(0.4820) Steps 0(0.00) | Grad Norm 11.7419(6.1650) | Total Time 0.00(0.00)\n",
      "Iter 0825 | Time 54.6548(67.0804) | Bit/dim 3.8025(3.8117) | Xent 1.3489(1.3488) | Loss 10.5567(11.3979) | Error 0.4821(0.4820) Steps 0(0.00) | Grad Norm 9.3260(6.2598) | Total Time 0.00(0.00)\n",
      "Iter 0826 | Time 69.8751(67.1642) | Bit/dim 3.8090(3.8117) | Xent 1.4331(1.3513) | Loss 10.1991(11.3619) | Error 0.5094(0.4828) Steps 0(0.00) | Grad Norm 11.1911(6.4077) | Total Time 0.00(0.00)\n",
      "Iter 0827 | Time 77.7879(67.4829) | Bit/dim 3.8301(3.8122) | Xent 1.3936(1.3526) | Loss 10.8508(11.3466) | Error 0.4975(0.4833) Steps 0(0.00) | Grad Norm 9.8050(6.5097) | Total Time 0.00(0.00)\n",
      "Iter 0828 | Time 64.5029(67.3935) | Bit/dim 3.8082(3.8121) | Xent 1.3156(1.3515) | Loss 10.7995(11.3302) | Error 0.4661(0.4827) Steps 0(0.00) | Grad Norm 4.9901(6.4641) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0138 | Time 25.9956, Epoch Time 439.8842(442.2700), Bit/dim 3.8129(best: 3.7766), Xent 1.2389, Loss 4.4323, Error 0.4446(best: 0.4346)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0829 | Time 60.8165(67.1962) | Bit/dim 3.8025(3.8118) | Xent 1.2830(1.3494) | Loss 15.0903(11.4430) | Error 0.4594(0.4820) Steps 0(0.00) | Grad Norm 4.6443(6.4095) | Total Time 0.00(0.00)\n",
      "Iter 0830 | Time 66.1537(67.1649) | Bit/dim 3.7999(3.8115) | Xent 1.3100(1.3483) | Loss 10.7694(11.4228) | Error 0.4728(0.4818) Steps 0(0.00) | Grad Norm 5.3029(6.3763) | Total Time 0.00(0.00)\n",
      "Iter 0831 | Time 63.6359(67.0591) | Bit/dim 3.8000(3.8111) | Xent 1.2850(1.3464) | Loss 10.6748(11.4003) | Error 0.4599(0.4811) Steps 0(0.00) | Grad Norm 5.1935(6.3408) | Total Time 0.00(0.00)\n",
      "Iter 0832 | Time 69.7134(67.1387) | Bit/dim 3.7907(3.8105) | Xent 1.3014(1.3450) | Loss 10.7467(11.3807) | Error 0.4604(0.4805) Steps 0(0.00) | Grad Norm 3.9623(6.2694) | Total Time 0.00(0.00)\n",
      "Iter 0833 | Time 62.7218(67.0062) | Bit/dim 3.7887(3.8098) | Xent 1.2979(1.3436) | Loss 10.6177(11.3578) | Error 0.4676(0.4801) Steps 0(0.00) | Grad Norm 4.3176(6.2109) | Total Time 0.00(0.00)\n",
      "Iter 0834 | Time 66.9065(67.0032) | Bit/dim 3.7927(3.8093) | Xent 1.2827(1.3418) | Loss 10.6659(11.3371) | Error 0.4664(0.4797) Steps 0(0.00) | Grad Norm 4.2100(6.1509) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0139 | Time 25.8380, Epoch Time 432.0539(441.9635), Bit/dim 3.7858(best: 3.7766), Xent 1.2219, Loss 4.3967, Error 0.4404(best: 0.4346)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0835 | Time 65.2479(66.9505) | Bit/dim 3.7867(3.8087) | Xent 1.2628(1.3394) | Loss 14.3592(11.4277) | Error 0.4561(0.4790) Steps 0(0.00) | Grad Norm 3.6114(6.0747) | Total Time 0.00(0.00)\n",
      "Iter 0836 | Time 66.5119(66.9374) | Bit/dim 3.7893(3.8081) | Xent 1.2702(1.3373) | Loss 10.6031(11.4030) | Error 0.4627(0.4785) Steps 0(0.00) | Grad Norm 3.5155(5.9979) | Total Time 0.00(0.00)\n",
      "Iter 0837 | Time 60.9137(66.7567) | Bit/dim 3.7826(3.8073) | Xent 1.2705(1.3353) | Loss 10.6275(11.3797) | Error 0.4511(0.4777) Steps 0(0.00) | Grad Norm 2.4544(5.8916) | Total Time 0.00(0.00)\n",
      "Iter 0838 | Time 62.2868(66.6226) | Bit/dim 3.7720(3.8062) | Xent 1.2661(1.3332) | Loss 10.4802(11.3527) | Error 0.4530(0.4769) Steps 0(0.00) | Grad Norm 3.8296(5.8297) | Total Time 0.00(0.00)\n",
      "Iter 0839 | Time 71.8885(66.7805) | Bit/dim 3.7618(3.8049) | Xent 1.2211(1.3299) | Loss 10.5423(11.3284) | Error 0.4385(0.4758) Steps 0(0.00) | Grad Norm 3.1206(5.7485) | Total Time 0.00(0.00)\n",
      "Iter 0840 | Time 64.4284(66.7100) | Bit/dim 3.7748(3.8040) | Xent 1.2592(1.3278) | Loss 10.6245(11.3073) | Error 0.4503(0.4750) Steps 0(0.00) | Grad Norm 6.3944(5.7678) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0140 | Time 25.8444, Epoch Time 433.4882(441.7093), Bit/dim 3.7886(best: 3.7766), Xent 1.2127, Loss 4.3950, Error 0.4357(best: 0.4346)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0841 | Time 65.9520(66.6872) | Bit/dim 3.7892(3.8036) | Xent 1.2480(1.3254) | Loss 15.2281(11.4249) | Error 0.4416(0.4740) Steps 0(0.00) | Grad Norm 6.9001(5.8018) | Total Time 0.00(0.00)\n",
      "Iter 0842 | Time 68.1053(66.7298) | Bit/dim 3.7665(3.8025) | Xent 1.2754(1.3239) | Loss 10.6597(11.4020) | Error 0.4575(0.4735) Steps 0(0.00) | Grad Norm 7.4346(5.8508) | Total Time 0.00(0.00)\n",
      "Iter 0843 | Time 65.3278(66.6877) | Bit/dim 3.7746(3.8016) | Xent 1.3446(1.3245) | Loss 10.3121(11.3693) | Error 0.4756(0.4736) Steps 0(0.00) | Grad Norm 8.4321(5.9282) | Total Time 0.00(0.00)\n",
      "Iter 0844 | Time 61.2496(66.5246) | Bit/dim 3.7712(3.8007) | Xent 1.2832(1.3233) | Loss 10.4428(11.3415) | Error 0.4676(0.4734) Steps 0(0.00) | Grad Norm 7.2246(5.9671) | Total Time 0.00(0.00)\n",
      "Iter 0845 | Time 67.2021(66.5449) | Bit/dim 3.7881(3.8003) | Xent 1.3678(1.3246) | Loss 10.7865(11.3248) | Error 0.4884(0.4739) Steps 0(0.00) | Grad Norm 6.1405(5.9723) | Total Time 0.00(0.00)\n",
      "Iter 0846 | Time 64.6704(66.4887) | Bit/dim 3.8005(3.8003) | Xent 1.2503(1.3224) | Loss 10.5215(11.3007) | Error 0.4516(0.4732) Steps 0(0.00) | Grad Norm 4.8429(5.9384) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0141 | Time 25.3992, Epoch Time 433.9089(441.4752), Bit/dim 3.7794(best: 3.7766), Xent 1.3116, Loss 4.4352, Error 0.4790(best: 0.4346)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0847 | Time 62.3345(66.3640) | Bit/dim 3.7826(3.7998) | Xent 1.3606(1.3235) | Loss 14.8972(11.4086) | Error 0.4954(0.4739) Steps 0(0.00) | Grad Norm 8.0881(6.0029) | Total Time 0.00(0.00)\n",
      "Iter 0848 | Time 68.9934(66.4429) | Bit/dim 3.7784(3.7992) | Xent 1.3112(1.3231) | Loss 10.7950(11.3902) | Error 0.4641(0.4736) Steps 0(0.00) | Grad Norm 8.8541(6.0885) | Total Time 0.00(0.00)\n",
      "Iter 0849 | Time 63.5961(66.3575) | Bit/dim 3.7816(3.7986) | Xent 1.2956(1.3223) | Loss 10.7348(11.3706) | Error 0.4640(0.4733) Steps 0(0.00) | Grad Norm 6.3448(6.0962) | Total Time 0.00(0.00)\n",
      "Iter 0850 | Time 68.6401(66.4260) | Bit/dim 3.7782(3.7980) | Xent 1.2540(1.3203) | Loss 10.2850(11.3380) | Error 0.4465(0.4725) Steps 0(0.00) | Grad Norm 3.8866(6.0299) | Total Time 0.00(0.00)\n",
      "Iter 0851 | Time 66.1873(66.4188) | Bit/dim 3.7737(3.7973) | Xent 1.2775(1.3190) | Loss 10.6652(11.3178) | Error 0.4551(0.4720) Steps 0(0.00) | Grad Norm 4.8689(5.9950) | Total Time 0.00(0.00)\n",
      "Iter 0852 | Time 64.9583(66.3750) | Bit/dim 3.7729(3.7966) | Xent 1.2652(1.3174) | Loss 10.6012(11.2963) | Error 0.4500(0.4713) Steps 0(0.00) | Grad Norm 5.1216(5.9688) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0142 | Time 25.5943, Epoch Time 436.2883(441.3196), Bit/dim 3.7727(best: 3.7766), Xent 1.1961, Loss 4.3707, Error 0.4257(best: 0.4346)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0853 | Time 70.3818(66.4952) | Bit/dim 3.7674(3.7957) | Xent 1.2301(1.3147) | Loss 14.8687(11.4035) | Error 0.4401(0.4704) Steps 0(0.00) | Grad Norm 3.0543(5.8814) | Total Time 0.00(0.00)\n",
      "Iter 0854 | Time 61.3900(66.3421) | Bit/dim 3.7820(3.7953) | Xent 1.2220(1.3120) | Loss 10.4905(11.3761) | Error 0.4364(0.4693) Steps 0(0.00) | Grad Norm 4.2700(5.8331) | Total Time 0.00(0.00)\n",
      "Iter 0855 | Time 63.9251(66.2696) | Bit/dim 3.7720(3.7946) | Xent 1.2465(1.3100) | Loss 10.4977(11.3497) | Error 0.4367(0.4684) Steps 0(0.00) | Grad Norm 4.2051(5.7842) | Total Time 0.00(0.00)\n",
      "Iter 0856 | Time 63.3236(66.1812) | Bit/dim 3.7680(3.7938) | Xent 1.2226(1.3074) | Loss 10.4804(11.3237) | Error 0.4404(0.4675) Steps 0(0.00) | Grad Norm 2.9248(5.6984) | Total Time 0.00(0.00)\n",
      "Iter 0857 | Time 69.7235(66.2875) | Bit/dim 3.7792(3.7933) | Xent 1.2234(1.3049) | Loss 10.4831(11.2984) | Error 0.4306(0.4664) Steps 0(0.00) | Grad Norm 4.0484(5.6489) | Total Time 0.00(0.00)\n",
      "Iter 0858 | Time 68.8195(66.3634) | Bit/dim 3.7690(3.7926) | Xent 1.2020(1.3018) | Loss 10.5497(11.2760) | Error 0.4324(0.4654) Steps 0(0.00) | Grad Norm 3.3810(5.5809) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0143 | Time 26.6482, Epoch Time 440.8978(441.3070), Bit/dim 3.7554(best: 3.7727), Xent 1.1789, Loss 4.3448, Error 0.4206(best: 0.4257)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0859 | Time 76.9809(66.6819) | Bit/dim 3.7556(3.7915) | Xent 1.2243(1.2994) | Loss 14.5451(11.3741) | Error 0.4401(0.4646) Steps 0(0.00) | Grad Norm 2.9679(5.5025) | Total Time 0.00(0.00)\n",
      "Iter 0860 | Time 62.2082(66.5477) | Bit/dim 3.7616(3.7906) | Xent 1.2143(1.2969) | Loss 10.3274(11.3427) | Error 0.4359(0.4638) Steps 0(0.00) | Grad Norm 5.3119(5.4968) | Total Time 0.00(0.00)\n",
      "Iter 0861 | Time 69.2550(66.6289) | Bit/dim 3.7844(3.7904) | Xent 1.1941(1.2938) | Loss 10.6353(11.3214) | Error 0.4265(0.4627) Steps 0(0.00) | Grad Norm 5.4751(5.4961) | Total Time 0.00(0.00)\n",
      "Iter 0862 | Time 72.9220(66.8177) | Bit/dim 3.7781(3.7900) | Xent 1.2097(1.2913) | Loss 10.3254(11.2916) | Error 0.4357(0.4618) Steps 0(0.00) | Grad Norm 3.8172(5.4458) | Total Time 0.00(0.00)\n",
      "Iter 0863 | Time 80.4638(67.2271) | Bit/dim 3.7491(3.7888) | Xent 1.2287(1.2894) | Loss 10.3950(11.2647) | Error 0.4380(0.4611) Steps 0(0.00) | Grad Norm 4.4944(5.4172) | Total Time 0.00(0.00)\n",
      "Iter 0864 | Time 63.3091(67.1096) | Bit/dim 3.7736(3.7884) | Xent 1.2268(1.2875) | Loss 10.3721(11.2379) | Error 0.4385(0.4605) Steps 0(0.00) | Grad Norm 6.3732(5.4459) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0144 | Time 25.8410, Epoch Time 467.4064(442.0900), Bit/dim 3.7688(best: 3.7554), Xent 1.2032, Loss 4.3705, Error 0.4251(best: 0.4206)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0865 | Time 71.8720(67.2524) | Bit/dim 3.7799(3.7881) | Xent 1.2398(1.2861) | Loss 14.6194(11.3393) | Error 0.4444(0.4600) Steps 0(0.00) | Grad Norm 7.8780(5.5189) | Total Time 0.00(0.00)\n",
      "Iter 0866 | Time 70.6652(67.3548) | Bit/dim 3.7602(3.7873) | Xent 1.2349(1.2846) | Loss 10.4385(11.3123) | Error 0.4405(0.4594) Steps 0(0.00) | Grad Norm 10.3250(5.6631) | Total Time 0.00(0.00)\n",
      "Iter 0867 | Time 67.0291(67.3451) | Bit/dim 3.7689(3.7867) | Xent 1.3009(1.2851) | Loss 10.6568(11.2926) | Error 0.4643(0.4595) Steps 0(0.00) | Grad Norm 12.6807(5.8736) | Total Time 0.00(0.00)\n",
      "Iter 0868 | Time 68.3940(67.3765) | Bit/dim 3.7914(3.7869) | Xent 1.3187(1.2861) | Loss 10.6916(11.2746) | Error 0.4674(0.4598) Steps 0(0.00) | Grad Norm 8.9815(5.9668) | Total Time 0.00(0.00)\n",
      "Iter 0869 | Time 59.8125(67.1496) | Bit/dim 3.7893(3.7869) | Xent 1.3671(1.2885) | Loss 10.7983(11.2603) | Error 0.4859(0.4605) Steps 0(0.00) | Grad Norm 8.6045(6.0460) | Total Time 0.00(0.00)\n",
      "Iter 0870 | Time 56.9815(66.8446) | Bit/dim 3.7681(3.7864) | Xent 1.4241(1.2926) | Loss 10.7591(11.2453) | Error 0.5011(0.4618) Steps 0(0.00) | Grad Norm 13.1803(6.2600) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0145 | Time 26.4584, Epoch Time 437.2588(441.9450), Bit/dim 3.8964(best: 3.7554), Xent 1.6058, Loss 4.6993, Error 0.5426(best: 0.4206)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0871 | Time 65.5768(66.8065) | Bit/dim 3.8998(3.7898) | Xent 1.6531(1.3034) | Loss 15.4811(11.3724) | Error 0.5426(0.4642) Steps 0(0.00) | Grad Norm 14.7721(6.5153) | Total Time 0.00(0.00)\n",
      "Iter 0872 | Time 64.8329(66.7473) | Bit/dim 3.8327(3.7911) | Xent 1.5672(1.3113) | Loss 10.7880(11.3548) | Error 0.5499(0.4668) Steps 0(0.00) | Grad Norm 7.8397(6.5551) | Total Time 0.00(0.00)\n",
      "Iter 0873 | Time 68.7405(66.8071) | Bit/dim 3.8892(3.7940) | Xent 1.4284(1.3148) | Loss 10.9708(11.3433) | Error 0.5230(0.4685) Steps 0(0.00) | Grad Norm 7.3661(6.5794) | Total Time 0.00(0.00)\n",
      "Iter 0874 | Time 60.4686(66.6170) | Bit/dim 3.8989(3.7971) | Xent 1.5551(1.3220) | Loss 11.1544(11.3376) | Error 0.5529(0.4710) Steps 0(0.00) | Grad Norm 9.8451(6.6774) | Total Time 0.00(0.00)\n",
      "Iter 0875 | Time 74.1980(66.8444) | Bit/dim 3.8978(3.8002) | Xent 1.6417(1.3316) | Loss 11.3953(11.3394) | Error 0.5870(0.4745) Steps 0(0.00) | Grad Norm 15.6262(6.9458) | Total Time 0.00(0.00)\n",
      "Iter 0876 | Time 71.7900(66.9928) | Bit/dim 3.9146(3.8036) | Xent 1.7586(1.3444) | Loss 11.3668(11.3402) | Error 0.5941(0.4781) Steps 0(0.00) | Grad Norm 25.9162(7.5150) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0146 | Time 27.3629, Epoch Time 449.6435(442.1760), Bit/dim 3.9242(best: 3.7554), Xent 1.5971, Loss 4.7227, Error 0.5613(best: 0.4206)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0877 | Time 63.7772(66.8963) | Bit/dim 3.9277(3.8073) | Xent 1.6503(1.3536) | Loss 15.9398(11.4782) | Error 0.5847(0.4813) Steps 0(0.00) | Grad Norm 10.9197(7.6171) | Total Time 0.00(0.00)\n",
      "Iter 0878 | Time 62.7095(66.7707) | Bit/dim 3.9275(3.8109) | Xent 1.4555(1.3567) | Loss 11.1251(11.4676) | Error 0.5201(0.4824) Steps 0(0.00) | Grad Norm 5.9788(7.5680) | Total Time 0.00(0.00)\n",
      "Iter 0879 | Time 73.3834(66.9691) | Bit/dim 3.9286(3.8145) | Xent 1.4656(1.3599) | Loss 11.4183(11.4661) | Error 0.5281(0.4838) Steps 0(0.00) | Grad Norm 4.5524(7.4775) | Total Time 0.00(0.00)\n",
      "Iter 0880 | Time 68.8583(67.0257) | Bit/dim 3.9420(3.8183) | Xent 1.4570(1.3628) | Loss 10.9396(11.4503) | Error 0.5249(0.4850) Steps 0(0.00) | Grad Norm 4.1072(7.3764) | Total Time 0.00(0.00)\n",
      "Iter 0881 | Time 66.5730(67.0122) | Bit/dim 3.8994(3.8207) | Xent 1.4304(1.3649) | Loss 10.9337(11.4348) | Error 0.5171(0.4860) Steps 0(0.00) | Grad Norm 4.1962(7.2810) | Total Time 0.00(0.00)\n",
      "Iter 0882 | Time 67.0106(67.0121) | Bit/dim 3.8677(3.8221) | Xent 1.3932(1.3657) | Loss 10.9465(11.4202) | Error 0.4939(0.4862) Steps 0(0.00) | Grad Norm 3.1361(7.1566) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0147 | Time 26.9739, Epoch Time 444.9976(442.2606), Bit/dim 3.8912(best: 3.7554), Xent 1.3467, Loss 4.5645, Error 0.4763(best: 0.4206)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0883 | Time 71.5966(67.1496) | Bit/dim 3.8829(3.8240) | Xent 1.3926(1.3665) | Loss 15.3564(11.5383) | Error 0.4960(0.4865) Steps 0(0.00) | Grad Norm 3.4962(7.0468) | Total Time 0.00(0.00)\n",
      "Iter 0884 | Time 66.5365(67.1312) | Bit/dim 3.8826(3.8257) | Xent 1.3760(1.3668) | Loss 11.0322(11.5231) | Error 0.4970(0.4868) Steps 0(0.00) | Grad Norm 3.5896(6.9431) | Total Time 0.00(0.00)\n",
      "Iter 0885 | Time 64.3639(67.0482) | Bit/dim 3.8656(3.8269) | Xent 1.3589(1.3666) | Loss 10.9775(11.5067) | Error 0.4850(0.4868) Steps 0(0.00) | Grad Norm 6.0610(6.9166) | Total Time 0.00(0.00)\n",
      "Iter 0886 | Time 72.9716(67.2259) | Bit/dim 3.8707(3.8282) | Xent 1.3444(1.3659) | Loss 11.0108(11.4918) | Error 0.4806(0.4866) Steps 0(0.00) | Grad Norm 4.3287(6.8390) | Total Time 0.00(0.00)\n",
      "Iter 0887 | Time 76.0535(67.4908) | Bit/dim 3.8582(3.8291) | Xent 1.3387(1.3651) | Loss 11.0002(11.4771) | Error 0.4824(0.4865) Steps 0(0.00) | Grad Norm 3.8659(6.7498) | Total Time 0.00(0.00)\n",
      "Iter 0888 | Time 75.8232(67.7407) | Bit/dim 3.8330(3.8292) | Xent 1.3165(1.3636) | Loss 10.8503(11.4583) | Error 0.4770(0.4862) Steps 0(0.00) | Grad Norm 2.1076(6.6105) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0148 | Time 26.4774, Epoch Time 469.7389(443.0850), Bit/dim 3.8306(best: 3.7554), Xent 1.2783, Loss 4.4697, Error 0.4546(best: 0.4206)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0889 | Time 67.6040(67.7366) | Bit/dim 3.8231(3.8291) | Xent 1.2915(1.3615) | Loss 15.1672(11.5695) | Error 0.4603(0.4854) Steps 0(0.00) | Grad Norm 3.0278(6.5031) | Total Time 0.00(0.00)\n",
      "Iter 0890 | Time 72.0184(67.8651) | Bit/dim 3.8207(3.8288) | Xent 1.3194(1.3602) | Loss 10.6612(11.5423) | Error 0.4786(0.4852) Steps 0(0.00) | Grad Norm 2.7296(6.3899) | Total Time 0.00(0.00)\n",
      "Iter 0891 | Time 69.7830(67.9226) | Bit/dim 3.8138(3.8284) | Xent 1.3043(1.3585) | Loss 10.8751(11.5223) | Error 0.4686(0.4847) Steps 0(0.00) | Grad Norm 2.5204(6.2738) | Total Time 0.00(0.00)\n",
      "Iter 0892 | Time 66.6028(67.8830) | Bit/dim 3.8000(3.8275) | Xent 1.2859(1.3563) | Loss 10.6229(11.4953) | Error 0.4621(0.4840) Steps 0(0.00) | Grad Norm 2.8629(6.1714) | Total Time 0.00(0.00)\n",
      "Iter 0893 | Time 75.6847(68.1171) | Bit/dim 3.7916(3.8264) | Xent 1.2937(1.3545) | Loss 10.5648(11.4674) | Error 0.4643(0.4834) Steps 0(0.00) | Grad Norm 3.0381(6.0774) | Total Time 0.00(0.00)\n",
      "Iter 0894 | Time 65.2391(68.0307) | Bit/dim 3.7946(3.8255) | Xent 1.2904(1.3525) | Loss 10.7769(11.4467) | Error 0.4629(0.4828) Steps 0(0.00) | Grad Norm 3.2236(5.9918) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0149 | Time 26.7498, Epoch Time 459.8409(443.5877), Bit/dim 3.7916(best: 3.7554), Xent 1.2390, Loss 4.4111, Error 0.4376(best: 0.4206)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0895 | Time 71.4209(68.1324) | Bit/dim 3.7916(3.8245) | Xent 1.2772(1.3503) | Loss 15.2290(11.5601) | Error 0.4527(0.4819) Steps 0(0.00) | Grad Norm 3.1052(5.9052) | Total Time 0.00(0.00)\n",
      "Iter 0896 | Time 72.4270(68.2613) | Bit/dim 3.7862(3.8233) | Xent 1.2604(1.3476) | Loss 10.8236(11.5380) | Error 0.4540(0.4811) Steps 0(0.00) | Grad Norm 3.5325(5.8340) | Total Time 0.00(0.00)\n",
      "Iter 0897 | Time 69.1650(68.2884) | Bit/dim 3.7805(3.8220) | Xent 1.2413(1.3444) | Loss 10.5696(11.5090) | Error 0.4403(0.4799) Steps 0(0.00) | Grad Norm 1.8670(5.7150) | Total Time 0.00(0.00)\n",
      "Iter 0898 | Time 70.8201(68.3643) | Bit/dim 3.7768(3.8207) | Xent 1.2643(1.3420) | Loss 10.8410(11.4889) | Error 0.4587(0.4792) Steps 0(0.00) | Grad Norm 4.7849(5.6871) | Total Time 0.00(0.00)\n",
      "Iter 0899 | Time 68.4599(68.3672) | Bit/dim 3.7765(3.8193) | Xent 1.2723(1.3399) | Loss 10.4475(11.4577) | Error 0.4486(0.4783) Steps 0(0.00) | Grad Norm 7.3338(5.7365) | Total Time 0.00(0.00)\n",
      "Iter 0900 | Time 71.4185(68.4587) | Bit/dim 3.7963(3.8187) | Xent 1.3486(1.3402) | Loss 10.6588(11.4337) | Error 0.4798(0.4783) Steps 0(0.00) | Grad Norm 9.8050(5.8586) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0150 | Time 27.6001, Epoch Time 467.2237(444.2967), Bit/dim 3.7999(best: 3.7554), Xent 1.2541, Loss 4.4270, Error 0.4426(best: 0.4206)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0901 | Time 65.5230(68.3707) | Bit/dim 3.7965(3.8180) | Xent 1.2949(1.3388) | Loss 14.7434(11.5330) | Error 0.4639(0.4779) Steps 0(0.00) | Grad Norm 7.3460(5.9032) | Total Time 0.00(0.00)\n",
      "Iter 0902 | Time 76.2602(68.6074) | Bit/dim 3.7746(3.8167) | Xent 1.2615(1.3365) | Loss 10.6408(11.5063) | Error 0.4526(0.4772) Steps 0(0.00) | Grad Norm 2.5541(5.8027) | Total Time 0.00(0.00)\n",
      "Iter 0903 | Time 67.3469(68.5695) | Bit/dim 3.7887(3.8158) | Xent 1.2974(1.3353) | Loss 10.6607(11.4809) | Error 0.4621(0.4767) Steps 0(0.00) | Grad Norm 7.3760(5.8499) | Total Time 0.00(0.00)\n",
      "Iter 0904 | Time 77.2765(68.8308) | Bit/dim 3.7660(3.8144) | Xent 1.2804(1.3337) | Loss 10.5850(11.4540) | Error 0.4571(0.4761) Steps 0(0.00) | Grad Norm 5.7904(5.8481) | Total Time 0.00(0.00)\n",
      "Iter 0905 | Time 64.8794(68.7122) | Bit/dim 3.7864(3.8135) | Xent 1.2705(1.3318) | Loss 10.5014(11.4254) | Error 0.4583(0.4756) Steps 0(0.00) | Grad Norm 7.0606(5.8845) | Total Time 0.00(0.00)\n",
      "Iter 0906 | Time 69.2608(68.7287) | Bit/dim 3.7881(3.8128) | Xent 1.3064(1.3310) | Loss 10.4984(11.3976) | Error 0.4708(0.4754) Steps 0(0.00) | Grad Norm 9.5688(5.9950) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0151 | Time 27.4041, Epoch Time 464.0423(444.8891), Bit/dim 3.7752(best: 3.7554), Xent 1.3070, Loss 4.4287, Error 0.4676(best: 0.4206)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0907 | Time 72.8331(68.8518) | Bit/dim 3.7820(3.8118) | Xent 1.3464(1.3315) | Loss 14.9845(11.5052) | Error 0.4816(0.4756) Steps 0(0.00) | Grad Norm 11.9181(6.1727) | Total Time 0.00(0.00)\n",
      "Iter 0908 | Time 72.3602(68.9571) | Bit/dim 3.7860(3.8111) | Xent 1.4936(1.3363) | Loss 10.5789(11.4774) | Error 0.5137(0.4768) Steps 0(0.00) | Grad Norm 14.1411(6.4118) | Total Time 0.00(0.00)\n",
      "Iter 0909 | Time 67.8308(68.9233) | Bit/dim 3.8112(3.8111) | Xent 1.3752(1.3375) | Loss 10.6928(11.4539) | Error 0.4824(0.4769) Steps 0(0.00) | Grad Norm 12.0197(6.5800) | Total Time 0.00(0.00)\n",
      "Iter 0910 | Time 65.3058(68.8147) | Bit/dim 3.7842(3.8103) | Xent 1.3725(1.3386) | Loss 10.6345(11.4293) | Error 0.4906(0.4773) Steps 0(0.00) | Grad Norm 6.5308(6.5785) | Total Time 0.00(0.00)\n",
      "Iter 0911 | Time 72.0484(68.9118) | Bit/dim 3.7898(3.8096) | Xent 1.2843(1.3369) | Loss 10.6950(11.4073) | Error 0.4566(0.4767) Steps 0(0.00) | Grad Norm 5.9386(6.5594) | Total Time 0.00(0.00)\n",
      "Iter 0912 | Time 66.2117(68.8308) | Bit/dim 3.7900(3.8091) | Xent 1.3004(1.3358) | Loss 10.6341(11.3841) | Error 0.4641(0.4763) Steps 0(0.00) | Grad Norm 4.9492(6.5110) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0152 | Time 26.0449, Epoch Time 458.5919(445.3002), Bit/dim 3.7985(best: 3.7554), Xent 1.2234, Loss 4.4102, Error 0.4413(best: 0.4206)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0913 | Time 64.2970(68.6947) | Bit/dim 3.8065(3.8090) | Xent 1.2804(1.3342) | Loss 14.9625(11.4914) | Error 0.4541(0.4757) Steps 0(0.00) | Grad Norm 3.2223(6.4124) | Total Time 0.00(0.00)\n",
      "Iter 0914 | Time 67.9114(68.6712) | Bit/dim 3.7912(3.8084) | Xent 1.2441(1.3315) | Loss 10.4473(11.4601) | Error 0.4435(0.4747) Steps 0(0.00) | Grad Norm 3.4926(6.3248) | Total Time 0.00(0.00)\n",
      "Iter 0915 | Time 75.8566(68.8868) | Bit/dim 3.7926(3.8080) | Xent 1.2319(1.3285) | Loss 10.5224(11.4320) | Error 0.4406(0.4737) Steps 0(0.00) | Grad Norm 3.2467(6.2324) | Total Time 0.00(0.00)\n",
      "Iter 0916 | Time 67.3510(68.8407) | Bit/dim 3.7921(3.8075) | Xent 1.2556(1.3263) | Loss 10.5963(11.4069) | Error 0.4454(0.4728) Steps 0(0.00) | Grad Norm 3.5041(6.1506) | Total Time 0.00(0.00)\n",
      "Iter 0917 | Time 73.1656(68.9705) | Bit/dim 3.7673(3.8063) | Xent 1.2322(1.3235) | Loss 10.4887(11.3794) | Error 0.4426(0.4719) Steps 0(0.00) | Grad Norm 3.0270(6.0569) | Total Time 0.00(0.00)\n",
      "Iter 0918 | Time 68.7813(68.9648) | Bit/dim 3.7756(3.8054) | Xent 1.2289(1.3206) | Loss 10.4138(11.3504) | Error 0.4365(0.4709) Steps 0(0.00) | Grad Norm 3.3533(5.9758) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0153 | Time 26.8262, Epoch Time 460.3289(445.7511), Bit/dim 3.7718(best: 3.7554), Xent 1.1895, Loss 4.3666, Error 0.4277(best: 0.4206)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0919 | Time 65.6461(68.8652) | Bit/dim 3.7753(3.8045) | Xent 1.2370(1.3181) | Loss 13.6727(11.4201) | Error 0.4443(0.4701) Steps 0(0.00) | Grad Norm 3.1720(5.8917) | Total Time 0.00(0.00)\n",
      "Iter 0920 | Time 65.5236(68.7650) | Bit/dim 3.7730(3.8035) | Xent 1.2095(1.3149) | Loss 10.4096(11.3898) | Error 0.4287(0.4688) Steps 0(0.00) | Grad Norm 3.3192(5.8145) | Total Time 0.00(0.00)\n",
      "Iter 0921 | Time 76.4230(68.9947) | Bit/dim 3.7569(3.8021) | Xent 1.2304(1.3123) | Loss 10.5702(11.3652) | Error 0.4417(0.4680) Steps 0(0.00) | Grad Norm 2.6419(5.7193) | Total Time 0.00(0.00)\n",
      "Iter 0922 | Time 69.4006(69.0069) | Bit/dim 3.7609(3.8009) | Xent 1.1993(1.3089) | Loss 10.5885(11.3419) | Error 0.4235(0.4667) Steps 0(0.00) | Grad Norm 2.7123(5.6291) | Total Time 0.00(0.00)\n",
      "Iter 0923 | Time 74.7862(69.1803) | Bit/dim 3.7607(3.7997) | Xent 1.2027(1.3057) | Loss 10.5154(11.3171) | Error 0.4363(0.4658) Steps 0(0.00) | Grad Norm 4.2514(5.5878) | Total Time 0.00(0.00)\n",
      "Iter 0924 | Time 71.2445(69.2422) | Bit/dim 3.7491(3.7982) | Xent 1.2025(1.3026) | Loss 10.5045(11.2927) | Error 0.4274(0.4646) Steps 0(0.00) | Grad Norm 4.6033(5.5582) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0154 | Time 26.8475, Epoch Time 465.8673(446.3545), Bit/dim 3.7652(best: 3.7554), Xent 1.1684, Loss 4.3494, Error 0.4200(best: 0.4206)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0925 | Time 67.1372(69.1791) | Bit/dim 3.7601(3.7970) | Xent 1.1903(1.2993) | Loss 14.5332(11.3899) | Error 0.4231(0.4634) Steps 0(0.00) | Grad Norm 4.6777(5.5318) | Total Time 0.00(0.00)\n",
      "Iter 0926 | Time 77.7988(69.4377) | Bit/dim 3.7479(3.7955) | Xent 1.2083(1.2965) | Loss 10.6978(11.3692) | Error 0.4337(0.4625) Steps 0(0.00) | Grad Norm 4.0703(5.4880) | Total Time 0.00(0.00)\n",
      "Iter 0927 | Time 69.9585(69.4533) | Bit/dim 3.7368(3.7938) | Xent 1.1999(1.2936) | Loss 10.4920(11.3428) | Error 0.4227(0.4613) Steps 0(0.00) | Grad Norm 4.3763(5.4546) | Total Time 0.00(0.00)\n",
      "Iter 0928 | Time 66.7918(69.3734) | Bit/dim 3.7503(3.7925) | Xent 1.2569(1.2925) | Loss 10.5850(11.3201) | Error 0.4520(0.4610) Steps 0(0.00) | Grad Norm 7.5867(5.5186) | Total Time 0.00(0.00)\n",
      "Iter 0929 | Time 72.9592(69.4810) | Bit/dim 3.7810(3.7921) | Xent 1.3239(1.2935) | Loss 10.5130(11.2959) | Error 0.4762(0.4615) Steps 0(0.00) | Grad Norm 9.8431(5.6483) | Total Time 0.00(0.00)\n",
      "Iter 0930 | Time 73.9879(69.6162) | Bit/dim 3.7537(3.7910) | Xent 1.2041(1.2908) | Loss 10.4284(11.2699) | Error 0.4355(0.4607) Steps 0(0.00) | Grad Norm 5.4434(5.6422) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0155 | Time 26.4226, Epoch Time 471.2249(447.1007), Bit/dim 3.7551(best: 3.7554), Xent 1.2010, Loss 4.3556, Error 0.4344(best: 0.4200)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0931 | Time 68.7155(69.5892) | Bit/dim 3.7661(3.7902) | Xent 1.2229(1.2888) | Loss 15.0070(11.3820) | Error 0.4389(0.4600) Steps 0(0.00) | Grad Norm 6.0600(5.6547) | Total Time 0.00(0.00)\n",
      "Iter 0932 | Time 74.1196(69.7251) | Bit/dim 3.7524(3.7891) | Xent 1.2454(1.2875) | Loss 10.6493(11.3600) | Error 0.4454(0.4596) Steps 0(0.00) | Grad Norm 8.3651(5.7360) | Total Time 0.00(0.00)\n",
      "Iter 0933 | Time 71.0942(69.7662) | Bit/dim 3.7495(3.7879) | Xent 1.2747(1.2871) | Loss 10.3981(11.3311) | Error 0.4527(0.4594) Steps 0(0.00) | Grad Norm 6.7168(5.7654) | Total Time 0.00(0.00)\n",
      "Iter 0934 | Time 67.8912(69.7099) | Bit/dim 3.7616(3.7871) | Xent 1.2619(1.2863) | Loss 10.5121(11.3066) | Error 0.4585(0.4594) Steps 0(0.00) | Grad Norm 4.4128(5.7249) | Total Time 0.00(0.00)\n",
      "Iter 0935 | Time 166.2444(72.6060) | Bit/dim 3.7529(3.7861) | Xent 1.2183(1.2843) | Loss 10.6247(11.2861) | Error 0.4405(0.4588) Steps 0(0.00) | Grad Norm 5.0380(5.7043) | Total Time 0.00(0.00)\n",
      "Iter 0936 | Time 72.3839(72.5993) | Bit/dim 3.7625(3.7854) | Xent 1.1865(1.2814) | Loss 10.4596(11.2613) | Error 0.4310(0.4580) Steps 0(0.00) | Grad Norm 3.3092(5.6324) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0156 | Time 26.5129, Epoch Time 563.3828(450.5891), Bit/dim 3.7511(best: 3.7551), Xent 1.1752, Loss 4.3387, Error 0.4231(best: 0.4200)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0937 | Time 65.2408(72.3785) | Bit/dim 3.7553(3.7845) | Xent 1.2350(1.2800) | Loss 14.4721(11.3576) | Error 0.4371(0.4573) Steps 0(0.00) | Grad Norm 3.7905(5.5772) | Total Time 0.00(0.00)\n",
      "Iter 0938 | Time 62.8264(72.0920) | Bit/dim 3.7620(3.7838) | Xent 1.1652(1.2765) | Loss 10.1010(11.3199) | Error 0.4223(0.4563) Steps 0(0.00) | Grad Norm 3.2176(5.5064) | Total Time 0.00(0.00)\n",
      "Iter 0939 | Time 73.1666(72.1242) | Bit/dim 3.7504(3.7828) | Xent 1.1963(1.2741) | Loss 10.4754(11.2946) | Error 0.4317(0.4556) Steps 0(0.00) | Grad Norm 3.4050(5.4433) | Total Time 0.00(0.00)\n",
      "Iter 0940 | Time 66.5425(71.9568) | Bit/dim 3.7390(3.7815) | Xent 1.1804(1.2713) | Loss 10.4148(11.2682) | Error 0.4195(0.4545) Steps 0(0.00) | Grad Norm 3.5453(5.3864) | Total Time 0.00(0.00)\n",
      "Iter 0941 | Time 76.1090(72.0813) | Bit/dim 3.7381(3.7802) | Xent 1.2090(1.2694) | Loss 10.5801(11.2476) | Error 0.4324(0.4538) Steps 0(0.00) | Grad Norm 3.8409(5.3400) | Total Time 0.00(0.00)\n",
      "Iter 0942 | Time 72.5058(72.0941) | Bit/dim 3.7273(3.7786) | Xent 1.1809(1.2668) | Loss 10.0193(11.2107) | Error 0.4269(0.4530) Steps 0(0.00) | Grad Norm 3.2066(5.2760) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0157 | Time 26.3810, Epoch Time 458.3985(450.8234), Bit/dim 3.7460(best: 3.7511), Xent 1.1454, Loss 4.3187, Error 0.4083(best: 0.4200)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0943 | Time 72.4244(72.1040) | Bit/dim 3.7360(3.7773) | Xent 1.1866(1.2644) | Loss 15.1568(11.3291) | Error 0.4240(0.4521) Steps 0(0.00) | Grad Norm 4.1127(5.2411) | Total Time 0.00(0.00)\n",
      "Iter 0944 | Time 71.8741(72.0971) | Bit/dim 3.7430(3.7763) | Xent 1.1849(1.2620) | Loss 10.2619(11.2971) | Error 0.4345(0.4516) Steps 0(0.00) | Grad Norm 6.6802(5.2843) | Total Time 0.00(0.00)\n",
      "Iter 0945 | Time 76.4178(72.2267) | Bit/dim 3.7427(3.7753) | Xent 1.2554(1.2618) | Loss 10.6076(11.2764) | Error 0.4510(0.4516) Steps 0(0.00) | Grad Norm 9.8359(5.4208) | Total Time 0.00(0.00)\n",
      "Iter 0946 | Time 72.7379(72.2420) | Bit/dim 3.7805(3.7754) | Xent 1.4927(1.2687) | Loss 10.8436(11.2634) | Error 0.5221(0.4537) Steps 0(0.00) | Grad Norm 14.7881(5.7019) | Total Time 0.00(0.00)\n",
      "Iter 0947 | Time 74.1813(72.3002) | Bit/dim 3.8066(3.7764) | Xent 1.6849(1.2812) | Loss 10.9924(11.2553) | Error 0.5664(0.4571) Steps 0(0.00) | Grad Norm 18.8556(6.0965) | Total Time 0.00(0.00)\n",
      "Iter 0948 | Time 71.0538(72.2628) | Bit/dim 3.9056(3.7803) | Xent 1.8335(1.2978) | Loss 11.2840(11.2561) | Error 0.5686(0.4604) Steps 0(0.00) | Grad Norm 14.2466(6.3410) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0158 | Time 26.6136, Epoch Time 481.3691(451.7398), Bit/dim 4.0330(best: 3.7460), Xent 2.7460, Loss 5.4061, Error 0.7449(best: 0.4083)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0949 | Time 62.7458(71.9773) | Bit/dim 4.0212(3.7875) | Xent 2.8043(1.3430) | Loss 17.5895(11.4461) | Error 0.7506(0.4691) Steps 0(0.00) | Grad Norm 20.4856(6.7653) | Total Time 0.00(0.00)\n",
      "Iter 0950 | Time 80.1106(72.2213) | Bit/dim 4.1075(3.7971) | Xent 1.8254(1.3574) | Loss 11.8595(11.4585) | Error 0.6330(0.4740) Steps 0(0.00) | Grad Norm 9.1664(6.8373) | Total Time 0.00(0.00)\n",
      "Iter 0951 | Time 71.6181(72.2032) | Bit/dim 4.1551(3.8078) | Xent 1.6623(1.3666) | Loss 11.8670(11.4708) | Error 0.6093(0.4781) Steps 0(0.00) | Grad Norm 9.7170(6.9237) | Total Time 0.00(0.00)\n",
      "Iter 0952 | Time 73.6876(72.2477) | Bit/dim 4.1456(3.8180) | Xent 1.7326(1.3776) | Loss 12.1508(11.4912) | Error 0.6215(0.4824) Steps 0(0.00) | Grad Norm 12.8029(7.1001) | Total Time 0.00(0.00)\n",
      "Iter 0953 | Time 85.4130(72.6427) | Bit/dim 4.1143(3.8269) | Xent 2.1207(1.3999) | Loss 12.2785(11.5148) | Error 0.6990(0.4889) Steps 0(0.00) | Grad Norm 28.7370(7.7492) | Total Time 0.00(0.00)\n",
      "Iter 0954 | Time 76.0873(72.7460) | Bit/dim 4.1059(3.8352) | Xent 1.8622(1.4137) | Loss 12.2218(11.5360) | Error 0.6669(0.4942) Steps 0(0.00) | Grad Norm 12.8310(7.9017) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0159 | Time 31.5131, Epoch Time 497.5097(453.1129), Bit/dim 4.1752(best: 3.7460), Xent 1.6334, Loss 4.9920, Error 0.5817(best: 0.4083)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_rl.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 8000 --test_batch_size 5000 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_rl_stdlearnscale_6_run1 --seed 1 --conditional True --controlled_tol True --train_mode semisup --lr 0.01 --warmup_iters 1000 --atol 1e-4  --rtol 1e-4 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5 --scale_fac 1.0 --gate cnn2 --scale_std 6.0 --max_grad_norm 20.0\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
