{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='4,5,6,7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_2cond_nosep.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import torch.utils.data as data\n",
      "from torch.utils.data import Dataset\n",
      "\n",
      "from PIL import Image\n",
      "import os.path\n",
      "import errno\n",
      "import codecs\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"colormnist\", \"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "parser.add_argument(\"--y_class\", type=int, default=10)\n",
      "parser.add_argument(\"--y_color\", type=int, default=10)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "if args.controlled_tol:\n",
      "    import lib.odenvp_conditional_tol as odenvp\n",
      "else:\n",
      "    import lib.odenvp_conditional_2cond as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "class ColorMNIST(data.Dataset):\n",
      "    \"\"\"\n",
      "    ColorMNIST\n",
      "    \"\"\"\n",
      "    urls = [\n",
      "        'http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz',\n",
      "    ]\n",
      "    raw_folder = 'raw'\n",
      "    processed_folder = 'processed'\n",
      "    training_file = 'training.pt'\n",
      "    test_file = 'test.pt'\n",
      "\n",
      "    def __init__(self, root, train=True, transform=None, target_transform=None, download=False):\n",
      "        self.root = os.path.expanduser(root)\n",
      "        self.transform = transform\n",
      "        self.target_transform = target_transform\n",
      "        self.train = train  # training set or test set\n",
      "\n",
      "        if download:\n",
      "            self.download()\n",
      "\n",
      "        if not self._check_exists():\n",
      "            raise RuntimeError('Dataset not found.' +\n",
      "                               ' You can use download=True to download it')\n",
      "\n",
      "        if self.train:\n",
      "            self.train_data, self.train_labels = torch.load(\n",
      "                os.path.join(self.root, self.processed_folder, self.training_file))\n",
      "            \n",
      "            self.train_data = np.tile(self.train_data[:, :, :, np.newaxis], 3)\n",
      "        else:\n",
      "            self.test_data, self.test_labels = torch.load(\n",
      "                os.path.join(self.root, self.processed_folder, self.test_file))\n",
      "            \n",
      "            self.test_data = np.tile(self.test_data[:, :, :, np.newaxis], 3)\n",
      "        \n",
      "        self.pallette = [[31, 119, 180],\n",
      "                         [255, 127, 14],\n",
      "                         [44, 160, 44],\n",
      "                         [214, 39, 40],\n",
      "                         [148, 103, 189],\n",
      "                         [140, 86, 75],\n",
      "                         [227, 119, 194],\n",
      "                         [127, 127, 127],\n",
      "                         [188, 189, 34],\n",
      "                         [23, 190, 207]]\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        \"\"\"\n",
      "        Args:\n",
      "            index (int): Index\n",
      "\n",
      "        Returns:\n",
      "            tuple: (image, target) where target is index of the target class.\n",
      "        \"\"\"\n",
      "        if self.train:\n",
      "            img, target = self.train_data[index].copy(), self.train_labels[index]\n",
      "        else:\n",
      "            img, target = self.test_data[index].copy(), self.test_labels[index]\n",
      "        \n",
      "        # doing this so that it is consistent with all other datasets\n",
      "        # to return a PIL Image\n",
      "        y_color_digit = np.random.randint(0, args.y_color)\n",
      "        c_digit = self.pallette[y_color_digit]\n",
      "        \n",
      "        img[:, :, 0] = img[:, :, 0] / 255 * c_digit[0]\n",
      "        img[:, :, 1] = img[:, :, 1] / 255 * c_digit[1]\n",
      "        img[:, :, 2] = img[:, :, 2] / 255 * c_digit[2]\n",
      "        \n",
      "        img = Image.fromarray(img)\n",
      "\n",
      "        if self.transform is not None:\n",
      "            img = self.transform(img)\n",
      "\n",
      "        if self.target_transform is not None:\n",
      "            target = self.target_transform(target)\n",
      "\n",
      "        return img, [target,torch.from_numpy(np.array(y_color_digit))]\n",
      "\n",
      "    def __len__(self):\n",
      "        if self.train:\n",
      "            return len(self.train_data)\n",
      "        else:\n",
      "            return len(self.test_data)\n",
      "\n",
      "    def _check_exists(self):\n",
      "        return os.path.exists(os.path.join(self.root, self.processed_folder, self.training_file)) and \\\n",
      "            os.path.exists(os.path.join(self.root, self.processed_folder, self.test_file))\n",
      "\n",
      "    def download(self):\n",
      "        \"\"\"Download the MNIST data if it doesn't exist in processed_folder already.\"\"\"\n",
      "        from six.moves import urllib\n",
      "        import gzip\n",
      "\n",
      "        if self._check_exists():\n",
      "            return\n",
      "\n",
      "        # download files\n",
      "        try:\n",
      "            os.makedirs(os.path.join(self.root, self.raw_folder))\n",
      "            os.makedirs(os.path.join(self.root, self.processed_folder))\n",
      "        except OSError as e:\n",
      "            if e.errno == errno.EEXIST:\n",
      "                pass\n",
      "            else:\n",
      "                raise\n",
      "\n",
      "        for url in self.urls:\n",
      "            print('Downloading ' + url)\n",
      "            data = urllib.request.urlopen(url)\n",
      "            filename = url.rpartition('/')[2]\n",
      "            file_path = os.path.join(self.root, self.raw_folder, filename)\n",
      "            with open(file_path, 'wb') as f:\n",
      "                f.write(data.read())\n",
      "            with open(file_path.replace('.gz', ''), 'wb') as out_f, \\\n",
      "                    gzip.GzipFile(file_path) as zip_f:\n",
      "                out_f.write(zip_f.read())\n",
      "            os.unlink(file_path)\n",
      "\n",
      "        # process and save as torch files\n",
      "        print('Processing...')\n",
      "\n",
      "        training_set = (\n",
      "            read_image_file(os.path.join(self.root, self.raw_folder, 'train-images-idx3-ubyte')),\n",
      "            read_label_file(os.path.join(self.root, self.raw_folder, 'train-labels-idx1-ubyte'))\n",
      "        )\n",
      "        test_set = (\n",
      "            read_image_file(os.path.join(self.root, self.raw_folder, 't10k-images-idx3-ubyte')),\n",
      "            read_label_file(os.path.join(self.root, self.raw_folder, 't10k-labels-idx1-ubyte'))\n",
      "        )\n",
      "        with open(os.path.join(self.root, self.processed_folder, self.training_file), 'wb') as f:\n",
      "            torch.save(training_set, f)\n",
      "        with open(os.path.join(self.root, self.processed_folder, self.test_file), 'wb') as f:\n",
      "            torch.save(test_set, f)\n",
      "\n",
      "        print('Done!')\n",
      "\n",
      "    def __repr__(self):\n",
      "        fmt_str = 'Dataset ' + self.__class__.__name__ + '\\n'\n",
      "        fmt_str += '    Number of datapoints: {}\\n'.format(self.__len__())\n",
      "        tmp = 'train' if self.train is True else 'test'\n",
      "        fmt_str += '    Split: {}\\n'.format(tmp)\n",
      "        fmt_str += '    Root Location: {}\\n'.format(self.root)\n",
      "        tmp = '    Transforms (if any): '\n",
      "        fmt_str += '{0}{1}\\n'.format(tmp, self.transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
      "        tmp = '    Target Transforms (if any): '\n",
      "        fmt_str += '{0}{1}'.format(tmp, self.target_transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
      "        return fmt_str\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"colormnist\":\n",
      "        im_dim = 3\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = ColorMNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = ColorMNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, y_color, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "    y_onehot_color = thops.onehot(y_color, num_classes=model.module.y_color).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "    mean_color, logs_color = model.module._prior_color(y_onehot_color)\n",
      "    \n",
      "    mean_sup = mean + mean_color\n",
      "    logs_sup = 0.5 * torch.log(torch.exp(2.*logs) + torch.exp(2.*logs_color))\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean_sup, logs_sup, z[:, 0:dim_sup]).view(-1,1)\n",
      "\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "    \n",
      "    y_logits_color = model.module.project_color(zsup)\n",
      "    loss_xent_color = model.module.loss_class(y_logits_color, y_color.to(x.get_device()))\n",
      "    y_color_predicted = np.argmax(y_logits_color.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, loss_xent_color, y_predicted, y_color_predicted\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,\n",
      "            y_class = args.y_class,\n",
      "            y_color = args.y_color)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    xent_color_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    error_color_meter = utils.RunningAverageMeter(0.97)\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        xent_color_meter.set(checkpt['xent_train_color'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        error_color_meter.set(checkpt['error_train_color'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        \n",
      "        fixed_y_color = torch.from_numpy(np.arange(model.module.y_color)).repeat(model.module.y_color).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot_color = thops.onehot(fixed_y_color, num_classes=model.module.y_color)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            mean_color, logs_color = model.module._prior_color(fixed_y_onehot_color)\n",
      "            mean_sup = mean + mean_color\n",
      "            logs_sup = 0.5 * torch.log(torch.exp(2.*logs) + torch.exp(2.*logs_color))\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean_sup, logs_sup)\n",
      "            dim_unsup = np.prod(data_shape) - np.prod(fixed_z_sup.shape[1:])\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    best_error_score_color = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y_all) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            \n",
      "            y = y_all[0]\n",
      "            y_color = y_all[1]\n",
      "            \n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, loss_xent_color, y_predicted, y_color_predicted = compute_bits_per_dim_conditional(x, y, y_color, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * 0.5 * (loss_xent + loss_xent_color)\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  0.5 * (loss_xent + loss_xent_color)\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy()) \n",
      "                error_score_color = 1. - np.mean(y_color_predicted.astype(int) == y_color.numpy())\n",
      "                \n",
      "            else:\n",
      "                loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, loss_xent_color, error_score, error_score_color = loss, 0., 0., 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            loss.backward()\n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "                xent_color_meter.update(loss_xent_color.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "                xent_color_meter.update(loss_xent_color)\n",
      "            error_meter.update(error_score)\n",
      "            error_color_meter.update(error_score_color)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('xent_color', {'train_iter': xent_color_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('error_color', {'train_iter': error_color_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Xent Color {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) | Error Color {:.4f}({:.4f}) |\"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, xent_color_meter.val, xent_color_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, error_color_meter.val, error_color_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent_color', {'train_epoch': xent_color_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('error_color', {'train_epoch': error_color_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses_xent_color = []; losses = []\n",
      "                total_correct = 0\n",
      "                total_correct_color = 0\n",
      "                \n",
      "                for (x, y_all) in test_loader:\n",
      "                    y = y_all[0]\n",
      "                    y_color = y_all[1]\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, loss_xent_color, y_predicted, y_color_predicted = compute_bits_per_dim_conditional(x, y, y_color, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * 0.5 * (loss_xent + loss_xent_color)\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  0.5 * (loss_xent + loss_xent_color)\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                        total_correct_color += np.sum(y_color_predicted.astype(int) == y_color.numpy())\n",
      "                    else:\n",
      "                        loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent, loss_xent_color = loss, 0., 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                        losses_xent_color.append(loss_xent_color.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                        losses_xent_color.append(loss_xent_color)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss_xent_color = np.mean(losses_xent_color); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                error_score_color =  1. - total_correct_color / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('xent_color', {'validation': loss_xent_color}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                writer.add_scalars('error_color', {'validation': error_score_color}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Xent Color {:.4f}. Loss {:.4f}, Error {:.4f}(best: {:.4f}), Error Color {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss_xent_color, loss, error_score, best_error_score, error_score_color, best_error_score_color)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"error_color\": error_score_color,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"xent_color\": loss_xent_color,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"best_error_score_color\": best_error_score_color,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"error_train_color\": error_color_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"xent_train_color\": xent_color_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"error_color\": error_score_color,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"xent_color\": loss_xent_color,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"best_error_score_color\": best_error_score_color,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"error_train_color\": error_color_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"xent_train_color\": xent_color_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"error_color\": error_score_color,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"xent_color\": loss_xent_color,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"best_error_score_color\": best_error_score_color,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"error_train_color\": error_color_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"xent_train_color\": xent_color_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"error_color\": error_score_color,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"xent_color\": loss_xent_color,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"best_error_score_color\": best_error_score_color,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"error_train_color\": error_color_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"xent_train_color\": xent_color_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "                    if error_score_color < best_error_score_color:\n",
      "                        best_error_score_color = error_score_color\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"error_color\": error_score_color,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"xent_color\": loss_xent_color,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"best_error_score_color\": best_error_score_color,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"error_train_color\": error_color_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"xent_train_color\": xent_color_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_color_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, condition_ratio=0.5, conditional=True, controlled_tol=False, conv=True, data='colormnist', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume=None, rtol=1e-05, save='../experiments_published/cnf_conditional_disentangle_colormnist_bs900_sratio_1_2th_drop_0_5_2cond_linear_nosep_run1', seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5, y_class=10, y_color=10)\n",
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=2352, bias=True)\n",
      "  (project_ycond_color): LinearZeros(in_features=10, out_features=2352, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1176, out_features=10, bias=True)\n",
      "  (project_color): LinearZeros(in_features=1176, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      "  (dropout_color): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 973670\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc.py:286: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "Iter 0010 | Time 30.7594(52.0812) | Bit/dim 16.2358(17.5955) | Xent 2.2780(2.2997) | Xent Color 2.2995(2.3021) | Loss 17.3802(18.7460) | Error 0.8844(0.8785) | Error Color 0.9067(0.8881) |Steps 410(410.00) | Grad Norm 98.9073(106.4878) | Total Time 10.00(10.00)\n",
      "Iter 0020 | Time 28.6982(46.0369) | Bit/dim 12.6336(16.7188) | Xent 2.2115(2.2852) | Xent Color 2.2812(2.2985) | Loss 13.7568(17.8648) | Error 0.6478(0.8602) | Error Color 0.8611(0.8842) |Steps 410(410.00) | Grad Norm 77.1086(101.4029) | Total Time 10.00(10.00)\n",
      "Iter 0030 | Time 28.5197(41.6487) | Bit/dim 8.8688(15.0592) | Xent 2.1049(2.2508) | Xent Color 2.2498(2.2899) | Loss 9.9575(16.1944) | Error 0.5411(0.7798) | Error Color 0.7589(0.8659) |Steps 410(410.00) | Grad Norm 49.4025(90.8750) | Total Time 10.00(10.00)\n",
      "Iter 0040 | Time 28.7725(38.3627) | Bit/dim 6.6379(13.0542) | Xent 1.9782(2.1950) | Xent Color 2.2168(2.2746) | Loss 7.6866(14.1716) | Error 0.2878(0.6864) | Error Color 0.7511(0.8487) |Steps 410(410.00) | Grad Norm 24.7632(75.9920) | Total Time 10.00(10.00)\n",
      "Iter 0050 | Time 28.6993(35.8984) | Bit/dim 5.4263(11.1661) | Xent 1.8481(2.1197) | Xent Color 2.1943(2.2536) | Loss 6.4369(12.2594) | Error 0.2656(0.5790) | Error Color 0.8167(0.8320) |Steps 410(410.00) | Grad Norm 11.6724(60.4329) | Total Time 10.00(10.00)\n",
      "Iter 0060 | Time 30.5868(34.1584) | Bit/dim 4.9041(9.5713) | Xent 1.7458(2.0332) | Xent Color 2.1678(2.2336) | Loss 5.8825(10.6380) | Error 0.2778(0.4988) | Error Color 0.8878(0.8397) |Steps 410(410.00) | Grad Norm 8.6663(46.8246) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0001 | Time 77.8104, Epoch Time 2057.9114(2057.9114), Bit/dim 4.6238(best: inf), Xent 1.6432, Xent Color 2.1159. Loss 5.5636, Error 0.2156(best: inf), Error Color 0.7042(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0070 | Time 28.9450(32.7864) | Bit/dim 4.4799(8.2820) | Xent 1.6365(1.9398) | Xent Color 2.1033(2.2070) | Loss 5.4148(9.3187) | Error 0.2611(0.4334) | Error Color 0.7378(0.8243) |Steps 410(410.00) | Grad Norm 7.5572(36.7984) | Total Time 10.00(10.00)\n",
      "Iter 0080 | Time 28.6733(31.8865) | Bit/dim 4.0766(7.2210) | Xent 1.5409(1.8417) | Xent Color 2.0701(2.1787) | Loss 4.9794(8.2261) | Error 0.2911(0.3911) | Error Color 0.7933(0.8121) |Steps 410(410.00) | Grad Norm 5.7479(28.7688) | Total Time 10.00(10.00)\n",
      "Iter 0090 | Time 28.8379(31.1891) | Bit/dim 3.7306(6.3486) | Xent 1.4413(1.7458) | Xent Color 2.0286(2.1423) | Loss 4.5981(7.3207) | Error 0.2311(0.3539) | Error Color 0.6822(0.7887) |Steps 410(410.00) | Grad Norm 4.4027(22.5042) | Total Time 10.00(10.00)\n",
      "Iter 0100 | Time 28.4404(30.7663) | Bit/dim 3.4343(5.6122) | Xent 1.3974(1.6587) | Xent Color 1.9484(2.1038) | Loss 4.2707(6.5528) | Error 0.2500(0.3224) | Error Color 0.6156(0.7559) |Steps 410(410.00) | Grad Norm 4.1149(17.7073) | Total Time 10.00(10.00)\n",
      "Iter 0110 | Time 30.4073(30.4445) | Bit/dim 3.1245(4.9839) | Xent 1.3586(1.5819) | Xent Color 1.9259(2.0608) | Loss 3.9456(5.8946) | Error 0.2356(0.3009) | Error Color 0.6322(0.7266) |Steps 416(410.35) | Grad Norm 3.4026(14.0451) | Total Time 10.00(10.00)\n",
      "Iter 0120 | Time 32.9555(30.6276) | Bit/dim 2.8555(4.4545) | Xent 1.3749(1.5243) | Xent Color 1.8678(2.0154) | Loss 3.6661(5.3394) | Error 0.2578(0.2874) | Error Color 0.6044(0.6907) |Steps 422(412.99) | Grad Norm 2.8212(11.1648) | Total Time 10.00(10.00)\n",
      "Iter 0130 | Time 30.6598(30.8590) | Bit/dim 2.6696(4.0060) | Xent 1.4132(1.4866) | Xent Color 1.8170(1.9687) | Loss 3.4772(4.8698) | Error 0.2711(0.2800) | Error Color 0.5033(0.6515) |Steps 422(415.36) | Grad Norm 2.0741(8.8699) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0002 | Time 77.8716, Epoch Time 2085.3447(2058.7344), Bit/dim 2.6268(best: 4.6238), Xent 1.3834, Xent Color 1.7750. Loss 3.4164, Error 0.2342(best: 0.2156), Error Color 0.4135(best: 0.7042)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0140 | Time 33.0246(31.1655) | Bit/dim 2.5558(3.6329) | Xent 1.4386(1.4704) | Xent Color 1.7443(1.9177) | Loss 3.3515(4.4799) | Error 0.2878(0.2817) | Error Color 0.4744(0.6082) |Steps 428(417.45) | Grad Norm 1.9482(7.0463) | Total Time 10.00(10.00)\n",
      "Iter 0150 | Time 33.5906(31.4555) | Bit/dim 2.4561(3.3298) | Xent 1.4995(1.4671) | Xent Color 1.6829(1.8635) | Loss 3.2517(4.1625) | Error 0.3522(0.2888) | Error Color 0.4289(0.5716) |Steps 428(420.22) | Grad Norm 1.5059(5.6106) | Total Time 10.00(10.00)\n",
      "Iter 0160 | Time 31.9761(31.6411) | Bit/dim 2.3606(3.0833) | Xent 1.5044(1.4741) | Xent Color 1.6173(1.8051) | Loss 3.1410(3.9032) | Error 0.3378(0.2982) | Error Color 0.4156(0.5371) |Steps 428(422.27) | Grad Norm 1.3638(4.5077) | Total Time 10.00(10.00)\n",
      "Iter 0170 | Time 37.5303(32.4296) | Bit/dim 2.3251(2.8864) | Xent 1.4746(1.4745) | Xent Color 1.5345(1.7444) | Loss 3.0773(3.6911) | Error 0.3511(0.3064) | Error Color 0.3800(0.5011) |Steps 440(425.77) | Grad Norm 1.2796(3.6945) | Total Time 10.00(10.00)\n",
      "Iter 0180 | Time 35.7684(33.3808) | Bit/dim 2.2944(2.7334) | Xent 1.3718(1.4609) | Xent Color 1.4684(1.6789) | Loss 3.0044(3.5183) | Error 0.2944(0.3108) | Error Color 0.3644(0.4674) |Steps 440(429.50) | Grad Norm 1.5868(3.1842) | Total Time 10.00(10.00)\n",
      "Iter 0190 | Time 36.9468(34.1679) | Bit/dim 2.2567(2.6109) | Xent 1.3088(1.4328) | Xent Color 1.3835(1.6099) | Loss 2.9297(3.3716) | Error 0.2844(0.3103) | Error Color 0.3733(0.4433) |Steps 440(432.26) | Grad Norm 1.7621(2.7290) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0003 | Time 82.6718, Epoch Time 2346.5303(2067.3683), Bit/dim 2.2574(best: 2.6268), Xent 1.2214, Xent Color 1.2787. Loss 2.8824, Error 0.2605(best: 0.2156), Error Color 0.3012(best: 0.4135)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0200 | Time 32.8824(34.0660) | Bit/dim 2.2707(2.5168) | Xent 1.2430(1.3928) | Xent Color 1.2856(1.5390) | Loss 2.9028(3.2498) | Error 0.3000(0.3067) | Error Color 0.3156(0.4230) |Steps 428(432.10) | Grad Norm 1.6096(2.4072) | Total Time 10.00(10.00)\n",
      "Iter 0210 | Time 32.0415(33.5830) | Bit/dim 2.2208(2.4438) | Xent 1.2348(1.3499) | Xent Color 1.2100(1.4658) | Loss 2.8320(3.1478) | Error 0.2911(0.3017) | Error Color 0.3289(0.4009) |Steps 428(431.03) | Grad Norm 3.6396(2.5341) | Total Time 10.00(10.00)\n",
      "Iter 0220 | Time 30.8486(33.3092) | Bit/dim 2.2253(2.3875) | Xent 1.0950(1.2968) | Xent Color 1.1404(1.3842) | Loss 2.7841(3.0578) | Error 0.2833(0.2948) | Error Color 0.3167(0.3757) |Steps 428(430.41) | Grad Norm 6.1482(2.8965) | Total Time 10.00(10.00)\n",
      "Iter 0230 | Time 33.5465(33.5500) | Bit/dim 2.2218(2.3441) | Xent 1.0187(1.2384) | Xent Color 1.0323(1.2971) | Loss 2.7346(2.9780) | Error 0.2433(0.2841) | Error Color 0.2967(0.3504) |Steps 434(431.35) | Grad Norm 9.2809(3.7863) | Total Time 10.00(10.00)\n",
      "Iter 0240 | Time 34.5418(33.7332) | Bit/dim 2.2038(2.3111) | Xent 0.9483(1.1750) | Xent Color 0.9967(1.2157) | Loss 2.6900(2.9087) | Error 0.2422(0.2744) | Error Color 0.3344(0.3293) |Steps 434(432.05) | Grad Norm 18.3332(6.0822) | Total Time 10.00(10.00)\n",
      "Iter 0250 | Time 35.5670(33.8501) | Bit/dim 2.2222(2.2866) | Xent 0.9144(1.1168) | Xent Color 0.8995(1.1385) | Loss 2.6757(2.8504) | Error 0.2200(0.2628) | Error Color 0.2233(0.3094) |Steps 434(432.89) | Grad Norm 14.7394(8.2926) | Total Time 10.00(10.00)\n",
      "Iter 0260 | Time 33.4591(33.5298) | Bit/dim 2.2287(2.2666) | Xent 0.8544(1.0605) | Xent Color 1.1299(1.0759) | Loss 2.7248(2.8006) | Error 0.2111(0.2508) | Error Color 0.5333(0.3000) |Steps 428(433.00) | Grad Norm 49.1710(11.6529) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0004 | Time 78.4284, Epoch Time 2294.1420(2074.1715), Bit/dim 2.2141(best: 2.2574), Xent 0.7915, Xent Color 0.8512. Loss 2.6248, Error 0.1606(best: 0.2156), Error Color 0.2584(best: 0.3012)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0270 | Time 31.8576(33.0956) | Bit/dim 2.1834(2.2500) | Xent 0.8967(1.0087) | Xent Color 0.8271(1.0314) | Loss 2.6144(2.7600) | Error 0.2356(0.2407) | Error Color 0.2056(0.2973) |Steps 428(431.82) | Grad Norm 16.2627(15.2282) | Total Time 10.00(10.00)\n",
      "Iter 0280 | Time 33.3051(32.9594) | Bit/dim 2.1837(2.2349) | Xent 0.7955(0.9599) | Xent Color 0.7032(0.9604) | Loss 2.5584(2.7150) | Error 0.1800(0.2303) | Error Color 0.1400(0.2631) |Steps 428(430.82) | Grad Norm 7.7279(14.1495) | Total Time 10.00(10.00)\n",
      "Iter 0290 | Time 32.6446(32.8448) | Bit/dim 2.1699(2.2211) | Xent 0.7799(0.9148) | Xent Color 0.8497(0.9158) | Loss 2.5773(2.6787) | Error 0.1833(0.2207) | Error Color 0.3611(0.2586) |Steps 440(432.67) | Grad Norm 32.8928(16.2866) | Total Time 10.00(10.00)\n",
      "Iter 0300 | Time 31.8321(32.8430) | Bit/dim 2.1520(2.2090) | Xent 0.7476(0.8740) | Xent Color 0.6326(0.8608) | Loss 2.4970(2.6427) | Error 0.1911(0.2123) | Error Color 0.1100(0.2358) |Steps 440(434.60) | Grad Norm 5.8605(15.7086) | Total Time 10.00(10.00)\n",
      "Iter 0310 | Time 31.9638(32.6891) | Bit/dim 2.1880(2.1997) | Xent 0.7254(0.8423) | Xent Color 0.6196(0.8010) | Loss 2.5243(2.6105) | Error 0.1844(0.2077) | Error Color 0.1344(0.2064) |Steps 440(436.02) | Grad Norm 11.6968(14.4492) | Total Time 10.00(10.00)\n",
      "Iter 0320 | Time 31.9772(32.5949) | Bit/dim 2.1798(2.1915) | Xent 0.6673(0.8063) | Xent Color 0.8075(0.8398) | Loss 2.5486(2.6030) | Error 0.1711(0.2021) | Error Color 0.3667(0.2504) |Steps 428(436.01) | Grad Norm 34.2821(21.8449) | Total Time 10.00(10.00)\n",
      "Iter 0330 | Time 30.4261(32.3184) | Bit/dim 2.1508(2.1825) | Xent 0.7565(0.7854) | Xent Color 0.6884(0.8035) | Loss 2.5120(2.5797) | Error 0.1956(0.1984) | Error Color 0.2411(0.2442) |Steps 428(433.91) | Grad Norm 21.3866(21.0512) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0005 | Time 78.4623, Epoch Time 2236.2981(2079.0353), Bit/dim 2.1504(best: 2.2141), Xent 0.6621, Xent Color 0.5235. Loss 2.4468, Error 0.1405(best: 0.1606), Error Color 0.0287(best: 0.2584)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0340 | Time 31.9775(32.2994) | Bit/dim 2.1342(2.1720) | Xent 0.6472(0.7630) | Xent Color 0.5748(0.7503) | Loss 2.4397(2.5503) | Error 0.1756(0.1942) | Error Color 0.1322(0.2170) |Steps 440(435.51) | Grad Norm 11.6002(18.5959) | Total Time 10.00(10.00)\n",
      "Iter 0350 | Time 31.9232(32.4113) | Bit/dim 2.1337(2.1654) | Xent 0.6160(0.7349) | Xent Color 0.5421(0.6984) | Loss 2.4232(2.5238) | Error 0.1678(0.1904) | Error Color 0.1222(0.1893) |Steps 440(436.69) | Grad Norm 7.4328(15.7866) | Total Time 10.00(10.00)\n",
      "Iter 0360 | Time 31.4451(32.5198) | Bit/dim 2.3032(2.1655) | Xent 0.6344(0.7124) | Xent Color 5.3741(0.8856) | Loss 3.8053(2.5650) | Error 0.1756(0.1889) | Error Color 0.8411(0.2154) |Steps 440(437.56) | Grad Norm 187.2687(24.6316) | Total Time 10.00(10.00)\n",
      "Iter 0370 | Time 33.2369(32.4111) | Bit/dim 2.1874(2.1726) | Xent 0.6715(0.7024) | Xent Color 0.8081(0.9834) | Loss 2.5573(2.5940) | Error 0.1933(0.1884) | Error Color 0.2744(0.2848) |Steps 428(435.60) | Grad Norm 11.5293(28.6674) | Total Time 10.00(10.00)\n",
      "Iter 0380 | Time 30.7954(32.2875) | Bit/dim 2.1141(2.1612) | Xent 0.7256(0.7028) | Xent Color 0.7385(0.9362) | Loss 2.4801(2.5709) | Error 0.1811(0.1885) | Error Color 0.1978(0.2772) |Steps 428(433.61) | Grad Norm 5.4406(24.5698) | Total Time 10.00(10.00)\n",
      "Iter 0390 | Time 33.9887(32.3362) | Bit/dim 2.1065(2.1490) | Xent 0.6290(0.6963) | Xent Color 0.6896(0.8710) | Loss 2.4362(2.5408) | Error 0.1644(0.1877) | Error Color 0.1800(0.2479) |Steps 440(433.69) | Grad Norm 5.0091(19.4022) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0006 | Time 77.6259, Epoch Time 2238.9867(2083.8338), Bit/dim 2.1100(best: 2.1504), Xent 0.5264, Xent Color 0.5596. Loss 2.3815, Error 0.1264(best: 0.1405), Error Color 0.0472(best: 0.0287)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0400 | Time 32.1019(32.4086) | Bit/dim 2.1077(2.1364) | Xent 0.6219(0.6823) | Xent Color 0.6047(0.8089) | Loss 2.4144(2.5091) | Error 0.1833(0.1865) | Error Color 0.1244(0.2203) |Steps 440(434.39) | Grad Norm 2.4979(15.2023) | Total Time 10.00(10.00)\n",
      "Iter 0410 | Time 31.4558(32.4540) | Bit/dim 2.0700(2.1262) | Xent 0.5642(0.6561) | Xent Color 0.5919(0.7475) | Loss 2.3590(2.4771) | Error 0.1467(0.1803) | Error Color 0.1233(0.1929) |Steps 440(435.86) | Grad Norm 1.0831(11.8533) | Total Time 10.00(10.00)\n",
      "Iter 0420 | Time 33.6103(32.7046) | Bit/dim 2.0718(2.1154) | Xent 0.5745(0.6363) | Xent Color 0.5325(0.6940) | Loss 2.3486(2.4480) | Error 0.1744(0.1776) | Error Color 0.1389(0.1727) |Steps 446(437.80) | Grad Norm 15.7151(10.2014) | Total Time 10.00(10.00)\n",
      "Iter 0430 | Time 35.1031(32.9151) | Bit/dim 2.1084(2.1097) | Xent 0.5709(0.6234) | Xent Color 0.6954(0.6577) | Loss 2.4250(2.4299) | Error 0.1622(0.1761) | Error Color 0.2800(0.1673) |Steps 446(439.60) | Grad Norm 42.7531(12.5505) | Total Time 10.00(10.00)\n",
      "Iter 0440 | Time 31.0593(32.9663) | Bit/dim 2.0689(2.1026) | Xent 0.6196(0.6110) | Xent Color 0.6376(0.6446) | Loss 2.3832(2.4166) | Error 0.1889(0.1740) | Error Color 0.2611(0.1799) |Steps 446(439.07) | Grad Norm 34.1907(16.4892) | Total Time 10.00(10.00)\n",
      "Iter 0450 | Time 33.4819(33.0228) | Bit/dim 2.0597(2.0951) | Xent 0.5716(0.6033) | Xent Color 0.5018(0.6058) | Loss 2.3280(2.3974) | Error 0.1711(0.1736) | Error Color 0.1100(0.1660) |Steps 434(439.04) | Grad Norm 12.9402(15.6359) | Total Time 10.00(10.00)\n",
      "Iter 0460 | Time 35.0046(33.1481) | Bit/dim 2.0496(2.0861) | Xent 0.5612(0.5926) | Xent Color 0.4102(0.5578) | Loss 2.2924(2.3737) | Error 0.1689(0.1719) | Error Color 0.0856(0.1428) |Steps 446(440.87) | Grad Norm 2.7837(12.5260) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0007 | Time 82.1854, Epoch Time 2298.1407(2090.2630), Bit/dim 2.0724(best: 2.1100), Xent 0.4297, Xent Color 0.3385. Loss 2.2645, Error 0.1126(best: 0.1264), Error Color 0.0173(best: 0.0287)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0470 | Time 36.7504(33.2788) | Bit/dim 2.0566(2.0804) | Xent 0.5446(0.5774) | Xent Color 0.3886(0.5147) | Loss 2.2899(2.3534) | Error 0.1722(0.1687) | Error Color 0.0667(0.1245) |Steps 446(442.21) | Grad Norm 5.7701(10.7537) | Total Time 10.00(10.00)\n",
      "Iter 0480 | Time 32.1348(33.2252) | Bit/dim 2.1207(2.0765) | Xent 0.5988(0.5684) | Xent Color 5.0394(0.6554) | Loss 3.5303(2.3825) | Error 0.1878(0.1675) | Error Color 0.8444(0.1556) |Steps 434(442.51) | Grad Norm 158.8350(18.6068) | Total Time 10.00(10.00)\n",
      "Iter 0490 | Time 34.3064(33.2453) | Bit/dim 2.0951(2.0886) | Xent 0.9551(0.5961) | Xent Color 0.7676(0.8443) | Loss 2.5257(2.4487) | Error 0.3200(0.1784) | Error Color 0.3078(0.2295) |Steps 440(439.93) | Grad Norm 25.6897(23.8582) | Total Time 10.00(10.00)\n",
      "Iter 0500 | Time 33.5535(33.5491) | Bit/dim 2.0709(2.0844) | Xent 0.4711(0.6199) | Xent Color 0.6471(0.7800) | Loss 2.3504(2.4343) | Error 0.1378(0.1849) | Error Color 0.2278(0.2232) |Steps 440(442.74) | Grad Norm 11.0167(20.8954) | Total Time 10.00(10.00)\n",
      "Iter 0510 | Time 35.8978(33.5384) | Bit/dim 2.0543(2.0791) | Xent 0.5530(0.5981) | Xent Color 0.4955(0.7142) | Loss 2.3165(2.4072) | Error 0.1633(0.1783) | Error Color 0.1256(0.2021) |Steps 464(445.31) | Grad Norm 5.6391(17.4057) | Total Time 10.00(10.00)\n",
      "Iter 0520 | Time 34.5895(33.5880) | Bit/dim 2.0474(2.0710) | Xent 0.5252(0.5878) | Xent Color 0.4458(0.6495) | Loss 2.2901(2.3803) | Error 0.1644(0.1754) | Error Color 0.1078(0.1801) |Steps 452(447.07) | Grad Norm 5.1451(14.2770) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0008 | Time 82.9445, Epoch Time 2324.0636(2097.2770), Bit/dim 2.0374(best: 2.0724), Xent 0.4027, Xent Color 0.3406. Loss 2.2232, Error 0.1101(best: 0.1126), Error Color 0.0250(best: 0.0173)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0530 | Time 33.6650(33.5982) | Bit/dim 2.0309(2.0607) | Xent 0.4998(0.5715) | Xent Color 0.4059(0.5890) | Loss 2.2574(2.3508) | Error 0.1556(0.1711) | Error Color 0.0856(0.1562) |Steps 452(448.36) | Grad Norm 3.8069(11.4355) | Total Time 10.00(10.00)\n",
      "Iter 0540 | Time 33.0685(33.6590) | Bit/dim 2.0282(2.0504) | Xent 0.5574(0.5618) | Xent Color 0.3688(0.5344) | Loss 2.2597(2.3244) | Error 0.1689(0.1689) | Error Color 0.0844(0.1363) |Steps 452(449.32) | Grad Norm 3.5492(9.1673) | Total Time 10.00(10.00)\n",
      "Iter 0550 | Time 34.5659(33.6025) | Bit/dim 2.0088(2.0412) | Xent 0.5331(0.5501) | Xent Color 0.3600(0.4861) | Loss 2.2320(2.3003) | Error 0.1667(0.1652) | Error Color 0.0844(0.1190) |Steps 452(450.02) | Grad Norm 3.6799(7.4776) | Total Time 10.00(10.00)\n",
      "Iter 0560 | Time 33.3874(33.5083) | Bit/dim 1.9898(2.0329) | Xent 0.5358(0.5421) | Xent Color 0.2994(0.4414) | Loss 2.1986(2.2788) | Error 0.1689(0.1631) | Error Color 0.0478(0.1028) |Steps 452(450.54) | Grad Norm 4.6029(6.0484) | Total Time 10.00(10.00)\n",
      "Iter 0570 | Time 35.1157(33.5748) | Bit/dim 1.9854(2.0238) | Xent 0.5079(0.5356) | Xent Color 0.2887(0.4036) | Loss 2.1846(2.2586) | Error 0.1611(0.1613) | Error Color 0.0600(0.0916) |Steps 452(450.92) | Grad Norm 2.9251(5.4420) | Total Time 10.00(10.00)\n",
      "Iter 0580 | Time 33.6317(33.6650) | Bit/dim 1.9859(2.0152) | Xent 0.4930(0.5314) | Xent Color 0.2952(0.3723) | Loss 2.1830(2.2412) | Error 0.1500(0.1614) | Error Color 0.0567(0.0823) |Steps 452(451.21) | Grad Norm 2.6073(5.1279) | Total Time 10.00(10.00)\n",
      "Iter 0590 | Time 33.5526(33.7299) | Bit/dim 1.9502(2.0051) | Xent 0.5459(0.5276) | Xent Color 0.2525(0.3450) | Loss 2.1498(2.2232) | Error 0.1511(0.1591) | Error Color 0.0478(0.0755) |Steps 452(451.42) | Grad Norm 5.1531(5.5120) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0009 | Time 83.4049, Epoch Time 2333.2817(2104.3572), Bit/dim 1.9852(best: 2.0374), Xent 0.3735, Xent Color 0.2812. Loss 2.1488, Error 0.1042(best: 0.1101), Error Color 0.0897(best: 0.0173)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0600 | Time 32.1703(33.8172) | Bit/dim 1.9684(1.9964) | Xent 0.5244(0.5266) | Xent Color 0.2600(0.3323) | Loss 2.1645(2.2111) | Error 0.1611(0.1589) | Error Color 0.0578(0.0778) |Steps 434(450.10) | Grad Norm 13.2125(8.1504) | Total Time 10.00(10.00)\n",
      "Iter 0610 | Time 31.9758(33.7719) | Bit/dim 1.9682(1.9864) | Xent 0.4792(0.5252) | Xent Color 0.2632(0.3105) | Loss 2.1538(2.1953) | Error 0.1356(0.1595) | Error Color 0.0578(0.0722) |Steps 434(446.40) | Grad Norm 7.6076(8.4057) | Total Time 10.00(10.00)\n",
      "Iter 0620 | Time 32.0055(33.5893) | Bit/dim 2.0010(1.9801) | Xent 0.4907(0.5230) | Xent Color 1.3100(0.3633) | Loss 2.4512(2.2017) | Error 0.1600(0.1618) | Error Color 0.4678(0.1011) |Steps 446(443.92) | Grad Norm 81.7254(14.8621) | Total Time 10.00(10.00)\n",
      "Iter 0630 | Time 31.3337(33.2967) | Bit/dim 1.9243(1.9753) | Xent 0.5644(0.5278) | Xent Color 0.3918(0.4016) | Loss 2.1633(2.2076) | Error 0.1789(0.1642) | Error Color 0.1622(0.1293) |Steps 434(441.32) | Grad Norm 23.4653(18.3509) | Total Time 10.00(10.00)\n",
      "Iter 0640 | Time 32.1812(33.2669) | Bit/dim 1.9363(1.9659) | Xent 0.5048(0.5287) | Xent Color 0.3735(0.3928) | Loss 2.1559(2.1963) | Error 0.1567(0.1635) | Error Color 0.1244(0.1305) |Steps 446(442.71) | Grad Norm 19.0989(18.0948) | Total Time 10.00(10.00)\n",
      "Iter 0650 | Time 37.5085(33.6886) | Bit/dim 1.9200(1.9560) | Xent 0.5249(0.5258) | Xent Color 0.2610(0.3617) | Loss 2.1165(2.1778) | Error 0.1600(0.1627) | Error Color 0.0589(0.1150) |Steps 458(445.26) | Grad Norm 9.8103(15.7447) | Total Time 10.00(10.00)\n",
      "Iter 0660 | Time 35.2599(34.2431) | Bit/dim 1.9091(1.9446) | Xent 0.5174(0.5196) | Xent Color 0.2385(0.3284) | Loss 2.0981(2.1566) | Error 0.1567(0.1608) | Error Color 0.0500(0.0989) |Steps 458(448.14) | Grad Norm 2.3247(12.9322) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0010 | Time 86.8708, Epoch Time 2342.5621(2111.5033), Bit/dim 1.9191(best: 1.9852), Xent 0.3629, Xent Color 0.1518. Loss 2.0478, Error 0.1051(best: 0.1042), Error Color 0.0057(best: 0.0173)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0670 | Time 36.1629(34.8493) | Bit/dim 1.9063(1.9338) | Xent 0.5208(0.5192) | Xent Color 0.1982(0.2965) | Loss 2.0860(2.1377) | Error 0.1611(0.1602) | Error Color 0.0433(0.0848) |Steps 452(449.72) | Grad Norm 6.2867(10.8092) | Total Time 10.00(10.00)\n",
      "Iter 0680 | Time 36.2516(35.1777) | Bit/dim 1.8602(1.9224) | Xent 0.4777(0.5103) | Xent Color 0.2288(0.2775) | Loss 2.0368(2.1194) | Error 0.1511(0.1578) | Error Color 0.0633(0.0785) |Steps 452(449.37) | Grad Norm 14.9795(11.5020) | Total Time 10.00(10.00)\n",
      "Iter 0690 | Time 32.1786(34.8478) | Bit/dim 2.1445(1.9535) | Xent 0.5649(0.5119) | Xent Color 1.0747(0.7942) | Loss 2.5544(2.2800) | Error 0.1722(0.1574) | Error Color 0.4022(0.1722) |Steps 428(445.86) | Grad Norm 18.6512(25.8459) | Total Time 10.00(10.00)\n",
      "Iter 0700 | Time 36.9390(35.0864) | Bit/dim 2.0287(1.9835) | Xent 0.6109(0.5759) | Xent Color 0.6854(0.8274) | Loss 2.3528(2.3343) | Error 0.1956(0.1799) | Error Color 0.2778(0.2163) |Steps 464(448.91) | Grad Norm 8.5368(24.1306) | Total Time 10.00(10.00)\n",
      "Iter 0710 | Time 33.1154(34.9973) | Bit/dim 1.9703(1.9840) | Xent 0.6104(0.5926) | Xent Color 0.5463(0.7774) | Loss 2.2595(2.3265) | Error 0.1889(0.1851) | Error Color 0.1944(0.2232) |Steps 464(451.81) | Grad Norm 7.2349(20.5825) | Total Time 10.00(10.00)\n",
      "Iter 0720 | Time 33.0223(34.5570) | Bit/dim 1.9478(1.9775) | Xent 0.5151(0.5883) | Xent Color 0.4511(0.7090) | Loss 2.1893(2.3018) | Error 0.1489(0.1827) | Error Color 0.1544(0.2145) |Steps 452(452.73) | Grad Norm 6.3888(17.2475) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0011 | Time 82.9380, Epoch Time 2414.3448(2120.5886), Bit/dim 1.9312(best: 1.9191), Xent 0.3655, Xent Color 0.2643. Loss 2.0887, Error 0.1028(best: 0.1042), Error Color 0.0507(best: 0.0057)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0730 | Time 33.2993(34.5147) | Bit/dim 1.9081(1.9656) | Xent 0.4874(0.5737) | Xent Color 0.3805(0.6318) | Loss 2.1251(2.2670) | Error 0.1511(0.1779) | Error Color 0.1244(0.1954) |Steps 458(452.60) | Grad Norm 4.5848(14.0653) | Total Time 10.00(10.00)\n",
      "Iter 0740 | Time 34.7504(34.6025) | Bit/dim 1.8960(1.9506) | Xent 0.5711(0.5641) | Xent Color 0.3144(0.5476) | Loss 2.1174(2.2286) | Error 0.1811(0.1750) | Error Color 0.0889(0.1675) |Steps 464(454.87) | Grad Norm 2.4009(11.3422) | Total Time 10.00(10.00)\n",
      "Iter 0750 | Time 35.3753(34.9250) | Bit/dim 1.8734(1.9341) | Xent 0.5149(0.5546) | Xent Color 0.2391(0.4708) | Loss 2.0619(2.1905) | Error 0.1578(0.1723) | Error Color 0.0611(0.1412) |Steps 458(456.76) | Grad Norm 0.9901(9.0737) | Total Time 10.00(10.00)\n",
      "Iter 0760 | Time 37.7942(35.0224) | Bit/dim 1.8705(1.9193) | Xent 0.5311(0.5487) | Xent Color 0.1995(0.4035) | Loss 2.0531(2.1574) | Error 0.1644(0.1697) | Error Color 0.0489(0.1182) |Steps 470(458.59) | Grad Norm 1.1480(7.2970) | Total Time 10.00(10.00)\n",
      "Iter 0770 | Time 36.9673(35.8229) | Bit/dim 1.8713(1.9056) | Xent 0.4525(0.5390) | Xent Color 0.1610(0.3484) | Loss 2.0247(2.1274) | Error 0.1489(0.1653) | Error Color 0.0289(0.0996) |Steps 476(462.53) | Grad Norm 0.8311(5.9622) | Total Time 10.00(10.00)\n",
      "Iter 0780 | Time 40.6124(36.6534) | Bit/dim 1.8704(1.8933) | Xent 0.5006(0.5299) | Xent Color 0.1614(0.3034) | Loss 2.0360(2.1016) | Error 0.1722(0.1641) | Error Color 0.0322(0.0847) |Steps 476(466.23) | Grad Norm 0.8757(5.4971) | Total Time 10.00(10.00)\n",
      "Iter 0790 | Time 37.7719(37.2180) | Bit/dim 1.8487(1.8799) | Xent 0.4646(0.5163) | Xent Color 0.1418(0.2646) | Loss 2.0003(2.0751) | Error 0.1356(0.1606) | Error Color 0.0322(0.0719) |Steps 476(468.98) | Grad Norm 2.0507(4.7975) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0012 | Time 88.4114, Epoch Time 2545.1703(2133.3260), Bit/dim 1.8409(best: 1.9191), Xent 0.3465, Xent Color 0.0800. Loss 1.9475, Error 0.0973(best: 0.1028), Error Color 0.0042(best: 0.0057)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0800 | Time 37.8826(37.5415) | Bit/dim 1.8493(1.8677) | Xent 0.5655(0.5141) | Xent Color 0.2058(0.2437) | Loss 2.0421(2.0572) | Error 0.1611(0.1594) | Error Color 0.0689(0.0673) |Steps 482(471.48) | Grad Norm 19.1799(6.6187) | Total Time 10.00(10.00)\n",
      "Iter 0810 | Time 34.4414(37.6470) | Bit/dim 2.0502(1.8729) | Xent 0.5196(0.5131) | Xent Color 2.0158(0.5333) | Loss 2.6840(2.1345) | Error 0.1389(0.1578) | Error Color 0.5256(0.1390) |Steps 446(469.14) | Grad Norm 51.5767(19.1353) | Total Time 10.00(10.00)\n",
      "Iter 0820 | Time 35.8402(37.3093) | Bit/dim 1.8650(1.8775) | Xent 0.4805(0.5328) | Xent Color 0.3733(0.5458) | Loss 2.0785(2.1472) | Error 0.1400(0.1665) | Error Color 0.1500(0.1636) |Steps 476(468.75) | Grad Norm 6.6207(18.7076) | Total Time 10.00(10.00)\n",
      "Iter 0830 | Time 38.8904(37.2194) | Bit/dim 1.8366(1.8734) | Xent 0.5574(0.5273) | Xent Color 0.2635(0.4880) | Loss 2.0418(2.1272) | Error 0.1689(0.1637) | Error Color 0.0744(0.1506) |Steps 488(472.16) | Grad Norm 1.6353(15.2614) | Total Time 10.00(10.00)\n",
      "Iter 0840 | Time 38.9909(37.4718) | Bit/dim 1.8183(1.8629) | Xent 0.5479(0.5254) | Xent Color 0.2189(0.4244) | Loss 2.0100(2.1003) | Error 0.1789(0.1636) | Error Color 0.0611(0.1302) |Steps 476(474.20) | Grad Norm 2.4922(12.1517) | Total Time 10.00(10.00)\n",
      "Iter 0850 | Time 39.0277(37.6834) | Bit/dim 1.7814(1.8475) | Xent 0.4877(0.5240) | Xent Color 0.1751(0.3621) | Loss 1.9471(2.0690) | Error 0.1500(0.1628) | Error Color 0.0322(0.1077) |Steps 476(475.59) | Grad Norm 0.9627(9.4814) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0013 | Time 96.9429, Epoch Time 2621.2727(2147.9644), Bit/dim 1.8036(best: 1.8409), Xent 0.3396, Xent Color 0.0789. Loss 1.9082, Error 0.0987(best: 0.0973), Error Color 0.0019(best: 0.0042)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0860 | Time 39.8551(38.1705) | Bit/dim 1.8012(1.8352) | Xent 0.4908(0.5183) | Xent Color 0.1321(0.3055) | Loss 1.9569(2.0411) | Error 0.1589(0.1621) | Error Color 0.0289(0.0877) |Steps 494(479.16) | Grad Norm 1.3930(7.4341) | Total Time 10.00(10.00)\n",
      "Iter 0870 | Time 39.4944(38.6300) | Bit/dim 1.7885(1.8222) | Xent 0.5349(0.5203) | Xent Color 0.1128(0.2571) | Loss 1.9504(2.0165) | Error 0.1500(0.1615) | Error Color 0.0144(0.0706) |Steps 494(483.15) | Grad Norm 0.7822(5.8189) | Total Time 10.00(10.00)\n",
      "Iter 0880 | Time 41.1066(39.1270) | Bit/dim 1.7927(1.8097) | Xent 0.5341(0.5138) | Xent Color 0.1279(0.2197) | Loss 1.9582(1.9930) | Error 0.1700(0.1595) | Error Color 0.0344(0.0579) |Steps 488(485.21) | Grad Norm 1.6579(4.6605) | Total Time 10.00(10.00)\n",
      "Iter 0890 | Time 40.9194(39.6865) | Bit/dim 1.7658(1.7988) | Xent 0.5117(0.5128) | Xent Color 0.1062(0.1904) | Loss 1.9202(1.9746) | Error 0.1456(0.1589) | Error Color 0.0256(0.0489) |Steps 506(488.52) | Grad Norm 2.6617(3.9234) | Total Time 10.00(10.00)\n",
      "Iter 0900 | Time 40.8623(40.2227) | Bit/dim 1.7266(1.7864) | Xent 0.5513(0.5121) | Xent Color 0.0874(0.1660) | Loss 1.8863(1.9560) | Error 0.1700(0.1601) | Error Color 0.0144(0.0412) |Steps 506(492.12) | Grad Norm 0.9517(3.4535) | Total Time 10.00(10.00)\n",
      "Iter 0910 | Time 40.9279(40.4510) | Bit/dim 1.7395(1.7770) | Xent 0.5496(0.5133) | Xent Color 0.0853(0.1461) | Loss 1.8982(1.9418) | Error 0.1544(0.1600) | Error Color 0.0156(0.0351) |Steps 488(495.52) | Grad Norm 2.2877(3.1280) | Total Time 10.00(10.00)\n",
      "Iter 0920 | Time 41.3924(40.8004) | Bit/dim 1.7350(1.7658) | Xent 0.5494(0.5088) | Xent Color 0.0906(0.1304) | Loss 1.8950(1.9256) | Error 0.1644(0.1584) | Error Color 0.0167(0.0308) |Steps 506(498.27) | Grad Norm 1.5838(2.7823) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0014 | Time 108.3757, Epoch Time 2838.9076(2168.6927), Bit/dim 1.7311(best: 1.8036), Xent 0.3319, Xent Color 0.0343. Loss 1.8226, Error 0.0976(best: 0.0973), Error Color 0.0013(best: 0.0019)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0930 | Time 42.1697(41.0549) | Bit/dim 1.6892(1.7539) | Xent 0.4665(0.5076) | Xent Color 0.0814(0.1166) | Loss 1.8262(1.9099) | Error 0.1378(0.1576) | Error Color 0.0167(0.0268) |Steps 506(500.46) | Grad Norm 1.4462(2.5451) | Total Time 10.00(10.00)\n",
      "Iter 0940 | Time 42.3091(41.3260) | Bit/dim 1.7056(1.7438) | Xent 0.4891(0.5047) | Xent Color 0.0765(0.1052) | Loss 1.8470(1.8962) | Error 0.1522(0.1570) | Error Color 0.0144(0.0238) |Steps 506(502.24) | Grad Norm 2.6109(2.3175) | Total Time 10.00(10.00)\n",
      "Iter 0950 | Time 41.0983(41.6119) | Bit/dim 1.7004(1.7325) | Xent 0.5195(0.5021) | Xent Color 0.0626(0.0956) | Loss 1.8459(1.8819) | Error 0.1544(0.1561) | Error Color 0.0089(0.0208) |Steps 512(503.89) | Grad Norm 2.1648(2.1789) | Total Time 10.00(10.00)\n",
      "Iter 0960 | Time 43.3863(41.7052) | Bit/dim 1.6810(1.7225) | Xent 0.5080(0.5036) | Xent Color 0.0742(0.0888) | Loss 1.8266(1.8707) | Error 0.1611(0.1572) | Error Color 0.0178(0.0189) |Steps 506(505.09) | Grad Norm 2.7411(2.4998) | Total Time 10.00(10.00)\n",
      "Iter 0970 | Time 42.5430(41.9120) | Bit/dim 1.6831(1.7109) | Xent 0.5283(0.5044) | Xent Color 0.0735(0.0842) | Loss 1.8335(1.8580) | Error 0.1678(0.1577) | Error Color 0.0178(0.0178) |Steps 512(506.13) | Grad Norm 3.8017(2.9815) | Total Time 10.00(10.00)\n",
      "Iter 0980 | Time 42.0109(41.7878) | Bit/dim 1.6544(1.6985) | Xent 0.5754(0.5032) | Xent Color 0.0584(0.0791) | Loss 1.8129(1.8441) | Error 0.1722(0.1576) | Error Color 0.0078(0.0167) |Steps 512(507.53) | Grad Norm 3.0924(3.3663) | Total Time 10.00(10.00)\n",
      "Iter 0990 | Time 41.4358(41.9694) | Bit/dim 1.6745(1.6876) | Xent 0.4639(0.5001) | Xent Color 0.0505(0.0742) | Loss 1.8031(1.8311) | Error 0.1511(0.1565) | Error Color 0.0067(0.0155) |Steps 512(508.54) | Grad Norm 2.0534(3.4698) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0015 | Time 108.9927, Epoch Time 2913.3286(2191.0318), Bit/dim 1.6517(best: 1.7311), Xent 0.3328, Xent Color 0.0229. Loss 1.7406, Error 0.0992(best: 0.0973), Error Color 0.0011(best: 0.0013)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1000 | Time 43.6887(42.0305) | Bit/dim 1.6489(1.6747) | Xent 0.4726(0.5028) | Xent Color 0.0575(0.0700) | Loss 1.7814(1.8179) | Error 0.1522(0.1579) | Error Color 0.0100(0.0142) |Steps 512(509.14) | Grad Norm 2.8973(3.2125) | Total Time 10.00(10.00)\n",
      "Iter 1010 | Time 42.2839(42.0467) | Bit/dim 1.6354(1.6613) | Xent 0.5164(0.5045) | Xent Color 0.0534(0.0662) | Loss 1.7778(1.8040) | Error 0.1711(0.1579) | Error Color 0.0100(0.0132) |Steps 512(509.89) | Grad Norm 2.7792(3.2091) | Total Time 10.00(10.00)\n",
      "Iter 1020 | Time 39.6100(42.1393) | Bit/dim 1.5912(1.6474) | Xent 0.4703(0.5009) | Xent Color 0.0414(0.0622) | Loss 1.7191(1.7882) | Error 0.1400(0.1572) | Error Color 0.0044(0.0120) |Steps 512(510.44) | Grad Norm 1.3173(3.0357) | Total Time 10.00(10.00)\n",
      "Iter 1030 | Time 43.6801(42.2630) | Bit/dim 1.5804(1.6331) | Xent 0.5170(0.4983) | Xent Color 0.0676(0.0626) | Loss 1.7265(1.7733) | Error 0.1744(0.1562) | Error Color 0.0167(0.0128) |Steps 512(510.85) | Grad Norm 7.7419(3.8093) | Total Time 10.00(10.00)\n",
      "Iter 1040 | Time 43.0682(42.4524) | Bit/dim 1.5771(1.6209) | Xent 0.6194(0.5024) | Xent Color 0.0507(0.0613) | Loss 1.7446(1.7618) | Error 0.1833(0.1564) | Error Color 0.0111(0.0128) |Steps 518(512.43) | Grad Norm 3.5668(4.3224) | Total Time 10.00(10.00)\n",
      "Iter 1050 | Time 33.9148(42.3517) | Bit/dim 3.2504(1.7235) | Xent 0.5270(0.5025) | Xent Color 8.9377(1.7399) | Loss 5.6166(2.2841) | Error 0.1500(0.1557) | Error Color 0.8267(0.0917) |Steps 434(509.97) | Grad Norm 49.5760(22.0725) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0016 | Time 105.6723, Epoch Time 2895.5343(2212.1669), Bit/dim 2.3976(best: 1.6517), Xent 0.9421, Xent Color 1.5936. Loss 3.0315, Error 0.3084(best: 0.0973), Error Color 0.4765(best: 0.0011)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1060 | Time 47.8200(41.6882) | Bit/dim 2.3236(1.9606) | Xent 1.2311(0.6193) | Xent Color 1.7560(2.2268) | Loss 3.0704(2.6722) | Error 0.4256(0.1962) | Error Color 0.5522(0.2331) |Steps 722(530.12) | Grad Norm 23.2989(24.4724) | Total Time 10.00(10.00)\n",
      "Iter 1070 | Time 39.4887(41.9267) | Bit/dim 2.1281(2.0229) | Xent 0.7520(0.7034) | Xent Color 1.7394(2.1852) | Loss 2.7510(2.7451) | Error 0.2367(0.2248) | Error Color 0.5056(0.3215) |Steps 608(556.39) | Grad Norm 32.7620(28.4748) | Total Time 10.00(10.00)\n",
      "Iter 1080 | Time 39.2129(40.9771) | Bit/dim 1.9982(2.0306) | Xent 0.6258(0.6866) | Xent Color 0.8242(1.9454) | Loss 2.3607(2.6886) | Error 0.2033(0.2189) | Error Color 0.3289(0.3509) |Steps 572(562.24) | Grad Norm 3.2093(25.3904) | Total Time 10.00(10.00)\n",
      "Iter 1090 | Time 37.9872(40.2101) | Bit/dim 1.9403(2.0127) | Xent 0.5951(0.6645) | Xent Color 0.6218(1.6302) | Loss 2.2445(2.5864) | Error 0.1867(0.2108) | Error Color 0.2456(0.3378) |Steps 536(560.51) | Grad Norm 3.0546(20.0431) | Total Time 10.00(10.00)\n",
      "Iter 1100 | Time 37.4689(39.4278) | Bit/dim 1.8783(1.9831) | Xent 0.4986(0.6312) | Xent Color 0.4531(1.3398) | Loss 2.1162(2.4759) | Error 0.1456(0.2000) | Error Color 0.1622(0.3001) |Steps 524(552.06) | Grad Norm 3.0382(15.4925) | Total Time 10.00(10.00)\n",
      "Iter 1110 | Time 36.0876(38.8575) | Bit/dim 1.8384(1.9519) | Xent 0.5017(0.6027) | Xent Color 0.3114(1.0863) | Loss 2.0416(2.3741) | Error 0.1522(0.1897) | Error Color 0.0944(0.2542) |Steps 518(542.34) | Grad Norm 1.7847(12.0205) | Total Time 10.00(10.00)\n",
      "Iter 1120 | Time 39.8870(38.4448) | Bit/dim 1.8006(1.9153) | Xent 0.4881(0.5829) | Xent Color 0.2521(0.8758) | Loss 1.9856(2.2799) | Error 0.1578(0.1823) | Error Color 0.0667(0.2101) |Steps 524(534.84) | Grad Norm 1.5802(9.2785) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0017 | Time 92.7607, Epoch Time 2676.4989(2226.0968), Bit/dim 1.7875(best: 1.6517), Xent 0.3644, Xent Color 0.1520. Loss 1.9166, Error 0.1076(best: 0.0973), Error Color 0.0123(best: 0.0011)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1130 | Time 40.3617(38.7912) | Bit/dim 1.7300(1.8761) | Xent 0.4843(0.5662) | Xent Color 0.2141(0.7089) | Loss 1.9046(2.1949) | Error 0.1467(0.1761) | Error Color 0.0578(0.1730) |Steps 530(533.27) | Grad Norm 1.7143(7.3171) | Total Time 10.00(10.00)\n",
      "Iter 1140 | Time 42.8518(39.3138) | Bit/dim 1.6850(1.8352) | Xent 0.5059(0.5542) | Xent Color 0.1974(0.5765) | Loss 1.8608(2.1179) | Error 0.1533(0.1717) | Error Color 0.0556(0.1420) |Steps 536(532.11) | Grad Norm 2.3176(5.8616) | Total Time 10.00(10.00)\n",
      "Iter 1150 | Time 42.6557(39.8936) | Bit/dim 1.6796(1.7965) | Xent 0.5464(0.5462) | Xent Color 0.1904(0.4761) | Loss 1.8638(2.0521) | Error 0.1733(0.1699) | Error Color 0.0556(0.1184) |Steps 530(532.03) | Grad Norm 4.3296(5.2669) | Total Time 10.00(10.00)\n",
      "Iter 1160 | Time 45.6229(40.9787) | Bit/dim 1.6444(1.7598) | Xent 0.4864(0.5359) | Xent Color 0.1716(0.3970) | Loss 1.8089(1.9930) | Error 0.1589(0.1674) | Error Color 0.0422(0.0992) |Steps 554(536.84) | Grad Norm 3.0235(5.0088) | Total Time 10.00(10.00)\n",
      "Iter 1170 | Time 43.7589(41.5469) | Bit/dim 1.6474(1.7276) | Xent 0.5495(0.5304) | Xent Color 0.1851(0.3426) | Loss 1.8311(1.9459) | Error 0.1722(0.1654) | Error Color 0.0600(0.0873) |Steps 560(540.73) | Grad Norm 9.9547(5.9312) | Total Time 10.00(10.00)\n",
      "Iter 1180 | Time 44.6118(42.2387) | Bit/dim 1.6060(1.6975) | Xent 0.4990(0.5317) | Xent Color 0.1295(0.2967) | Loss 1.7631(1.9046) | Error 0.1500(0.1655) | Error Color 0.0378(0.0776) |Steps 566(545.24) | Grad Norm 3.1737(6.3489) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0018 | Time 113.1555, Epoch Time 2941.7404(2247.5661), Bit/dim 1.5982(best: 1.6517), Xent 0.3484, Xent Color 0.0633. Loss 1.7011, Error 0.1006(best: 0.0973), Error Color 0.0036(best: 0.0011)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1190 | Time 44.8137(42.7235) | Bit/dim 1.5963(1.6709) | Xent 0.4760(0.5242) | Xent Color 0.1053(0.2534) | Loss 1.7417(1.8653) | Error 0.1589(0.1636) | Error Color 0.0300(0.0661) |Steps 566(550.41) | Grad Norm 3.5564(5.8833) | Total Time 10.00(10.00)\n",
      "Iter 1200 | Time 45.9232(43.3266) | Bit/dim 1.5792(1.6481) | Xent 0.5024(0.5206) | Xent Color 0.1452(0.2168) | Loss 1.7412(1.8324) | Error 0.1556(0.1621) | Error Color 0.0389(0.0551) |Steps 566(553.66) | Grad Norm 8.4820(5.2397) | Total Time 10.00(10.00)\n",
      "Iter 1210 | Time 45.1226(43.7307) | Bit/dim 1.5688(1.6291) | Xent 0.4870(0.5196) | Xent Color 0.1141(0.2046) | Loss 1.7191(1.8102) | Error 0.1478(0.1614) | Error Color 0.0311(0.0559) |Steps 560(555.34) | Grad Norm 2.7497(7.2810) | Total Time 10.00(10.00)\n",
      "Iter 1220 | Time 43.4623(43.7023) | Bit/dim 1.5462(1.6100) | Xent 0.5336(0.5186) | Xent Color 0.1019(0.1934) | Loss 1.7051(1.7880) | Error 0.1678(0.1621) | Error Color 0.0244(0.0551) |Steps 554(555.77) | Grad Norm 5.6568(8.3988) | Total Time 10.00(10.00)\n",
      "Iter 1230 | Time 43.8875(43.7047) | Bit/dim 1.5295(1.5941) | Xent 0.4937(0.5156) | Xent Color 0.0889(0.1713) | Loss 1.6751(1.7658) | Error 0.1567(0.1614) | Error Color 0.0200(0.0481) |Steps 554(555.93) | Grad Norm 2.2095(7.6800) | Total Time 10.00(10.00)\n",
      "Iter 1240 | Time 45.6079(43.9207) | Bit/dim 1.5338(1.5801) | Xent 0.4962(0.5117) | Xent Color 0.0826(0.1509) | Loss 1.6785(1.7457) | Error 0.1644(0.1610) | Error Color 0.0144(0.0413) |Steps 554(555.86) | Grad Norm 2.1673(6.7223) | Total Time 10.00(10.00)\n",
      "Iter 1250 | Time 43.7279(43.8825) | Bit/dim 1.5251(1.5655) | Xent 0.5452(0.5105) | Xent Color 0.0888(0.1347) | Loss 1.6836(1.7268) | Error 0.1700(0.1603) | Error Color 0.0256(0.0362) |Steps 554(557.07) | Grad Norm 4.7886(6.1757) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0019 | Time 116.5696, Epoch Time 3063.0893(2272.0318), Bit/dim 1.5219(best: 1.5982), Xent 0.3420, Xent Color 0.0318. Loss 1.6154, Error 0.1018(best: 0.0973), Error Color 0.0008(best: 0.0011)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1260 | Time 45.2118(43.9983) | Bit/dim 1.5054(1.5541) | Xent 0.5309(0.5164) | Xent Color 0.0719(0.1203) | Loss 1.6561(1.7132) | Error 0.1533(0.1614) | Error Color 0.0178(0.0317) |Steps 566(558.50) | Grad Norm 1.5938(5.7062) | Total Time 10.00(10.00)\n",
      "Iter 1270 | Time 46.1463(44.1496) | Bit/dim 1.5081(1.5413) | Xent 0.4441(0.5157) | Xent Color 0.0823(0.1092) | Loss 1.6397(1.6975) | Error 0.1456(0.1619) | Error Color 0.0200(0.0282) |Steps 572(560.66) | Grad Norm 3.5852(5.5341) | Total Time 10.00(10.00)\n",
      "Iter 1280 | Time 43.4057(44.2001) | Bit/dim 1.5017(1.5297) | Xent 0.5187(0.5133) | Xent Color 0.1259(0.0999) | Loss 1.6629(1.6830) | Error 0.1656(0.1620) | Error Color 0.0433(0.0252) |Steps 578(562.64) | Grad Norm 17.5580(5.3583) | Total Time 10.00(10.00)\n",
      "Iter 1290 | Time 41.3399(43.4702) | Bit/dim 1.7109(1.5742) | Xent 0.5657(0.5173) | Xent Color 1.2475(0.8065) | Loss 2.1642(1.9051) | Error 0.1633(0.1629) | Error Color 0.5111(0.1525) |Steps 542(557.98) | Grad Norm 8.5037(18.1953) | Total Time 10.00(10.00)\n",
      "Iter 1300 | Time 45.7233(42.8572) | Bit/dim 1.6784(1.6065) | Xent 0.6572(0.5586) | Xent Color 0.6629(0.8019) | Loss 2.0084(1.9466) | Error 0.2044(0.1766) | Error Color 0.2689(0.1982) |Steps 560(557.78) | Grad Norm 6.2015(15.3830) | Total Time 10.00(10.00)\n",
      "Iter 1310 | Time 41.3857(42.6048) | Bit/dim 1.5919(1.6117) | Xent 0.4814(0.5533) | Xent Color 0.5293(0.7394) | Loss 1.8446(1.9348) | Error 0.1600(0.1746) | Error Color 0.1911(0.2038) |Steps 524(552.99) | Grad Norm 3.4584(12.3127) | Total Time 10.00(10.00)\n",
      "Iter 1320 | Time 43.6147(42.9513) | Bit/dim 1.5446(1.5989) | Xent 0.4922(0.5414) | Xent Color 0.4083(0.6620) | Loss 1.7697(1.8997) | Error 0.1411(0.1709) | Error Color 0.1311(0.1917) |Steps 554(551.48) | Grad Norm 3.4233(9.9573) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0020 | Time 115.9179, Epoch Time 2977.5238(2293.1966), Bit/dim 1.5453(best: 1.5219), Xent 0.3533, Xent Color 0.2440. Loss 1.6946, Error 0.1014(best: 0.0973), Error Color 0.0230(best: 0.0008)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1330 | Time 43.2948(43.0894) | Bit/dim 1.5327(1.5815) | Xent 0.4745(0.5332) | Xent Color 0.2705(0.5735) | Loss 1.7190(1.8582) | Error 0.1467(0.1676) | Error Color 0.0889(0.1682) |Steps 554(551.71) | Grad Norm 2.0532(7.9005) | Total Time 10.00(10.00)\n",
      "Iter 1340 | Time 45.2032(43.3212) | Bit/dim 1.4972(1.5618) | Xent 0.4948(0.5277) | Xent Color 0.2057(0.4842) | Loss 1.6723(1.8148) | Error 0.1622(0.1655) | Error Color 0.0500(0.1400) |Steps 560(552.49) | Grad Norm 1.1472(6.1410) | Total Time 10.00(10.00)\n",
      "Iter 1350 | Time 46.2216(43.5324) | Bit/dim 1.4942(1.5438) | Xent 0.5431(0.5246) | Xent Color 0.1669(0.4042) | Loss 1.6717(1.7760) | Error 0.1589(0.1648) | Error Color 0.0478(0.1141) |Steps 554(553.95) | Grad Norm 1.3376(4.8364) | Total Time 10.00(10.00)\n",
      "Iter 1360 | Time 44.0020(43.5993) | Bit/dim 1.4846(1.5288) | Xent 0.5125(0.5170) | Xent Color 0.1273(0.3348) | Loss 1.6446(1.7417) | Error 0.1689(0.1636) | Error Color 0.0311(0.0922) |Steps 554(554.11) | Grad Norm 1.5867(3.9574) | Total Time 10.00(10.00)\n",
      "Iter 1370 | Time 44.8799(43.9796) | Bit/dim 1.4797(1.5153) | Xent 0.5539(0.5188) | Xent Color 0.1106(0.2792) | Loss 1.6458(1.7148) | Error 0.1733(0.1640) | Error Color 0.0256(0.0755) |Steps 554(554.08) | Grad Norm 1.8842(3.3955) | Total Time 10.00(10.00)\n",
      "Iter 1380 | Time 45.5439(44.0948) | Bit/dim 1.4727(1.5024) | Xent 0.5371(0.5161) | Xent Color 0.1086(0.2349) | Loss 1.6341(1.6902) | Error 0.1633(0.1622) | Error Color 0.0267(0.0619) |Steps 554(554.06) | Grad Norm 1.0839(2.9304) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0021 | Time 123.4595, Epoch Time 3063.7916(2316.3144), Bit/dim 1.4654(best: 1.5219), Xent 0.3425, Xent Color 0.0443. Loss 1.5621, Error 0.0997(best: 0.0973), Error Color 0.0011(best: 0.0008)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1390 | Time 43.8343(44.1657) | Bit/dim 1.4587(1.4915) | Xent 0.5087(0.5183) | Xent Color 0.1075(0.1988) | Loss 1.6127(1.6707) | Error 0.1644(0.1620) | Error Color 0.0222(0.0506) |Steps 554(554.37) | Grad Norm 1.0199(2.5685) | Total Time 10.00(10.00)\n",
      "Iter 1400 | Time 44.5257(44.4496) | Bit/dim 1.4495(1.4812) | Xent 0.4828(0.5118) | Xent Color 0.0856(0.1692) | Loss 1.5917(1.6514) | Error 0.1622(0.1597) | Error Color 0.0156(0.0412) |Steps 566(555.86) | Grad Norm 0.8564(2.1907) | Total Time 10.00(10.00)\n",
      "Iter 1410 | Time 49.2931(45.3087) | Bit/dim 1.4515(1.4734) | Xent 0.4835(0.5124) | Xent Color 0.0917(0.1456) | Loss 1.5953(1.6379) | Error 0.1478(0.1591) | Error Color 0.0200(0.0339) |Steps 560(557.12) | Grad Norm 0.8334(1.8740) | Total Time 10.00(10.00)\n",
      "Iter 1420 | Time 47.0254(46.0507) | Bit/dim 1.4391(1.4643) | Xent 0.5095(0.5094) | Xent Color 0.0695(0.1278) | Loss 1.5838(1.6236) | Error 0.1633(0.1592) | Error Color 0.0100(0.0289) |Steps 566(559.00) | Grad Norm 0.9638(1.7829) | Total Time 10.00(10.00)\n",
      "Iter 1430 | Time 47.9344(46.5049) | Bit/dim 1.4289(1.4571) | Xent 0.5163(0.5064) | Xent Color 0.0688(0.1125) | Loss 1.5752(1.6118) | Error 0.1556(0.1584) | Error Color 0.0111(0.0247) |Steps 578(561.71) | Grad Norm 3.4484(1.8686) | Total Time 10.00(10.00)\n",
      "Iter 1440 | Time 47.7274(46.7761) | Bit/dim 1.4417(1.4497) | Xent 0.5656(0.5093) | Xent Color 0.0655(0.1002) | Loss 1.5995(1.6021) | Error 0.1611(0.1588) | Error Color 0.0144(0.0212) |Steps 578(565.09) | Grad Norm 2.6388(1.9476) | Total Time 10.00(10.00)\n",
      "Iter 1450 | Time 48.9770(47.0498) | Bit/dim 1.4320(1.4418) | Xent 0.5033(0.5051) | Xent Color 0.0574(0.0899) | Loss 1.5722(1.5905) | Error 0.1744(0.1587) | Error Color 0.0111(0.0185) |Steps 578(567.39) | Grad Norm 2.1895(1.8676) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0022 | Time 117.5438, Epoch Time 3251.2183(2344.3616), Bit/dim 1.4225(best: 1.4654), Xent 0.3362, Xent Color 0.0236. Loss 1.5124, Error 0.0987(best: 0.0973), Error Color 0.0005(best: 0.0008)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1460 | Time 47.2737(47.2077) | Bit/dim 1.4071(1.4338) | Xent 0.5050(0.5029) | Xent Color 0.0490(0.0814) | Loss 1.5456(1.5799) | Error 0.1578(0.1587) | Error Color 0.0056(0.0160) |Steps 572(569.22) | Grad Norm 1.2287(1.9541) | Total Time 10.00(10.00)\n",
      "Iter 1470 | Time 47.8926(47.4600) | Bit/dim 1.4189(1.4272) | Xent 0.5160(0.5006) | Xent Color 0.0596(0.0746) | Loss 1.5628(1.5710) | Error 0.1544(0.1570) | Error Color 0.0100(0.0141) |Steps 572(570.75) | Grad Norm 1.0696(1.9048) | Total Time 10.00(10.00)\n",
      "Iter 1480 | Time 48.1911(47.3991) | Bit/dim 1.3993(1.4212) | Xent 0.5121(0.5020) | Xent Color 0.0560(0.0696) | Loss 1.5413(1.5641) | Error 0.1556(0.1570) | Error Color 0.0122(0.0131) |Steps 572(571.08) | Grad Norm 1.9004(2.1349) | Total Time 10.00(10.00)\n",
      "Iter 1490 | Time 46.9933(47.5115) | Bit/dim 1.4016(1.4161) | Xent 0.4847(0.4996) | Xent Color 0.0510(0.0653) | Loss 1.5355(1.5573) | Error 0.1578(0.1569) | Error Color 0.0122(0.0120) |Steps 572(571.73) | Grad Norm 3.2411(2.3168) | Total Time 10.00(10.00)\n",
      "Iter 1500 | Time 45.4266(47.8862) | Bit/dim 1.3726(1.4097) | Xent 0.5375(0.4995) | Xent Color 0.0465(0.0615) | Loss 1.5186(1.5500) | Error 0.1544(0.1566) | Error Color 0.0056(0.0109) |Steps 572(572.57) | Grad Norm 2.0005(2.5105) | Total Time 10.00(10.00)\n",
      "Iter 1510 | Time 49.4277(47.9810) | Bit/dim 1.3909(1.4046) | Xent 0.5787(0.5001) | Xent Color 0.0439(0.0593) | Loss 1.5465(1.5444) | Error 0.1689(0.1568) | Error Color 0.0056(0.0107) |Steps 572(572.70) | Grad Norm 2.4465(2.9468) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0023 | Time 116.2779, Epoch Time 3302.6649(2373.1107), Bit/dim 1.3832(best: 1.4225), Xent 0.3238, Xent Color 0.0191. Loss 1.4689, Error 0.0931(best: 0.0973), Error Color 0.0000(best: 0.0005)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1520 | Time 47.2806(47.7568) | Bit/dim 1.3924(1.3979) | Xent 0.5083(0.4993) | Xent Color 0.0576(0.0575) | Loss 1.5338(1.5371) | Error 0.1678(0.1578) | Error Color 0.0100(0.0104) |Steps 584(574.37) | Grad Norm 2.2376(3.0995) | Total Time 10.00(10.00)\n",
      "Iter 1530 | Time 45.4527(47.6879) | Bit/dim 1.3917(1.3925) | Xent 0.4988(0.4964) | Xent Color 0.0523(0.0545) | Loss 1.5295(1.5302) | Error 0.1644(0.1568) | Error Color 0.0100(0.0098) |Steps 578(574.83) | Grad Norm 4.1773(3.1683) | Total Time 10.00(10.00)\n",
      "Iter 1540 | Time 48.2201(47.6536) | Bit/dim 1.3530(1.3861) | Xent 0.5218(0.4942) | Xent Color 0.0446(0.0527) | Loss 1.4946(1.5229) | Error 0.1422(0.1559) | Error Color 0.0089(0.0097) |Steps 572(575.34) | Grad Norm 4.2020(3.3568) | Total Time 10.00(10.00)\n",
      "Iter 1550 | Time 48.2540(47.5281) | Bit/dim 1.3523(1.3800) | Xent 0.4337(0.4882) | Xent Color 0.0383(0.0499) | Loss 1.4703(1.5145) | Error 0.1422(0.1542) | Error Color 0.0033(0.0090) |Steps 566(575.61) | Grad Norm 3.5506(3.1927) | Total Time 10.00(10.00)\n",
      "Iter 1560 | Time 47.8851(47.5398) | Bit/dim 1.3653(1.3741) | Xent 0.5345(0.4858) | Xent Color 0.0442(0.0482) | Loss 1.5099(1.5076) | Error 0.1622(0.1530) | Error Color 0.0078(0.0089) |Steps 572(576.59) | Grad Norm 2.5735(3.1690) | Total Time 10.00(10.00)\n",
      "Iter 1570 | Time 46.1656(47.5318) | Bit/dim 1.3398(1.3680) | Xent 0.4775(0.4858) | Xent Color 0.0405(0.0463) | Loss 1.4693(1.5010) | Error 0.1322(0.1533) | Error Color 0.0078(0.0084) |Steps 578(575.82) | Grad Norm 2.5665(3.3060) | Total Time 10.00(10.00)\n",
      "Iter 1580 | Time 48.4354(47.5177) | Bit/dim 1.3186(1.3599) | Xent 0.5169(0.4872) | Xent Color 0.0401(0.0463) | Loss 1.4579(1.4933) | Error 0.1667(0.1535) | Error Color 0.0022(0.0087) |Steps 560(574.64) | Grad Norm 3.2644(3.7284) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0024 | Time 121.9232, Epoch Time 3273.8490(2400.1328), Bit/dim 1.3373(best: 1.3832), Xent 0.3254, Xent Color 0.0157. Loss 1.4226, Error 0.0962(best: 0.0931), Error Color 0.0005(best: 0.0000)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1590 | Time 49.0895(47.2372) | Bit/dim 1.3276(1.3533) | Xent 0.4829(0.4854) | Xent Color 0.0408(0.0465) | Loss 1.4585(1.4863) | Error 0.1522(0.1536) | Error Color 0.0067(0.0089) |Steps 572(575.13) | Grad Norm 3.8523(4.1301) | Total Time 10.00(10.00)\n",
      "Iter 1600 | Time 48.4449(47.2530) | Bit/dim 1.3257(1.3461) | Xent 0.4793(0.4863) | Xent Color 0.0329(0.0438) | Loss 1.4537(1.4787) | Error 0.1344(0.1520) | Error Color 0.0056(0.0082) |Steps 572(576.80) | Grad Norm 1.4985(3.7702) | Total Time 10.00(10.00)\n",
      "Iter 1610 | Time 46.6600(47.5931) | Bit/dim 1.3089(1.3383) | Xent 0.4813(0.4835) | Xent Color 0.0364(0.0421) | Loss 1.4383(1.4697) | Error 0.1500(0.1508) | Error Color 0.0033(0.0078) |Steps 584(578.00) | Grad Norm 4.3981(3.6049) | Total Time 10.00(10.00)\n",
      "Iter 1620 | Time 45.3736(47.7206) | Bit/dim 1.3010(1.3284) | Xent 0.4998(0.4816) | Xent Color 0.0343(0.0403) | Loss 1.4345(1.4589) | Error 0.1633(0.1499) | Error Color 0.0056(0.0074) |Steps 584(577.99) | Grad Norm 1.6632(3.2891) | Total Time 10.00(10.00)\n",
      "Iter 1630 | Time 47.4478(47.9144) | Bit/dim 1.2883(1.3208) | Xent 0.4353(0.4768) | Xent Color 0.0351(0.0384) | Loss 1.4058(1.4496) | Error 0.1200(0.1495) | Error Color 0.0056(0.0070) |Steps 566(580.76) | Grad Norm 4.8575(3.0496) | Total Time 10.00(10.00)\n",
      "Iter 1640 | Time 49.4790(48.1535) | Bit/dim 1.2908(1.3100) | Xent 0.4519(0.4756) | Xent Color 0.0356(0.0368) | Loss 1.4127(1.4381) | Error 0.1289(0.1489) | Error Color 0.0044(0.0065) |Steps 590(582.11) | Grad Norm 5.1321(2.9487) | Total Time 10.00(10.00)\n",
      "Iter 1650 | Time 48.5189(48.2531) | Bit/dim 1.2735(1.3006) | Xent 0.5255(0.4721) | Xent Color 0.0302(0.0361) | Loss 1.4124(1.4276) | Error 0.1611(0.1483) | Error Color 0.0044(0.0063) |Steps 590(582.65) | Grad Norm 2.0925(3.1554) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0025 | Time 123.0357, Epoch Time 3320.9519(2427.7574), Bit/dim 1.2741(best: 1.3373), Xent 0.3069, Xent Color 0.0084. Loss 1.3529, Error 0.0913(best: 0.0931), Error Color 0.0001(best: 0.0000)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1660 | Time 48.0899(48.1915) | Bit/dim 1.2473(1.2913) | Xent 0.4653(0.4740) | Xent Color 0.0298(0.0354) | Loss 1.3711(1.4187) | Error 0.1400(0.1492) | Error Color 0.0044(0.0061) |Steps 572(583.06) | Grad Norm 2.8538(3.2946) | Total Time 10.00(10.00)\n",
      "Iter 1670 | Time 41.2418(47.7348) | Bit/dim 2.3383(1.4846) | Xent 0.5024(0.4994) | Xent Color 9.5362(2.5527) | Loss 4.8479(2.2476) | Error 0.1544(0.1567) | Error Color 0.8367(0.1275) |Steps 530(580.96) | Grad Norm 45.0374(26.4609) | Total Time 10.00(10.00)\n",
      "Iter 1680 | Time 42.6671(45.4619) | Bit/dim 2.2066(1.7201) | Xent 1.2777(0.6418) | Xent Color 1.1328(2.5220) | Loss 2.8092(2.5110) | Error 0.4111(0.2031) | Error Color 0.4367(0.2371) |Steps 626(583.06) | Grad Norm 13.0031(24.1394) | Total Time 10.00(10.00)\n",
      "Iter 1690 | Time 35.8216(44.0538) | Bit/dim 2.0939(1.8336) | Xent 0.6784(0.7036) | Xent Color 1.1093(2.1513) | Loss 2.5408(2.5473) | Error 0.2233(0.2224) | Error Color 0.4356(0.2885) |Steps 530(584.79) | Grad Norm 13.7166(21.1921) | Total Time 10.00(10.00)\n",
      "Iter 1700 | Time 40.1597(42.2575) | Bit/dim 1.9668(1.8858) | Xent 0.6730(0.6890) | Xent Color 0.7822(1.8177) | Loss 2.3307(2.5124) | Error 0.2111(0.2166) | Error Color 0.3044(0.3055) |Steps 518(566.92) | Grad Norm 5.0095(17.2595) | Total Time 10.00(10.00)\n",
      "Iter 1710 | Time 37.0818(41.0468) | Bit/dim 1.9070(1.9030) | Xent 0.5464(0.6545) | Xent Color 0.4979(1.5060) | Loss 2.1681(2.4431) | Error 0.1700(0.2048) | Error Color 0.1867(0.2904) |Steps 506(551.21) | Grad Norm 4.1036(13.7584) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0026 | Time 102.9912, Epoch Time 2851.9624(2440.4835), Bit/dim 1.8947(best: 1.2741), Xent 0.4085, Xent Color 0.2948. Loss 2.0705, Error 0.1195(best: 0.0913), Error Color 0.0882(best: 0.0000)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1720 | Time 39.7139(40.7794) | Bit/dim 1.8814(1.9004) | Xent 0.5542(0.6201) | Xent Color 0.3742(1.2301) | Loss 2.1135(2.3629) | Error 0.1811(0.1934) | Error Color 0.1522(0.2609) |Steps 518(541.35) | Grad Norm 2.9962(11.1846) | Total Time 10.00(10.00)\n",
      "Iter 1730 | Time 37.9716(40.3903) | Bit/dim 1.8243(1.8840) | Xent 0.5727(0.5939) | Xent Color 0.2875(0.9962) | Loss 2.0393(2.2816) | Error 0.1811(0.1851) | Error Color 0.0944(0.2236) |Steps 512(534.57) | Grad Norm 2.4151(8.9839) | Total Time 10.00(10.00)\n",
      "Iter 1740 | Time 39.3253(40.0706) | Bit/dim 1.7629(1.8582) | Xent 0.5250(0.5755) | Xent Color 0.2385(0.8038) | Loss 1.9538(2.2030) | Error 0.1567(0.1781) | Error Color 0.0800(0.1872) |Steps 518(529.62) | Grad Norm 2.4054(7.0868) | Total Time 10.00(10.00)\n",
      "Iter 1750 | Time 39.1260(39.8033) | Bit/dim 1.6929(1.8253) | Xent 0.5095(0.5570) | Xent Color 0.2283(0.6540) | Loss 1.8773(2.1281) | Error 0.1467(0.1710) | Error Color 0.0678(0.1569) |Steps 518(526.41) | Grad Norm 2.1457(6.1961) | Total Time 10.00(10.00)\n",
      "Iter 1760 | Time 40.4663(39.5660) | Bit/dim 1.6836(1.7908) | Xent 0.5139(0.5413) | Xent Color 0.1873(0.5387) | Loss 1.8589(2.0608) | Error 0.1478(0.1664) | Error Color 0.0467(0.1325) |Steps 518(524.02) | Grad Norm 5.0982(5.8742) | Total Time 10.00(10.00)\n",
      "Iter 1770 | Time 39.0249(39.4261) | Bit/dim 1.6345(1.7548) | Xent 0.5537(0.5324) | Xent Color 0.2166(0.4518) | Loss 1.8270(2.0009) | Error 0.1767(0.1639) | Error Color 0.0633(0.1147) |Steps 524(523.00) | Grad Norm 6.1846(6.1173) | Total Time 10.00(10.00)\n",
      "Iter 1780 | Time 40.0763(39.3546) | Bit/dim 1.6024(1.7197) | Xent 0.5216(0.5311) | Xent Color 0.1349(0.3789) | Loss 1.7665(1.9472) | Error 0.1644(0.1647) | Error Color 0.0344(0.0965) |Steps 518(522.14) | Grad Norm 1.5490(5.6658) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0027 | Time 101.9061, Epoch Time 2708.5785(2448.5264), Bit/dim 1.6062(best: 1.2741), Xent 0.3677, Xent Color 0.0752. Loss 1.7170, Error 0.1022(best: 0.0913), Error Color 0.0060(best: 0.0000)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1790 | Time 39.7132(39.2255) | Bit/dim 1.5970(1.6868) | Xent 0.4949(0.5231) | Xent Color 0.1231(0.3137) | Loss 1.7515(1.8959) | Error 0.1533(0.1623) | Error Color 0.0300(0.0783) |Steps 524(521.52) | Grad Norm 0.6759(4.8485) | Total Time 10.00(10.00)\n",
      "Iter 1800 | Time 37.6379(39.1808) | Bit/dim 1.5711(1.6557) | Xent 0.5601(0.5187) | Xent Color 0.1346(0.2645) | Loss 1.7447(1.8515) | Error 0.1656(0.1610) | Error Color 0.0356(0.0654) |Steps 524(521.41) | Grad Norm 8.1036(4.5027) | Total Time 10.00(10.00)\n",
      "Iter 1810 | Time 39.2063(39.2221) | Bit/dim 1.5229(1.6273) | Xent 0.5426(0.5148) | Xent Color 0.2313(0.2328) | Loss 1.7164(1.8142) | Error 0.1700(0.1611) | Error Color 0.0900(0.0592) |Steps 518(520.69) | Grad Norm 17.3062(5.2520) | Total Time 10.00(10.00)\n",
      "Iter 1820 | Time 39.9911(39.1902) | Bit/dim 1.5417(1.6072) | Xent 0.5347(0.5198) | Xent Color 0.1429(0.2731) | Loss 1.7111(1.8054) | Error 0.1600(0.1618) | Error Color 0.0400(0.0822) |Steps 530(522.27) | Grad Norm 2.5071(8.3768) | Total Time 10.00(10.00)\n",
      "Iter 1830 | Time 38.3066(39.3816) | Bit/dim 1.5111(1.5838) | Xent 0.5000(0.5242) | Xent Color 0.1915(0.2561) | Loss 1.6839(1.7788) | Error 0.1678(0.1644) | Error Color 0.0733(0.0812) |Steps 524(523.65) | Grad Norm 9.1967(8.6432) | Total Time 10.00(10.00)\n",
      "Iter 1840 | Time 41.6089(39.8497) | Bit/dim 1.4875(1.5609) | Xent 0.4344(0.5165) | Xent Color 0.1107(0.2227) | Loss 1.6238(1.7457) | Error 0.1389(0.1629) | Error Color 0.0289(0.0686) |Steps 542(526.07) | Grad Norm 4.8153(7.5715) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0028 | Time 109.8998, Epoch Time 2764.3818(2458.0021), Bit/dim 1.4746(best: 1.2741), Xent 0.3524, Xent Color 0.0477. Loss 1.5746, Error 0.1046(best: 0.0913), Error Color 0.0019(best: 0.0000)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1850 | Time 43.4701(40.3007) | Bit/dim 1.4651(1.5392) | Xent 0.5487(0.5151) | Xent Color 0.1003(0.1912) | Loss 1.6273(1.7158) | Error 0.1744(0.1624) | Error Color 0.0189(0.0563) |Steps 536(530.07) | Grad Norm 2.5116(6.3585) | Total Time 10.00(10.00)\n",
      "Iter 1860 | Time 40.1462(40.6588) | Bit/dim 1.4398(1.5171) | Xent 0.4587(0.5075) | Xent Color 0.0791(0.1648) | Loss 1.5743(1.6851) | Error 0.1300(0.1593) | Error Color 0.0133(0.0462) |Steps 536(532.54) | Grad Norm 2.1246(5.1521) | Total Time 10.00(10.00)\n",
      "Iter 1870 | Time 42.8285(41.2604) | Bit/dim 1.4402(1.4963) | Xent 0.5396(0.5061) | Xent Color 0.0728(0.1427) | Loss 1.5933(1.6585) | Error 0.1756(0.1581) | Error Color 0.0133(0.0381) |Steps 548(534.13) | Grad Norm 0.9733(4.1286) | Total Time 10.00(10.00)\n",
      "Iter 1880 | Time 43.8005(41.8500) | Bit/dim 1.4212(1.4760) | Xent 0.5065(0.5030) | Xent Color 0.0700(0.1245) | Loss 1.5654(1.6328) | Error 0.1511(0.1582) | Error Color 0.0100(0.0317) |Steps 548(536.53) | Grad Norm 2.6656(3.5636) | Total Time 10.00(10.00)\n",
      "Iter 1890 | Time 43.6971(42.2697) | Bit/dim 1.3935(1.4545) | Xent 0.5053(0.5069) | Xent Color 0.0642(0.1100) | Loss 1.5359(1.6087) | Error 0.1533(0.1587) | Error Color 0.0100(0.0266) |Steps 542(538.11) | Grad Norm 0.9556(3.1719) | Total Time 10.00(10.00)\n",
      "Iter 1900 | Time 45.4385(42.7146) | Bit/dim 1.3694(1.4338) | Xent 0.5012(0.5027) | Xent Color 0.0665(0.0995) | Loss 1.5114(1.5844) | Error 0.1622(0.1580) | Error Color 0.0156(0.0233) |Steps 536(538.81) | Grad Norm 3.2818(3.0058) | Total Time 10.00(10.00)\n",
      "Iter 1910 | Time 43.0511(43.0041) | Bit/dim 1.3482(1.4141) | Xent 0.4075(0.4945) | Xent Color 0.0752(0.0943) | Loss 1.4688(1.5613) | Error 0.1478(0.1572) | Error Color 0.0178(0.0225) |Steps 542(538.72) | Grad Norm 4.7010(3.8865) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0029 | Time 103.3290, Epoch Time 2974.4787(2473.4964), Bit/dim 1.3470(best: 1.2741), Xent 0.3327, Xent Color 0.0240. Loss 1.4362, Error 0.0974(best: 0.0913), Error Color 0.0010(best: 0.0000)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1920 | Time 42.9467(42.9956) | Bit/dim 1.3295(1.3948) | Xent 0.4661(0.4969) | Xent Color 0.0620(0.0866) | Loss 1.4615(1.5406) | Error 0.1711(0.1584) | Error Color 0.0122(0.0198) |Steps 536(537.83) | Grad Norm 2.0081(3.9046) | Total Time 10.00(10.00)\n",
      "Iter 1930 | Time 42.6014(42.9734) | Bit/dim 1.3165(1.3773) | Xent 0.5094(0.4963) | Xent Color 0.0687(0.0789) | Loss 1.4610(1.5211) | Error 0.1533(0.1576) | Error Color 0.0144(0.0175) |Steps 530(536.24) | Grad Norm 6.2529(3.5875) | Total Time 10.00(10.00)\n",
      "Iter 1940 | Time 41.9518(43.2133) | Bit/dim 1.3656(1.3624) | Xent 0.5337(0.4918) | Xent Color 1.3194(0.1185) | Loss 1.8289(1.5149) | Error 0.1556(0.1554) | Error Color 0.3744(0.0297) |Steps 530(536.62) | Grad Norm 86.5760(7.3625) | Total Time 10.00(10.00)\n",
      "Iter 1950 | Time 36.9948(42.4550) | Bit/dim 1.7945(1.4864) | Xent 0.7250(0.5465) | Xent Color 1.0742(0.9906) | Loss 2.2443(1.8707) | Error 0.2233(0.1744) | Error Color 0.4178(0.1733) |Steps 512(534.78) | Grad Norm 7.8547(16.4111) | Total Time 10.00(10.00)\n",
      "Iter 1960 | Time 42.5459(41.6605) | Bit/dim 1.6443(1.5479) | Xent 0.6468(0.5855) | Xent Color 0.6637(0.9576) | Loss 1.9720(1.9336) | Error 0.2056(0.1884) | Error Color 0.2822(0.2164) |Steps 548(530.77) | Grad Norm 5.8578(13.9433) | Total Time 10.00(10.00)\n",
      "Iter 1970 | Time 42.6090(41.6434) | Bit/dim 1.5189(1.5535) | Xent 0.5809(0.5845) | Xent Color 0.4108(0.8365) | Loss 1.7669(1.9087) | Error 0.1878(0.1879) | Error Color 0.1678(0.2122) |Steps 542(532.17) | Grad Norm 3.1041(11.3480) | Total Time 10.00(10.00)\n",
      "Iter 1980 | Time 44.3753(41.8454) | Bit/dim 1.4498(1.5322) | Xent 0.5653(0.5817) | Xent Color 0.3012(0.7108) | Loss 1.6664(1.8553) | Error 0.1622(0.1854) | Error Color 0.0956(0.1916) |Steps 536(533.50) | Grad Norm 2.4600(9.0890) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0030 | Time 113.8899, Epoch Time 2901.0479(2486.3229), Bit/dim 1.4571(best: 1.2741), Xent 0.3831, Xent Color 0.1732. Loss 1.5962, Error 0.1127(best: 0.0913), Error Color 0.0367(best: 0.0000)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1990 | Time 43.4287(42.5087) | Bit/dim 1.4148(1.5069) | Xent 0.5731(0.5692) | Xent Color 0.2397(0.5908) | Loss 1.6180(1.7969) | Error 0.1800(0.1798) | Error Color 0.0800(0.1647) |Steps 530(535.36) | Grad Norm 1.8689(7.2245) | Total Time 10.00(10.00)\n",
      "Iter 2000 | Time 44.6493(42.9442) | Bit/dim 1.3748(1.4765) | Xent 0.5014(0.5526) | Xent Color 0.1887(0.4883) | Loss 1.5473(1.7368) | Error 0.1556(0.1749) | Error Color 0.0600(0.1387) |Steps 530(535.66) | Grad Norm 1.1089(5.7072) | Total Time 10.00(10.00)\n",
      "Iter 2010 | Time 43.2976(42.9892) | Bit/dim 1.3410(1.4450) | Xent 0.5346(0.5450) | Xent Color 0.1670(0.4056) | Loss 1.5164(1.6827) | Error 0.1733(0.1718) | Error Color 0.0511(0.1160) |Steps 536(535.30) | Grad Norm 0.9501(4.5193) | Total Time 10.00(10.00)\n",
      "Iter 2020 | Time 43.7870(43.1052) | Bit/dim 1.3374(1.4176) | Xent 0.5249(0.5348) | Xent Color 0.1333(0.3378) | Loss 1.5020(1.6357) | Error 0.1778(0.1691) | Error Color 0.0300(0.0964) |Steps 536(535.79) | Grad Norm 1.1347(3.6222) | Total Time 10.00(10.00)\n",
      "Iter 2030 | Time 43.2427(43.0324) | Bit/dim 1.3047(1.3913) | Xent 0.5161(0.5277) | Xent Color 0.1279(0.2843) | Loss 1.4657(1.5943) | Error 0.1544(0.1664) | Error Color 0.0344(0.0802) |Steps 548(537.01) | Grad Norm 0.7268(2.9092) | Total Time 10.00(10.00)\n",
      "Iter 2040 | Time 42.1792(43.2215) | Bit/dim 1.3111(1.3695) | Xent 0.4668(0.5245) | Xent Color 0.1147(0.2393) | Loss 1.4565(1.5605) | Error 0.1300(0.1631) | Error Color 0.0333(0.0664) |Steps 542(539.40) | Grad Norm 0.7783(2.3805) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0031 | Time 105.8760, Epoch Time 3009.1876(2502.0088), Bit/dim 1.2938(best: 1.2741), Xent 0.3424, Xent Color 0.0443. Loss 1.3905, Error 0.0989(best: 0.0913), Error Color 0.0018(best: 0.0000)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2050 | Time 44.0771(43.4468) | Bit/dim 1.2886(1.3490) | Xent 0.5075(0.5179) | Xent Color 0.1053(0.2041) | Loss 1.4419(1.5295) | Error 0.1500(0.1604) | Error Color 0.0211(0.0555) |Steps 536(540.38) | Grad Norm 0.6939(2.0114) | Total Time 10.00(10.00)\n",
      "Iter 2060 | Time 43.1881(43.3948) | Bit/dim 1.2820(1.3318) | Xent 0.4697(0.5107) | Xent Color 0.0877(0.1763) | Loss 1.4213(1.5036) | Error 0.1556(0.1594) | Error Color 0.0167(0.0469) |Steps 548(541.44) | Grad Norm 0.8300(1.7324) | Total Time 10.00(10.00)\n",
      "Iter 2070 | Time 44.3733(43.4988) | Bit/dim 1.2808(1.3175) | Xent 0.4519(0.5088) | Xent Color 0.0803(0.1536) | Loss 1.4138(1.4831) | Error 0.1489(0.1596) | Error Color 0.0156(0.0400) |Steps 542(542.01) | Grad Norm 0.6573(1.4847) | Total Time 10.00(10.00)\n",
      "Iter 2080 | Time 44.0846(43.4243) | Bit/dim 1.2708(1.3034) | Xent 0.5467(0.4994) | Xent Color 0.0843(0.1349) | Loss 1.4285(1.4620) | Error 0.1689(0.1576) | Error Color 0.0189(0.0342) |Steps 542(542.01) | Grad Norm 0.7848(1.3009) | Total Time 10.00(10.00)\n",
      "Iter 2090 | Time 43.7802(43.5140) | Bit/dim 1.2565(1.2913) | Xent 0.4797(0.4932) | Xent Color 0.0636(0.1197) | Loss 1.3924(1.4445) | Error 0.1367(0.1555) | Error Color 0.0089(0.0294) |Steps 542(542.03) | Grad Norm 0.9227(1.2059) | Total Time 10.00(10.00)\n",
      "Iter 2100 | Time 42.0033(43.5435) | Bit/dim 1.2526(1.2811) | Xent 0.4552(0.4910) | Xent Color 0.0717(0.1079) | Loss 1.3843(1.4308) | Error 0.1522(0.1545) | Error Color 0.0178(0.0263) |Steps 536(541.53) | Grad Norm 0.6566(1.0990) | Total Time 10.00(10.00)\n",
      "Iter 2110 | Time 42.5406(43.4842) | Bit/dim 1.2309(1.2717) | Xent 0.5109(0.4938) | Xent Color 0.0691(0.0982) | Loss 1.3759(1.4197) | Error 0.1589(0.1547) | Error Color 0.0144(0.0235) |Steps 542(541.01) | Grad Norm 1.3646(1.0621) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0032 | Time 103.8270, Epoch Time 2999.4257(2516.9313), Bit/dim 1.2415(best: 1.2741), Xent 0.3215, Xent Color 0.0231. Loss 1.3276, Error 0.0963(best: 0.0913), Error Color 0.0008(best: 0.0000)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2120 | Time 41.5856(43.4021) | Bit/dim 1.2200(1.2625) | Xent 0.4287(0.4855) | Xent Color 0.0619(0.0891) | Loss 1.3427(1.4061) | Error 0.1333(0.1534) | Error Color 0.0122(0.0205) |Steps 536(540.33) | Grad Norm 0.8730(1.0536) | Total Time 10.00(10.00)\n",
      "Iter 2130 | Time 45.2063(43.3991) | Bit/dim 1.2335(1.2541) | Xent 0.4470(0.4829) | Xent Color 0.0672(0.0824) | Loss 1.3621(1.3954) | Error 0.1333(0.1518) | Error Color 0.0156(0.0185) |Steps 542(540.46) | Grad Norm 1.0960(1.0448) | Total Time 10.00(10.00)\n",
      "Iter 2140 | Time 42.1454(43.3037) | Bit/dim 1.2204(1.2465) | Xent 0.5077(0.4828) | Xent Color 0.0594(0.0757) | Loss 1.3621(1.3861) | Error 0.1478(0.1508) | Error Color 0.0122(0.0164) |Steps 542(540.38) | Grad Norm 0.8837(0.9896) | Total Time 10.00(10.00)\n",
      "Iter 2150 | Time 42.5519(43.3150) | Bit/dim 1.2121(1.2401) | Xent 0.4396(0.4780) | Xent Color 0.0579(0.0711) | Loss 1.3364(1.3774) | Error 0.1478(0.1507) | Error Color 0.0133(0.0155) |Steps 536(540.17) | Grad Norm 0.9995(0.9431) | Total Time 10.00(10.00)\n",
      "Iter 2160 | Time 43.7140(43.3146) | Bit/dim 1.2171(1.2333) | Xent 0.5529(0.4824) | Xent Color 0.0478(0.0668) | Loss 1.3672(1.3706) | Error 0.1656(0.1513) | Error Color 0.0100(0.0139) |Steps 536(539.55) | Grad Norm 0.6214(0.9530) | Total Time 10.00(10.00)\n",
      "Iter 2170 | Time 43.5541(43.3919) | Bit/dim 1.2199(1.2280) | Xent 0.4126(0.4783) | Xent Color 0.0502(0.0631) | Loss 1.3356(1.3634) | Error 0.1344(0.1508) | Error Color 0.0089(0.0130) |Steps 542(539.93) | Grad Norm 0.7924(1.0130) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0033 | Time 105.9691, Epoch Time 2993.3474(2531.2238), Bit/dim 1.2034(best: 1.2415), Xent 0.3157, Xent Color 0.0148. Loss 1.2860, Error 0.0963(best: 0.0913), Error Color 0.0002(best: 0.0000)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2180 | Time 44.0042(43.4830) | Bit/dim 1.2031(1.2220) | Xent 0.5036(0.4756) | Xent Color 0.0493(0.0608) | Loss 1.3413(1.3561) | Error 0.1511(0.1494) | Error Color 0.0122(0.0124) |Steps 536(539.79) | Grad Norm 1.4425(1.1043) | Total Time 10.00(10.00)\n",
      "Iter 2190 | Time 44.3518(43.5686) | Bit/dim 1.2009(1.2167) | Xent 0.4655(0.4733) | Xent Color 0.0463(0.0580) | Loss 1.3288(1.3495) | Error 0.1667(0.1500) | Error Color 0.0122(0.0121) |Steps 542(540.08) | Grad Norm 1.3134(1.1745) | Total Time 10.00(10.00)\n",
      "Iter 2200 | Time 47.5260(43.8582) | Bit/dim 1.1925(1.2106) | Xent 0.4652(0.4659) | Xent Color 0.0435(0.0549) | Loss 1.3197(1.3408) | Error 0.1333(0.1481) | Error Color 0.0078(0.0111) |Steps 542(541.01) | Grad Norm 1.4213(1.1958) | Total Time 10.00(10.00)\n",
      "Iter 2210 | Time 44.1310(43.9275) | Bit/dim 1.1769(1.2039) | Xent 0.4930(0.4684) | Xent Color 0.0371(0.0522) | Loss 1.3094(1.3340) | Error 0.1544(0.1485) | Error Color 0.0022(0.0100) |Steps 536(539.98) | Grad Norm 0.9706(1.2818) | Total Time 10.00(10.00)\n",
      "Iter 2220 | Time 44.1985(43.7197) | Bit/dim 1.1917(1.1985) | Xent 0.4837(0.4686) | Xent Color 0.0437(0.0502) | Loss 1.3236(1.3282) | Error 0.1567(0.1482) | Error Color 0.0100(0.0098) |Steps 536(539.81) | Grad Norm 1.4158(1.3344) | Total Time 10.00(10.00)\n",
      "Iter 2230 | Time 44.0373(43.6777) | Bit/dim 1.1817(1.1934) | Xent 0.4273(0.4596) | Xent Color 0.0440(0.0479) | Loss 1.2996(1.3203) | Error 0.1433(0.1469) | Error Color 0.0056(0.0089) |Steps 542(539.15) | Grad Norm 0.8452(1.3214) | Total Time 10.00(10.00)\n",
      "Iter 2240 | Time 43.2020(43.8392) | Bit/dim 1.1699(1.1887) | Xent 0.4566(0.4588) | Xent Color 0.0380(0.0463) | Loss 1.2936(1.3149) | Error 0.1467(0.1461) | Error Color 0.0056(0.0087) |Steps 536(538.17) | Grad Norm 1.3555(1.2903) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0034 | Time 107.6308, Epoch Time 3030.0771(2546.1894), Bit/dim 1.1695(best: 1.2034), Xent 0.2971, Xent Color 0.0122. Loss 1.2468, Error 0.0893(best: 0.0913), Error Color 0.0001(best: 0.0000)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2250 | Time 44.1939(43.7854) | Bit/dim 1.1667(1.1840) | Xent 0.3956(0.4524) | Xent Color 0.0414(0.0456) | Loss 1.2759(1.3085) | Error 0.1322(0.1444) | Error Color 0.0089(0.0088) |Steps 530(537.83) | Grad Norm 0.8025(1.2849) | Total Time 10.00(10.00)\n",
      "Iter 2260 | Time 43.0278(43.6596) | Bit/dim 1.1741(1.1791) | Xent 0.4542(0.4531) | Xent Color 0.0432(0.0446) | Loss 1.2985(1.3036) | Error 0.1367(0.1440) | Error Color 0.0078(0.0086) |Steps 530(536.39) | Grad Norm 2.0201(1.3114) | Total Time 10.00(10.00)\n",
      "Iter 2270 | Time 42.4917(43.7262) | Bit/dim 1.1687(1.1741) | Xent 0.4518(0.4562) | Xent Color 0.0321(0.0429) | Loss 1.2897(1.2989) | Error 0.1411(0.1449) | Error Color 0.0056(0.0079) |Steps 536(536.28) | Grad Norm 0.8762(1.2814) | Total Time 10.00(10.00)\n",
      "Iter 2280 | Time 44.3155(43.7897) | Bit/dim 1.1602(1.1699) | Xent 0.4527(0.4506) | Xent Color 0.0376(0.0411) | Loss 1.2827(1.2928) | Error 0.1444(0.1431) | Error Color 0.0067(0.0072) |Steps 530(537.73) | Grad Norm 0.8017(1.3178) | Total Time 10.00(10.00)\n",
      "Iter 2290 | Time 43.6884(43.7854) | Bit/dim 1.1417(1.1656) | Xent 0.4568(0.4466) | Xent Color 0.0375(0.0396) | Loss 1.2653(1.2871) | Error 0.1578(0.1422) | Error Color 0.0078(0.0071) |Steps 536(536.17) | Grad Norm 1.3566(1.3606) | Total Time 10.00(10.00)\n",
      "Iter 2300 | Time 43.5433(43.6577) | Bit/dim 1.1563(1.1613) | Xent 0.4350(0.4370) | Xent Color 0.0401(0.0398) | Loss 1.2751(1.2805) | Error 0.1367(0.1401) | Error Color 0.0089(0.0075) |Steps 536(535.71) | Grad Norm 1.4079(1.4540) | Total Time 10.00(10.00)\n",
      "Iter 2310 | Time 41.0263(43.5589) | Bit/dim 1.1412(1.1574) | Xent 0.4671(0.4395) | Xent Color 0.0428(0.0390) | Loss 1.2687(1.2770) | Error 0.1344(0.1408) | Error Color 0.0111(0.0074) |Steps 530(534.66) | Grad Norm 2.7285(1.5774) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0035 | Time 107.3675, Epoch Time 3006.5884(2560.0014), Bit/dim 1.1420(best: 1.1695), Xent 0.2848, Xent Color 0.0098. Loss 1.2157, Error 0.0816(best: 0.0893), Error Color 0.0003(best: 0.0000)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2320 | Time 43.5721(43.5214) | Bit/dim 1.1405(1.1529) | Xent 0.3912(0.4363) | Xent Color 0.0346(0.0384) | Loss 1.2469(1.2716) | Error 0.1233(0.1386) | Error Color 0.0089(0.0074) |Steps 530(534.47) | Grad Norm 2.4739(1.7999) | Total Time 10.00(10.00)\n",
      "Iter 2330 | Time 45.4091(43.4407) | Bit/dim 1.1508(1.1483) | Xent 0.4913(0.4342) | Xent Color 0.0324(0.0380) | Loss 1.2817(1.2664) | Error 0.1600(0.1383) | Error Color 0.0056(0.0076) |Steps 536(534.13) | Grad Norm 2.7934(1.9208) | Total Time 10.00(10.00)\n",
      "Iter 2340 | Time 41.6718(43.2361) | Bit/dim 1.1299(1.1454) | Xent 0.4110(0.4288) | Xent Color 0.0352(0.0375) | Loss 1.2415(1.2620) | Error 0.1289(0.1369) | Error Color 0.0100(0.0076) |Steps 536(534.17) | Grad Norm 1.8955(2.1566) | Total Time 10.00(10.00)\n",
      "Iter 2350 | Time 43.1484(43.2152) | Bit/dim 1.1194(1.1412) | Xent 0.4393(0.4243) | Xent Color 0.0332(0.0367) | Loss 1.2376(1.2565) | Error 0.1422(0.1354) | Error Color 0.0056(0.0071) |Steps 536(533.58) | Grad Norm 2.2746(2.2444) | Total Time 10.00(10.00)\n",
      "Iter 2360 | Time 45.5716(43.2429) | Bit/dim 1.1189(1.1377) | Xent 0.3755(0.4223) | Xent Color 0.0317(0.0367) | Loss 1.2207(1.2525) | Error 0.1089(0.1345) | Error Color 0.0033(0.0067) |Steps 524(533.05) | Grad Norm 1.8852(2.3704) | Total Time 10.00(10.00)\n",
      "Iter 2370 | Time 42.7195(43.1989) | Bit/dim 1.1096(1.1324) | Xent 0.4356(0.4170) | Xent Color 0.0309(0.0360) | Loss 1.2262(1.2457) | Error 0.1400(0.1326) | Error Color 0.0056(0.0067) |Steps 524(531.54) | Grad Norm 1.4537(2.2982) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0036 | Time 117.6733, Epoch Time 2987.8835(2572.8379), Bit/dim 1.1202(best: 1.1420), Xent 0.2654, Xent Color 0.0094. Loss 1.1889, Error 0.0786(best: 0.0816), Error Color 0.0000(best: 0.0000)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2380 | Time 43.5925(43.0899) | Bit/dim 1.1229(1.1287) | Xent 0.4270(0.4152) | Xent Color 0.0441(0.0354) | Loss 1.2407(1.2414) | Error 0.1278(0.1305) | Error Color 0.0111(0.0066) |Steps 530(529.85) | Grad Norm 6.9428(2.7206) | Total Time 10.00(10.00)\n",
      "Iter 2390 | Time 42.3678(42.8636) | Bit/dim 1.1226(1.1252) | Xent 0.3847(0.4109) | Xent Color 0.0457(0.0369) | Loss 1.2302(1.2371) | Error 0.1233(0.1293) | Error Color 0.0144(0.0072) |Steps 518(528.60) | Grad Norm 8.2774(3.5813) | Total Time 10.00(10.00)\n",
      "Iter 2400 | Time 41.6116(42.7578) | Bit/dim 1.1157(1.1229) | Xent 0.4665(0.4091) | Xent Color 0.0369(0.0372) | Loss 1.2416(1.2345) | Error 0.1533(0.1307) | Error Color 0.0067(0.0073) |Steps 524(526.93) | Grad Norm 7.2086(4.1553) | Total Time 10.00(10.00)\n",
      "Iter 2410 | Time 41.9016(42.7011) | Bit/dim 1.1090(1.1207) | Xent 0.3708(0.3979) | Xent Color 0.0753(0.0413) | Loss 1.2205(1.2305) | Error 0.1078(0.1269) | Error Color 0.0256(0.0090) |Steps 524(525.95) | Grad Norm 12.6997(5.1767) | Total Time 10.00(10.00)\n",
      "Iter 2420 | Time 41.0006(42.8005) | Bit/dim 1.1159(1.1177) | Xent 0.3548(0.3968) | Xent Color 0.0534(0.0415) | Loss 1.2180(1.2273) | Error 0.1133(0.1259) | Error Color 0.0144(0.0091) |Steps 524(526.07) | Grad Norm 8.7879(5.5306) | Total Time 10.00(10.00)\n",
      "Iter 2430 | Time 42.1132(42.6919) | Bit/dim 1.1040(1.1158) | Xent 0.3589(0.3930) | Xent Color 0.0472(0.0437) | Loss 1.2055(1.2249) | Error 0.1211(0.1251) | Error Color 0.0144(0.0104) |Steps 524(525.40) | Grad Norm 5.3563(6.1028) | Total Time 10.00(10.00)\n",
      "Iter 2440 | Time 43.4917(42.6974) | Bit/dim 1.0997(1.1116) | Xent 0.3619(0.3857) | Xent Color 0.0393(0.0420) | Loss 1.2000(1.2185) | Error 0.1111(0.1229) | Error Color 0.0100(0.0096) |Steps 524(524.71) | Grad Norm 6.8504(5.8385) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0037 | Time 117.2972, Epoch Time 2952.4435(2584.2260), Bit/dim 1.0974(best: 1.1202), Xent 0.2352, Xent Color 0.0072. Loss 1.1580, Error 0.0721(best: 0.0786), Error Color 0.0000(best: 0.0000)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2450 | Time 41.6347(42.7167) | Bit/dim 1.0980(1.1090) | Xent 0.3580(0.3808) | Xent Color 0.0331(0.0400) | Loss 1.1957(1.2142) | Error 0.1211(0.1215) | Error Color 0.0056(0.0089) |Steps 530(524.56) | Grad Norm 5.7876(5.5922) | Total Time 10.00(10.00)\n",
      "Iter 2460 | Time 41.0982(42.7732) | Bit/dim 1.0740(1.1044) | Xent 0.3741(0.3744) | Xent Color 0.0254(0.0376) | Loss 1.1739(1.2074) | Error 0.1211(0.1198) | Error Color 0.0022(0.0080) |Steps 524(524.88) | Grad Norm 3.0507(5.3272) | Total Time 10.00(10.00)\n",
      "Iter 2470 | Time 40.2650(42.5720) | Bit/dim 1.0906(1.0997) | Xent 0.3694(0.3691) | Xent Color 0.0282(0.0358) | Loss 1.1900(1.2009) | Error 0.1322(0.1195) | Error Color 0.0011(0.0070) |Steps 530(525.30) | Grad Norm 2.8580(4.6498) | Total Time 10.00(10.00)\n",
      "Iter 2480 | Time 44.7283(42.5832) | Bit/dim 1.0867(1.0961) | Xent 0.3795(0.3703) | Xent Color 0.0371(0.0342) | Loss 1.1909(1.1972) | Error 0.1244(0.1203) | Error Color 0.0067(0.0067) |Steps 536(526.07) | Grad Norm 6.8955(4.3056) | Total Time 10.00(10.00)\n",
      "Iter 2490 | Time 42.5951(42.4788) | Bit/dim 1.0773(1.0933) | Xent 0.3432(0.3666) | Xent Color 0.0264(0.0341) | Loss 1.1697(1.1935) | Error 0.1033(0.1182) | Error Color 0.0044(0.0065) |Steps 524(526.15) | Grad Norm 3.3024(4.5534) | Total Time 10.00(10.00)\n",
      "Iter 2500 | Time 41.8693(42.7296) | Bit/dim 1.0805(1.0901) | Xent 0.3079(0.3568) | Xent Color 0.0372(0.0336) | Loss 1.1668(1.1878) | Error 0.0956(0.1148) | Error Color 0.0067(0.0064) |Steps 524(527.43) | Grad Norm 6.7912(4.7820) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0038 | Time 126.2694, Epoch Time 2969.8283(2595.7941), Bit/dim 1.0746(best: 1.0974), Xent 0.2024, Xent Color 0.0079. Loss 1.1272, Error 0.0628(best: 0.0721), Error Color 0.0001(best: 0.0000)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2510 | Time 47.8893(43.0589) | Bit/dim 1.0809(1.0855) | Xent 0.3758(0.3470) | Xent Color 0.0400(0.0329) | Loss 1.1848(1.1804) | Error 0.1200(0.1114) | Error Color 0.0056(0.0061) |Steps 536(528.85) | Grad Norm 7.4604(4.7694) | Total Time 10.00(10.00)\n",
      "Iter 2520 | Time 45.0632(43.4566) | Bit/dim 1.0764(1.0815) | Xent 0.2648(0.3370) | Xent Color 0.0295(0.0328) | Loss 1.1500(1.1739) | Error 0.0856(0.1084) | Error Color 0.0067(0.0063) |Steps 530(530.88) | Grad Norm 1.5454(4.8682) | Total Time 10.00(10.00)\n",
      "Iter 2530 | Time 46.2951(44.1772) | Bit/dim 1.0660(1.0779) | Xent 0.3355(0.3340) | Xent Color 0.0227(0.0314) | Loss 1.1555(1.1693) | Error 0.1067(0.1074) | Error Color 0.0011(0.0060) |Steps 542(533.13) | Grad Norm 1.2178(4.4710) | Total Time 10.00(10.00)\n",
      "Iter 2540 | Time 46.1086(44.6046) | Bit/dim 1.0651(1.0760) | Xent 0.3081(0.3317) | Xent Color 0.0311(0.0310) | Loss 1.1499(1.1666) | Error 0.1078(0.1066) | Error Color 0.0067(0.0062) |Steps 548(534.55) | Grad Norm 4.0822(4.4237) | Total Time 10.00(10.00)\n",
      "Iter 2550 | Time 46.5974(44.9966) | Bit/dim 2.1302(1.1078) | Xent 0.7122(0.3368) | Xent Color 25.7277(0.8496) | Loss 8.7401(1.4044) | Error 0.2178(0.1085) | Error Color 0.8133(0.0441) |Steps 554(536.80) | Grad Norm 323.7693(16.4317) | Total Time 10.00(10.00)\n",
      "Iter 2560 | Time 43.1271(44.1657) | Bit/dim 2.2020(1.4485) | Xent 1.3404(0.6174) | Xent Color 1.8472(2.6838) | Loss 2.9989(2.2738) | Error 0.3878(0.1798) | Error Color 0.5144(0.2067) |Steps 644(555.23) | Grad Norm 11.3342(25.5532) | Total Time 10.00(10.00)\n",
      "Iter 2570 | Time 38.8019(43.2605) | Bit/dim 2.1070(1.6282) | Xent 0.7197(0.7527) | Xent Color 0.8409(2.2580) | Loss 2.4971(2.3809) | Error 0.2189(0.2210) | Error Color 0.3222(0.2506) |Steps 536(564.76) | Grad Norm 5.1528(22.2830) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0039 | Time 115.0437, Epoch Time 3033.8182(2608.9348), Bit/dim 2.0495(best: 1.0746), Xent 0.4953, Xent Color 0.3776. Loss 2.2677, Error 0.1535(best: 0.0628), Error Color 0.1380(best: 0.0000)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2580 | Time 38.9485(42.3022) | Bit/dim 1.9682(1.7354) | Xent 0.5780(0.7194) | Xent Color 0.5064(1.8318) | Loss 2.2393(2.3732) | Error 0.1844(0.2148) | Error Color 0.1989(0.2521) |Steps 542(558.64) | Grad Norm 3.1695(17.8322) | Total Time 10.00(10.00)\n",
      "Iter 2590 | Time 42.8541(42.0590) | Bit/dim 1.9264(1.7914) | Xent 0.5177(0.6768) | Xent Color 0.4003(1.4653) | Loss 2.1559(2.3270) | Error 0.1733(0.2053) | Error Color 0.1689(0.2309) |Steps 536(554.80) | Grad Norm 2.7329(13.8669) | Total Time 10.00(10.00)\n",
      "Iter 2600 | Time 44.1593(41.9311) | Bit/dim 1.8710(1.8195) | Xent 0.5771(0.6312) | Xent Color 0.3165(1.1698) | Loss 2.0944(2.2698) | Error 0.1700(0.1929) | Error Color 0.1167(0.2035) |Steps 536(551.35) | Grad Norm 1.5683(10.6841) | Total Time 10.00(10.00)\n",
      "Iter 2610 | Time 44.0477(42.4026) | Bit/dim 1.8380(1.8269) | Xent 0.4631(0.5881) | Xent Color 0.2301(0.9337) | Loss 2.0113(2.2073) | Error 0.1456(0.1810) | Error Color 0.0611(0.1732) |Steps 554(550.66) | Grad Norm 0.8498(8.1757) | Total Time 10.00(10.00)\n",
      "Iter 2620 | Time 41.2964(42.3861) | Bit/dim 1.7780(1.8218) | Xent 0.4159(0.5539) | Xent Color 0.2075(0.7484) | Loss 1.9338(2.1473) | Error 0.1367(0.1716) | Error Color 0.0633(0.1457) |Steps 548(550.55) | Grad Norm 1.5417(6.3782) | Total Time 10.00(10.00)\n",
      "Iter 2630 | Time 41.1224(42.2437) | Bit/dim 1.7777(1.8079) | Xent 0.3992(0.5206) | Xent Color 0.1921(0.6084) | Loss 1.9255(2.0901) | Error 0.1300(0.1616) | Error Color 0.0600(0.1251) |Steps 542(548.74) | Grad Norm 1.4852(5.1473) | Total Time 10.00(10.00)\n",
      "Iter 2640 | Time 44.3155(42.3623) | Bit/dim 1.7236(1.7873) | Xent 0.3983(0.4926) | Xent Color 0.1676(0.4946) | Loss 1.8651(2.0341) | Error 0.1156(0.1534) | Error Color 0.0456(0.1053) |Steps 554(549.36) | Grad Norm 1.2118(4.1418) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0040 | Time 108.6407, Epoch Time 2912.7250(2618.0485), Bit/dim 1.7190(best: 1.0746), Xent 0.2978, Xent Color 0.0804. Loss 1.8135, Error 0.0907(best: 0.0628), Error Color 0.0070(best: 0.0000)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2650 | Time 43.3319(42.4884) | Bit/dim 1.6781(1.7620) | Xent 0.3963(0.4713) | Xent Color 0.1314(0.4047) | Loss 1.8100(1.9810) | Error 0.1356(0.1476) | Error Color 0.0211(0.0879) |Steps 554(550.15) | Grad Norm 0.9556(3.3789) | Total Time 10.00(10.00)\n",
      "Iter 2660 | Time 41.3931(42.4612) | Bit/dim 1.6554(1.7346) | Xent 0.3986(0.4502) | Xent Color 0.1277(0.3348) | Loss 1.7870(1.9308) | Error 0.1256(0.1420) | Error Color 0.0289(0.0741) |Steps 554(551.16) | Grad Norm 1.7441(2.9224) | Total Time 10.00(10.00)\n",
      "Iter 2670 | Time 43.0777(42.4829) | Bit/dim 1.5973(1.7004) | Xent 0.3675(0.4332) | Xent Color 0.1137(0.2796) | Loss 1.7176(1.8786) | Error 0.1256(0.1362) | Error Color 0.0222(0.0626) |Steps 554(551.91) | Grad Norm 1.6985(2.5730) | Total Time 10.00(10.00)\n",
      "Iter 2680 | Time 43.0089(42.5926) | Bit/dim 1.5504(1.6680) | Xent 0.3951(0.4226) | Xent Color 0.0998(0.2361) | Loss 1.6741(1.8327) | Error 0.1278(0.1324) | Error Color 0.0200(0.0530) |Steps 554(552.46) | Grad Norm 1.1740(2.2806) | Total Time 10.00(10.00)\n",
      "Iter 2690 | Time 43.5211(42.7042) | Bit/dim 1.5086(1.6321) | Xent 0.4003(0.4172) | Xent Color 0.1156(0.2029) | Loss 1.6376(1.7871) | Error 0.1222(0.1314) | Error Color 0.0300(0.0466) |Steps 572(553.40) | Grad Norm 2.7965(2.2442) | Total Time 10.00(10.00)\n",
      "Iter 2700 | Time 45.4752(43.2799) | Bit/dim 1.4665(1.5946) | Xent 0.4125(0.4091) | Xent Color 0.0962(0.1754) | Loss 1.5936(1.7407) | Error 0.1289(0.1284) | Error Color 0.0211(0.0400) |Steps 560(555.77) | Grad Norm 1.3208(2.2114) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0041 | Time 123.4703, Epoch Time 2997.3564(2629.4278), Bit/dim 1.4485(best: 1.0746), Xent 0.2612, Xent Color 0.0391. Loss 1.5236, Error 0.0809(best: 0.0628), Error Color 0.0026(best: 0.0000)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2710 | Time 42.9804(43.3953) | Bit/dim 1.4215(1.5560) | Xent 0.3749(0.4010) | Xent Color 0.1300(0.1550) | Loss 1.5477(1.6950) | Error 0.1156(0.1261) | Error Color 0.0378(0.0360) |Steps 554(556.01) | Grad Norm 8.9488(2.4882) | Total Time 10.00(10.00)\n",
      "Iter 2720 | Time 45.8985(43.6143) | Bit/dim 1.3913(1.5176) | Xent 0.3355(0.3941) | Xent Color 0.1069(0.1424) | Loss 1.5019(1.6517) | Error 0.1022(0.1242) | Error Color 0.0256(0.0339) |Steps 566(556.61) | Grad Norm 3.6515(3.0449) | Total Time 10.00(10.00)\n",
      "Iter 2730 | Time 46.3133(43.9499) | Bit/dim 1.3510(1.4761) | Xent 0.3676(0.3910) | Xent Color 0.0899(0.1290) | Loss 1.4653(1.6062) | Error 0.1233(0.1236) | Error Color 0.0222(0.0311) |Steps 560(558.18) | Grad Norm 2.9948(3.1655) | Total Time 10.00(10.00)\n",
      "Iter 2740 | Time 47.1415(44.3006) | Bit/dim 1.2982(1.4352) | Xent 0.3674(0.3892) | Xent Color 0.0951(0.1180) | Loss 1.4138(1.5620) | Error 0.1211(0.1239) | Error Color 0.0244(0.0286) |Steps 560(558.65) | Grad Norm 4.7390(3.0621) | Total Time 10.00(10.00)\n",
      "Iter 2750 | Time 44.1172(44.4680) | Bit/dim 1.2864(1.3989) | Xent 0.4327(0.3876) | Xent Color 0.0681(0.1084) | Loss 1.4116(1.5229) | Error 0.1411(0.1233) | Error Color 0.0144(0.0264) |Steps 560(558.86) | Grad Norm 4.2337(3.0481) | Total Time 10.00(10.00)\n",
      "Iter 2760 | Time 44.9908(44.5877) | Bit/dim 1.2647(1.3639) | Xent 0.3903(0.3845) | Xent Color 0.0671(0.1004) | Loss 1.3791(1.4851) | Error 0.1356(0.1229) | Error Color 0.0167(0.0244) |Steps 560(559.99) | Grad Norm 2.3329(2.9047) | Total Time 10.00(10.00)\n",
      "Iter 2770 | Time 45.4073(44.7762) | Bit/dim 1.2652(1.3380) | Xent 0.3174(0.3738) | Xent Color 0.0698(0.0923) | Loss 1.3620(1.4545) | Error 0.0933(0.1190) | Error Color 0.0178(0.0220) |Steps 560(560.48) | Grad Norm 4.8933(2.9929) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0042 | Time 116.0089, Epoch Time 3094.0808(2643.3674), Bit/dim 1.2548(best: 1.0746), Xent 0.2217, Xent Color 0.0240. Loss 1.3162, Error 0.0686(best: 0.0628), Error Color 0.0017(best: 0.0000)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2780 | Time 46.3973(44.8172) | Bit/dim 1.2391(1.3149) | Xent 0.3372(0.3700) | Xent Color 0.0734(0.0862) | Loss 1.3418(1.4289) | Error 0.1144(0.1190) | Error Color 0.0178(0.0205) |Steps 554(560.77) | Grad Norm 2.5313(2.8559) | Total Time 10.00(10.00)\n",
      "Iter 2790 | Time 45.0454(44.9655) | Bit/dim 1.2345(1.2939) | Xent 0.3207(0.3612) | Xent Color 0.0714(0.0805) | Loss 1.3325(1.4043) | Error 0.1044(0.1160) | Error Color 0.0200(0.0192) |Steps 560(560.62) | Grad Norm 3.0987(2.6721) | Total Time 10.00(10.00)\n",
      "Iter 2800 | Time 44.9819(44.9948) | Bit/dim 1.2231(1.2761) | Xent 0.3254(0.3568) | Xent Color 0.0684(0.0781) | Loss 1.3215(1.3848) | Error 0.0978(0.1141) | Error Color 0.0189(0.0189) |Steps 566(561.07) | Grad Norm 2.4528(2.9307) | Total Time 10.00(10.00)\n",
      "Iter 2810 | Time 47.0964(45.0845) | Bit/dim 1.2205(1.2610) | Xent 0.3046(0.3500) | Xent Color 0.0710(0.0749) | Loss 1.3144(1.3672) | Error 0.0889(0.1113) | Error Color 0.0189(0.0179) |Steps 560(560.14) | Grad Norm 3.4827(2.7772) | Total Time 10.00(10.00)\n",
      "Iter 2820 | Time 45.1045(45.1597) | Bit/dim 1.2062(1.2468) | Xent 0.3133(0.3438) | Xent Color 0.0617(0.0708) | Loss 1.3000(1.3504) | Error 0.0978(0.1107) | Error Color 0.0178(0.0163) |Steps 560(559.01) | Grad Norm 3.1881(2.6493) | Total Time 10.00(10.00)\n",
      "Iter 2830 | Time 48.5763(45.4380) | Bit/dim 1.1999(1.2342) | Xent 0.3731(0.3389) | Xent Color 0.0681(0.0677) | Loss 1.3102(1.3358) | Error 0.1133(0.1094) | Error Color 0.0189(0.0157) |Steps 560(558.17) | Grad Norm 1.4059(2.4911) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0043 | Time 124.9024, Epoch Time 3151.8529(2658.6219), Bit/dim 1.1927(best: 1.0746), Xent 0.2001, Xent Color 0.0150. Loss 1.2465, Error 0.0602(best: 0.0628), Error Color 0.0013(best: 0.0000)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2840 | Time 43.5617(45.4658) | Bit/dim 1.2083(1.2233) | Xent 0.3141(0.3336) | Xent Color 0.0616(0.0643) | Loss 1.3022(1.3228) | Error 0.1044(0.1079) | Error Color 0.0167(0.0147) |Steps 566(557.93) | Grad Norm 3.6647(2.4338) | Total Time 10.00(10.00)\n",
      "Iter 2850 | Time 44.0958(45.6388) | Bit/dim 1.1844(1.2138) | Xent 0.2999(0.3303) | Xent Color 0.0632(0.0619) | Loss 1.2751(1.3119) | Error 0.1011(0.1061) | Error Color 0.0156(0.0140) |Steps 554(557.27) | Grad Norm 1.4409(2.5396) | Total Time 10.00(10.00)\n",
      "Iter 2860 | Time 45.7879(45.5837) | Bit/dim 1.1768(1.2049) | Xent 0.3457(0.3252) | Xent Color 0.0716(0.0609) | Loss 1.2811(1.3014) | Error 0.1100(0.1046) | Error Color 0.0178(0.0138) |Steps 548(555.95) | Grad Norm 2.1054(2.4618) | Total Time 10.00(10.00)\n",
      "Iter 2870 | Time 45.7138(45.6319) | Bit/dim 1.1723(1.1961) | Xent 0.2949(0.3209) | Xent Color 0.0695(0.0591) | Loss 1.2634(1.2911) | Error 0.0878(0.1033) | Error Color 0.0167(0.0130) |Steps 542(554.09) | Grad Norm 3.6376(2.3297) | Total Time 10.00(10.00)\n",
      "Iter 2880 | Time 47.3166(45.4503) | Bit/dim 1.1582(1.1884) | Xent 0.3219(0.3195) | Xent Color 0.0441(0.0568) | Loss 1.2497(1.2825) | Error 0.0911(0.1021) | Error Color 0.0067(0.0126) |Steps 548(553.26) | Grad Norm 1.9265(2.3414) | Total Time 10.00(10.00)\n",
      "Iter 2890 | Time 47.4419(45.2868) | Bit/dim 1.1604(1.1814) | Xent 0.2896(0.3125) | Xent Color 0.0567(0.0562) | Loss 1.2470(1.2736) | Error 0.0956(0.1002) | Error Color 0.0133(0.0124) |Steps 560(552.23) | Grad Norm 4.3144(2.5704) | Total Time 10.00(10.00)\n",
      "Iter 2900 | Time 45.5519(45.1872) | Bit/dim 1.1647(1.1759) | Xent 0.3019(0.3075) | Xent Color 0.0552(0.0549) | Loss 1.2539(1.2665) | Error 0.1044(0.0990) | Error Color 0.0133(0.0123) |Steps 548(552.17) | Grad Norm 4.3182(2.8126) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0044 | Time 126.3390, Epoch Time 3137.0184(2672.9738), Bit/dim 1.1551(best: 1.0746), Xent 0.1798, Xent Color 0.0132. Loss 1.2033, Error 0.0560(best: 0.0602), Error Color 0.0005(best: 0.0000)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2910 | Time 42.6491(45.1459) | Bit/dim 1.1361(1.1685) | Xent 0.2689(0.3014) | Xent Color 0.0408(0.0529) | Loss 1.2135(1.2571) | Error 0.0833(0.0970) | Error Color 0.0067(0.0116) |Steps 548(551.40) | Grad Norm 2.6263(2.6634) | Total Time 10.00(10.00)\n",
      "Iter 2920 | Time 44.3816(45.0184) | Bit/dim 1.1456(1.1621) | Xent 0.2873(0.2968) | Xent Color 0.0426(0.0508) | Loss 1.2281(1.2490) | Error 0.0844(0.0954) | Error Color 0.0111(0.0114) |Steps 542(550.77) | Grad Norm 1.8174(2.5028) | Total Time 10.00(10.00)\n",
      "Iter 2930 | Time 44.8194(44.9198) | Bit/dim 1.1366(1.1576) | Xent 0.2618(0.2957) | Xent Color 0.0441(0.0487) | Loss 1.2130(1.2437) | Error 0.0856(0.0953) | Error Color 0.0044(0.0105) |Steps 548(550.97) | Grad Norm 2.5781(2.4649) | Total Time 10.00(10.00)\n",
      "Iter 2940 | Time 46.8553(45.1619) | Bit/dim 1.1320(1.1523) | Xent 0.2895(0.2964) | Xent Color 0.0471(0.0481) | Loss 1.2161(1.2384) | Error 0.1011(0.0958) | Error Color 0.0144(0.0104) |Steps 554(551.30) | Grad Norm 1.8651(2.4562) | Total Time 10.00(10.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_2cond_nosep.py --data colormnist --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/cnf_conditional_disentangle_colormnist_bs900_sratio_1_2th_drop_0_5_2cond_linear_nosep_run1 --resume ../experiments_published/cnf_conditional_disentangle_colormnist_bs900_sratio_1_2th_drop_0_5_2cond_linear_nosep_run1/current_checkpt.pth --seed 1 --lr 0.0001 --conditional True --controlled_tol False --train_mode semisup --log_freq 10 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5 --y_color 10 --y_class 10\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
