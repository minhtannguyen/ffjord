{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0,1,2,3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_rl.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import torch.utils.data as data\n",
      "from torch.utils.data import Dataset\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "plt.rcParams['figure.dpi'] = 300\n",
      "\n",
      "from PIL import Image\n",
      "import os.path\n",
      "import errno\n",
      "import codecs\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time, count_nfe_gate\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "GATES = [\"cnn1\", \"cnn2\", \"rnn\"]\n",
      "\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"colormnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--gate', type=str, default='cnn1', choices=GATES)\n",
      "parser.add_argument('--scale', type=float, default=1.0)\n",
      "parser.add_argument('--scale_fac', type=float, default=1.0)\n",
      "parser.add_argument('--scale_std', type=float, default=1.0)\n",
      "parser.add_argument('--eta', default=0.1, type=float,\n",
      "                        help='tuning parameter that allows us to trade-off the competing goals of' \n",
      "                                'minimizing the prediction loss and maximizing the gate rewards ')\n",
      "parser.add_argument('--rl-weight', default=0.01, type=float,\n",
      "                        help='rl weight')\n",
      "\n",
      "parser.add_argument('--gamma', default=0.99, type=float,\n",
      "                        help='discount factor, default: (0.99)')\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=500)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "parser.add_argument(\"--annealing_std\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--y_class\", type=int, default=10)\n",
      "parser.add_argument(\"--y_color\", type=int, default=10)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "import lib.odenvp_conditional_rl as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "class ColorMNIST(data.Dataset):\n",
      "    \"\"\"\n",
      "    ColorMNIST\n",
      "    \"\"\"\n",
      "    urls = [\n",
      "        'http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz',\n",
      "    ]\n",
      "    raw_folder = 'raw'\n",
      "    processed_folder = 'processed'\n",
      "    training_file = 'training.pt'\n",
      "    test_file = 'test.pt'\n",
      "\n",
      "    def __init__(self, root, train=True, transform=None, target_transform=None, download=False):\n",
      "        self.root = os.path.expanduser(root)\n",
      "        self.transform = transform\n",
      "        self.target_transform = target_transform\n",
      "        self.train = train  # training set or test set\n",
      "\n",
      "        if download:\n",
      "            self.download()\n",
      "\n",
      "        if not self._check_exists():\n",
      "            raise RuntimeError('Dataset not found.' +\n",
      "                               ' You can use download=True to download it')\n",
      "\n",
      "        if self.train:\n",
      "            self.train_data, self.train_labels = torch.load(\n",
      "                os.path.join(self.root, self.processed_folder, self.training_file))\n",
      "            \n",
      "            self.train_data = np.tile(self.train_data[:, :, :, np.newaxis], 3)\n",
      "        else:\n",
      "            self.test_data, self.test_labels = torch.load(\n",
      "                os.path.join(self.root, self.processed_folder, self.test_file))\n",
      "            \n",
      "            self.test_data = np.tile(self.test_data[:, :, :, np.newaxis], 3)\n",
      "        \n",
      "        self.pallette = [[31, 119, 180],\n",
      "                         [255, 127, 14],\n",
      "                         [44, 160, 44],\n",
      "                         [214, 39, 40],\n",
      "                         [148, 103, 189],\n",
      "                         [140, 86, 75],\n",
      "                         [227, 119, 194],\n",
      "                         [127, 127, 127],\n",
      "                         [188, 189, 34],\n",
      "                         [23, 190, 207]]\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        \"\"\"\n",
      "        Args:\n",
      "            index (int): Index\n",
      "\n",
      "        Returns:\n",
      "            tuple: (image, target) where target is index of the target class.\n",
      "        \"\"\"\n",
      "        if self.train:\n",
      "            img, target = self.train_data[index].copy(), self.train_labels[index]\n",
      "        else:\n",
      "            img, target = self.test_data[index].copy(), self.test_labels[index]\n",
      "        \n",
      "        # doing this so that it is consistent with all other datasets\n",
      "        # to return a PIL Image\n",
      "        y_color_digit = np.random.randint(0, args.y_color)\n",
      "        c_digit = self.pallette[y_color_digit]\n",
      "        \n",
      "        img[:, :, 0] = img[:, :, 0] / 255 * c_digit[0]\n",
      "        img[:, :, 1] = img[:, :, 1] / 255 * c_digit[1]\n",
      "        img[:, :, 2] = img[:, :, 2] / 255 * c_digit[2]\n",
      "        \n",
      "        img = Image.fromarray(img)\n",
      "\n",
      "        if self.transform is not None:\n",
      "            img = self.transform(img)\n",
      "\n",
      "        if self.target_transform is not None:\n",
      "            target = self.target_transform(target)\n",
      "\n",
      "        return img, [target,torch.from_numpy(np.array(y_color_digit))]\n",
      "\n",
      "    def __len__(self):\n",
      "        if self.train:\n",
      "            return len(self.train_data)\n",
      "        else:\n",
      "            return len(self.test_data)\n",
      "\n",
      "    def _check_exists(self):\n",
      "        return os.path.exists(os.path.join(self.root, self.processed_folder, self.training_file)) and \\\n",
      "            os.path.exists(os.path.join(self.root, self.processed_folder, self.test_file))\n",
      "\n",
      "    def download(self):\n",
      "        \"\"\"Download the MNIST data if it doesn't exist in processed_folder already.\"\"\"\n",
      "        from six.moves import urllib\n",
      "        import gzip\n",
      "\n",
      "        if self._check_exists():\n",
      "            return\n",
      "\n",
      "        # download files\n",
      "        try:\n",
      "            os.makedirs(os.path.join(self.root, self.raw_folder))\n",
      "            os.makedirs(os.path.join(self.root, self.processed_folder))\n",
      "        except OSError as e:\n",
      "            if e.errno == errno.EEXIST:\n",
      "                pass\n",
      "            else:\n",
      "                raise\n",
      "\n",
      "        for url in self.urls:\n",
      "            print('Downloading ' + url)\n",
      "            data = urllib.request.urlopen(url)\n",
      "            filename = url.rpartition('/')[2]\n",
      "            file_path = os.path.join(self.root, self.raw_folder, filename)\n",
      "            with open(file_path, 'wb') as f:\n",
      "                f.write(data.read())\n",
      "            with open(file_path.replace('.gz', ''), 'wb') as out_f, \\\n",
      "                    gzip.GzipFile(file_path) as zip_f:\n",
      "                out_f.write(zip_f.read())\n",
      "            os.unlink(file_path)\n",
      "\n",
      "        # process and save as torch files\n",
      "        print('Processing...')\n",
      "\n",
      "        training_set = (\n",
      "            read_image_file(os.path.join(self.root, self.raw_folder, 'train-images-idx3-ubyte')),\n",
      "            read_label_file(os.path.join(self.root, self.raw_folder, 'train-labels-idx1-ubyte'))\n",
      "        )\n",
      "        test_set = (\n",
      "            read_image_file(os.path.join(self.root, self.raw_folder, 't10k-images-idx3-ubyte')),\n",
      "            read_label_file(os.path.join(self.root, self.raw_folder, 't10k-labels-idx1-ubyte'))\n",
      "        )\n",
      "        with open(os.path.join(self.root, self.processed_folder, self.training_file), 'wb') as f:\n",
      "            torch.save(training_set, f)\n",
      "        with open(os.path.join(self.root, self.processed_folder, self.test_file), 'wb') as f:\n",
      "            torch.save(test_set, f)\n",
      "\n",
      "        print('Done!')\n",
      "\n",
      "    def __repr__(self):\n",
      "        fmt_str = 'Dataset ' + self.__class__.__name__ + '\\n'\n",
      "        fmt_str += '    Number of datapoints: {}\\n'.format(self.__len__())\n",
      "        tmp = 'train' if self.train is True else 'test'\n",
      "        fmt_str += '    Split: {}\\n'.format(tmp)\n",
      "        fmt_str += '    Root Location: {}\\n'.format(self.root)\n",
      "        tmp = '    Transforms (if any): '\n",
      "        fmt_str += '{0}{1}\\n'.format(tmp, self.transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
      "        tmp = '    Target Transforms (if any): '\n",
      "        fmt_str += '{0}{1}'.format(tmp, self.target_transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
      "        return fmt_str\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "        \n",
      "def update_scale_std(model, epoch):\n",
      "    epoch_frac = 1.0 - float(epoch - 1) / max(args.num_epochs + 1, 1)\n",
      "    scale_std = args.scale_std * epoch_frac\n",
      "    model.set_scale_std(scale_std)\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"colormnist\":\n",
      "        im_dim = 3\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = ColorMNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = ColorMNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol, \"scale\": args.scale, \"scale_fac\": args.scale_fac, \"scale_std\": args.scale_std, \"gate\": args.gate},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,\n",
      "            y_class = args.y_class)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        if args.annealing_std:\n",
      "            update_scale_std(model.module, epoch)\n",
      "            \n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            if args.data == \"colormnist\":\n",
      "                y = y[0]\n",
      "            \n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            # re-weight the gate rewards\n",
      "            normalized_eta = args.eta / len(logp_actions)\n",
      "            \n",
      "            # collect cumulative future rewards\n",
      "            R = - loss\n",
      "            cum_rewards = []\n",
      "            for r in nfe[::-1]:\n",
      "                R = -normalized_eta * r.view(-1,1) + args.gamma * R\n",
      "                cum_rewards.insert(0,R)\n",
      "            \n",
      "            # apply REINFORCE\n",
      "            rl_loss = 0\n",
      "            for lpa, r in zip(logp_actions, cum_rewards):\n",
      "                rl_loss = rl_loss - lpa.view(-1,1) * args.rl_weight * r\n",
      "                \n",
      "            loss = loss + rl_loss.mean()\n",
      "            \n",
      "            loss.backward()\n",
      "            \n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe_gate(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            for idx in range(len(model.module.transforms)):\n",
      "                for layer in model.module.transforms[idx].chain:\n",
      "                    if hasattr(layer, 'atol'):\n",
      "                        layer.odefunc.after_odeint()\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'train': atol[tol_indx].mean()}, itr)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'train': rtol[tol_indx].mean()}, itr)\n",
      "                    \n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        if args.data == \"colormnist\":\n",
      "            # print train images\n",
      "            xall = []\n",
      "            ximg = x[0:40].cpu().numpy().transpose((0,2,3,1))\n",
      "            for i in range(ximg.shape[0]):\n",
      "                xall.append(ximg[i])\n",
      "        \n",
      "            xall = np.hstack(xall)\n",
      "\n",
      "            plt.imshow(xall)\n",
      "            plt.axis('off')\n",
      "            plt.show()\n",
      "            \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if args.data == \"colormnist\":\n",
      "                        y = y[0]\n",
      "                        \n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                if args.data == \"colormnist\":\n",
      "                    # print test images\n",
      "                    xall = []\n",
      "                    ximg = x[0:40].cpu().numpy().transpose((0,2,3,1))\n",
      "                    for i in range(ximg.shape[0]):\n",
      "                        xall.append(ximg[i])\n",
      "\n",
      "                    xall = np.hstack(xall)\n",
      "\n",
      "                    plt.imshow(xall)\n",
      "                    plt.axis('off')\n",
      "                    plt.show()\n",
      "                    \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples, atol, rtol, logp_actions, nfe = model(fixed_z, reverse=True)\n",
      "            generated_samples = generated_samples.view(-1, *data_shape)\n",
      "            for tol_indx in range(len(atol)):\n",
      "                writer.add_scalars('atol_gen_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                writer.add_scalars('rtol_gen_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, annealing_std=False, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, condition_ratio=0.5, conditional=True, controlled_tol=False, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, eta=0.1, gamma=0.99, gate='cnn1', imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.0001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=500, parallel=False, rademacher=True, residual=False, resume='../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_6_run1/epoch_270_checkpt.pth', rl_weight=0.01, rtol=1e-05, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_6_run1', scale=1.0, scale_fac=1.0, scale_std=6.0, seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5, y_class=10, y_color=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1450732\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc_rl.py:291: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Iter 14860 | Time 25.6773(26.0773) | Bit/dim 3.5713(3.5628) | Xent 0.1038(0.1183) | Loss 9.2695(10.3478) | Error 0.0378(0.0399) Steps 1018(1029.40) | Grad Norm 2.1305(3.0518) | Total Time 0.00(0.00)\n",
      "Iter 14870 | Time 25.4394(25.8643) | Bit/dim 3.5719(3.5646) | Xent 0.1092(0.1160) | Loss 9.4281(10.0858) | Error 0.0322(0.0389) Steps 1036(1030.58) | Grad Norm 3.0532(2.9705) | Total Time 0.00(0.00)\n",
      "Iter 14880 | Time 25.3255(25.7115) | Bit/dim 3.5500(3.5637) | Xent 0.1073(0.1161) | Loss 9.1784(9.8932) | Error 0.0289(0.0386) Steps 1060(1028.28) | Grad Norm 2.4905(3.0073) | Total Time 0.00(0.00)\n",
      "Iter 14890 | Time 25.9316(25.6372) | Bit/dim 3.5484(3.5645) | Xent 0.0938(0.1142) | Loss 9.4243(9.7646) | Error 0.0289(0.0383) Steps 1048(1028.67) | Grad Norm 2.3415(3.0112) | Total Time 0.00(0.00)\n",
      "Iter 14900 | Time 25.6300(25.5422) | Bit/dim 3.5679(3.5643) | Xent 0.1012(0.1129) | Loss 9.3542(9.6599) | Error 0.0411(0.0382) Steps 1000(1027.74) | Grad Norm 3.0272(3.1326) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0271 | Time 124.5039, Epoch Time 1558.5289(1468.1443), Bit/dim 3.5736(best: inf), Xent 0.9466, Loss 4.0469, Error 0.2214(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14910 | Time 25.2671(25.4604) | Bit/dim 3.5449(3.5645) | Xent 0.0883(0.1135) | Loss 9.3133(10.5743) | Error 0.0233(0.0377) Steps 1018(1026.13) | Grad Norm 2.8462(3.1038) | Total Time 0.00(0.00)\n",
      "Iter 14920 | Time 25.0945(25.5455) | Bit/dim 3.5720(3.5645) | Xent 0.1168(0.1134) | Loss 9.3066(10.2569) | Error 0.0400(0.0382) Steps 976(1028.18) | Grad Norm 2.6712(3.1352) | Total Time 0.00(0.00)\n",
      "Iter 14930 | Time 25.5019(25.5267) | Bit/dim 3.5315(3.5611) | Xent 0.1209(0.1127) | Loss 9.3509(10.0207) | Error 0.0311(0.0380) Steps 1054(1025.70) | Grad Norm 2.9641(3.1180) | Total Time 0.00(0.00)\n",
      "Iter 14940 | Time 25.9504(25.6516) | Bit/dim 3.5818(3.5619) | Xent 0.1134(0.1147) | Loss 9.4599(9.8506) | Error 0.0411(0.0388) Steps 1012(1021.22) | Grad Norm 3.1948(3.1590) | Total Time 0.00(0.00)\n",
      "Iter 14950 | Time 25.8672(25.6595) | Bit/dim 3.5560(3.5644) | Xent 0.1154(0.1149) | Loss 9.2986(9.7253) | Error 0.0389(0.0387) Steps 1030(1023.78) | Grad Norm 2.6208(3.1197) | Total Time 0.00(0.00)\n",
      "Iter 14960 | Time 25.3941(25.6688) | Bit/dim 3.5508(3.5646) | Xent 0.1068(0.1161) | Loss 9.3921(9.6453) | Error 0.0389(0.0395) Steps 1030(1025.29) | Grad Norm 3.1739(3.1457) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0272 | Time 120.9446, Epoch Time 1555.0342(1470.7510), Bit/dim 3.5718(best: 3.5736), Xent 0.9451, Loss 4.0444, Error 0.2247(best: 0.2214)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14970 | Time 26.9168(25.7037) | Bit/dim 3.5464(3.5635) | Xent 0.1000(0.1153) | Loss 9.3356(10.4175) | Error 0.0322(0.0393) Steps 1084(1028.42) | Grad Norm 3.1478(3.1273) | Total Time 0.00(0.00)\n",
      "Iter 14980 | Time 24.9254(25.6995) | Bit/dim 3.5820(3.5658) | Xent 0.1357(0.1154) | Loss 9.4258(10.1515) | Error 0.0467(0.0391) Steps 1042(1031.90) | Grad Norm 4.1413(3.1214) | Total Time 0.00(0.00)\n",
      "Iter 14990 | Time 25.0158(25.5890) | Bit/dim 3.5599(3.5649) | Xent 0.1023(0.1157) | Loss 9.2773(9.9442) | Error 0.0389(0.0391) Steps 994(1029.53) | Grad Norm 3.8833(3.2341) | Total Time 0.00(0.00)\n",
      "Iter 15000 | Time 25.1850(25.5307) | Bit/dim 3.5976(3.5653) | Xent 0.1151(0.1157) | Loss 9.5346(9.7918) | Error 0.0389(0.0390) Steps 1030(1028.47) | Grad Norm 2.7816(3.2197) | Total Time 0.00(0.00)\n",
      "Iter 15010 | Time 25.5981(25.5310) | Bit/dim 3.5546(3.5652) | Xent 0.1092(0.1136) | Loss 9.4380(9.6770) | Error 0.0400(0.0381) Steps 1042(1030.36) | Grad Norm 3.1451(3.1503) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0273 | Time 119.3858, Epoch Time 1542.1829(1472.8939), Bit/dim 3.5723(best: 3.5718), Xent 0.9521, Loss 4.0483, Error 0.2226(best: 0.2214)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15020 | Time 25.6821(25.5513) | Bit/dim 3.5627(3.5655) | Xent 0.1049(0.1130) | Loss 9.3827(10.5760) | Error 0.0322(0.0381) Steps 1066(1032.06) | Grad Norm 2.7226(3.0408) | Total Time 0.00(0.00)\n",
      "Iter 15030 | Time 25.9520(25.5992) | Bit/dim 3.6069(3.5644) | Xent 0.1099(0.1129) | Loss 9.5056(10.2655) | Error 0.0456(0.0386) Steps 1030(1031.45) | Grad Norm 3.0195(3.0095) | Total Time 0.00(0.00)\n",
      "Iter 15040 | Time 26.2651(25.6798) | Bit/dim 3.5759(3.5618) | Xent 0.1265(0.1135) | Loss 9.5209(10.0386) | Error 0.0333(0.0377) Steps 1000(1032.97) | Grad Norm 3.0323(3.0128) | Total Time 0.00(0.00)\n",
      "Iter 15050 | Time 25.0476(25.6421) | Bit/dim 3.5417(3.5628) | Xent 0.1355(0.1123) | Loss 9.2161(9.8594) | Error 0.0411(0.0376) Steps 1066(1033.63) | Grad Norm 4.9028(3.0238) | Total Time 0.00(0.00)\n",
      "Iter 15060 | Time 25.1285(25.6267) | Bit/dim 3.5601(3.5628) | Xent 0.1258(0.1115) | Loss 9.2460(9.7219) | Error 0.0433(0.0374) Steps 994(1032.64) | Grad Norm 2.9251(3.0122) | Total Time 0.00(0.00)\n",
      "Iter 15070 | Time 24.5833(25.5970) | Bit/dim 3.6047(3.5648) | Xent 0.1559(0.1141) | Loss 9.4853(9.6371) | Error 0.0467(0.0385) Steps 1030(1034.77) | Grad Norm 3.6383(3.0002) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0274 | Time 119.4219, Epoch Time 1548.7725(1475.1703), Bit/dim 3.5712(best: 3.5718), Xent 0.9627, Loss 4.0526, Error 0.2241(best: 0.2214)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15080 | Time 25.6072(25.6840) | Bit/dim 3.5325(3.5633) | Xent 0.0901(0.1132) | Loss 9.2019(10.4089) | Error 0.0300(0.0382) Steps 1042(1032.32) | Grad Norm 2.8639(3.0289) | Total Time 0.00(0.00)\n",
      "Iter 15090 | Time 26.2171(25.7130) | Bit/dim 3.5698(3.5670) | Xent 0.1132(0.1127) | Loss 9.4193(10.1535) | Error 0.0433(0.0382) Steps 1018(1033.70) | Grad Norm 2.8215(2.9695) | Total Time 0.00(0.00)\n",
      "Iter 15100 | Time 25.4125(25.6670) | Bit/dim 3.5670(3.5674) | Xent 0.0842(0.1102) | Loss 9.3461(9.9344) | Error 0.0322(0.0368) Steps 1024(1031.67) | Grad Norm 2.6431(2.9080) | Total Time 0.00(0.00)\n",
      "Iter 15110 | Time 25.7263(25.7235) | Bit/dim 3.5786(3.5653) | Xent 0.1399(0.1126) | Loss 9.2582(9.7886) | Error 0.0511(0.0383) Steps 1024(1034.01) | Grad Norm 4.0608(2.9902) | Total Time 0.00(0.00)\n",
      "Iter 15120 | Time 25.6297(25.6898) | Bit/dim 3.5694(3.5624) | Xent 0.1271(0.1130) | Loss 9.4748(9.6781) | Error 0.0367(0.0380) Steps 1036(1035.28) | Grad Norm 3.3748(3.1304) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0275 | Time 121.1700, Epoch Time 1554.3461(1477.5456), Bit/dim 3.5735(best: 3.5712), Xent 0.9696, Loss 4.0583, Error 0.2231(best: 0.2214)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15130 | Time 25.0432(25.6943) | Bit/dim 3.5515(3.5610) | Xent 0.0889(0.1121) | Loss 9.2533(10.5891) | Error 0.0300(0.0379) Steps 1054(1035.59) | Grad Norm 2.2629(3.0671) | Total Time 0.00(0.00)\n",
      "Iter 15140 | Time 25.4106(25.6702) | Bit/dim 3.5349(3.5598) | Xent 0.0968(0.1114) | Loss 9.3107(10.2595) | Error 0.0344(0.0375) Steps 1042(1037.55) | Grad Norm 2.3126(3.0248) | Total Time 0.00(0.00)\n",
      "Iter 15150 | Time 25.3225(25.6897) | Bit/dim 3.5535(3.5621) | Xent 0.1063(0.1103) | Loss 9.1742(10.0243) | Error 0.0300(0.0371) Steps 1024(1037.00) | Grad Norm 2.8234(3.0313) | Total Time 0.00(0.00)\n",
      "Iter 15160 | Time 25.1802(25.6354) | Bit/dim 3.5519(3.5618) | Xent 0.1445(0.1118) | Loss 9.3947(9.8508) | Error 0.0522(0.0379) Steps 1042(1038.53) | Grad Norm 2.9381(3.0227) | Total Time 0.00(0.00)\n",
      "Iter 15170 | Time 25.0899(25.5454) | Bit/dim 3.5739(3.5623) | Xent 0.1176(0.1133) | Loss 9.3524(9.7183) | Error 0.0367(0.0383) Steps 1036(1037.58) | Grad Norm 4.1486(3.1984) | Total Time 0.00(0.00)\n",
      "Iter 15180 | Time 26.0732(25.6217) | Bit/dim 3.5801(3.5649) | Xent 0.0922(0.1143) | Loss 9.4645(9.6386) | Error 0.0300(0.0383) Steps 1066(1039.31) | Grad Norm 4.0376(3.2605) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0276 | Time 119.8695, Epoch Time 1547.5887(1479.6469), Bit/dim 3.5731(best: 3.5712), Xent 0.9694, Loss 4.0578, Error 0.2237(best: 0.2214)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15190 | Time 25.2362(25.6333) | Bit/dim 3.5456(3.5641) | Xent 0.1221(0.1132) | Loss 9.3341(10.4051) | Error 0.0433(0.0387) Steps 1006(1038.84) | Grad Norm 2.8167(3.2220) | Total Time 0.00(0.00)\n",
      "Iter 15200 | Time 26.0276(25.6463) | Bit/dim 3.5412(3.5638) | Xent 0.1194(0.1138) | Loss 9.3804(10.1272) | Error 0.0511(0.0389) Steps 1030(1036.56) | Grad Norm 3.0088(3.2399) | Total Time 0.00(0.00)\n",
      "Iter 15210 | Time 25.2831(25.6763) | Bit/dim 3.5831(3.5633) | Xent 0.1419(0.1151) | Loss 9.4403(9.9252) | Error 0.0478(0.0395) Steps 994(1035.22) | Grad Norm 4.8438(3.3383) | Total Time 0.00(0.00)\n",
      "Iter 15220 | Time 25.8075(25.7006) | Bit/dim 3.6055(3.5652) | Xent 0.1032(0.1155) | Loss 9.4015(9.7833) | Error 0.0389(0.0394) Steps 1042(1035.18) | Grad Norm 4.1778(3.4001) | Total Time 0.00(0.00)\n",
      "Iter 15230 | Time 25.8131(25.7691) | Bit/dim 3.5269(3.5663) | Xent 0.1157(0.1177) | Loss 9.3237(9.6962) | Error 0.0489(0.0406) Steps 1030(1036.12) | Grad Norm 3.8044(3.5545) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0277 | Time 120.4979, Epoch Time 1559.5419(1482.0437), Bit/dim 3.5748(best: 3.5712), Xent 0.9846, Loss 4.0671, Error 0.2240(best: 0.2214)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15240 | Time 27.1029(25.8811) | Bit/dim 3.5688(3.5650) | Xent 0.1171(0.1177) | Loss 9.3627(10.6278) | Error 0.0389(0.0408) Steps 1072(1039.35) | Grad Norm 3.1256(3.6916) | Total Time 0.00(0.00)\n",
      "Iter 15250 | Time 26.1527(25.9461) | Bit/dim 3.5883(3.5662) | Xent 0.1124(0.1160) | Loss 9.3839(10.2953) | Error 0.0344(0.0398) Steps 1060(1039.11) | Grad Norm 3.0324(3.6674) | Total Time 0.00(0.00)\n",
      "Iter 15260 | Time 25.5803(26.0176) | Bit/dim 3.5586(3.5653) | Xent 0.1159(0.1155) | Loss 9.3724(10.0559) | Error 0.0400(0.0393) Steps 1066(1043.67) | Grad Norm 2.6220(3.6732) | Total Time 0.00(0.00)\n",
      "Iter 15270 | Time 26.5275(26.1578) | Bit/dim 3.5480(3.5648) | Xent 0.1372(0.1161) | Loss 9.3237(9.8679) | Error 0.0456(0.0394) Steps 1096(1042.22) | Grad Norm 4.0237(3.5047) | Total Time 0.00(0.00)\n",
      "Iter 15280 | Time 26.3465(26.2337) | Bit/dim 3.5631(3.5654) | Xent 0.1178(0.1175) | Loss 9.4060(9.7481) | Error 0.0422(0.0398) Steps 1030(1039.70) | Grad Norm 3.7103(3.4921) | Total Time 0.00(0.00)\n",
      "Iter 15290 | Time 26.4460(26.2305) | Bit/dim 3.6178(3.5681) | Xent 0.1177(0.1173) | Loss 9.4826(9.6479) | Error 0.0344(0.0399) Steps 1078(1040.83) | Grad Norm 2.7666(3.3994) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0278 | Time 120.1544, Epoch Time 1584.8111(1485.1267), Bit/dim 3.5755(best: 3.5712), Xent 0.9661, Loss 4.0586, Error 0.2195(best: 0.2214)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15300 | Time 26.5460(26.2952) | Bit/dim 3.5425(3.5645) | Xent 0.1168(0.1169) | Loss 9.4375(10.4201) | Error 0.0378(0.0395) Steps 1048(1041.13) | Grad Norm 2.4385(3.5965) | Total Time 0.00(0.00)\n",
      "Iter 15310 | Time 26.7821(26.3078) | Bit/dim 3.5821(3.5662) | Xent 0.1444(0.1179) | Loss 9.4523(10.1652) | Error 0.0478(0.0401) Steps 1072(1044.10) | Grad Norm 3.9447(3.6064) | Total Time 0.00(0.00)\n",
      "Iter 15320 | Time 25.7691(26.2224) | Bit/dim 3.5878(3.5674) | Xent 0.1014(0.1164) | Loss 9.4373(9.9617) | Error 0.0356(0.0393) Steps 1054(1041.50) | Grad Norm 2.8338(3.4710) | Total Time 0.00(0.00)\n",
      "Iter 15330 | Time 25.8811(26.2062) | Bit/dim 3.5667(3.5679) | Xent 0.1225(0.1165) | Loss 9.4226(9.8053) | Error 0.0422(0.0395) Steps 1066(1038.74) | Grad Norm 3.4323(3.5259) | Total Time 0.00(0.00)\n",
      "Iter 15340 | Time 25.9756(26.2126) | Bit/dim 3.5478(3.5681) | Xent 0.1036(0.1160) | Loss 9.3005(9.6930) | Error 0.0322(0.0393) Steps 1054(1040.79) | Grad Norm 3.4528(3.5809) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0279 | Time 119.7064, Epoch Time 1581.3715(1488.0141), Bit/dim 3.5785(best: 3.5712), Xent 0.9864, Loss 4.0717, Error 0.2213(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15350 | Time 25.9933(26.1766) | Bit/dim 3.5559(3.5686) | Xent 0.1097(0.1166) | Loss 9.2900(10.5978) | Error 0.0411(0.0402) Steps 994(1040.19) | Grad Norm 3.5018(3.9650) | Total Time 0.00(0.00)\n",
      "Iter 15360 | Time 25.8729(26.1727) | Bit/dim 3.5600(3.5697) | Xent 0.0962(0.1151) | Loss 9.2613(10.2760) | Error 0.0267(0.0396) Steps 1042(1045.42) | Grad Norm 2.7997(3.8535) | Total Time 0.00(0.00)\n",
      "Iter 15370 | Time 26.9828(26.2730) | Bit/dim 3.5689(3.5687) | Xent 0.1023(0.1164) | Loss 9.4627(10.0378) | Error 0.0389(0.0403) Steps 1036(1041.35) | Grad Norm 2.9876(3.8514) | Total Time 0.00(0.00)\n",
      "Iter 15380 | Time 26.9826(26.3078) | Bit/dim 3.5584(3.5684) | Xent 0.1038(0.1178) | Loss 9.2497(9.8700) | Error 0.0333(0.0405) Steps 1030(1040.97) | Grad Norm 3.2832(4.0262) | Total Time 0.00(0.00)\n",
      "Iter 15390 | Time 27.7519(26.4411) | Bit/dim 3.5872(3.5685) | Xent 0.1436(0.1184) | Loss 9.5326(9.7504) | Error 0.0544(0.0406) Steps 1054(1047.93) | Grad Norm 5.9658(4.1010) | Total Time 0.00(0.00)\n",
      "Iter 15400 | Time 26.3163(26.4558) | Bit/dim 3.5657(3.5714) | Xent 0.1023(0.1182) | Loss 9.3845(9.6607) | Error 0.0433(0.0409) Steps 1042(1049.17) | Grad Norm 3.7312(4.6131) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0280 | Time 121.1487, Epoch Time 1593.3653(1491.1746), Bit/dim 3.5899(best: 3.5712), Xent 0.9950, Loss 4.0874, Error 0.2215(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15410 | Time 27.0877(26.4344) | Bit/dim 3.6019(3.5747) | Xent 0.1004(0.1188) | Loss 9.4781(10.4547) | Error 0.0256(0.0406) Steps 1072(1051.29) | Grad Norm 3.1675(4.6834) | Total Time 0.00(0.00)\n",
      "Iter 15420 | Time 26.4951(26.5255) | Bit/dim 3.5619(3.5733) | Xent 0.1403(0.1176) | Loss 9.4267(10.1840) | Error 0.0433(0.0398) Steps 1096(1052.13) | Grad Norm 5.6912(4.6436) | Total Time 0.00(0.00)\n",
      "Iter 15430 | Time 26.7856(26.6184) | Bit/dim 3.5767(3.5742) | Xent 0.1141(0.1161) | Loss 9.3869(9.9777) | Error 0.0356(0.0400) Steps 1072(1055.48) | Grad Norm 3.2389(4.2581) | Total Time 0.00(0.00)\n",
      "Iter 15440 | Time 26.7187(26.6107) | Bit/dim 3.5411(3.5737) | Xent 0.1001(0.1177) | Loss 9.2411(9.8265) | Error 0.0356(0.0408) Steps 1072(1055.29) | Grad Norm 2.7520(4.0778) | Total Time 0.00(0.00)\n",
      "Iter 15450 | Time 26.1108(26.5807) | Bit/dim 3.5647(3.5725) | Xent 0.1402(0.1160) | Loss 9.4586(9.7132) | Error 0.0489(0.0401) Steps 1030(1053.69) | Grad Norm 6.4624(4.0665) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0281 | Time 121.3141, Epoch Time 1602.9540(1494.5280), Bit/dim 3.5778(best: 3.5712), Xent 0.9879, Loss 4.0717, Error 0.2230(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15460 | Time 26.7648(26.4871) | Bit/dim 3.5978(3.5699) | Xent 0.1254(0.1152) | Loss 9.5794(10.6133) | Error 0.0444(0.0397) Steps 1054(1050.83) | Grad Norm 2.6967(3.9944) | Total Time 0.00(0.00)\n",
      "Iter 15470 | Time 27.1454(26.5109) | Bit/dim 3.5238(3.5704) | Xent 0.1516(0.1146) | Loss 9.3815(10.3029) | Error 0.0522(0.0390) Steps 1018(1049.96) | Grad Norm 4.1153(4.0187) | Total Time 0.00(0.00)\n",
      "Iter 15480 | Time 26.5768(26.5735) | Bit/dim 3.5908(3.5694) | Xent 0.0980(0.1133) | Loss 9.5676(10.0686) | Error 0.0344(0.0390) Steps 1072(1047.62) | Grad Norm 3.2438(4.0264) | Total Time 0.00(0.00)\n",
      "Iter 15490 | Time 26.1872(26.5572) | Bit/dim 3.5854(3.5712) | Xent 0.1496(0.1141) | Loss 9.6017(9.8949) | Error 0.0556(0.0390) Steps 1060(1047.55) | Grad Norm 4.3022(4.0496) | Total Time 0.00(0.00)\n",
      "Iter 15500 | Time 26.2457(26.4612) | Bit/dim 3.5784(3.5704) | Xent 0.1248(0.1141) | Loss 9.3946(9.7658) | Error 0.0489(0.0391) Steps 1072(1050.88) | Grad Norm 3.2368(3.9146) | Total Time 0.00(0.00)\n",
      "Iter 15510 | Time 26.5579(26.3687) | Bit/dim 3.5618(3.5682) | Xent 0.1104(0.1146) | Loss 9.4404(9.6703) | Error 0.0422(0.0393) Steps 1066(1050.50) | Grad Norm 4.8697(4.3612) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0282 | Time 120.1400, Epoch Time 1590.4240(1497.4049), Bit/dim 3.5770(best: 3.5712), Xent 0.9768, Loss 4.0654, Error 0.2221(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15520 | Time 26.6572(26.3839) | Bit/dim 3.5765(3.5664) | Xent 0.1140(0.1143) | Loss 9.3762(10.4335) | Error 0.0378(0.0394) Steps 1018(1048.49) | Grad Norm 4.2577(4.2790) | Total Time 0.00(0.00)\n",
      "Iter 15530 | Time 26.1706(26.3224) | Bit/dim 3.5563(3.5665) | Xent 0.0927(0.1111) | Loss 9.2671(10.1528) | Error 0.0289(0.0384) Steps 1072(1048.29) | Grad Norm 3.3309(4.1149) | Total Time 0.00(0.00)\n",
      "Iter 15540 | Time 26.8912(26.3728) | Bit/dim 3.5578(3.5656) | Xent 0.1011(0.1111) | Loss 9.2752(9.9522) | Error 0.0367(0.0385) Steps 1006(1047.49) | Grad Norm 22.7872(4.6146) | Total Time 0.00(0.00)\n",
      "Iter 15550 | Time 26.9874(26.4204) | Bit/dim 3.5914(3.5709) | Xent 0.1365(0.1177) | Loss 9.6088(9.8262) | Error 0.0456(0.0407) Steps 1054(1049.01) | Grad Norm 15.9026(5.4770) | Total Time 0.00(0.00)\n",
      "Iter 15560 | Time 24.1558(26.2426) | Bit/dim 3.5665(3.5750) | Xent 0.1718(0.1253) | Loss 9.4925(9.7143) | Error 0.0711(0.0441) Steps 1030(1045.65) | Grad Norm 6.0142(7.7131) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0283 | Time 120.3994, Epoch Time 1580.5602(1499.8995), Bit/dim 3.5984(best: 3.5712), Xent 1.0080, Loss 4.1024, Error 0.2236(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15570 | Time 25.2603(26.0148) | Bit/dim 3.5618(3.5779) | Xent 0.1424(0.1315) | Loss 9.5195(10.6282) | Error 0.0478(0.0465) Steps 1024(1039.32) | Grad Norm 5.7220(7.1242) | Total Time 0.00(0.00)\n",
      "Iter 15580 | Time 26.4357(26.0356) | Bit/dim 3.5735(3.5772) | Xent 0.1339(0.1327) | Loss 9.4242(10.3031) | Error 0.0478(0.0467) Steps 1012(1035.62) | Grad Norm 6.4195(6.7983) | Total Time 0.00(0.00)\n",
      "Iter 15590 | Time 25.4663(25.9531) | Bit/dim 3.5588(3.5758) | Xent 0.1211(0.1301) | Loss 9.3769(10.0534) | Error 0.0400(0.0456) Steps 1054(1035.53) | Grad Norm 4.5089(6.2584) | Total Time 0.00(0.00)\n",
      "Iter 15600 | Time 26.0014(26.0012) | Bit/dim 3.5536(3.5740) | Xent 0.1147(0.1263) | Loss 9.3368(9.8788) | Error 0.0356(0.0444) Steps 1030(1036.27) | Grad Norm 3.1004(5.5438) | Total Time 0.00(0.00)\n",
      "Iter 15610 | Time 26.0057(26.0921) | Bit/dim 3.5493(3.5710) | Xent 0.1362(0.1225) | Loss 9.4604(9.7539) | Error 0.0489(0.0428) Steps 1036(1037.10) | Grad Norm 4.5363(5.0700) | Total Time 0.00(0.00)\n",
      "Iter 15620 | Time 25.9135(26.0339) | Bit/dim 3.6169(3.5715) | Xent 0.0971(0.1214) | Loss 9.5427(9.6680) | Error 0.0322(0.0423) Steps 1072(1040.10) | Grad Norm 3.4374(4.6535) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0284 | Time 120.8546, Epoch Time 1566.4291(1501.8954), Bit/dim 3.5752(best: 3.5712), Xent 0.9902, Loss 4.0703, Error 0.2216(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15630 | Time 25.5622(25.9805) | Bit/dim 3.5450(3.5699) | Xent 0.1100(0.1183) | Loss 9.2053(10.4483) | Error 0.0356(0.0410) Steps 1042(1040.11) | Grad Norm 3.0097(4.3385) | Total Time 0.00(0.00)\n",
      "Iter 15640 | Time 26.1862(26.0276) | Bit/dim 3.5722(3.5682) | Xent 0.1158(0.1155) | Loss 9.3605(10.1644) | Error 0.0467(0.0403) Steps 1036(1040.65) | Grad Norm 3.6333(4.1595) | Total Time 0.00(0.00)\n",
      "Iter 15650 | Time 25.9525(26.0312) | Bit/dim 3.5730(3.5687) | Xent 0.1179(0.1136) | Loss 9.3621(9.9592) | Error 0.0411(0.0393) Steps 1060(1042.53) | Grad Norm 2.8825(4.0471) | Total Time 0.00(0.00)\n",
      "Iter 15660 | Time 27.1778(26.2254) | Bit/dim 3.5739(3.5688) | Xent 0.1306(0.1164) | Loss 9.3762(9.8122) | Error 0.0378(0.0398) Steps 1024(1044.01) | Grad Norm 4.2362(4.3957) | Total Time 0.00(0.00)\n",
      "Iter 15670 | Time 27.3996(26.4444) | Bit/dim 3.6161(3.5767) | Xent 0.1520(0.1212) | Loss 9.5535(9.7291) | Error 0.0400(0.0410) Steps 1078(1049.36) | Grad Norm 4.4478(4.7751) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0285 | Time 121.5523, Epoch Time 1593.1404(1504.6328), Bit/dim 3.5891(best: 3.5712), Xent 0.9721, Loss 4.0751, Error 0.2265(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15680 | Time 26.2905(26.5754) | Bit/dim 3.6039(3.5783) | Xent 0.1238(0.1208) | Loss 9.3823(10.6761) | Error 0.0500(0.0410) Steps 1054(1050.70) | Grad Norm 4.7316(4.6973) | Total Time 0.00(0.00)\n",
      "Iter 15690 | Time 28.3422(26.6953) | Bit/dim 3.5816(3.5788) | Xent 0.1181(0.1210) | Loss 9.5741(10.3507) | Error 0.0378(0.0407) Steps 1048(1050.39) | Grad Norm 5.7315(4.6072) | Total Time 0.00(0.00)\n",
      "Iter 15700 | Time 26.9758(26.7547) | Bit/dim 3.5618(3.5799) | Xent 0.1391(0.1214) | Loss 9.4776(10.1083) | Error 0.0533(0.0416) Steps 1084(1050.30) | Grad Norm 5.0969(4.6219) | Total Time 0.00(0.00)\n",
      "Iter 15710 | Time 27.0724(26.8006) | Bit/dim 3.5559(3.5769) | Xent 0.1434(0.1236) | Loss 9.4378(9.9233) | Error 0.0433(0.0423) Steps 1096(1054.80) | Grad Norm 3.7430(5.1066) | Total Time 0.00(0.00)\n",
      "Iter 15720 | Time 26.9356(26.8599) | Bit/dim 3.5969(3.5827) | Xent 0.1447(0.1234) | Loss 9.5481(9.8111) | Error 0.0489(0.0420) Steps 1054(1058.14) | Grad Norm 5.2029(5.8266) | Total Time 0.00(0.00)\n",
      "Iter 15730 | Time 27.0496(27.0088) | Bit/dim 3.5623(3.5825) | Xent 0.1246(0.1261) | Loss 9.2823(9.7054) | Error 0.0456(0.0434) Steps 1060(1059.86) | Grad Norm 5.2222(6.7310) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0286 | Time 123.9794, Epoch Time 1630.0467(1508.3952), Bit/dim 3.5973(best: 3.5712), Xent 0.9897, Loss 4.0922, Error 0.2248(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15740 | Time 29.1621(27.1156) | Bit/dim 3.6379(3.5857) | Xent 0.1215(0.1293) | Loss 9.6951(10.5188) | Error 0.0422(0.0446) Steps 1156(1063.45) | Grad Norm 7.9696(8.8272) | Total Time 0.00(0.00)\n",
      "Iter 15750 | Time 27.7350(27.2875) | Bit/dim 3.5827(3.5903) | Xent 0.1510(0.1316) | Loss 9.2854(10.2554) | Error 0.0556(0.0460) Steps 1084(1066.49) | Grad Norm 13.0472(8.3556) | Total Time 0.00(0.00)\n",
      "Iter 15760 | Time 27.5054(27.4242) | Bit/dim 3.6038(3.5940) | Xent 0.1365(0.1336) | Loss 9.4791(10.0613) | Error 0.0467(0.0466) Steps 1048(1070.26) | Grad Norm 4.0112(8.7214) | Total Time 0.00(0.00)\n",
      "Iter 15770 | Time 28.3022(27.6331) | Bit/dim 3.6070(3.5961) | Xent 0.1428(0.1343) | Loss 9.5223(9.9091) | Error 0.0444(0.0461) Steps 1084(1067.11) | Grad Norm 20.4411(9.5679) | Total Time 0.00(0.00)\n",
      "Iter 15780 | Time 27.5311(27.4687) | Bit/dim 3.6251(3.5981) | Xent 0.1178(0.1333) | Loss 9.5249(9.8030) | Error 0.0367(0.0461) Steps 1048(1063.44) | Grad Norm 4.0975(8.5646) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0287 | Time 122.0990, Epoch Time 1655.5180(1512.8089), Bit/dim 3.5896(best: 3.5712), Xent 0.9802, Loss 4.0797, Error 0.2259(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15790 | Time 26.6436(27.2050) | Bit/dim 3.5946(3.5929) | Xent 0.1265(0.1298) | Loss 9.4476(10.6944) | Error 0.0478(0.0449) Steps 1066(1059.00) | Grad Norm 6.9135(7.6481) | Total Time 0.00(0.00)\n",
      "Iter 15800 | Time 27.5087(27.1252) | Bit/dim 3.5845(3.5895) | Xent 0.1395(0.1283) | Loss 9.5138(10.3625) | Error 0.0456(0.0452) Steps 1096(1057.17) | Grad Norm 7.2889(7.4559) | Total Time 0.00(0.00)\n",
      "Iter 15810 | Time 27.0531(27.0995) | Bit/dim 3.5809(3.5842) | Xent 0.0862(0.1256) | Loss 9.3739(10.1143) | Error 0.0267(0.0433) Steps 1072(1058.69) | Grad Norm 3.3187(6.6967) | Total Time 0.00(0.00)\n",
      "Iter 15820 | Time 25.7220(27.1100) | Bit/dim 3.5729(3.5798) | Xent 0.1298(0.1232) | Loss 9.2737(9.9271) | Error 0.0444(0.0426) Steps 1024(1056.64) | Grad Norm 12.2804(7.1350) | Total Time 0.00(0.00)\n",
      "Iter 15830 | Time 27.2448(27.0745) | Bit/dim 3.5745(3.5784) | Xent 0.1363(0.1207) | Loss 9.4739(9.7888) | Error 0.0522(0.0424) Steps 1066(1058.19) | Grad Norm 4.4471(6.4619) | Total Time 0.00(0.00)\n",
      "Iter 15840 | Time 26.7347(27.2442) | Bit/dim 3.5914(3.5801) | Xent 0.0947(0.1190) | Loss 9.3465(9.6880) | Error 0.0322(0.0420) Steps 1090(1060.83) | Grad Norm 4.6477(6.3831) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0288 | Time 122.7842, Epoch Time 1631.5582(1516.3713), Bit/dim 3.5828(best: 3.5712), Xent 0.9706, Loss 4.0681, Error 0.2231(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15850 | Time 27.7182(27.2348) | Bit/dim 3.5768(3.5790) | Xent 0.0935(0.1166) | Loss 9.3963(10.4783) | Error 0.0356(0.0406) Steps 1114(1061.57) | Grad Norm 8.2443(6.6214) | Total Time 0.00(0.00)\n",
      "Iter 15860 | Time 28.1302(27.2944) | Bit/dim 3.5960(3.5823) | Xent 0.1664(0.1212) | Loss 9.5148(10.2114) | Error 0.0622(0.0427) Steps 1084(1062.29) | Grad Norm 5.7356(8.0044) | Total Time 0.00(0.00)\n",
      "Iter 15870 | Time 26.2528(27.2572) | Bit/dim 3.6258(3.5960) | Xent 0.2103(0.1329) | Loss 9.5371(10.0534) | Error 0.0644(0.0459) Steps 1060(1064.67) | Grad Norm 12.0741(12.3855) | Total Time 0.00(0.00)\n",
      "Iter 15880 | Time 27.7760(27.1596) | Bit/dim 3.6236(3.6064) | Xent 0.1729(0.1431) | Loss 9.4561(9.9251) | Error 0.0533(0.0496) Steps 1042(1062.03) | Grad Norm 6.2822(11.3421) | Total Time 0.00(0.00)\n",
      "Iter 15890 | Time 27.4587(27.1502) | Bit/dim 3.6035(3.6047) | Xent 0.1533(0.1473) | Loss 9.4313(9.8087) | Error 0.0522(0.0511) Steps 1084(1058.46) | Grad Norm 5.6148(9.9863) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0289 | Time 120.9735, Epoch Time 1632.0953(1519.8431), Bit/dim 3.6005(best: 3.5712), Xent 0.9982, Loss 4.0996, Error 0.2268(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15900 | Time 26.5560(27.0805) | Bit/dim 3.5658(3.6004) | Xent 0.1479(0.1448) | Loss 9.4050(10.7283) | Error 0.0522(0.0508) Steps 1078(1058.71) | Grad Norm 3.2742(8.5343) | Total Time 0.00(0.00)\n",
      "Iter 15910 | Time 27.3368(27.1130) | Bit/dim 3.6007(3.5971) | Xent 0.1126(0.1384) | Loss 9.4895(10.3958) | Error 0.0433(0.0481) Steps 1024(1057.52) | Grad Norm 3.6104(7.7048) | Total Time 0.00(0.00)\n",
      "Iter 15920 | Time 26.8001(27.0790) | Bit/dim 3.5410(3.5905) | Xent 0.1224(0.1333) | Loss 9.2778(10.1413) | Error 0.0367(0.0460) Steps 1024(1058.60) | Grad Norm 3.0712(7.2115) | Total Time 0.00(0.00)\n",
      "Iter 15930 | Time 26.0360(26.9774) | Bit/dim 3.5838(3.5874) | Xent 0.1068(0.1315) | Loss 9.3483(9.9416) | Error 0.0456(0.0457) Steps 1066(1058.69) | Grad Norm 3.9603(6.9210) | Total Time 0.00(0.00)\n",
      "Iter 15940 | Time 25.9139(26.9203) | Bit/dim 3.5303(3.5847) | Xent 0.1270(0.1302) | Loss 9.4195(9.8220) | Error 0.0489(0.0457) Steps 1054(1060.04) | Grad Norm 5.2992(6.4461) | Total Time 0.00(0.00)\n",
      "Iter 15950 | Time 27.1726(26.9085) | Bit/dim 3.5463(3.5799) | Xent 0.0964(0.1267) | Loss 9.2536(9.7086) | Error 0.0344(0.0442) Steps 1054(1060.92) | Grad Norm 2.8084(5.8973) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0290 | Time 121.4081, Epoch Time 1619.4598(1522.8316), Bit/dim 3.5814(best: 3.5712), Xent 0.9929, Loss 4.0778, Error 0.2241(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15960 | Time 26.8553(26.9380) | Bit/dim 3.5612(3.5772) | Xent 0.0893(0.1250) | Loss 9.4924(10.4899) | Error 0.0311(0.0437) Steps 1054(1058.85) | Grad Norm 3.1428(5.3921) | Total Time 0.00(0.00)\n",
      "Iter 15970 | Time 26.0359(26.8690) | Bit/dim 3.5842(3.5770) | Xent 0.1009(0.1217) | Loss 9.2813(10.2016) | Error 0.0311(0.0423) Steps 1042(1057.92) | Grad Norm 5.9039(4.9634) | Total Time 0.00(0.00)\n",
      "Iter 15980 | Time 26.0571(26.8778) | Bit/dim 3.5383(3.5734) | Xent 0.0810(0.1174) | Loss 9.3570(9.9949) | Error 0.0233(0.0405) Steps 1048(1060.38) | Grad Norm 4.2451(4.6212) | Total Time 0.00(0.00)\n",
      "Iter 15990 | Time 27.5060(26.9221) | Bit/dim 3.5766(3.5716) | Xent 0.1030(0.1173) | Loss 9.4201(9.8408) | Error 0.0389(0.0409) Steps 1084(1058.41) | Grad Norm 5.1381(4.7673) | Total Time 0.00(0.00)\n",
      "Iter 16000 | Time 28.3785(27.0011) | Bit/dim 3.5519(3.5698) | Xent 0.0854(0.1146) | Loss 9.4195(9.7154) | Error 0.0300(0.0404) Steps 1114(1060.52) | Grad Norm 2.2717(4.9682) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0291 | Time 121.4315, Epoch Time 1626.4898(1525.9413), Bit/dim 3.5792(best: 3.5712), Xent 0.9844, Loss 4.0714, Error 0.2218(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16010 | Time 27.9406(27.1297) | Bit/dim 3.5776(3.5709) | Xent 0.0891(0.1130) | Loss 9.4279(10.6448) | Error 0.0367(0.0399) Steps 1024(1060.12) | Grad Norm 5.2809(5.0215) | Total Time 0.00(0.00)\n",
      "Iter 16020 | Time 27.5964(27.0875) | Bit/dim 3.5558(3.5728) | Xent 0.1328(0.1143) | Loss 9.3543(10.3185) | Error 0.0433(0.0404) Steps 1024(1058.82) | Grad Norm 3.6134(4.7753) | Total Time 0.00(0.00)\n",
      "Iter 16030 | Time 26.7089(27.1300) | Bit/dim 3.5775(3.5731) | Xent 0.1319(0.1144) | Loss 9.4793(10.0844) | Error 0.0489(0.0405) Steps 1066(1056.07) | Grad Norm 4.5005(4.6638) | Total Time 0.00(0.00)\n",
      "Iter 16040 | Time 27.5002(27.1171) | Bit/dim 3.5832(3.5703) | Xent 0.1074(0.1139) | Loss 9.4571(9.8982) | Error 0.0344(0.0403) Steps 1078(1057.54) | Grad Norm 4.2195(4.6512) | Total Time 0.00(0.00)\n",
      "Iter 16050 | Time 27.1135(27.0663) | Bit/dim 3.5568(3.5654) | Xent 0.1381(0.1145) | Loss 9.3284(9.7506) | Error 0.0456(0.0398) Steps 1084(1057.01) | Grad Norm 6.1090(5.1252) | Total Time 0.00(0.00)\n",
      "Iter 16060 | Time 25.6624(26.9166) | Bit/dim 3.5974(3.5672) | Xent 0.1441(0.1164) | Loss 9.4813(9.6578) | Error 0.0544(0.0407) Steps 1036(1057.52) | Grad Norm 5.3476(5.4169) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0292 | Time 120.9970, Epoch Time 1623.9797(1528.8825), Bit/dim 3.5796(best: 3.5712), Xent 1.0108, Loss 4.0851, Error 0.2260(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16070 | Time 26.4034(26.8677) | Bit/dim 3.6146(3.5696) | Xent 0.1160(0.1154) | Loss 9.5253(10.4310) | Error 0.0389(0.0402) Steps 1078(1062.36) | Grad Norm 4.4619(6.3489) | Total Time 0.00(0.00)\n",
      "Iter 16080 | Time 25.6564(26.8402) | Bit/dim 3.5827(3.5701) | Xent 0.1299(0.1187) | Loss 9.3564(10.1516) | Error 0.0456(0.0413) Steps 1060(1060.46) | Grad Norm 4.1620(5.9633) | Total Time 0.00(0.00)\n",
      "Iter 16090 | Time 26.9241(26.7628) | Bit/dim 3.5659(3.5692) | Xent 0.1289(0.1194) | Loss 9.5193(9.9502) | Error 0.0489(0.0414) Steps 1096(1061.30) | Grad Norm 3.8503(5.4575) | Total Time 0.00(0.00)\n",
      "Iter 16100 | Time 27.4275(26.8032) | Bit/dim 3.6103(3.5698) | Xent 0.1001(0.1172) | Loss 9.3148(9.8067) | Error 0.0344(0.0404) Steps 1042(1057.38) | Grad Norm 4.6961(5.2084) | Total Time 0.00(0.00)\n",
      "Iter 16110 | Time 27.0342(26.8695) | Bit/dim 3.5828(3.5706) | Xent 0.1001(0.1175) | Loss 9.5140(9.7164) | Error 0.0367(0.0406) Steps 1072(1057.44) | Grad Norm 4.5892(4.9704) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0293 | Time 121.0911, Epoch Time 1613.6505(1531.4255), Bit/dim 3.5784(best: 3.5712), Xent 1.0261, Loss 4.0914, Error 0.2250(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16120 | Time 27.2627(26.9062) | Bit/dim 3.5657(3.5703) | Xent 0.1187(0.1157) | Loss 9.4780(10.6600) | Error 0.0378(0.0400) Steps 1036(1057.73) | Grad Norm 3.9121(5.0301) | Total Time 0.00(0.00)\n",
      "Iter 16130 | Time 27.5478(26.8507) | Bit/dim 3.5775(3.5696) | Xent 0.1077(0.1193) | Loss 9.3196(10.3279) | Error 0.0400(0.0415) Steps 1102(1055.19) | Grad Norm 3.4088(6.3729) | Total Time 0.00(0.00)\n",
      "Iter 16140 | Time 26.5048(26.7780) | Bit/dim 3.6046(3.5716) | Xent 0.1043(0.1178) | Loss 9.4653(10.0977) | Error 0.0367(0.0410) Steps 1030(1053.36) | Grad Norm 5.0034(5.9866) | Total Time 0.00(0.00)\n",
      "Iter 16150 | Time 26.5381(26.6967) | Bit/dim 3.5735(3.5713) | Xent 0.1163(0.1198) | Loss 9.4495(9.9126) | Error 0.0467(0.0419) Steps 1036(1049.52) | Grad Norm 4.0107(5.6348) | Total Time 0.00(0.00)\n",
      "Iter 16160 | Time 26.8395(26.7151) | Bit/dim 3.5824(3.5682) | Xent 0.1053(0.1181) | Loss 9.4267(9.7655) | Error 0.0300(0.0410) Steps 1054(1052.24) | Grad Norm 3.5701(5.4007) | Total Time 0.00(0.00)\n",
      "Iter 16170 | Time 25.9726(26.6375) | Bit/dim 3.5873(3.5683) | Xent 0.0887(0.1165) | Loss 9.5350(9.6772) | Error 0.0367(0.0404) Steps 1048(1052.08) | Grad Norm 5.3507(5.2150) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0294 | Time 121.2202, Epoch Time 1604.1441(1533.6071), Bit/dim 3.5749(best: 3.5712), Xent 0.9861, Loss 4.0679, Error 0.2200(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16180 | Time 28.1954(26.6660) | Bit/dim 3.5511(3.5661) | Xent 0.1031(0.1131) | Loss 9.2927(10.4452) | Error 0.0378(0.0392) Steps 1006(1049.85) | Grad Norm 6.5929(4.9836) | Total Time 0.00(0.00)\n",
      "Iter 16190 | Time 26.3706(26.6584) | Bit/dim 3.5470(3.5621) | Xent 0.0939(0.1100) | Loss 9.3364(10.1477) | Error 0.0278(0.0379) Steps 1024(1049.04) | Grad Norm 4.0619(4.6258) | Total Time 0.00(0.00)\n",
      "Iter 16200 | Time 27.3863(26.7607) | Bit/dim 3.5767(3.5677) | Xent 0.1177(0.1090) | Loss 9.3709(9.9576) | Error 0.0344(0.0372) Steps 1024(1051.81) | Grad Norm 3.5806(4.4132) | Total Time 0.00(0.00)\n",
      "Iter 16210 | Time 26.9200(26.7616) | Bit/dim 3.5619(3.5653) | Xent 0.1096(0.1085) | Loss 9.3756(9.8020) | Error 0.0367(0.0370) Steps 1042(1052.90) | Grad Norm 3.3562(4.1267) | Total Time 0.00(0.00)\n",
      "Iter 16220 | Time 26.4659(26.7778) | Bit/dim 3.5461(3.5666) | Xent 0.0955(0.1105) | Loss 9.3091(9.6931) | Error 0.0278(0.0377) Steps 1024(1051.42) | Grad Norm 4.1833(4.2127) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0295 | Time 119.5750, Epoch Time 1612.6162(1535.9773), Bit/dim 3.5735(best: 3.5712), Xent 1.0139, Loss 4.0805, Error 0.2259(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16230 | Time 25.7345(26.7006) | Bit/dim 3.5608(3.5620) | Xent 0.0976(0.1101) | Loss 9.3091(10.6102) | Error 0.0356(0.0375) Steps 1036(1048.64) | Grad Norm 5.3492(4.3516) | Total Time 0.00(0.00)\n",
      "Iter 16240 | Time 26.7554(26.6447) | Bit/dim 3.5493(3.5613) | Xent 0.1194(0.1108) | Loss 9.3085(10.2782) | Error 0.0422(0.0379) Steps 1042(1048.43) | Grad Norm 3.2336(4.4838) | Total Time 0.00(0.00)\n",
      "Iter 16250 | Time 27.1942(26.6259) | Bit/dim 3.5612(3.5633) | Xent 0.0932(0.1095) | Loss 9.3925(10.0418) | Error 0.0267(0.0374) Steps 1018(1047.90) | Grad Norm 2.9489(4.3871) | Total Time 0.00(0.00)\n",
      "Iter 16260 | Time 27.4160(26.5698) | Bit/dim 3.5930(3.5652) | Xent 0.0997(0.1095) | Loss 9.3713(9.8625) | Error 0.0344(0.0373) Steps 1084(1048.72) | Grad Norm 4.2570(4.6866) | Total Time 0.00(0.00)\n",
      "Iter 16270 | Time 26.7218(26.5549) | Bit/dim 3.5367(3.5653) | Xent 0.1014(0.1097) | Loss 9.2718(9.7275) | Error 0.0356(0.0378) Steps 1036(1051.53) | Grad Norm 3.5146(4.5722) | Total Time 0.00(0.00)\n",
      "Iter 16280 | Time 28.3650(26.6551) | Bit/dim 3.5740(3.5623) | Xent 0.1180(0.1107) | Loss 9.2914(9.6299) | Error 0.0456(0.0385) Steps 1012(1052.49) | Grad Norm 4.2264(4.5920) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0296 | Time 121.9229, Epoch Time 1599.6021(1537.8861), Bit/dim 3.5726(best: 3.5712), Xent 1.0003, Loss 4.0727, Error 0.2215(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16290 | Time 27.3223(26.7255) | Bit/dim 3.5625(3.5624) | Xent 0.1072(0.1090) | Loss 9.4488(10.4304) | Error 0.0311(0.0376) Steps 1060(1055.34) | Grad Norm 3.8388(4.3640) | Total Time 0.00(0.00)\n",
      "Iter 16300 | Time 27.0607(26.6671) | Bit/dim 3.5975(3.5621) | Xent 0.1061(0.1067) | Loss 9.4933(10.1616) | Error 0.0356(0.0365) Steps 1060(1057.26) | Grad Norm 4.3012(4.3302) | Total Time 0.00(0.00)\n",
      "Iter 16310 | Time 26.2978(26.5355) | Bit/dim 3.5322(3.5611) | Xent 0.0844(0.1059) | Loss 9.2589(9.9419) | Error 0.0267(0.0365) Steps 1066(1053.16) | Grad Norm 2.3854(4.0130) | Total Time 0.00(0.00)\n",
      "Iter 16320 | Time 27.2020(26.5434) | Bit/dim 3.5531(3.5619) | Xent 0.1000(0.1040) | Loss 9.3697(9.7894) | Error 0.0344(0.0356) Steps 1066(1053.54) | Grad Norm 3.4106(3.9647) | Total Time 0.00(0.00)\n",
      "Iter 16330 | Time 27.5901(26.7361) | Bit/dim 3.5864(3.5631) | Xent 0.1261(0.1047) | Loss 9.3956(9.6852) | Error 0.0389(0.0358) Steps 1036(1056.59) | Grad Norm 4.4968(3.9649) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0297 | Time 120.9039, Epoch Time 1608.7316(1540.0115), Bit/dim 3.5710(best: 3.5712), Xent 1.0251, Loss 4.0835, Error 0.2257(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16340 | Time 26.9066(26.7729) | Bit/dim 3.5125(3.5640) | Xent 0.0935(0.1033) | Loss 9.2313(10.6093) | Error 0.0367(0.0356) Steps 1078(1056.01) | Grad Norm 5.5791(3.9634) | Total Time 0.00(0.00)\n",
      "Iter 16350 | Time 27.9769(26.7835) | Bit/dim 3.5680(3.5621) | Xent 0.1035(0.1036) | Loss 9.4309(10.2860) | Error 0.0378(0.0356) Steps 1042(1056.85) | Grad Norm 3.1487(3.9863) | Total Time 0.00(0.00)\n",
      "Iter 16360 | Time 27.2201(26.8310) | Bit/dim 3.5994(3.5625) | Xent 0.1142(0.1045) | Loss 9.4085(10.0452) | Error 0.0367(0.0360) Steps 1096(1056.39) | Grad Norm 3.6676(3.9361) | Total Time 0.00(0.00)\n",
      "Iter 16370 | Time 25.4742(26.7566) | Bit/dim 3.5652(3.5631) | Xent 0.1140(0.1050) | Loss 9.3767(9.8729) | Error 0.0411(0.0363) Steps 1048(1056.98) | Grad Norm 3.0545(3.9231) | Total Time 0.00(0.00)\n",
      "Iter 16380 | Time 26.7163(26.6884) | Bit/dim 3.5637(3.5639) | Xent 0.1024(0.1040) | Loss 9.4953(9.7471) | Error 0.0422(0.0359) Steps 1108(1059.51) | Grad Norm 2.5333(3.7135) | Total Time 0.00(0.00)\n",
      "Iter 16390 | Time 25.5118(26.6258) | Bit/dim 3.5317(3.5605) | Xent 0.1114(0.1061) | Loss 9.1984(9.6418) | Error 0.0400(0.0366) Steps 1030(1057.81) | Grad Norm 3.7518(3.7548) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0298 | Time 120.8781, Epoch Time 1606.7272(1542.0129), Bit/dim 3.5673(best: 3.5710), Xent 0.9985, Loss 4.0666, Error 0.2227(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16400 | Time 26.0144(26.6267) | Bit/dim 3.5624(3.5612) | Xent 0.1145(0.1075) | Loss 9.4437(10.4319) | Error 0.0344(0.0373) Steps 1054(1051.99) | Grad Norm 4.4990(3.8899) | Total Time 0.00(0.00)\n",
      "Iter 16410 | Time 26.3893(26.5473) | Bit/dim 3.5374(3.5622) | Xent 0.1085(0.1054) | Loss 9.2578(10.1524) | Error 0.0422(0.0366) Steps 1060(1051.66) | Grad Norm 2.7666(3.7030) | Total Time 0.00(0.00)\n",
      "Iter 16420 | Time 26.8572(26.5909) | Bit/dim 3.5695(3.5605) | Xent 0.0764(0.1046) | Loss 9.2309(9.9503) | Error 0.0233(0.0362) Steps 1042(1053.37) | Grad Norm 3.2935(3.6679) | Total Time 0.00(0.00)\n",
      "Iter 16430 | Time 27.1190(26.5827) | Bit/dim 3.5525(3.5604) | Xent 0.1224(0.1051) | Loss 9.4431(9.8064) | Error 0.0422(0.0362) Steps 1084(1053.73) | Grad Norm 3.5850(3.6418) | Total Time 0.00(0.00)\n",
      "Iter 16440 | Time 26.3530(26.6616) | Bit/dim 3.5741(3.5613) | Xent 0.0633(0.1041) | Loss 9.1931(9.6920) | Error 0.0200(0.0358) Steps 1024(1050.15) | Grad Norm 4.0184(3.7066) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0299 | Time 121.9400, Epoch Time 1605.2240(1543.9093), Bit/dim 3.5702(best: 3.5673), Xent 1.0085, Loss 4.0744, Error 0.2248(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16450 | Time 26.0241(26.6349) | Bit/dim 3.5827(3.5620) | Xent 0.0865(0.1016) | Loss 9.3812(10.5923) | Error 0.0278(0.0348) Steps 1024(1051.01) | Grad Norm 2.9663(3.9978) | Total Time 0.00(0.00)\n",
      "Iter 16460 | Time 27.2542(26.6101) | Bit/dim 3.5707(3.5611) | Xent 0.1054(0.1021) | Loss 9.4098(10.2742) | Error 0.0322(0.0353) Steps 1096(1053.91) | Grad Norm 4.5901(4.1103) | Total Time 0.00(0.00)\n",
      "Iter 16470 | Time 26.6690(26.6385) | Bit/dim 3.5736(3.5605) | Xent 0.1146(0.1008) | Loss 9.4060(10.0393) | Error 0.0456(0.0349) Steps 1036(1055.01) | Grad Norm 3.0347(4.1192) | Total Time 0.00(0.00)\n",
      "Iter 16480 | Time 26.0282(26.5200) | Bit/dim 3.5446(3.5591) | Xent 0.1340(0.1030) | Loss 9.4349(9.8661) | Error 0.0456(0.0354) Steps 1060(1054.29) | Grad Norm 3.6490(3.8998) | Total Time 0.00(0.00)\n",
      "Iter 16490 | Time 26.7609(26.5852) | Bit/dim 3.5646(3.5637) | Xent 0.0958(0.1020) | Loss 9.3858(9.7314) | Error 0.0322(0.0353) Steps 1042(1055.22) | Grad Norm 3.3764(3.8561) | Total Time 0.00(0.00)\n",
      "Iter 16500 | Time 26.1835(26.6276) | Bit/dim 3.5728(3.5615) | Xent 0.1031(0.1023) | Loss 9.4444(9.6362) | Error 0.0333(0.0353) Steps 1060(1052.57) | Grad Norm 3.2158(3.8147) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0300 | Time 123.1957, Epoch Time 1603.5010(1545.6970), Bit/dim 3.5761(best: 3.5673), Xent 1.0527, Loss 4.1024, Error 0.2276(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16510 | Time 26.9586(26.6453) | Bit/dim 3.5735(3.5627) | Xent 0.1162(0.1026) | Loss 9.4339(10.3957) | Error 0.0489(0.0358) Steps 1078(1054.74) | Grad Norm 4.5461(3.9056) | Total Time 0.00(0.00)\n",
      "Iter 16520 | Time 26.8562(26.5976) | Bit/dim 3.5905(3.5645) | Xent 0.0865(0.1022) | Loss 9.3892(10.1224) | Error 0.0278(0.0355) Steps 1042(1055.53) | Grad Norm 3.5899(3.9525) | Total Time 0.00(0.00)\n",
      "Iter 16530 | Time 26.6285(26.5368) | Bit/dim 3.5576(3.5647) | Xent 0.1076(0.1027) | Loss 9.3027(9.9117) | Error 0.0367(0.0355) Steps 1072(1053.31) | Grad Norm 3.5061(3.9378) | Total Time 0.00(0.00)\n",
      "Iter 16540 | Time 26.3476(26.5209) | Bit/dim 3.5571(3.5648) | Xent 0.1303(0.1063) | Loss 9.2048(9.7749) | Error 0.0456(0.0372) Steps 1042(1053.69) | Grad Norm 5.9456(4.2108) | Total Time 0.00(0.00)\n",
      "Iter 16550 | Time 26.7807(26.6188) | Bit/dim 3.5951(3.5623) | Xent 0.0987(0.1068) | Loss 9.5412(9.6691) | Error 0.0322(0.0380) Steps 1042(1055.22) | Grad Norm 3.8824(4.1376) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0301 | Time 122.6578, Epoch Time 1605.2506(1547.4836), Bit/dim 3.5805(best: 3.5673), Xent 1.0548, Loss 4.1079, Error 0.2257(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16560 | Time 27.5490(26.7294) | Bit/dim 3.5863(3.5629) | Xent 0.1390(0.1075) | Loss 9.5105(10.6179) | Error 0.0467(0.0381) Steps 1066(1055.89) | Grad Norm 4.8805(4.0945) | Total Time 0.00(0.00)\n",
      "Iter 16570 | Time 27.9265(26.8114) | Bit/dim 3.6011(3.5647) | Xent 0.0769(0.1073) | Loss 9.3591(10.2928) | Error 0.0278(0.0371) Steps 1006(1055.36) | Grad Norm 4.0302(4.3626) | Total Time 0.00(0.00)\n",
      "Iter 16580 | Time 27.7737(27.1112) | Bit/dim 3.5711(3.5662) | Xent 0.1113(0.1096) | Loss 9.4326(10.0782) | Error 0.0378(0.0378) Steps 1066(1062.26) | Grad Norm 4.3780(4.8377) | Total Time 0.00(0.00)\n",
      "Iter 16590 | Time 27.2085(27.3059) | Bit/dim 3.5319(3.5678) | Xent 0.1290(0.1117) | Loss 9.3304(9.9142) | Error 0.0456(0.0387) Steps 1048(1066.45) | Grad Norm 3.1903(5.6402) | Total Time 0.00(0.00)\n",
      "Iter 16600 | Time 29.3297(27.5011) | Bit/dim 3.6030(3.5722) | Xent 0.1352(0.1154) | Loss 9.5030(9.8086) | Error 0.0489(0.0403) Steps 1114(1073.31) | Grad Norm 17.4982(6.8922) | Total Time 0.00(0.00)\n",
      "Iter 16610 | Time 28.9915(27.9503) | Bit/dim 3.6027(3.5790) | Xent 0.1440(0.1228) | Loss 9.3689(9.7330) | Error 0.0578(0.0427) Steps 1096(1084.50) | Grad Norm 11.8827(9.2305) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0302 | Time 130.3525, Epoch Time 1685.1564(1551.6138), Bit/dim 3.5956(best: 3.5673), Xent 1.0439, Loss 4.1176, Error 0.2319(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16620 | Time 29.4656(28.2587) | Bit/dim 3.6259(3.5865) | Xent 0.1687(0.1256) | Loss 9.6300(10.5565) | Error 0.0567(0.0435) Steps 1126(1090.88) | Grad Norm 23.0283(10.7251) | Total Time 0.00(0.00)\n",
      "Iter 16630 | Time 31.2743(28.7253) | Bit/dim 3.6190(3.5907) | Xent 0.1841(0.1321) | Loss 9.7124(10.3012) | Error 0.0667(0.0457) Steps 1174(1103.71) | Grad Norm 21.8984(12.1719) | Total Time 0.00(0.00)\n",
      "Iter 16640 | Time 28.8108(29.0980) | Bit/dim 3.5610(3.5933) | Xent 0.1642(0.1372) | Loss 9.4880(10.1141) | Error 0.0611(0.0476) Steps 1090(1111.74) | Grad Norm 18.9990(12.6195) | Total Time 0.00(0.00)\n",
      "Iter 16650 | Time 28.5015(28.9567) | Bit/dim 3.5504(3.5916) | Xent 0.1335(0.1335) | Loss 9.3867(9.9417) | Error 0.0444(0.0469) Steps 1060(1107.38) | Grad Norm 8.0068(11.7295) | Total Time 0.00(0.00)\n",
      "Iter 16660 | Time 29.0827(28.8082) | Bit/dim 3.5937(3.5898) | Xent 0.1408(0.1326) | Loss 9.4497(9.8199) | Error 0.0433(0.0468) Steps 1072(1099.97) | Grad Norm 25.9790(11.3263) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0303 | Time 130.3615, Epoch Time 1756.1493(1557.7499), Bit/dim 3.6007(best: 3.5673), Xent 1.0264, Loss 4.1139, Error 0.2233(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16670 | Time 27.5619(28.7820) | Bit/dim 3.6164(3.5931) | Xent 0.1033(0.1297) | Loss 9.4914(10.7423) | Error 0.0389(0.0458) Steps 1108(1099.58) | Grad Norm 11.8581(10.1524) | Total Time 0.00(0.00)\n",
      "Iter 16680 | Time 30.1882(29.0729) | Bit/dim 3.5723(3.5964) | Xent 0.1284(0.1290) | Loss 9.5696(10.4328) | Error 0.0467(0.0459) Steps 1150(1108.91) | Grad Norm 34.1212(11.2901) | Total Time 0.00(0.00)\n",
      "Iter 16690 | Time 28.7190(29.0362) | Bit/dim 3.6106(3.5964) | Xent 0.1070(0.1302) | Loss 9.4284(10.1976) | Error 0.0389(0.0460) Steps 1090(1112.08) | Grad Norm 8.6717(10.9242) | Total Time 0.00(0.00)\n",
      "Iter 16700 | Time 28.7983(29.3156) | Bit/dim 3.6335(3.6029) | Xent 0.1347(0.1370) | Loss 9.5165(10.0461) | Error 0.0467(0.0475) Steps 1108(1119.83) | Grad Norm 17.3253(12.1265) | Total Time 0.00(0.00)\n",
      "Iter 16710 | Time 29.2759(29.4646) | Bit/dim 3.5812(3.6026) | Xent 0.1497(0.1358) | Loss 9.5066(9.9118) | Error 0.0544(0.0471) Steps 1144(1117.29) | Grad Norm 15.5948(15.2299) | Total Time 0.00(0.00)\n",
      "Iter 16720 | Time 27.9464(29.2450) | Bit/dim 3.5796(3.6009) | Xent 0.1421(0.1340) | Loss 9.5725(9.8067) | Error 0.0511(0.0473) Steps 1138(1115.93) | Grad Norm 6.9059(14.6628) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0304 | Time 129.5799, Epoch Time 1765.1664(1563.9724), Bit/dim 3.6116(best: 3.5673), Xent 1.0295, Loss 4.1264, Error 0.2273(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16730 | Time 31.5611(29.4105) | Bit/dim 3.8544(3.6185) | Xent 0.4910(0.1557) | Loss 10.6435(10.6632) | Error 0.1589(0.0539) Steps 1192(1123.94) | Grad Norm 87.0074(28.8582) | Total Time 0.00(0.00)\n",
      "Iter 16740 | Time 30.0443(29.7797) | Bit/dim 3.7612(3.6670) | Xent 0.4387(0.2373) | Loss 10.0252(10.5638) | Error 0.1589(0.0783) Steps 1030(1123.96) | Grad Norm 15.4568(40.5803) | Total Time 0.00(0.00)\n",
      "Iter 16750 | Time 29.0171(29.5360) | Bit/dim 3.7011(3.6879) | Xent 0.3010(0.2557) | Loss 9.6296(10.3888) | Error 0.1000(0.0848) Steps 1096(1112.83) | Grad Norm 8.5277(33.4840) | Total Time 0.00(0.00)\n",
      "Iter 16760 | Time 27.6594(29.1317) | Bit/dim 3.6649(3.6817) | Xent 0.2008(0.2472) | Loss 9.5850(10.1970) | Error 0.0689(0.0841) Steps 1018(1097.28) | Grad Norm 9.1443(26.8693) | Total Time 0.00(0.00)\n",
      "Iter 16770 | Time 28.5297(28.7303) | Bit/dim 3.6361(3.6734) | Xent 0.1543(0.2258) | Loss 9.4675(10.0305) | Error 0.0522(0.0776) Steps 1042(1083.32) | Grad Norm 73.5685(23.4717) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0305 | Time 120.5034, Epoch Time 1728.7198(1568.9148), Bit/dim 3.6262(best: 3.5673), Xent 0.9440, Loss 4.0982, Error 0.2235(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16780 | Time 27.7659(28.4432) | Bit/dim 3.5933(3.6605) | Xent 0.1064(0.2055) | Loss 9.4200(10.8735) | Error 0.0433(0.0715) Steps 1024(1073.94) | Grad Norm 4.9427(19.2630) | Total Time 0.00(0.00)\n",
      "Iter 16790 | Time 28.7721(28.2808) | Bit/dim 3.6087(3.6477) | Xent 0.1506(0.1878) | Loss 9.4158(10.4994) | Error 0.0511(0.0655) Steps 1054(1073.56) | Grad Norm 5.1970(16.2831) | Total Time 0.00(0.00)\n",
      "Iter 16800 | Time 28.0326(28.2265) | Bit/dim 3.6024(3.6338) | Xent 0.1187(0.1727) | Loss 9.4838(10.2152) | Error 0.0389(0.0598) Steps 1084(1072.43) | Grad Norm 4.7263(13.1185) | Total Time 0.00(0.00)\n",
      "Iter 16810 | Time 28.0549(28.0448) | Bit/dim 3.5852(3.6233) | Xent 0.1847(0.1596) | Loss 9.5068(10.0197) | Error 0.0667(0.0551) Steps 1060(1070.05) | Grad Norm 4.6209(11.2053) | Total Time 0.00(0.00)\n",
      "Iter 16820 | Time 28.0607(27.9758) | Bit/dim 3.5499(3.6125) | Xent 0.1298(0.1468) | Loss 9.3795(9.8665) | Error 0.0422(0.0504) Steps 1066(1067.31) | Grad Norm 4.5635(9.4114) | Total Time 0.00(0.00)\n",
      "Iter 16830 | Time 28.0672(27.9271) | Bit/dim 3.5614(3.6040) | Xent 0.1131(0.1396) | Loss 9.4666(9.7565) | Error 0.0333(0.0477) Steps 1054(1062.60) | Grad Norm 5.8088(8.2805) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0306 | Time 121.9907, Epoch Time 1668.5590(1571.9041), Bit/dim 3.5924(best: 3.5673), Xent 0.9996, Loss 4.0922, Error 0.2209(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16840 | Time 27.9304(27.9557) | Bit/dim 3.5909(3.5970) | Xent 0.1274(0.1343) | Loss 9.4980(10.5330) | Error 0.0467(0.0464) Steps 1048(1065.15) | Grad Norm 6.2133(8.0162) | Total Time 0.00(0.00)\n",
      "Iter 16850 | Time 27.3357(27.8758) | Bit/dim 3.5953(3.5922) | Xent 0.1214(0.1303) | Loss 9.6184(10.2462) | Error 0.0433(0.0456) Steps 1096(1062.60) | Grad Norm 4.1176(7.2470) | Total Time 0.00(0.00)\n",
      "Iter 16860 | Time 27.9385(27.7986) | Bit/dim 3.6338(3.5926) | Xent 0.1180(0.1284) | Loss 9.4847(10.0379) | Error 0.0444(0.0445) Steps 1090(1063.07) | Grad Norm 3.0255(6.9455) | Total Time 0.00(0.00)\n",
      "Iter 16870 | Time 27.3261(27.7717) | Bit/dim 3.5615(3.5907) | Xent 0.1181(0.1270) | Loss 9.3362(9.8764) | Error 0.0433(0.0438) Steps 1084(1063.91) | Grad Norm 6.8273(7.2720) | Total Time 0.00(0.00)\n",
      "Iter 16880 | Time 27.7774(27.8197) | Bit/dim 3.6001(3.5908) | Xent 0.1135(0.1246) | Loss 9.5014(9.7682) | Error 0.0378(0.0426) Steps 1066(1067.22) | Grad Norm 4.0043(6.6520) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0307 | Time 120.4049, Epoch Time 1666.6082(1574.7452), Bit/dim 3.5945(best: 3.5673), Xent 1.0251, Loss 4.1071, Error 0.2229(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16890 | Time 27.6718(27.8048) | Bit/dim 3.5828(3.5908) | Xent 0.1312(0.1205) | Loss 9.4941(10.6910) | Error 0.0433(0.0411) Steps 1042(1066.50) | Grad Norm 6.5021(7.2978) | Total Time 0.00(0.00)\n",
      "Iter 16900 | Time 27.8593(27.8234) | Bit/dim 3.5923(3.5876) | Xent 0.0978(0.1173) | Loss 9.6156(10.3667) | Error 0.0344(0.0400) Steps 1054(1067.51) | Grad Norm 3.1369(6.8839) | Total Time 0.00(0.00)\n",
      "Iter 16910 | Time 28.8771(27.8381) | Bit/dim 3.5905(3.5876) | Xent 0.1070(0.1140) | Loss 9.5494(10.1289) | Error 0.0411(0.0389) Steps 1078(1070.11) | Grad Norm 4.3413(6.1262) | Total Time 0.00(0.00)\n",
      "Iter 16920 | Time 28.2241(27.8985) | Bit/dim 3.5711(3.5848) | Xent 0.1096(0.1137) | Loss 9.3751(9.9436) | Error 0.0378(0.0392) Steps 1078(1070.76) | Grad Norm 6.0343(6.1146) | Total Time 0.00(0.00)\n",
      "Iter 16930 | Time 27.2715(27.7977) | Bit/dim 3.5937(3.5844) | Xent 0.1038(0.1140) | Loss 9.3515(9.7964) | Error 0.0389(0.0397) Steps 1072(1069.02) | Grad Norm 4.6808(6.3250) | Total Time 0.00(0.00)\n",
      "Iter 16940 | Time 27.0938(27.7221) | Bit/dim 3.5974(3.5829) | Xent 0.1185(0.1153) | Loss 9.4352(9.7050) | Error 0.0333(0.0402) Steps 1024(1068.20) | Grad Norm 3.1145(6.6572) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0308 | Time 123.3077, Epoch Time 1668.8226(1577.5676), Bit/dim 3.6026(best: 3.5673), Xent 1.0255, Loss 4.1154, Error 0.2269(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16950 | Time 29.5005(27.8600) | Bit/dim 3.6120(3.5882) | Xent 0.0938(0.1152) | Loss 9.5256(10.5177) | Error 0.0378(0.0400) Steps 1114(1071.82) | Grad Norm 4.6616(6.3382) | Total Time 0.00(0.00)\n",
      "Iter 16960 | Time 30.0046(27.9535) | Bit/dim 3.6278(3.5895) | Xent 0.1266(0.1156) | Loss 9.5622(10.2457) | Error 0.0433(0.0400) Steps 1078(1072.85) | Grad Norm 5.9622(6.6077) | Total Time 0.00(0.00)\n",
      "Iter 16970 | Time 28.0794(28.0541) | Bit/dim 3.5974(3.5941) | Xent 0.1463(0.1200) | Loss 9.4766(10.0560) | Error 0.0489(0.0418) Steps 1078(1080.58) | Grad Norm 8.8664(7.4029) | Total Time 0.00(0.00)\n",
      "Iter 16980 | Time 29.6598(28.4111) | Bit/dim 3.6110(3.5969) | Xent 0.0927(0.1215) | Loss 9.4276(9.9219) | Error 0.0322(0.0427) Steps 1156(1090.50) | Grad Norm 4.2925(8.0717) | Total Time 0.00(0.00)\n",
      "Iter 16990 | Time 29.0677(28.6324) | Bit/dim 3.6293(3.6031) | Xent 0.1190(0.1239) | Loss 9.4317(9.8237) | Error 0.0422(0.0433) Steps 1096(1095.59) | Grad Norm 7.2773(8.1796) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0309 | Time 127.7534, Epoch Time 1726.1269(1582.0243), Bit/dim 3.6546(best: 3.5673), Xent 1.0434, Loss 4.1763, Error 0.2325(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17000 | Time 30.7609(28.8063) | Bit/dim 3.6659(3.6140) | Xent 0.1760(0.1310) | Loss 9.7516(10.7682) | Error 0.0567(0.0452) Steps 1150(1098.02) | Grad Norm 24.0864(12.2187) | Total Time 0.00(0.00)\n",
      "Iter 17010 | Time 30.3076(29.2445) | Bit/dim 3.7169(3.6357) | Xent 0.2072(0.1475) | Loss 9.6753(10.5096) | Error 0.0589(0.0503) Steps 1072(1100.26) | Grad Norm 53.4281(16.2097) | Total Time 0.00(0.00)\n",
      "Iter 17020 | Time 30.9889(29.6823) | Bit/dim 3.7007(3.6478) | Xent 0.1885(0.1576) | Loss 9.8595(10.3200) | Error 0.0711(0.0532) Steps 1132(1104.17) | Grad Norm 28.9779(21.0800) | Total Time 0.00(0.00)\n",
      "Iter 17030 | Time 33.4470(30.3774) | Bit/dim 3.7952(3.6720) | Xent 0.2509(0.1856) | Loss 10.1207(10.2479) | Error 0.0767(0.0605) Steps 1156(1122.40) | Grad Norm 84.8154(32.3222) | Total Time 0.00(0.00)\n",
      "Iter 17040 | Time 29.3663(30.5423) | Bit/dim 3.6947(3.6883) | Xent 0.2500(0.2112) | Loss 9.8904(10.1733) | Error 0.0767(0.0681) Steps 1138(1128.25) | Grad Norm 52.2287(43.5235) | Total Time 0.00(0.00)\n",
      "Iter 17050 | Time 27.9374(29.7791) | Bit/dim 3.6313(3.6832) | Xent 0.2217(0.2151) | Loss 9.7206(10.0491) | Error 0.0756(0.0716) Steps 1108(1111.93) | Grad Norm 8.5032(34.3108) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0310 | Time 120.4677, Epoch Time 1809.5742(1588.8508), Bit/dim 3.6669(best: 3.5673), Xent 0.9381, Loss 4.1360, Error 0.2269(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17060 | Time 28.4182(29.1836) | Bit/dim 3.6181(3.6740) | Xent 0.1456(0.2037) | Loss 9.4891(10.7649) | Error 0.0522(0.0690) Steps 1036(1098.74) | Grad Norm 8.6977(27.5109) | Total Time 0.00(0.00)\n",
      "Iter 17070 | Time 28.5001(28.9481) | Bit/dim 3.6100(3.6587) | Xent 0.1579(0.1906) | Loss 9.5882(10.4267) | Error 0.0567(0.0656) Steps 1066(1090.26) | Grad Norm 7.5330(22.1997) | Total Time 0.00(0.00)\n",
      "Iter 17080 | Time 29.5018(29.0437) | Bit/dim 3.6221(3.6451) | Xent 0.1200(0.1767) | Loss 9.5007(10.1803) | Error 0.0400(0.0612) Steps 1096(1088.30) | Grad Norm 4.6315(18.4375) | Total Time 0.00(0.00)\n",
      "Iter 17090 | Time 28.9839(29.0288) | Bit/dim 3.5595(3.6323) | Xent 0.1345(0.1664) | Loss 9.2906(9.9855) | Error 0.0478(0.0582) Steps 1054(1084.54) | Grad Norm 14.8005(15.6648) | Total Time 0.00(0.00)\n",
      "Iter 17100 | Time 30.0545(29.2141) | Bit/dim 3.5783(3.6216) | Xent 0.1053(0.1579) | Loss 9.4550(9.8542) | Error 0.0311(0.0551) Steps 1138(1090.36) | Grad Norm 5.2878(13.6568) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0311 | Time 123.6953, Epoch Time 1727.8007(1593.0193), Bit/dim 3.6076(best: 3.5673), Xent 1.0089, Loss 4.1120, Error 0.2244(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17110 | Time 29.7079(29.2861) | Bit/dim 3.6200(3.6153) | Xent 0.1065(0.1478) | Loss 9.5276(10.7288) | Error 0.0344(0.0518) Steps 1078(1088.90) | Grad Norm 16.0331(12.6616) | Total Time 0.00(0.00)\n",
      "Iter 17120 | Time 30.2899(29.2910) | Bit/dim 3.6131(3.6089) | Xent 0.1238(0.1402) | Loss 9.5522(10.3816) | Error 0.0422(0.0491) Steps 1138(1089.85) | Grad Norm 5.3460(12.5962) | Total Time 0.00(0.00)\n",
      "Iter 17130 | Time 28.5068(29.3426) | Bit/dim 3.6185(3.6034) | Xent 0.1029(0.1348) | Loss 9.5336(10.1345) | Error 0.0356(0.0476) Steps 1084(1091.00) | Grad Norm 25.9616(14.4399) | Total Time 0.00(0.00)\n",
      "Iter 17140 | Time 31.2017(29.5557) | Bit/dim 3.6190(3.6032) | Xent 0.1120(0.1303) | Loss 9.6270(9.9627) | Error 0.0344(0.0456) Steps 1114(1098.91) | Grad Norm 15.5778(16.2325) | Total Time 0.00(0.00)\n",
      "Iter 17150 | Time 30.8930(29.8163) | Bit/dim 3.6062(3.6055) | Xent 0.1193(0.1281) | Loss 9.5443(9.8515) | Error 0.0456(0.0448) Steps 1168(1105.96) | Grad Norm 9.5714(14.8944) | Total Time 0.00(0.00)\n",
      "Iter 17160 | Time 31.3934(29.8754) | Bit/dim 3.6446(3.6136) | Xent 0.1056(0.1290) | Loss 9.5319(9.7843) | Error 0.0344(0.0448) Steps 1132(1107.82) | Grad Norm 70.0650(23.6594) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0312 | Time 126.5627, Epoch Time 1786.9254(1598.8365), Bit/dim 3.6349(best: 3.5673), Xent 1.0198, Loss 4.1448, Error 0.2274(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17170 | Time 29.4833(29.9341) | Bit/dim 3.6112(3.6115) | Xent 0.1216(0.1287) | Loss 9.4924(10.5653) | Error 0.0411(0.0441) Steps 1066(1107.66) | Grad Norm 17.5919(21.8149) | Total Time 0.00(0.00)\n",
      "Iter 17180 | Time 32.1601(30.0999) | Bit/dim 3.6228(3.6148) | Xent 0.1560(0.1284) | Loss 9.4777(10.2948) | Error 0.0544(0.0436) Steps 1156(1107.93) | Grad Norm 10.1943(19.5732) | Total Time 0.00(0.00)\n",
      "Iter 17190 | Time 30.2486(30.1249) | Bit/dim 3.6031(3.6181) | Xent 0.1248(0.1278) | Loss 9.6015(10.1048) | Error 0.0444(0.0435) Steps 1102(1106.03) | Grad Norm 11.4106(19.9622) | Total Time 0.00(0.00)\n",
      "Iter 17200 | Time 31.3436(30.2466) | Bit/dim 3.6547(3.6234) | Xent 0.1426(0.1315) | Loss 9.5792(9.9727) | Error 0.0467(0.0451) Steps 1114(1109.43) | Grad Norm 55.7355(20.9036) | Total Time 0.00(0.00)\n",
      "Iter 17210 | Time 31.0691(30.5799) | Bit/dim 3.6576(3.6323) | Xent 0.1907(0.1406) | Loss 9.6325(9.8864) | Error 0.0589(0.0477) Steps 1114(1111.80) | Grad Norm 12.5532(23.4495) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0313 | Time 130.6353, Epoch Time 1831.8944(1605.8282), Bit/dim 3.7083(best: 3.5673), Xent 1.0274, Loss 4.2220, Error 0.2351(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17220 | Time 34.9381(30.9909) | Bit/dim 3.8423(3.6557) | Xent 0.4284(0.1680) | Loss 10.3570(10.9334) | Error 0.1244(0.0552) Steps 1204(1125.93) | Grad Norm 57.5006(40.3100) | Total Time 0.00(0.00)\n",
      "Iter 17230 | Time 32.6412(31.6883) | Bit/dim 3.8244(3.7134) | Xent 0.4011(0.2468) | Loss 10.3531(10.8250) | Error 0.1200(0.0770) Steps 1186(1145.06) | Grad Norm 85.1382(60.6886) | Total Time 0.00(0.00)\n",
      "Iter 17240 | Time 30.1510(31.8208) | Bit/dim 3.7189(3.7262) | Xent 0.2415(0.2671) | Loss 9.9167(10.6192) | Error 0.0811(0.0833) Steps 1108(1148.16) | Grad Norm 126.4724(74.1783) | Total Time 0.00(0.00)\n",
      "Iter 17250 | Time 29.8553(31.3893) | Bit/dim 3.6898(3.7198) | Xent 0.2191(0.2571) | Loss 9.7426(10.4006) | Error 0.0811(0.0826) Steps 1138(1140.10) | Grad Norm 6.5002(58.5414) | Total Time 0.00(0.00)\n",
      "Iter 17260 | Time 29.2381(30.9787) | Bit/dim 3.6367(3.7002) | Xent 0.2155(0.2409) | Loss 9.6594(10.2054) | Error 0.0700(0.0782) Steps 1072(1128.03) | Grad Norm 39.1931(47.8223) | Total Time 0.00(0.00)\n",
      "Iter 17270 | Time 30.3586(30.6801) | Bit/dim 3.6382(3.6848) | Xent 0.1540(0.2208) | Loss 9.5294(10.0507) | Error 0.0611(0.0726) Steps 1120(1122.80) | Grad Norm 7.4309(40.1145) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0314 | Time 125.6153, Epoch Time 1868.9642(1613.7223), Bit/dim 3.6478(best: 3.5673), Xent 1.0006, Loss 4.1481, Error 0.2254(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17280 | Time 28.1749(30.3772) | Bit/dim 3.6148(3.6731) | Xent 0.1210(0.2032) | Loss 9.4895(10.7876) | Error 0.0456(0.0676) Steps 1072(1115.05) | Grad Norm 6.9975(33.1587) | Total Time 0.00(0.00)\n",
      "Iter 17290 | Time 29.4383(29.9369) | Bit/dim 3.6532(3.6619) | Xent 0.1538(0.1897) | Loss 9.6635(10.4703) | Error 0.0556(0.0633) Steps 1078(1108.15) | Grad Norm 13.0131(27.4387) | Total Time 0.00(0.00)\n",
      "Iter 17300 | Time 28.5304(29.6256) | Bit/dim 3.6206(3.6488) | Xent 0.1265(0.1751) | Loss 9.4609(10.2301) | Error 0.0444(0.0591) Steps 1060(1103.68) | Grad Norm 5.4780(21.7404) | Total Time 0.00(0.00)\n",
      "Iter 17310 | Time 30.2983(29.5504) | Bit/dim 3.6424(3.6378) | Xent 0.1391(0.1656) | Loss 9.6044(10.0368) | Error 0.0456(0.0558) Steps 1108(1102.20) | Grad Norm 11.9195(20.5787) | Total Time 0.00(0.00)\n",
      "Iter 17320 | Time 29.9665(29.6318) | Bit/dim 3.6074(3.6327) | Xent 0.1407(0.1588) | Loss 9.5739(9.8995) | Error 0.0444(0.0537) Steps 1126(1100.33) | Grad Norm 20.9404(18.9930) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0315 | Time 126.5393, Epoch Time 1757.0468(1618.0221), Bit/dim 3.6284(best: 3.5673), Xent 0.9836, Loss 4.1202, Error 0.2246(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17330 | Time 29.0805(29.7158) | Bit/dim 3.6013(3.6277) | Xent 0.1314(0.1543) | Loss 9.5947(10.8704) | Error 0.0433(0.0520) Steps 1102(1105.16) | Grad Norm 5.7329(18.4607) | Total Time 0.00(0.00)\n",
      "Iter 17340 | Time 29.5541(29.7631) | Bit/dim 3.6539(3.6267) | Xent 0.1460(0.1507) | Loss 9.7174(10.5291) | Error 0.0478(0.0504) Steps 1114(1106.38) | Grad Norm 32.2870(22.6011) | Total Time 0.00(0.00)\n",
      "Iter 17350 | Time 30.8926(30.1540) | Bit/dim 3.6675(3.6343) | Xent 0.1469(0.1505) | Loss 9.5738(10.3089) | Error 0.0544(0.0505) Steps 1156(1117.24) | Grad Norm 29.9498(29.2612) | Total Time 0.00(0.00)\n",
      "Iter 17360 | Time 33.3685(30.8242) | Bit/dim 3.7025(3.6530) | Xent 0.2436(0.1680) | Loss 9.8489(10.1903) | Error 0.0756(0.0557) Steps 1222(1132.29) | Grad Norm 98.3352(47.3591) | Total Time 0.00(0.00)\n",
      "Iter 17370 | Time 35.1951(31.9401) | Bit/dim 4.7764(3.8816) | Xent 2.0663(0.6059) | Loss 14.1640(11.0474) | Error 0.3678(0.1148) Steps 1294(1169.50) | Grad Norm 375.5166(236.8719) | Total Time 0.00(0.00)\n",
      "Iter 17380 | Time 33.3253(32.6524) | Bit/dim 4.1087(3.9988) | Xent 0.8472(0.7841) | Loss 11.3730(11.4114) | Error 0.2400(0.1637) Steps 1252(1195.54) | Grad Norm 72.2885(231.8082) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0316 | Time 139.9692, Epoch Time 1943.7231(1627.7931), Bit/dim 4.0626(best: 3.5673), Xent 1.0589, Loss 4.5921, Error 0.3095(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17390 | Time 31.1649(32.3784) | Bit/dim 3.9020(3.9896) | Xent 0.4922(0.7462) | Loss 10.3817(12.1542) | Error 0.1644(0.1757) Steps 1096(1181.88) | Grad Norm 13.5158(182.3011) | Total Time 0.00(0.00)\n",
      "Iter 17400 | Time 28.9607(31.9300) | Bit/dim 3.7750(3.9468) | Xent 0.3924(0.6757) | Loss 10.0809(11.6640) | Error 0.1378(0.1735) Steps 1126(1168.87) | Grad Norm 13.4937(142.2962) | Total Time 0.00(0.00)\n",
      "Iter 17410 | Time 29.5111(31.3728) | Bit/dim 3.7417(3.8965) | Xent 0.3005(0.5940) | Loss 9.8595(11.2323) | Error 0.1122(0.1602) Steps 1066(1152.17) | Grad Norm 7.9529(107.5734) | Total Time 0.00(0.00)\n",
      "Iter 17420 | Time 30.6868(31.0277) | Bit/dim 3.7001(3.8485) | Xent 0.2410(0.5138) | Loss 9.9783(10.8769) | Error 0.0856(0.1441) Steps 1126(1140.59) | Grad Norm 9.9082(82.3272) | Total Time 0.00(0.00)\n",
      "Iter 17430 | Time 29.7878(30.7250) | Bit/dim 3.6656(3.8060) | Xent 0.2021(0.4459) | Loss 9.7051(10.5805) | Error 0.0678(0.1293) Steps 1102(1137.31) | Grad Norm 33.0113(64.7075) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0317 | Time 128.5196, Epoch Time 1818.9202(1633.5269), Bit/dim 3.6807(best: 3.5673), Xent 0.8677, Loss 4.1145, Error 0.2257(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17440 | Time 30.3846(30.5298) | Bit/dim 3.6577(3.7702) | Xent 0.2191(0.3847) | Loss 9.6123(11.3747) | Error 0.0833(0.1146) Steps 1132(1130.86) | Grad Norm 5.6547(53.2720) | Total Time 0.00(0.00)\n",
      "Iter 17450 | Time 28.6658(30.2807) | Bit/dim 3.6818(3.7415) | Xent 0.2057(0.3382) | Loss 9.6116(10.9231) | Error 0.0700(0.1041) Steps 1102(1126.06) | Grad Norm 7.1299(44.1874) | Total Time 0.00(0.00)\n",
      "Iter 17460 | Time 29.1413(30.0732) | Bit/dim 3.6403(3.7163) | Xent 0.1582(0.3019) | Loss 9.4791(10.5906) | Error 0.0589(0.0951) Steps 1072(1122.17) | Grad Norm 5.7654(35.7543) | Total Time 0.00(0.00)\n",
      "Iter 17470 | Time 31.4025(29.9800) | Bit/dim 3.6700(3.6963) | Xent 0.1656(0.2712) | Loss 9.6857(10.3357) | Error 0.0589(0.0874) Steps 1078(1116.29) | Grad Norm 13.7189(28.4935) | Total Time 0.00(0.00)\n",
      "Iter 17480 | Time 29.2618(29.7921) | Bit/dim 3.6391(3.6836) | Xent 0.1950(0.2502) | Loss 9.6291(10.1515) | Error 0.0711(0.0819) Steps 1108(1114.16) | Grad Norm 8.5936(23.1483) | Total Time 0.00(0.00)\n",
      "Iter 17490 | Time 28.2764(29.6346) | Bit/dim 3.6047(3.6697) | Xent 0.1847(0.2306) | Loss 9.5006(9.9944) | Error 0.0589(0.0761) Steps 1108(1109.04) | Grad Norm 4.4387(18.8615) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0318 | Time 123.7506, Epoch Time 1763.7060(1637.4323), Bit/dim 3.6373(best: 3.5673), Xent 0.9091, Loss 4.0919, Error 0.2268(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17500 | Time 28.4206(29.3306) | Bit/dim 3.5959(3.6570) | Xent 0.1441(0.2111) | Loss 9.5097(10.7687) | Error 0.0567(0.0710) Steps 1084(1104.42) | Grad Norm 5.0308(15.5549) | Total Time 0.00(0.00)\n",
      "Iter 17510 | Time 28.8206(29.1648) | Bit/dim 3.6181(3.6496) | Xent 0.1437(0.2004) | Loss 9.5000(10.4620) | Error 0.0533(0.0677) Steps 1102(1098.98) | Grad Norm 4.6738(13.4411) | Total Time 0.00(0.00)\n",
      "Iter 17520 | Time 28.4154(29.0640) | Bit/dim 3.6365(3.6407) | Xent 0.1694(0.1908) | Loss 9.6973(10.2277) | Error 0.0589(0.0644) Steps 1084(1094.35) | Grad Norm 3.9726(11.4439) | Total Time 0.00(0.00)\n",
      "Iter 17530 | Time 28.7961(28.9372) | Bit/dim 3.5982(3.6364) | Xent 0.1336(0.1784) | Loss 9.5362(10.0584) | Error 0.0478(0.0604) Steps 1054(1091.63) | Grad Norm 9.8620(10.8441) | Total Time 0.00(0.00)\n",
      "Iter 17540 | Time 28.2475(28.7492) | Bit/dim 3.6532(3.6302) | Xent 0.1623(0.1695) | Loss 9.6113(9.9250) | Error 0.0489(0.0576) Steps 1090(1086.07) | Grad Norm 7.9451(9.3100) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0319 | Time 122.9922, Epoch Time 1708.3296(1639.5592), Bit/dim 3.6178(best: 3.5673), Xent 0.9273, Loss 4.0815, Error 0.2210(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17550 | Time 27.8160(28.5064) | Bit/dim 3.6431(3.6233) | Xent 0.1564(0.1637) | Loss 9.6702(10.8191) | Error 0.0556(0.0558) Steps 1108(1084.08) | Grad Norm 6.3111(8.3980) | Total Time 0.00(0.00)\n",
      "Iter 17560 | Time 28.2681(28.4618) | Bit/dim 3.6133(3.6205) | Xent 0.1600(0.1595) | Loss 9.5810(10.4855) | Error 0.0522(0.0546) Steps 1060(1082.70) | Grad Norm 4.5975(7.4727) | Total Time 0.00(0.00)\n",
      "Iter 17570 | Time 28.7995(28.4631) | Bit/dim 3.5968(3.6143) | Xent 0.1757(0.1559) | Loss 9.5997(10.2268) | Error 0.0622(0.0535) Steps 1066(1082.70) | Grad Norm 14.8022(8.0114) | Total Time 0.00(0.00)\n",
      "Iter 17580 | Time 26.9080(28.3445) | Bit/dim 3.6143(3.6137) | Xent 0.1321(0.1533) | Loss 9.4485(10.0389) | Error 0.0467(0.0527) Steps 1060(1079.97) | Grad Norm 16.6991(7.6331) | Total Time 0.00(0.00)\n",
      "Iter 17590 | Time 28.5633(28.2117) | Bit/dim 3.5788(3.6101) | Xent 0.1509(0.1500) | Loss 9.4910(9.8929) | Error 0.0544(0.0518) Steps 1054(1078.02) | Grad Norm 5.0153(6.9407) | Total Time 0.00(0.00)\n",
      "Iter 17600 | Time 27.4148(28.0867) | Bit/dim 3.5763(3.6059) | Xent 0.1144(0.1466) | Loss 9.4401(9.7866) | Error 0.0344(0.0503) Steps 1072(1075.53) | Grad Norm 2.9172(6.5920) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0320 | Time 120.2724, Epoch Time 1681.7572(1640.8251), Bit/dim 3.6094(best: 3.5673), Xent 0.9567, Loss 4.0878, Error 0.2260(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17610 | Time 27.0424(27.9693) | Bit/dim 3.5887(3.6035) | Xent 0.1257(0.1420) | Loss 9.3736(10.5871) | Error 0.0444(0.0485) Steps 1060(1072.73) | Grad Norm 4.6287(5.9986) | Total Time 0.00(0.00)\n",
      "Iter 17620 | Time 26.3578(27.7845) | Bit/dim 3.6059(3.5997) | Xent 0.1302(0.1416) | Loss 9.4156(10.2978) | Error 0.0456(0.0487) Steps 1018(1069.84) | Grad Norm 3.9089(6.4413) | Total Time 0.00(0.00)\n",
      "Iter 17630 | Time 27.1339(27.6979) | Bit/dim 3.6041(3.6002) | Xent 0.1064(0.1394) | Loss 9.4173(10.0797) | Error 0.0356(0.0477) Steps 1102(1072.44) | Grad Norm 3.3292(6.0179) | Total Time 0.00(0.00)\n",
      "Iter 17640 | Time 28.4524(27.8768) | Bit/dim 3.5896(3.6015) | Xent 0.1276(0.1378) | Loss 9.5470(9.9261) | Error 0.0456(0.0471) Steps 1084(1071.29) | Grad Norm 5.1414(5.5878) | Total Time 0.00(0.00)\n",
      "Iter 17650 | Time 28.6643(28.0398) | Bit/dim 3.5757(3.6005) | Xent 0.1172(0.1368) | Loss 9.5897(9.8130) | Error 0.0378(0.0466) Steps 1084(1072.31) | Grad Norm 3.5014(5.7688) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0321 | Time 121.9371, Epoch Time 1675.4682(1641.8644), Bit/dim 3.6055(best: 3.5673), Xent 0.9658, Loss 4.0883, Error 0.2252(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17660 | Time 27.3929(28.1301) | Bit/dim 3.5735(3.5995) | Xent 0.1336(0.1352) | Loss 9.3523(10.7368) | Error 0.0500(0.0460) Steps 1084(1072.92) | Grad Norm 4.1888(7.9360) | Total Time 0.00(0.00)\n",
      "Iter 17670 | Time 28.1255(28.1181) | Bit/dim 3.5825(3.5975) | Xent 0.1342(0.1345) | Loss 9.3137(10.3975) | Error 0.0533(0.0461) Steps 1084(1073.59) | Grad Norm 3.4420(7.8769) | Total Time 0.00(0.00)\n",
      "Iter 17680 | Time 26.6920(28.1167) | Bit/dim 3.5846(3.5960) | Xent 0.1251(0.1311) | Loss 9.2515(10.1413) | Error 0.0467(0.0452) Steps 1048(1074.24) | Grad Norm 6.7150(7.7120) | Total Time 0.00(0.00)\n",
      "Iter 17690 | Time 26.7883(27.8932) | Bit/dim 3.6022(3.5970) | Xent 0.1342(0.1323) | Loss 9.5175(9.9770) | Error 0.0478(0.0451) Steps 1042(1070.21) | Grad Norm 3.6029(6.9222) | Total Time 0.00(0.00)\n",
      "Iter 17700 | Time 27.7699(27.7800) | Bit/dim 3.5792(3.5960) | Xent 0.1295(0.1304) | Loss 9.3821(9.8390) | Error 0.0478(0.0449) Steps 1072(1069.50) | Grad Norm 3.3607(6.0937) | Total Time 0.00(0.00)\n",
      "Iter 17710 | Time 27.2130(27.6217) | Bit/dim 3.5729(3.5914) | Xent 0.1203(0.1280) | Loss 9.3844(9.7242) | Error 0.0533(0.0442) Steps 1102(1067.55) | Grad Norm 3.7364(5.4768) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0322 | Time 123.0752, Epoch Time 1664.3192(1642.5381), Bit/dim 3.5985(best: 3.5673), Xent 0.9632, Loss 4.0801, Error 0.2223(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17720 | Time 27.6898(27.5492) | Bit/dim 3.5825(3.5905) | Xent 0.1287(0.1270) | Loss 9.3941(10.4954) | Error 0.0444(0.0438) Steps 1036(1066.55) | Grad Norm 2.9622(5.0597) | Total Time 0.00(0.00)\n",
      "Iter 17730 | Time 28.0531(27.5227) | Bit/dim 3.6088(3.5913) | Xent 0.1413(0.1272) | Loss 9.3043(10.2178) | Error 0.0511(0.0442) Steps 1042(1064.35) | Grad Norm 4.3561(4.9476) | Total Time 0.00(0.00)\n",
      "Iter 17740 | Time 27.9458(27.4862) | Bit/dim 3.5696(3.5900) | Xent 0.1389(0.1271) | Loss 9.4193(10.0061) | Error 0.0522(0.0444) Steps 1060(1062.07) | Grad Norm 7.8392(5.0346) | Total Time 0.00(0.00)\n",
      "Iter 17750 | Time 26.0061(27.4085) | Bit/dim 3.5794(3.5876) | Xent 0.1140(0.1263) | Loss 9.3961(9.8606) | Error 0.0389(0.0440) Steps 1036(1062.82) | Grad Norm 5.4840(4.8654) | Total Time 0.00(0.00)\n",
      "Iter 17760 | Time 26.9406(27.3081) | Bit/dim 3.6021(3.5882) | Xent 0.1142(0.1266) | Loss 9.4350(9.7514) | Error 0.0356(0.0440) Steps 1060(1063.15) | Grad Norm 4.1779(5.0954) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0323 | Time 122.6342, Epoch Time 1638.2803(1642.4103), Bit/dim 3.5970(best: 3.5673), Xent 0.9619, Loss 4.0779, Error 0.2220(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17770 | Time 26.2190(27.1712) | Bit/dim 3.5514(3.5870) | Xent 0.1069(0.1245) | Loss 9.2233(10.6796) | Error 0.0367(0.0432) Steps 1048(1064.28) | Grad Norm 3.5689(4.8938) | Total Time 0.00(0.00)\n",
      "Iter 17780 | Time 27.2996(27.1156) | Bit/dim 3.5362(3.5852) | Xent 0.1005(0.1228) | Loss 9.3176(10.3453) | Error 0.0333(0.0426) Steps 1102(1061.01) | Grad Norm 3.7388(4.8496) | Total Time 0.00(0.00)\n",
      "Iter 17790 | Time 28.0068(27.1711) | Bit/dim 3.5445(3.5859) | Xent 0.1057(0.1225) | Loss 9.5240(10.1107) | Error 0.0344(0.0425) Steps 1066(1059.60) | Grad Norm 3.1310(4.9593) | Total Time 0.00(0.00)\n",
      "Iter 17800 | Time 26.8657(27.1508) | Bit/dim 3.5821(3.5856) | Xent 0.1082(0.1202) | Loss 9.3984(9.9233) | Error 0.0367(0.0418) Steps 1030(1056.56) | Grad Norm 4.2472(4.6031) | Total Time 0.00(0.00)\n",
      "Iter 17810 | Time 26.7782(27.1076) | Bit/dim 3.5987(3.5843) | Xent 0.1235(0.1194) | Loss 9.5203(9.8009) | Error 0.0411(0.0416) Steps 1054(1058.72) | Grad Norm 4.8831(4.4661) | Total Time 0.00(0.00)\n",
      "Iter 17820 | Time 26.7168(27.1644) | Bit/dim 3.5534(3.5840) | Xent 0.1312(0.1194) | Loss 9.3296(9.7068) | Error 0.0500(0.0418) Steps 1042(1062.26) | Grad Norm 3.2360(4.6528) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0324 | Time 120.6394, Epoch Time 1631.2716(1642.0762), Bit/dim 3.5933(best: 3.5673), Xent 0.9733, Loss 4.0799, Error 0.2243(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17830 | Time 28.5536(27.3254) | Bit/dim 3.6167(3.5844) | Xent 0.1038(0.1199) | Loss 9.5868(10.4826) | Error 0.0356(0.0418) Steps 1108(1063.46) | Grad Norm 4.9519(4.6976) | Total Time 0.00(0.00)\n",
      "Iter 17840 | Time 26.4700(27.2661) | Bit/dim 3.5869(3.5847) | Xent 0.1035(0.1178) | Loss 9.3142(10.1962) | Error 0.0322(0.0410) Steps 1036(1060.76) | Grad Norm 3.1439(4.8751) | Total Time 0.00(0.00)\n",
      "Iter 17850 | Time 27.4414(27.2886) | Bit/dim 3.6139(3.5835) | Xent 0.1072(0.1179) | Loss 9.4191(9.9903) | Error 0.0389(0.0412) Steps 1036(1061.62) | Grad Norm 3.5831(4.6944) | Total Time 0.00(0.00)\n",
      "Iter 17860 | Time 27.4308(27.2912) | Bit/dim 3.5875(3.5830) | Xent 0.1070(0.1179) | Loss 9.4718(9.8460) | Error 0.0367(0.0413) Steps 1078(1060.06) | Grad Norm 5.6342(4.6371) | Total Time 0.00(0.00)\n",
      "Iter 17870 | Time 26.9160(27.3192) | Bit/dim 3.5796(3.5784) | Xent 0.1155(0.1166) | Loss 9.5007(9.7279) | Error 0.0422(0.0404) Steps 1060(1063.04) | Grad Norm 6.1291(4.6402) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0325 | Time 121.8091, Epoch Time 1647.0037(1642.2240), Bit/dim 3.5899(best: 3.5673), Xent 0.9890, Loss 4.0844, Error 0.2225(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17880 | Time 27.5055(27.3828) | Bit/dim 3.5758(3.5819) | Xent 0.0910(0.1156) | Loss 9.3475(10.6513) | Error 0.0322(0.0398) Steps 1078(1064.87) | Grad Norm 3.4912(4.3888) | Total Time 0.00(0.00)\n",
      "Iter 17890 | Time 28.3227(27.5087) | Bit/dim 3.5936(3.5831) | Xent 0.1243(0.1136) | Loss 9.5851(10.3396) | Error 0.0444(0.0394) Steps 1120(1066.84) | Grad Norm 4.8278(4.3456) | Total Time 0.00(0.00)\n",
      "Iter 17900 | Time 28.3091(27.5817) | Bit/dim 3.5590(3.5806) | Xent 0.1443(0.1143) | Loss 9.3958(10.1019) | Error 0.0467(0.0398) Steps 1054(1070.11) | Grad Norm 3.7921(4.6851) | Total Time 0.00(0.00)\n",
      "Iter 17910 | Time 27.2268(27.6806) | Bit/dim 3.5821(3.5822) | Xent 0.0883(0.1159) | Loss 9.4007(9.9213) | Error 0.0256(0.0402) Steps 1030(1070.30) | Grad Norm 6.5445(5.3049) | Total Time 0.00(0.00)\n",
      "Iter 17920 | Time 26.7561(27.5333) | Bit/dim 3.5915(3.5808) | Xent 0.1334(0.1157) | Loss 9.3517(9.7826) | Error 0.0411(0.0404) Steps 1066(1064.90) | Grad Norm 4.9371(6.1902) | Total Time 0.00(0.00)\n",
      "Iter 17930 | Time 27.7439(27.5093) | Bit/dim 3.5874(3.5806) | Xent 0.1031(0.1151) | Loss 9.3705(9.6832) | Error 0.0456(0.0405) Steps 1084(1069.38) | Grad Norm 3.5686(6.5740) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0326 | Time 121.8348, Epoch Time 1659.4126(1642.7397), Bit/dim 3.5920(best: 3.5673), Xent 0.9760, Loss 4.0800, Error 0.2210(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17940 | Time 27.2428(27.5416) | Bit/dim 3.5805(3.5837) | Xent 0.0916(0.1118) | Loss 9.2932(10.4827) | Error 0.0256(0.0394) Steps 1042(1069.17) | Grad Norm 4.3144(6.9473) | Total Time 0.00(0.00)\n",
      "Iter 17950 | Time 26.6085(27.4121) | Bit/dim 3.5556(3.5837) | Xent 0.1194(0.1135) | Loss 9.3751(10.2029) | Error 0.0411(0.0397) Steps 1066(1067.45) | Grad Norm 44.7150(8.0464) | Total Time 0.00(0.00)\n",
      "Iter 17960 | Time 27.9438(27.4335) | Bit/dim 3.6071(3.5842) | Xent 0.1112(0.1152) | Loss 9.3282(9.9826) | Error 0.0433(0.0406) Steps 1072(1067.82) | Grad Norm 3.6415(8.8248) | Total Time 0.00(0.00)\n",
      "Iter 17970 | Time 28.7084(27.5848) | Bit/dim 3.5576(3.5816) | Xent 0.1174(0.1154) | Loss 9.2466(9.8301) | Error 0.0378(0.0400) Steps 1090(1067.99) | Grad Norm 5.0150(8.8377) | Total Time 0.00(0.00)\n",
      "Iter 17980 | Time 28.5590(27.7798) | Bit/dim 3.6130(3.5842) | Xent 0.1041(0.1191) | Loss 9.6061(9.7438) | Error 0.0344(0.0412) Steps 1102(1072.68) | Grad Norm 5.2509(10.0230) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0327 | Time 124.2546, Epoch Time 1674.4424(1643.6907), Bit/dim 3.6079(best: 3.5673), Xent 0.9919, Loss 4.1039, Error 0.2223(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17990 | Time 28.7709(28.0880) | Bit/dim 3.5982(3.5870) | Xent 0.1466(0.1208) | Loss 9.4052(10.6918) | Error 0.0489(0.0419) Steps 1138(1079.20) | Grad Norm 7.5034(10.3653) | Total Time 0.00(0.00)\n",
      "Iter 18000 | Time 29.2608(28.3348) | Bit/dim 3.6115(3.5908) | Xent 0.1211(0.1197) | Loss 9.5564(10.3818) | Error 0.0467(0.0419) Steps 1060(1084.24) | Grad Norm 18.3860(11.7138) | Total Time 0.00(0.00)\n",
      "Iter 18010 | Time 30.2211(28.8418) | Bit/dim 3.6085(3.5950) | Xent 0.1435(0.1226) | Loss 9.5072(10.1631) | Error 0.0456(0.0425) Steps 1120(1096.42) | Grad Norm 92.0284(13.7945) | Total Time 0.00(0.00)\n",
      "Iter 18020 | Time 30.8424(29.2417) | Bit/dim 3.6084(3.5990) | Xent 0.1672(0.1249) | Loss 9.5635(10.0018) | Error 0.0544(0.0432) Steps 1096(1102.98) | Grad Norm 8.0458(15.4453) | Total Time 0.00(0.00)\n",
      "Iter 18030 | Time 30.4067(29.5664) | Bit/dim 3.6440(3.6061) | Xent 0.1326(0.1269) | Loss 9.5403(9.8819) | Error 0.0467(0.0437) Steps 1138(1107.97) | Grad Norm 41.8563(20.4818) | Total Time 0.00(0.00)\n",
      "Iter 18040 | Time 31.7154(30.0932) | Bit/dim 3.6929(3.6205) | Xent 0.2449(0.1474) | Loss 9.8214(9.8315) | Error 0.0867(0.0505) Steps 1192(1119.06) | Grad Norm 45.5909(41.1496) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0328 | Time 130.2546, Epoch Time 1807.4544(1648.6036), Bit/dim 3.7116(best: 3.5673), Xent 1.0108, Loss 4.2170, Error 0.2311(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18050 | Time 32.7283(30.7059) | Bit/dim 3.7934(3.6556) | Xent 0.3000(0.1750) | Loss 10.0627(10.7713) | Error 0.0978(0.0585) Steps 1198(1134.95) | Grad Norm 553.1733(71.7182) | Total Time 0.00(0.00)\n",
      "Iter 18060 | Time 34.1818(31.2911) | Bit/dim 3.9213(3.7192) | Xent 0.4589(0.2531) | Loss 10.7541(10.7183) | Error 0.1478(0.0804) Steps 1216(1156.13) | Grad Norm 23.8191(93.8792) | Total Time 0.00(0.00)\n",
      "Iter 18070 | Time 30.9086(31.5301) | Bit/dim 3.7924(3.7538) | Xent 0.2719(0.2909) | Loss 10.2256(10.6141) | Error 0.0889(0.0935) Steps 1168(1166.95) | Grad Norm 22.2681(90.8183) | Total Time 0.00(0.00)\n",
      "Iter 18080 | Time 30.6094(31.4339) | Bit/dim 3.7017(3.7497) | Xent 0.2698(0.2862) | Loss 9.9767(10.4456) | Error 0.0867(0.0933) Steps 1144(1160.37) | Grad Norm 9.6635(73.6997) | Total Time 0.00(0.00)\n",
      "Iter 18090 | Time 29.6540(31.1218) | Bit/dim 3.6849(3.7349) | Xent 0.2037(0.2655) | Loss 9.7818(10.2710) | Error 0.0822(0.0877) Steps 1144(1155.44) | Grad Norm 8.8213(64.4946) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0329 | Time 125.9811, Epoch Time 1886.0919(1655.7283), Bit/dim 3.6551(best: 3.5673), Xent 0.9314, Loss 4.1208, Error 0.2252(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18100 | Time 29.5958(30.8288) | Bit/dim 3.6280(3.7117) | Xent 0.1552(0.2393) | Loss 9.4538(11.1068) | Error 0.0611(0.0798) Steps 1126(1147.79) | Grad Norm 5.5401(59.2543) | Total Time 0.00(0.00)\n",
      "Iter 18110 | Time 28.4374(30.3818) | Bit/dim 3.6208(3.6943) | Xent 0.1627(0.2167) | Loss 9.4807(10.7038) | Error 0.0556(0.0727) Steps 1114(1139.40) | Grad Norm 69.3723(48.0820) | Total Time 0.00(0.00)\n",
      "Iter 18120 | Time 29.7114(30.0058) | Bit/dim 3.6030(3.6746) | Xent 0.1506(0.1972) | Loss 9.5353(10.3950) | Error 0.0589(0.0661) Steps 1084(1127.87) | Grad Norm 4.0518(39.1531) | Total Time 0.00(0.00)\n",
      "Iter 18130 | Time 28.9291(29.6781) | Bit/dim 3.6313(3.6580) | Xent 0.1022(0.1790) | Loss 9.5288(10.1608) | Error 0.0300(0.0602) Steps 1132(1124.73) | Grad Norm 5.2123(32.0578) | Total Time 0.00(0.00)\n",
      "Iter 18140 | Time 29.0029(29.4299) | Bit/dim 3.5612(3.6409) | Xent 0.1623(0.1680) | Loss 9.5054(9.9858) | Error 0.0478(0.0573) Steps 1144(1117.14) | Grad Norm 24.8007(28.4244) | Total Time 0.00(0.00)\n",
      "Iter 18150 | Time 27.7259(29.1791) | Bit/dim 3.5817(3.6287) | Xent 0.1318(0.1579) | Loss 9.3546(9.8508) | Error 0.0467(0.0543) Steps 1078(1114.06) | Grad Norm 3.6454(26.3113) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0330 | Time 125.1993, Epoch Time 1732.6045(1658.0346), Bit/dim 3.6105(best: 3.5673), Xent 0.9634, Loss 4.0922, Error 0.2216(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18160 | Time 28.4635(28.9212) | Bit/dim 3.5922(3.6201) | Xent 0.1269(0.1479) | Loss 9.5584(10.6239) | Error 0.0422(0.0507) Steps 1132(1110.67) | Grad Norm 12.2516(21.4731) | Total Time 0.00(0.00)\n",
      "Iter 18170 | Time 28.0578(28.7958) | Bit/dim 3.5838(3.6120) | Xent 0.1339(0.1421) | Loss 9.5287(10.3342) | Error 0.0433(0.0490) Steps 1102(1106.91) | Grad Norm 21.2386(19.2263) | Total Time 0.00(0.00)\n",
      "Iter 18180 | Time 28.4467(28.5268) | Bit/dim 3.6363(3.6131) | Xent 0.1269(0.1374) | Loss 9.5621(10.1115) | Error 0.0433(0.0469) Steps 1090(1100.34) | Grad Norm 5.0284(16.6106) | Total Time 0.00(0.00)\n",
      "Iter 18190 | Time 27.3568(28.3729) | Bit/dim 3.6008(3.6082) | Xent 0.1323(0.1342) | Loss 9.5195(9.9587) | Error 0.0489(0.0457) Steps 1060(1097.35) | Grad Norm 5.8664(13.8106) | Total Time 0.00(0.00)\n",
      "Iter 18200 | Time 27.2779(28.1580) | Bit/dim 3.5790(3.6041) | Xent 0.1168(0.1324) | Loss 9.4782(9.8321) | Error 0.0422(0.0457) Steps 1090(1094.53) | Grad Norm 7.4199(12.3992) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0331 | Time 124.0777, Epoch Time 1681.3273(1658.7334), Bit/dim 3.6005(best: 3.5673), Xent 0.9802, Loss 4.0906, Error 0.2245(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18210 | Time 28.4551(28.1213) | Bit/dim 3.5821(3.5997) | Xent 0.1059(0.1292) | Loss 9.3940(10.7441) | Error 0.0300(0.0442) Steps 1066(1092.14) | Grad Norm 28.7815(14.7896) | Total Time 0.00(0.00)\n",
      "Iter 18220 | Time 28.4799(28.1880) | Bit/dim 3.5847(3.5995) | Xent 0.1119(0.1257) | Loss 9.5149(10.4280) | Error 0.0411(0.0432) Steps 1102(1093.28) | Grad Norm 31.4231(15.8737) | Total Time 0.00(0.00)\n",
      "Iter 18230 | Time 29.6437(28.5368) | Bit/dim 3.5764(3.5983) | Xent 0.1260(0.1238) | Loss 9.5396(10.1931) | Error 0.0356(0.0426) Steps 1090(1098.05) | Grad Norm 8.2242(16.3921) | Total Time 0.00(0.00)\n",
      "Iter 18240 | Time 28.6961(28.5640) | Bit/dim 3.5967(3.5958) | Xent 0.1376(0.1250) | Loss 9.5315(10.0101) | Error 0.0500(0.0423) Steps 1060(1098.62) | Grad Norm 20.9697(18.7267) | Total Time 0.00(0.00)\n",
      "Iter 18250 | Time 30.0722(28.7223) | Bit/dim 3.5728(3.5961) | Xent 0.1477(0.1254) | Loss 9.5352(9.8864) | Error 0.0522(0.0428) Steps 1084(1099.42) | Grad Norm 5.3716(21.1680) | Total Time 0.00(0.00)\n",
      "Iter 18260 | Time 29.2867(28.8949) | Bit/dim 3.6553(3.6006) | Xent 0.1537(0.1261) | Loss 9.6809(9.8067) | Error 0.0522(0.0434) Steps 1066(1101.17) | Grad Norm 94.8813(22.9845) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0332 | Time 127.9313, Epoch Time 1736.8651(1661.0773), Bit/dim 3.6181(best: 3.5673), Xent 0.9873, Loss 4.1118, Error 0.2222(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18270 | Time 27.9131(28.9252) | Bit/dim 3.5850(3.6013) | Xent 0.1289(0.1278) | Loss 9.4799(10.6169) | Error 0.0444(0.0443) Steps 1102(1101.98) | Grad Norm 31.4895(23.8058) | Total Time 0.00(0.00)\n",
      "Iter 18280 | Time 28.1318(28.6769) | Bit/dim 3.6126(3.6006) | Xent 0.1276(0.1252) | Loss 9.6215(10.3248) | Error 0.0522(0.0435) Steps 1078(1097.95) | Grad Norm 5.9105(20.9104) | Total Time 0.00(0.00)\n",
      "Iter 18290 | Time 28.3473(28.4618) | Bit/dim 3.6094(3.5987) | Xent 0.1203(0.1241) | Loss 9.4059(10.1015) | Error 0.0433(0.0433) Steps 1096(1096.21) | Grad Norm 7.4643(20.0742) | Total Time 0.00(0.00)\n",
      "Iter 18300 | Time 28.6601(28.3853) | Bit/dim 3.5565(3.5966) | Xent 0.1416(0.1225) | Loss 9.4633(9.9408) | Error 0.0456(0.0423) Steps 1114(1098.23) | Grad Norm 4.2886(19.5069) | Total Time 0.00(0.00)\n",
      "Iter 18310 | Time 28.4424(28.2777) | Bit/dim 3.6122(3.5944) | Xent 0.1290(0.1225) | Loss 9.3830(9.8160) | Error 0.0478(0.0423) Steps 1030(1094.42) | Grad Norm 6.9480(24.3496) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0333 | Time 122.4173, Epoch Time 1686.6212(1661.8436), Bit/dim 3.5979(best: 3.5673), Xent 0.9867, Loss 4.0913, Error 0.2221(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18320 | Time 25.7317(27.9421) | Bit/dim 3.6068(3.5958) | Xent 0.1289(0.1205) | Loss 9.4563(10.7367) | Error 0.0489(0.0417) Steps 1036(1085.75) | Grad Norm 4.5788(25.5706) | Total Time 0.00(0.00)\n",
      "Iter 18330 | Time 27.2779(27.5834) | Bit/dim 3.5769(3.5946) | Xent 0.1077(0.1181) | Loss 9.4472(10.3914) | Error 0.0311(0.0404) Steps 1054(1076.85) | Grad Norm 3.7172(19.9986) | Total Time 0.00(0.00)\n",
      "Iter 18340 | Time 26.4003(27.2427) | Bit/dim 3.5634(3.5911) | Xent 0.0883(0.1176) | Loss 9.2875(10.1397) | Error 0.0289(0.0402) Steps 1042(1069.78) | Grad Norm 5.1421(15.8656) | Total Time 0.00(0.00)\n",
      "Iter 18350 | Time 26.6617(27.0566) | Bit/dim 3.6099(3.5870) | Xent 0.0994(0.1163) | Loss 9.4667(9.9390) | Error 0.0333(0.0395) Steps 1054(1065.26) | Grad Norm 3.3693(12.6670) | Total Time 0.00(0.00)\n",
      "Iter 18360 | Time 27.8160(26.9644) | Bit/dim 3.5953(3.5883) | Xent 0.1266(0.1200) | Loss 9.5149(9.8151) | Error 0.0456(0.0404) Steps 1090(1066.44) | Grad Norm 3.4014(14.3910) | Total Time 0.00(0.00)\n",
      "Iter 18370 | Time 28.0557(27.0258) | Bit/dim 3.5872(3.5866) | Xent 0.1046(0.1182) | Loss 9.4800(9.7175) | Error 0.0411(0.0401) Steps 1030(1067.70) | Grad Norm 3.9140(14.3012) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0334 | Time 120.2400, Epoch Time 1605.6061(1660.1565), Bit/dim 3.5923(best: 3.5673), Xent 1.0095, Loss 4.0970, Error 0.2253(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18380 | Time 28.2360(27.0150) | Bit/dim 3.6054(3.5862) | Xent 0.1091(0.1143) | Loss 9.4559(10.5092) | Error 0.0311(0.0384) Steps 1084(1069.93) | Grad Norm 10.0680(13.2771) | Total Time 0.00(0.00)\n",
      "Iter 18390 | Time 27.9097(27.0485) | Bit/dim 3.6093(3.5842) | Xent 0.1165(0.1159) | Loss 9.5134(10.2336) | Error 0.0400(0.0396) Steps 1090(1070.13) | Grad Norm 5.0682(12.2179) | Total Time 0.00(0.00)\n",
      "Iter 18400 | Time 27.7549(27.1270) | Bit/dim 3.6109(3.5839) | Xent 0.0993(0.1132) | Loss 9.2765(10.0197) | Error 0.0289(0.0386) Steps 1078(1070.07) | Grad Norm 3.2942(10.8525) | Total Time 0.00(0.00)\n",
      "Iter 18410 | Time 26.4587(27.0954) | Bit/dim 3.6084(3.5838) | Xent 0.1342(0.1133) | Loss 9.5659(9.8774) | Error 0.0456(0.0392) Steps 1066(1068.35) | Grad Norm 4.9510(10.8398) | Total Time 0.00(0.00)\n",
      "Iter 18420 | Time 27.7665(27.2607) | Bit/dim 3.5737(3.5817) | Xent 0.0958(0.1126) | Loss 9.3588(9.7557) | Error 0.0311(0.0389) Steps 1054(1063.51) | Grad Norm 3.4043(12.9296) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0335 | Time 121.9978, Epoch Time 1636.4565(1659.4455), Bit/dim 3.5892(best: 3.5673), Xent 1.0041, Loss 4.0913, Error 0.2245(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18430 | Time 27.5344(27.2277) | Bit/dim 3.5336(3.5828) | Xent 0.1285(0.1115) | Loss 9.3709(10.6685) | Error 0.0422(0.0384) Steps 1078(1065.55) | Grad Norm 20.6093(17.6378) | Total Time 0.00(0.00)\n",
      "Iter 18440 | Time 27.6087(27.3096) | Bit/dim 3.5713(3.5839) | Xent 0.1344(0.1113) | Loss 9.4900(10.3527) | Error 0.0489(0.0380) Steps 1078(1069.26) | Grad Norm 9.5806(18.0890) | Total Time 0.00(0.00)\n",
      "Iter 18450 | Time 26.0055(27.3097) | Bit/dim 3.5931(3.5823) | Xent 0.1246(0.1115) | Loss 9.4919(10.1046) | Error 0.0444(0.0383) Steps 1066(1066.87) | Grad Norm 10.7652(19.0733) | Total Time 0.00(0.00)\n",
      "Iter 18460 | Time 26.2993(27.2939) | Bit/dim 3.5681(3.5820) | Xent 0.1228(0.1139) | Loss 9.3426(9.9217) | Error 0.0356(0.0392) Steps 1054(1067.31) | Grad Norm 14.5868(16.1702) | Total Time 0.00(0.00)\n",
      "Iter 18470 | Time 27.5903(27.3575) | Bit/dim 3.6028(3.5838) | Xent 0.0944(0.1130) | Loss 9.3060(9.7814) | Error 0.0211(0.0388) Steps 1018(1066.82) | Grad Norm 5.6989(14.0837) | Total Time 0.00(0.00)\n",
      "Iter 18480 | Time 27.5185(27.2592) | Bit/dim 3.5896(3.5809) | Xent 0.1052(0.1151) | Loss 9.4681(9.6832) | Error 0.0322(0.0405) Steps 1108(1069.72) | Grad Norm 4.8193(11.9881) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0336 | Time 122.2488, Epoch Time 1644.2366(1658.9892), Bit/dim 3.5847(best: 3.5673), Xent 1.0241, Loss 4.0968, Error 0.2247(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18490 | Time 26.0001(27.1689) | Bit/dim 3.5778(3.5771) | Xent 0.1119(0.1165) | Loss 9.3895(10.4404) | Error 0.0356(0.0409) Steps 1072(1066.44) | Grad Norm 6.1781(10.7961) | Total Time 0.00(0.00)\n",
      "Iter 18500 | Time 27.2786(27.0910) | Bit/dim 3.5840(3.5779) | Xent 0.1150(0.1144) | Loss 9.5437(10.1719) | Error 0.0344(0.0396) Steps 1102(1066.81) | Grad Norm 23.4441(10.0714) | Total Time 0.00(0.00)\n",
      "Iter 18510 | Time 27.1229(27.0830) | Bit/dim 3.5632(3.5759) | Xent 0.1076(0.1136) | Loss 9.3617(9.9550) | Error 0.0356(0.0389) Steps 1072(1063.61) | Grad Norm 4.6593(8.7390) | Total Time 0.00(0.00)\n",
      "Iter 18520 | Time 27.6916(27.1404) | Bit/dim 3.5627(3.5760) | Xent 0.0861(0.1095) | Loss 9.4692(9.8079) | Error 0.0256(0.0375) Steps 1060(1065.04) | Grad Norm 3.1560(8.2599) | Total Time 0.00(0.00)\n",
      "Iter 18530 | Time 26.6943(27.2311) | Bit/dim 3.5892(3.5783) | Xent 0.1269(0.1088) | Loss 9.4925(9.7186) | Error 0.0511(0.0374) Steps 1048(1062.94) | Grad Norm 6.9855(7.8681) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0337 | Time 121.5983, Epoch Time 1633.4857(1658.2241), Bit/dim 3.5870(best: 3.5673), Xent 1.0021, Loss 4.0881, Error 0.2250(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18540 | Time 26.5670(27.2130) | Bit/dim 3.5394(3.5799) | Xent 0.1013(0.1097) | Loss 9.3597(10.6711) | Error 0.0411(0.0379) Steps 1048(1062.42) | Grad Norm 7.4074(7.2186) | Total Time 0.00(0.00)\n",
      "Iter 18550 | Time 27.0423(27.1728) | Bit/dim 3.5893(3.5799) | Xent 0.1072(0.1074) | Loss 9.4695(10.3430) | Error 0.0344(0.0367) Steps 1090(1063.43) | Grad Norm 4.1934(7.3615) | Total Time 0.00(0.00)\n",
      "Iter 18560 | Time 27.7183(27.1549) | Bit/dim 3.6097(3.5783) | Xent 0.0939(0.1086) | Loss 9.4554(10.1000) | Error 0.0400(0.0372) Steps 1042(1063.76) | Grad Norm 10.9708(7.0916) | Total Time 0.00(0.00)\n",
      "Iter 18570 | Time 27.2503(27.2673) | Bit/dim 3.5820(3.5782) | Xent 0.0891(0.1061) | Loss 9.4525(9.9137) | Error 0.0278(0.0364) Steps 1054(1066.85) | Grad Norm 5.9586(6.7695) | Total Time 0.00(0.00)\n",
      "Iter 18580 | Time 27.3378(27.2461) | Bit/dim 3.5811(3.5790) | Xent 0.1398(0.1058) | Loss 9.5213(9.7731) | Error 0.0556(0.0367) Steps 1102(1068.34) | Grad Norm 45.0298(7.8251) | Total Time 0.00(0.00)\n",
      "Iter 18590 | Time 27.1000(27.1533) | Bit/dim 3.5811(3.5783) | Xent 0.1178(0.1077) | Loss 9.4714(9.6721) | Error 0.0411(0.0370) Steps 1054(1068.12) | Grad Norm 6.1788(7.1857) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0338 | Time 121.8817, Epoch Time 1634.6148(1657.5159), Bit/dim 3.5805(best: 3.5673), Xent 1.0188, Loss 4.0898, Error 0.2245(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18600 | Time 26.0394(27.1089) | Bit/dim 3.5902(3.5775) | Xent 0.1062(0.1076) | Loss 9.3806(10.4554) | Error 0.0344(0.0367) Steps 1066(1067.98) | Grad Norm 4.0152(6.8513) | Total Time 0.00(0.00)\n",
      "Iter 18610 | Time 26.4207(27.0614) | Bit/dim 3.5761(3.5760) | Xent 0.1195(0.1061) | Loss 9.3002(10.1689) | Error 0.0478(0.0368) Steps 1060(1067.42) | Grad Norm 3.1318(6.7512) | Total Time 0.00(0.00)\n",
      "Iter 18620 | Time 26.1667(27.1369) | Bit/dim 3.5762(3.5769) | Xent 0.1112(0.1074) | Loss 9.3439(9.9700) | Error 0.0300(0.0370) Steps 1072(1065.43) | Grad Norm 3.0287(6.1464) | Total Time 0.00(0.00)\n",
      "Iter 18630 | Time 26.1403(26.9679) | Bit/dim 3.5472(3.5740) | Xent 0.1191(0.1066) | Loss 9.4014(9.8128) | Error 0.0467(0.0369) Steps 1066(1060.70) | Grad Norm 3.8508(5.8807) | Total Time 0.00(0.00)\n",
      "Iter 18640 | Time 27.0267(26.9879) | Bit/dim 3.5661(3.5716) | Xent 0.0927(0.1060) | Loss 9.4013(9.7035) | Error 0.0278(0.0367) Steps 1012(1062.37) | Grad Norm 2.6595(5.5280) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0339 | Time 121.4009, Epoch Time 1627.3287(1656.6102), Bit/dim 3.5790(best: 3.5673), Xent 1.0245, Loss 4.0912, Error 0.2244(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18650 | Time 27.9879(27.1150) | Bit/dim 3.5947(3.5732) | Xent 0.1015(0.1045) | Loss 9.4725(10.6629) | Error 0.0356(0.0363) Steps 1030(1064.95) | Grad Norm 3.6407(5.1507) | Total Time 0.00(0.00)\n",
      "Iter 18660 | Time 26.8588(27.2109) | Bit/dim 3.5724(3.5726) | Xent 0.1008(0.1036) | Loss 9.3346(10.3372) | Error 0.0378(0.0357) Steps 1078(1065.51) | Grad Norm 7.4063(5.4267) | Total Time 0.00(0.00)\n",
      "Iter 18670 | Time 26.5308(27.2220) | Bit/dim 3.5825(3.5715) | Xent 0.0960(0.1040) | Loss 9.5198(10.0987) | Error 0.0333(0.0360) Steps 1066(1065.36) | Grad Norm 3.5702(5.4246) | Total Time 0.00(0.00)\n",
      "Iter 18680 | Time 27.0359(27.1673) | Bit/dim 3.5825(3.5707) | Xent 0.0937(0.1034) | Loss 9.4306(9.9104) | Error 0.0344(0.0361) Steps 1072(1065.85) | Grad Norm 3.3211(5.0163) | Total Time 0.00(0.00)\n",
      "Iter 18690 | Time 26.1685(27.0146) | Bit/dim 3.5654(3.5703) | Xent 0.1060(0.1035) | Loss 9.3772(9.7793) | Error 0.0367(0.0364) Steps 1066(1064.79) | Grad Norm 3.3580(5.0441) | Total Time 0.00(0.00)\n",
      "Iter 18700 | Time 27.5175(27.0734) | Bit/dim 3.5811(3.5709) | Xent 0.0997(0.1030) | Loss 9.5233(9.6951) | Error 0.0378(0.0361) Steps 1090(1067.97) | Grad Norm 3.2112(4.8332) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0340 | Time 122.3559, Epoch Time 1632.8262(1655.8967), Bit/dim 3.5851(best: 3.5673), Xent 1.0600, Loss 4.1151, Error 0.2290(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18710 | Time 27.6841(27.0901) | Bit/dim 3.5138(3.5720) | Xent 0.0736(0.1010) | Loss 9.3338(10.4680) | Error 0.0200(0.0351) Steps 1054(1065.94) | Grad Norm 2.7406(4.6145) | Total Time 0.00(0.00)\n",
      "Iter 18720 | Time 28.4196(27.1972) | Bit/dim 3.5412(3.5715) | Xent 0.1310(0.1033) | Loss 9.5501(10.1875) | Error 0.0467(0.0359) Steps 1114(1068.36) | Grad Norm 3.9353(5.2878) | Total Time 0.00(0.00)\n",
      "Iter 18730 | Time 27.6754(27.3582) | Bit/dim 3.5985(3.5731) | Xent 0.0767(0.1026) | Loss 9.3954(9.9875) | Error 0.0256(0.0355) Steps 1066(1067.12) | Grad Norm 4.5135(5.0259) | Total Time 0.00(0.00)\n",
      "Iter 18740 | Time 27.4322(27.2757) | Bit/dim 3.5465(3.5737) | Xent 0.1488(0.1049) | Loss 9.4303(9.8269) | Error 0.0478(0.0367) Steps 1072(1065.85) | Grad Norm 5.8061(5.6175) | Total Time 0.00(0.00)\n",
      "Iter 18750 | Time 27.0159(27.3569) | Bit/dim 3.5779(3.5709) | Xent 0.0913(0.1066) | Loss 9.4861(9.7178) | Error 0.0344(0.0372) Steps 1078(1069.95) | Grad Norm 6.2612(5.5490) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0341 | Time 123.2431, Epoch Time 1650.0657(1655.7218), Bit/dim 3.5854(best: 3.5673), Xent 1.0401, Loss 4.1055, Error 0.2255(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18760 | Time 27.6002(27.3609) | Bit/dim 3.5953(3.5721) | Xent 0.1079(0.1065) | Loss 9.5217(10.6735) | Error 0.0422(0.0370) Steps 1048(1070.95) | Grad Norm 5.1438(5.6489) | Total Time 0.00(0.00)\n",
      "Iter 18770 | Time 27.5047(27.4458) | Bit/dim 3.5719(3.5701) | Xent 0.0739(0.1050) | Loss 9.4813(10.3448) | Error 0.0244(0.0362) Steps 1066(1069.65) | Grad Norm 2.5251(5.6969) | Total Time 0.00(0.00)\n",
      "Iter 18780 | Time 27.7675(27.4427) | Bit/dim 3.5714(3.5711) | Xent 0.0947(0.1038) | Loss 9.4500(10.0937) | Error 0.0356(0.0363) Steps 1048(1068.41) | Grad Norm 7.6770(5.7322) | Total Time 0.00(0.00)\n",
      "Iter 18790 | Time 27.2705(27.3852) | Bit/dim 3.5893(3.5755) | Xent 0.0911(0.1049) | Loss 9.4171(9.9183) | Error 0.0300(0.0361) Steps 1036(1067.32) | Grad Norm 3.2819(7.0377) | Total Time 0.00(0.00)\n",
      "Iter 18800 | Time 27.0710(27.2959) | Bit/dim 3.5874(3.5746) | Xent 0.1286(0.1046) | Loss 9.4287(9.7840) | Error 0.0433(0.0366) Steps 1054(1063.55) | Grad Norm 4.2032(6.5428) | Total Time 0.00(0.00)\n",
      "Iter 18810 | Time 26.6503(27.3209) | Bit/dim 3.5542(3.5705) | Xent 0.0971(0.1039) | Loss 9.2393(9.6647) | Error 0.0344(0.0365) Steps 1066(1062.20) | Grad Norm 8.7412(6.4341) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0342 | Time 122.3302, Epoch Time 1644.6026(1655.3882), Bit/dim 3.5831(best: 3.5673), Xent 1.0525, Loss 4.1094, Error 0.2261(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18820 | Time 27.1321(27.2868) | Bit/dim 3.5640(3.5728) | Xent 0.0700(0.1022) | Loss 9.3948(10.4782) | Error 0.0222(0.0356) Steps 1084(1064.36) | Grad Norm 3.6726(5.8988) | Total Time 0.00(0.00)\n",
      "Iter 18830 | Time 27.4087(27.2560) | Bit/dim 3.5665(3.5715) | Xent 0.1084(0.1022) | Loss 9.3192(10.2020) | Error 0.0356(0.0353) Steps 1042(1066.86) | Grad Norm 4.7996(5.4334) | Total Time 0.00(0.00)\n",
      "Iter 18840 | Time 27.8597(27.3685) | Bit/dim 3.5982(3.5727) | Xent 0.0948(0.1025) | Loss 9.5604(9.9982) | Error 0.0333(0.0358) Steps 1072(1062.47) | Grad Norm 7.0485(5.3152) | Total Time 0.00(0.00)\n",
      "Iter 18850 | Time 27.2192(27.2680) | Bit/dim 3.5460(3.5695) | Xent 0.1187(0.1030) | Loss 9.4773(9.8397) | Error 0.0422(0.0355) Steps 1078(1066.55) | Grad Norm 4.8445(5.3173) | Total Time 0.00(0.00)\n",
      "Iter 18860 | Time 28.2475(27.2842) | Bit/dim 3.5923(3.5686) | Xent 0.0881(0.1003) | Loss 9.4142(9.7149) | Error 0.0300(0.0346) Steps 1036(1065.09) | Grad Norm 4.1683(5.4452) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0343 | Time 122.6686, Epoch Time 1641.2329(1654.9636), Bit/dim 3.5795(best: 3.5673), Xent 1.0657, Loss 4.1123, Error 0.2263(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18870 | Time 27.7305(27.2666) | Bit/dim 3.5712(3.5717) | Xent 0.0965(0.1017) | Loss 9.5400(10.6307) | Error 0.0344(0.0355) Steps 1072(1065.14) | Grad Norm 14.4564(5.5732) | Total Time 0.00(0.00)\n",
      "Iter 18880 | Time 26.5529(27.2741) | Bit/dim 3.6072(3.5713) | Xent 0.0821(0.1009) | Loss 9.5577(10.3141) | Error 0.0289(0.0351) Steps 1114(1065.94) | Grad Norm 5.1927(5.3214) | Total Time 0.00(0.00)\n",
      "Iter 18890 | Time 27.9157(27.5744) | Bit/dim 3.5439(3.5698) | Xent 0.1006(0.1003) | Loss 9.5064(10.0827) | Error 0.0322(0.0349) Steps 1102(1073.98) | Grad Norm 5.6913(6.2630) | Total Time 0.00(0.00)\n",
      "Iter 18900 | Time 27.0380(27.7329) | Bit/dim 3.5761(3.5707) | Xent 0.1045(0.0993) | Loss 9.5068(9.9159) | Error 0.0389(0.0350) Steps 1102(1075.73) | Grad Norm 11.1955(9.5520) | Total Time 0.00(0.00)\n",
      "Iter 18910 | Time 27.7934(27.7278) | Bit/dim 3.5497(3.5708) | Xent 0.1154(0.1017) | Loss 9.3719(9.7837) | Error 0.0389(0.0356) Steps 1090(1076.74) | Grad Norm 15.7452(9.2969) | Total Time 0.00(0.00)\n",
      "Iter 18920 | Time 28.7561(27.8119) | Bit/dim 3.5717(3.5710) | Xent 0.0935(0.1028) | Loss 9.3455(9.6784) | Error 0.0389(0.0363) Steps 1066(1077.99) | Grad Norm 8.2143(10.1408) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0344 | Time 123.1672, Epoch Time 1674.0943(1655.5375), Bit/dim 3.5853(best: 3.5673), Xent 1.0608, Loss 4.1157, Error 0.2276(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18930 | Time 28.0477(27.8630) | Bit/dim 3.5968(3.5711) | Xent 0.1118(0.0999) | Loss 9.5981(10.4891) | Error 0.0367(0.0347) Steps 1102(1084.55) | Grad Norm 25.4127(10.0032) | Total Time 0.00(0.00)\n",
      "Iter 18940 | Time 26.9573(27.8573) | Bit/dim 3.5826(3.5735) | Xent 0.0786(0.1012) | Loss 9.3463(10.2175) | Error 0.0300(0.0357) Steps 1054(1082.89) | Grad Norm 16.1782(9.8192) | Total Time 0.00(0.00)\n",
      "Iter 18950 | Time 28.7443(27.8779) | Bit/dim 3.5879(3.5745) | Xent 0.1261(0.1013) | Loss 9.5860(10.0167) | Error 0.0344(0.0356) Steps 1126(1082.45) | Grad Norm 10.6109(10.5399) | Total Time 0.00(0.00)\n",
      "Iter 18960 | Time 27.4510(27.8290) | Bit/dim 3.5158(3.5730) | Xent 0.0977(0.1013) | Loss 9.3316(9.8482) | Error 0.0333(0.0354) Steps 1054(1082.97) | Grad Norm 6.8323(12.0450) | Total Time 0.00(0.00)\n",
      "Iter 18970 | Time 28.0391(27.8796) | Bit/dim 3.5735(3.5733) | Xent 0.0866(0.0997) | Loss 9.5649(9.7387) | Error 0.0322(0.0351) Steps 1126(1085.20) | Grad Norm 6.0958(12.1934) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0345 | Time 124.1308, Epoch Time 1678.4774(1656.2257), Bit/dim 3.5844(best: 3.5673), Xent 1.0768, Loss 4.1229, Error 0.2284(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18980 | Time 27.1793(27.9636) | Bit/dim 3.5627(3.5747) | Xent 0.0728(0.0998) | Loss 9.3523(10.6917) | Error 0.0233(0.0351) Steps 1072(1084.55) | Grad Norm 6.1516(15.8447) | Total Time 0.00(0.00)\n",
      "Iter 18990 | Time 28.9134(27.9587) | Bit/dim 3.5913(3.5764) | Xent 0.0972(0.1017) | Loss 9.4460(10.3558) | Error 0.0367(0.0363) Steps 1108(1083.99) | Grad Norm 28.3674(18.4912) | Total Time 0.00(0.00)\n",
      "Iter 19000 | Time 31.6474(28.3980) | Bit/dim 3.7405(3.5966) | Xent 0.3022(0.1280) | Loss 10.0191(10.1902) | Error 0.1056(0.0442) Steps 1186(1093.67) | Grad Norm 161.1786(162.1314) | Total Time 0.00(0.00)\n",
      "Iter 19010 | Time 35.8310(29.9128) | Bit/dim 4.3620(3.7334) | Xent 1.0227(0.2976) | Loss 12.3637(10.5478) | Error 0.2311(0.0807) Steps 1324(1138.80) | Grad Norm 1135.8781(410.9503) | Total Time 0.00(0.00)\n",
      "Iter 19020 | Time 26.3450(30.1422) | Bit/dim 4.1216(3.8195) | Xent 1.0259(0.4750) | Loss 11.3500(10.7720) | Error 0.3089(0.1308) Steps 1060(1143.64) | Grad Norm 53.7140(1263.4559) | Total Time 0.00(0.00)\n",
      "Iter 19030 | Time 25.4923(29.1484) | Bit/dim 4.0499(3.8817) | Xent 0.7174(0.5445) | Loss 11.0217(10.8316) | Error 0.2356(0.1581) Steps 1012(1114.32) | Grad Norm 13.4660(937.7399) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0346 | Time 113.5608, Epoch Time 1762.6459(1659.4183), Bit/dim 4.0136(best: 3.5673), Xent 0.9253, Loss 4.4763, Error 0.2702(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 19040 | Time 24.5190(28.0771) | Bit/dim 3.9149(3.9036) | Xent 0.5215(0.5520) | Loss 10.4509(11.6483) | Error 0.1822(0.1665) Steps 1012(1090.81) | Grad Norm 7.7738(693.9219) | Total Time 0.00(0.00)\n",
      "Iter 19050 | Time 24.2587(27.2372) | Bit/dim 3.8745(3.9062) | Xent 0.4446(0.5257) | Loss 10.1973(11.3062) | Error 0.1467(0.1630) Steps 1030(1071.65) | Grad Norm 6.6751(514.1457) | Total Time 0.00(0.00)\n",
      "Iter 19060 | Time 25.5845(26.7022) | Bit/dim 3.8846(3.8987) | Xent 0.3469(0.4793) | Loss 10.1964(11.0242) | Error 0.1167(0.1519) Steps 1006(1057.11) | Grad Norm 5.6018(381.1347) | Total Time 0.00(0.00)\n",
      "Iter 19070 | Time 26.1045(26.3685) | Bit/dim 3.8434(3.8835) | Xent 0.2971(0.4374) | Loss 10.1763(10.7891) | Error 0.1100(0.1411) Steps 1066(1052.36) | Grad Norm 8.1648(283.0795) | Total Time 0.00(0.00)\n",
      "Iter 19080 | Time 25.5420(26.1541) | Bit/dim 3.8138(3.8653) | Xent 0.2742(0.4017) | Loss 9.9244(10.5849) | Error 0.0922(0.1311) Steps 1024(1047.76) | Grad Norm 6.7200(210.8179) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0347 | Time 113.5281, Epoch Time 1522.9981(1655.3257), Bit/dim 3.7977(best: 3.5673), Xent 0.8676, Loss 4.2315, Error 0.2268(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 19090 | Time 26.2035(26.1992) | Bit/dim 3.8016(3.8463) | Xent 0.2712(0.3638) | Loss 10.0492(11.3961) | Error 0.0911(0.1206) Steps 1072(1050.17) | Grad Norm 8.4480(157.0233) | Total Time 0.00(0.00)\n",
      "Iter 19100 | Time 24.5119(26.2049) | Bit/dim 3.7710(3.8280) | Xent 0.2168(0.3281) | Loss 9.7208(10.9943) | Error 0.0811(0.1100) Steps 994(1048.00) | Grad Norm 5.2138(117.6316) | Total Time 0.00(0.00)\n",
      "Iter 19110 | Time 26.4120(26.2490) | Bit/dim 3.7476(3.8108) | Xent 0.2887(0.3031) | Loss 9.9397(10.6978) | Error 0.0900(0.1027) Steps 1072(1051.37) | Grad Norm 6.1353(88.3705) | Total Time 0.00(0.00)\n",
      "Iter 19120 | Time 25.4138(26.2906) | Bit/dim 3.7283(3.7964) | Xent 0.2156(0.2844) | Loss 9.7821(10.4793) | Error 0.0822(0.0972) Steps 1054(1052.65) | Grad Norm 5.3758(67.2086) | Total Time 0.00(0.00)\n",
      "Iter 19130 | Time 26.5796(26.2780) | Bit/dim 3.7122(3.7826) | Xent 0.2065(0.2667) | Loss 9.7514(10.3050) | Error 0.0789(0.0922) Steps 1048(1053.25) | Grad Norm 4.7555(51.0262) | Total Time 0.00(0.00)\n",
      "Iter 19140 | Time 27.0686(26.2465) | Bit/dim 3.7719(3.7717) | Xent 0.2214(0.2511) | Loss 10.0395(10.1754) | Error 0.0711(0.0870) Steps 1060(1053.88) | Grad Norm 31.7569(39.8867) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0348 | Time 113.3498, Epoch Time 1576.7678(1652.9689), Bit/dim 3.7405(best: 3.5673), Xent 0.8977, Loss 4.1894, Error 0.2219(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 19150 | Time 26.8373(26.2487) | Bit/dim 3.7497(3.7618) | Xent 0.2043(0.2366) | Loss 9.8926(10.8912) | Error 0.0700(0.0815) Steps 1096(1056.30) | Grad Norm 6.6967(30.8597) | Total Time 0.00(0.00)\n",
      "Iter 19160 | Time 24.3992(26.0669) | Bit/dim 3.7076(3.7517) | Xent 0.1515(0.2267) | Loss 9.4427(10.5814) | Error 0.0622(0.0787) Steps 1006(1051.72) | Grad Norm 4.5754(24.3104) | Total Time 0.00(0.00)\n",
      "Iter 19170 | Time 26.2292(26.0783) | Bit/dim 3.7556(3.7428) | Xent 0.1807(0.2174) | Loss 9.7100(10.3513) | Error 0.0644(0.0759) Steps 1030(1054.96) | Grad Norm 5.3184(19.4145) | Total Time 0.00(0.00)\n",
      "Iter 19180 | Time 25.7431(26.0702) | Bit/dim 3.7060(3.7312) | Xent 0.1770(0.2083) | Loss 9.6153(10.1677) | Error 0.0656(0.0726) Steps 1072(1052.04) | Grad Norm 4.7549(16.5954) | Total Time 0.00(0.00)\n",
      "Iter 19190 | Time 25.6765(26.1213) | Bit/dim 3.7104(3.7248) | Xent 0.1810(0.2011) | Loss 9.7403(10.0359) | Error 0.0700(0.0704) Steps 1030(1050.24) | Grad Norm 5.8802(13.7923) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0349 | Time 113.6543, Epoch Time 1562.9670(1650.2689), Bit/dim 3.7081(best: 3.5673), Xent 0.9151, Loss 4.1656, Error 0.2219(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 19200 | Time 25.9899(26.1884) | Bit/dim 3.6612(3.7164) | Xent 0.1606(0.1949) | Loss 9.6886(10.9448) | Error 0.0478(0.0675) Steps 1066(1049.89) | Grad Norm 4.7738(17.2908) | Total Time 0.00(0.00)\n",
      "Iter 19210 | Time 26.8920(26.2581) | Bit/dim 3.6985(3.7127) | Xent 0.1777(0.1864) | Loss 9.7251(10.6140) | Error 0.0600(0.0642) Steps 1060(1048.68) | Grad Norm 8.1443(14.1522) | Total Time 0.00(0.00)\n",
      "Iter 19220 | Time 26.1957(26.2623) | Bit/dim 3.7209(3.7084) | Xent 0.1603(0.1819) | Loss 9.6116(10.3652) | Error 0.0556(0.0636) Steps 1042(1050.02) | Grad Norm 3.8386(12.2823) | Total Time 0.00(0.00)\n",
      "Iter 19230 | Time 26.9414(26.4010) | Bit/dim 3.6849(3.7027) | Xent 0.1864(0.1785) | Loss 9.6668(10.1650) | Error 0.0633(0.0624) Steps 1078(1049.77) | Grad Norm 5.1784(11.8153) | Total Time 0.00(0.00)\n",
      "Iter 19240 | Time 26.5543(26.4138) | Bit/dim 3.7016(3.6980) | Xent 0.1774(0.1753) | Loss 9.7939(10.0418) | Error 0.0622(0.0613) Steps 1066(1048.44) | Grad Norm 6.4251(10.5858) | Total Time 0.00(0.00)\n",
      "Iter 19250 | Time 26.5032(26.3167) | Bit/dim 3.6812(3.6936) | Xent 0.1859(0.1716) | Loss 9.6805(9.9231) | Error 0.0711(0.0606) Steps 1012(1046.10) | Grad Norm 5.8845(9.7683) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0350 | Time 115.0460, Epoch Time 1583.9039(1648.2779), Bit/dim 3.6867(best: 3.5673), Xent 0.9331, Loss 4.1532, Error 0.2215(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 19260 | Time 26.6359(26.3251) | Bit/dim 3.6674(3.6874) | Xent 0.1181(0.1649) | Loss 9.5540(10.6852) | Error 0.0422(0.0580) Steps 1084(1050.86) | Grad Norm 4.9659(8.5291) | Total Time 0.00(0.00)\n",
      "Iter 19270 | Time 27.1818(26.4170) | Bit/dim 3.6548(3.6846) | Xent 0.1856(0.1639) | Loss 9.5726(10.4126) | Error 0.0678(0.0581) Steps 1090(1053.40) | Grad Norm 6.4387(9.4560) | Total Time 0.00(0.00)\n",
      "Iter 19280 | Time 27.5357(26.5378) | Bit/dim 3.7030(3.6844) | Xent 0.1630(0.1601) | Loss 9.7192(10.2120) | Error 0.0578(0.0571) Steps 1066(1053.84) | Grad Norm 10.4936(15.4492) | Total Time 0.00(0.00)\n",
      "Iter 19290 | Time 27.2943(26.5198) | Bit/dim 3.6478(3.6801) | Xent 0.1871(0.1591) | Loss 9.4985(10.0505) | Error 0.0689(0.0565) Steps 1066(1053.56) | Grad Norm 4.8702(13.1437) | Total Time 0.00(0.00)\n",
      "Iter 19300 | Time 26.2858(26.5513) | Bit/dim 3.6543(3.6758) | Xent 0.1539(0.1570) | Loss 9.6242(9.9336) | Error 0.0633(0.0561) Steps 1096(1055.74) | Grad Norm 8.7223(11.8953) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0351 | Time 114.0141, Epoch Time 1595.8986(1646.7065), Bit/dim 3.6758(best: 3.5673), Xent 0.9542, Loss 4.1530, Error 0.2245(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 19310 | Time 27.6089(26.6299) | Bit/dim 3.6911(3.6749) | Xent 0.1457(0.1534) | Loss 9.7155(10.8511) | Error 0.0489(0.0543) Steps 1084(1054.14) | Grad Norm 7.3408(10.3526) | Total Time 0.00(0.00)\n",
      "Iter 19320 | Time 25.2762(26.4915) | Bit/dim 3.6441(3.6716) | Xent 0.1427(0.1512) | Loss 9.4582(10.5133) | Error 0.0500(0.0534) Steps 1060(1057.56) | Grad Norm 7.7481(9.1329) | Total Time 0.00(0.00)\n",
      "Iter 19330 | Time 26.3217(26.3808) | Bit/dim 3.6290(3.6678) | Xent 0.1238(0.1483) | Loss 9.5975(10.2728) | Error 0.0422(0.0527) Steps 1078(1057.69) | Grad Norm 5.6182(8.2239) | Total Time 0.00(0.00)\n",
      "Iter 19340 | Time 26.6727(26.3772) | Bit/dim 3.7039(3.6679) | Xent 0.1639(0.1479) | Loss 9.6451(10.0966) | Error 0.0578(0.0524) Steps 1072(1061.85) | Grad Norm 4.7083(9.4390) | Total Time 0.00(0.00)\n",
      "Iter 19350 | Time 26.0848(26.4798) | Bit/dim 3.6546(3.6645) | Xent 0.1456(0.1484) | Loss 9.5826(9.9715) | Error 0.0544(0.0521) Steps 1060(1065.20) | Grad Norm 4.0172(10.0028) | Total Time 0.00(0.00)\n",
      "Iter 19360 | Time 27.2947(26.5941) | Bit/dim 3.6366(3.6620) | Xent 0.1482(0.1471) | Loss 9.6639(9.8836) | Error 0.0489(0.0515) Steps 1054(1065.88) | Grad Norm 4.9601(9.7308) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0352 | Time 114.0261, Epoch Time 1586.4463(1644.8987), Bit/dim 3.6619(best: 3.5673), Xent 0.9620, Loss 4.1429, Error 0.2231(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 19370 | Time 26.0764(26.5469) | Bit/dim 3.6854(3.6568) | Xent 0.0984(0.1437) | Loss 9.6502(10.6647) | Error 0.0344(0.0503) Steps 1090(1066.94) | Grad Norm 3.8939(9.0094) | Total Time 0.00(0.00)\n",
      "Iter 19380 | Time 25.9901(26.4876) | Bit/dim 3.6885(3.6581) | Xent 0.1606(0.1433) | Loss 9.6932(10.3844) | Error 0.0644(0.0506) Steps 1102(1063.95) | Grad Norm 7.5801(8.9086) | Total Time 0.00(0.00)\n",
      "Iter 19390 | Time 25.9780(26.4763) | Bit/dim 3.6847(3.6559) | Xent 0.1031(0.1431) | Loss 9.6616(10.1769) | Error 0.0367(0.0506) Steps 1060(1065.29) | Grad Norm 3.5929(8.4920) | Total Time 0.00(0.00)\n",
      "Iter 19400 | Time 26.5503(26.4939) | Bit/dim 3.6561(3.6563) | Xent 0.1350(0.1414) | Loss 9.5991(10.0165) | Error 0.0511(0.0500) Steps 1048(1062.44) | Grad Norm 4.7703(8.4947) | Total Time 0.00(0.00)\n",
      "Iter 19410 | Time 25.8161(26.3940) | Bit/dim 3.6652(3.6531) | Xent 0.1365(0.1396) | Loss 9.5942(9.8892) | Error 0.0456(0.0482) Steps 1030(1061.14) | Grad Norm 7.2177(8.3184) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0353 | Time 115.1409, Epoch Time 1583.4538(1643.0554), Bit/dim 3.6556(best: 3.5673), Xent 0.9587, Loss 4.1350, Error 0.2239(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 19420 | Time 27.2144(26.3654) | Bit/dim 3.6377(3.6541) | Xent 0.1494(0.1387) | Loss 9.5689(10.7976) | Error 0.0589(0.0485) Steps 1102(1064.49) | Grad Norm 3.7387(7.6756) | Total Time 0.00(0.00)\n",
      "Iter 19430 | Time 26.3572(26.3415) | Bit/dim 3.6527(3.6530) | Xent 0.1453(0.1379) | Loss 9.6678(10.4683) | Error 0.0589(0.0484) Steps 1072(1065.50) | Grad Norm 76.2461(10.5719) | Total Time 0.00(0.00)\n",
      "Iter 19440 | Time 27.0347(26.5104) | Bit/dim 3.6371(3.6509) | Xent 0.1158(0.1355) | Loss 9.6003(10.2359) | Error 0.0433(0.0479) Steps 1048(1065.47) | Grad Norm 4.3180(9.5386) | Total Time 0.00(0.00)\n",
      "Iter 19450 | Time 26.6193(26.4445) | Bit/dim 3.6410(3.6483) | Xent 0.1600(0.1339) | Loss 9.4567(10.0425) | Error 0.0544(0.0471) Steps 1060(1066.38) | Grad Norm 4.0566(12.7953) | Total Time 0.00(0.00)\n",
      "Iter 19460 | Time 26.5902(26.5076) | Bit/dim 3.6268(3.6449) | Xent 0.1301(0.1307) | Loss 9.4711(9.9044) | Error 0.0389(0.0457) Steps 1078(1070.60) | Grad Norm 3.3075(13.0197) | Total Time 0.00(0.00)\n",
      "Iter 19470 | Time 26.6200(26.5170) | Bit/dim 3.6275(3.6441) | Xent 0.1216(0.1307) | Loss 9.5730(9.8193) | Error 0.0500(0.0452) Steps 1078(1067.92) | Grad Norm 4.1328(11.4617) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0354 | Time 115.7123, Epoch Time 1590.0066(1641.4639), Bit/dim 3.6502(best: 3.5673), Xent 0.9946, Loss 4.1475, Error 0.2228(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 19480 | Time 26.1171(26.5266) | Bit/dim 3.6360(3.6417) | Xent 0.1209(0.1305) | Loss 9.5058(10.5812) | Error 0.0400(0.0454) Steps 1066(1061.41) | Grad Norm 5.4986(10.4931) | Total Time 0.00(0.00)\n",
      "Iter 19490 | Time 25.3056(26.4967) | Bit/dim 3.6206(3.6416) | Xent 0.1429(0.1313) | Loss 9.4589(10.3034) | Error 0.0544(0.0456) Steps 1078(1062.44) | Grad Norm 4.2362(9.2785) | Total Time 0.00(0.00)\n",
      "Iter 19500 | Time 26.7777(26.4936) | Bit/dim 3.6544(3.6405) | Xent 0.1153(0.1273) | Loss 9.7047(10.1016) | Error 0.0389(0.0438) Steps 1060(1060.88) | Grad Norm 4.2728(7.9072) | Total Time 0.00(0.00)\n",
      "Iter 19510 | Time 25.4412(26.4171) | Bit/dim 3.6424(3.6379) | Xent 0.1062(0.1269) | Loss 9.6177(9.9414) | Error 0.0400(0.0441) Steps 1066(1060.88) | Grad Norm 3.3161(7.0426) | Total Time 0.00(0.00)\n",
      "Iter 19520 | Time 26.6867(26.4077) | Bit/dim 3.6435(3.6384) | Xent 0.1260(0.1274) | Loss 9.4956(9.8389) | Error 0.0433(0.0447) Steps 1090(1055.98) | Grad Norm 5.3875(6.6432) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0355 | Time 115.2880, Epoch Time 1582.2762(1639.6883), Bit/dim 3.6458(best: 3.5673), Xent 1.0221, Loss 4.1568, Error 0.2271(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 19530 | Time 26.2553(26.3098) | Bit/dim 3.6045(3.6364) | Xent 0.1551(0.1286) | Loss 9.4438(10.7540) | Error 0.0511(0.0453) Steps 1078(1056.17) | Grad Norm 5.4217(6.4838) | Total Time 0.00(0.00)\n",
      "Iter 19540 | Time 26.9276(26.2777) | Bit/dim 3.6921(3.6359) | Xent 0.1184(0.1293) | Loss 9.6055(10.4247) | Error 0.0422(0.0454) Steps 1090(1059.22) | Grad Norm 5.8657(6.2827) | Total Time 0.00(0.00)\n",
      "Iter 19550 | Time 26.2132(26.2918) | Bit/dim 3.6227(3.6313) | Xent 0.1292(0.1280) | Loss 9.4039(10.1749) | Error 0.0478(0.0452) Steps 1078(1058.98) | Grad Norm 8.6029(6.6460) | Total Time 0.00(0.00)\n",
      "Iter 19560 | Time 26.4811(26.3397) | Bit/dim 3.6439(3.6312) | Xent 0.1316(0.1300) | Loss 9.5736(10.0068) | Error 0.0511(0.0459) Steps 1072(1060.06) | Grad Norm 4.2541(6.4410) | Total Time 0.00(0.00)\n",
      "Iter 19570 | Time 25.6723(26.4444) | Bit/dim 3.6314(3.6295) | Xent 0.1149(0.1284) | Loss 9.5142(9.8738) | Error 0.0389(0.0452) Steps 1018(1061.57) | Grad Norm 6.2048(6.3985) | Total Time 0.00(0.00)\n",
      "Iter 19580 | Time 26.9596(26.5651) | Bit/dim 3.6131(3.6268) | Xent 0.0974(0.1249) | Loss 9.5195(9.7777) | Error 0.0300(0.0438) Steps 1090(1063.00) | Grad Norm 3.9222(5.8359) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0356 | Time 114.0072, Epoch Time 1588.5891(1638.1553), Bit/dim 3.6311(best: 3.5673), Xent 0.9906, Loss 4.1264, Error 0.2234(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 19590 | Time 26.2515(26.4897) | Bit/dim 3.6300(3.6265) | Xent 0.1576(0.1215) | Loss 9.5275(10.5698) | Error 0.0511(0.0429) Steps 1048(1061.79) | Grad Norm 5.1526(5.7525) | Total Time 0.00(0.00)\n",
      "Iter 19600 | Time 25.9406(26.3717) | Bit/dim 3.6076(3.6272) | Xent 0.1382(0.1218) | Loss 9.5520(10.2977) | Error 0.0478(0.0429) Steps 1054(1062.48) | Grad Norm 4.6906(5.3974) | Total Time 0.00(0.00)\n",
      "Iter 19610 | Time 26.6154(26.3191) | Bit/dim 3.6107(3.6249) | Xent 0.1138(0.1225) | Loss 9.4944(10.0956) | Error 0.0411(0.0428) Steps 1072(1062.98) | Grad Norm 4.8497(5.3374) | Total Time 0.00(0.00)\n",
      "Iter 19620 | Time 26.4704(26.3008) | Bit/dim 3.5910(3.6197) | Xent 0.1389(0.1223) | Loss 9.3678(9.9251) | Error 0.0444(0.0422) Steps 1066(1062.61) | Grad Norm 5.9373(5.2795) | Total Time 0.00(0.00)\n",
      "Iter 19630 | Time 26.6920(26.3012) | Bit/dim 3.6088(3.6145) | Xent 0.1389(0.1232) | Loss 9.5190(9.8089) | Error 0.0556(0.0435) Steps 1060(1063.15) | Grad Norm 5.7114(5.3528) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0357 | Time 116.0288, Epoch Time 1571.8156(1636.1651), Bit/dim 3.6200(best: 3.5673), Xent 0.9939, Loss 4.1169, Error 0.2237(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 19640 | Time 25.5998(26.2350) | Bit/dim 3.6156(3.6137) | Xent 0.0939(0.1194) | Loss 9.3935(10.7002) | Error 0.0367(0.0427) Steps 1036(1060.11) | Grad Norm 4.5056(5.3913) | Total Time 0.00(0.00)\n",
      "Iter 19650 | Time 26.4542(26.2252) | Bit/dim 3.5930(3.6139) | Xent 0.0941(0.1178) | Loss 9.4927(10.3804) | Error 0.0356(0.0419) Steps 1072(1061.14) | Grad Norm 2.9062(5.0812) | Total Time 0.00(0.00)\n",
      "Iter 19660 | Time 25.7394(26.2272) | Bit/dim 3.6377(3.6151) | Xent 0.1556(0.1193) | Loss 9.5444(10.1575) | Error 0.0500(0.0419) Steps 1042(1060.17) | Grad Norm 6.3509(5.1081) | Total Time 0.00(0.00)\n",
      "Iter 19670 | Time 26.4759(26.3076) | Bit/dim 3.6437(3.6158) | Xent 0.1274(0.1186) | Loss 9.7150(9.9916) | Error 0.0444(0.0421) Steps 1042(1057.94) | Grad Norm 3.9233(4.9252) | Total Time 0.00(0.00)\n",
      "Iter 19680 | Time 25.5892(26.2899) | Bit/dim 3.6387(3.6138) | Xent 0.1429(0.1213) | Loss 9.6059(9.8550) | Error 0.0511(0.0430) Steps 1078(1057.82) | Grad Norm 4.6166(5.5059) | Total Time 0.00(0.00)\n",
      "Iter 19690 | Time 27.0683(26.4083) | Bit/dim 3.6121(3.6107) | Xent 0.1067(0.1201) | Loss 9.4939(9.7627) | Error 0.0400(0.0428) Steps 1114(1062.27) | Grad Norm 7.5278(5.4607) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0358 | Time 116.8429, Epoch Time 1583.6398(1634.5894), Bit/dim 3.6183(best: 3.5673), Xent 0.9908, Loss 4.1137, Error 0.2225(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 19700 | Time 27.9490(26.4980) | Bit/dim 3.6164(3.6117) | Xent 0.1279(0.1187) | Loss 9.5856(10.5466) | Error 0.0444(0.0425) Steps 1120(1066.50) | Grad Norm 4.6127(5.6345) | Total Time 0.00(0.00)\n",
      "Iter 19710 | Time 25.3443(26.4789) | Bit/dim 3.6313(3.6116) | Xent 0.1150(0.1162) | Loss 9.4415(10.2645) | Error 0.0389(0.0416) Steps 1084(1066.03) | Grad Norm 4.2864(5.3873) | Total Time 0.00(0.00)\n",
      "Iter 19720 | Time 26.1241(26.4155) | Bit/dim 3.6280(3.6114) | Xent 0.1086(0.1147) | Loss 9.4798(10.0531) | Error 0.0389(0.0404) Steps 1066(1066.63) | Grad Norm 4.2935(6.2618) | Total Time 0.00(0.00)\n",
      "Iter 19730 | Time 26.3201(26.5384) | Bit/dim 3.5939(3.6095) | Xent 0.1140(0.1143) | Loss 9.3430(9.9039) | Error 0.0389(0.0403) Steps 1036(1066.77) | Grad Norm 6.4753(6.0524) | Total Time 0.00(0.00)\n",
      "Iter 19740 | Time 27.4006(26.7035) | Bit/dim 3.6378(3.6081) | Xent 0.1121(0.1142) | Loss 9.5360(9.8027) | Error 0.0356(0.0401) Steps 1036(1068.85) | Grad Norm 4.3139(6.9500) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0359 | Time 116.6336, Epoch Time 1604.4111(1633.6840), Bit/dim 3.6196(best: 3.5673), Xent 1.0074, Loss 4.1232, Error 0.2244(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 19750 | Time 25.9411(26.8160) | Bit/dim 3.5762(3.6066) | Xent 0.1101(0.1140) | Loss 9.3567(10.7267) | Error 0.0367(0.0397) Steps 1042(1070.01) | Grad Norm 13.8653(7.9160) | Total Time 0.00(0.00)\n",
      "Iter 19760 | Time 26.8238(26.7650) | Bit/dim 3.6344(3.6079) | Xent 0.1071(0.1114) | Loss 9.4121(10.3876) | Error 0.0356(0.0392) Steps 1078(1073.00) | Grad Norm 3.7898(6.9367) | Total Time 0.00(0.00)\n",
      "Iter 19770 | Time 27.7330(26.8309) | Bit/dim 3.6072(3.6077) | Xent 0.1087(0.1118) | Loss 9.4603(10.1636) | Error 0.0411(0.0391) Steps 1078(1076.88) | Grad Norm 3.9645(7.4117) | Total Time 0.00(0.00)\n",
      "Iter 19780 | Time 28.1419(26.7621) | Bit/dim 3.6139(3.6056) | Xent 0.0967(0.1107) | Loss 9.6380(9.9835) | Error 0.0267(0.0384) Steps 1102(1075.96) | Grad Norm 4.9100(6.7310) | Total Time 0.00(0.00)\n",
      "Iter 19790 | Time 26.8915(26.8540) | Bit/dim 3.5744(3.6046) | Xent 0.0985(0.1126) | Loss 9.2753(9.8519) | Error 0.0322(0.0389) Steps 1048(1079.94) | Grad Norm 6.3243(6.3910) | Total Time 0.00(0.00)\n",
      "Iter 19800 | Time 26.9523(26.9225) | Bit/dim 3.6193(3.6047) | Xent 0.1222(0.1113) | Loss 9.6125(9.7574) | Error 0.0433(0.0384) Steps 1084(1081.49) | Grad Norm 5.1006(7.4247) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0360 | Time 117.4077, Epoch Time 1612.8104(1633.0578), Bit/dim 3.6118(best: 3.5673), Xent 1.0147, Loss 4.1192, Error 0.2219(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 19810 | Time 26.7451(27.0977) | Bit/dim 3.6069(3.6049) | Xent 0.0853(0.1115) | Loss 9.4699(10.5834) | Error 0.0322(0.0385) Steps 1090(1085.82) | Grad Norm 4.0286(8.4084) | Total Time 0.00(0.00)\n",
      "Iter 19820 | Time 26.9232(27.1628) | Bit/dim 3.5788(3.6054) | Xent 0.1156(0.1151) | Loss 9.4797(10.2972) | Error 0.0411(0.0399) Steps 1090(1086.13) | Grad Norm 11.7677(8.0504) | Total Time 0.00(0.00)\n",
      "Iter 19830 | Time 28.1396(27.3706) | Bit/dim 3.6211(3.6073) | Xent 0.1101(0.1164) | Loss 9.2850(10.0857) | Error 0.0378(0.0401) Steps 1000(1082.90) | Grad Norm 10.4584(9.2181) | Total Time 0.00(0.00)\n",
      "Iter 19840 | Time 26.9786(27.3921) | Bit/dim 3.6395(3.6076) | Xent 0.1217(0.1156) | Loss 9.6740(9.9356) | Error 0.0478(0.0401) Steps 1108(1081.30) | Grad Norm 9.1634(9.8294) | Total Time 0.00(0.00)\n",
      "Iter 19850 | Time 26.9809(27.5036) | Bit/dim 3.6036(3.6088) | Xent 0.1269(0.1175) | Loss 9.4230(9.8212) | Error 0.0400(0.0408) Steps 1084(1083.79) | Grad Norm 16.3864(18.9952) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0361 | Time 118.8294, Epoch Time 1657.0187(1633.7766), Bit/dim 3.6284(best: 3.5673), Xent 1.0367, Loss 4.1468, Error 0.2257(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 19860 | Time 27.5185(27.5596) | Bit/dim 3.6318(3.6129) | Xent 0.1580(0.1183) | Loss 9.5749(10.7551) | Error 0.0556(0.0412) Steps 1114(1087.12) | Grad Norm 13.1531(16.8903) | Total Time 0.00(0.00)\n",
      "Iter 19870 | Time 27.3744(27.6872) | Bit/dim 3.6083(3.6161) | Xent 0.1493(0.1244) | Loss 9.5649(10.4507) | Error 0.0511(0.0434) Steps 1126(1090.26) | Grad Norm 7.2118(15.3677) | Total Time 0.00(0.00)\n",
      "Iter 19880 | Time 28.2582(27.8732) | Bit/dim 3.6381(3.6231) | Xent 0.1597(0.1359) | Loss 9.6773(10.2397) | Error 0.0433(0.0466) Steps 1126(1098.73) | Grad Norm 6.7254(13.6263) | Total Time 0.00(0.00)\n",
      "Iter 19890 | Time 27.9082(28.1815) | Bit/dim 3.7387(3.6521) | Xent 0.3159(0.1793) | Loss 10.0392(10.1792) | Error 0.1111(0.0603) Steps 1126(1111.24) | Grad Norm 194.7164(34.5933) | Total Time 0.00(0.00)\n",
      "Iter 19900 | Time 29.1779(28.4290) | Bit/dim 3.6947(3.6632) | Xent 0.2058(0.1971) | Loss 9.8567(10.0952) | Error 0.0700(0.0670) Steps 1120(1115.34) | Grad Norm 13.6989(36.6775) | Total Time 0.00(0.00)\n",
      "Iter 19910 | Time 28.0514(28.5214) | Bit/dim 3.6427(3.6614) | Xent 0.1624(0.1966) | Loss 9.5487(9.9919) | Error 0.0700(0.0680) Steps 1078(1114.45) | Grad Norm 16.4202(31.2495) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0362 | Time 117.3597, Epoch Time 1705.6445(1635.9327), Bit/dim 3.6574(best: 3.5673), Xent 0.9410, Loss 4.1279, Error 0.2294(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 19920 | Time 28.4410(28.3749) | Bit/dim 3.5838(3.6543) | Xent 0.1363(0.1849) | Loss 9.4299(10.7511) | Error 0.0467(0.0640) Steps 1108(1110.59) | Grad Norm 9.5275(25.1130) | Total Time 0.00(0.00)\n",
      "Iter 19930 | Time 27.2856(28.1795) | Bit/dim 3.6464(3.6488) | Xent 0.1235(0.1710) | Loss 9.6358(10.4317) | Error 0.0378(0.0597) Steps 1096(1106.19) | Grad Norm 9.3363(21.1740) | Total Time 0.00(0.00)\n",
      "Iter 19940 | Time 27.5854(27.9162) | Bit/dim 3.5923(3.6409) | Xent 0.0825(0.1574) | Loss 9.3963(10.1855) | Error 0.0300(0.0547) Steps 1084(1099.59) | Grad Norm 5.8058(17.1700) | Total Time 0.00(0.00)\n",
      "Iter 19950 | Time 26.4347(27.5778) | Bit/dim 3.6347(3.6342) | Xent 0.1118(0.1482) | Loss 9.4842(9.9915) | Error 0.0378(0.0519) Steps 1072(1090.27) | Grad Norm 4.9073(13.7504) | Total Time 0.00(0.00)\n",
      "Iter 19960 | Time 26.6192(27.4275) | Bit/dim 3.5999(3.6268) | Xent 0.1262(0.1400) | Loss 9.2354(9.8403) | Error 0.0467(0.0488) Steps 1036(1082.68) | Grad Norm 5.2308(11.4815) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0363 | Time 116.6455, Epoch Time 1631.8386(1635.8098), Bit/dim 3.6155(best: 3.5673), Xent 1.0078, Loss 4.1194, Error 0.2229(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 19970 | Time 26.0170(27.2674) | Bit/dim 3.6245(3.6202) | Xent 0.1117(0.1342) | Loss 9.4926(10.7742) | Error 0.0367(0.0468) Steps 1072(1078.48) | Grad Norm 3.9304(9.5435) | Total Time 0.00(0.00)\n",
      "Iter 19980 | Time 26.9963(27.1583) | Bit/dim 3.5767(3.6162) | Xent 0.1154(0.1274) | Loss 9.2596(10.4261) | Error 0.0422(0.0442) Steps 1060(1074.82) | Grad Norm 4.5431(8.2675) | Total Time 0.00(0.00)\n",
      "Iter 19990 | Time 27.5894(27.1577) | Bit/dim 3.6096(3.6142) | Xent 0.1122(0.1225) | Loss 9.5293(10.1711) | Error 0.0444(0.0429) Steps 1084(1071.63) | Grad Norm 2.7851(7.2038) | Total Time 0.00(0.00)\n",
      "Iter 20000 | Time 28.0797(27.2286) | Bit/dim 3.5870(3.6097) | Xent 0.1193(0.1195) | Loss 9.3205(9.9882) | Error 0.0467(0.0418) Steps 1048(1072.96) | Grad Norm 4.8840(6.4677) | Total Time 0.00(0.00)\n",
      "Iter 20010 | Time 28.4955(27.2539) | Bit/dim 3.6056(3.6071) | Xent 0.1371(0.1206) | Loss 9.5228(9.8567) | Error 0.0489(0.0420) Steps 1126(1076.29) | Grad Norm 5.4305(6.4365) | Total Time 0.00(0.00)\n",
      "Iter 20020 | Time 26.4273(27.1581) | Bit/dim 3.6223(3.6057) | Xent 0.1348(0.1211) | Loss 9.5522(9.7617) | Error 0.0433(0.0418) Steps 1042(1073.62) | Grad Norm 6.3266(6.2818) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0364 | Time 115.2651, Epoch Time 1621.8868(1635.3922), Bit/dim 3.6098(best: 3.5673), Xent 0.9868, Loss 4.1032, Error 0.2235(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 20030 | Time 27.1394(27.1003) | Bit/dim 3.6142(3.6027) | Xent 0.1235(0.1180) | Loss 9.3881(10.5397) | Error 0.0478(0.0415) Steps 1048(1071.90) | Grad Norm 7.0626(6.1710) | Total Time 0.00(0.00)\n",
      "Iter 20040 | Time 28.3429(27.1277) | Bit/dim 3.5859(3.6029) | Xent 0.1216(0.1164) | Loss 9.4823(10.2667) | Error 0.0411(0.0406) Steps 1066(1075.23) | Grad Norm 4.3978(5.5613) | Total Time 0.00(0.00)\n",
      "Iter 20050 | Time 26.1934(27.0641) | Bit/dim 3.6248(3.6030) | Xent 0.1203(0.1140) | Loss 9.6098(10.0613) | Error 0.0500(0.0399) Steps 1066(1075.62) | Grad Norm 3.4119(5.1123) | Total Time 0.00(0.00)\n",
      "Iter 20060 | Time 26.4436(26.9836) | Bit/dim 3.6056(3.6025) | Xent 0.1066(0.1119) | Loss 9.5965(9.8978) | Error 0.0422(0.0391) Steps 1078(1072.16) | Grad Norm 7.0359(4.9047) | Total Time 0.00(0.00)\n",
      "Iter 20070 | Time 27.1857(26.9191) | Bit/dim 3.6161(3.6007) | Xent 0.1009(0.1109) | Loss 9.5581(9.7740) | Error 0.0311(0.0384) Steps 1054(1069.93) | Grad Norm 4.6150(4.6682) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0365 | Time 116.4951, Epoch Time 1608.8687(1634.5964), Bit/dim 3.6073(best: 3.5673), Xent 1.0531, Loss 4.1339, Error 0.2255(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 20080 | Time 26.4229(26.8058) | Bit/dim 3.5472(3.5981) | Xent 0.1092(0.1086) | Loss 9.4431(10.6983) | Error 0.0422(0.0382) Steps 1054(1071.10) | Grad Norm 5.9945(4.4886) | Total Time 0.00(0.00)\n",
      "Iter 20090 | Time 28.5328(26.8694) | Bit/dim 3.5617(3.5984) | Xent 0.0847(0.1062) | Loss 9.4614(10.3720) | Error 0.0267(0.0372) Steps 1078(1073.34) | Grad Norm 4.0379(4.4518) | Total Time 0.00(0.00)\n",
      "Iter 20100 | Time 26.1113(26.8426) | Bit/dim 3.6099(3.5992) | Xent 0.1161(0.1050) | Loss 9.4606(10.1397) | Error 0.0400(0.0370) Steps 1036(1071.53) | Grad Norm 6.9590(4.5263) | Total Time 0.00(0.00)\n",
      "Iter 20110 | Time 27.6852(26.8077) | Bit/dim 3.6164(3.5994) | Xent 0.1021(0.1064) | Loss 9.3693(9.9499) | Error 0.0356(0.0374) Steps 1066(1068.52) | Grad Norm 4.5720(4.6853) | Total Time 0.00(0.00)\n",
      "Iter 20120 | Time 26.2521(26.6459) | Bit/dim 3.6032(3.5962) | Xent 0.1413(0.1063) | Loss 9.5298(9.8052) | Error 0.0478(0.0371) Steps 1078(1068.33) | Grad Norm 5.6043(4.5488) | Total Time 0.00(0.00)\n",
      "Iter 20130 | Time 26.0634(26.6258) | Bit/dim 3.6161(3.5951) | Xent 0.1004(0.1070) | Loss 9.5074(9.7093) | Error 0.0333(0.0371) Steps 1048(1068.60) | Grad Norm 3.0272(4.5676) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0366 | Time 115.9022, Epoch Time 1599.5194(1633.5441), Bit/dim 3.6033(best: 3.5673), Xent 1.0332, Loss 4.1199, Error 0.2259(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 20140 | Time 27.9876(26.6442) | Bit/dim 3.5913(3.5944) | Xent 0.1175(0.1063) | Loss 9.4519(10.5158) | Error 0.0367(0.0368) Steps 1048(1068.82) | Grad Norm 3.9566(4.7782) | Total Time 0.00(0.00)\n",
      "Iter 20150 | Time 26.2376(26.7006) | Bit/dim 3.6187(3.5935) | Xent 0.1047(0.1065) | Loss 9.4615(10.2286) | Error 0.0378(0.0371) Steps 1066(1069.83) | Grad Norm 5.1816(4.6268) | Total Time 0.00(0.00)\n",
      "Iter 20160 | Time 27.4765(26.8292) | Bit/dim 3.6007(3.5951) | Xent 0.0990(0.1069) | Loss 9.4866(10.0364) | Error 0.0367(0.0370) Steps 1060(1068.55) | Grad Norm 3.6696(4.5138) | Total Time 0.00(0.00)\n",
      "Iter 20170 | Time 27.5842(26.9128) | Bit/dim 3.5837(3.5962) | Xent 0.1249(0.1055) | Loss 9.4700(9.8780) | Error 0.0411(0.0371) Steps 1066(1070.16) | Grad Norm 4.5950(4.3517) | Total Time 0.00(0.00)\n",
      "Iter 20180 | Time 25.8491(26.7338) | Bit/dim 3.6049(3.5942) | Xent 0.0789(0.1037) | Loss 9.4175(9.7559) | Error 0.0322(0.0360) Steps 1102(1070.41) | Grad Norm 4.2436(4.3931) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0367 | Time 117.2619, Epoch Time 1608.2537(1632.7854), Bit/dim 3.6028(best: 3.5673), Xent 1.0428, Loss 4.1242, Error 0.2223(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 20190 | Time 27.8082(26.7439) | Bit/dim 3.5899(3.5938) | Xent 0.1210(0.1024) | Loss 9.5692(10.6788) | Error 0.0422(0.0355) Steps 1078(1067.99) | Grad Norm 5.0281(4.3480) | Total Time 0.00(0.00)\n",
      "Iter 20200 | Time 27.9742(26.8042) | Bit/dim 3.5880(3.5916) | Xent 0.1120(0.1015) | Loss 9.5287(10.3410) | Error 0.0422(0.0357) Steps 1048(1071.89) | Grad Norm 4.8316(4.5352) | Total Time 0.00(0.00)\n",
      "Iter 20210 | Time 26.3989(26.7661) | Bit/dim 3.5880(3.5913) | Xent 0.1050(0.1022) | Loss 9.4193(10.1035) | Error 0.0367(0.0353) Steps 1084(1072.18) | Grad Norm 4.5956(5.1868) | Total Time 0.00(0.00)\n",
      "Iter 20220 | Time 26.3242(26.7275) | Bit/dim 3.5813(3.5904) | Xent 0.1121(0.1031) | Loss 9.4522(9.9286) | Error 0.0411(0.0359) Steps 1072(1071.99) | Grad Norm 3.8886(5.4167) | Total Time 0.00(0.00)\n",
      "Iter 20230 | Time 26.5113(26.6524) | Bit/dim 3.6059(3.5925) | Xent 0.0982(0.1036) | Loss 9.2989(9.7972) | Error 0.0356(0.0363) Steps 1036(1073.26) | Grad Norm 3.4866(5.0652) | Total Time 0.00(0.00)\n",
      "Iter 20240 | Time 26.2087(26.5763) | Bit/dim 3.6034(3.5922) | Xent 0.1102(0.1046) | Loss 9.3570(9.7121) | Error 0.0389(0.0366) Steps 1036(1071.45) | Grad Norm 4.7761(5.1014) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0368 | Time 115.3830, Epoch Time 1596.1974(1631.6878), Bit/dim 3.5966(best: 3.5673), Xent 1.0364, Loss 4.1148, Error 0.2232(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 20250 | Time 27.1728(26.5566) | Bit/dim 3.5828(3.5891) | Xent 0.0831(0.1044) | Loss 9.3607(10.4686) | Error 0.0267(0.0364) Steps 1090(1073.24) | Grad Norm 6.8121(5.1976) | Total Time 0.00(0.00)\n",
      "Iter 20260 | Time 27.0620(26.6233) | Bit/dim 3.5942(3.5888) | Xent 0.1094(0.1021) | Loss 9.3716(10.1965) | Error 0.0356(0.0358) Steps 1042(1074.25) | Grad Norm 3.3918(4.9774) | Total Time 0.00(0.00)\n",
      "Iter 20270 | Time 27.0762(26.6888) | Bit/dim 3.5691(3.5902) | Xent 0.0678(0.1005) | Loss 9.2116(9.9949) | Error 0.0222(0.0354) Steps 1042(1069.68) | Grad Norm 4.5511(5.0730) | Total Time 0.00(0.00)\n",
      "Iter 20280 | Time 26.9711(26.8252) | Bit/dim 3.5921(3.5926) | Xent 0.0837(0.0999) | Loss 9.4174(9.8581) | Error 0.0333(0.0351) Steps 1072(1070.65) | Grad Norm 3.0436(4.9163) | Total Time 0.00(0.00)\n",
      "Iter 20290 | Time 26.8724(26.7759) | Bit/dim 3.5970(3.5916) | Xent 0.1049(0.1013) | Loss 9.3876(9.7506) | Error 0.0356(0.0355) Steps 1060(1066.13) | Grad Norm 5.4262(4.9861) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0369 | Time 115.9892, Epoch Time 1604.1662(1630.8621), Bit/dim 3.5966(best: 3.5673), Xent 1.0317, Loss 4.1125, Error 0.2231(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 20300 | Time 27.9223(26.7109) | Bit/dim 3.5969(3.5897) | Xent 0.1057(0.1019) | Loss 9.4677(10.6422) | Error 0.0356(0.0358) Steps 1018(1064.06) | Grad Norm 3.7893(4.7875) | Total Time 0.00(0.00)\n",
      "Iter 20310 | Time 27.1618(26.6549) | Bit/dim 3.5615(3.5882) | Xent 0.1050(0.1009) | Loss 9.5323(10.3188) | Error 0.0411(0.0354) Steps 1102(1066.05) | Grad Norm 5.0588(4.4958) | Total Time 0.00(0.00)\n",
      "Iter 20320 | Time 27.6110(26.7106) | Bit/dim 3.5853(3.5901) | Xent 0.1237(0.1017) | Loss 9.4007(10.1012) | Error 0.0411(0.0352) Steps 1108(1069.03) | Grad Norm 4.4547(4.3368) | Total Time 0.00(0.00)\n",
      "Iter 20330 | Time 26.8576(26.6442) | Bit/dim 3.5709(3.5871) | Xent 0.1225(0.1012) | Loss 9.3940(9.9229) | Error 0.0489(0.0355) Steps 1042(1067.08) | Grad Norm 5.0474(4.5095) | Total Time 0.00(0.00)\n",
      "Iter 20340 | Time 26.0057(26.4926) | Bit/dim 3.5650(3.5873) | Xent 0.1089(0.1011) | Loss 9.3434(9.7905) | Error 0.0400(0.0353) Steps 1030(1065.67) | Grad Norm 3.8196(4.4276) | Total Time 0.00(0.00)\n",
      "Iter 20350 | Time 26.6594(26.4966) | Bit/dim 3.6148(3.5896) | Xent 0.1030(0.0999) | Loss 9.5065(9.7006) | Error 0.0344(0.0347) Steps 1060(1068.20) | Grad Norm 4.6898(4.3218) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0370 | Time 116.3000, Epoch Time 1590.3051(1629.6454), Bit/dim 3.5966(best: 3.5673), Xent 1.0577, Loss 4.1255, Error 0.2246(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 20360 | Time 26.3481(26.4950) | Bit/dim 3.5955(3.5864) | Xent 0.1297(0.0986) | Loss 9.4012(10.4762) | Error 0.0389(0.0342) Steps 1090(1067.77) | Grad Norm 36.4833(5.2752) | Total Time 0.00(0.00)\n",
      "Iter 20370 | Time 25.2550(26.5151) | Bit/dim 3.5810(3.5875) | Xent 0.0964(0.0988) | Loss 9.3343(10.1924) | Error 0.0344(0.0344) Steps 1048(1066.56) | Grad Norm 2.8915(5.0267) | Total Time 0.00(0.00)\n",
      "Iter 20380 | Time 26.4558(26.5959) | Bit/dim 3.6097(3.5896) | Xent 0.0955(0.0977) | Loss 9.5198(9.9970) | Error 0.0378(0.0337) Steps 1060(1066.99) | Grad Norm 3.7195(6.8641) | Total Time 0.00(0.00)\n",
      "Iter 20390 | Time 26.7735(26.6246) | Bit/dim 3.5841(3.5886) | Xent 0.0813(0.0975) | Loss 9.4696(9.8379) | Error 0.0322(0.0344) Steps 1036(1064.16) | Grad Norm 2.7109(6.0852) | Total Time 0.00(0.00)\n",
      "Iter 20400 | Time 26.5856(26.7090) | Bit/dim 3.5823(3.5863) | Xent 0.0939(0.0961) | Loss 9.4535(9.7308) | Error 0.0333(0.0334) Steps 1096(1068.80) | Grad Norm 3.9094(5.5538) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0371 | Time 116.9258, Epoch Time 1602.0025(1628.8161), Bit/dim 3.5954(best: 3.5673), Xent 1.0637, Loss 4.1273, Error 0.2276(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 20410 | Time 27.4779(26.7395) | Bit/dim 3.5511(3.5840) | Xent 0.1122(0.0949) | Loss 9.3893(10.6531) | Error 0.0389(0.0330) Steps 1072(1071.15) | Grad Norm 4.0483(5.0613) | Total Time 0.00(0.00)\n",
      "Iter 20420 | Time 26.8587(26.7056) | Bit/dim 3.5882(3.5855) | Xent 0.0992(0.0956) | Loss 9.4244(10.3308) | Error 0.0378(0.0332) Steps 1048(1071.29) | Grad Norm 2.5525(4.8245) | Total Time 0.00(0.00)\n",
      "Iter 20430 | Time 26.1289(26.7146) | Bit/dim 3.5738(3.5855) | Xent 0.0875(0.0976) | Loss 9.5452(10.0988) | Error 0.0333(0.0340) Steps 1078(1070.35) | Grad Norm 4.5735(4.9040) | Total Time 0.00(0.00)\n",
      "Iter 20440 | Time 26.9552(26.8221) | Bit/dim 3.6001(3.5868) | Xent 0.1336(0.0994) | Loss 9.6016(9.9381) | Error 0.0467(0.0352) Steps 1096(1074.68) | Grad Norm 33.2581(5.9885) | Total Time 0.00(0.00)\n",
      "Iter 20450 | Time 27.5126(26.8703) | Bit/dim 3.6223(3.5876) | Xent 0.0881(0.0996) | Loss 9.5496(9.7985) | Error 0.0289(0.0347) Steps 1096(1075.20) | Grad Norm 3.2293(5.9720) | Total Time 0.00(0.00)\n",
      "Iter 20460 | Time 26.6581(26.9832) | Bit/dim 3.5806(3.5876) | Xent 0.1042(0.0991) | Loss 9.4231(9.7026) | Error 0.0411(0.0347) Steps 1066(1075.53) | Grad Norm 11.1858(6.3965) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0372 | Time 118.5097, Epoch Time 1617.9035(1628.4888), Bit/dim 3.6016(best: 3.5673), Xent 1.0727, Loss 4.1379, Error 0.2244(best: 0.2195)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 20470 | Time 28.1048(27.0666) | Bit/dim 3.5664(3.5861) | Xent 0.1104(0.0985) | Loss 9.4678(10.5138) | Error 0.0389(0.0341) Steps 1096(1079.90) | Grad Norm 31.4186(7.6777) | Total Time 0.00(0.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_rl.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_6_run1 --resume ../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_6_run1/epoch_270_checkpt.pth --seed 1 --lr 0.0001 --conditional True --controlled_tol False --train_mode semisup --log_freq 10 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5 --scale_fac 1.0 --scale_std 6.0\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
