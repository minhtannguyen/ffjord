{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0,1,2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "if args.controlled_tol:\n",
      "    import lib.odenvp_conditional_tol as odenvp\n",
      "else:\n",
      "    import lib.odenvp_conditional as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz = modules.GaussianDiag.logp(mean, logs, z).view(-1,1)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(z)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol},)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z = modules.GaussianDiag.sample(mean, logs)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            loss.backward()\n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, conditional=True, controlled_tol=False, conv=True, data='mnist', dims='64,64,64', divergence_fn='approximate', dl2int=None, imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.0001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume='../experiments_published/cnf_conditional_bs900_run1/epoch_250_checkpt.pth', rtol=1e-05, save='../experiments_published/cnf_conditional_bs900_run1', seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5)\n",
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=1568, bias=True)\n",
      "  (project_class): LinearZeros(in_features=784, out_features=10, bias=True)\n",
      ")\n",
      "Number of trainable parameters: 828890\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc.py:286: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "Iter 16510 | Time 14.4538(14.8544) | Bit/dim 0.9998(1.0072) | Xent 0.0002(0.0010) | Loss 0.9999(1.0077) | Error 0.0000(0.0003) Steps 728(716.39) | Grad Norm 1.8980(4.5206) | Total Time 10.00(10.00)\n",
      "Iter 16520 | Time 14.7163(14.8281) | Bit/dim 0.9961(1.0035) | Xent 0.0007(0.0009) | Loss 0.9964(1.0039) | Error 0.0000(0.0003) Steps 686(718.74) | Grad Norm 1.3762(3.6660) | Total Time 10.00(10.00)\n",
      "Iter 16530 | Time 14.9244(14.7801) | Bit/dim 0.9953(1.0010) | Xent 0.0000(0.0008) | Loss 0.9953(1.0014) | Error 0.0000(0.0003) Steps 710(719.07) | Grad Norm 0.2771(2.9072) | Total Time 10.00(10.00)\n",
      "Iter 16540 | Time 14.3991(14.7111) | Bit/dim 0.9867(0.9980) | Xent 0.0001(0.0008) | Loss 0.9867(0.9984) | Error 0.0000(0.0002) Steps 716(720.45) | Grad Norm 0.3575(2.2490) | Total Time 10.00(10.00)\n",
      "Iter 16550 | Time 14.5139(14.7177) | Bit/dim 0.9968(0.9974) | Xent 0.0000(0.0006) | Loss 0.9968(0.9977) | Error 0.0000(0.0002) Steps 740(720.38) | Grad Norm 0.5322(1.7961) | Total Time 10.00(10.00)\n",
      "Iter 16560 | Time 14.6365(14.7319) | Bit/dim 0.9791(0.9952) | Xent 0.0000(0.0005) | Loss 0.9791(0.9955) | Error 0.0000(0.0002) Steps 728(719.91) | Grad Norm 0.2058(1.3915) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0251 | Time 60.0534, Epoch Time 1054.9064(986.7882), Bit/dim 0.9892(best: inf), Xent 0.0444, Loss 1.0114, Error 0.0080(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 16570 | Time 14.5025(14.6721) | Bit/dim 0.9814(0.9935) | Xent 0.0000(0.0004) | Loss 0.9814(0.9937) | Error 0.0000(0.0002) Steps 734(719.52) | Grad Norm 0.5842(1.1204) | Total Time 10.00(10.00)\n",
      "Iter 16580 | Time 13.9081(14.6641) | Bit/dim 0.9937(0.9922) | Xent 0.0000(0.0003) | Loss 0.9938(0.9924) | Error 0.0000(0.0001) Steps 698(720.55) | Grad Norm 0.2643(0.9173) | Total Time 10.00(10.00)\n",
      "Iter 16590 | Time 15.0530(14.6376) | Bit/dim 0.9837(0.9914) | Xent 0.0001(0.0004) | Loss 0.9837(0.9916) | Error 0.0000(0.0002) Steps 692(719.03) | Grad Norm 0.3610(0.8139) | Total Time 10.00(10.00)\n",
      "Iter 16600 | Time 14.7143(14.6319) | Bit/dim 0.9910(0.9924) | Xent 0.0008(0.0003) | Loss 0.9914(0.9925) | Error 0.0000(0.0001) Steps 716(719.41) | Grad Norm 0.5664(0.7176) | Total Time 10.00(10.00)\n",
      "Iter 16610 | Time 14.6472(14.5931) | Bit/dim 1.0033(0.9920) | Xent 0.0000(0.0003) | Loss 1.0033(0.9921) | Error 0.0000(0.0001) Steps 704(719.28) | Grad Norm 0.4817(0.6436) | Total Time 10.00(10.00)\n",
      "Iter 16620 | Time 14.1741(14.5903) | Bit/dim 0.9965(0.9918) | Xent 0.0000(0.0003) | Loss 0.9965(0.9920) | Error 0.0000(0.0001) Steps 728(721.54) | Grad Norm 0.2687(0.5628) | Total Time 10.00(10.00)\n",
      "Iter 16630 | Time 14.5261(14.5710) | Bit/dim 0.9795(0.9911) | Xent 0.0018(0.0003) | Loss 0.9804(0.9912) | Error 0.0011(0.0001) Steps 734(720.02) | Grad Norm 0.7648(0.5309) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0252 | Time 57.0406, Epoch Time 1033.9498(988.2030), Bit/dim 0.9891(best: 0.9892), Xent 0.0413, Loss 1.0098, Error 0.0075(best: 0.0080)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 16640 | Time 14.5674(14.5526) | Bit/dim 0.9830(0.9900) | Xent 0.0001(0.0003) | Loss 0.9831(0.9901) | Error 0.0000(0.0002) Steps 728(721.72) | Grad Norm 0.3435(0.4868) | Total Time 10.00(10.00)\n",
      "Iter 16650 | Time 14.1984(14.5675) | Bit/dim 0.9876(0.9901) | Xent 0.0000(0.0003) | Loss 0.9876(0.9903) | Error 0.0000(0.0001) Steps 710(721.41) | Grad Norm 0.3623(0.4506) | Total Time 10.00(10.00)\n",
      "Iter 16660 | Time 14.4010(14.5807) | Bit/dim 0.9894(0.9899) | Xent 0.0000(0.0002) | Loss 0.9894(0.9900) | Error 0.0000(0.0001) Steps 704(720.78) | Grad Norm 0.2201(0.4216) | Total Time 10.00(10.00)\n",
      "Iter 16670 | Time 14.8818(14.5638) | Bit/dim 0.9901(0.9907) | Xent 0.0001(0.0002) | Loss 0.9901(0.9908) | Error 0.0000(0.0001) Steps 734(720.14) | Grad Norm 0.5196(0.3963) | Total Time 10.00(10.00)\n",
      "Iter 16680 | Time 14.3884(14.5530) | Bit/dim 0.9857(0.9897) | Xent 0.0001(0.0002) | Loss 0.9857(0.9898) | Error 0.0000(0.0001) Steps 740(721.69) | Grad Norm 0.2074(0.3705) | Total Time 10.00(10.00)\n",
      "Iter 16690 | Time 14.7731(14.5700) | Bit/dim 0.9964(0.9902) | Xent 0.0000(0.0003) | Loss 0.9964(0.9903) | Error 0.0000(0.0001) Steps 698(722.65) | Grad Norm 0.3320(0.4160) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0253 | Time 56.0406, Epoch Time 1033.2443(989.5543), Bit/dim 0.9877(best: 0.9891), Xent 0.0411, Loss 1.0083, Error 0.0078(best: 0.0075)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 16700 | Time 14.4221(14.5794) | Bit/dim 0.9891(0.9898) | Xent 0.0003(0.0005) | Loss 0.9892(0.9901) | Error 0.0000(0.0001) Steps 710(722.42) | Grad Norm 0.8076(0.4239) | Total Time 10.00(10.00)\n",
      "Iter 16710 | Time 14.6734(14.5812) | Bit/dim 0.9846(0.9897) | Xent 0.0001(0.0004) | Loss 0.9846(0.9899) | Error 0.0000(0.0001) Steps 740(721.54) | Grad Norm 0.5553(0.4261) | Total Time 10.00(10.00)\n",
      "Iter 16720 | Time 14.2320(14.6028) | Bit/dim 0.9833(0.9892) | Xent 0.0004(0.0004) | Loss 0.9835(0.9894) | Error 0.0000(0.0002) Steps 710(719.68) | Grad Norm 1.2163(0.4898) | Total Time 10.00(10.00)\n",
      "Iter 16730 | Time 14.3219(14.6456) | Bit/dim 0.9934(0.9892) | Xent 0.0000(0.0004) | Loss 0.9934(0.9894) | Error 0.0000(0.0002) Steps 722(719.33) | Grad Norm 0.2389(0.4732) | Total Time 10.00(10.00)\n",
      "Iter 16740 | Time 14.1967(14.6163) | Bit/dim 0.9963(0.9897) | Xent 0.0002(0.0003) | Loss 0.9964(0.9898) | Error 0.0000(0.0002) Steps 728(719.37) | Grad Norm 0.6906(0.4620) | Total Time 10.00(10.00)\n",
      "Iter 16750 | Time 14.4525(14.6075) | Bit/dim 0.9940(0.9897) | Xent 0.0010(0.0003) | Loss 0.9945(0.9899) | Error 0.0011(0.0002) Steps 728(719.32) | Grad Norm 0.5616(0.5205) | Total Time 10.00(10.00)\n",
      "Iter 16760 | Time 14.6206(14.5997) | Bit/dim 0.9900(0.9892) | Xent 0.0001(0.0002) | Loss 0.9901(0.9893) | Error 0.0000(0.0001) Steps 710(719.48) | Grad Norm 0.2772(0.4800) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0254 | Time 57.2959, Epoch Time 1037.7608(991.0005), Bit/dim 0.9873(best: 0.9877), Xent 0.0369, Loss 1.0057, Error 0.0080(best: 0.0075)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 16770 | Time 14.9794(14.6194) | Bit/dim 0.9828(0.9889) | Xent 0.0000(0.0003) | Loss 0.9828(0.9891) | Error 0.0000(0.0001) Steps 734(720.33) | Grad Norm 0.3343(0.4884) | Total Time 10.00(10.00)\n",
      "Iter 16780 | Time 14.7615(14.5995) | Bit/dim 0.9782(0.9881) | Xent 0.0008(0.0003) | Loss 0.9786(0.9883) | Error 0.0000(0.0001) Steps 716(719.88) | Grad Norm 0.2381(0.5193) | Total Time 10.00(10.00)\n",
      "Iter 16790 | Time 14.2303(14.6111) | Bit/dim 1.0004(0.9882) | Xent 0.0000(0.0003) | Loss 1.0005(0.9883) | Error 0.0000(0.0001) Steps 728(720.34) | Grad Norm 0.8474(0.5202) | Total Time 10.00(10.00)\n",
      "Iter 16800 | Time 14.7471(14.6219) | Bit/dim 0.9849(0.9893) | Xent 0.0000(0.0004) | Loss 0.9849(0.9895) | Error 0.0000(0.0001) Steps 722(719.97) | Grad Norm 0.3767(0.5080) | Total Time 10.00(10.00)\n",
      "Iter 16810 | Time 14.6687(14.5968) | Bit/dim 0.9913(0.9889) | Xent 0.0000(0.0003) | Loss 0.9913(0.9891) | Error 0.0000(0.0001) Steps 734(721.38) | Grad Norm 0.4402(0.4858) | Total Time 10.00(10.00)\n",
      "Iter 16820 | Time 14.5415(14.6151) | Bit/dim 0.9889(0.9882) | Xent 0.0001(0.0003) | Loss 0.9889(0.9883) | Error 0.0000(0.0001) Steps 734(720.02) | Grad Norm 0.2370(0.4451) | Total Time 10.00(10.00)\n",
      "Iter 16830 | Time 14.2213(14.6125) | Bit/dim 0.9850(0.9886) | Xent 0.0001(0.0002) | Loss 0.9850(0.9887) | Error 0.0000(0.0001) Steps 710(719.12) | Grad Norm 0.2768(0.4291) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0255 | Time 57.4342, Epoch Time 1036.9542(992.3791), Bit/dim 0.9867(best: 0.9873), Xent 0.0400, Loss 1.0067, Error 0.0078(best: 0.0075)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 16840 | Time 14.7524(14.6439) | Bit/dim 1.0017(0.9891) | Xent 0.0002(0.0002) | Loss 1.0018(0.9892) | Error 0.0000(0.0000) Steps 734(719.00) | Grad Norm 0.3193(0.3985) | Total Time 10.00(10.00)\n",
      "Iter 16850 | Time 14.4203(14.6629) | Bit/dim 0.9911(0.9890) | Xent 0.0000(0.0002) | Loss 0.9911(0.9891) | Error 0.0000(0.0000) Steps 728(721.19) | Grad Norm 0.7427(0.3916) | Total Time 10.00(10.00)\n",
      "Iter 16860 | Time 14.5678(14.6551) | Bit/dim 0.9800(0.9889) | Xent 0.0000(0.0004) | Loss 0.9800(0.9891) | Error 0.0000(0.0001) Steps 728(720.68) | Grad Norm 0.3125(0.4397) | Total Time 10.00(10.00)\n",
      "Iter 16870 | Time 14.9408(14.6623) | Bit/dim 0.9897(0.9880) | Xent 0.0000(0.0004) | Loss 0.9897(0.9882) | Error 0.0000(0.0001) Steps 722(721.19) | Grad Norm 0.2943(0.4219) | Total Time 10.00(10.00)\n",
      "Iter 16880 | Time 14.4859(14.6692) | Bit/dim 0.9832(0.9884) | Xent 0.0000(0.0003) | Loss 0.9832(0.9886) | Error 0.0000(0.0001) Steps 716(721.45) | Grad Norm 0.7213(0.4537) | Total Time 10.00(10.00)\n",
      "Iter 16890 | Time 14.6070(14.6510) | Bit/dim 0.9815(0.9877) | Xent 0.0001(0.0003) | Loss 0.9815(0.9878) | Error 0.0000(0.0001) Steps 722(719.86) | Grad Norm 0.4839(0.4378) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0256 | Time 56.5596, Epoch Time 1039.9259(993.8055), Bit/dim 0.9867(best: 0.9867), Xent 0.0387, Loss 1.0060, Error 0.0070(best: 0.0075)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 16900 | Time 14.1855(14.6496) | Bit/dim 0.9900(0.9881) | Xent 0.0000(0.0002) | Loss 0.9900(0.9882) | Error 0.0000(0.0001) Steps 710(719.40) | Grad Norm 0.4366(0.4289) | Total Time 10.00(10.00)\n",
      "Iter 16910 | Time 15.0706(14.6575) | Bit/dim 0.9936(0.9886) | Xent 0.0004(0.0002) | Loss 0.9938(0.9887) | Error 0.0000(0.0000) Steps 734(722.15) | Grad Norm 0.3794(0.4164) | Total Time 10.00(10.00)\n",
      "Iter 16920 | Time 14.7805(14.6236) | Bit/dim 0.9822(0.9882) | Xent 0.0001(0.0002) | Loss 0.9823(0.9884) | Error 0.0000(0.0001) Steps 710(723.00) | Grad Norm 0.2183(0.3920) | Total Time 10.00(10.00)\n",
      "Iter 16930 | Time 15.0770(14.6263) | Bit/dim 0.9882(0.9877) | Xent 0.0000(0.0004) | Loss 0.9882(0.9879) | Error 0.0000(0.0002) Steps 734(724.43) | Grad Norm 0.3424(0.4156) | Total Time 10.00(10.00)\n",
      "Iter 16940 | Time 14.9143(14.6485) | Bit/dim 0.9828(0.9873) | Xent 0.0000(0.0004) | Loss 0.9828(0.9875) | Error 0.0000(0.0001) Steps 686(723.46) | Grad Norm 0.4002(0.4131) | Total Time 10.00(10.00)\n",
      "Iter 16950 | Time 14.3629(14.6408) | Bit/dim 0.9841(0.9871) | Xent 0.0031(0.0004) | Loss 0.9856(0.9873) | Error 0.0011(0.0001) Steps 734(724.09) | Grad Norm 0.4293(0.3877) | Total Time 10.00(10.00)\n",
      "Iter 16960 | Time 14.5558(14.6218) | Bit/dim 0.9930(0.9870) | Xent 0.0000(0.0003) | Loss 0.9930(0.9872) | Error 0.0000(0.0001) Steps 716(724.36) | Grad Norm 0.2132(0.4274) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0257 | Time 57.4721, Epoch Time 1037.4649(995.1153), Bit/dim 0.9859(best: 0.9867), Xent 0.0372, Loss 1.0045, Error 0.0081(best: 0.0070)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 16970 | Time 14.5511(14.6389) | Bit/dim 0.9955(0.9881) | Xent 0.0003(0.0006) | Loss 0.9957(0.9884) | Error 0.0000(0.0001) Steps 728(724.56) | Grad Norm 1.3229(0.5339) | Total Time 10.00(10.00)\n",
      "Iter 16980 | Time 14.5271(14.6318) | Bit/dim 0.9835(0.9886) | Xent 0.0002(0.0005) | Loss 0.9836(0.9889) | Error 0.0000(0.0001) Steps 734(723.43) | Grad Norm 1.2249(0.6108) | Total Time 10.00(10.00)\n",
      "Iter 16990 | Time 15.3843(14.6844) | Bit/dim 0.9909(0.9886) | Xent 0.0013(0.0004) | Loss 0.9916(0.9889) | Error 0.0011(0.0001) Steps 704(721.66) | Grad Norm 0.6937(0.6436) | Total Time 10.00(10.00)\n",
      "Iter 17000 | Time 14.3423(14.6388) | Bit/dim 0.9828(0.9883) | Xent 0.0000(0.0004) | Loss 0.9828(0.9885) | Error 0.0000(0.0001) Steps 728(721.90) | Grad Norm 0.5081(0.6777) | Total Time 10.00(10.00)\n",
      "Iter 17010 | Time 14.7078(14.6392) | Bit/dim 0.9822(0.9876) | Xent 0.0000(0.0003) | Loss 0.9822(0.9878) | Error 0.0000(0.0001) Steps 716(720.49) | Grad Norm 0.2730(0.7006) | Total Time 10.00(10.00)\n",
      "Iter 17020 | Time 14.6078(14.6627) | Bit/dim 0.9726(0.9858) | Xent 0.0001(0.0003) | Loss 0.9726(0.9859) | Error 0.0000(0.0001) Steps 728(721.54) | Grad Norm 0.5303(0.6895) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0258 | Time 57.6118, Epoch Time 1042.1931(996.5276), Bit/dim 0.9853(best: 0.9859), Xent 0.0354, Loss 1.0030, Error 0.0076(best: 0.0070)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 17030 | Time 14.6379(14.6578) | Bit/dim 1.0038(0.9863) | Xent 0.0001(0.0002) | Loss 1.0038(0.9864) | Error 0.0000(0.0000) Steps 710(721.32) | Grad Norm 0.2530(0.6559) | Total Time 10.00(10.00)\n",
      "Iter 17040 | Time 14.3006(14.6185) | Bit/dim 0.9864(0.9869) | Xent 0.0000(0.0002) | Loss 0.9864(0.9871) | Error 0.0000(0.0001) Steps 710(720.79) | Grad Norm 0.4595(0.6461) | Total Time 10.00(10.00)\n",
      "Iter 17050 | Time 13.9781(14.5815) | Bit/dim 0.9815(0.9871) | Xent 0.0006(0.0003) | Loss 0.9818(0.9872) | Error 0.0000(0.0001) Steps 710(720.39) | Grad Norm 0.3288(0.6112) | Total Time 10.00(10.00)\n",
      "Iter 17060 | Time 15.0137(14.5954) | Bit/dim 1.0041(0.9878) | Xent 0.0001(0.0003) | Loss 1.0041(0.9880) | Error 0.0000(0.0001) Steps 722(719.63) | Grad Norm 0.4398(0.6462) | Total Time 10.00(10.00)\n",
      "Iter 17070 | Time 14.5675(14.6210) | Bit/dim 0.9803(0.9866) | Xent 0.0000(0.0003) | Loss 0.9803(0.9867) | Error 0.0000(0.0001) Steps 728(720.86) | Grad Norm 0.5979(0.6273) | Total Time 10.00(10.00)\n",
      "Iter 17080 | Time 14.3607(14.6547) | Bit/dim 0.9954(0.9860) | Xent 0.0000(0.0003) | Loss 0.9954(0.9861) | Error 0.0000(0.0001) Steps 710(721.81) | Grad Norm 0.5731(0.6324) | Total Time 10.00(10.00)\n",
      "Iter 17090 | Time 15.0337(14.6566) | Bit/dim 0.9715(0.9858) | Xent 0.0000(0.0003) | Loss 0.9715(0.9860) | Error 0.0000(0.0001) Steps 728(720.90) | Grad Norm 0.6650(0.6326) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0259 | Time 56.9332, Epoch Time 1037.7580(997.7645), Bit/dim 0.9859(best: 0.9853), Xent 0.0367, Loss 1.0043, Error 0.0074(best: 0.0070)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 17100 | Time 14.4976(14.6799) | Bit/dim 0.9773(0.9867) | Xent 0.0001(0.0003) | Loss 0.9774(0.9868) | Error 0.0000(0.0001) Steps 728(722.08) | Grad Norm 0.5870(0.6511) | Total Time 10.00(10.00)\n",
      "Iter 17110 | Time 14.0752(14.6313) | Bit/dim 0.9964(0.9867) | Xent 0.0000(0.0002) | Loss 0.9964(0.9868) | Error 0.0000(0.0001) Steps 716(722.51) | Grad Norm 0.2703(0.6277) | Total Time 10.00(10.00)\n",
      "Iter 17120 | Time 14.7310(14.7098) | Bit/dim 0.9917(0.9875) | Xent 0.0000(0.0002) | Loss 0.9917(0.9876) | Error 0.0000(0.0001) Steps 734(722.01) | Grad Norm 0.2516(0.6288) | Total Time 10.00(10.00)\n",
      "Iter 17130 | Time 14.6004(14.6844) | Bit/dim 0.9808(0.9864) | Xent 0.0000(0.0003) | Loss 0.9808(0.9865) | Error 0.0000(0.0001) Steps 704(719.70) | Grad Norm 0.3241(0.6080) | Total Time 10.00(10.00)\n",
      "Iter 17140 | Time 14.8328(14.6894) | Bit/dim 0.9881(0.9864) | Xent 0.0001(0.0004) | Loss 0.9882(0.9866) | Error 0.0000(0.0001) Steps 722(721.90) | Grad Norm 0.3386(0.5857) | Total Time 10.00(10.00)\n",
      "Iter 17150 | Time 14.3773(14.6247) | Bit/dim 0.9865(0.9872) | Xent 0.0000(0.0003) | Loss 0.9865(0.9873) | Error 0.0000(0.0001) Steps 734(724.05) | Grad Norm 0.9437(0.5666) | Total Time 10.00(10.00)\n",
      "Iter 17160 | Time 14.7760(14.6717) | Bit/dim 0.9918(0.9867) | Xent 0.0000(0.0002) | Loss 0.9918(0.9868) | Error 0.0000(0.0001) Steps 728(722.00) | Grad Norm 0.4931(0.5781) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0260 | Time 56.8042, Epoch Time 1040.1381(999.0357), Bit/dim 0.9852(best: 0.9853), Xent 0.0432, Loss 1.0068, Error 0.0076(best: 0.0070)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 17170 | Time 14.2413(14.6748) | Bit/dim 0.9885(0.9857) | Xent 0.0000(0.0002) | Loss 0.9885(0.9858) | Error 0.0000(0.0001) Steps 728(719.97) | Grad Norm 0.2055(0.5361) | Total Time 10.00(10.00)\n",
      "Iter 17180 | Time 14.7044(14.6895) | Bit/dim 0.9858(0.9859) | Xent 0.0000(0.0002) | Loss 0.9858(0.9860) | Error 0.0000(0.0000) Steps 710(719.38) | Grad Norm 0.7227(0.5242) | Total Time 10.00(10.00)\n",
      "Iter 17190 | Time 14.4224(14.6371) | Bit/dim 0.9865(0.9859) | Xent 0.0002(0.0002) | Loss 0.9865(0.9860) | Error 0.0000(0.0000) Steps 716(721.52) | Grad Norm 0.2836(0.5256) | Total Time 10.00(10.00)\n",
      "Iter 17200 | Time 14.7753(14.6477) | Bit/dim 1.0032(0.9858) | Xent 0.0000(0.0002) | Loss 1.0032(0.9859) | Error 0.0000(0.0001) Steps 734(722.68) | Grad Norm 0.7887(0.6124) | Total Time 10.00(10.00)\n",
      "Iter 17210 | Time 14.3252(14.6665) | Bit/dim 0.9767(0.9868) | Xent 0.0001(0.0002) | Loss 0.9768(0.9869) | Error 0.0000(0.0000) Steps 710(721.81) | Grad Norm 0.9893(0.6322) | Total Time 10.00(10.00)\n",
      "Iter 17220 | Time 14.7817(14.6283) | Bit/dim 1.0092(0.9879) | Xent 0.0001(0.0002) | Loss 1.0092(0.9879) | Error 0.0000(0.0000) Steps 728(722.69) | Grad Norm 0.2669(0.6299) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0261 | Time 56.5413, Epoch Time 1038.0493(1000.2061), Bit/dim 0.9848(best: 0.9852), Xent 0.0406, Loss 1.0051, Error 0.0074(best: 0.0070)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 17230 | Time 15.0453(14.6582) | Bit/dim 0.9812(0.9862) | Xent 0.0000(0.0001) | Loss 0.9812(0.9862) | Error 0.0000(0.0000) Steps 716(723.17) | Grad Norm 0.2671(0.5979) | Total Time 10.00(10.00)\n",
      "Iter 17240 | Time 14.7410(14.6603) | Bit/dim 0.9891(0.9862) | Xent 0.0002(0.0001) | Loss 0.9892(0.9863) | Error 0.0000(0.0000) Steps 734(721.66) | Grad Norm 0.3084(0.5598) | Total Time 10.00(10.00)\n",
      "Iter 17250 | Time 15.0289(14.6477) | Bit/dim 0.9815(0.9859) | Xent 0.0001(0.0001) | Loss 0.9815(0.9859) | Error 0.0000(0.0000) Steps 716(722.47) | Grad Norm 1.0464(0.5371) | Total Time 10.00(10.00)\n",
      "Iter 17260 | Time 14.6092(14.6289) | Bit/dim 0.9847(0.9853) | Xent 0.0006(0.0001) | Loss 0.9850(0.9854) | Error 0.0000(0.0000) Steps 716(725.18) | Grad Norm 0.2724(0.5694) | Total Time 10.00(10.00)\n",
      "Iter 17270 | Time 14.0960(14.6067) | Bit/dim 0.9773(0.9855) | Xent 0.0001(0.0001) | Loss 0.9774(0.9856) | Error 0.0000(0.0000) Steps 716(723.49) | Grad Norm 0.4429(0.5907) | Total Time 10.00(10.00)\n",
      "Iter 17280 | Time 14.3711(14.6123) | Bit/dim 0.9810(0.9855) | Xent 0.0001(0.0002) | Loss 0.9810(0.9856) | Error 0.0000(0.0000) Steps 704(721.01) | Grad Norm 1.0802(0.6848) | Total Time 10.00(10.00)\n",
      "Iter 17290 | Time 14.5689(14.6402) | Bit/dim 0.9893(0.9867) | Xent 0.0000(0.0002) | Loss 0.9894(0.9868) | Error 0.0000(0.0000) Steps 734(720.31) | Grad Norm 0.4813(0.6418) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0262 | Time 57.2621, Epoch Time 1038.3826(1001.3514), Bit/dim 0.9843(best: 0.9848), Xent 0.0395, Loss 1.0040, Error 0.0076(best: 0.0070)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 17300 | Time 14.5825(14.6682) | Bit/dim 0.9753(0.9863) | Xent 0.0000(0.0003) | Loss 0.9753(0.9864) | Error 0.0000(0.0001) Steps 740(721.42) | Grad Norm 0.2689(0.6009) | Total Time 10.00(10.00)\n",
      "Iter 17310 | Time 15.1294(14.6639) | Bit/dim 0.9904(0.9863) | Xent 0.0002(0.0002) | Loss 0.9905(0.9864) | Error 0.0000(0.0001) Steps 728(720.96) | Grad Norm 1.2231(0.6677) | Total Time 10.00(10.00)\n",
      "Iter 17320 | Time 14.2973(14.6342) | Bit/dim 0.9756(0.9868) | Xent 0.0001(0.0002) | Loss 0.9756(0.9869) | Error 0.0000(0.0000) Steps 710(719.12) | Grad Norm 0.4385(0.7083) | Total Time 10.00(10.00)\n",
      "Iter 17330 | Time 14.9910(14.6553) | Bit/dim 0.9599(0.9853) | Xent 0.0001(0.0002) | Loss 0.9600(0.9854) | Error 0.0000(0.0000) Steps 728(721.72) | Grad Norm 0.7338(0.6708) | Total Time 10.00(10.00)\n",
      "Iter 17340 | Time 14.3163(14.6947) | Bit/dim 0.9859(0.9852) | Xent 0.0000(0.0002) | Loss 0.9859(0.9853) | Error 0.0000(0.0000) Steps 734(722.00) | Grad Norm 1.0101(0.6811) | Total Time 10.00(10.00)\n",
      "Iter 17350 | Time 15.1331(14.6771) | Bit/dim 0.9785(0.9850) | Xent 0.0000(0.0002) | Loss 0.9785(0.9851) | Error 0.0000(0.0001) Steps 734(723.09) | Grad Norm 1.2834(0.7537) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0263 | Time 57.8474, Epoch Time 1041.9303(1002.5688), Bit/dim 0.9849(best: 0.9843), Xent 0.0401, Loss 1.0049, Error 0.0077(best: 0.0070)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 17360 | Time 14.1835(14.6639) | Bit/dim 0.9975(0.9859) | Xent 0.0000(0.0002) | Loss 0.9975(0.9860) | Error 0.0000(0.0001) Steps 716(722.76) | Grad Norm 0.5683(0.8050) | Total Time 10.00(10.00)\n",
      "Iter 17370 | Time 14.2814(14.6247) | Bit/dim 0.9786(0.9854) | Xent 0.0000(0.0002) | Loss 0.9786(0.9855) | Error 0.0000(0.0000) Steps 734(721.93) | Grad Norm 0.8522(0.7576) | Total Time 10.00(10.00)\n",
      "Iter 17380 | Time 15.1578(14.5986) | Bit/dim 0.9963(0.9856) | Xent 0.0006(0.0002) | Loss 0.9967(0.9857) | Error 0.0000(0.0000) Steps 710(720.65) | Grad Norm 3.0366(0.8216) | Total Time 10.00(10.00)\n",
      "Iter 17390 | Time 14.4122(14.6177) | Bit/dim 0.9915(0.9855) | Xent 0.0001(0.0001) | Loss 0.9915(0.9856) | Error 0.0000(0.0000) Steps 710(718.63) | Grad Norm 1.4820(0.9464) | Total Time 10.00(10.00)\n",
      "Iter 17400 | Time 14.6535(14.5867) | Bit/dim 0.9943(0.9859) | Xent 0.0001(0.0001) | Loss 0.9943(0.9859) | Error 0.0000(0.0000) Steps 710(719.18) | Grad Norm 0.7697(0.8834) | Total Time 10.00(10.00)\n",
      "Iter 17410 | Time 14.7518(14.6272) | Bit/dim 0.9748(0.9853) | Xent 0.0024(0.0002) | Loss 0.9760(0.9854) | Error 0.0011(0.0000) Steps 692(719.16) | Grad Norm 1.2788(0.7995) | Total Time 10.00(10.00)\n",
      "Iter 17420 | Time 14.4910(14.6122) | Bit/dim 0.9967(0.9856) | Xent 0.0001(0.0002) | Loss 0.9967(0.9857) | Error 0.0000(0.0001) Steps 734(720.13) | Grad Norm 1.0560(0.8209) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0264 | Time 57.1052, Epoch Time 1034.1468(1003.5161), Bit/dim 0.9841(best: 0.9843), Xent 0.0464, Loss 1.0073, Error 0.0088(best: 0.0070)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 17430 | Time 14.6386(14.6148) | Bit/dim 0.9819(0.9859) | Xent 0.0002(0.0003) | Loss 0.9820(0.9860) | Error 0.0000(0.0001) Steps 740(722.09) | Grad Norm 0.4050(0.9107) | Total Time 10.00(10.00)\n",
      "Iter 17440 | Time 14.6498(14.6257) | Bit/dim 0.9858(0.9848) | Xent 0.0000(0.0004) | Loss 0.9859(0.9849) | Error 0.0000(0.0002) Steps 704(723.20) | Grad Norm 0.5432(0.8821) | Total Time 10.00(10.00)\n",
      "Iter 17450 | Time 14.7904(14.6288) | Bit/dim 0.9839(0.9856) | Xent 0.0002(0.0004) | Loss 0.9840(0.9858) | Error 0.0000(0.0002) Steps 728(722.86) | Grad Norm 0.5396(0.7571) | Total Time 10.00(10.00)\n",
      "Iter 17460 | Time 14.4067(14.5813) | Bit/dim 0.9939(0.9857) | Xent 0.0000(0.0003) | Loss 0.9939(0.9858) | Error 0.0000(0.0001) Steps 734(722.07) | Grad Norm 0.4352(0.6923) | Total Time 10.00(10.00)\n",
      "Iter 17470 | Time 14.6229(14.5602) | Bit/dim 0.9763(0.9854) | Xent 0.0002(0.0003) | Loss 0.9764(0.9855) | Error 0.0000(0.0001) Steps 716(721.98) | Grad Norm 0.6370(0.6687) | Total Time 10.00(10.00)\n",
      "Iter 17480 | Time 14.7408(14.5996) | Bit/dim 0.9852(0.9853) | Xent 0.0000(0.0002) | Loss 0.9853(0.9854) | Error 0.0000(0.0001) Steps 728(720.90) | Grad Norm 1.3654(0.7399) | Total Time 10.00(10.00)\n",
      "Iter 17490 | Time 14.4956(14.5883) | Bit/dim 0.9864(0.9851) | Xent 0.0000(0.0002) | Loss 0.9864(0.9852) | Error 0.0000(0.0000) Steps 710(721.26) | Grad Norm 0.6825(0.7889) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0265 | Time 57.5777, Epoch Time 1035.8002(1004.4847), Bit/dim 0.9837(best: 0.9841), Xent 0.0426, Loss 1.0050, Error 0.0074(best: 0.0070)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 17500 | Time 14.5533(14.6262) | Bit/dim 0.9870(0.9856) | Xent 0.0000(0.0002) | Loss 0.9870(0.9857) | Error 0.0000(0.0000) Steps 716(722.30) | Grad Norm 0.3680(0.7382) | Total Time 10.00(10.00)\n",
      "Iter 17510 | Time 14.2751(14.5743) | Bit/dim 0.9722(0.9849) | Xent 0.0002(0.0001) | Loss 0.9723(0.9849) | Error 0.0000(0.0000) Steps 734(722.97) | Grad Norm 1.0533(0.6990) | Total Time 10.00(10.00)\n",
      "Iter 17520 | Time 15.0539(14.5949) | Bit/dim 0.9878(0.9846) | Xent 0.0001(0.0001) | Loss 0.9878(0.9847) | Error 0.0000(0.0000) Steps 710(722.90) | Grad Norm 0.4251(0.6405) | Total Time 10.00(10.00)\n",
      "Iter 17530 | Time 14.1467(14.5710) | Bit/dim 0.9768(0.9853) | Xent 0.0000(0.0002) | Loss 0.9768(0.9854) | Error 0.0000(0.0000) Steps 734(723.36) | Grad Norm 1.2007(0.6950) | Total Time 10.00(10.00)\n",
      "Iter 17540 | Time 14.7443(14.5642) | Bit/dim 0.9892(0.9855) | Xent 0.0000(0.0002) | Loss 0.9892(0.9856) | Error 0.0000(0.0000) Steps 734(724.13) | Grad Norm 1.0593(0.7517) | Total Time 10.00(10.00)\n",
      "Iter 17550 | Time 14.5757(14.5261) | Bit/dim 0.9887(0.9855) | Xent 0.0000(0.0002) | Loss 0.9887(0.9855) | Error 0.0000(0.0000) Steps 734(724.38) | Grad Norm 0.4110(0.8527) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0266 | Time 57.7565, Epoch Time 1033.0367(1005.3412), Bit/dim 0.9836(best: 0.9837), Xent 0.0374, Loss 1.0023, Error 0.0074(best: 0.0070)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 17560 | Time 14.8149(14.5651) | Bit/dim 0.9815(0.9848) | Xent 0.0001(0.0002) | Loss 0.9815(0.9849) | Error 0.0000(0.0000) Steps 734(723.99) | Grad Norm 0.3682(0.8520) | Total Time 10.00(10.00)\n",
      "Iter 17570 | Time 14.2312(14.5769) | Bit/dim 0.9855(0.9846) | Xent 0.0000(0.0002) | Loss 0.9855(0.9847) | Error 0.0000(0.0000) Steps 734(724.30) | Grad Norm 0.3778(0.7825) | Total Time 10.00(10.00)\n",
      "Iter 17580 | Time 14.3927(14.6032) | Bit/dim 0.9842(0.9847) | Xent 0.0000(0.0002) | Loss 0.9842(0.9848) | Error 0.0000(0.0000) Steps 734(724.22) | Grad Norm 1.8381(0.8314) | Total Time 10.00(10.00)\n",
      "Iter 17590 | Time 14.6998(14.5996) | Bit/dim 0.9915(0.9854) | Xent 0.0000(0.0002) | Loss 0.9915(0.9855) | Error 0.0000(0.0000) Steps 710(723.10) | Grad Norm 1.9086(0.9037) | Total Time 10.00(10.00)\n",
      "Iter 17600 | Time 14.6483(14.5870) | Bit/dim 0.9817(0.9855) | Xent 0.0000(0.0002) | Loss 0.9817(0.9855) | Error 0.0000(0.0000) Steps 710(722.65) | Grad Norm 1.3455(0.9234) | Total Time 10.00(10.00)\n",
      "Iter 17610 | Time 14.8376(14.6031) | Bit/dim 0.9946(0.9854) | Xent 0.0005(0.0002) | Loss 0.9948(0.9855) | Error 0.0000(0.0001) Steps 728(723.10) | Grad Norm 1.6980(0.9927) | Total Time 10.00(10.00)\n",
      "Iter 17620 | Time 14.6918(14.6266) | Bit/dim 0.9862(0.9838) | Xent 0.0001(0.0002) | Loss 0.9862(0.9840) | Error 0.0000(0.0001) Steps 734(723.42) | Grad Norm 0.4518(1.0204) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0267 | Time 57.3845, Epoch Time 1038.4592(1006.3348), Bit/dim 0.9834(best: 0.9836), Xent 0.0432, Loss 1.0050, Error 0.0078(best: 0.0070)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 17630 | Time 15.3252(14.6422) | Bit/dim 0.9886(0.9846) | Xent 0.0002(0.0002) | Loss 0.9887(0.9847) | Error 0.0000(0.0001) Steps 692(722.68) | Grad Norm 1.3661(0.9028) | Total Time 10.00(10.00)\n",
      "Iter 17640 | Time 14.7364(14.6410) | Bit/dim 0.9952(0.9840) | Xent 0.0000(0.0003) | Loss 0.9952(0.9841) | Error 0.0000(0.0001) Steps 734(723.87) | Grad Norm 0.9175(0.8840) | Total Time 10.00(10.00)\n",
      "Iter 17650 | Time 14.8162(14.6804) | Bit/dim 0.9853(0.9843) | Xent 0.0003(0.0002) | Loss 0.9855(0.9844) | Error 0.0000(0.0001) Steps 716(722.42) | Grad Norm 1.0347(0.8272) | Total Time 10.00(10.00)\n",
      "Iter 17660 | Time 14.3940(14.6493) | Bit/dim 0.9938(0.9850) | Xent 0.0006(0.0002) | Loss 0.9941(0.9851) | Error 0.0000(0.0000) Steps 722(722.61) | Grad Norm 0.5478(0.8210) | Total Time 10.00(10.00)\n",
      "Iter 17670 | Time 14.2818(14.5921) | Bit/dim 0.9967(0.9845) | Xent 0.0000(0.0002) | Loss 0.9967(0.9846) | Error 0.0000(0.0000) Steps 710(721.70) | Grad Norm 0.3626(0.7201) | Total Time 10.00(10.00)\n",
      "Iter 17680 | Time 14.4068(14.5755) | Bit/dim 0.9905(0.9842) | Xent 0.0000(0.0002) | Loss 0.9905(0.9843) | Error 0.0000(0.0000) Steps 710(722.79) | Grad Norm 0.2909(0.7028) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0268 | Time 58.0164, Epoch Time 1036.9725(1007.2539), Bit/dim 0.9835(best: 0.9834), Xent 0.0359, Loss 1.0015, Error 0.0079(best: 0.0070)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 17690 | Time 14.5055(14.5761) | Bit/dim 0.9777(0.9839) | Xent 0.0000(0.0001) | Loss 0.9777(0.9840) | Error 0.0000(0.0000) Steps 746(721.96) | Grad Norm 0.4635(0.6974) | Total Time 10.00(10.00)\n",
      "Iter 17700 | Time 14.0856(14.5962) | Bit/dim 0.9969(0.9837) | Xent 0.0000(0.0002) | Loss 0.9969(0.9838) | Error 0.0000(0.0000) Steps 728(723.93) | Grad Norm 0.4020(0.6939) | Total Time 10.00(10.00)\n",
      "Iter 17710 | Time 14.5590(14.5931) | Bit/dim 0.9865(0.9845) | Xent 0.0000(0.0002) | Loss 0.9865(0.9846) | Error 0.0000(0.0000) Steps 710(724.14) | Grad Norm 0.8250(0.6371) | Total Time 10.00(10.00)\n",
      "Iter 17720 | Time 14.6386(14.6097) | Bit/dim 1.0012(0.9856) | Xent 0.0002(0.0002) | Loss 1.0013(0.9856) | Error 0.0000(0.0000) Steps 728(723.30) | Grad Norm 0.9447(0.8820) | Total Time 10.00(10.00)\n",
      "Iter 17730 | Time 13.9539(14.6056) | Bit/dim 0.9837(0.9846) | Xent 0.0000(0.0001) | Loss 0.9837(0.9847) | Error 0.0000(0.0000) Steps 722(723.32) | Grad Norm 0.2949(0.8119) | Total Time 10.00(10.00)\n",
      "Iter 17740 | Time 14.7795(14.6170) | Bit/dim 0.9810(0.9841) | Xent 0.0000(0.0001) | Loss 0.9810(0.9842) | Error 0.0000(0.0000) Steps 710(721.20) | Grad Norm 0.9178(0.7719) | Total Time 10.00(10.00)\n",
      "Iter 17750 | Time 14.4570(14.5979) | Bit/dim 0.9968(0.9843) | Xent 0.0000(0.0001) | Loss 0.9968(0.9843) | Error 0.0000(0.0000) Steps 710(721.41) | Grad Norm 0.3228(0.7339) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0269 | Time 57.6624, Epoch Time 1036.8706(1008.1424), Bit/dim 0.9832(best: 0.9834), Xent 0.0431, Loss 1.0047, Error 0.0073(best: 0.0070)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 17760 | Time 14.3851(14.5729) | Bit/dim 0.9816(0.9839) | Xent 0.0001(0.0002) | Loss 0.9816(0.9840) | Error 0.0000(0.0001) Steps 740(723.37) | Grad Norm 0.5498(0.7550) | Total Time 10.00(10.00)\n",
      "Iter 17770 | Time 14.9298(14.6098) | Bit/dim 0.9810(0.9836) | Xent 0.0000(0.0002) | Loss 0.9810(0.9837) | Error 0.0000(0.0001) Steps 692(723.71) | Grad Norm 1.1278(0.7626) | Total Time 10.00(10.00)\n",
      "Iter 17780 | Time 14.3382(14.5806) | Bit/dim 0.9889(0.9837) | Xent 0.0000(0.0002) | Loss 0.9889(0.9838) | Error 0.0000(0.0000) Steps 710(722.39) | Grad Norm 0.5862(0.7719) | Total Time 10.00(10.00)\n",
      "Iter 17790 | Time 14.7591(14.6170) | Bit/dim 0.9833(0.9827) | Xent 0.0003(0.0001) | Loss 0.9835(0.9828) | Error 0.0000(0.0000) Steps 734(723.32) | Grad Norm 1.9386(0.7910) | Total Time 10.00(10.00)\n",
      "Iter 17800 | Time 14.8655(14.6422) | Bit/dim 0.9907(0.9835) | Xent 0.0003(0.0002) | Loss 0.9908(0.9836) | Error 0.0000(0.0000) Steps 722(723.50) | Grad Norm 0.6302(0.7527) | Total Time 10.00(10.00)\n",
      "Iter 17810 | Time 14.8464(14.6464) | Bit/dim 0.9944(0.9846) | Xent 0.0001(0.0002) | Loss 0.9944(0.9847) | Error 0.0000(0.0001) Steps 728(722.93) | Grad Norm 0.9534(0.8242) | Total Time 10.00(10.00)\n",
      "Iter 17820 | Time 14.7010(14.6480) | Bit/dim 0.9873(0.9851) | Xent 0.0003(0.0002) | Loss 0.9874(0.9852) | Error 0.0000(0.0000) Steps 734(722.89) | Grad Norm 0.5672(0.7700) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0270 | Time 57.2660, Epoch Time 1038.8040(1009.0622), Bit/dim 0.9828(best: 0.9832), Xent 0.0395, Loss 1.0025, Error 0.0081(best: 0.0070)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 17830 | Time 14.5186(14.6268) | Bit/dim 0.9777(0.9844) | Xent 0.0004(0.0002) | Loss 0.9779(0.9845) | Error 0.0000(0.0000) Steps 710(721.76) | Grad Norm 0.5795(0.6947) | Total Time 10.00(10.00)\n",
      "Iter 17840 | Time 14.9645(14.6106) | Bit/dim 0.9860(0.9843) | Xent 0.0000(0.0002) | Loss 0.9860(0.9844) | Error 0.0000(0.0001) Steps 734(722.76) | Grad Norm 1.5605(0.9894) | Total Time 10.00(10.00)\n",
      "Iter 17850 | Time 14.7027(14.6344) | Bit/dim 0.9872(0.9854) | Xent 0.0000(0.0002) | Loss 0.9872(0.9855) | Error 0.0000(0.0000) Steps 728(722.99) | Grad Norm 0.7857(1.0398) | Total Time 10.00(10.00)\n",
      "Iter 17860 | Time 14.7340(14.6345) | Bit/dim 0.9833(0.9849) | Xent 0.0000(0.0001) | Loss 0.9833(0.9850) | Error 0.0000(0.0000) Steps 716(723.61) | Grad Norm 0.2373(1.0229) | Total Time 10.00(10.00)\n",
      "Iter 17870 | Time 14.6702(14.6321) | Bit/dim 0.9727(0.9834) | Xent 0.0000(0.0001) | Loss 0.9728(0.9834) | Error 0.0000(0.0000) Steps 734(722.63) | Grad Norm 0.7234(0.9685) | Total Time 10.00(10.00)\n",
      "Iter 17880 | Time 14.6721(14.6369) | Bit/dim 0.9913(0.9831) | Xent 0.0001(0.0001) | Loss 0.9914(0.9831) | Error 0.0000(0.0000) Steps 704(720.38) | Grad Norm 0.4735(0.9169) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0271 | Time 57.0250, Epoch Time 1036.4996(1009.8854), Bit/dim 0.9832(best: 0.9828), Xent 0.0464, Loss 1.0064, Error 0.0082(best: 0.0070)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 17890 | Time 14.4637(14.6111) | Bit/dim 0.9739(0.9839) | Xent 0.0012(0.0002) | Loss 0.9745(0.9840) | Error 0.0011(0.0000) Steps 710(721.09) | Grad Norm 0.3918(0.8623) | Total Time 10.00(10.00)\n",
      "Iter 17900 | Time 14.5164(14.6313) | Bit/dim 0.9812(0.9831) | Xent 0.0002(0.0001) | Loss 0.9813(0.9832) | Error 0.0000(0.0000) Steps 728(721.48) | Grad Norm 0.3929(0.7745) | Total Time 10.00(10.00)\n",
      "Iter 17910 | Time 14.2593(14.6329) | Bit/dim 0.9799(0.9832) | Xent 0.0000(0.0001) | Loss 0.9799(0.9832) | Error 0.0000(0.0000) Steps 734(724.59) | Grad Norm 0.5391(0.9409) | Total Time 10.00(10.00)\n",
      "Iter 17920 | Time 14.7485(14.6708) | Bit/dim 0.9895(0.9828) | Xent 0.0003(0.0001) | Loss 0.9896(0.9828) | Error 0.0000(0.0000) Steps 728(725.54) | Grad Norm 1.9161(1.2356) | Total Time 10.00(10.00)\n",
      "Iter 17930 | Time 14.2984(14.6584) | Bit/dim 0.9770(0.9827) | Xent 0.0000(0.0001) | Loss 0.9770(0.9828) | Error 0.0000(0.0000) Steps 734(725.59) | Grad Norm 0.9620(1.3129) | Total Time 10.00(10.00)\n",
      "Iter 17940 | Time 14.9229(14.6792) | Bit/dim 0.9883(0.9830) | Xent 0.0001(0.0002) | Loss 0.9883(0.9831) | Error 0.0000(0.0000) Steps 734(726.77) | Grad Norm 0.9273(1.4132) | Total Time 10.00(10.00)\n",
      "Iter 17950 | Time 14.8460(14.6663) | Bit/dim 0.9809(0.9842) | Xent 0.0000(0.0002) | Loss 0.9810(0.9843) | Error 0.0000(0.0000) Steps 734(725.86) | Grad Norm 2.2321(1.3061) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0272 | Time 57.1621, Epoch Time 1041.0468(1010.8202), Bit/dim 0.9829(best: 0.9828), Xent 0.0421, Loss 1.0040, Error 0.0078(best: 0.0070)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 17960 | Time 14.8686(14.6616) | Bit/dim 0.9692(0.9832) | Xent 0.0000(0.0001) | Loss 0.9692(0.9832) | Error 0.0000(0.0000) Steps 740(726.91) | Grad Norm 1.2423(1.2704) | Total Time 10.00(10.00)\n",
      "Iter 17970 | Time 14.4289(14.6619) | Bit/dim 0.9798(0.9832) | Xent 0.0000(0.0001) | Loss 0.9798(0.9832) | Error 0.0000(0.0000) Steps 740(726.75) | Grad Norm 0.3581(1.2449) | Total Time 10.00(10.00)\n",
      "Iter 17980 | Time 14.7035(14.6578) | Bit/dim 0.9774(0.9833) | Xent 0.0000(0.0001) | Loss 0.9774(0.9834) | Error 0.0000(0.0000) Steps 728(725.93) | Grad Norm 1.0690(1.2535) | Total Time 10.00(10.00)\n",
      "Iter 17990 | Time 14.3363(14.6174) | Bit/dim 0.9827(0.9825) | Xent 0.0001(0.0002) | Loss 0.9827(0.9826) | Error 0.0000(0.0001) Steps 728(723.98) | Grad Norm 2.2353(1.6505) | Total Time 10.00(10.00)\n",
      "Iter 18000 | Time 14.4551(14.6574) | Bit/dim 0.9683(0.9831) | Xent 0.0000(0.0001) | Loss 0.9683(0.9832) | Error 0.0000(0.0000) Steps 734(722.55) | Grad Norm 0.8236(1.6113) | Total Time 10.00(10.00)\n",
      "Iter 18010 | Time 14.2912(14.6618) | Bit/dim 0.9806(0.9837) | Xent 0.0002(0.0001) | Loss 0.9806(0.9838) | Error 0.0000(0.0000) Steps 734(721.80) | Grad Norm 1.5614(1.7197) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0273 | Time 57.0984, Epoch Time 1037.4100(1011.6179), Bit/dim 0.9824(best: 0.9828), Xent 0.0408, Loss 1.0028, Error 0.0073(best: 0.0070)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 18020 | Time 14.6343(14.6247) | Bit/dim 0.9790(0.9837) | Xent 0.0003(0.0001) | Loss 0.9792(0.9838) | Error 0.0000(0.0000) Steps 722(721.95) | Grad Norm 0.3961(1.5428) | Total Time 10.00(10.00)\n",
      "Iter 18030 | Time 14.2095(14.6504) | Bit/dim 0.9826(0.9837) | Xent 0.0000(0.0002) | Loss 0.9826(0.9838) | Error 0.0000(0.0000) Steps 734(722.68) | Grad Norm 2.1991(1.4021) | Total Time 10.00(10.00)\n",
      "Iter 18040 | Time 14.5216(14.6698) | Bit/dim 0.9977(0.9845) | Xent 0.0000(0.0002) | Loss 0.9977(0.9846) | Error 0.0000(0.0000) Steps 722(721.92) | Grad Norm 2.1046(1.4640) | Total Time 10.00(10.00)\n",
      "Iter 18050 | Time 14.7343(14.6430) | Bit/dim 0.9742(0.9834) | Xent 0.0011(0.0003) | Loss 0.9747(0.9835) | Error 0.0011(0.0001) Steps 710(720.99) | Grad Norm 1.1379(1.3036) | Total Time 10.00(10.00)\n",
      "Iter 18060 | Time 14.8443(14.6820) | Bit/dim 0.9755(0.9836) | Xent 0.0000(0.0002) | Loss 0.9755(0.9837) | Error 0.0000(0.0001) Steps 728(721.10) | Grad Norm 0.4113(1.2310) | Total Time 10.00(10.00)\n",
      "Iter 18070 | Time 14.5108(14.6527) | Bit/dim 0.9730(0.9838) | Xent 0.0001(0.0002) | Loss 0.9730(0.9838) | Error 0.0000(0.0001) Steps 722(720.42) | Grad Norm 1.3397(1.3086) | Total Time 10.00(10.00)\n",
      "Iter 18080 | Time 14.1334(14.6626) | Bit/dim 0.9791(0.9837) | Xent 0.0000(0.0002) | Loss 0.9791(0.9838) | Error 0.0000(0.0001) Steps 722(722.26) | Grad Norm 0.4550(1.1930) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0274 | Time 57.4597, Epoch Time 1041.1466(1012.5038), Bit/dim 0.9819(best: 0.9824), Xent 0.0418, Loss 1.0028, Error 0.0081(best: 0.0070)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 18090 | Time 14.4907(14.6418) | Bit/dim 0.9764(0.9835) | Xent 0.0001(0.0002) | Loss 0.9764(0.9835) | Error 0.0000(0.0001) Steps 722(723.94) | Grad Norm 1.4518(1.0990) | Total Time 10.00(10.00)\n",
      "Iter 18100 | Time 14.3179(14.6432) | Bit/dim 0.9781(0.9838) | Xent 0.0000(0.0001) | Loss 0.9781(0.9838) | Error 0.0000(0.0000) Steps 728(723.86) | Grad Norm 0.8854(0.9991) | Total Time 10.00(10.00)\n",
      "Iter 18110 | Time 14.7869(14.6237) | Bit/dim 0.9849(0.9829) | Xent 0.0000(0.0002) | Loss 0.9849(0.9830) | Error 0.0000(0.0001) Steps 704(723.14) | Grad Norm 0.6155(0.9193) | Total Time 10.00(10.00)\n",
      "Iter 18120 | Time 14.9238(14.6533) | Bit/dim 0.9888(0.9833) | Xent 0.0003(0.0001) | Loss 0.9889(0.9833) | Error 0.0000(0.0000) Steps 722(722.92) | Grad Norm 0.2818(0.8526) | Total Time 10.00(10.00)\n",
      "Iter 18130 | Time 15.0409(14.6212) | Bit/dim 0.9755(0.9829) | Xent 0.0000(0.0002) | Loss 0.9755(0.9830) | Error 0.0000(0.0001) Steps 722(723.39) | Grad Norm 2.3208(1.1013) | Total Time 10.00(10.00)\n",
      "Iter 18140 | Time 15.2570(14.6480) | Bit/dim 0.9919(0.9827) | Xent 0.0000(0.0001) | Loss 0.9919(0.9828) | Error 0.0000(0.0000) Steps 710(722.94) | Grad Norm 1.7803(1.2505) | Total Time 10.00(10.00)\n",
      "Iter 18150 | Time 14.3544(14.6239) | Bit/dim 0.9861(0.9832) | Xent 0.0002(0.0002) | Loss 0.9862(0.9833) | Error 0.0000(0.0001) Steps 728(723.41) | Grad Norm 1.5447(1.4671) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0275 | Time 57.5375, Epoch Time 1037.2550(1013.2463), Bit/dim 0.9821(best: 0.9819), Xent 0.0416, Loss 1.0028, Error 0.0082(best: 0.0070)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 18160 | Time 14.7408(14.6213) | Bit/dim 0.9824(0.9821) | Xent 0.0019(0.0002) | Loss 0.9833(0.9822) | Error 0.0011(0.0001) Steps 716(722.94) | Grad Norm 3.0031(1.4859) | Total Time 10.00(10.00)\n",
      "Iter 18170 | Time 14.4301(14.6051) | Bit/dim 0.9930(0.9826) | Xent 0.0000(0.0002) | Loss 0.9930(0.9827) | Error 0.0000(0.0001) Steps 722(723.12) | Grad Norm 3.4580(1.7638) | Total Time 10.00(10.00)\n",
      "Iter 18180 | Time 14.5873(14.5927) | Bit/dim 0.9832(0.9830) | Xent 0.0001(0.0003) | Loss 0.9833(0.9832) | Error 0.0000(0.0001) Steps 698(722.82) | Grad Norm 3.7642(2.4053) | Total Time 10.00(10.00)\n",
      "Iter 18190 | Time 14.8571(14.6086) | Bit/dim 0.9880(0.9838) | Xent 0.0001(0.0004) | Loss 0.9881(0.9840) | Error 0.0000(0.0001) Steps 710(722.27) | Grad Norm 1.3875(2.4727) | Total Time 10.00(10.00)\n",
      "Iter 18200 | Time 14.3175(14.5936) | Bit/dim 0.9929(0.9840) | Xent 0.0000(0.0003) | Loss 0.9929(0.9841) | Error 0.0000(0.0001) Steps 728(721.94) | Grad Norm 5.7883(2.6551) | Total Time 10.00(10.00)\n",
      "Iter 18210 | Time 14.7737(14.6235) | Bit/dim 0.9771(0.9829) | Xent 0.0000(0.0003) | Loss 0.9771(0.9830) | Error 0.0000(0.0001) Steps 734(723.09) | Grad Norm 0.6881(2.6199) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0276 | Time 57.0794, Epoch Time 1036.6654(1013.9489), Bit/dim 0.9816(best: 0.9819), Xent 0.0382, Loss 1.0006, Error 0.0071(best: 0.0070)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 18220 | Time 14.8747(14.6184) | Bit/dim 0.9949(0.9841) | Xent 0.0000(0.0002) | Loss 0.9949(0.9842) | Error 0.0000(0.0001) Steps 740(722.84) | Grad Norm 3.7397(2.7964) | Total Time 10.00(10.00)\n",
      "Iter 18230 | Time 14.5501(14.5959) | Bit/dim 0.9769(0.9830) | Xent 0.0000(0.0002) | Loss 0.9769(0.9830) | Error 0.0000(0.0000) Steps 734(723.94) | Grad Norm 2.1016(2.6299) | Total Time 10.00(10.00)\n",
      "Iter 18240 | Time 15.2627(14.6329) | Bit/dim 0.9756(0.9829) | Xent 0.0000(0.0002) | Loss 0.9756(0.9830) | Error 0.0000(0.0000) Steps 710(722.90) | Grad Norm 1.8789(2.5495) | Total Time 10.00(10.00)\n",
      "Iter 18250 | Time 14.5784(14.6436) | Bit/dim 0.9767(0.9833) | Xent 0.0000(0.0002) | Loss 0.9767(0.9834) | Error 0.0000(0.0001) Steps 722(725.06) | Grad Norm 2.5872(2.1398) | Total Time 10.00(10.00)\n",
      "Iter 18260 | Time 14.9269(14.6416) | Bit/dim 0.9875(0.9832) | Xent 0.0001(0.0003) | Loss 0.9875(0.9834) | Error 0.0000(0.0001) Steps 728(724.80) | Grad Norm 0.4043(2.0018) | Total Time 10.00(10.00)\n",
      "Iter 18270 | Time 15.1689(14.6492) | Bit/dim 0.9840(0.9831) | Xent 0.0001(0.0003) | Loss 0.9841(0.9833) | Error 0.0000(0.0001) Steps 734(726.58) | Grad Norm 0.9417(1.6914) | Total Time 10.00(10.00)\n",
      "Iter 18280 | Time 14.5457(14.6685) | Bit/dim 0.9757(0.9821) | Xent 0.0000(0.0002) | Loss 0.9758(0.9822) | Error 0.0000(0.0001) Steps 710(727.73) | Grad Norm 0.6443(1.5030) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0277 | Time 57.5410, Epoch Time 1039.6610(1014.7202), Bit/dim 0.9815(best: 0.9816), Xent 0.0406, Loss 1.0018, Error 0.0084(best: 0.0070)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 18290 | Time 14.6121(14.6515) | Bit/dim 0.9690(0.9814) | Xent 0.0000(0.0002) | Loss 0.9690(0.9815) | Error 0.0000(0.0000) Steps 734(727.08) | Grad Norm 1.3028(1.2855) | Total Time 10.00(10.00)\n",
      "Iter 18300 | Time 15.0246(14.6769) | Bit/dim 0.9905(0.9813) | Xent 0.0004(0.0002) | Loss 0.9907(0.9814) | Error 0.0000(0.0000) Steps 746(728.18) | Grad Norm 3.5084(1.3837) | Total Time 10.00(10.00)\n",
      "Iter 18310 | Time 15.2463(14.6958) | Bit/dim 0.9869(0.9819) | Xent 0.0000(0.0001) | Loss 0.9869(0.9820) | Error 0.0000(0.0000) Steps 734(727.00) | Grad Norm 0.9304(1.4202) | Total Time 10.00(10.00)\n",
      "Iter 18320 | Time 13.9729(14.6581) | Bit/dim 0.9870(0.9825) | Xent 0.0000(0.0001) | Loss 0.9870(0.9826) | Error 0.0000(0.0000) Steps 710(726.14) | Grad Norm 0.3561(1.2902) | Total Time 10.00(10.00)\n",
      "Iter 18330 | Time 14.7086(14.6613) | Bit/dim 0.9877(0.9820) | Xent 0.0000(0.0002) | Loss 0.9877(0.9821) | Error 0.0000(0.0000) Steps 734(725.62) | Grad Norm 0.2916(1.4976) | Total Time 10.00(10.00)\n",
      "Iter 18340 | Time 15.1288(14.6960) | Bit/dim 0.9948(0.9824) | Xent 0.0001(0.0002) | Loss 0.9948(0.9825) | Error 0.0000(0.0001) Steps 740(724.96) | Grad Norm 1.5157(1.5670) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0278 | Time 57.0654, Epoch Time 1041.1912(1015.5144), Bit/dim 0.9812(best: 0.9815), Xent 0.0393, Loss 1.0009, Error 0.0070(best: 0.0070)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 18350 | Time 14.8999(14.7055) | Bit/dim 0.9750(0.9828) | Xent 0.0000(0.0002) | Loss 0.9750(0.9829) | Error 0.0000(0.0000) Steps 734(725.30) | Grad Norm 1.6175(1.4566) | Total Time 10.00(10.00)\n",
      "Iter 18360 | Time 14.5409(14.6849) | Bit/dim 0.9816(0.9822) | Xent 0.0000(0.0002) | Loss 0.9816(0.9823) | Error 0.0000(0.0000) Steps 710(725.74) | Grad Norm 0.9841(1.3972) | Total Time 10.00(10.00)\n",
      "Iter 18370 | Time 15.1788(14.6797) | Bit/dim 0.9864(0.9832) | Xent 0.0001(0.0003) | Loss 0.9864(0.9834) | Error 0.0000(0.0001) Steps 740(726.50) | Grad Norm 3.4012(1.5606) | Total Time 10.00(10.00)\n",
      "Iter 18380 | Time 14.4597(14.6177) | Bit/dim 0.9749(0.9830) | Xent 0.0014(0.0003) | Loss 0.9756(0.9832) | Error 0.0011(0.0001) Steps 698(722.51) | Grad Norm 0.7161(2.0798) | Total Time 10.00(10.00)\n",
      "Iter 18390 | Time 14.9550(14.6357) | Bit/dim 0.9715(0.9835) | Xent 0.0000(0.0002) | Loss 0.9715(0.9836) | Error 0.0000(0.0001) Steps 728(722.85) | Grad Norm 3.2976(1.9673) | Total Time 10.00(10.00)\n",
      "Iter 18400 | Time 14.4471(14.5884) | Bit/dim 0.9897(0.9826) | Xent 0.0007(0.0002) | Loss 0.9901(0.9827) | Error 0.0000(0.0000) Steps 710(721.41) | Grad Norm 1.8427(1.8305) | Total Time 10.00(10.00)\n",
      "Iter 18410 | Time 14.3942(14.5533) | Bit/dim 0.9848(0.9827) | Xent 0.0000(0.0002) | Loss 0.9848(0.9828) | Error 0.0000(0.0000) Steps 728(721.55) | Grad Norm 2.0552(1.7568) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0279 | Time 57.4599, Epoch Time 1034.2936(1016.0777), Bit/dim 0.9814(best: 0.9812), Xent 0.0412, Loss 1.0020, Error 0.0070(best: 0.0070)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 18420 | Time 14.7968(14.5617) | Bit/dim 0.9868(0.9827) | Xent 0.0001(0.0002) | Loss 0.9869(0.9828) | Error 0.0000(0.0001) Steps 722(722.68) | Grad Norm 0.5813(1.5757) | Total Time 10.00(10.00)\n",
      "Iter 18430 | Time 14.5800(14.5735) | Bit/dim 0.9912(0.9825) | Xent 0.0000(0.0003) | Loss 0.9912(0.9826) | Error 0.0000(0.0001) Steps 728(723.80) | Grad Norm 0.4262(1.4334) | Total Time 10.00(10.00)\n",
      "Iter 18440 | Time 14.4283(14.5738) | Bit/dim 0.9782(0.9818) | Xent 0.0004(0.0002) | Loss 0.9784(0.9819) | Error 0.0000(0.0000) Steps 740(725.26) | Grad Norm 0.5271(1.2598) | Total Time 10.00(10.00)\n",
      "Iter 18450 | Time 14.8161(14.6120) | Bit/dim 0.9821(0.9819) | Xent 0.0000(0.0002) | Loss 0.9821(0.9820) | Error 0.0000(0.0001) Steps 716(723.17) | Grad Norm 0.8401(1.1033) | Total Time 10.00(10.00)\n",
      "Iter 18460 | Time 14.1549(14.6455) | Bit/dim 0.9850(0.9822) | Xent 0.0004(0.0002) | Loss 0.9852(0.9823) | Error 0.0000(0.0000) Steps 704(723.01) | Grad Norm 2.8321(1.1518) | Total Time 10.00(10.00)\n",
      "Iter 18470 | Time 14.5114(14.6560) | Bit/dim 0.9893(0.9823) | Xent 0.0022(0.0002) | Loss 0.9903(0.9824) | Error 0.0011(0.0001) Steps 728(723.17) | Grad Norm 0.5437(1.4272) | Total Time 10.00(10.00)\n",
      "Iter 18480 | Time 14.3294(14.6563) | Bit/dim 0.9814(0.9829) | Xent 0.0000(0.0002) | Loss 0.9814(0.9829) | Error 0.0000(0.0001) Steps 710(722.44) | Grad Norm 0.8875(1.3367) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0280 | Time 57.5723, Epoch Time 1040.3250(1016.8052), Bit/dim 0.9812(best: 0.9812), Xent 0.0396, Loss 1.0010, Error 0.0076(best: 0.0070)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 18490 | Time 14.7744(14.6392) | Bit/dim 0.9879(0.9820) | Xent 0.0001(0.0002) | Loss 0.9880(0.9821) | Error 0.0000(0.0000) Steps 728(723.89) | Grad Norm 1.8838(1.4916) | Total Time 10.00(10.00)\n",
      "Iter 18500 | Time 15.0548(14.6541) | Bit/dim 0.9759(0.9819) | Xent 0.0000(0.0001) | Loss 0.9759(0.9820) | Error 0.0000(0.0000) Steps 734(723.06) | Grad Norm 2.0038(1.8820) | Total Time 10.00(10.00)\n",
      "Iter 18510 | Time 15.3219(14.6908) | Bit/dim 0.9884(0.9832) | Xent 0.0001(0.0001) | Loss 0.9885(0.9833) | Error 0.0000(0.0000) Steps 728(722.76) | Grad Norm 8.3240(2.8849) | Total Time 10.00(10.00)\n",
      "Iter 18520 | Time 14.7555(14.6635) | Bit/dim 0.9819(0.9822) | Xent 0.0000(0.0001) | Loss 0.9819(0.9822) | Error 0.0000(0.0000) Steps 716(721.92) | Grad Norm 2.6554(2.7879) | Total Time 10.00(10.00)\n",
      "Iter 18530 | Time 14.2759(14.6711) | Bit/dim 0.9866(0.9823) | Xent 0.0013(0.0002) | Loss 0.9873(0.9824) | Error 0.0011(0.0001) Steps 734(721.06) | Grad Norm 2.7676(2.5356) | Total Time 10.00(10.00)\n",
      "Iter 18540 | Time 14.3243(14.7043) | Bit/dim 0.9823(0.9821) | Xent 0.0010(0.0002) | Loss 0.9828(0.9822) | Error 0.0011(0.0001) Steps 734(721.31) | Grad Norm 3.0056(2.4264) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0281 | Time 56.5152, Epoch Time 1040.7117(1017.5224), Bit/dim 0.9807(best: 0.9812), Xent 0.0383, Loss 0.9999, Error 0.0080(best: 0.0070)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 18550 | Time 14.5116(14.6902) | Bit/dim 0.9865(0.9827) | Xent 0.0000(0.0002) | Loss 0.9865(0.9828) | Error 0.0000(0.0001) Steps 722(723.94) | Grad Norm 3.3681(2.7538) | Total Time 10.00(10.00)\n",
      "Iter 18560 | Time 14.4027(14.6744) | Bit/dim 0.9820(0.9824) | Xent 0.0000(0.0001) | Loss 0.9820(0.9825) | Error 0.0000(0.0001) Steps 734(725.02) | Grad Norm 0.5756(3.2679) | Total Time 10.00(10.00)\n",
      "Iter 18570 | Time 15.1202(14.6571) | Bit/dim 0.9903(0.9827) | Xent 0.0000(0.0001) | Loss 0.9903(0.9828) | Error 0.0000(0.0001) Steps 722(723.19) | Grad Norm 1.7143(3.5755) | Total Time 10.00(10.00)\n",
      "Iter 18580 | Time 14.7323(14.6684) | Bit/dim 0.9850(0.9826) | Xent 0.0006(0.0001) | Loss 0.9853(0.9827) | Error 0.0000(0.0001) Steps 728(724.97) | Grad Norm 0.9703(3.1845) | Total Time 10.00(10.00)\n",
      "Iter 18590 | Time 14.5697(14.6535) | Bit/dim 0.9760(0.9826) | Xent 0.0005(0.0001) | Loss 0.9763(0.9827) | Error 0.0000(0.0000) Steps 728(724.85) | Grad Norm 1.4518(2.8570) | Total Time 10.00(10.00)\n",
      "Iter 18600 | Time 14.9218(14.6695) | Bit/dim 0.9814(0.9822) | Xent 0.0000(0.0001) | Loss 0.9814(0.9823) | Error 0.0000(0.0001) Steps 728(723.57) | Grad Norm 2.8797(2.6724) | Total Time 10.00(10.00)\n",
      "Iter 18610 | Time 14.2883(14.6683) | Bit/dim 0.9788(0.9822) | Xent 0.0003(0.0001) | Loss 0.9789(0.9822) | Error 0.0000(0.0000) Steps 722(723.19) | Grad Norm 2.4525(2.3266) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0282 | Time 58.0911, Epoch Time 1040.6596(1018.2165), Bit/dim 0.9799(best: 0.9807), Xent 0.0446, Loss 1.0022, Error 0.0070(best: 0.0070)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 18620 | Time 14.6000(14.6566) | Bit/dim 0.9801(0.9811) | Xent 0.0031(0.0003) | Loss 0.9816(0.9812) | Error 0.0011(0.0001) Steps 722(724.89) | Grad Norm 1.1433(1.9786) | Total Time 10.00(10.00)\n",
      "Iter 18630 | Time 14.5188(14.6511) | Bit/dim 0.9781(0.9826) | Xent 0.0001(0.0003) | Loss 0.9781(0.9827) | Error 0.0000(0.0001) Steps 704(724.56) | Grad Norm 0.9087(1.7355) | Total Time 10.00(10.00)\n",
      "Iter 18640 | Time 14.0176(14.5743) | Bit/dim 0.9790(0.9823) | Xent 0.0000(0.0003) | Loss 0.9790(0.9824) | Error 0.0000(0.0001) Steps 722(723.27) | Grad Norm 5.6778(2.6979) | Total Time 10.00(10.00)\n",
      "Iter 18650 | Time 14.0703(14.5528) | Bit/dim 0.9900(0.9818) | Xent 0.0005(0.0003) | Loss 0.9902(0.9819) | Error 0.0000(0.0001) Steps 722(724.31) | Grad Norm 0.8929(3.3863) | Total Time 10.00(10.00)\n",
      "Iter 18660 | Time 14.9913(14.5789) | Bit/dim 0.9794(0.9816) | Xent 0.0001(0.0002) | Loss 0.9795(0.9817) | Error 0.0000(0.0001) Steps 716(723.99) | Grad Norm 2.0039(3.2351) | Total Time 10.00(10.00)\n",
      "Iter 18670 | Time 14.7655(14.5967) | Bit/dim 0.9776(0.9816) | Xent 0.0002(0.0002) | Loss 0.9777(0.9817) | Error 0.0000(0.0000) Steps 722(723.89) | Grad Norm 0.2603(2.7385) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0283 | Time 57.4198, Epoch Time 1032.8032(1018.6541), Bit/dim 0.9812(best: 0.9799), Xent 0.0439, Loss 1.0032, Error 0.0076(best: 0.0070)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 18680 | Time 14.4645(14.5528) | Bit/dim 0.9726(0.9818) | Xent 0.0002(0.0002) | Loss 0.9727(0.9819) | Error 0.0000(0.0001) Steps 728(723.01) | Grad Norm 3.4186(3.1746) | Total Time 10.00(10.00)\n",
      "Iter 18690 | Time 14.1104(14.5760) | Bit/dim 0.9799(0.9825) | Xent 0.0000(0.0002) | Loss 0.9799(0.9826) | Error 0.0000(0.0000) Steps 728(722.44) | Grad Norm 3.0565(3.0044) | Total Time 10.00(10.00)\n",
      "Iter 18700 | Time 14.4408(14.6079) | Bit/dim 0.9781(0.9817) | Xent 0.0000(0.0002) | Loss 0.9781(0.9817) | Error 0.0000(0.0000) Steps 728(720.52) | Grad Norm 3.6164(2.6772) | Total Time 10.00(10.00)\n",
      "Iter 18710 | Time 13.9312(14.6188) | Bit/dim 0.9858(0.9823) | Xent 0.0003(0.0001) | Loss 0.9860(0.9824) | Error 0.0000(0.0000) Steps 728(722.41) | Grad Norm 0.4983(2.3873) | Total Time 10.00(10.00)\n",
      "Iter 18720 | Time 14.6036(14.6385) | Bit/dim 0.9845(0.9828) | Xent 0.0014(0.0002) | Loss 0.9852(0.9829) | Error 0.0011(0.0000) Steps 734(721.86) | Grad Norm 2.2025(2.1302) | Total Time 10.00(10.00)\n",
      "Iter 18730 | Time 14.8358(14.6227) | Bit/dim 0.9732(0.9820) | Xent 0.0001(0.0002) | Loss 0.9733(0.9821) | Error 0.0000(0.0000) Steps 740(721.81) | Grad Norm 0.3442(2.0935) | Total Time 10.00(10.00)\n",
      "Iter 18740 | Time 14.8138(14.6706) | Bit/dim 1.0048(0.9823) | Xent 0.0000(0.0002) | Loss 1.0048(0.9823) | Error 0.0000(0.0000) Steps 734(723.91) | Grad Norm 4.8789(2.3130) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0284 | Time 57.0324, Epoch Time 1040.8575(1019.3202), Bit/dim 0.9804(best: 0.9799), Xent 0.0403, Loss 1.0006, Error 0.0075(best: 0.0070)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 18750 | Time 14.7414(14.6313) | Bit/dim 1.0013(0.9822) | Xent 0.0000(0.0001) | Loss 1.0013(0.9822) | Error 0.0000(0.0000) Steps 716(723.07) | Grad Norm 1.1372(2.4800) | Total Time 10.00(10.00)\n",
      "Iter 18760 | Time 14.3016(14.6371) | Bit/dim 0.9882(0.9826) | Xent 0.0001(0.0001) | Loss 0.9882(0.9826) | Error 0.0000(0.0000) Steps 728(723.48) | Grad Norm 0.3265(2.2001) | Total Time 10.00(10.00)\n",
      "Iter 18770 | Time 14.2858(14.6273) | Bit/dim 0.9708(0.9821) | Xent 0.0000(0.0002) | Loss 0.9708(0.9822) | Error 0.0000(0.0000) Steps 728(723.61) | Grad Norm 6.6047(2.6786) | Total Time 10.00(10.00)\n",
      "Iter 18780 | Time 14.4241(14.6361) | Bit/dim 0.9727(0.9822) | Xent 0.0000(0.0002) | Loss 0.9727(0.9823) | Error 0.0000(0.0000) Steps 722(723.59) | Grad Norm 2.4872(2.7117) | Total Time 10.00(10.00)\n",
      "Iter 18790 | Time 14.5166(14.6572) | Bit/dim 0.9689(0.9806) | Xent 0.0000(0.0001) | Loss 0.9689(0.9807) | Error 0.0000(0.0000) Steps 722(724.15) | Grad Norm 0.4361(2.6390) | Total Time 10.00(10.00)\n",
      "Iter 18800 | Time 14.4612(14.6602) | Bit/dim 0.9714(0.9814) | Xent 0.0000(0.0001) | Loss 0.9714(0.9815) | Error 0.0000(0.0000) Steps 716(722.54) | Grad Norm 0.6546(2.3517) | Total Time 10.00(10.00)\n",
      "Iter 18810 | Time 14.6376(14.5921) | Bit/dim 0.9942(0.9818) | Xent 0.0000(0.0003) | Loss 0.9942(0.9819) | Error 0.0000(0.0001) Steps 704(720.65) | Grad Norm 4.2959(2.2600) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0285 | Time 57.0334, Epoch Time 1035.7850(1019.8141), Bit/dim 0.9799(best: 0.9799), Xent 0.0434, Loss 1.0016, Error 0.0075(best: 0.0070)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 18820 | Time 14.5146(14.5349) | Bit/dim 0.9782(0.9814) | Xent 0.0004(0.0002) | Loss 0.9784(0.9816) | Error 0.0000(0.0001) Steps 734(721.03) | Grad Norm 3.2467(2.7618) | Total Time 10.00(10.00)\n",
      "Iter 18830 | Time 14.6478(14.5273) | Bit/dim 0.9816(0.9830) | Xent 0.0000(0.0002) | Loss 0.9816(0.9831) | Error 0.0000(0.0000) Steps 728(721.11) | Grad Norm 4.5411(3.7628) | Total Time 10.00(10.00)\n",
      "Iter 18840 | Time 14.6606(14.5704) | Bit/dim 0.9847(0.9823) | Xent 0.0002(0.0002) | Loss 0.9848(0.9824) | Error 0.0000(0.0000) Steps 716(723.00) | Grad Norm 5.8747(3.7969) | Total Time 10.00(10.00)\n",
      "Iter 18850 | Time 14.7027(14.5996) | Bit/dim 0.9811(0.9816) | Xent 0.0000(0.0003) | Loss 0.9812(0.9817) | Error 0.0000(0.0000) Steps 728(724.20) | Grad Norm 5.6555(3.5436) | Total Time 10.00(10.00)\n",
      "Iter 18860 | Time 14.7361(14.5908) | Bit/dim 0.9865(0.9811) | Xent 0.0000(0.0003) | Loss 0.9865(0.9812) | Error 0.0000(0.0001) Steps 734(724.71) | Grad Norm 1.1172(3.4334) | Total Time 10.00(10.00)\n",
      "Iter 18870 | Time 14.4501(14.5903) | Bit/dim 0.9850(0.9816) | Xent 0.0001(0.0003) | Loss 0.9851(0.9817) | Error 0.0000(0.0001) Steps 728(723.36) | Grad Norm 4.4993(3.2772) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0286 | Time 58.4108, Epoch Time 1037.0843(1020.3322), Bit/dim 0.9805(best: 0.9799), Xent 0.0369, Loss 0.9989, Error 0.0081(best: 0.0070)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 18880 | Time 14.5986(14.6348) | Bit/dim 0.9689(0.9809) | Xent 0.0001(0.0002) | Loss 0.9690(0.9810) | Error 0.0000(0.0001) Steps 734(724.43) | Grad Norm 2.7576(3.1703) | Total Time 10.00(10.00)\n",
      "Iter 18890 | Time 14.9645(14.6539) | Bit/dim 0.9758(0.9797) | Xent 0.0001(0.0002) | Loss 0.9758(0.9798) | Error 0.0000(0.0000) Steps 740(723.69) | Grad Norm 1.2941(2.9633) | Total Time 10.00(10.00)\n",
      "Iter 18900 | Time 14.4331(14.6295) | Bit/dim 0.9775(0.9802) | Xent 0.0002(0.0002) | Loss 0.9776(0.9803) | Error 0.0000(0.0000) Steps 698(723.76) | Grad Norm 3.4364(2.9037) | Total Time 10.00(10.00)\n",
      "Iter 18910 | Time 14.6073(14.6050) | Bit/dim 0.9791(0.9810) | Xent 0.0000(0.0003) | Loss 0.9791(0.9811) | Error 0.0000(0.0001) Steps 704(724.08) | Grad Norm 0.5640(2.7282) | Total Time 10.00(10.00)\n",
      "Iter 18920 | Time 14.4831(14.5689) | Bit/dim 0.9840(0.9814) | Xent 0.0000(0.0003) | Loss 0.9840(0.9816) | Error 0.0000(0.0001) Steps 722(722.13) | Grad Norm 10.9859(3.9518) | Total Time 10.00(10.00)\n",
      "Iter 18930 | Time 14.6294(14.5838) | Bit/dim 0.9839(0.9820) | Xent 0.0001(0.0003) | Loss 0.9839(0.9822) | Error 0.0000(0.0001) Steps 722(723.02) | Grad Norm 8.4659(4.2691) | Total Time 10.00(10.00)\n",
      "Iter 18940 | Time 14.4083(14.5709) | Bit/dim 0.9893(0.9820) | Xent 0.0003(0.0003) | Loss 0.9894(0.9822) | Error 0.0000(0.0001) Steps 716(722.17) | Grad Norm 12.3435(4.4870) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0287 | Time 57.3404, Epoch Time 1033.6772(1020.7326), Bit/dim 0.9812(best: 0.9799), Xent 0.0440, Loss 1.0032, Error 0.0081(best: 0.0070)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 18950 | Time 14.7226(14.5693) | Bit/dim 0.9941(0.9825) | Xent 0.0004(0.0003) | Loss 0.9943(0.9826) | Error 0.0000(0.0002) Steps 734(721.05) | Grad Norm 6.6562(4.5791) | Total Time 10.00(10.00)\n",
      "Iter 18960 | Time 14.7425(14.5224) | Bit/dim 0.9813(0.9830) | Xent 0.0000(0.0003) | Loss 0.9814(0.9832) | Error 0.0000(0.0001) Steps 746(723.23) | Grad Norm 2.8206(4.1327) | Total Time 10.00(10.00)\n",
      "Iter 18970 | Time 14.2262(14.5415) | Bit/dim 0.9685(0.9826) | Xent 0.0003(0.0002) | Loss 0.9686(0.9827) | Error 0.0000(0.0001) Steps 740(724.67) | Grad Norm 4.6327(3.7224) | Total Time 10.00(10.00)\n",
      "Iter 18980 | Time 14.3715(14.5484) | Bit/dim 0.9998(0.9815) | Xent 0.0008(0.0003) | Loss 1.0001(0.9817) | Error 0.0000(0.0001) Steps 734(724.88) | Grad Norm 3.9815(3.3814) | Total Time 10.00(10.00)\n",
      "Iter 18990 | Time 14.3241(14.5485) | Bit/dim 0.9783(0.9811) | Xent 0.0000(0.0003) | Loss 0.9783(0.9812) | Error 0.0000(0.0001) Steps 740(723.98) | Grad Norm 5.7578(3.4636) | Total Time 10.00(10.00)\n",
      "Iter 19000 | Time 14.2254(14.5474) | Bit/dim 0.9867(0.9808) | Xent 0.0002(0.0003) | Loss 0.9868(0.9809) | Error 0.0000(0.0001) Steps 716(723.19) | Grad Norm 4.7420(3.9963) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0288 | Time 57.3873, Epoch Time 1032.9842(1021.1001), Bit/dim 0.9801(best: 0.9799), Xent 0.0394, Loss 0.9997, Error 0.0077(best: 0.0070)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 19010 | Time 14.5225(14.5631) | Bit/dim 0.9727(0.9816) | Xent 0.0000(0.0003) | Loss 0.9728(0.9817) | Error 0.0000(0.0001) Steps 740(724.61) | Grad Norm 9.3212(4.2091) | Total Time 10.00(10.00)\n",
      "Iter 19020 | Time 14.8870(14.6256) | Bit/dim 0.9896(0.9816) | Xent 0.0000(0.0003) | Loss 0.9896(0.9818) | Error 0.0000(0.0001) Steps 734(727.03) | Grad Norm 7.7665(4.4376) | Total Time 10.00(10.00)\n",
      "Iter 19030 | Time 14.5941(14.6237) | Bit/dim 0.9847(0.9820) | Xent 0.0001(0.0002) | Loss 0.9847(0.9821) | Error 0.0000(0.0001) Steps 728(725.22) | Grad Norm 3.2474(4.2616) | Total Time 10.00(10.00)\n",
      "Iter 19040 | Time 14.6298(14.6618) | Bit/dim 0.9887(0.9823) | Xent 0.0000(0.0002) | Loss 0.9887(0.9824) | Error 0.0000(0.0001) Steps 734(724.43) | Grad Norm 0.3694(4.0252) | Total Time 10.00(10.00)\n",
      "Iter 19050 | Time 14.4226(14.6776) | Bit/dim 0.9822(0.9820) | Xent 0.0000(0.0002) | Loss 0.9822(0.9821) | Error 0.0000(0.0001) Steps 728(725.81) | Grad Norm 0.3228(3.5121) | Total Time 10.00(10.00)\n",
      "Iter 19060 | Time 15.0306(14.6699) | Bit/dim 0.9798(0.9805) | Xent 0.0004(0.0002) | Loss 0.9800(0.9806) | Error 0.0000(0.0000) Steps 716(726.37) | Grad Norm 0.4360(2.7628) | Total Time 10.00(10.00)\n",
      "Iter 19070 | Time 14.7238(14.6858) | Bit/dim 0.9828(0.9806) | Xent 0.0001(0.0001) | Loss 0.9828(0.9806) | Error 0.0000(0.0000) Steps 734(726.58) | Grad Norm 0.8303(2.2952) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0289 | Time 57.4569, Epoch Time 1045.5881(1021.8348), Bit/dim 0.9796(best: 0.9799), Xent 0.0418, Loss 1.0005, Error 0.0080(best: 0.0070)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 19080 | Time 14.3667(14.7260) | Bit/dim 0.9610(0.9803) | Xent 0.0001(0.0001) | Loss 0.9610(0.9804) | Error 0.0000(0.0000) Steps 722(726.39) | Grad Norm 1.2447(1.9648) | Total Time 10.00(10.00)\n",
      "Iter 19090 | Time 15.0366(14.7263) | Bit/dim 0.9849(0.9812) | Xent 0.0000(0.0001) | Loss 0.9850(0.9813) | Error 0.0000(0.0000) Steps 740(728.28) | Grad Norm 0.5327(1.6223) | Total Time 10.00(10.00)\n",
      "Iter 19100 | Time 15.0812(14.7515) | Bit/dim 0.9866(0.9814) | Xent 0.0002(0.0001) | Loss 0.9867(0.9814) | Error 0.0000(0.0000) Steps 728(729.14) | Grad Norm 0.5683(1.4626) | Total Time 10.00(10.00)\n",
      "Iter 19110 | Time 14.6288(14.7074) | Bit/dim 0.9691(0.9795) | Xent 0.0000(0.0001) | Loss 0.9691(0.9795) | Error 0.0000(0.0000) Steps 710(727.25) | Grad Norm 2.3442(1.6698) | Total Time 10.00(10.00)\n",
      "Iter 19120 | Time 15.1798(14.6860) | Bit/dim 0.9860(0.9800) | Xent 0.0000(0.0002) | Loss 0.9860(0.9801) | Error 0.0000(0.0001) Steps 704(727.81) | Grad Norm 7.4972(2.3260) | Total Time 10.00(10.00)\n",
      "Iter 19130 | Time 14.2818(14.6442) | Bit/dim 0.9774(0.9804) | Xent 0.0000(0.0002) | Loss 0.9774(0.9805) | Error 0.0000(0.0001) Steps 704(725.87) | Grad Norm 3.1335(3.3028) | Total Time 10.00(10.00)\n",
      "Iter 19140 | Time 14.1526(14.6495) | Bit/dim 0.9853(0.9813) | Xent 0.0000(0.0002) | Loss 0.9853(0.9814) | Error 0.0000(0.0001) Steps 728(726.10) | Grad Norm 0.8128(2.9580) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0290 | Time 57.7638, Epoch Time 1040.2708(1022.3878), Bit/dim 0.9797(best: 0.9796), Xent 0.0429, Loss 1.0011, Error 0.0082(best: 0.0070)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 19150 | Time 14.9878(14.6623) | Bit/dim 0.9882(0.9804) | Xent 0.0001(0.0002) | Loss 0.9882(0.9805) | Error 0.0000(0.0000) Steps 734(727.64) | Grad Norm 1.4355(2.3968) | Total Time 10.00(10.00)\n",
      "Iter 19160 | Time 13.9050(14.6475) | Bit/dim 0.9822(0.9809) | Xent 0.0002(0.0003) | Loss 0.9823(0.9810) | Error 0.0000(0.0001) Steps 722(727.51) | Grad Norm 4.6612(2.5014) | Total Time 10.00(10.00)\n",
      "Iter 19170 | Time 14.8196(14.7246) | Bit/dim 0.9893(0.9819) | Xent 0.0000(0.0003) | Loss 0.9893(0.9821) | Error 0.0000(0.0001) Steps 728(726.33) | Grad Norm 4.4979(3.3650) | Total Time 10.00(10.00)\n",
      "Iter 19180 | Time 14.7087(14.6644) | Bit/dim 0.9698(0.9815) | Xent 0.0009(0.0003) | Loss 0.9703(0.9816) | Error 0.0011(0.0001) Steps 734(727.29) | Grad Norm 1.0630(2.9964) | Total Time 10.00(10.00)\n",
      "Iter 19190 | Time 14.8362(14.6344) | Bit/dim 0.9775(0.9811) | Xent 0.0000(0.0003) | Loss 0.9776(0.9813) | Error 0.0000(0.0001) Steps 716(725.12) | Grad Norm 2.7834(2.6288) | Total Time 10.00(10.00)\n",
      "Iter 19200 | Time 14.3376(14.6165) | Bit/dim 0.9721(0.9803) | Xent 0.0000(0.0002) | Loss 0.9721(0.9805) | Error 0.0000(0.0001) Steps 740(725.40) | Grad Norm 1.7132(2.2963) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0291 | Time 58.1496, Epoch Time 1038.4855(1022.8708), Bit/dim 0.9796(best: 0.9796), Xent 0.0420, Loss 1.0006, Error 0.0091(best: 0.0070)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 19210 | Time 14.6717(14.5841) | Bit/dim 0.9894(0.9807) | Xent 0.0000(0.0003) | Loss 0.9894(0.9808) | Error 0.0000(0.0001) Steps 740(724.40) | Grad Norm 1.9254(2.0216) | Total Time 10.00(10.00)\n",
      "Iter 19220 | Time 15.1591(14.6327) | Bit/dim 0.9840(0.9801) | Xent 0.0001(0.0002) | Loss 0.9841(0.9802) | Error 0.0000(0.0001) Steps 740(726.46) | Grad Norm 1.1919(1.8873) | Total Time 10.00(10.00)\n",
      "Iter 19230 | Time 14.4697(14.5879) | Bit/dim 0.9654(0.9803) | Xent 0.0000(0.0002) | Loss 0.9655(0.9804) | Error 0.0000(0.0001) Steps 722(725.90) | Grad Norm 1.3243(1.9197) | Total Time 10.00(10.00)\n",
      "Iter 19240 | Time 14.8878(14.5929) | Bit/dim 0.9934(0.9801) | Xent 0.0001(0.0002) | Loss 0.9934(0.9802) | Error 0.0000(0.0001) Steps 722(724.87) | Grad Norm 2.5200(1.9840) | Total Time 10.00(10.00)\n",
      "Iter 19250 | Time 15.2205(14.6076) | Bit/dim 0.9759(0.9806) | Xent 0.0030(0.0004) | Loss 0.9774(0.9808) | Error 0.0011(0.0002) Steps 722(724.70) | Grad Norm 7.7551(3.6781) | Total Time 10.00(10.00)\n",
      "Iter 19260 | Time 14.6834(14.5771) | Bit/dim 0.9679(0.9804) | Xent 0.0001(0.0004) | Loss 0.9679(0.9806) | Error 0.0000(0.0002) Steps 734(724.95) | Grad Norm 2.9826(3.4388) | Total Time 10.00(10.00)\n",
      "Iter 19270 | Time 14.2481(14.5489) | Bit/dim 0.9948(0.9810) | Xent 0.0000(0.0004) | Loss 0.9949(0.9812) | Error 0.0000(0.0002) Steps 734(725.81) | Grad Norm 3.2320(3.5380) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0292 | Time 57.3278, Epoch Time 1034.0275(1023.2055), Bit/dim 0.9794(best: 0.9796), Xent 0.0407, Loss 0.9997, Error 0.0089(best: 0.0070)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 19280 | Time 14.9827(14.5979) | Bit/dim 0.9866(0.9816) | Xent 0.0000(0.0003) | Loss 0.9866(0.9818) | Error 0.0000(0.0001) Steps 752(725.60) | Grad Norm 1.2850(3.0970) | Total Time 10.00(10.00)\n",
      "Iter 19290 | Time 13.8949(14.5530) | Bit/dim 0.9848(0.9806) | Xent 0.0000(0.0003) | Loss 0.9848(0.9807) | Error 0.0000(0.0001) Steps 734(726.15) | Grad Norm 1.1579(2.6474) | Total Time 10.00(10.00)\n",
      "Iter 19300 | Time 15.0621(14.5883) | Bit/dim 0.9860(0.9813) | Xent 0.0003(0.0002) | Loss 0.9862(0.9814) | Error 0.0000(0.0001) Steps 728(726.03) | Grad Norm 2.9503(2.2611) | Total Time 10.00(10.00)\n",
      "Iter 19310 | Time 14.1441(14.5723) | Bit/dim 0.9781(0.9816) | Xent 0.0001(0.0002) | Loss 0.9781(0.9817) | Error 0.0000(0.0000) Steps 734(726.00) | Grad Norm 2.9927(2.3177) | Total Time 10.00(10.00)\n",
      "Iter 19320 | Time 14.4659(14.6011) | Bit/dim 0.9712(0.9813) | Xent 0.0001(0.0002) | Loss 0.9712(0.9814) | Error 0.0000(0.0000) Steps 734(725.97) | Grad Norm 1.0429(1.9726) | Total Time 10.00(10.00)\n",
      "Iter 19330 | Time 15.0516(14.6227) | Bit/dim 0.9813(0.9806) | Xent 0.0000(0.0002) | Loss 0.9813(0.9807) | Error 0.0000(0.0000) Steps 716(724.36) | Grad Norm 3.7889(1.8824) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0293 | Time 58.9063, Epoch Time 1039.3485(1023.6898), Bit/dim 0.9794(best: 0.9794), Xent 0.0397, Loss 0.9993, Error 0.0077(best: 0.0070)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 19340 | Time 13.8513(14.6203) | Bit/dim 0.9840(0.9797) | Xent 0.0000(0.0002) | Loss 0.9840(0.9798) | Error 0.0000(0.0001) Steps 734(723.48) | Grad Norm 6.7569(2.4002) | Total Time 10.00(10.00)\n",
      "Iter 19350 | Time 14.6023(14.6170) | Bit/dim 0.9769(0.9802) | Xent 0.0000(0.0002) | Loss 0.9769(0.9803) | Error 0.0000(0.0001) Steps 722(722.45) | Grad Norm 6.1558(3.8807) | Total Time 10.00(10.00)\n",
      "Iter 19360 | Time 15.0199(14.6134) | Bit/dim 0.9653(0.9808) | Xent 0.0000(0.0002) | Loss 0.9653(0.9809) | Error 0.0000(0.0001) Steps 734(722.70) | Grad Norm 4.2819(4.3027) | Total Time 10.00(10.00)\n",
      "Iter 19370 | Time 14.5862(14.6087) | Bit/dim 0.9778(0.9798) | Xent 0.0000(0.0002) | Loss 0.9778(0.9798) | Error 0.0000(0.0000) Steps 722(721.78) | Grad Norm 3.2897(4.1920) | Total Time 10.00(10.00)\n",
      "Iter 19380 | Time 14.4393(14.6435) | Bit/dim 0.9950(0.9804) | Xent 0.0003(0.0002) | Loss 0.9951(0.9805) | Error 0.0000(0.0000) Steps 722(722.41) | Grad Norm 3.8580(3.9398) | Total Time 10.00(10.00)\n",
      "Iter 19390 | Time 14.8404(14.6255) | Bit/dim 0.9679(0.9798) | Xent 0.0001(0.0002) | Loss 0.9679(0.9799) | Error 0.0000(0.0001) Steps 722(721.80) | Grad Norm 0.4730(3.5131) | Total Time 10.00(10.00)\n",
      "Iter 19400 | Time 14.6737(14.6217) | Bit/dim 0.9907(0.9800) | Xent 0.0000(0.0002) | Loss 0.9907(0.9801) | Error 0.0000(0.0000) Steps 740(721.83) | Grad Norm 1.2418(2.8727) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0294 | Time 58.4412, Epoch Time 1038.3668(1024.1301), Bit/dim 0.9794(best: 0.9794), Xent 0.0399, Loss 0.9993, Error 0.0076(best: 0.0070)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 19410 | Time 14.5637(14.6421) | Bit/dim 0.9756(0.9806) | Xent 0.0000(0.0002) | Loss 0.9756(0.9807) | Error 0.0000(0.0001) Steps 722(721.98) | Grad Norm 1.9182(2.6274) | Total Time 10.00(10.00)\n",
      "Iter 19420 | Time 14.6296(14.6333) | Bit/dim 0.9744(0.9795) | Xent 0.0000(0.0002) | Loss 0.9744(0.9796) | Error 0.0000(0.0000) Steps 734(723.84) | Grad Norm 3.6081(2.4699) | Total Time 10.00(10.00)\n",
      "Iter 19430 | Time 14.4641(14.5726) | Bit/dim 0.9785(0.9808) | Xent 0.0000(0.0002) | Loss 0.9785(0.9809) | Error 0.0000(0.0000) Steps 740(723.86) | Grad Norm 3.4849(2.9534) | Total Time 10.00(10.00)\n",
      "Iter 19440 | Time 14.1807(14.5363) | Bit/dim 0.9916(0.9803) | Xent 0.0001(0.0002) | Loss 0.9916(0.9804) | Error 0.0000(0.0001) Steps 728(722.35) | Grad Norm 3.9892(4.3074) | Total Time 10.00(10.00)\n",
      "Iter 19450 | Time 14.7177(14.5299) | Bit/dim 0.9876(0.9814) | Xent 0.0000(0.0001) | Loss 0.9876(0.9814) | Error 0.0000(0.0000) Steps 746(722.41) | Grad Norm 7.8541(5.4221) | Total Time 10.00(10.00)\n",
      "Iter 19460 | Time 14.2842(14.5201) | Bit/dim 0.9799(0.9811) | Xent 0.0001(0.0002) | Loss 0.9800(0.9811) | Error 0.0000(0.0000) Steps 728(722.55) | Grad Norm 2.6515(5.1681) | Total Time 10.00(10.00)\n",
      "Iter 19470 | Time 14.4299(14.5542) | Bit/dim 0.9748(0.9809) | Xent 0.0002(0.0001) | Loss 0.9749(0.9809) | Error 0.0000(0.0000) Steps 722(722.33) | Grad Norm 0.6909(4.1607) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0295 | Time 58.2418, Epoch Time 1032.8762(1024.3925), Bit/dim 0.9787(best: 0.9794), Xent 0.0360, Loss 0.9967, Error 0.0069(best: 0.0070)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 19480 | Time 14.5431(14.5889) | Bit/dim 0.9777(0.9803) | Xent 0.0001(0.0002) | Loss 0.9778(0.9804) | Error 0.0000(0.0000) Steps 716(722.61) | Grad Norm 1.3874(3.3746) | Total Time 10.00(10.00)\n",
      "Iter 19490 | Time 14.9810(14.6236) | Bit/dim 0.9801(0.9794) | Xent 0.0024(0.0002) | Loss 0.9813(0.9795) | Error 0.0011(0.0001) Steps 728(722.86) | Grad Norm 2.2991(3.1372) | Total Time 10.00(10.00)\n",
      "Iter 19500 | Time 14.4597(14.5925) | Bit/dim 0.9781(0.9798) | Xent 0.0001(0.0002) | Loss 0.9781(0.9799) | Error 0.0000(0.0000) Steps 722(722.98) | Grad Norm 1.5121(3.0680) | Total Time 10.00(10.00)\n",
      "Iter 19510 | Time 15.2550(14.5953) | Bit/dim 0.9929(0.9805) | Xent 0.0000(0.0002) | Loss 0.9930(0.9806) | Error 0.0000(0.0001) Steps 716(723.32) | Grad Norm 3.2708(3.0534) | Total Time 10.00(10.00)\n",
      "Iter 19520 | Time 14.4811(14.6278) | Bit/dim 0.9775(0.9805) | Xent 0.0027(0.0002) | Loss 0.9789(0.9806) | Error 0.0011(0.0001) Steps 722(723.45) | Grad Norm 6.0681(2.9890) | Total Time 10.00(10.00)\n",
      "Iter 19530 | Time 14.3451(14.6190) | Bit/dim 0.9826(0.9799) | Xent 0.0011(0.0003) | Loss 0.9831(0.9801) | Error 0.0011(0.0001) Steps 728(724.90) | Grad Norm 6.2269(3.0237) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0296 | Time 57.5975, Epoch Time 1036.6561(1024.7604), Bit/dim 0.9792(best: 0.9787), Xent 0.0396, Loss 0.9990, Error 0.0072(best: 0.0069)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 19540 | Time 14.3527(14.5512) | Bit/dim 0.9672(0.9791) | Xent 0.0002(0.0003) | Loss 0.9673(0.9792) | Error 0.0000(0.0001) Steps 722(722.74) | Grad Norm 1.2428(3.4198) | Total Time 10.00(10.00)\n",
      "Iter 19550 | Time 14.5321(14.5705) | Bit/dim 0.9869(0.9797) | Xent 0.0000(0.0003) | Loss 0.9869(0.9799) | Error 0.0000(0.0001) Steps 734(722.46) | Grad Norm 5.6916(3.8407) | Total Time 10.00(10.00)\n",
      "Iter 19560 | Time 14.1102(14.5558) | Bit/dim 0.9826(0.9791) | Xent 0.0000(0.0002) | Loss 0.9826(0.9793) | Error 0.0000(0.0001) Steps 722(721.89) | Grad Norm 5.3704(4.4661) | Total Time 10.00(10.00)\n",
      "Iter 19570 | Time 14.8041(14.5848) | Bit/dim 0.9633(0.9790) | Xent 0.0000(0.0003) | Loss 0.9633(0.9791) | Error 0.0000(0.0001) Steps 710(721.90) | Grad Norm 3.7852(4.4784) | Total Time 10.00(10.00)\n",
      "Iter 19580 | Time 14.6264(14.6035) | Bit/dim 0.9848(0.9791) | Xent 0.0014(0.0003) | Loss 0.9855(0.9793) | Error 0.0011(0.0001) Steps 704(723.32) | Grad Norm 2.4603(4.2062) | Total Time 10.00(10.00)\n",
      "Iter 19590 | Time 15.6385(14.6942) | Bit/dim 0.9791(0.9804) | Xent 0.0002(0.0003) | Loss 0.9792(0.9805) | Error 0.0000(0.0001) Steps 716(725.26) | Grad Norm 2.8086(3.9508) | Total Time 10.00(10.00)\n",
      "Iter 19600 | Time 14.2279(14.6745) | Bit/dim 0.9899(0.9811) | Xent 0.0002(0.0002) | Loss 0.9900(0.9813) | Error 0.0000(0.0001) Steps 716(726.35) | Grad Norm 4.9396(3.6751) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0297 | Time 58.5781, Epoch Time 1040.9878(1025.2472), Bit/dim 0.9795(best: 0.9787), Xent 0.0414, Loss 1.0002, Error 0.0074(best: 0.0069)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 19610 | Time 14.5244(14.6585) | Bit/dim 0.9738(0.9805) | Xent 0.0001(0.0002) | Loss 0.9739(0.9806) | Error 0.0000(0.0001) Steps 704(725.34) | Grad Norm 5.3958(3.7175) | Total Time 10.00(10.00)\n",
      "Iter 19620 | Time 14.3066(14.6413) | Bit/dim 0.9815(0.9799) | Xent 0.0000(0.0002) | Loss 0.9815(0.9800) | Error 0.0000(0.0001) Steps 734(725.20) | Grad Norm 4.8811(3.7911) | Total Time 10.00(10.00)\n",
      "Iter 19630 | Time 14.6718(14.6649) | Bit/dim 0.9755(0.9799) | Xent 0.0000(0.0002) | Loss 0.9756(0.9800) | Error 0.0000(0.0000) Steps 704(722.56) | Grad Norm 5.1103(3.7721) | Total Time 10.00(10.00)\n",
      "Iter 19640 | Time 13.9665(14.5873) | Bit/dim 0.9875(0.9799) | Xent 0.0004(0.0002) | Loss 0.9877(0.9800) | Error 0.0000(0.0000) Steps 716(722.21) | Grad Norm 5.8492(4.4202) | Total Time 10.00(10.00)\n",
      "Iter 19650 | Time 14.7595(14.5743) | Bit/dim 0.9881(0.9804) | Xent 0.0001(0.0001) | Loss 0.9882(0.9805) | Error 0.0000(0.0000) Steps 728(720.94) | Grad Norm 6.1905(4.8424) | Total Time 10.00(10.00)\n",
      "Iter 19660 | Time 14.3403(14.5927) | Bit/dim 0.9843(0.9808) | Xent 0.0000(0.0001) | Loss 0.9843(0.9809) | Error 0.0000(0.0000) Steps 728(723.21) | Grad Norm 1.9305(4.7132) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0298 | Time 57.4855, Epoch Time 1034.4302(1025.5227), Bit/dim 0.9785(best: 0.9787), Xent 0.0384, Loss 0.9977, Error 0.0067(best: 0.0069)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 19670 | Time 14.5247(14.5748) | Bit/dim 0.9949(0.9806) | Xent 0.0000(0.0001) | Loss 0.9949(0.9806) | Error 0.0000(0.0000) Steps 722(723.28) | Grad Norm 2.8307(4.7283) | Total Time 10.00(10.00)\n",
      "Iter 19680 | Time 15.2553(14.6114) | Bit/dim 0.9733(0.9799) | Xent 0.0001(0.0002) | Loss 0.9733(0.9800) | Error 0.0000(0.0001) Steps 728(724.01) | Grad Norm 6.4988(4.4325) | Total Time 10.00(10.00)\n",
      "Iter 19690 | Time 14.3000(14.6151) | Bit/dim 1.0001(0.9800) | Xent 0.0000(0.0002) | Loss 1.0001(0.9801) | Error 0.0000(0.0001) Steps 710(723.33) | Grad Norm 10.0969(5.0439) | Total Time 10.00(10.00)\n",
      "Iter 19700 | Time 14.9862(14.5739) | Bit/dim 0.9746(0.9798) | Xent 0.0001(0.0001) | Loss 0.9747(0.9799) | Error 0.0000(0.0001) Steps 728(720.82) | Grad Norm 5.7776(5.0580) | Total Time 10.00(10.00)\n",
      "Iter 19710 | Time 15.0024(14.5640) | Bit/dim 0.9792(0.9800) | Xent 0.0000(0.0001) | Loss 0.9793(0.9801) | Error 0.0000(0.0000) Steps 734(721.05) | Grad Norm 1.4764(4.4531) | Total Time 10.00(10.00)\n",
      "Iter 19720 | Time 14.4025(14.5902) | Bit/dim 0.9805(0.9797) | Xent 0.0001(0.0001) | Loss 0.9805(0.9797) | Error 0.0000(0.0000) Steps 728(723.50) | Grad Norm 2.9505(4.1617) | Total Time 10.00(10.00)\n",
      "Iter 19730 | Time 14.8437(14.6074) | Bit/dim 0.9751(0.9794) | Xent 0.0001(0.0001) | Loss 0.9751(0.9795) | Error 0.0000(0.0000) Steps 722(723.89) | Grad Norm 2.3884(3.5873) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0299 | Time 58.0429, Epoch Time 1036.3712(1025.8481), Bit/dim 0.9780(best: 0.9785), Xent 0.0400, Loss 0.9980, Error 0.0073(best: 0.0067)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 19740 | Time 14.4538(14.5914) | Bit/dim 0.9887(0.9795) | Xent 0.0001(0.0001) | Loss 0.9887(0.9795) | Error 0.0000(0.0000) Steps 728(725.29) | Grad Norm 2.9756(3.2625) | Total Time 10.00(10.00)\n",
      "Iter 19750 | Time 14.4897(14.5735) | Bit/dim 0.9775(0.9789) | Xent 0.0000(0.0001) | Loss 0.9775(0.9789) | Error 0.0000(0.0000) Steps 728(725.54) | Grad Norm 0.5730(2.9231) | Total Time 10.00(10.00)\n",
      "Iter 19760 | Time 14.1501(14.5716) | Bit/dim 0.9770(0.9792) | Xent 0.0000(0.0001) | Loss 0.9770(0.9792) | Error 0.0000(0.0000) Steps 722(724.10) | Grad Norm 1.1895(2.6468) | Total Time 10.00(10.00)\n",
      "Iter 19770 | Time 14.7814(14.6144) | Bit/dim 0.9583(0.9780) | Xent 0.0000(0.0001) | Loss 0.9583(0.9780) | Error 0.0000(0.0000) Steps 734(722.80) | Grad Norm 0.6365(2.1883) | Total Time 10.00(10.00)\n",
      "Iter 19780 | Time 14.2473(14.5557) | Bit/dim 0.9772(0.9787) | Xent 0.0000(0.0001) | Loss 0.9772(0.9787) | Error 0.0000(0.0000) Steps 728(723.12) | Grad Norm 2.8119(2.0853) | Total Time 10.00(10.00)\n",
      "Iter 19790 | Time 14.8054(14.5606) | Bit/dim 0.9766(0.9799) | Xent 0.0001(0.0001) | Loss 0.9766(0.9799) | Error 0.0000(0.0000) Steps 728(724.12) | Grad Norm 3.0494(2.0877) | Total Time 10.00(10.00)\n",
      "Iter 19800 | Time 14.4235(14.5924) | Bit/dim 0.9793(0.9798) | Xent 0.0000(0.0002) | Loss 0.9793(0.9799) | Error 0.0000(0.0001) Steps 698(724.31) | Grad Norm 3.3151(2.0268) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0300 | Time 58.5944, Epoch Time 1035.9418(1026.1510), Bit/dim 0.9775(best: 0.9780), Xent 0.0404, Loss 0.9977, Error 0.0074(best: 0.0067)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 19810 | Time 14.5846(14.5947) | Bit/dim 0.9943(0.9808) | Xent 0.0000(0.0002) | Loss 0.9943(0.9809) | Error 0.0000(0.0001) Steps 722(724.84) | Grad Norm 0.7135(1.7809) | Total Time 10.00(10.00)\n",
      "Iter 19820 | Time 14.5576(14.5979) | Bit/dim 0.9872(0.9810) | Xent 0.0000(0.0002) | Loss 0.9872(0.9811) | Error 0.0000(0.0001) Steps 716(725.66) | Grad Norm 1.9232(1.6354) | Total Time 10.00(10.00)\n",
      "Iter 19830 | Time 14.6831(14.6007) | Bit/dim 0.9763(0.9804) | Xent 0.0000(0.0002) | Loss 0.9763(0.9805) | Error 0.0000(0.0001) Steps 728(725.34) | Grad Norm 2.2770(1.6549) | Total Time 10.00(10.00)\n",
      "Iter 19840 | Time 14.8023(14.6311) | Bit/dim 0.9870(0.9807) | Xent 0.0003(0.0002) | Loss 0.9871(0.9808) | Error 0.0000(0.0001) Steps 716(724.33) | Grad Norm 0.5797(1.5639) | Total Time 10.00(10.00)\n",
      "Iter 19850 | Time 14.3082(14.6511) | Bit/dim 0.9748(0.9797) | Xent 0.0000(0.0001) | Loss 0.9748(0.9798) | Error 0.0000(0.0000) Steps 740(724.42) | Grad Norm 1.0736(1.4177) | Total Time 10.00(10.00)\n",
      "Iter 19860 | Time 14.2037(14.6426) | Bit/dim 0.9855(0.9785) | Xent 0.0000(0.0001) | Loss 0.9855(0.9786) | Error 0.0000(0.0001) Steps 734(725.26) | Grad Norm 2.8426(1.5625) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0301 | Time 58.6839, Epoch Time 1041.6881(1026.6171), Bit/dim 0.9780(best: 0.9775), Xent 0.0415, Loss 0.9988, Error 0.0073(best: 0.0067)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 19870 | Time 14.5913(14.6777) | Bit/dim 0.9675(0.9782) | Xent 0.0001(0.0002) | Loss 0.9676(0.9783) | Error 0.0000(0.0001) Steps 734(724.98) | Grad Norm 0.5854(1.4628) | Total Time 10.00(10.00)\n",
      "Iter 19880 | Time 14.3534(14.6679) | Bit/dim 0.9772(0.9771) | Xent 0.0001(0.0003) | Loss 0.9772(0.9772) | Error 0.0000(0.0001) Steps 728(724.79) | Grad Norm 3.0674(1.4301) | Total Time 10.00(10.00)\n",
      "Iter 19890 | Time 13.9767(14.7088) | Bit/dim 0.9755(0.9780) | Xent 0.0000(0.0002) | Loss 0.9755(0.9781) | Error 0.0000(0.0001) Steps 710(723.26) | Grad Norm 4.5485(1.6114) | Total Time 10.00(10.00)\n",
      "Iter 19900 | Time 14.2798(14.6189) | Bit/dim 0.9822(0.9780) | Xent 0.0000(0.0002) | Loss 0.9822(0.9781) | Error 0.0000(0.0001) Steps 728(724.01) | Grad Norm 6.6490(2.5397) | Total Time 10.00(10.00)\n",
      "Iter 19910 | Time 14.0711(14.5658) | Bit/dim 0.9794(0.9800) | Xent 0.0000(0.0002) | Loss 0.9794(0.9801) | Error 0.0000(0.0001) Steps 716(722.37) | Grad Norm 6.4780(4.2697) | Total Time 10.00(10.00)\n",
      "Iter 19920 | Time 14.2364(14.5756) | Bit/dim 0.9628(0.9793) | Xent 0.0000(0.0001) | Loss 0.9628(0.9793) | Error 0.0000(0.0000) Steps 692(721.84) | Grad Norm 5.8606(4.8439) | Total Time 10.00(10.00)\n",
      "Iter 19930 | Time 14.3795(14.5780) | Bit/dim 0.9765(0.9790) | Xent 0.0000(0.0001) | Loss 0.9766(0.9791) | Error 0.0000(0.0000) Steps 692(719.15) | Grad Norm 5.8804(4.9793) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0302 | Time 58.2585, Epoch Time 1036.1932(1026.9043), Bit/dim 0.9783(best: 0.9775), Xent 0.0394, Loss 0.9980, Error 0.0073(best: 0.0067)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 19940 | Time 14.8263(14.5988) | Bit/dim 0.9638(0.9794) | Xent 0.0006(0.0001) | Loss 0.9641(0.9795) | Error 0.0000(0.0000) Steps 728(720.12) | Grad Norm 3.8651(4.8463) | Total Time 10.00(10.00)\n",
      "Iter 19950 | Time 14.8729(14.6112) | Bit/dim 0.9734(0.9790) | Xent 0.0000(0.0001) | Loss 0.9734(0.9791) | Error 0.0000(0.0000) Steps 692(721.74) | Grad Norm 0.9835(4.2671) | Total Time 10.00(10.00)\n",
      "Iter 19960 | Time 14.4332(14.5890) | Bit/dim 0.9696(0.9786) | Xent 0.0000(0.0001) | Loss 0.9696(0.9787) | Error 0.0000(0.0000) Steps 710(722.66) | Grad Norm 1.1770(3.7492) | Total Time 10.00(10.00)\n",
      "Iter 19970 | Time 14.5739(14.5961) | Bit/dim 0.9690(0.9789) | Xent 0.0000(0.0002) | Loss 0.9690(0.9790) | Error 0.0000(0.0001) Steps 740(722.61) | Grad Norm 0.6431(3.3610) | Total Time 10.00(10.00)\n",
      "Iter 19980 | Time 14.3923(14.5940) | Bit/dim 0.9818(0.9792) | Xent 0.0000(0.0001) | Loss 0.9818(0.9793) | Error 0.0000(0.0000) Steps 722(725.04) | Grad Norm 3.8840(3.2580) | Total Time 10.00(10.00)\n",
      "Iter 19990 | Time 14.6176(14.6254) | Bit/dim 0.9713(0.9793) | Xent 0.0001(0.0001) | Loss 0.9714(0.9794) | Error 0.0000(0.0001) Steps 716(724.48) | Grad Norm 4.2654(3.5300) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0303 | Time 58.1672, Epoch Time 1038.2363(1027.2443), Bit/dim 0.9774(best: 0.9775), Xent 0.0414, Loss 0.9980, Error 0.0075(best: 0.0067)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 20000 | Time 14.5648(14.6292) | Bit/dim 0.9862(0.9792) | Xent 0.0001(0.0001) | Loss 0.9863(0.9792) | Error 0.0000(0.0000) Steps 722(725.09) | Grad Norm 4.6515(3.8045) | Total Time 10.00(10.00)\n",
      "Iter 20010 | Time 14.6853(14.6450) | Bit/dim 0.9825(0.9794) | Xent 0.0000(0.0001) | Loss 0.9825(0.9795) | Error 0.0000(0.0001) Steps 734(725.67) | Grad Norm 3.8725(3.7895) | Total Time 10.00(10.00)\n",
      "Iter 20020 | Time 14.7475(14.6215) | Bit/dim 0.9652(0.9784) | Xent 0.0000(0.0002) | Loss 0.9652(0.9785) | Error 0.0000(0.0001) Steps 722(725.00) | Grad Norm 1.9276(3.5778) | Total Time 10.00(10.00)\n",
      "Iter 20030 | Time 14.7899(14.6684) | Bit/dim 0.9684(0.9785) | Xent 0.0000(0.0003) | Loss 0.9684(0.9786) | Error 0.0000(0.0001) Steps 710(724.60) | Grad Norm 2.8638(3.2494) | Total Time 10.00(10.00)\n",
      "Iter 20040 | Time 14.4600(14.6436) | Bit/dim 0.9873(0.9786) | Xent 0.0000(0.0003) | Loss 0.9873(0.9788) | Error 0.0000(0.0001) Steps 704(723.63) | Grad Norm 0.3765(2.6256) | Total Time 10.00(10.00)\n",
      "Iter 20050 | Time 14.4617(14.6737) | Bit/dim 0.9765(0.9786) | Xent 0.0004(0.0003) | Loss 0.9767(0.9787) | Error 0.0000(0.0002) Steps 716(721.61) | Grad Norm 2.7611(2.3691) | Total Time 10.00(10.00)\n",
      "Iter 20060 | Time 14.7501(14.6390) | Bit/dim 0.9768(0.9794) | Xent 0.0000(0.0003) | Loss 0.9768(0.9795) | Error 0.0000(0.0001) Steps 728(722.65) | Grad Norm 0.9573(2.1839) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0304 | Time 57.6325, Epoch Time 1040.0705(1027.6291), Bit/dim 0.9774(best: 0.9774), Xent 0.0414, Loss 0.9981, Error 0.0074(best: 0.0067)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 20070 | Time 14.8674(14.6656) | Bit/dim 0.9676(0.9791) | Xent 0.0005(0.0002) | Loss 0.9678(0.9792) | Error 0.0000(0.0001) Steps 728(725.45) | Grad Norm 1.2437(1.9985) | Total Time 10.00(10.00)\n",
      "Iter 20080 | Time 15.6430(14.6650) | Bit/dim 0.9748(0.9793) | Xent 0.0000(0.0002) | Loss 0.9749(0.9794) | Error 0.0000(0.0001) Steps 758(724.88) | Grad Norm 5.7571(2.4972) | Total Time 10.00(10.00)\n",
      "Iter 20090 | Time 13.8030(14.6225) | Bit/dim 0.9895(0.9788) | Xent 0.0003(0.0002) | Loss 0.9896(0.9789) | Error 0.0000(0.0001) Steps 710(722.78) | Grad Norm 7.4463(4.0095) | Total Time 10.00(10.00)\n",
      "Iter 20100 | Time 14.7294(14.5861) | Bit/dim 0.9815(0.9795) | Xent 0.0001(0.0002) | Loss 0.9816(0.9796) | Error 0.0000(0.0001) Steps 680(722.82) | Grad Norm 2.6046(4.0537) | Total Time 10.00(10.00)\n",
      "Iter 20110 | Time 14.3877(14.5795) | Bit/dim 0.9834(0.9783) | Xent 0.0000(0.0002) | Loss 0.9834(0.9784) | Error 0.0000(0.0000) Steps 734(723.73) | Grad Norm 0.6511(3.9601) | Total Time 10.00(10.00)\n",
      "Iter 20120 | Time 14.5779(14.5955) | Bit/dim 0.9715(0.9791) | Xent 0.0000(0.0002) | Loss 0.9715(0.9792) | Error 0.0000(0.0001) Steps 704(722.45) | Grad Norm 2.2899(3.7427) | Total Time 10.00(10.00)\n",
      "Iter 20130 | Time 14.6683(14.5961) | Bit/dim 0.9802(0.9789) | Xent 0.0000(0.0002) | Loss 0.9802(0.9790) | Error 0.0000(0.0001) Steps 722(721.92) | Grad Norm 4.9749(3.7160) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0305 | Time 57.8290, Epoch Time 1036.6425(1027.8995), Bit/dim 0.9780(best: 0.9774), Xent 0.0407, Loss 0.9984, Error 0.0079(best: 0.0067)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 20140 | Time 14.5392(14.6207) | Bit/dim 0.9867(0.9786) | Xent 0.0000(0.0002) | Loss 0.9868(0.9787) | Error 0.0000(0.0001) Steps 728(723.46) | Grad Norm 1.3233(3.2086) | Total Time 10.00(10.00)\n",
      "Iter 20150 | Time 14.7118(14.6349) | Bit/dim 0.9720(0.9784) | Xent 0.0000(0.0002) | Loss 0.9720(0.9785) | Error 0.0000(0.0001) Steps 704(723.78) | Grad Norm 2.2624(2.7286) | Total Time 10.00(10.00)\n",
      "Iter 20160 | Time 14.8725(14.6222) | Bit/dim 0.9700(0.9782) | Xent 0.0003(0.0002) | Loss 0.9701(0.9783) | Error 0.0000(0.0000) Steps 710(724.46) | Grad Norm 0.3460(2.2696) | Total Time 10.00(10.00)\n",
      "Iter 20170 | Time 13.9676(14.5834) | Bit/dim 0.9751(0.9791) | Xent 0.0000(0.0002) | Loss 0.9751(0.9792) | Error 0.0000(0.0001) Steps 722(722.77) | Grad Norm 1.5337(1.9847) | Total Time 10.00(10.00)\n",
      "Iter 20180 | Time 14.3557(14.5612) | Bit/dim 0.9814(0.9789) | Xent 0.0000(0.0002) | Loss 0.9814(0.9790) | Error 0.0000(0.0001) Steps 722(723.20) | Grad Norm 1.6494(1.9442) | Total Time 10.00(10.00)\n",
      "Iter 20190 | Time 14.4616(14.5363) | Bit/dim 0.9894(0.9795) | Xent 0.0000(0.0002) | Loss 0.9895(0.9796) | Error 0.0000(0.0001) Steps 728(722.37) | Grad Norm 1.5945(2.2418) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0306 | Time 58.2227, Epoch Time 1035.1915(1028.1183), Bit/dim 0.9773(best: 0.9774), Xent 0.0445, Loss 0.9995, Error 0.0084(best: 0.0067)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 20200 | Time 15.0524(14.5735) | Bit/dim 0.9758(0.9785) | Xent 0.0000(0.0002) | Loss 0.9758(0.9786) | Error 0.0000(0.0001) Steps 698(723.26) | Grad Norm 5.6501(2.4078) | Total Time 10.00(10.00)\n",
      "Iter 20210 | Time 14.3585(14.5304) | Bit/dim 0.9862(0.9792) | Xent 0.0000(0.0002) | Loss 0.9863(0.9793) | Error 0.0000(0.0001) Steps 722(722.26) | Grad Norm 8.1210(3.8755) | Total Time 10.00(10.00)\n",
      "Iter 20220 | Time 14.7931(14.5456) | Bit/dim 0.9865(0.9793) | Xent 0.0002(0.0001) | Loss 0.9866(0.9793) | Error 0.0000(0.0000) Steps 740(722.43) | Grad Norm 4.7183(4.5260) | Total Time 10.00(10.00)\n",
      "Iter 20230 | Time 14.4300(14.5511) | Bit/dim 0.9749(0.9790) | Xent 0.0000(0.0001) | Loss 0.9749(0.9791) | Error 0.0000(0.0001) Steps 728(721.12) | Grad Norm 4.1747(4.3341) | Total Time 10.00(10.00)\n",
      "Iter 20240 | Time 14.0397(14.5278) | Bit/dim 0.9633(0.9786) | Xent 0.0000(0.0001) | Loss 0.9634(0.9787) | Error 0.0000(0.0000) Steps 728(722.83) | Grad Norm 4.1680(4.5182) | Total Time 10.00(10.00)\n",
      "Iter 20250 | Time 14.0824(14.5127) | Bit/dim 0.9783(0.9784) | Xent 0.0000(0.0001) | Loss 0.9783(0.9784) | Error 0.0000(0.0000) Steps 746(724.76) | Grad Norm 0.5259(4.1305) | Total Time 10.00(10.00)\n",
      "Iter 20260 | Time 14.3177(14.5158) | Bit/dim 0.9883(0.9787) | Xent 0.0000(0.0001) | Loss 0.9884(0.9787) | Error 0.0000(0.0000) Steps 722(723.58) | Grad Norm 4.7479(3.8710) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0307 | Time 58.1699, Epoch Time 1031.9817(1028.2342), Bit/dim 0.9774(best: 0.9773), Xent 0.0424, Loss 0.9986, Error 0.0081(best: 0.0067)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 20270 | Time 14.5405(14.5254) | Bit/dim 0.9811(0.9795) | Xent 0.0000(0.0001) | Loss 0.9811(0.9796) | Error 0.0000(0.0000) Steps 740(723.36) | Grad Norm 3.3308(3.6096) | Total Time 10.00(10.00)\n",
      "Iter 20280 | Time 14.5115(14.5246) | Bit/dim 0.9797(0.9790) | Xent 0.0000(0.0001) | Loss 0.9797(0.9791) | Error 0.0000(0.0000) Steps 728(725.00) | Grad Norm 3.3495(3.4381) | Total Time 10.00(10.00)\n",
      "Iter 20290 | Time 15.1653(14.5727) | Bit/dim 0.9742(0.9785) | Xent 0.0000(0.0001) | Loss 0.9742(0.9785) | Error 0.0000(0.0000) Steps 716(724.01) | Grad Norm 0.3651(2.8557) | Total Time 10.00(10.00)\n",
      "Iter 20300 | Time 14.2532(14.5300) | Bit/dim 0.9946(0.9785) | Xent 0.0000(0.0002) | Loss 0.9946(0.9786) | Error 0.0000(0.0000) Steps 740(720.75) | Grad Norm 5.1643(2.7594) | Total Time 10.00(10.00)\n",
      "Iter 20310 | Time 13.3140(14.5181) | Bit/dim 0.9754(0.9780) | Xent 0.0000(0.0001) | Loss 0.9754(0.9781) | Error 0.0000(0.0000) Steps 698(718.29) | Grad Norm 8.5579(3.5359) | Total Time 10.00(10.00)\n",
      "Iter 20320 | Time 13.9632(14.5140) | Bit/dim 0.9923(0.9787) | Xent 0.0000(0.0001) | Loss 0.9924(0.9788) | Error 0.0000(0.0000) Steps 710(719.03) | Grad Norm 9.6976(5.0832) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0308 | Time 56.3899, Epoch Time 1029.5928(1028.2749), Bit/dim 0.9934(best: 0.9773), Xent 0.0491, Loss 1.0180, Error 0.0085(best: 0.0067)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 20330 | Time 14.3061(14.4753) | Bit/dim 0.9954(0.9800) | Xent 0.0000(0.0002) | Loss 0.9954(0.9801) | Error 0.0000(0.0000) Steps 722(719.07) | Grad Norm 10.2921(6.6485) | Total Time 10.00(10.00)\n",
      "Iter 20340 | Time 15.0966(14.4968) | Bit/dim 0.9901(0.9814) | Xent 0.0000(0.0001) | Loss 0.9901(0.9814) | Error 0.0000(0.0000) Steps 716(719.39) | Grad Norm 10.8105(7.8491) | Total Time 10.00(10.00)\n",
      "Iter 20350 | Time 14.5416(14.5012) | Bit/dim 0.9928(0.9808) | Xent 0.0000(0.0001) | Loss 0.9929(0.9809) | Error 0.0000(0.0000) Steps 716(720.86) | Grad Norm 3.5591(7.2244) | Total Time 10.00(10.00)\n",
      "Iter 20360 | Time 14.3292(14.5001) | Bit/dim 0.9829(0.9804) | Xent 0.0000(0.0001) | Loss 0.9829(0.9805) | Error 0.0000(0.0000) Steps 728(718.98) | Grad Norm 3.7130(6.4095) | Total Time 10.00(10.00)\n",
      "Iter 20370 | Time 14.5538(14.5323) | Bit/dim 0.9703(0.9787) | Xent 0.0000(0.0001) | Loss 0.9703(0.9788) | Error 0.0000(0.0000) Steps 728(720.20) | Grad Norm 2.5425(5.4051) | Total Time 10.00(10.00)\n",
      "Iter 20380 | Time 14.5506(14.5686) | Bit/dim 0.9827(0.9789) | Xent 0.0001(0.0001) | Loss 0.9828(0.9789) | Error 0.0000(0.0000) Steps 728(720.99) | Grad Norm 1.8435(4.6257) | Total Time 10.00(10.00)\n",
      "Iter 20390 | Time 14.2922(14.5468) | Bit/dim 0.9791(0.9785) | Xent 0.0001(0.0001) | Loss 0.9792(0.9786) | Error 0.0000(0.0000) Steps 716(720.83) | Grad Norm 1.7559(3.9197) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0309 | Time 58.4639, Epoch Time 1033.5445(1028.4330), Bit/dim 0.9773(best: 0.9773), Xent 0.0395, Loss 0.9971, Error 0.0085(best: 0.0067)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 20400 | Time 14.8165(14.5143) | Bit/dim 0.9702(0.9781) | Xent 0.0000(0.0001) | Loss 0.9702(0.9781) | Error 0.0000(0.0000) Steps 704(719.59) | Grad Norm 0.5083(3.0947) | Total Time 10.00(10.00)\n",
      "Iter 20410 | Time 14.4914(14.5508) | Bit/dim 0.9742(0.9768) | Xent 0.0000(0.0001) | Loss 0.9742(0.9768) | Error 0.0000(0.0000) Steps 716(718.41) | Grad Norm 0.8361(2.5206) | Total Time 10.00(10.00)\n",
      "Iter 20420 | Time 14.6297(14.5905) | Bit/dim 0.9766(0.9771) | Xent 0.0000(0.0001) | Loss 0.9766(0.9772) | Error 0.0000(0.0000) Steps 740(718.24) | Grad Norm 0.6692(2.0430) | Total Time 10.00(10.00)\n",
      "Iter 20430 | Time 14.7943(14.5979) | Bit/dim 0.9636(0.9778) | Xent 0.0001(0.0001) | Loss 0.9636(0.9779) | Error 0.0000(0.0000) Steps 692(719.10) | Grad Norm 0.4931(1.7590) | Total Time 10.00(10.00)\n",
      "Iter 20440 | Time 14.9680(14.5782) | Bit/dim 0.9655(0.9772) | Xent 0.0000(0.0001) | Loss 0.9655(0.9772) | Error 0.0000(0.0000) Steps 704(719.92) | Grad Norm 0.8541(1.5836) | Total Time 10.00(10.00)\n",
      "Iter 20450 | Time 14.4259(14.5800) | Bit/dim 0.9644(0.9775) | Xent 0.0000(0.0001) | Loss 0.9644(0.9776) | Error 0.0000(0.0000) Steps 734(720.04) | Grad Norm 0.8841(1.4199) | Total Time 10.00(10.00)\n",
      "Iter 20460 | Time 14.3836(14.5961) | Bit/dim 0.9745(0.9783) | Xent 0.0001(0.0002) | Loss 0.9746(0.9784) | Error 0.0000(0.0001) Steps 734(719.76) | Grad Norm 2.4751(1.3401) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0310 | Time 57.6750, Epoch Time 1036.1043(1028.6631), Bit/dim 0.9770(best: 0.9773), Xent 0.0434, Loss 0.9987, Error 0.0079(best: 0.0067)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 20470 | Time 14.2957(14.5627) | Bit/dim 0.9731(0.9774) | Xent 0.0000(0.0002) | Loss 0.9731(0.9775) | Error 0.0000(0.0001) Steps 728(720.93) | Grad Norm 2.8538(1.6887) | Total Time 10.00(10.00)\n",
      "Iter 20480 | Time 14.2171(14.4984) | Bit/dim 0.9696(0.9777) | Xent 0.0001(0.0002) | Loss 0.9697(0.9778) | Error 0.0000(0.0000) Steps 710(717.95) | Grad Norm 4.2860(2.4289) | Total Time 10.00(10.00)\n",
      "Iter 20490 | Time 15.1575(14.5349) | Bit/dim 0.9649(0.9778) | Xent 0.0000(0.0003) | Loss 0.9649(0.9780) | Error 0.0000(0.0001) Steps 710(718.52) | Grad Norm 3.6142(2.3878) | Total Time 10.00(10.00)\n",
      "Iter 20500 | Time 14.3520(14.5398) | Bit/dim 0.9756(0.9780) | Xent 0.0000(0.0003) | Loss 0.9756(0.9781) | Error 0.0000(0.0001) Steps 716(717.76) | Grad Norm 4.5257(2.7335) | Total Time 10.00(10.00)\n",
      "Iter 20510 | Time 13.8932(14.5283) | Bit/dim 0.9808(0.9781) | Xent 0.0000(0.0002) | Loss 0.9808(0.9782) | Error 0.0000(0.0001) Steps 692(719.82) | Grad Norm 5.7540(3.0723) | Total Time 10.00(10.00)\n",
      "Iter 20520 | Time 13.9460(14.5120) | Bit/dim 0.9791(0.9779) | Xent 0.0002(0.0002) | Loss 0.9792(0.9780) | Error 0.0000(0.0001) Steps 710(720.22) | Grad Norm 2.9974(3.2875) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0311 | Time 57.4002, Epoch Time 1028.6241(1028.6620), Bit/dim 0.9771(best: 0.9770), Xent 0.0467, Loss 1.0004, Error 0.0079(best: 0.0067)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 20530 | Time 14.3401(14.4907) | Bit/dim 0.9952(0.9787) | Xent 0.0002(0.0002) | Loss 0.9954(0.9788) | Error 0.0000(0.0001) Steps 734(719.94) | Grad Norm 4.5175(3.7602) | Total Time 10.00(10.00)\n",
      "Iter 20540 | Time 14.4900(14.5082) | Bit/dim 0.9729(0.9776) | Xent 0.0001(0.0001) | Loss 0.9729(0.9776) | Error 0.0000(0.0000) Steps 722(721.45) | Grad Norm 0.8896(3.5757) | Total Time 10.00(10.00)\n",
      "Iter 20550 | Time 14.2848(14.5336) | Bit/dim 0.9714(0.9773) | Xent 0.0000(0.0001) | Loss 0.9714(0.9773) | Error 0.0000(0.0000) Steps 728(721.35) | Grad Norm 0.4660(3.1861) | Total Time 10.00(10.00)\n",
      "Iter 20560 | Time 14.6373(14.5293) | Bit/dim 0.9722(0.9771) | Xent 0.0001(0.0001) | Loss 0.9722(0.9771) | Error 0.0000(0.0000) Steps 710(720.85) | Grad Norm 2.1606(2.6463) | Total Time 10.00(10.00)\n",
      "Iter 20570 | Time 15.0866(14.5416) | Bit/dim 0.9910(0.9778) | Xent 0.0000(0.0001) | Loss 0.9910(0.9779) | Error 0.0000(0.0000) Steps 704(720.48) | Grad Norm 0.8909(2.2548) | Total Time 10.00(10.00)\n",
      "Iter 20580 | Time 14.9525(14.5695) | Bit/dim 0.9816(0.9783) | Xent 0.0002(0.0001) | Loss 0.9817(0.9784) | Error 0.0000(0.0000) Steps 722(719.87) | Grad Norm 5.6088(2.1959) | Total Time 10.00(10.00)\n",
      "Iter 20590 | Time 14.1345(14.4938) | Bit/dim 0.9756(0.9788) | Xent 0.0000(0.0001) | Loss 0.9756(0.9788) | Error 0.0000(0.0000) Steps 710(720.34) | Grad Norm 6.5818(3.3647) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0312 | Time 57.6247, Epoch Time 1031.5537(1028.7487), Bit/dim 0.9768(best: 0.9770), Xent 0.0413, Loss 0.9974, Error 0.0075(best: 0.0067)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 20600 | Time 13.7695(14.4911) | Bit/dim 0.9733(0.9794) | Xent 0.0000(0.0002) | Loss 0.9734(0.9795) | Error 0.0000(0.0001) Steps 698(720.99) | Grad Norm 2.3362(3.5590) | Total Time 10.00(10.00)\n",
      "Iter 20610 | Time 14.4235(14.4903) | Bit/dim 0.9831(0.9793) | Xent 0.0000(0.0002) | Loss 0.9831(0.9794) | Error 0.0000(0.0000) Steps 734(722.05) | Grad Norm 0.2702(2.9972) | Total Time 10.00(10.00)\n",
      "Iter 20620 | Time 14.3264(14.4585) | Bit/dim 0.9884(0.9786) | Xent 0.0000(0.0001) | Loss 0.9884(0.9787) | Error 0.0000(0.0000) Steps 716(721.02) | Grad Norm 1.6642(2.5581) | Total Time 10.00(10.00)\n",
      "Iter 20630 | Time 14.5455(14.4997) | Bit/dim 0.9820(0.9783) | Xent 0.0000(0.0001) | Loss 0.9820(0.9784) | Error 0.0000(0.0000) Steps 734(721.64) | Grad Norm 12.2489(3.1323) | Total Time 10.00(10.00)\n",
      "Iter 20640 | Time 14.7377(14.4528) | Bit/dim 0.9809(0.9782) | Xent 0.0000(0.0001) | Loss 0.9809(0.9783) | Error 0.0000(0.0000) Steps 692(720.86) | Grad Norm 0.5854(3.6830) | Total Time 10.00(10.00)\n",
      "Iter 20650 | Time 14.3667(14.5151) | Bit/dim 0.9712(0.9769) | Xent 0.0000(0.0001) | Loss 0.9712(0.9770) | Error 0.0000(0.0000) Steps 722(720.49) | Grad Norm 1.2512(3.3992) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0313 | Time 57.5921, Epoch Time 1028.8311(1028.7512), Bit/dim 0.9768(best: 0.9768), Xent 0.0409, Loss 0.9973, Error 0.0083(best: 0.0067)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 20660 | Time 14.3813(14.4944) | Bit/dim 0.9794(0.9773) | Xent 0.0001(0.0001) | Loss 0.9794(0.9773) | Error 0.0000(0.0000) Steps 728(721.95) | Grad Norm 3.4291(3.3635) | Total Time 10.00(10.00)\n",
      "Iter 20670 | Time 14.5090(14.4805) | Bit/dim 0.9762(0.9778) | Xent 0.0006(0.0001) | Loss 0.9766(0.9779) | Error 0.0000(0.0000) Steps 722(720.85) | Grad Norm 1.8518(3.4544) | Total Time 10.00(10.00)\n",
      "Iter 20680 | Time 14.4456(14.4885) | Bit/dim 0.9757(0.9779) | Xent 0.0000(0.0002) | Loss 0.9757(0.9779) | Error 0.0000(0.0000) Steps 734(720.09) | Grad Norm 12.6489(4.4873) | Total Time 10.00(10.00)\n",
      "Iter 20690 | Time 14.4567(14.4859) | Bit/dim 0.9903(0.9779) | Xent 0.0000(0.0002) | Loss 0.9903(0.9779) | Error 0.0000(0.0000) Steps 728(719.72) | Grad Norm 2.1708(4.2567) | Total Time 10.00(10.00)\n",
      "Iter 20700 | Time 14.8629(14.5016) | Bit/dim 0.9840(0.9780) | Xent 0.0000(0.0001) | Loss 0.9840(0.9781) | Error 0.0000(0.0000) Steps 728(720.05) | Grad Norm 3.9371(3.7375) | Total Time 10.00(10.00)\n",
      "Iter 20710 | Time 14.6873(14.5279) | Bit/dim 0.9834(0.9781) | Xent 0.0000(0.0001) | Loss 0.9834(0.9782) | Error 0.0000(0.0000) Steps 704(719.83) | Grad Norm 0.7301(3.4589) | Total Time 10.00(10.00)\n",
      "Iter 20720 | Time 14.5585(14.5101) | Bit/dim 0.9893(0.9786) | Xent 0.0001(0.0002) | Loss 0.9893(0.9787) | Error 0.0000(0.0000) Steps 734(721.03) | Grad Norm 0.3825(4.1883) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0314 | Time 57.7361, Epoch Time 1030.9990(1028.8186), Bit/dim 0.9773(best: 0.9768), Xent 0.0394, Loss 0.9970, Error 0.0078(best: 0.0067)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 20730 | Time 14.6971(14.5289) | Bit/dim 0.9745(0.9779) | Xent 0.0001(0.0002) | Loss 0.9745(0.9780) | Error 0.0000(0.0000) Steps 740(720.86) | Grad Norm 2.0851(4.5421) | Total Time 10.00(10.00)\n",
      "Iter 20740 | Time 15.1154(14.5417) | Bit/dim 0.9841(0.9775) | Xent 0.0000(0.0002) | Loss 0.9841(0.9775) | Error 0.0000(0.0000) Steps 716(720.58) | Grad Norm 5.2403(4.4948) | Total Time 10.00(10.00)\n",
      "Iter 20750 | Time 15.0987(14.5232) | Bit/dim 0.9893(0.9765) | Xent 0.0001(0.0002) | Loss 0.9894(0.9766) | Error 0.0000(0.0000) Steps 728(720.05) | Grad Norm 3.5836(4.5773) | Total Time 10.00(10.00)\n",
      "Iter 20760 | Time 14.7360(14.5578) | Bit/dim 0.9715(0.9765) | Xent 0.0000(0.0001) | Loss 0.9715(0.9766) | Error 0.0000(0.0000) Steps 716(718.61) | Grad Norm 3.1800(3.9920) | Total Time 10.00(10.00)\n",
      "Iter 20770 | Time 14.9820(14.5625) | Bit/dim 0.9844(0.9779) | Xent 0.0001(0.0001) | Loss 0.9845(0.9779) | Error 0.0000(0.0000) Steps 704(719.68) | Grad Norm 0.3589(3.1964) | Total Time 10.00(10.00)\n",
      "Iter 20780 | Time 14.6690(14.5665) | Bit/dim 0.9720(0.9784) | Xent 0.0000(0.0001) | Loss 0.9720(0.9785) | Error 0.0000(0.0000) Steps 728(719.99) | Grad Norm 3.5194(2.7005) | Total Time 10.00(10.00)\n",
      "Iter 20790 | Time 13.9386(14.5151) | Bit/dim 0.9874(0.9785) | Xent 0.0000(0.0001) | Loss 0.9874(0.9786) | Error 0.0000(0.0000) Steps 728(719.84) | Grad Norm 8.4615(3.7986) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0315 | Time 58.7503, Epoch Time 1032.5676(1028.9311), Bit/dim 0.9786(best: 0.9768), Xent 0.0432, Loss 1.0002, Error 0.0091(best: 0.0067)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 20800 | Time 14.8152(14.5090) | Bit/dim 0.9748(0.9774) | Xent 0.0000(0.0001) | Loss 0.9748(0.9775) | Error 0.0000(0.0001) Steps 710(718.22) | Grad Norm 2.9271(3.9756) | Total Time 10.00(10.00)\n",
      "Iter 20810 | Time 14.1257(14.4879) | Bit/dim 0.9771(0.9771) | Xent 0.0000(0.0001) | Loss 0.9771(0.9772) | Error 0.0000(0.0000) Steps 710(718.23) | Grad Norm 5.9056(4.2130) | Total Time 10.00(10.00)\n",
      "Iter 20820 | Time 14.2376(14.5040) | Bit/dim 0.9894(0.9768) | Xent 0.0000(0.0001) | Loss 0.9894(0.9769) | Error 0.0000(0.0000) Steps 698(716.84) | Grad Norm 3.6663(3.6019) | Total Time 10.00(10.00)\n",
      "Iter 20830 | Time 14.6116(14.4898) | Bit/dim 0.9784(0.9764) | Xent 0.0000(0.0001) | Loss 0.9784(0.9765) | Error 0.0000(0.0001) Steps 746(717.79) | Grad Norm 6.0647(3.3705) | Total Time 10.00(10.00)\n",
      "Iter 20840 | Time 14.1625(14.5187) | Bit/dim 0.9816(0.9782) | Xent 0.0001(0.0001) | Loss 0.9817(0.9783) | Error 0.0000(0.0000) Steps 722(718.79) | Grad Norm 2.8933(3.3363) | Total Time 10.00(10.00)\n",
      "Iter 20850 | Time 14.8504(14.5562) | Bit/dim 0.9894(0.9779) | Xent 0.0000(0.0002) | Loss 0.9894(0.9780) | Error 0.0000(0.0001) Steps 716(720.84) | Grad Norm 0.4253(3.3689) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0316 | Time 57.4590, Epoch Time 1031.8873(1029.0198), Bit/dim 0.9765(best: 0.9768), Xent 0.0368, Loss 0.9949, Error 0.0079(best: 0.0067)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 20860 | Time 14.4332(14.5687) | Bit/dim 0.9756(0.9785) | Xent 0.0000(0.0002) | Loss 0.9756(0.9786) | Error 0.0000(0.0000) Steps 728(721.85) | Grad Norm 1.1733(2.9016) | Total Time 10.00(10.00)\n",
      "Iter 20870 | Time 14.8392(14.5644) | Bit/dim 0.9693(0.9786) | Xent 0.0000(0.0001) | Loss 0.9693(0.9787) | Error 0.0000(0.0000) Steps 710(723.08) | Grad Norm 2.9938(2.5563) | Total Time 10.00(10.00)\n",
      "Iter 20880 | Time 14.4636(14.5341) | Bit/dim 0.9876(0.9786) | Xent 0.0000(0.0001) | Loss 0.9877(0.9786) | Error 0.0000(0.0001) Steps 698(719.72) | Grad Norm 1.1099(2.2949) | Total Time 10.00(10.00)\n",
      "Iter 20890 | Time 14.6055(14.4717) | Bit/dim 0.9751(0.9780) | Xent 0.0001(0.0001) | Loss 0.9751(0.9781) | Error 0.0000(0.0000) Steps 698(717.71) | Grad Norm 9.9242(3.0818) | Total Time 10.00(10.00)\n",
      "Iter 20900 | Time 14.1097(14.4734) | Bit/dim 0.9741(0.9776) | Xent 0.0000(0.0001) | Loss 0.9741(0.9777) | Error 0.0000(0.0000) Steps 722(716.22) | Grad Norm 6.3393(4.5631) | Total Time 10.00(10.00)\n",
      "Iter 20910 | Time 13.9738(14.4263) | Bit/dim 0.9825(0.9776) | Xent 0.0001(0.0001) | Loss 0.9826(0.9776) | Error 0.0000(0.0000) Steps 722(716.27) | Grad Norm 6.2844(4.8441) | Total Time 10.00(10.00)\n",
      "Iter 20920 | Time 14.5745(14.4747) | Bit/dim 0.9850(0.9779) | Xent 0.0002(0.0001) | Loss 0.9851(0.9780) | Error 0.0000(0.0001) Steps 698(717.09) | Grad Norm 4.7022(4.8403) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0317 | Time 57.4465, Epoch Time 1027.0602(1028.9610), Bit/dim 0.9770(best: 0.9765), Xent 0.0464, Loss 1.0002, Error 0.0078(best: 0.0067)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 20930 | Time 14.8836(14.4922) | Bit/dim 0.9629(0.9763) | Xent 0.0000(0.0001) | Loss 0.9629(0.9764) | Error 0.0000(0.0000) Steps 740(715.79) | Grad Norm 3.9772(4.6492) | Total Time 10.00(10.00)\n",
      "Iter 20940 | Time 14.7383(14.5473) | Bit/dim 0.9808(0.9755) | Xent 0.0002(0.0001) | Loss 0.9809(0.9755) | Error 0.0000(0.0000) Steps 716(717.12) | Grad Norm 0.7369(4.1328) | Total Time 10.00(10.00)\n",
      "Iter 20950 | Time 14.9237(14.5778) | Bit/dim 0.9861(0.9759) | Xent 0.0001(0.0001) | Loss 0.9862(0.9760) | Error 0.0000(0.0001) Steps 728(716.78) | Grad Norm 1.1670(3.5297) | Total Time 10.00(10.00)\n",
      "Iter 20960 | Time 14.6051(14.5520) | Bit/dim 0.9798(0.9772) | Xent 0.0000(0.0002) | Loss 0.9798(0.9773) | Error 0.0000(0.0001) Steps 734(717.96) | Grad Norm 10.1746(4.0490) | Total Time 10.00(10.00)\n",
      "Iter 20970 | Time 14.8639(14.5234) | Bit/dim 0.9762(0.9778) | Xent 0.0000(0.0001) | Loss 0.9762(0.9778) | Error 0.0000(0.0000) Steps 746(719.73) | Grad Norm 9.1976(4.2927) | Total Time 10.00(10.00)\n",
      "Iter 20980 | Time 14.3497(14.4934) | Bit/dim 0.9846(0.9781) | Xent 0.0003(0.0001) | Loss 0.9847(0.9782) | Error 0.0000(0.0000) Steps 740(720.10) | Grad Norm 0.8435(4.4473) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0318 | Time 56.7865, Epoch Time 1030.7319(1029.0141), Bit/dim 0.9764(best: 0.9765), Xent 0.0427, Loss 0.9977, Error 0.0086(best: 0.0067)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 20990 | Time 14.7065(14.4772) | Bit/dim 0.9801(0.9785) | Xent 0.0001(0.0001) | Loss 0.9802(0.9786) | Error 0.0000(0.0000) Steps 704(720.53) | Grad Norm 2.8987(3.9598) | Total Time 10.00(10.00)\n",
      "Iter 21000 | Time 14.6462(14.5001) | Bit/dim 0.9856(0.9777) | Xent 0.0000(0.0001) | Loss 0.9856(0.9778) | Error 0.0000(0.0000) Steps 722(719.34) | Grad Norm 0.3385(3.3911) | Total Time 10.00(10.00)\n",
      "Iter 21010 | Time 14.9594(14.5342) | Bit/dim 0.9902(0.9768) | Xent 0.0003(0.0001) | Loss 0.9904(0.9769) | Error 0.0000(0.0000) Steps 716(718.93) | Grad Norm 1.9187(3.0854) | Total Time 10.00(10.00)\n",
      "Iter 21020 | Time 14.8202(14.4886) | Bit/dim 0.9734(0.9763) | Xent 0.0012(0.0001) | Loss 0.9740(0.9764) | Error 0.0011(0.0000) Steps 704(717.63) | Grad Norm 2.0205(2.8153) | Total Time 10.00(10.00)\n",
      "Iter 21030 | Time 14.8821(14.4938) | Bit/dim 0.9718(0.9763) | Xent 0.0028(0.0002) | Loss 0.9732(0.9764) | Error 0.0022(0.0001) Steps 728(718.50) | Grad Norm 2.3555(3.1559) | Total Time 10.00(10.00)\n",
      "Iter 21040 | Time 14.5862(14.4656) | Bit/dim 0.9712(0.9772) | Xent 0.0001(0.0002) | Loss 0.9712(0.9773) | Error 0.0000(0.0001) Steps 698(716.56) | Grad Norm 1.0112(2.8455) | Total Time 10.00(10.00)\n",
      "Iter 21050 | Time 14.8073(14.5083) | Bit/dim 0.9809(0.9776) | Xent 0.0000(0.0002) | Loss 0.9809(0.9777) | Error 0.0000(0.0001) Steps 740(715.28) | Grad Norm 1.0134(2.4218) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0319 | Time 57.5692, Epoch Time 1031.0361(1029.0748), Bit/dim 0.9766(best: 0.9764), Xent 0.0435, Loss 0.9983, Error 0.0082(best: 0.0067)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 21060 | Time 14.1734(14.4995) | Bit/dim 0.9619(0.9769) | Xent 0.0000(0.0001) | Loss 0.9619(0.9769) | Error 0.0000(0.0001) Steps 728(716.22) | Grad Norm 5.2451(2.9929) | Total Time 10.00(10.00)\n",
      "Iter 21070 | Time 14.5980(14.4715) | Bit/dim 0.9811(0.9765) | Xent 0.0001(0.0001) | Loss 0.9812(0.9766) | Error 0.0000(0.0000) Steps 722(718.68) | Grad Norm 7.9830(3.9539) | Total Time 10.00(10.00)\n",
      "Iter 21080 | Time 14.1786(14.4877) | Bit/dim 0.9632(0.9769) | Xent 0.0000(0.0001) | Loss 0.9632(0.9770) | Error 0.0000(0.0000) Steps 698(718.36) | Grad Norm 6.6206(4.2360) | Total Time 10.00(10.00)\n",
      "Iter 21090 | Time 14.8706(14.5004) | Bit/dim 0.9866(0.9774) | Xent 0.0000(0.0001) | Loss 0.9866(0.9775) | Error 0.0000(0.0000) Steps 734(721.06) | Grad Norm 2.4233(4.6022) | Total Time 10.00(10.00)\n",
      "Iter 21100 | Time 14.0173(14.4769) | Bit/dim 0.9660(0.9776) | Xent 0.0000(0.0001) | Loss 0.9661(0.9776) | Error 0.0000(0.0000) Steps 698(719.18) | Grad Norm 0.4635(3.9021) | Total Time 10.00(10.00)\n",
      "Iter 21110 | Time 14.3627(14.4747) | Bit/dim 0.9814(0.9782) | Xent 0.0001(0.0001) | Loss 0.9814(0.9782) | Error 0.0000(0.0000) Steps 728(718.66) | Grad Norm 0.5950(3.3339) | Total Time 10.00(10.00)\n",
      "Iter 21120 | Time 14.6794(14.4774) | Bit/dim 0.9789(0.9775) | Xent 0.0012(0.0002) | Loss 0.9796(0.9776) | Error 0.0011(0.0001) Steps 722(718.86) | Grad Norm 15.7267(3.9023) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0320 | Time 56.9701, Epoch Time 1026.3501(1028.9930), Bit/dim 0.9795(best: 0.9764), Xent 0.0388, Loss 0.9989, Error 0.0079(best: 0.0067)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 21130 | Time 14.8777(14.4763) | Bit/dim 0.9953(0.9776) | Xent 0.0000(0.0002) | Loss 0.9953(0.9777) | Error 0.0000(0.0001) Steps 752(717.60) | Grad Norm 16.0440(4.9405) | Total Time 10.00(10.00)\n",
      "Iter 21140 | Time 14.9052(14.4534) | Bit/dim 0.9857(0.9784) | Xent 0.0000(0.0002) | Loss 0.9857(0.9785) | Error 0.0000(0.0001) Steps 734(717.58) | Grad Norm 7.8189(5.8603) | Total Time 10.00(10.00)\n",
      "Iter 21150 | Time 14.0344(14.4125) | Bit/dim 0.9911(0.9789) | Xent 0.0000(0.0002) | Loss 0.9911(0.9790) | Error 0.0000(0.0001) Steps 698(717.07) | Grad Norm 0.5320(5.4911) | Total Time 10.00(10.00)\n",
      "Iter 21160 | Time 14.1000(14.4028) | Bit/dim 0.9751(0.9782) | Xent 0.0000(0.0002) | Loss 0.9751(0.9783) | Error 0.0000(0.0001) Steps 692(716.83) | Grad Norm 1.5497(5.2240) | Total Time 10.00(10.00)\n",
      "Iter 21170 | Time 14.5106(14.4229) | Bit/dim 0.9853(0.9780) | Xent 0.0000(0.0002) | Loss 0.9853(0.9781) | Error 0.0000(0.0001) Steps 710(717.06) | Grad Norm 2.0392(5.0405) | Total Time 10.00(10.00)\n",
      "Iter 21180 | Time 14.8274(14.4830) | Bit/dim 0.9846(0.9774) | Xent 0.0001(0.0001) | Loss 0.9846(0.9775) | Error 0.0000(0.0001) Steps 728(716.68) | Grad Norm 7.8531(4.9113) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0321 | Time 57.3649, Epoch Time 1027.3405(1028.9435), Bit/dim 0.9771(best: 0.9764), Xent 0.0411, Loss 0.9977, Error 0.0069(best: 0.0067)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 21190 | Time 14.2726(14.5095) | Bit/dim 0.9726(0.9774) | Xent 0.0000(0.0001) | Loss 0.9726(0.9775) | Error 0.0000(0.0000) Steps 722(719.62) | Grad Norm 7.9276(4.7900) | Total Time 10.00(10.00)\n",
      "Iter 21200 | Time 14.2727(14.4775) | Bit/dim 0.9764(0.9775) | Xent 0.0000(0.0001) | Loss 0.9765(0.9776) | Error 0.0000(0.0000) Steps 710(719.79) | Grad Norm 5.6607(4.4241) | Total Time 10.00(10.00)\n",
      "Iter 21210 | Time 13.9541(14.4837) | Bit/dim 0.9714(0.9778) | Xent 0.0000(0.0002) | Loss 0.9714(0.9779) | Error 0.0000(0.0001) Steps 722(721.32) | Grad Norm 1.0972(4.0206) | Total Time 10.00(10.00)\n",
      "Iter 21220 | Time 14.4551(14.4853) | Bit/dim 0.9701(0.9770) | Xent 0.0000(0.0002) | Loss 0.9701(0.9771) | Error 0.0000(0.0001) Steps 722(720.27) | Grad Norm 1.7245(3.3409) | Total Time 10.00(10.00)\n",
      "Iter 21230 | Time 14.8871(14.5113) | Bit/dim 0.9773(0.9765) | Xent 0.0000(0.0002) | Loss 0.9773(0.9766) | Error 0.0000(0.0001) Steps 716(719.34) | Grad Norm 1.9475(2.8669) | Total Time 10.00(10.00)\n",
      "Iter 21240 | Time 14.0129(14.4752) | Bit/dim 0.9823(0.9771) | Xent 0.0000(0.0002) | Loss 0.9823(0.9772) | Error 0.0000(0.0001) Steps 710(719.29) | Grad Norm 2.2043(2.7895) | Total Time 10.00(10.00)\n",
      "Iter 21250 | Time 14.2620(14.4805) | Bit/dim 0.9819(0.9761) | Xent 0.0000(0.0002) | Loss 0.9819(0.9762) | Error 0.0000(0.0001) Steps 698(720.56) | Grad Norm 0.4627(2.3523) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0322 | Time 56.7060, Epoch Time 1027.4256(1028.8979), Bit/dim 0.9756(best: 0.9764), Xent 0.0433, Loss 0.9972, Error 0.0083(best: 0.0067)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 21260 | Time 14.5527(14.5209) | Bit/dim 0.9708(0.9768) | Xent 0.0000(0.0002) | Loss 0.9709(0.9769) | Error 0.0000(0.0001) Steps 698(718.70) | Grad Norm 0.4562(1.9083) | Total Time 10.00(10.00)\n",
      "Iter 21270 | Time 14.8810(14.4915) | Bit/dim 0.9865(0.9766) | Xent 0.0001(0.0001) | Loss 0.9866(0.9767) | Error 0.0000(0.0000) Steps 698(717.08) | Grad Norm 1.1652(1.7552) | Total Time 10.00(10.00)\n",
      "Iter 21280 | Time 14.5636(14.5449) | Bit/dim 0.9850(0.9760) | Xent 0.0000(0.0001) | Loss 0.9850(0.9760) | Error 0.0000(0.0000) Steps 728(715.61) | Grad Norm 1.2107(1.5316) | Total Time 10.00(10.00)\n",
      "Iter 21290 | Time 14.0077(14.5778) | Bit/dim 0.9653(0.9762) | Xent 0.0000(0.0001) | Loss 0.9653(0.9762) | Error 0.0000(0.0000) Steps 716(716.86) | Grad Norm 1.7267(1.5734) | Total Time 10.00(10.00)\n",
      "Iter 21300 | Time 14.8184(14.5310) | Bit/dim 0.9790(0.9768) | Xent 0.0000(0.0001) | Loss 0.9790(0.9768) | Error 0.0000(0.0000) Steps 722(716.95) | Grad Norm 1.8583(1.4996) | Total Time 10.00(10.00)\n",
      "Iter 21310 | Time 14.4892(14.4998) | Bit/dim 0.9806(0.9765) | Xent 0.0000(0.0002) | Loss 0.9806(0.9766) | Error 0.0000(0.0001) Steps 722(716.97) | Grad Norm 2.0195(1.6240) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0323 | Time 57.2170, Epoch Time 1031.4339(1028.9740), Bit/dim 0.9768(best: 0.9756), Xent 0.0414, Loss 0.9975, Error 0.0076(best: 0.0067)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 21320 | Time 14.5234(14.5106) | Bit/dim 0.9888(0.9767) | Xent 0.0003(0.0002) | Loss 0.9890(0.9768) | Error 0.0000(0.0001) Steps 734(719.10) | Grad Norm 12.7099(2.5775) | Total Time 10.00(10.00)\n",
      "Iter 21330 | Time 15.2568(14.4963) | Bit/dim 1.0029(0.9776) | Xent 0.0000(0.0002) | Loss 1.0029(0.9777) | Error 0.0000(0.0000) Steps 746(719.02) | Grad Norm 16.8476(4.7079) | Total Time 10.00(10.00)\n",
      "Iter 21340 | Time 14.1863(14.4594) | Bit/dim 0.9719(0.9775) | Xent 0.0007(0.0002) | Loss 0.9722(0.9776) | Error 0.0000(0.0000) Steps 722(719.42) | Grad Norm 9.1811(4.8840) | Total Time 10.00(10.00)\n",
      "Iter 21350 | Time 14.6832(14.5017) | Bit/dim 0.9815(0.9769) | Xent 0.0000(0.0001) | Loss 0.9815(0.9770) | Error 0.0000(0.0000) Steps 728(719.34) | Grad Norm 2.7704(5.0080) | Total Time 10.00(10.00)\n",
      "Iter 21360 | Time 14.3480(14.5287) | Bit/dim 0.9862(0.9782) | Xent 0.0003(0.0001) | Loss 0.9863(0.9783) | Error 0.0000(0.0000) Steps 728(721.65) | Grad Norm 3.7382(4.5580) | Total Time 10.00(10.00)\n",
      "Iter 21370 | Time 14.2732(14.5350) | Bit/dim 0.9743(0.9769) | Xent 0.0000(0.0001) | Loss 0.9743(0.9769) | Error 0.0000(0.0000) Steps 710(721.52) | Grad Norm 5.0303(4.9456) | Total Time 10.00(10.00)\n",
      "Iter 21380 | Time 14.5808(14.5500) | Bit/dim 0.9753(0.9769) | Xent 0.0001(0.0001) | Loss 0.9754(0.9770) | Error 0.0000(0.0000) Steps 716(722.60) | Grad Norm 3.5045(4.7167) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0324 | Time 57.2415, Epoch Time 1031.6493(1029.0543), Bit/dim 0.9758(best: 0.9756), Xent 0.0422, Loss 0.9969, Error 0.0084(best: 0.0067)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 21390 | Time 14.4724(14.5562) | Bit/dim 0.9650(0.9754) | Xent 0.0000(0.0001) | Loss 0.9650(0.9755) | Error 0.0000(0.0000) Steps 722(721.83) | Grad Norm 3.1335(4.0763) | Total Time 10.00(10.00)\n",
      "Iter 21400 | Time 14.1577(14.5419) | Bit/dim 0.9763(0.9756) | Xent 0.0000(0.0002) | Loss 0.9763(0.9757) | Error 0.0000(0.0001) Steps 710(719.12) | Grad Norm 2.6350(3.7919) | Total Time 10.00(10.00)\n",
      "Iter 21410 | Time 14.7772(14.5591) | Bit/dim 0.9831(0.9769) | Xent 0.0000(0.0002) | Loss 0.9831(0.9770) | Error 0.0000(0.0001) Steps 698(717.20) | Grad Norm 3.4158(3.4839) | Total Time 10.00(10.00)\n",
      "Iter 21420 | Time 14.8609(14.5746) | Bit/dim 0.9843(0.9772) | Xent 0.0011(0.0002) | Loss 0.9848(0.9773) | Error 0.0011(0.0001) Steps 746(718.29) | Grad Norm 2.1354(3.0862) | Total Time 10.00(10.00)\n",
      "Iter 21430 | Time 14.2171(14.5818) | Bit/dim 0.9808(0.9768) | Xent 0.0000(0.0002) | Loss 0.9808(0.9769) | Error 0.0000(0.0001) Steps 716(716.33) | Grad Norm 4.1650(2.8865) | Total Time 10.00(10.00)\n",
      "Iter 21440 | Time 14.6486(14.6043) | Bit/dim 0.9678(0.9755) | Xent 0.0001(0.0002) | Loss 0.9678(0.9756) | Error 0.0000(0.0001) Steps 698(719.00) | Grad Norm 4.0199(3.2021) | Total Time 10.00(10.00)\n",
      "Iter 21450 | Time 14.3213(14.5984) | Bit/dim 0.9906(0.9764) | Xent 0.0000(0.0001) | Loss 0.9907(0.9764) | Error 0.0000(0.0000) Steps 710(718.07) | Grad Norm 4.6562(3.7510) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0325 | Time 58.5896, Epoch Time 1037.4392(1029.3058), Bit/dim 0.9763(best: 0.9756), Xent 0.0457, Loss 0.9992, Error 0.0084(best: 0.0067)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 21460 | Time 14.0851(14.4964) | Bit/dim 0.9754(0.9762) | Xent 0.0002(0.0001) | Loss 0.9755(0.9763) | Error 0.0000(0.0000) Steps 728(718.24) | Grad Norm 0.9480(3.9270) | Total Time 10.00(10.00)\n",
      "Iter 21470 | Time 14.6120(14.5255) | Bit/dim 0.9809(0.9765) | Xent 0.0001(0.0001) | Loss 0.9809(0.9766) | Error 0.0000(0.0000) Steps 710(718.64) | Grad Norm 1.1574(3.1968) | Total Time 10.00(10.00)\n",
      "Iter 21480 | Time 14.5655(14.5311) | Bit/dim 0.9799(0.9778) | Xent 0.0000(0.0003) | Loss 0.9799(0.9780) | Error 0.0000(0.0000) Steps 722(719.32) | Grad Norm 4.5770(2.9174) | Total Time 10.00(10.00)\n",
      "Iter 21490 | Time 14.7261(14.5371) | Bit/dim 0.9815(0.9774) | Xent 0.0018(0.0003) | Loss 0.9824(0.9775) | Error 0.0011(0.0001) Steps 740(720.69) | Grad Norm 0.4640(2.7879) | Total Time 10.00(10.00)\n",
      "Iter 21500 | Time 14.4433(14.5163) | Bit/dim 0.9697(0.9770) | Xent 0.0001(0.0002) | Loss 0.9698(0.9772) | Error 0.0000(0.0001) Steps 698(719.36) | Grad Norm 5.7090(2.6708) | Total Time 10.00(10.00)\n",
      "Iter 21510 | Time 14.4595(14.5198) | Bit/dim 0.9575(0.9759) | Xent 0.0000(0.0002) | Loss 0.9575(0.9760) | Error 0.0000(0.0001) Steps 728(719.76) | Grad Norm 6.0237(3.6668) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0326 | Time 58.4922, Epoch Time 1028.9375(1029.2948), Bit/dim 0.9755(best: 0.9756), Xent 0.0409, Loss 0.9960, Error 0.0078(best: 0.0067)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 21520 | Time 14.3526(14.4960) | Bit/dim 0.9840(0.9767) | Xent 0.0000(0.0002) | Loss 0.9840(0.9767) | Error 0.0000(0.0000) Steps 716(717.41) | Grad Norm 1.9873(4.5774) | Total Time 10.00(10.00)\n",
      "Iter 21530 | Time 14.8590(14.4482) | Bit/dim 0.9770(0.9765) | Xent 0.0001(0.0001) | Loss 0.9771(0.9766) | Error 0.0000(0.0000) Steps 740(719.30) | Grad Norm 2.1001(4.8283) | Total Time 10.00(10.00)\n",
      "Iter 21540 | Time 14.0590(14.4451) | Bit/dim 0.9749(0.9767) | Xent 0.0001(0.0001) | Loss 0.9750(0.9767) | Error 0.0000(0.0000) Steps 716(719.84) | Grad Norm 2.8765(4.0337) | Total Time 10.00(10.00)\n",
      "Iter 21550 | Time 14.5157(14.4921) | Bit/dim 0.9806(0.9754) | Xent 0.0003(0.0001) | Loss 0.9807(0.9755) | Error 0.0000(0.0000) Steps 734(720.29) | Grad Norm 0.5052(3.6242) | Total Time 10.00(10.00)\n",
      "Iter 21560 | Time 14.6688(14.5345) | Bit/dim 0.9691(0.9758) | Xent 0.0001(0.0001) | Loss 0.9692(0.9758) | Error 0.0000(0.0000) Steps 728(721.25) | Grad Norm 1.8051(3.2811) | Total Time 10.00(10.00)\n",
      "Iter 21570 | Time 14.7260(14.5538) | Bit/dim 0.9920(0.9770) | Xent 0.0000(0.0001) | Loss 0.9920(0.9770) | Error 0.0000(0.0000) Steps 716(720.15) | Grad Norm 0.3759(2.8163) | Total Time 10.00(10.00)\n",
      "Iter 21580 | Time 14.6035(14.5651) | Bit/dim 0.9750(0.9764) | Xent 0.0000(0.0001) | Loss 0.9750(0.9765) | Error 0.0000(0.0000) Steps 728(717.12) | Grad Norm 1.5285(2.5207) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0327 | Time 57.8740, Epoch Time 1031.9852(1029.3755), Bit/dim 0.9753(best: 0.9755), Xent 0.0396, Loss 0.9952, Error 0.0081(best: 0.0067)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 21590 | Time 14.7404(14.5560) | Bit/dim 0.9722(0.9755) | Xent 0.0003(0.0001) | Loss 0.9723(0.9755) | Error 0.0000(0.0000) Steps 716(718.32) | Grad Norm 2.5448(2.3169) | Total Time 10.00(10.00)\n",
      "Iter 21600 | Time 14.0969(14.5375) | Bit/dim 0.9691(0.9756) | Xent 0.0000(0.0001) | Loss 0.9691(0.9757) | Error 0.0000(0.0000) Steps 722(716.33) | Grad Norm 2.2336(2.1770) | Total Time 10.00(10.00)\n",
      "Iter 21610 | Time 14.2499(14.5710) | Bit/dim 0.9800(0.9765) | Xent 0.0020(0.0003) | Loss 0.9810(0.9767) | Error 0.0011(0.0001) Steps 710(719.31) | Grad Norm 2.8126(2.0990) | Total Time 10.00(10.00)\n",
      "Iter 21620 | Time 14.3874(14.5250) | Bit/dim 0.9758(0.9772) | Xent 0.0000(0.0003) | Loss 0.9758(0.9773) | Error 0.0000(0.0001) Steps 710(718.30) | Grad Norm 7.6344(2.9462) | Total Time 10.00(10.00)\n",
      "Iter 21630 | Time 14.1892(14.5014) | Bit/dim 0.9923(0.9765) | Xent 0.0001(0.0003) | Loss 0.9923(0.9767) | Error 0.0000(0.0001) Steps 722(719.12) | Grad Norm 6.4703(4.1292) | Total Time 10.00(10.00)\n",
      "Iter 21640 | Time 14.7250(14.5473) | Bit/dim 0.9864(0.9776) | Xent 0.0000(0.0002) | Loss 0.9864(0.9777) | Error 0.0000(0.0001) Steps 704(717.66) | Grad Norm 5.6606(4.9592) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0328 | Time 57.8222, Epoch Time 1032.5463(1029.4706), Bit/dim 0.9755(best: 0.9753), Xent 0.0472, Loss 0.9991, Error 0.0081(best: 0.0067)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 21650 | Time 14.2839(14.5279) | Bit/dim 0.9672(0.9772) | Xent 0.0001(0.0002) | Loss 0.9673(0.9773) | Error 0.0000(0.0001) Steps 704(717.56) | Grad Norm 0.3412(4.6290) | Total Time 10.00(10.00)\n",
      "Iter 21660 | Time 14.4128(14.5689) | Bit/dim 0.9633(0.9764) | Xent 0.0001(0.0002) | Loss 0.9634(0.9765) | Error 0.0000(0.0001) Steps 716(716.07) | Grad Norm 0.3424(3.9305) | Total Time 10.00(10.00)\n",
      "Iter 21670 | Time 14.9908(14.6125) | Bit/dim 0.9748(0.9774) | Xent 0.0000(0.0002) | Loss 0.9748(0.9774) | Error 0.0000(0.0000) Steps 710(717.11) | Grad Norm 4.7209(3.4907) | Total Time 10.00(10.00)\n",
      "Iter 21680 | Time 14.6453(14.6575) | Bit/dim 0.9812(0.9776) | Xent 0.0000(0.0001) | Loss 0.9812(0.9777) | Error 0.0000(0.0000) Steps 728(718.79) | Grad Norm 2.7712(3.3386) | Total Time 10.00(10.00)\n",
      "Iter 21690 | Time 14.6757(14.5864) | Bit/dim 0.9707(0.9769) | Xent 0.0001(0.0001) | Loss 0.9707(0.9770) | Error 0.0000(0.0000) Steps 716(719.41) | Grad Norm 0.3649(2.8827) | Total Time 10.00(10.00)\n",
      "Iter 21700 | Time 14.6468(14.5942) | Bit/dim 0.9681(0.9756) | Xent 0.0002(0.0001) | Loss 0.9682(0.9757) | Error 0.0000(0.0000) Steps 746(720.31) | Grad Norm 2.9809(2.7509) | Total Time 10.00(10.00)\n",
      "Iter 21710 | Time 14.0502(14.5586) | Bit/dim 0.9880(0.9759) | Xent 0.0000(0.0002) | Loss 0.9880(0.9760) | Error 0.0000(0.0000) Steps 716(721.10) | Grad Norm 12.4622(4.0125) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0329 | Time 56.3286, Epoch Time 1034.5833(1029.6240), Bit/dim 0.9829(best: 0.9753), Xent 0.0407, Loss 1.0033, Error 0.0071(best: 0.0067)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 21720 | Time 14.2257(14.4953) | Bit/dim 0.9834(0.9783) | Xent 0.0000(0.0002) | Loss 0.9834(0.9784) | Error 0.0000(0.0000) Steps 716(719.63) | Grad Norm 8.5703(6.4590) | Total Time 10.00(10.00)\n",
      "Iter 21730 | Time 14.9643(14.4537) | Bit/dim 0.9750(0.9796) | Xent 0.0001(0.0003) | Loss 0.9750(0.9798) | Error 0.0000(0.0001) Steps 728(716.95) | Grad Norm 17.7171(8.2390) | Total Time 10.00(10.00)\n",
      "Iter 21740 | Time 14.8149(14.4564) | Bit/dim 0.9758(0.9785) | Xent 0.0000(0.0002) | Loss 0.9758(0.9787) | Error 0.0000(0.0001) Steps 722(718.57) | Grad Norm 4.8690(7.5003) | Total Time 10.00(10.00)\n",
      "Iter 21750 | Time 15.1121(14.5107) | Bit/dim 0.9729(0.9788) | Xent 0.0000(0.0002) | Loss 0.9729(0.9789) | Error 0.0000(0.0001) Steps 704(718.99) | Grad Norm 7.2514(6.9407) | Total Time 10.00(10.00)\n",
      "Iter 21760 | Time 13.9741(14.5032) | Bit/dim 0.9727(0.9776) | Xent 0.0000(0.0002) | Loss 0.9727(0.9777) | Error 0.0000(0.0000) Steps 728(721.24) | Grad Norm 1.0327(5.6187) | Total Time 10.00(10.00)\n",
      "Iter 21770 | Time 14.7499(14.5263) | Bit/dim 0.9750(0.9778) | Xent 0.0000(0.0002) | Loss 0.9750(0.9779) | Error 0.0000(0.0001) Steps 698(721.79) | Grad Norm 1.4470(4.6835) | Total Time 10.00(10.00)\n",
      "Iter 21780 | Time 15.1618(14.5373) | Bit/dim 0.9823(0.9766) | Xent 0.0000(0.0001) | Loss 0.9823(0.9767) | Error 0.0000(0.0000) Steps 698(721.26) | Grad Norm 1.0929(3.7464) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0330 | Time 57.2602, Epoch Time 1028.4320(1029.5882), Bit/dim 0.9748(best: 0.9753), Xent 0.0454, Loss 0.9976, Error 0.0071(best: 0.0067)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 21790 | Time 14.3214(14.5432) | Bit/dim 0.9870(0.9769) | Xent 0.0002(0.0001) | Loss 0.9871(0.9769) | Error 0.0000(0.0000) Steps 734(721.94) | Grad Norm 0.9526(3.0908) | Total Time 10.00(10.00)\n",
      "Iter 21800 | Time 14.9513(14.6155) | Bit/dim 0.9634(0.9773) | Xent 0.0000(0.0001) | Loss 0.9634(0.9773) | Error 0.0000(0.0000) Steps 764(725.58) | Grad Norm 1.6866(2.6588) | Total Time 10.00(10.00)\n",
      "Iter 21810 | Time 14.6454(14.6333) | Bit/dim 0.9622(0.9758) | Xent 0.0000(0.0001) | Loss 0.9622(0.9758) | Error 0.0000(0.0000) Steps 734(724.47) | Grad Norm 2.0495(2.4691) | Total Time 10.00(10.00)\n",
      "Iter 21820 | Time 14.6639(14.6368) | Bit/dim 0.9839(0.9760) | Xent 0.0000(0.0001) | Loss 0.9839(0.9761) | Error 0.0000(0.0000) Steps 734(723.21) | Grad Norm 2.2402(2.3355) | Total Time 10.00(10.00)\n",
      "Iter 21830 | Time 14.8149(14.6652) | Bit/dim 0.9779(0.9759) | Xent 0.0001(0.0001) | Loss 0.9780(0.9760) | Error 0.0000(0.0000) Steps 728(725.17) | Grad Norm 2.1616(2.1300) | Total Time 10.00(10.00)\n",
      "Iter 21840 | Time 14.2648(14.7033) | Bit/dim 0.9797(0.9760) | Xent 0.0001(0.0001) | Loss 0.9797(0.9760) | Error 0.0000(0.0000) Steps 686(723.50) | Grad Norm 2.3429(1.9785) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0331 | Time 58.5506, Epoch Time 1044.3185(1030.0301), Bit/dim 0.9750(best: 0.9748), Xent 0.0399, Loss 0.9949, Error 0.0077(best: 0.0067)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 21850 | Time 14.7879(14.6872) | Bit/dim 0.9887(0.9763) | Xent 0.0003(0.0001) | Loss 0.9888(0.9764) | Error 0.0000(0.0000) Steps 722(724.07) | Grad Norm 2.5418(1.8992) | Total Time 10.00(10.00)\n",
      "Iter 21860 | Time 14.6253(14.7044) | Bit/dim 0.9774(0.9760) | Xent 0.0000(0.0002) | Loss 0.9774(0.9761) | Error 0.0000(0.0000) Steps 728(723.53) | Grad Norm 0.4745(2.0538) | Total Time 10.00(10.00)\n",
      "Iter 21870 | Time 14.6406(14.6665) | Bit/dim 0.9890(0.9767) | Xent 0.0000(0.0001) | Loss 0.9890(0.9768) | Error 0.0000(0.0000) Steps 704(723.76) | Grad Norm 1.2791(1.8113) | Total Time 10.00(10.00)\n",
      "Iter 21880 | Time 14.3017(14.6901) | Bit/dim 0.9744(0.9760) | Xent 0.0002(0.0002) | Loss 0.9745(0.9761) | Error 0.0000(0.0001) Steps 728(725.08) | Grad Norm 5.5557(2.2085) | Total Time 10.00(10.00)\n",
      "Iter 21890 | Time 13.9419(14.6276) | Bit/dim 0.9748(0.9764) | Xent 0.0000(0.0001) | Loss 0.9748(0.9764) | Error 0.0000(0.0001) Steps 704(726.20) | Grad Norm 1.5704(2.7645) | Total Time 10.00(10.00)\n",
      "Iter 21900 | Time 14.8219(14.6396) | Bit/dim 0.9781(0.9760) | Xent 0.0001(0.0001) | Loss 0.9782(0.9761) | Error 0.0000(0.0000) Steps 716(724.53) | Grad Norm 2.2504(2.6812) | Total Time 10.00(10.00)\n",
      "Iter 21910 | Time 14.8497(14.6913) | Bit/dim 0.9767(0.9754) | Xent 0.0000(0.0001) | Loss 0.9767(0.9755) | Error 0.0000(0.0000) Steps 728(725.50) | Grad Norm 1.6836(2.3519) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0332 | Time 58.3034, Epoch Time 1042.0711(1030.3914), Bit/dim 0.9744(best: 0.9748), Xent 0.0406, Loss 0.9947, Error 0.0076(best: 0.0067)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 21920 | Time 15.0002(14.6938) | Bit/dim 0.9784(0.9750) | Xent 0.0000(0.0001) | Loss 0.9784(0.9751) | Error 0.0000(0.0000) Steps 746(724.73) | Grad Norm 6.4824(2.6375) | Total Time 10.00(10.00)\n",
      "Iter 21930 | Time 14.5754(14.6797) | Bit/dim 0.9779(0.9755) | Xent 0.0000(0.0001) | Loss 0.9779(0.9756) | Error 0.0000(0.0000) Steps 728(724.73) | Grad Norm 3.6700(2.7878) | Total Time 10.00(10.00)\n",
      "Iter 21940 | Time 14.8979(14.7068) | Bit/dim 0.9840(0.9762) | Xent 0.0000(0.0001) | Loss 0.9841(0.9763) | Error 0.0000(0.0000) Steps 716(725.85) | Grad Norm 0.8740(2.7819) | Total Time 10.00(10.00)\n",
      "Iter 21950 | Time 14.8670(14.6920) | Bit/dim 0.9754(0.9753) | Xent 0.0000(0.0001) | Loss 0.9754(0.9754) | Error 0.0000(0.0000) Steps 734(724.84) | Grad Norm 4.9139(2.8490) | Total Time 10.00(10.00)\n",
      "Iter 21960 | Time 14.3850(14.7348) | Bit/dim 0.9770(0.9749) | Xent 0.0000(0.0002) | Loss 0.9770(0.9750) | Error 0.0000(0.0000) Steps 716(724.75) | Grad Norm 5.1851(3.4632) | Total Time 10.00(10.00)\n",
      "Iter 21970 | Time 14.2224(14.6998) | Bit/dim 0.9708(0.9752) | Xent 0.0000(0.0002) | Loss 0.9708(0.9753) | Error 0.0000(0.0000) Steps 710(725.50) | Grad Norm 4.6610(3.8455) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0333 | Time 58.0502, Epoch Time 1042.9618(1030.7685), Bit/dim 0.9753(best: 0.9744), Xent 0.0392, Loss 0.9949, Error 0.0068(best: 0.0067)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 21980 | Time 14.5430(14.6798) | Bit/dim 0.9661(0.9758) | Xent 0.0004(0.0002) | Loss 0.9663(0.9759) | Error 0.0000(0.0000) Steps 710(724.30) | Grad Norm 5.4504(4.7325) | Total Time 10.00(10.00)\n",
      "Iter 21990 | Time 15.2733(14.7248) | Bit/dim 0.9803(0.9750) | Xent 0.0002(0.0001) | Loss 0.9804(0.9751) | Error 0.0000(0.0000) Steps 740(725.56) | Grad Norm 1.0232(3.9504) | Total Time 10.00(10.00)\n",
      "Iter 22000 | Time 14.9800(14.7680) | Bit/dim 0.9799(0.9755) | Xent 0.0000(0.0001) | Loss 0.9799(0.9755) | Error 0.0000(0.0000) Steps 722(726.01) | Grad Norm 1.8320(3.2745) | Total Time 10.00(10.00)\n",
      "Iter 22010 | Time 15.0845(14.7806) | Bit/dim 0.9746(0.9752) | Xent 0.0000(0.0001) | Loss 0.9746(0.9753) | Error 0.0000(0.0000) Steps 698(726.07) | Grad Norm 4.4851(3.3735) | Total Time 10.00(10.00)\n",
      "Iter 22020 | Time 14.5887(14.7643) | Bit/dim 0.9774(0.9743) | Xent 0.0000(0.0001) | Loss 0.9774(0.9744) | Error 0.0000(0.0000) Steps 746(726.86) | Grad Norm 2.4533(2.9322) | Total Time 10.00(10.00)\n",
      "Iter 22030 | Time 14.6078(14.7967) | Bit/dim 0.9751(0.9750) | Xent 0.0000(0.0001) | Loss 0.9751(0.9750) | Error 0.0000(0.0000) Steps 740(727.20) | Grad Norm 0.3998(2.7122) | Total Time 10.00(10.00)\n",
      "Iter 22040 | Time 14.6802(14.7566) | Bit/dim 0.9820(0.9763) | Xent 0.0000(0.0001) | Loss 0.9821(0.9763) | Error 0.0000(0.0000) Steps 692(727.24) | Grad Norm 8.9635(3.4996) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0334 | Time 59.1302, Epoch Time 1049.9753(1031.3447), Bit/dim 0.9755(best: 0.9744), Xent 0.0464, Loss 0.9986, Error 0.0080(best: 0.0067)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 22050 | Time 14.8689(14.7760) | Bit/dim 0.9721(0.9759) | Xent 0.0000(0.0002) | Loss 0.9721(0.9760) | Error 0.0000(0.0000) Steps 728(728.69) | Grad Norm 1.0311(3.9072) | Total Time 10.00(10.00)\n",
      "Iter 22060 | Time 14.6713(14.7892) | Bit/dim 0.9723(0.9767) | Xent 0.0000(0.0001) | Loss 0.9724(0.9768) | Error 0.0000(0.0000) Steps 710(727.37) | Grad Norm 9.8621(4.1959) | Total Time 10.00(10.00)\n",
      "Iter 22070 | Time 15.2186(14.7907) | Bit/dim 0.9807(0.9767) | Xent 0.0001(0.0001) | Loss 0.9808(0.9768) | Error 0.0000(0.0000) Steps 740(728.16) | Grad Norm 12.4470(5.1034) | Total Time 10.00(10.00)\n",
      "Iter 22080 | Time 15.1065(14.7742) | Bit/dim 0.9836(0.9767) | Xent 0.0001(0.0001) | Loss 0.9836(0.9768) | Error 0.0000(0.0000) Steps 740(729.79) | Grad Norm 4.4759(5.0842) | Total Time 10.00(10.00)\n",
      "Iter 22090 | Time 14.8567(14.8269) | Bit/dim 0.9768(0.9770) | Xent 0.0001(0.0001) | Loss 0.9768(0.9771) | Error 0.0000(0.0000) Steps 722(729.65) | Grad Norm 2.7325(4.3755) | Total Time 10.00(10.00)\n",
      "Iter 22100 | Time 14.9996(14.8490) | Bit/dim 0.9737(0.9762) | Xent 0.0000(0.0001) | Loss 0.9737(0.9762) | Error 0.0000(0.0000) Steps 722(727.94) | Grad Norm 3.4855(3.8914) | Total Time 10.00(10.00)\n",
      "Iter 22110 | Time 14.4549(14.7964) | Bit/dim 0.9715(0.9753) | Xent 0.0000(0.0002) | Loss 0.9715(0.9753) | Error 0.0000(0.0000) Steps 704(725.74) | Grad Norm 2.0466(3.5513) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0335 | Time 58.8550, Epoch Time 1052.2654(1031.9723), Bit/dim 0.9747(best: 0.9744), Xent 0.0410, Loss 0.9952, Error 0.0075(best: 0.0067)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 22120 | Time 14.8591(14.7801) | Bit/dim 0.9812(0.9759) | Xent 0.0001(0.0001) | Loss 0.9813(0.9760) | Error 0.0000(0.0000) Steps 728(726.66) | Grad Norm 6.9621(3.6774) | Total Time 10.00(10.00)\n",
      "Iter 22130 | Time 14.3490(14.7298) | Bit/dim 0.9792(0.9761) | Xent 0.0000(0.0001) | Loss 0.9792(0.9762) | Error 0.0000(0.0001) Steps 722(725.78) | Grad Norm 6.3968(4.4769) | Total Time 10.00(10.00)\n",
      "Iter 22140 | Time 15.0954(14.7676) | Bit/dim 0.9751(0.9764) | Xent 0.0000(0.0001) | Loss 0.9751(0.9765) | Error 0.0000(0.0000) Steps 710(726.59) | Grad Norm 2.1462(4.5051) | Total Time 10.00(10.00)\n",
      "Iter 22150 | Time 14.7710(14.7436) | Bit/dim 0.9785(0.9768) | Xent 0.0001(0.0001) | Loss 0.9785(0.9768) | Error 0.0000(0.0000) Steps 728(728.32) | Grad Norm 0.7969(3.6460) | Total Time 10.00(10.00)\n",
      "Iter 22160 | Time 14.8439(14.7502) | Bit/dim 0.9701(0.9758) | Xent 0.0000(0.0001) | Loss 0.9701(0.9759) | Error 0.0000(0.0000) Steps 722(726.26) | Grad Norm 2.6040(3.0574) | Total Time 10.00(10.00)\n",
      "Iter 22170 | Time 15.5149(14.8058) | Bit/dim 0.9747(0.9747) | Xent 0.0000(0.0001) | Loss 0.9747(0.9747) | Error 0.0000(0.0000) Steps 722(729.93) | Grad Norm 3.5162(2.7180) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0336 | Time 58.1514, Epoch Time 1049.3366(1032.4932), Bit/dim 0.9759(best: 0.9744), Xent 0.0437, Loss 0.9977, Error 0.0088(best: 0.0067)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 22180 | Time 14.0895(14.8152) | Bit/dim 0.9768(0.9755) | Xent 0.0000(0.0001) | Loss 0.9768(0.9755) | Error 0.0000(0.0000) Steps 722(727.02) | Grad Norm 7.7702(3.3303) | Total Time 10.00(10.00)\n",
      "Iter 22190 | Time 14.3966(14.7743) | Bit/dim 0.9723(0.9748) | Xent 0.0000(0.0001) | Loss 0.9723(0.9748) | Error 0.0000(0.0000) Steps 728(727.61) | Grad Norm 9.5958(4.5724) | Total Time 10.00(10.00)\n",
      "Iter 22200 | Time 14.2020(14.7108) | Bit/dim 0.9781(0.9761) | Xent 0.0003(0.0001) | Loss 0.9782(0.9761) | Error 0.0000(0.0000) Steps 704(726.40) | Grad Norm 6.9268(5.6716) | Total Time 10.00(10.00)\n",
      "Iter 22210 | Time 14.6594(14.6864) | Bit/dim 0.9782(0.9752) | Xent 0.0001(0.0001) | Loss 0.9783(0.9752) | Error 0.0000(0.0000) Steps 722(726.23) | Grad Norm 5.5714(5.7275) | Total Time 10.00(10.00)\n",
      "Iter 22220 | Time 14.4819(14.6911) | Bit/dim 0.9747(0.9754) | Xent 0.0000(0.0001) | Loss 0.9748(0.9755) | Error 0.0000(0.0000) Steps 704(724.66) | Grad Norm 8.0321(5.9568) | Total Time 10.00(10.00)\n",
      "Iter 22230 | Time 14.3827(14.7282) | Bit/dim 0.9818(0.9763) | Xent 0.0000(0.0002) | Loss 0.9818(0.9764) | Error 0.0000(0.0001) Steps 728(725.99) | Grad Norm 3.4300(5.7211) | Total Time 10.00(10.00)\n",
      "Iter 22240 | Time 15.0593(14.7616) | Bit/dim 0.9812(0.9759) | Xent 0.0000(0.0001) | Loss 0.9812(0.9760) | Error 0.0000(0.0000) Steps 740(726.98) | Grad Norm 1.3903(4.8885) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0337 | Time 59.0665, Epoch Time 1044.1222(1032.8421), Bit/dim 0.9743(best: 0.9744), Xent 0.0380, Loss 0.9933, Error 0.0076(best: 0.0067)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 22250 | Time 15.1035(14.7853) | Bit/dim 0.9729(0.9751) | Xent 0.0001(0.0001) | Loss 0.9729(0.9752) | Error 0.0000(0.0000) Steps 710(728.04) | Grad Norm 1.7149(3.9055) | Total Time 10.00(10.00)\n",
      "Iter 22260 | Time 14.6574(14.7727) | Bit/dim 0.9738(0.9750) | Xent 0.0000(0.0002) | Loss 0.9738(0.9751) | Error 0.0000(0.0001) Steps 728(729.06) | Grad Norm 0.4707(3.1362) | Total Time 10.00(10.00)\n",
      "Iter 22270 | Time 15.4845(14.8356) | Bit/dim 0.9744(0.9747) | Xent 0.0001(0.0002) | Loss 0.9745(0.9748) | Error 0.0000(0.0000) Steps 722(728.85) | Grad Norm 1.6136(2.6406) | Total Time 10.00(10.00)\n",
      "Iter 22280 | Time 14.8051(14.8468) | Bit/dim 0.9693(0.9750) | Xent 0.0000(0.0001) | Loss 0.9694(0.9751) | Error 0.0000(0.0001) Steps 716(728.79) | Grad Norm 2.6450(2.4269) | Total Time 10.00(10.00)\n",
      "Iter 22290 | Time 14.7384(14.8142) | Bit/dim 0.9754(0.9751) | Xent 0.0000(0.0001) | Loss 0.9754(0.9752) | Error 0.0000(0.0000) Steps 740(729.39) | Grad Norm 2.2429(2.1986) | Total Time 10.00(10.00)\n",
      "Iter 22300 | Time 14.7929(14.8617) | Bit/dim 0.9823(0.9752) | Xent 0.0000(0.0001) | Loss 0.9823(0.9752) | Error 0.0000(0.0000) Steps 734(730.30) | Grad Norm 1.8359(2.4525) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0338 | Time 58.3835, Epoch Time 1053.5961(1033.4647), Bit/dim 0.9762(best: 0.9743), Xent 0.0457, Loss 0.9991, Error 0.0086(best: 0.0067)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 22310 | Time 15.0222(14.8294) | Bit/dim 0.9754(0.9762) | Xent 0.0000(0.0001) | Loss 0.9754(0.9763) | Error 0.0000(0.0000) Steps 722(729.01) | Grad Norm 0.6228(3.8499) | Total Time 10.00(10.00)\n",
      "Iter 22320 | Time 14.7029(14.7875) | Bit/dim 0.9630(0.9767) | Xent 0.0000(0.0001) | Loss 0.9630(0.9768) | Error 0.0000(0.0000) Steps 746(728.33) | Grad Norm 5.3675(4.9286) | Total Time 10.00(10.00)\n",
      "Iter 22330 | Time 14.2616(14.8170) | Bit/dim 0.9560(0.9760) | Xent 0.0000(0.0001) | Loss 0.9560(0.9761) | Error 0.0000(0.0000) Steps 734(728.93) | Grad Norm 5.0317(5.4354) | Total Time 10.00(10.00)\n",
      "Iter 22340 | Time 14.6601(14.8171) | Bit/dim 0.9900(0.9762) | Xent 0.0000(0.0001) | Loss 0.9901(0.9763) | Error 0.0000(0.0000) Steps 752(729.24) | Grad Norm 3.0045(5.0930) | Total Time 10.00(10.00)\n",
      "Iter 22350 | Time 15.1610(14.8296) | Bit/dim 0.9847(0.9757) | Xent 0.0001(0.0001) | Loss 0.9847(0.9758) | Error 0.0000(0.0000) Steps 746(730.46) | Grad Norm 1.1447(4.2204) | Total Time 10.00(10.00)\n",
      "Iter 22360 | Time 14.5909(14.8045) | Bit/dim 0.9773(0.9750) | Xent 0.0000(0.0001) | Loss 0.9773(0.9750) | Error 0.0000(0.0000) Steps 746(733.91) | Grad Norm 1.8796(3.4700) | Total Time 10.00(10.00)\n",
      "Iter 22370 | Time 15.0180(14.8020) | Bit/dim 0.9709(0.9752) | Xent 0.0000(0.0001) | Loss 0.9709(0.9752) | Error 0.0000(0.0000) Steps 710(734.10) | Grad Norm 3.0671(3.0684) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0339 | Time 58.4505, Epoch Time 1051.2401(1033.9980), Bit/dim 0.9751(best: 0.9743), Xent 0.0354, Loss 0.9928, Error 0.0081(best: 0.0067)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 22380 | Time 14.8289(14.7982) | Bit/dim 0.9690(0.9751) | Xent 0.0000(0.0001) | Loss 0.9691(0.9751) | Error 0.0000(0.0000) Steps 740(732.96) | Grad Norm 11.9411(3.5707) | Total Time 10.00(10.00)\n",
      "Iter 22390 | Time 14.8773(14.8007) | Bit/dim 0.9756(0.9751) | Xent 0.0000(0.0001) | Loss 0.9756(0.9752) | Error 0.0000(0.0000) Steps 740(730.94) | Grad Norm 5.9863(4.0764) | Total Time 10.00(10.00)\n",
      "Iter 22400 | Time 14.0159(14.8472) | Bit/dim 0.9805(0.9755) | Xent 0.0000(0.0001) | Loss 0.9805(0.9755) | Error 0.0000(0.0000) Steps 722(728.98) | Grad Norm 6.8010(4.7832) | Total Time 10.00(10.00)\n",
      "Iter 22410 | Time 15.1188(14.8575) | Bit/dim 0.9755(0.9745) | Xent 0.0000(0.0001) | Loss 0.9755(0.9746) | Error 0.0000(0.0000) Steps 716(729.95) | Grad Norm 4.7718(4.7868) | Total Time 10.00(10.00)\n",
      "Iter 22420 | Time 15.1464(14.8675) | Bit/dim 0.9754(0.9744) | Xent 0.0001(0.0001) | Loss 0.9754(0.9744) | Error 0.0000(0.0000) Steps 704(728.51) | Grad Norm 4.6956(4.6268) | Total Time 10.00(10.00)\n",
      "Iter 22430 | Time 14.8295(14.9064) | Bit/dim 0.9677(0.9739) | Xent 0.0001(0.0002) | Loss 0.9678(0.9740) | Error 0.0000(0.0000) Steps 746(729.53) | Grad Norm 3.7113(5.0204) | Total Time 10.00(10.00)\n",
      "Iter 22440 | Time 14.3562(14.9066) | Bit/dim 0.9871(0.9768) | Xent 0.0002(0.0002) | Loss 0.9872(0.9769) | Error 0.0000(0.0001) Steps 740(730.98) | Grad Norm 1.3198(5.5672) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0340 | Time 60.0953, Epoch Time 1058.3270(1034.7278), Bit/dim 0.9763(best: 0.9743), Xent 0.0465, Loss 0.9996, Error 0.0087(best: 0.0067)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 22450 | Time 14.6494(14.8663) | Bit/dim 0.9642(0.9763) | Xent 0.0000(0.0002) | Loss 0.9642(0.9764) | Error 0.0000(0.0000) Steps 746(730.05) | Grad Norm 1.8647(5.4591) | Total Time 10.00(10.00)\n",
      "Iter 22460 | Time 14.5708(14.8540) | Bit/dim 0.9690(0.9758) | Xent 0.0000(0.0001) | Loss 0.9690(0.9758) | Error 0.0000(0.0000) Steps 740(731.75) | Grad Norm 2.0456(5.0555) | Total Time 10.00(10.00)\n",
      "Iter 22470 | Time 14.9049(14.8200) | Bit/dim 0.9712(0.9758) | Xent 0.0000(0.0001) | Loss 0.9712(0.9758) | Error 0.0000(0.0000) Steps 704(731.61) | Grad Norm 5.8251(4.8427) | Total Time 10.00(10.00)\n",
      "Iter 22480 | Time 15.5804(14.8331) | Bit/dim 0.9796(0.9750) | Xent 0.0000(0.0001) | Loss 0.9796(0.9751) | Error 0.0000(0.0000) Steps 710(731.52) | Grad Norm 1.5344(4.3369) | Total Time 10.00(10.00)\n",
      "Iter 22490 | Time 14.8115(14.8365) | Bit/dim 0.9841(0.9761) | Xent 0.0000(0.0002) | Loss 0.9842(0.9761) | Error 0.0000(0.0000) Steps 704(731.21) | Grad Norm 3.2476(4.1165) | Total Time 10.00(10.00)\n",
      "Iter 22500 | Time 15.5547(14.8421) | Bit/dim 0.9711(0.9760) | Xent 0.0000(0.0002) | Loss 0.9711(0.9761) | Error 0.0000(0.0001) Steps 746(732.63) | Grad Norm 3.5988(4.5489) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0341 | Time 59.5504, Epoch Time 1054.4025(1035.3181), Bit/dim 0.9750(best: 0.9743), Xent 0.0379, Loss 0.9939, Error 0.0078(best: 0.0067)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 22510 | Time 15.0924(14.9253) | Bit/dim 0.9766(0.9756) | Xent 0.0000(0.0002) | Loss 0.9766(0.9756) | Error 0.0000(0.0001) Steps 734(735.88) | Grad Norm 5.9376(4.5663) | Total Time 10.00(10.00)\n",
      "Iter 22520 | Time 14.4549(14.8856) | Bit/dim 0.9672(0.9756) | Xent 0.0000(0.0001) | Loss 0.9672(0.9757) | Error 0.0000(0.0001) Steps 734(735.38) | Grad Norm 1.0566(4.0833) | Total Time 10.00(10.00)\n",
      "Iter 22530 | Time 15.0122(14.8882) | Bit/dim 0.9772(0.9753) | Xent 0.0000(0.0003) | Loss 0.9772(0.9754) | Error 0.0000(0.0001) Steps 728(734.68) | Grad Norm 1.0337(3.4823) | Total Time 10.00(10.00)\n",
      "Iter 22540 | Time 14.6359(14.8609) | Bit/dim 0.9584(0.9743) | Xent 0.0002(0.0002) | Loss 0.9585(0.9744) | Error 0.0000(0.0001) Steps 722(733.82) | Grad Norm 3.4910(2.9428) | Total Time 10.00(10.00)\n",
      "Iter 22550 | Time 15.5091(14.8796) | Bit/dim 0.9906(0.9736) | Xent 0.0000(0.0002) | Loss 0.9906(0.9736) | Error 0.0000(0.0001) Steps 704(731.61) | Grad Norm 4.5572(2.8480) | Total Time 10.00(10.00)\n",
      "Iter 22560 | Time 14.7385(14.8045) | Bit/dim 0.9737(0.9745) | Xent 0.0000(0.0001) | Loss 0.9737(0.9746) | Error 0.0000(0.0000) Steps 722(732.67) | Grad Norm 4.3503(3.3808) | Total Time 10.00(10.00)\n",
      "Iter 22570 | Time 14.8141(14.7981) | Bit/dim 0.9757(0.9754) | Xent 0.0000(0.0001) | Loss 0.9757(0.9755) | Error 0.0000(0.0000) Steps 746(732.35) | Grad Norm 1.0395(4.2732) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0342 | Time 59.9684, Epoch Time 1052.7227(1035.8402), Bit/dim 0.9745(best: 0.9743), Xent 0.0398, Loss 0.9944, Error 0.0071(best: 0.0067)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 22580 | Time 15.2901(14.8028) | Bit/dim 0.9686(0.9754) | Xent 0.0000(0.0001) | Loss 0.9686(0.9754) | Error 0.0000(0.0001) Steps 710(730.25) | Grad Norm 8.3183(5.2992) | Total Time 10.00(10.00)\n",
      "Iter 22590 | Time 15.1031(14.8610) | Bit/dim 0.9742(0.9757) | Xent 0.0002(0.0001) | Loss 0.9743(0.9757) | Error 0.0000(0.0000) Steps 734(731.24) | Grad Norm 9.1629(5.3722) | Total Time 10.00(10.00)\n",
      "Iter 22600 | Time 15.0364(14.8348) | Bit/dim 0.9679(0.9751) | Xent 0.0000(0.0001) | Loss 0.9679(0.9751) | Error 0.0000(0.0000) Steps 734(731.37) | Grad Norm 6.8534(4.8481) | Total Time 10.00(10.00)\n",
      "Iter 22610 | Time 14.9284(14.8263) | Bit/dim 0.9699(0.9744) | Xent 0.0001(0.0001) | Loss 0.9700(0.9744) | Error 0.0000(0.0000) Steps 740(729.62) | Grad Norm 3.4388(5.0449) | Total Time 10.00(10.00)\n",
      "Iter 22620 | Time 14.8664(14.8150) | Bit/dim 0.9704(0.9746) | Xent 0.0014(0.0002) | Loss 0.9711(0.9747) | Error 0.0011(0.0000) Steps 740(728.89) | Grad Norm 1.7610(5.0822) | Total Time 10.00(10.00)\n",
      "Iter 22630 | Time 14.7309(14.8244) | Bit/dim 0.9898(0.9757) | Xent 0.0005(0.0001) | Loss 0.9900(0.9758) | Error 0.0000(0.0000) Steps 746(730.15) | Grad Norm 3.3279(4.5120) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0343 | Time 58.3421, Epoch Time 1053.8399(1036.3802), Bit/dim 0.9745(best: 0.9743), Xent 0.0420, Loss 0.9955, Error 0.0078(best: 0.0067)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 22640 | Time 14.9830(14.8571) | Bit/dim 0.9754(0.9751) | Xent 0.0001(0.0001) | Loss 0.9754(0.9752) | Error 0.0000(0.0000) Steps 716(730.73) | Grad Norm 2.6227(4.2984) | Total Time 10.00(10.00)\n",
      "Iter 22650 | Time 15.0510(14.8633) | Bit/dim 0.9812(0.9750) | Xent 0.0000(0.0001) | Loss 0.9812(0.9751) | Error 0.0000(0.0000) Steps 728(731.53) | Grad Norm 2.5348(4.0016) | Total Time 10.00(10.00)\n",
      "Iter 22660 | Time 14.5055(14.8474) | Bit/dim 0.9786(0.9752) | Xent 0.0006(0.0001) | Loss 0.9789(0.9753) | Error 0.0000(0.0000) Steps 746(733.80) | Grad Norm 0.4059(3.2453) | Total Time 10.00(10.00)\n",
      "Iter 22670 | Time 15.2955(14.8702) | Bit/dim 0.9704(0.9748) | Xent 0.0000(0.0002) | Loss 0.9704(0.9749) | Error 0.0000(0.0001) Steps 704(731.60) | Grad Norm 0.4404(2.6981) | Total Time 10.00(10.00)\n",
      "Iter 22680 | Time 14.2944(14.9090) | Bit/dim 0.9791(0.9748) | Xent 0.0000(0.0002) | Loss 0.9791(0.9748) | Error 0.0000(0.0000) Steps 734(728.49) | Grad Norm 2.4464(2.4344) | Total Time 10.00(10.00)\n",
      "Iter 22690 | Time 15.2226(14.8995) | Bit/dim 0.9784(0.9748) | Xent 0.0002(0.0002) | Loss 0.9785(0.9748) | Error 0.0000(0.0001) Steps 722(728.74) | Grad Norm 7.0652(3.0493) | Total Time 10.00(10.00)\n",
      "Iter 22700 | Time 14.9436(14.8834) | Bit/dim 0.9724(0.9746) | Xent 0.0000(0.0002) | Loss 0.9724(0.9747) | Error 0.0000(0.0001) Steps 740(729.78) | Grad Norm 4.3216(4.1052) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0344 | Time 59.2640, Epoch Time 1058.0264(1037.0296), Bit/dim 0.9736(best: 0.9743), Xent 0.0441, Loss 0.9957, Error 0.0075(best: 0.0067)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 22710 | Time 15.0814(14.8742) | Bit/dim 0.9716(0.9745) | Xent 0.0000(0.0001) | Loss 0.9716(0.9745) | Error 0.0000(0.0000) Steps 740(730.08) | Grad Norm 6.0095(4.6067) | Total Time 10.00(10.00)\n",
      "Iter 22720 | Time 15.5440(14.9185) | Bit/dim 0.9899(0.9752) | Xent 0.0000(0.0001) | Loss 0.9899(0.9753) | Error 0.0000(0.0000) Steps 734(731.93) | Grad Norm 3.2081(4.0072) | Total Time 10.00(10.00)\n",
      "Iter 22730 | Time 15.2215(14.9063) | Bit/dim 0.9696(0.9741) | Xent 0.0000(0.0001) | Loss 0.9696(0.9741) | Error 0.0000(0.0000) Steps 752(733.66) | Grad Norm 1.6773(3.5650) | Total Time 10.00(10.00)\n",
      "Iter 22740 | Time 15.4154(14.9116) | Bit/dim 0.9764(0.9741) | Xent 0.0002(0.0001) | Loss 0.9765(0.9742) | Error 0.0000(0.0001) Steps 728(733.33) | Grad Norm 1.8443(3.0920) | Total Time 10.00(10.00)\n",
      "Iter 22750 | Time 14.9127(14.9545) | Bit/dim 0.9722(0.9751) | Xent 0.0002(0.0001) | Loss 0.9723(0.9752) | Error 0.0000(0.0000) Steps 746(734.66) | Grad Norm 2.0716(2.9301) | Total Time 10.00(10.00)\n",
      "Iter 22760 | Time 14.7977(14.9216) | Bit/dim 0.9768(0.9758) | Xent 0.0000(0.0001) | Loss 0.9768(0.9759) | Error 0.0000(0.0000) Steps 734(733.93) | Grad Norm 3.6752(3.2855) | Total Time 10.00(10.00)\n",
      "Iter 22770 | Time 14.8298(14.9084) | Bit/dim 0.9716(0.9751) | Xent 0.0002(0.0001) | Loss 0.9717(0.9751) | Error 0.0000(0.0000) Steps 716(733.86) | Grad Norm 3.1844(2.8322) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0345 | Time 59.4760, Epoch Time 1058.9734(1037.6879), Bit/dim 0.9738(best: 0.9736), Xent 0.0399, Loss 0.9938, Error 0.0075(best: 0.0067)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 22780 | Time 14.4863(14.8573) | Bit/dim 0.9637(0.9758) | Xent 0.0000(0.0001) | Loss 0.9637(0.9758) | Error 0.0000(0.0000) Steps 698(732.52) | Grad Norm 0.4529(2.5534) | Total Time 10.00(10.00)\n",
      "Iter 22790 | Time 15.0021(14.8792) | Bit/dim 0.9837(0.9744) | Xent 0.0000(0.0001) | Loss 0.9837(0.9744) | Error 0.0000(0.0000) Steps 728(730.82) | Grad Norm 1.3925(2.3588) | Total Time 10.00(10.00)\n",
      "Iter 22800 | Time 15.1100(14.9076) | Bit/dim 0.9591(0.9745) | Xent 0.0000(0.0001) | Loss 0.9592(0.9745) | Error 0.0000(0.0000) Steps 740(731.98) | Grad Norm 0.6108(1.9717) | Total Time 10.00(10.00)\n",
      "Iter 22810 | Time 15.1732(14.8904) | Bit/dim 0.9701(0.9744) | Xent 0.0000(0.0001) | Loss 0.9701(0.9744) | Error 0.0000(0.0000) Steps 746(732.92) | Grad Norm 3.5938(2.0502) | Total Time 10.00(10.00)\n",
      "Iter 22820 | Time 14.1865(14.8346) | Bit/dim 0.9681(0.9739) | Xent 0.0001(0.0002) | Loss 0.9681(0.9740) | Error 0.0000(0.0001) Steps 728(733.53) | Grad Norm 10.6082(3.4754) | Total Time 10.00(10.00)\n",
      "Iter 22830 | Time 15.1120(14.8094) | Bit/dim 0.9828(0.9753) | Xent 0.0000(0.0002) | Loss 0.9828(0.9754) | Error 0.0000(0.0001) Steps 740(733.72) | Grad Norm 8.8550(5.0913) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0346 | Time 59.6268, Epoch Time 1053.0838(1038.1498), Bit/dim 0.9753(best: 0.9736), Xent 0.0414, Loss 0.9961, Error 0.0076(best: 0.0067)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 22840 | Time 14.6341(14.7967) | Bit/dim 0.9840(0.9762) | Xent 0.0001(0.0002) | Loss 0.9841(0.9763) | Error 0.0000(0.0001) Steps 716(733.22) | Grad Norm 15.1101(6.0784) | Total Time 10.00(10.00)\n",
      "Iter 22850 | Time 14.5735(14.7759) | Bit/dim 0.9876(0.9765) | Xent 0.0002(0.0002) | Loss 0.9877(0.9766) | Error 0.0000(0.0001) Steps 710(731.65) | Grad Norm 6.6447(5.8017) | Total Time 10.00(10.00)\n",
      "Iter 22860 | Time 14.4877(14.7667) | Bit/dim 0.9677(0.9756) | Xent 0.0000(0.0002) | Loss 0.9677(0.9757) | Error 0.0000(0.0001) Steps 728(732.81) | Grad Norm 3.2744(4.8714) | Total Time 10.00(10.00)\n",
      "Iter 22870 | Time 14.5961(14.8533) | Bit/dim 0.9693(0.9752) | Xent 0.0001(0.0002) | Loss 0.9694(0.9753) | Error 0.0000(0.0001) Steps 746(733.99) | Grad Norm 0.4866(4.0051) | Total Time 10.00(10.00)\n",
      "Iter 22880 | Time 14.8014(14.9063) | Bit/dim 0.9774(0.9751) | Xent 0.0000(0.0001) | Loss 0.9774(0.9751) | Error 0.0000(0.0000) Steps 746(734.76) | Grad Norm 2.1904(3.2303) | Total Time 10.00(10.00)\n",
      "Iter 22890 | Time 14.4206(14.9079) | Bit/dim 0.9800(0.9749) | Xent 0.0000(0.0001) | Loss 0.9800(0.9750) | Error 0.0000(0.0000) Steps 710(735.99) | Grad Norm 0.5706(2.7412) | Total Time 10.00(10.00)\n",
      "Iter 22900 | Time 15.6592(14.9150) | Bit/dim 0.9663(0.9741) | Xent 0.0000(0.0001) | Loss 0.9663(0.9742) | Error 0.0000(0.0000) Steps 740(733.86) | Grad Norm 0.4729(2.3517) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0347 | Time 58.6078, Epoch Time 1057.1414(1038.7195), Bit/dim 0.9735(best: 0.9736), Xent 0.0450, Loss 0.9960, Error 0.0081(best: 0.0067)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 22910 | Time 14.8131(14.9095) | Bit/dim 0.9821(0.9735) | Xent 0.0000(0.0001) | Loss 0.9821(0.9736) | Error 0.0000(0.0000) Steps 722(731.98) | Grad Norm 0.7445(2.0575) | Total Time 10.00(10.00)\n",
      "Iter 22920 | Time 14.7434(14.8705) | Bit/dim 0.9746(0.9736) | Xent 0.0001(0.0001) | Loss 0.9746(0.9737) | Error 0.0000(0.0000) Steps 740(733.64) | Grad Norm 0.7737(1.8787) | Total Time 10.00(10.00)\n",
      "Iter 22930 | Time 14.8738(14.9281) | Bit/dim 0.9964(0.9750) | Xent 0.0000(0.0001) | Loss 0.9964(0.9750) | Error 0.0000(0.0000) Steps 728(734.50) | Grad Norm 0.8799(1.7848) | Total Time 10.00(10.00)\n",
      "Iter 22940 | Time 14.5242(14.8748) | Bit/dim 0.9780(0.9739) | Xent 0.0000(0.0001) | Loss 0.9781(0.9740) | Error 0.0000(0.0000) Steps 728(730.28) | Grad Norm 1.8508(2.1270) | Total Time 10.00(10.00)\n",
      "Iter 22950 | Time 14.6309(14.8632) | Bit/dim 0.9729(0.9748) | Xent 0.0000(0.0001) | Loss 0.9729(0.9748) | Error 0.0000(0.0000) Steps 734(727.21) | Grad Norm 8.9073(4.0877) | Total Time 10.00(10.00)\n",
      "Iter 22960 | Time 15.0121(14.8941) | Bit/dim 0.9686(0.9746) | Xent 0.0000(0.0001) | Loss 0.9686(0.9747) | Error 0.0000(0.0000) Steps 746(727.45) | Grad Norm 7.3252(3.8461) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0348 | Time 58.5906, Epoch Time 1054.1781(1039.1833), Bit/dim 0.9742(best: 0.9735), Xent 0.0435, Loss 0.9959, Error 0.0072(best: 0.0067)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 22970 | Time 14.5599(14.8377) | Bit/dim 0.9865(0.9751) | Xent 0.0000(0.0001) | Loss 0.9865(0.9751) | Error 0.0000(0.0000) Steps 746(728.94) | Grad Norm 4.6369(5.1074) | Total Time 10.00(10.00)\n",
      "Iter 22980 | Time 14.5233(14.8113) | Bit/dim 0.9846(0.9757) | Xent 0.0000(0.0001) | Loss 0.9846(0.9757) | Error 0.0000(0.0000) Steps 716(728.11) | Grad Norm 8.4610(5.6509) | Total Time 10.00(10.00)\n",
      "Iter 22990 | Time 14.6176(14.8662) | Bit/dim 0.9771(0.9751) | Xent 0.0001(0.0001) | Loss 0.9772(0.9751) | Error 0.0000(0.0000) Steps 734(728.40) | Grad Norm 3.6358(5.6579) | Total Time 10.00(10.00)\n",
      "Iter 23000 | Time 14.7495(14.8681) | Bit/dim 0.9671(0.9735) | Xent 0.0007(0.0002) | Loss 0.9674(0.9735) | Error 0.0000(0.0000) Steps 734(730.30) | Grad Norm 0.5837(4.8034) | Total Time 10.00(10.00)\n",
      "Iter 23010 | Time 15.3113(14.9232) | Bit/dim 0.9711(0.9729) | Xent 0.0000(0.0001) | Loss 0.9711(0.9730) | Error 0.0000(0.0000) Steps 722(728.77) | Grad Norm 0.4974(3.7428) | Total Time 10.00(10.00)\n",
      "Iter 23020 | Time 14.8855(14.9347) | Bit/dim 0.9677(0.9738) | Xent 0.0001(0.0002) | Loss 0.9678(0.9738) | Error 0.0000(0.0000) Steps 728(728.69) | Grad Norm 0.4381(2.9636) | Total Time 10.00(10.00)\n",
      "Iter 23030 | Time 15.0856(14.8988) | Bit/dim 0.9644(0.9746) | Xent 0.0000(0.0001) | Loss 0.9644(0.9746) | Error 0.0000(0.0000) Steps 728(730.63) | Grad Norm 1.4774(2.6842) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0349 | Time 59.1917, Epoch Time 1057.1203(1039.7214), Bit/dim 0.9733(best: 0.9735), Xent 0.0397, Loss 0.9932, Error 0.0075(best: 0.0067)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 23040 | Time 14.8294(14.8792) | Bit/dim 0.9831(0.9752) | Xent 0.0001(0.0001) | Loss 0.9832(0.9752) | Error 0.0000(0.0000) Steps 740(732.35) | Grad Norm 2.5838(2.7715) | Total Time 10.00(10.00)\n",
      "Iter 23050 | Time 14.8575(14.8809) | Bit/dim 0.9885(0.9745) | Xent 0.0000(0.0001) | Loss 0.9886(0.9746) | Error 0.0000(0.0000) Steps 716(732.88) | Grad Norm 1.5443(2.5290) | Total Time 10.00(10.00)\n",
      "Iter 23060 | Time 15.0670(14.8656) | Bit/dim 0.9486(0.9736) | Xent 0.0000(0.0001) | Loss 0.9486(0.9737) | Error 0.0000(0.0000) Steps 734(733.08) | Grad Norm 2.3964(2.4313) | Total Time 10.00(10.00)\n",
      "Iter 23070 | Time 14.7398(14.8796) | Bit/dim 0.9795(0.9728) | Xent 0.0000(0.0001) | Loss 0.9796(0.9728) | Error 0.0000(0.0000) Steps 728(733.26) | Grad Norm 2.2075(2.3652) | Total Time 10.00(10.00)\n",
      "Iter 23080 | Time 14.7517(14.8862) | Bit/dim 0.9900(0.9741) | Xent 0.0000(0.0001) | Loss 0.9900(0.9741) | Error 0.0000(0.0000) Steps 734(731.47) | Grad Norm 9.6305(3.1633) | Total Time 10.00(10.00)\n",
      "Iter 23090 | Time 14.5565(14.8324) | Bit/dim 0.9855(0.9772) | Xent 0.0000(0.0002) | Loss 0.9856(0.9773) | Error 0.0000(0.0001) Steps 728(728.04) | Grad Norm 8.7977(5.8460) | Total Time 10.00(10.00)\n",
      "Iter 23100 | Time 14.0382(14.7795) | Bit/dim 0.9908(0.9810) | Xent 0.0002(0.0002) | Loss 0.9909(0.9811) | Error 0.0000(0.0001) Steps 704(727.75) | Grad Norm 9.6398(8.2800) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0350 | Time 59.5188, Epoch Time 1053.0851(1040.1223), Bit/dim 0.9814(best: 0.9733), Xent 0.0374, Loss 1.0001, Error 0.0073(best: 0.0067)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 23110 | Time 14.3687(14.7475) | Bit/dim 0.9843(0.9815) | Xent 0.0000(0.0002) | Loss 0.9843(0.9816) | Error 0.0000(0.0001) Steps 728(726.87) | Grad Norm 8.9316(9.1640) | Total Time 10.00(10.00)\n",
      "Iter 23120 | Time 14.8047(14.7350) | Bit/dim 0.9762(0.9800) | Xent 0.0001(0.0001) | Loss 0.9762(0.9801) | Error 0.0000(0.0000) Steps 704(727.94) | Grad Norm 5.8039(8.5218) | Total Time 10.00(10.00)\n",
      "Iter 23130 | Time 14.7612(14.6805) | Bit/dim 0.9737(0.9785) | Xent 0.0000(0.0001) | Loss 0.9737(0.9785) | Error 0.0000(0.0000) Steps 722(727.93) | Grad Norm 6.9732(7.5439) | Total Time 10.00(10.00)\n",
      "Iter 23140 | Time 14.7623(14.7214) | Bit/dim 0.9682(0.9782) | Xent 0.0000(0.0001) | Loss 0.9682(0.9783) | Error 0.0000(0.0000) Steps 740(727.75) | Grad Norm 2.3998(6.2798) | Total Time 10.00(10.00)\n",
      "Iter 23150 | Time 14.1898(14.7265) | Bit/dim 0.9609(0.9765) | Xent 0.0000(0.0002) | Loss 0.9610(0.9766) | Error 0.0000(0.0000) Steps 734(729.72) | Grad Norm 2.5781(5.1880) | Total Time 10.00(10.00)\n",
      "Iter 23160 | Time 14.9454(14.7609) | Bit/dim 0.9930(0.9765) | Xent 0.0002(0.0002) | Loss 0.9931(0.9766) | Error 0.0000(0.0001) Steps 746(730.44) | Grad Norm 1.8253(4.3140) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0351 | Time 59.3990, Epoch Time 1047.7840(1040.3522), Bit/dim 0.9739(best: 0.9733), Xent 0.0437, Loss 0.9958, Error 0.0078(best: 0.0067)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 23170 | Time 15.0668(14.7726) | Bit/dim 0.9634(0.9769) | Xent 0.0007(0.0002) | Loss 0.9638(0.9770) | Error 0.0000(0.0000) Steps 740(728.94) | Grad Norm 1.7861(3.5408) | Total Time 10.00(10.00)\n",
      "Iter 23180 | Time 14.2997(14.7635) | Bit/dim 0.9771(0.9769) | Xent 0.0000(0.0002) | Loss 0.9772(0.9770) | Error 0.0000(0.0000) Steps 728(730.72) | Grad Norm 2.5005(3.1454) | Total Time 10.00(10.00)\n",
      "Iter 23190 | Time 15.0276(14.7667) | Bit/dim 0.9686(0.9759) | Xent 0.0003(0.0002) | Loss 0.9687(0.9760) | Error 0.0000(0.0000) Steps 728(731.52) | Grad Norm 0.9161(2.9028) | Total Time 10.00(10.00)\n",
      "Iter 23200 | Time 14.2502(14.7503) | Bit/dim 0.9759(0.9739) | Xent 0.0000(0.0002) | Loss 0.9759(0.9740) | Error 0.0000(0.0000) Steps 716(728.56) | Grad Norm 2.6663(2.6206) | Total Time 10.00(10.00)\n",
      "Iter 23210 | Time 14.8705(14.7341) | Bit/dim 0.9799(0.9745) | Xent 0.0001(0.0001) | Loss 0.9799(0.9746) | Error 0.0000(0.0000) Steps 734(729.98) | Grad Norm 2.6900(2.6593) | Total Time 10.00(10.00)\n",
      "Iter 23220 | Time 15.0232(14.7559) | Bit/dim 0.9645(0.9738) | Xent 0.0000(0.0001) | Loss 0.9645(0.9739) | Error 0.0000(0.0000) Steps 740(728.64) | Grad Norm 0.3796(2.6209) | Total Time 10.00(10.00)\n",
      "Iter 23230 | Time 14.6646(14.7380) | Bit/dim 0.9752(0.9739) | Xent 0.0012(0.0001) | Loss 0.9758(0.9739) | Error 0.0000(0.0000) Steps 740(730.05) | Grad Norm 1.2775(2.8278) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0352 | Time 59.0050, Epoch Time 1047.0898(1040.5543), Bit/dim 0.9742(best: 0.9733), Xent 0.0402, Loss 0.9943, Error 0.0072(best: 0.0067)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 23240 | Time 15.0472(14.7332) | Bit/dim 0.9616(0.9735) | Xent 0.0027(0.0002) | Loss 0.9629(0.9736) | Error 0.0011(0.0001) Steps 740(732.33) | Grad Norm 9.3477(3.0202) | Total Time 10.00(10.00)\n",
      "Iter 23250 | Time 14.7507(14.7649) | Bit/dim 0.9543(0.9727) | Xent 0.0000(0.0002) | Loss 0.9543(0.9728) | Error 0.0000(0.0001) Steps 722(732.99) | Grad Norm 3.9380(3.1951) | Total Time 10.00(10.00)\n",
      "Iter 23260 | Time 15.7343(14.8109) | Bit/dim 0.9764(0.9738) | Xent 0.0001(0.0003) | Loss 0.9764(0.9739) | Error 0.0000(0.0001) Steps 704(732.32) | Grad Norm 1.8107(2.7840) | Total Time 10.00(10.00)\n",
      "Iter 23270 | Time 14.8995(14.8250) | Bit/dim 0.9803(0.9747) | Xent 0.0000(0.0002) | Loss 0.9803(0.9748) | Error 0.0000(0.0001) Steps 716(731.72) | Grad Norm 4.0047(2.7965) | Total Time 10.00(10.00)\n",
      "Iter 23280 | Time 14.5317(14.7840) | Bit/dim 0.9769(0.9744) | Xent 0.0001(0.0002) | Loss 0.9770(0.9745) | Error 0.0000(0.0001) Steps 716(732.28) | Grad Norm 1.6485(2.7056) | Total Time 10.00(10.00)\n",
      "Iter 23290 | Time 15.1243(14.7840) | Bit/dim 0.9725(0.9744) | Xent 0.0000(0.0002) | Loss 0.9725(0.9745) | Error 0.0000(0.0001) Steps 740(730.97) | Grad Norm 7.6659(3.1526) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0353 | Time 59.0396, Epoch Time 1049.6628(1040.8276), Bit/dim 0.9735(best: 0.9733), Xent 0.0407, Loss 0.9938, Error 0.0080(best: 0.0067)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 23300 | Time 14.2907(14.7577) | Bit/dim 0.9637(0.9747) | Xent 0.0000(0.0001) | Loss 0.9637(0.9748) | Error 0.0000(0.0000) Steps 728(728.04) | Grad Norm 0.9666(2.9886) | Total Time 10.00(10.00)\n",
      "Iter 23310 | Time 14.6899(14.7679) | Bit/dim 0.9774(0.9740) | Xent 0.0019(0.0002) | Loss 0.9783(0.9740) | Error 0.0011(0.0001) Steps 692(727.64) | Grad Norm 0.5618(3.1610) | Total Time 10.00(10.00)\n",
      "Iter 23320 | Time 15.1124(14.7424) | Bit/dim 0.9662(0.9746) | Xent 0.0000(0.0002) | Loss 0.9662(0.9747) | Error 0.0000(0.0001) Steps 728(727.30) | Grad Norm 0.6807(2.9003) | Total Time 10.00(10.00)\n",
      "Iter 23330 | Time 14.0606(14.7058) | Bit/dim 0.9633(0.9750) | Xent 0.0000(0.0002) | Loss 0.9633(0.9751) | Error 0.0000(0.0001) Steps 722(728.13) | Grad Norm 1.4214(2.4688) | Total Time 10.00(10.00)\n",
      "Iter 23340 | Time 15.0152(14.7221) | Bit/dim 0.9809(0.9743) | Xent 0.0005(0.0002) | Loss 0.9811(0.9744) | Error 0.0000(0.0001) Steps 734(729.45) | Grad Norm 2.1176(2.1430) | Total Time 10.00(10.00)\n",
      "Iter 23350 | Time 14.5585(14.7740) | Bit/dim 0.9851(0.9744) | Xent 0.0000(0.0002) | Loss 0.9851(0.9745) | Error 0.0000(0.0000) Steps 728(732.66) | Grad Norm 1.2519(2.0265) | Total Time 10.00(10.00)\n",
      "Iter 23360 | Time 15.4025(14.7606) | Bit/dim 0.9823(0.9747) | Xent 0.0000(0.0003) | Loss 0.9823(0.9748) | Error 0.0000(0.0001) Steps 704(731.39) | Grad Norm 2.2331(1.9698) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0354 | Time 58.8715, Epoch Time 1049.0759(1041.0750), Bit/dim 0.9734(best: 0.9733), Xent 0.0418, Loss 0.9943, Error 0.0078(best: 0.0067)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 23370 | Time 14.7867(14.8105) | Bit/dim 0.9906(0.9743) | Xent 0.0000(0.0003) | Loss 0.9906(0.9744) | Error 0.0000(0.0001) Steps 722(730.21) | Grad Norm 2.4654(1.7711) | Total Time 10.00(10.00)\n",
      "Iter 23380 | Time 14.7565(14.7994) | Bit/dim 0.9657(0.9732) | Xent 0.0002(0.0004) | Loss 0.9658(0.9734) | Error 0.0000(0.0001) Steps 722(728.99) | Grad Norm 5.4436(2.0749) | Total Time 10.00(10.00)\n",
      "Iter 23390 | Time 14.8805(14.8070) | Bit/dim 0.9670(0.9739) | Xent 0.0000(0.0003) | Loss 0.9670(0.9741) | Error 0.0000(0.0001) Steps 704(728.49) | Grad Norm 4.6667(3.6975) | Total Time 10.00(10.00)\n",
      "Iter 23400 | Time 14.3367(14.7829) | Bit/dim 0.9792(0.9745) | Xent 0.0000(0.0003) | Loss 0.9792(0.9746) | Error 0.0000(0.0001) Steps 722(726.73) | Grad Norm 3.0426(3.6756) | Total Time 10.00(10.00)\n",
      "Iter 23410 | Time 14.5342(14.7879) | Bit/dim 0.9877(0.9742) | Xent 0.0000(0.0002) | Loss 0.9877(0.9743) | Error 0.0000(0.0001) Steps 740(727.03) | Grad Norm 2.1163(3.3973) | Total Time 10.00(10.00)\n",
      "Iter 23420 | Time 14.8763(14.8078) | Bit/dim 0.9601(0.9745) | Xent 0.0001(0.0002) | Loss 0.9601(0.9747) | Error 0.0000(0.0001) Steps 722(728.25) | Grad Norm 1.4092(3.0724) | Total Time 10.00(10.00)\n",
      "Iter 23430 | Time 15.0830(14.7884) | Bit/dim 0.9747(0.9749) | Xent 0.0000(0.0002) | Loss 0.9748(0.9751) | Error 0.0000(0.0001) Steps 710(727.70) | Grad Norm 1.2136(2.7636) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0355 | Time 58.8176, Epoch Time 1049.7663(1041.3357), Bit/dim 0.9744(best: 0.9733), Xent 0.0452, Loss 0.9970, Error 0.0075(best: 0.0067)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 23440 | Time 14.7245(14.7941) | Bit/dim 0.9742(0.9748) | Xent 0.0001(0.0002) | Loss 0.9742(0.9749) | Error 0.0000(0.0001) Steps 734(725.64) | Grad Norm 13.1008(3.6197) | Total Time 10.00(10.00)\n",
      "Iter 23450 | Time 15.2014(14.8356) | Bit/dim 0.9861(0.9756) | Xent 0.0000(0.0002) | Loss 0.9861(0.9757) | Error 0.0000(0.0000) Steps 752(724.62) | Grad Norm 6.9384(4.6637) | Total Time 10.00(10.00)\n",
      "Iter 23460 | Time 15.3208(14.8077) | Bit/dim 0.9702(0.9745) | Xent 0.0000(0.0001) | Loss 0.9702(0.9746) | Error 0.0000(0.0000) Steps 728(723.61) | Grad Norm 6.4127(5.1908) | Total Time 10.00(10.00)\n",
      "Iter 23470 | Time 14.9949(14.8410) | Bit/dim 0.9658(0.9753) | Xent 0.0001(0.0002) | Loss 0.9658(0.9754) | Error 0.0000(0.0001) Steps 686(723.61) | Grad Norm 7.1564(5.7385) | Total Time 10.00(10.00)\n",
      "Iter 23480 | Time 14.9401(14.8441) | Bit/dim 0.9784(0.9741) | Xent 0.0001(0.0001) | Loss 0.9785(0.9742) | Error 0.0000(0.0000) Steps 722(725.22) | Grad Norm 6.6108(5.7097) | Total Time 10.00(10.00)\n",
      "Iter 23490 | Time 15.4347(14.9022) | Bit/dim 0.9713(0.9745) | Xent 0.0000(0.0001) | Loss 0.9713(0.9746) | Error 0.0000(0.0000) Steps 692(725.24) | Grad Norm 4.5507(5.5176) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0356 | Time 58.9508, Epoch Time 1056.1662(1041.7807), Bit/dim 0.9739(best: 0.9733), Xent 0.0458, Loss 0.9968, Error 0.0088(best: 0.0067)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 23500 | Time 14.4136(14.8800) | Bit/dim 0.9762(0.9759) | Xent 0.0001(0.0001) | Loss 0.9763(0.9760) | Error 0.0000(0.0000) Steps 728(726.74) | Grad Norm 2.6411(4.9735) | Total Time 10.00(10.00)\n",
      "Iter 23510 | Time 15.5658(14.9508) | Bit/dim 0.9719(0.9757) | Xent 0.0000(0.0002) | Loss 0.9719(0.9758) | Error 0.0000(0.0000) Steps 752(729.92) | Grad Norm 1.7641(4.1997) | Total Time 10.00(10.00)\n",
      "Iter 23520 | Time 14.8116(14.9694) | Bit/dim 0.9711(0.9755) | Xent 0.0004(0.0002) | Loss 0.9713(0.9756) | Error 0.0000(0.0000) Steps 734(730.98) | Grad Norm 1.8121(3.3250) | Total Time 10.00(10.00)\n",
      "Iter 23530 | Time 14.2473(14.9977) | Bit/dim 0.9666(0.9746) | Xent 0.0001(0.0002) | Loss 0.9666(0.9747) | Error 0.0000(0.0000) Steps 734(729.72) | Grad Norm 3.0447(2.8837) | Total Time 10.00(10.00)\n",
      "Iter 23540 | Time 14.8692(15.0342) | Bit/dim 0.9742(0.9736) | Xent 0.0001(0.0001) | Loss 0.9742(0.9737) | Error 0.0000(0.0000) Steps 752(732.62) | Grad Norm 5.8714(3.6195) | Total Time 10.00(10.00)\n",
      "Iter 23550 | Time 15.5531(15.0221) | Bit/dim 0.9703(0.9740) | Xent 0.0000(0.0003) | Loss 0.9703(0.9741) | Error 0.0000(0.0001) Steps 758(732.99) | Grad Norm 3.2731(3.9238) | Total Time 10.00(10.00)\n",
      "Iter 23560 | Time 15.4218(15.0810) | Bit/dim 0.9749(0.9746) | Xent 0.0000(0.0003) | Loss 0.9749(0.9748) | Error 0.0000(0.0001) Steps 746(731.33) | Grad Norm 2.9791(3.5117) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0357 | Time 59.4227, Epoch Time 1069.5568(1042.6139), Bit/dim 0.9738(best: 0.9733), Xent 0.0382, Loss 0.9929, Error 0.0080(best: 0.0067)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 23570 | Time 14.8551(15.0620) | Bit/dim 0.9762(0.9736) | Xent 0.0000(0.0002) | Loss 0.9762(0.9737) | Error 0.0000(0.0001) Steps 734(730.68) | Grad Norm 2.9887(3.1669) | Total Time 10.00(10.00)\n",
      "Iter 23580 | Time 15.0852(15.1762) | Bit/dim 0.9670(0.9737) | Xent 0.0001(0.0002) | Loss 0.9670(0.9738) | Error 0.0000(0.0001) Steps 740(732.09) | Grad Norm 3.8806(2.8694) | Total Time 10.00(10.00)\n",
      "Iter 23590 | Time 15.1479(15.2296) | Bit/dim 0.9809(0.9748) | Xent 0.0000(0.0002) | Loss 0.9809(0.9749) | Error 0.0000(0.0001) Steps 746(730.99) | Grad Norm 2.2908(2.7573) | Total Time 10.00(10.00)\n",
      "Iter 23600 | Time 15.4498(15.2386) | Bit/dim 0.9683(0.9736) | Xent 0.0000(0.0002) | Loss 0.9683(0.9737) | Error 0.0000(0.0001) Steps 746(733.04) | Grad Norm 3.3118(2.8109) | Total Time 10.00(10.00)\n",
      "Iter 23610 | Time 15.3935(15.2993) | Bit/dim 0.9752(0.9748) | Xent 0.0000(0.0002) | Loss 0.9752(0.9748) | Error 0.0000(0.0001) Steps 728(732.93) | Grad Norm 4.6737(3.0836) | Total Time 10.00(10.00)\n",
      "Iter 23620 | Time 14.7795(15.2259) | Bit/dim 0.9809(0.9746) | Xent 0.0001(0.0001) | Loss 0.9810(0.9747) | Error 0.0000(0.0001) Steps 722(734.07) | Grad Norm 1.4225(2.9685) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0358 | Time 58.8834, Epoch Time 1082.3672(1043.8065), Bit/dim 0.9737(best: 0.9733), Xent 0.0461, Loss 0.9967, Error 0.0085(best: 0.0067)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 23630 | Time 15.2272(15.2067) | Bit/dim 0.9684(0.9729) | Xent 0.0000(0.0002) | Loss 0.9684(0.9730) | Error 0.0000(0.0001) Steps 722(733.94) | Grad Norm 0.4971(2.5723) | Total Time 10.00(10.00)\n",
      "Iter 23640 | Time 15.5362(15.2249) | Bit/dim 0.9642(0.9737) | Xent 0.0000(0.0002) | Loss 0.9642(0.9738) | Error 0.0000(0.0000) Steps 740(735.01) | Grad Norm 0.9605(2.1794) | Total Time 10.00(10.00)\n",
      "Iter 23650 | Time 14.8046(15.2487) | Bit/dim 0.9786(0.9742) | Xent 0.0000(0.0001) | Loss 0.9786(0.9743) | Error 0.0000(0.0000) Steps 704(732.58) | Grad Norm 0.7929(1.8164) | Total Time 10.00(10.00)\n",
      "Iter 23660 | Time 15.2413(15.2178) | Bit/dim 0.9843(0.9744) | Xent 0.0000(0.0001) | Loss 0.9843(0.9744) | Error 0.0000(0.0000) Steps 752(735.01) | Grad Norm 0.7145(1.9275) | Total Time 10.00(10.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_drop.py --data mnist --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/cnf_conditional_bs900_drop_0_5_run1 --seed 1 --lr 0.001 --conditional True --controlled_tol False --train_mode semisup --log_freq 10 --weight_y 0.5 --dropout_rate 0.5\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
