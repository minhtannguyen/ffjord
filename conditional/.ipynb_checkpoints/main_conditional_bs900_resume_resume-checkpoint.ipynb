{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0,1,2,3,4,5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_conditional.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.odenvp_conditional as odenvp\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=True, choices=[True, False])\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"./data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"./data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"./data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"./data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"./data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"./data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            './data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            './data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz = modules.GaussianDiag.logp(mean, logs, z).view(-1,1)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(z)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol},)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    # fixed_y = torch.randint(high=10, size=(100,)).type(torch.long).to(device, non_blocking=True)\n",
      "    fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "    fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "    with torch.no_grad():\n",
      "        mean, logs = model.module._prior(fixed_y_onehot)\n",
      "        fixed_z = modules.GaussianDiag.sample(mean, logs)\n",
      "    \n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    if args.conditional: best_error_score = float(\"inf\")\n",
      "    \n",
      "    itr = 0\n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                loss =  loss_nll + args.weight_y * loss_xent\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            loss.backward()\n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            xent_meter.update(loss_xent.item())\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits/dim', {'train': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses_xent.append(loss_xent.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'validation': time.time() - start}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits/dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}, Xent {:.4f}, Loss {:.4f}, Error {:.4f}\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, loss_xent, loss, error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, conditional=True, conv=True, data='mnist', dims='64,64,64', divergence_fn='approximate', dl2int=None, imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.0001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume='experiments/cnf_cond_bs900_wy_0_5/best_nll_checkpt.pth', rtol=1e-05, save='experiments/cnf_cond_bs900_wy_0_5_cont_lr_0_0001', solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=900, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5)\n",
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=1568, bias=True)\n",
      "  (project_class): LinearZeros(in_features=784, out_features=10, bias=True)\n",
      ")\n",
      "Number of trainable parameters: 828890\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc.py:286: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "Iter 0000 | Time 41.3508(41.3508) | Bit/dim 1.1103(1.1103) | Xent 0.0230(0.0230) | Loss 1.1218(1.1218) | Error 0.0067(0.0067) Steps 548(548.00) | Grad Norm 1.8417(1.8417) | Total Time 10.00(10.00)\n",
      "Iter 0010 | Time 16.8160(34.8849) | Bit/dim 1.1462(1.1138) | Xent 0.0112(0.0222) | Loss 1.1517(1.1249) | Error 0.0033(0.0069) Steps 536(546.27) | Grad Norm 2.0435(1.8731) | Total Time 10.00(10.00)\n",
      "Iter 0020 | Time 16.7557(30.0962) | Bit/dim 1.1195(1.1142) | Xent 0.0290(0.0233) | Loss 1.1341(1.1259) | Error 0.0089(0.0073) Steps 548(545.13) | Grad Norm 1.6058(1.8633) | Total Time 10.00(10.00)\n",
      "Iter 0030 | Time 16.6893(26.5374) | Bit/dim 1.1150(1.1147) | Xent 0.0297(0.0235) | Loss 1.1298(1.1265) | Error 0.0111(0.0076) Steps 542(544.94) | Grad Norm 1.5235(1.7840) | Total Time 10.00(10.00)\n",
      "Iter 0040 | Time 16.7718(23.9356) | Bit/dim 1.1147(1.1161) | Xent 0.0203(0.0232) | Loss 1.1249(1.1277) | Error 0.0078(0.0079) Steps 542(544.19) | Grad Norm 1.0402(1.6585) | Total Time 10.00(10.00)\n",
      "Iter 0050 | Time 16.5519(22.0044) | Bit/dim 1.1265(1.1167) | Xent 0.0262(0.0216) | Loss 1.1396(1.1275) | Error 0.0089(0.0072) Steps 548(543.82) | Grad Norm 0.8292(1.4622) | Total Time 10.00(10.00)\n",
      "Iter 0060 | Time 16.4402(20.5915) | Bit/dim 1.1039(1.1150) | Xent 0.0180(0.0210) | Loss 1.1129(1.1255) | Error 0.0089(0.0071) Steps 542(543.76) | Grad Norm 0.7860(1.2927) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0001 | Time 67.3490, Epoch Time 1201.9940(1201.9940), Bit/dim 1.1076, Xent 0.0342, Loss 1.1247, Error 0.0206\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0070 | Time 16.7321(19.5552) | Bit/dim 1.1260(1.1149) | Xent 0.0167(0.0202) | Loss 1.1344(1.1250) | Error 0.0022(0.0067) Steps 548(543.97) | Grad Norm 0.6364(1.1535) | Total Time 10.00(10.00)\n",
      "Iter 0080 | Time 16.8505(18.8374) | Bit/dim 1.1083(1.1146) | Xent 0.0184(0.0201) | Loss 1.1175(1.1246) | Error 0.0067(0.0067) Steps 542(544.04) | Grad Norm 0.5791(1.0211) | Total Time 10.00(10.00)\n",
      "Iter 0090 | Time 16.4947(18.2583) | Bit/dim 1.1059(1.1134) | Xent 0.0157(0.0191) | Loss 1.1138(1.1230) | Error 0.0078(0.0064) Steps 542(543.20) | Grad Norm 0.7620(0.9192) | Total Time 10.00(10.00)\n",
      "Iter 0100 | Time 16.5041(17.8453) | Bit/dim 1.1250(1.1132) | Xent 0.0301(0.0200) | Loss 1.1400(1.1232) | Error 0.0100(0.0067) Steps 542(543.08) | Grad Norm 0.6984(0.8428) | Total Time 10.00(10.00)\n",
      "Iter 0110 | Time 16.8095(17.5436) | Bit/dim 1.1134(1.1136) | Xent 0.0303(0.0203) | Loss 1.1286(1.1238) | Error 0.0078(0.0067) Steps 548(543.63) | Grad Norm 0.6967(0.7843) | Total Time 10.00(10.00)\n",
      "Iter 0120 | Time 16.7651(17.3388) | Bit/dim 1.1084(1.1139) | Xent 0.0231(0.0208) | Loss 1.1199(1.1243) | Error 0.0067(0.0068) Steps 542(543.79) | Grad Norm 0.7292(0.7479) | Total Time 10.00(10.00)\n",
      "Iter 0130 | Time 16.5227(17.1709) | Bit/dim 1.0969(1.1135) | Xent 0.0134(0.0193) | Loss 1.1036(1.1231) | Error 0.0056(0.0065) Steps 548(544.30) | Grad Norm 0.6316(0.7207) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0002 | Time 67.2260, Epoch Time 1184.4895(1201.4689), Bit/dim 1.1056, Xent 0.0321, Loss 1.1216, Error 0.0204\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0140 | Time 16.6437(17.0572) | Bit/dim 1.1103(1.1133) | Xent 0.0208(0.0184) | Loss 1.1207(1.1225) | Error 0.0089(0.0061) Steps 548(544.15) | Grad Norm 0.4947(0.6910) | Total Time 10.00(10.00)\n",
      "Iter 0150 | Time 16.7841(16.9429) | Bit/dim 1.1073(1.1134) | Xent 0.0171(0.0187) | Loss 1.1159(1.1227) | Error 0.0044(0.0060) Steps 542(543.69) | Grad Norm 0.5621(0.6689) | Total Time 10.00(10.00)\n",
      "Iter 0160 | Time 16.6304(16.8462) | Bit/dim 1.1192(1.1134) | Xent 0.0188(0.0191) | Loss 1.1287(1.1230) | Error 0.0078(0.0063) Steps 542(543.18) | Grad Norm 0.5811(0.6715) | Total Time 10.00(10.00)\n",
      "Iter 0170 | Time 16.7594(16.8069) | Bit/dim 1.1166(1.1130) | Xent 0.0189(0.0194) | Loss 1.1261(1.1227) | Error 0.0067(0.0064) Steps 548(543.66) | Grad Norm 0.6343(0.6699) | Total Time 10.00(10.00)\n",
      "Iter 0180 | Time 16.7046(16.7567) | Bit/dim 1.1207(1.1123) | Xent 0.0145(0.0197) | Loss 1.1280(1.1222) | Error 0.0067(0.0065) Steps 548(543.74) | Grad Norm 0.4964(0.6669) | Total Time 10.00(10.00)\n",
      "Iter 0190 | Time 16.4208(16.7125) | Bit/dim 1.1114(1.1125) | Xent 0.0153(0.0195) | Loss 1.1191(1.1222) | Error 0.0033(0.0064) Steps 548(544.25) | Grad Norm 0.6181(0.6806) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0003 | Time 68.3698, Epoch Time 1181.0702(1200.8569), Bit/dim 1.1052, Xent 0.0316, Loss 1.1210, Error 0.0190\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0200 | Time 16.4615(16.7019) | Bit/dim 1.1027(1.1111) | Xent 0.0188(0.0196) | Loss 1.1121(1.1209) | Error 0.0056(0.0065) Steps 548(544.82) | Grad Norm 0.4529(0.6923) | Total Time 10.00(10.00)\n",
      "Iter 0210 | Time 16.4633(16.6869) | Bit/dim 1.1138(1.1114) | Xent 0.0236(0.0200) | Loss 1.1256(1.1214) | Error 0.0078(0.0065) Steps 548(545.30) | Grad Norm 0.6897(0.7000) | Total Time 10.00(10.00)\n",
      "Iter 0220 | Time 16.9078(16.6971) | Bit/dim 1.1092(1.1109) | Xent 0.0274(0.0190) | Loss 1.1229(1.1204) | Error 0.0111(0.0063) Steps 554(545.49) | Grad Norm 0.6530(0.6571) | Total Time 10.00(10.00)\n",
      "Iter 0230 | Time 16.6488(16.6998) | Bit/dim 1.1123(1.1103) | Xent 0.0086(0.0180) | Loss 1.1166(1.1193) | Error 0.0033(0.0059) Steps 542(545.36) | Grad Norm 0.4718(0.6519) | Total Time 10.00(10.00)\n",
      "Iter 0240 | Time 16.4687(16.6911) | Bit/dim 1.1062(1.1106) | Xent 0.0122(0.0179) | Loss 1.1123(1.1195) | Error 0.0056(0.0060) Steps 542(544.80) | Grad Norm 0.6418(0.6648) | Total Time 10.00(10.00)\n",
      "Iter 0250 | Time 16.5436(16.6709) | Bit/dim 1.1229(1.1118) | Xent 0.0147(0.0182) | Loss 1.1303(1.1209) | Error 0.0033(0.0061) Steps 542(544.88) | Grad Norm 0.6742(0.6664) | Total Time 10.00(10.00)\n",
      "Iter 0260 | Time 16.7007(16.6829) | Bit/dim 1.1264(1.1120) | Xent 0.0083(0.0181) | Loss 1.1306(1.1211) | Error 0.0022(0.0063) Steps 542(544.90) | Grad Norm 0.4833(0.6366) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0004 | Time 67.4450, Epoch Time 1182.2742(1200.2995), Bit/dim 1.1050, Xent 0.0315, Loss 1.1208, Error 0.0205\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0270 | Time 16.3837(16.6684) | Bit/dim 1.1190(1.1118) | Xent 0.0193(0.0196) | Loss 1.1287(1.1216) | Error 0.0067(0.0066) Steps 542(544.55) | Grad Norm 0.6489(0.6473) | Total Time 10.00(10.00)\n",
      "Iter 0280 | Time 16.8474(16.6808) | Bit/dim 1.1057(1.1116) | Xent 0.0127(0.0183) | Loss 1.1120(1.1208) | Error 0.0033(0.0063) Steps 548(544.88) | Grad Norm 0.5460(0.6321) | Total Time 10.00(10.00)\n",
      "Iter 0290 | Time 16.8011(16.6899) | Bit/dim 1.1076(1.1109) | Xent 0.0132(0.0181) | Loss 1.1142(1.1199) | Error 0.0044(0.0062) Steps 548(544.15) | Grad Norm 0.4782(0.6369) | Total Time 10.00(10.00)\n",
      "Iter 0300 | Time 16.7176(16.6868) | Bit/dim 1.1082(1.1102) | Xent 0.0059(0.0176) | Loss 1.1112(1.1191) | Error 0.0011(0.0059) Steps 542(544.08) | Grad Norm 0.3875(0.6609) | Total Time 10.00(10.00)\n",
      "Iter 0310 | Time 16.6261(16.7029) | Bit/dim 1.1170(1.1102) | Xent 0.0145(0.0189) | Loss 1.1243(1.1196) | Error 0.0011(0.0062) Steps 548(544.85) | Grad Norm 0.4554(0.6656) | Total Time 10.00(10.00)\n",
      "Iter 0320 | Time 16.6847(16.6986) | Bit/dim 1.1150(1.1109) | Xent 0.0226(0.0184) | Loss 1.1263(1.1201) | Error 0.0067(0.0061) Steps 548(544.76) | Grad Norm 0.6111(0.6499) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0005 | Time 68.0179, Epoch Time 1183.9524(1199.8090), Bit/dim 1.1046, Xent 0.0314, Loss 1.1203, Error 0.0199\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0330 | Time 16.8673(16.7165) | Bit/dim 1.1046(1.1112) | Xent 0.0145(0.0185) | Loss 1.1119(1.1204) | Error 0.0056(0.0062) Steps 548(544.87) | Grad Norm 0.6280(0.6373) | Total Time 10.00(10.00)\n",
      "Iter 0340 | Time 16.7428(16.7203) | Bit/dim 1.1186(1.1120) | Xent 0.0239(0.0175) | Loss 1.1306(1.1208) | Error 0.0089(0.0057) Steps 548(545.04) | Grad Norm 0.9691(0.6316) | Total Time 10.00(10.00)\n",
      "Iter 0350 | Time 16.8011(16.7154) | Bit/dim 1.1152(1.1115) | Xent 0.0115(0.0178) | Loss 1.1209(1.1204) | Error 0.0056(0.0056) Steps 554(545.23) | Grad Norm 0.3951(0.6304) | Total Time 10.00(10.00)\n",
      "Iter 0360 | Time 16.8103(16.7057) | Bit/dim 1.0969(1.1106) | Xent 0.0436(0.0185) | Loss 1.1187(1.1199) | Error 0.0189(0.0063) Steps 542(544.71) | Grad Norm 1.4009(0.6659) | Total Time 10.00(10.00)\n",
      "Iter 0370 | Time 16.7416(16.6939) | Bit/dim 1.1238(1.1119) | Xent 0.0096(0.0190) | Loss 1.1286(1.1215) | Error 0.0044(0.0063) Steps 536(544.33) | Grad Norm 0.7188(0.6833) | Total Time 10.00(10.00)\n",
      "Iter 0380 | Time 16.5114(16.6773) | Bit/dim 1.1228(1.1117) | Xent 0.0191(0.0175) | Loss 1.1323(1.1205) | Error 0.0111(0.0058) Steps 548(544.49) | Grad Norm 0.6581(0.6591) | Total Time 10.00(10.00)\n",
      "Iter 0390 | Time 16.8262(16.6900) | Bit/dim 1.1093(1.1106) | Xent 0.0165(0.0180) | Loss 1.1175(1.1196) | Error 0.0044(0.0060) Steps 548(544.16) | Grad Norm 0.6607(0.6691) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0006 | Time 67.7948, Epoch Time 1183.9321(1199.3327), Bit/dim 1.1036, Xent 0.0327, Loss 1.1200, Error 0.0206\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0400 | Time 16.9180(16.7267) | Bit/dim 1.1004(1.1101) | Xent 0.0159(0.0177) | Loss 1.1083(1.1189) | Error 0.0078(0.0059) Steps 548(544.36) | Grad Norm 0.5658(0.6815) | Total Time 10.00(10.00)\n",
      "Iter 0410 | Time 16.6615(16.7422) | Bit/dim 1.0988(1.1103) | Xent 0.0355(0.0177) | Loss 1.1166(1.1192) | Error 0.0100(0.0058) Steps 542(545.01) | Grad Norm 0.5290(0.6560) | Total Time 10.00(10.00)\n",
      "Iter 0420 | Time 16.8751(16.7406) | Bit/dim 1.1059(1.1103) | Xent 0.0142(0.0181) | Loss 1.1130(1.1193) | Error 0.0044(0.0059) Steps 548(545.65) | Grad Norm 0.5827(0.6819) | Total Time 10.00(10.00)\n",
      "Iter 0430 | Time 16.9132(16.7521) | Bit/dim 1.1092(1.1093) | Xent 0.0079(0.0176) | Loss 1.1131(1.1181) | Error 0.0011(0.0058) Steps 542(545.52) | Grad Norm 0.4518(0.6691) | Total Time 10.00(10.00)\n",
      "Iter 0440 | Time 16.5953(16.7507) | Bit/dim 1.0898(1.1085) | Xent 0.0164(0.0178) | Loss 1.0980(1.1174) | Error 0.0067(0.0060) Steps 542(546.00) | Grad Norm 0.6101(0.6765) | Total Time 10.00(10.00)\n",
      "Iter 0450 | Time 16.6766(16.7129) | Bit/dim 1.1099(1.1096) | Xent 0.0137(0.0177) | Loss 1.1168(1.1185) | Error 0.0044(0.0060) Steps 548(545.59) | Grad Norm 0.8473(0.6931) | Total Time 10.00(10.00)\n",
      "Iter 0460 | Time 16.8478(16.7070) | Bit/dim 1.1281(1.1098) | Xent 0.0067(0.0170) | Loss 1.1315(1.1183) | Error 0.0011(0.0058) Steps 542(545.73) | Grad Norm 0.7030(0.6651) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0007 | Time 68.4008, Epoch Time 1186.2717(1198.9409), Bit/dim 1.1035, Xent 0.0339, Loss 1.1205, Error 0.0207\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0470 | Time 16.6213(16.6993) | Bit/dim 1.1184(1.1109) | Xent 0.0180(0.0168) | Loss 1.1274(1.1193) | Error 0.0089(0.0058) Steps 542(544.93) | Grad Norm 0.6637(0.6715) | Total Time 10.00(10.00)\n",
      "Iter 0480 | Time 16.7173(16.6984) | Bit/dim 1.1108(1.1111) | Xent 0.0093(0.0161) | Loss 1.1154(1.1191) | Error 0.0022(0.0058) Steps 542(545.07) | Grad Norm 0.6325(0.6588) | Total Time 10.00(10.00)\n",
      "Iter 0490 | Time 16.4314(16.6820) | Bit/dim 1.1051(1.1099) | Xent 0.0152(0.0167) | Loss 1.1127(1.1183) | Error 0.0033(0.0059) Steps 542(544.87) | Grad Norm 0.7975(0.6590) | Total Time 10.00(10.00)\n",
      "Iter 0500 | Time 16.9557(16.6823) | Bit/dim 1.0979(1.1101) | Xent 0.0184(0.0167) | Loss 1.1070(1.1184) | Error 0.0044(0.0057) Steps 554(545.23) | Grad Norm 0.5579(0.6541) | Total Time 10.00(10.00)\n",
      "Iter 0510 | Time 16.7815(16.7043) | Bit/dim 1.1077(1.1090) | Xent 0.0211(0.0170) | Loss 1.1183(1.1175) | Error 0.0033(0.0055) Steps 554(545.22) | Grad Norm 0.7059(0.6518) | Total Time 10.00(10.00)\n",
      "Iter 0520 | Time 16.3912(16.6946) | Bit/dim 1.1030(1.1094) | Xent 0.0105(0.0161) | Loss 1.1082(1.1174) | Error 0.0044(0.0052) Steps 542(544.54) | Grad Norm 0.5945(0.6300) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0008 | Time 68.3826, Epoch Time 1184.0588(1198.4944), Bit/dim 1.1028, Xent 0.0346, Loss 1.1202, Error 0.0211\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0530 | Time 16.3057(16.6731) | Bit/dim 1.1154(1.1090) | Xent 0.0188(0.0169) | Loss 1.1248(1.1174) | Error 0.0089(0.0055) Steps 536(544.62) | Grad Norm 0.8110(0.6644) | Total Time 10.00(10.00)\n",
      "Iter 0540 | Time 16.6021(16.6696) | Bit/dim 1.0909(1.1078) | Xent 0.0071(0.0173) | Loss 1.0944(1.1165) | Error 0.0011(0.0055) Steps 542(545.06) | Grad Norm 0.4462(0.7394) | Total Time 10.00(10.00)\n",
      "Iter 0550 | Time 16.7466(16.6935) | Bit/dim 1.1073(1.1087) | Xent 0.0121(0.0172) | Loss 1.1134(1.1173) | Error 0.0044(0.0056) Steps 542(545.61) | Grad Norm 0.6804(0.7749) | Total Time 10.00(10.00)\n",
      "Iter 0560 | Time 16.7428(16.7163) | Bit/dim 1.1142(1.1093) | Xent 0.0115(0.0174) | Loss 1.1199(1.1179) | Error 0.0022(0.0055) Steps 554(545.50) | Grad Norm 0.6285(0.7597) | Total Time 10.00(10.00)\n",
      "Iter 0570 | Time 16.7535(16.7225) | Bit/dim 1.0986(1.1088) | Xent 0.0250(0.0171) | Loss 1.1110(1.1173) | Error 0.0089(0.0054) Steps 542(545.22) | Grad Norm 0.8449(0.7315) | Total Time 10.00(10.00)\n",
      "Iter 0580 | Time 16.6718(16.7092) | Bit/dim 1.1113(1.1092) | Xent 0.0153(0.0168) | Loss 1.1189(1.1176) | Error 0.0056(0.0054) Steps 536(544.98) | Grad Norm 0.6896(0.7117) | Total Time 10.00(10.00)\n",
      "Iter 0590 | Time 16.6727(16.7273) | Bit/dim 1.1222(1.1095) | Xent 0.0148(0.0174) | Loss 1.1295(1.1182) | Error 0.0044(0.0055) Steps 542(544.53) | Grad Norm 0.5750(0.7283) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0009 | Time 67.8696, Epoch Time 1185.1847(1198.0951), Bit/dim 1.1032, Xent 0.0352, Loss 1.1208, Error 0.0212\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0600 | Time 16.7811(16.7288) | Bit/dim 1.1091(1.1092) | Xent 0.0181(0.0173) | Loss 1.1182(1.1178) | Error 0.0044(0.0055) Steps 542(544.95) | Grad Norm 0.5080(0.7475) | Total Time 10.00(10.00)\n",
      "Iter 0610 | Time 16.4907(16.7237) | Bit/dim 1.1129(1.1095) | Xent 0.0213(0.0181) | Loss 1.1236(1.1185) | Error 0.0078(0.0060) Steps 542(545.09) | Grad Norm 0.6408(0.7296) | Total Time 10.00(10.00)\n",
      "Iter 0620 | Time 16.5841(16.7230) | Bit/dim 1.1198(1.1096) | Xent 0.0118(0.0184) | Loss 1.1257(1.1188) | Error 0.0056(0.0059) Steps 542(544.90) | Grad Norm 0.5356(0.7185) | Total Time 10.00(10.00)\n",
      "Iter 0630 | Time 16.6807(16.7238) | Bit/dim 1.1056(1.1095) | Xent 0.0229(0.0174) | Loss 1.1171(1.1183) | Error 0.0056(0.0055) Steps 554(546.23) | Grad Norm 0.5709(0.7136) | Total Time 10.00(10.00)\n",
      "Iter 0640 | Time 16.6698(16.7319) | Bit/dim 1.1084(1.1086) | Xent 0.0205(0.0178) | Loss 1.1187(1.1175) | Error 0.0044(0.0056) Steps 548(545.69) | Grad Norm 0.7364(0.7593) | Total Time 10.00(10.00)\n",
      "Iter 0650 | Time 16.7479(16.7363) | Bit/dim 1.1182(1.1090) | Xent 0.0241(0.0177) | Loss 1.1303(1.1179) | Error 0.0067(0.0056) Steps 536(545.36) | Grad Norm 0.6908(0.8083) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0010 | Time 68.5235, Epoch Time 1186.6294(1197.7512), Bit/dim 1.1022, Xent 0.0341, Loss 1.1192, Error 0.0203\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0660 | Time 16.7659(16.7263) | Bit/dim 1.1060(1.1085) | Xent 0.0058(0.0164) | Loss 1.1089(1.1167) | Error 0.0011(0.0053) Steps 548(545.49) | Grad Norm 0.5153(0.7681) | Total Time 10.00(10.00)\n",
      "Iter 0670 | Time 16.8910(16.7118) | Bit/dim 1.1057(1.1079) | Xent 0.0149(0.0163) | Loss 1.1131(1.1160) | Error 0.0022(0.0053) Steps 548(545.84) | Grad Norm 0.5821(0.7319) | Total Time 10.00(10.00)\n",
      "Iter 0680 | Time 16.5218(16.6950) | Bit/dim 1.1056(1.1080) | Xent 0.0239(0.0165) | Loss 1.1176(1.1162) | Error 0.0056(0.0053) Steps 542(545.62) | Grad Norm 0.4778(0.7055) | Total Time 10.00(10.00)\n",
      "Iter 0690 | Time 16.9195(16.6947) | Bit/dim 1.1080(1.1075) | Xent 0.0214(0.0174) | Loss 1.1187(1.1162) | Error 0.0056(0.0054) Steps 548(545.67) | Grad Norm 1.2734(0.7455) | Total Time 10.00(10.00)\n",
      "Iter 0700 | Time 16.7063(16.7115) | Bit/dim 1.1084(1.1074) | Xent 0.0186(0.0180) | Loss 1.1177(1.1164) | Error 0.0067(0.0055) Steps 548(545.98) | Grad Norm 0.9221(0.7564) | Total Time 10.00(10.00)\n",
      "Iter 0710 | Time 16.8557(16.7485) | Bit/dim 1.0993(1.1079) | Xent 0.0083(0.0174) | Loss 1.1035(1.1166) | Error 0.0022(0.0055) Steps 542(546.50) | Grad Norm 0.5723(0.7361) | Total Time 10.00(10.00)\n",
      "Iter 0720 | Time 17.0272(16.7855) | Bit/dim 1.1023(1.1077) | Xent 0.0204(0.0166) | Loss 1.1125(1.1161) | Error 0.0078(0.0054) Steps 548(546.29) | Grad Norm 1.7024(0.8290) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0011 | Time 68.0176, Epoch Time 1187.7971(1197.4526), Bit/dim 1.1014, Xent 0.0335, Loss 1.1182, Error 0.0196\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0730 | Time 17.0662(16.7919) | Bit/dim 1.1066(1.1081) | Xent 0.0131(0.0177) | Loss 1.1132(1.1170) | Error 0.0044(0.0056) Steps 542(546.11) | Grad Norm 1.5712(0.9353) | Total Time 10.00(10.00)\n",
      "Iter 0740 | Time 16.6093(16.7643) | Bit/dim 1.1077(1.1092) | Xent 0.0213(0.0175) | Loss 1.1183(1.1179) | Error 0.0056(0.0055) Steps 542(544.89) | Grad Norm 1.9051(1.0113) | Total Time 10.00(10.00)\n",
      "Iter 0750 | Time 16.5775(16.7615) | Bit/dim 1.1142(1.1088) | Xent 0.0259(0.0175) | Loss 1.1272(1.1176) | Error 0.0033(0.0053) Steps 542(544.92) | Grad Norm 0.4750(1.0270) | Total Time 10.00(10.00)\n",
      "Iter 0760 | Time 16.7673(16.7777) | Bit/dim 1.0969(1.1086) | Xent 0.0211(0.0180) | Loss 1.1074(1.1176) | Error 0.0056(0.0054) Steps 548(545.06) | Grad Norm 0.6366(0.9919) | Total Time 10.00(10.00)\n",
      "Iter 0770 | Time 16.6768(16.7716) | Bit/dim 1.1033(1.1079) | Xent 0.0101(0.0167) | Loss 1.1084(1.1163) | Error 0.0022(0.0051) Steps 548(545.66) | Grad Norm 0.6865(0.9166) | Total Time 10.00(10.00)\n",
      "Iter 0780 | Time 16.8453(16.7601) | Bit/dim 1.1037(1.1065) | Xent 0.0166(0.0168) | Loss 1.1121(1.1149) | Error 0.0089(0.0053) Steps 542(545.85) | Grad Norm 0.7131(0.9315) | Total Time 10.00(10.00)\n",
      "Iter 0790 | Time 16.6685(16.7210) | Bit/dim 1.1107(1.1077) | Xent 0.0232(0.0177) | Loss 1.1223(1.1166) | Error 0.0067(0.0056) Steps 542(545.32) | Grad Norm 0.9781(0.8973) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0012 | Time 68.0499, Epoch Time 1185.7051(1197.1001), Bit/dim 1.1016, Xent 0.0336, Loss 1.1183, Error 0.0203\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0800 | Time 16.8283(16.6935) | Bit/dim 1.1115(1.1079) | Xent 0.0181(0.0177) | Loss 1.1205(1.1167) | Error 0.0067(0.0056) Steps 548(545.21) | Grad Norm 0.9685(0.8971) | Total Time 10.00(10.00)\n",
      "Iter 0810 | Time 16.5854(16.6926) | Bit/dim 1.1054(1.1084) | Xent 0.0327(0.0189) | Loss 1.1218(1.1179) | Error 0.0056(0.0060) Steps 542(545.32) | Grad Norm 0.9624(0.9220) | Total Time 10.00(10.00)\n",
      "Iter 0820 | Time 16.6181(16.6874) | Bit/dim 1.1041(1.1079) | Xent 0.0212(0.0192) | Loss 1.1147(1.1175) | Error 0.0078(0.0061) Steps 548(545.26) | Grad Norm 0.6659(0.8902) | Total Time 10.00(10.00)\n",
      "Iter 0830 | Time 16.8444(16.7162) | Bit/dim 1.1103(1.1077) | Xent 0.0105(0.0182) | Loss 1.1156(1.1168) | Error 0.0033(0.0058) Steps 542(545.46) | Grad Norm 1.3699(0.9184) | Total Time 10.00(10.00)\n",
      "Iter 0840 | Time 16.7246(16.7142) | Bit/dim 1.1244(1.1087) | Xent 0.0061(0.0173) | Loss 1.1274(1.1173) | Error 0.0000(0.0057) Steps 554(545.52) | Grad Norm 0.4972(0.9585) | Total Time 10.00(10.00)\n",
      "Iter 0850 | Time 16.6025(16.6995) | Bit/dim 1.1073(1.1077) | Xent 0.0159(0.0174) | Loss 1.1152(1.1164) | Error 0.0033(0.0055) Steps 548(545.54) | Grad Norm 1.3195(1.0632) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0013 | Time 68.3169, Epoch Time 1183.4391(1196.6903), Bit/dim 1.1010, Xent 0.0308, Loss 1.1164, Error 0.0195\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0860 | Time 16.7763(16.6833) | Bit/dim 1.0953(1.1060) | Xent 0.0083(0.0170) | Loss 1.0995(1.1146) | Error 0.0033(0.0054) Steps 554(545.76) | Grad Norm 0.8460(1.0429) | Total Time 10.00(10.00)\n",
      "Iter 0870 | Time 16.7138(16.6805) | Bit/dim 1.1154(1.1066) | Xent 0.0203(0.0171) | Loss 1.1256(1.1152) | Error 0.0089(0.0055) Steps 542(545.85) | Grad Norm 0.8361(1.0330) | Total Time 10.00(10.00)\n",
      "Iter 0880 | Time 16.7466(16.6939) | Bit/dim 1.0936(1.1061) | Xent 0.0041(0.0165) | Loss 1.0956(1.1144) | Error 0.0000(0.0052) Steps 554(545.98) | Grad Norm 1.1214(0.9625) | Total Time 10.00(10.00)\n",
      "Iter 0890 | Time 16.3380(16.7009) | Bit/dim 1.1041(1.1065) | Xent 0.0221(0.0169) | Loss 1.1151(1.1149) | Error 0.0078(0.0055) Steps 542(546.20) | Grad Norm 1.4023(0.9666) | Total Time 10.00(10.00)\n",
      "Iter 0900 | Time 16.9967(16.7230) | Bit/dim 1.0998(1.1059) | Xent 0.0124(0.0173) | Loss 1.1060(1.1146) | Error 0.0033(0.0057) Steps 542(546.07) | Grad Norm 0.7407(0.9760) | Total Time 10.00(10.00)\n",
      "Iter 0910 | Time 16.6234(16.7151) | Bit/dim 1.1163(1.1067) | Xent 0.0208(0.0172) | Loss 1.1267(1.1153) | Error 0.0089(0.0057) Steps 554(546.45) | Grad Norm 0.7484(0.9493) | Total Time 10.00(10.00)\n",
      "Iter 0920 | Time 16.8881(16.7184) | Bit/dim 1.1087(1.1068) | Xent 0.0153(0.0171) | Loss 1.1163(1.1153) | Error 0.0067(0.0059) Steps 548(546.25) | Grad Norm 1.1148(0.9183) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0014 | Time 68.0562, Epoch Time 1185.2805(1196.3480), Bit/dim 1.1000, Xent 0.0307, Loss 1.1153, Error 0.0195\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0930 | Time 16.7463(16.7126) | Bit/dim 1.1187(1.1067) | Xent 0.0149(0.0170) | Loss 1.1262(1.1152) | Error 0.0067(0.0058) Steps 542(546.26) | Grad Norm 0.5903(0.9103) | Total Time 10.00(10.00)\n",
      "Iter 0940 | Time 16.3931(16.7073) | Bit/dim 1.1021(1.1075) | Xent 0.0211(0.0171) | Loss 1.1126(1.1161) | Error 0.0089(0.0058) Steps 548(546.21) | Grad Norm 1.2192(0.9089) | Total Time 10.00(10.00)\n",
      "Iter 0950 | Time 16.7205(16.6971) | Bit/dim 1.1089(1.1072) | Xent 0.0219(0.0162) | Loss 1.1198(1.1153) | Error 0.0067(0.0053) Steps 548(545.91) | Grad Norm 0.9021(0.9159) | Total Time 10.00(10.00)\n",
      "Iter 0960 | Time 16.7113(16.7154) | Bit/dim 1.1069(1.1067) | Xent 0.0207(0.0172) | Loss 1.1173(1.1153) | Error 0.0078(0.0056) Steps 548(546.46) | Grad Norm 1.2067(0.9141) | Total Time 10.00(10.00)\n",
      "Iter 0970 | Time 16.6513(16.7061) | Bit/dim 1.1101(1.1072) | Xent 0.0145(0.0167) | Loss 1.1174(1.1155) | Error 0.0044(0.0054) Steps 548(546.11) | Grad Norm 0.8355(0.9471) | Total Time 10.00(10.00)\n",
      "Iter 0980 | Time 16.8459(16.7176) | Bit/dim 1.1010(1.1058) | Xent 0.0098(0.0169) | Loss 1.1060(1.1143) | Error 0.0022(0.0054) Steps 548(545.67) | Grad Norm 1.4226(1.0643) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0015 | Time 68.8363, Epoch Time 1185.6899(1196.0283), Bit/dim 1.0992, Xent 0.0311, Loss 1.1147, Error 0.0190\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0990 | Time 16.8126(16.7282) | Bit/dim 1.0938(1.1050) | Xent 0.0284(0.0174) | Loss 1.1080(1.1138) | Error 0.0100(0.0055) Steps 548(545.20) | Grad Norm 1.2862(1.0855) | Total Time 10.00(10.00)\n",
      "Iter 1000 | Time 16.7007(16.7066) | Bit/dim 1.1017(1.1066) | Xent 0.0155(0.0171) | Loss 1.1094(1.1152) | Error 0.0044(0.0053) Steps 548(544.70) | Grad Norm 0.7957(1.1292) | Total Time 10.00(10.00)\n",
      "Iter 1010 | Time 16.7067(16.7161) | Bit/dim 1.1144(1.1061) | Xent 0.0293(0.0165) | Loss 1.1290(1.1143) | Error 0.0078(0.0051) Steps 548(544.47) | Grad Norm 1.9285(1.1495) | Total Time 10.00(10.00)\n",
      "Iter 1020 | Time 16.7492(16.7279) | Bit/dim 1.1042(1.1058) | Xent 0.0148(0.0164) | Loss 1.1116(1.1140) | Error 0.0044(0.0049) Steps 548(544.75) | Grad Norm 0.9296(1.1256) | Total Time 10.00(10.00)\n",
      "Iter 1030 | Time 16.9252(16.7415) | Bit/dim 1.1065(1.1046) | Xent 0.0258(0.0170) | Loss 1.1194(1.1130) | Error 0.0067(0.0053) Steps 542(544.62) | Grad Norm 1.6659(1.1621) | Total Time 10.00(10.00)\n",
      "Iter 1040 | Time 16.8446(16.7419) | Bit/dim 1.1059(1.1055) | Xent 0.0192(0.0165) | Loss 1.1155(1.1138) | Error 0.0056(0.0051) Steps 548(544.41) | Grad Norm 1.1245(1.3181) | Total Time 10.00(10.00)\n",
      "Iter 1050 | Time 17.0644(16.7386) | Bit/dim 1.1069(1.1050) | Xent 0.0087(0.0161) | Loss 1.1113(1.1131) | Error 0.0033(0.0051) Steps 542(544.69) | Grad Norm 0.4529(1.2736) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0016 | Time 68.4988, Epoch Time 1186.9313(1195.7554), Bit/dim 1.0998, Xent 0.0310, Loss 1.1153, Error 0.0191\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1060 | Time 16.5916(16.7328) | Bit/dim 1.1061(1.1052) | Xent 0.0282(0.0171) | Loss 1.1202(1.1137) | Error 0.0078(0.0052) Steps 548(545.51) | Grad Norm 1.0612(1.2145) | Total Time 10.00(10.00)\n",
      "Iter 1070 | Time 16.6247(16.7437) | Bit/dim 1.1124(1.1057) | Xent 0.0091(0.0170) | Loss 1.1169(1.1142) | Error 0.0033(0.0053) Steps 548(545.70) | Grad Norm 1.6708(1.2189) | Total Time 10.00(10.00)\n",
      "Iter 1080 | Time 16.5394(16.7555) | Bit/dim 1.1136(1.1058) | Xent 0.0131(0.0165) | Loss 1.1202(1.1140) | Error 0.0033(0.0052) Steps 548(545.18) | Grad Norm 1.3669(1.2642) | Total Time 10.00(10.00)\n",
      "Iter 1090 | Time 16.8270(16.7284) | Bit/dim 1.1206(1.1065) | Xent 0.0135(0.0163) | Loss 1.1273(1.1146) | Error 0.0056(0.0050) Steps 542(544.86) | Grad Norm 0.8024(1.2182) | Total Time 10.00(10.00)\n",
      "Iter 1100 | Time 16.7177(16.7526) | Bit/dim 1.1039(1.1060) | Xent 0.0148(0.0158) | Loss 1.1113(1.1139) | Error 0.0078(0.0049) Steps 548(544.89) | Grad Norm 1.0403(1.1161) | Total Time 10.00(10.00)\n",
      "Iter 1110 | Time 16.6076(16.7442) | Bit/dim 1.1000(1.1059) | Xent 0.0117(0.0157) | Loss 1.1059(1.1138) | Error 0.0044(0.0049) Steps 542(545.10) | Grad Norm 1.3907(1.1137) | Total Time 10.00(10.00)\n",
      "Iter 1120 | Time 16.6063(16.7347) | Bit/dim 1.0957(1.1051) | Xent 0.0329(0.0168) | Loss 1.1121(1.1136) | Error 0.0078(0.0054) Steps 548(545.11) | Grad Norm 1.8415(1.1784) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0017 | Time 67.9919, Epoch Time 1186.4164(1195.4752), Bit/dim 1.0987, Xent 0.0334, Loss 1.1154, Error 0.0195\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1130 | Time 17.1160(16.7517) | Bit/dim 1.1013(1.1052) | Xent 0.0283(0.0167) | Loss 1.1155(1.1135) | Error 0.0100(0.0055) Steps 542(544.74) | Grad Norm 1.5930(1.2317) | Total Time 10.00(10.00)\n",
      "Iter 1140 | Time 16.8201(16.7460) | Bit/dim 1.0933(1.1044) | Xent 0.0229(0.0174) | Loss 1.1047(1.1130) | Error 0.0100(0.0056) Steps 548(545.30) | Grad Norm 1.8217(1.2972) | Total Time 10.00(10.00)\n",
      "Iter 1150 | Time 16.8641(16.7432) | Bit/dim 1.1075(1.1045) | Xent 0.0214(0.0168) | Loss 1.1183(1.1129) | Error 0.0078(0.0055) Steps 542(545.05) | Grad Norm 1.7786(1.3050) | Total Time 10.00(10.00)\n",
      "Iter 1160 | Time 16.7186(16.7556) | Bit/dim 1.1028(1.1042) | Xent 0.0154(0.0167) | Loss 1.1105(1.1125) | Error 0.0067(0.0056) Steps 548(544.87) | Grad Norm 2.2895(1.3255) | Total Time 10.00(10.00)\n",
      "Iter 1170 | Time 16.7606(16.7519) | Bit/dim 1.0955(1.1039) | Xent 0.0096(0.0166) | Loss 1.1003(1.1122) | Error 0.0044(0.0055) Steps 548(544.47) | Grad Norm 1.9263(1.4573) | Total Time 10.00(10.00)\n",
      "Iter 1180 | Time 16.7203(16.7459) | Bit/dim 1.1107(1.1051) | Xent 0.0292(0.0170) | Loss 1.1252(1.1136) | Error 0.0122(0.0056) Steps 542(544.61) | Grad Norm 1.7455(1.4213) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0018 | Time 68.5288, Epoch Time 1188.1699(1195.2560), Bit/dim 1.0988, Xent 0.0314, Loss 1.1145, Error 0.0193\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1190 | Time 16.6320(16.7386) | Bit/dim 1.0757(1.1043) | Xent 0.0185(0.0168) | Loss 1.0849(1.1127) | Error 0.0044(0.0056) Steps 542(544.43) | Grad Norm 1.0026(1.3510) | Total Time 10.00(10.00)\n",
      "Iter 1200 | Time 16.8248(16.7313) | Bit/dim 1.1047(1.1052) | Xent 0.0125(0.0165) | Loss 1.1109(1.1135) | Error 0.0078(0.0054) Steps 548(544.92) | Grad Norm 1.0823(1.2247) | Total Time 10.00(10.00)\n",
      "Iter 1210 | Time 16.9134(16.7696) | Bit/dim 1.1138(1.1046) | Xent 0.0261(0.0165) | Loss 1.1269(1.1129) | Error 0.0100(0.0057) Steps 542(544.77) | Grad Norm 1.6199(1.1900) | Total Time 10.00(10.00)\n",
      "Iter 1220 | Time 17.2904(16.7779) | Bit/dim 1.0978(1.1056) | Xent 0.0132(0.0159) | Loss 1.1044(1.1135) | Error 0.0044(0.0051) Steps 554(545.36) | Grad Norm 0.9994(1.2584) | Total Time 10.00(10.00)\n",
      "Iter 1230 | Time 16.9243(16.7762) | Bit/dim 1.0995(1.1046) | Xent 0.0271(0.0163) | Loss 1.1130(1.1128) | Error 0.0056(0.0053) Steps 548(545.89) | Grad Norm 1.8325(1.2306) | Total Time 10.00(10.00)\n",
      "Iter 1240 | Time 16.5518(16.7448) | Bit/dim 1.1143(1.1046) | Xent 0.0203(0.0165) | Loss 1.1244(1.1128) | Error 0.0056(0.0053) Steps 542(545.61) | Grad Norm 1.7280(1.2925) | Total Time 10.00(10.00)\n",
      "Iter 1250 | Time 16.5390(16.7666) | Bit/dim 1.1061(1.1041) | Xent 0.0170(0.0162) | Loss 1.1146(1.1122) | Error 0.0044(0.0052) Steps 548(546.08) | Grad Norm 0.7041(1.2295) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0019 | Time 68.0030, Epoch Time 1187.9435(1195.0367), Bit/dim 1.0979, Xent 0.0316, Loss 1.1137, Error 0.0199\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1260 | Time 16.6123(16.7534) | Bit/dim 1.0977(1.1048) | Xent 0.0071(0.0156) | Loss 1.1013(1.1125) | Error 0.0022(0.0050) Steps 548(545.66) | Grad Norm 0.7143(1.1268) | Total Time 10.00(10.00)\n",
      "Iter 1270 | Time 16.7619(16.7796) | Bit/dim 1.0945(1.1041) | Xent 0.0178(0.0160) | Loss 1.1034(1.1121) | Error 0.0089(0.0052) Steps 548(545.17) | Grad Norm 1.5572(1.1458) | Total Time 10.00(10.00)\n",
      "Iter 1280 | Time 16.9581(16.7979) | Bit/dim 1.0970(1.1043) | Xent 0.0134(0.0158) | Loss 1.1038(1.1122) | Error 0.0033(0.0051) Steps 548(545.74) | Grad Norm 0.8111(1.1035) | Total Time 10.00(10.00)\n",
      "Iter 1290 | Time 16.7651(16.7855) | Bit/dim 1.1158(1.1043) | Xent 0.0118(0.0162) | Loss 1.1217(1.1124) | Error 0.0056(0.0053) Steps 542(545.40) | Grad Norm 0.5330(1.0876) | Total Time 10.00(10.00)\n",
      "Iter 1300 | Time 16.6903(16.7835) | Bit/dim 1.1173(1.1047) | Xent 0.0165(0.0148) | Loss 1.1255(1.1121) | Error 0.0067(0.0049) Steps 542(545.48) | Grad Norm 1.4931(1.0457) | Total Time 10.00(10.00)\n",
      "Iter 1310 | Time 16.7434(16.7862) | Bit/dim 1.0988(1.1036) | Xent 0.0138(0.0148) | Loss 1.1057(1.1111) | Error 0.0033(0.0049) Steps 542(545.91) | Grad Norm 0.8515(1.0616) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0020 | Time 68.4787, Epoch Time 1189.9635(1194.8845), Bit/dim 1.0972, Xent 0.0295, Loss 1.1119, Error 0.0197\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1320 | Time 16.4962(16.7543) | Bit/dim 1.1020(1.1037) | Xent 0.0074(0.0150) | Loss 1.1057(1.1112) | Error 0.0022(0.0050) Steps 542(545.83) | Grad Norm 1.0595(1.0217) | Total Time 10.00(10.00)\n",
      "Iter 1330 | Time 16.4082(16.7211) | Bit/dim 1.1024(1.1039) | Xent 0.0084(0.0141) | Loss 1.1066(1.1110) | Error 0.0011(0.0046) Steps 542(545.91) | Grad Norm 0.8922(1.0524) | Total Time 10.00(10.00)\n",
      "Iter 1340 | Time 16.7934(16.6890) | Bit/dim 1.1118(1.1034) | Xent 0.0092(0.0140) | Loss 1.1164(1.1104) | Error 0.0011(0.0045) Steps 548(545.67) | Grad Norm 1.1254(1.0372) | Total Time 10.00(10.00)\n",
      "Iter 1350 | Time 16.7250(16.6689) | Bit/dim 1.1000(1.1030) | Xent 0.0150(0.0152) | Loss 1.1075(1.1106) | Error 0.0044(0.0050) Steps 542(545.96) | Grad Norm 2.6743(1.1115) | Total Time 10.00(10.00)\n",
      "Iter 1360 | Time 17.1319(16.6771) | Bit/dim 1.0933(1.1033) | Xent 0.0142(0.0150) | Loss 1.1004(1.1108) | Error 0.0067(0.0050) Steps 542(545.65) | Grad Norm 1.5677(1.2898) | Total Time 10.00(10.00)\n",
      "Iter 1370 | Time 16.6221(16.6808) | Bit/dim 1.0968(1.1043) | Xent 0.0215(0.0156) | Loss 1.1075(1.1121) | Error 0.0078(0.0053) Steps 542(545.60) | Grad Norm 1.1985(1.3586) | Total Time 10.00(10.00)\n",
      "Iter 1380 | Time 16.6919(16.6828) | Bit/dim 1.0936(1.1042) | Xent 0.0175(0.0164) | Loss 1.1023(1.1124) | Error 0.0089(0.0055) Steps 542(545.70) | Grad Norm 1.6600(1.3143) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0021 | Time 68.2204, Epoch Time 1180.9434(1194.4662), Bit/dim 1.0974, Xent 0.0313, Loss 1.1131, Error 0.0205\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1390 | Time 16.7684(16.6772) | Bit/dim 1.1070(1.1033) | Xent 0.0102(0.0159) | Loss 1.1121(1.1112) | Error 0.0033(0.0053) Steps 548(546.61) | Grad Norm 1.1385(1.4757) | Total Time 10.00(10.00)\n",
      "Iter 1400 | Time 16.5953(16.6683) | Bit/dim 1.0982(1.1021) | Xent 0.0401(0.0166) | Loss 1.1183(1.1104) | Error 0.0122(0.0054) Steps 542(546.17) | Grad Norm 1.5393(1.4271) | Total Time 10.00(10.00)\n",
      "Iter 1410 | Time 16.6964(16.6690) | Bit/dim 1.1001(1.1022) | Xent 0.0110(0.0161) | Loss 1.1056(1.1103) | Error 0.0056(0.0054) Steps 548(546.18) | Grad Norm 2.1921(1.4481) | Total Time 10.00(10.00)\n",
      "Iter 1420 | Time 16.7524(16.6673) | Bit/dim 1.1077(1.1033) | Xent 0.0090(0.0157) | Loss 1.1122(1.1111) | Error 0.0022(0.0052) Steps 548(546.36) | Grad Norm 1.0039(1.4455) | Total Time 10.00(10.00)\n",
      "Iter 1430 | Time 16.5520(16.6653) | Bit/dim 1.1150(1.1035) | Xent 0.0402(0.0162) | Loss 1.1351(1.1116) | Error 0.0111(0.0052) Steps 542(546.46) | Grad Norm 1.1347(1.4327) | Total Time 10.00(10.00)\n",
      "Iter 1440 | Time 16.6240(16.6638) | Bit/dim 1.1036(1.1029) | Xent 0.0098(0.0158) | Loss 1.1086(1.1108) | Error 0.0011(0.0051) Steps 548(546.59) | Grad Norm 1.7605(1.3306) | Total Time 10.00(10.00)\n",
      "Iter 1450 | Time 16.6614(16.6480) | Bit/dim 1.0914(1.1030) | Xent 0.0181(0.0155) | Loss 1.1005(1.1107) | Error 0.0056(0.0050) Steps 554(546.52) | Grad Norm 1.4891(1.3921) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0022 | Time 67.7680, Epoch Time 1180.4867(1194.0468), Bit/dim 1.0959, Xent 0.0332, Loss 1.1125, Error 0.0195\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1460 | Time 16.7592(16.6248) | Bit/dim 1.1115(1.1036) | Xent 0.0102(0.0153) | Loss 1.1166(1.1112) | Error 0.0033(0.0049) Steps 548(546.91) | Grad Norm 0.4676(1.3206) | Total Time 10.00(10.00)\n",
      "Iter 1470 | Time 16.5120(16.6406) | Bit/dim 1.1028(1.1035) | Xent 0.0108(0.0154) | Loss 1.1082(1.1111) | Error 0.0067(0.0051) Steps 554(546.81) | Grad Norm 0.7348(1.2762) | Total Time 10.00(10.00)\n",
      "Iter 1480 | Time 16.4729(16.6303) | Bit/dim 1.1120(1.1034) | Xent 0.0211(0.0153) | Loss 1.1225(1.1111) | Error 0.0089(0.0051) Steps 548(546.63) | Grad Norm 0.8021(1.2825) | Total Time 10.00(10.00)\n",
      "Iter 1490 | Time 16.5193(16.6499) | Bit/dim 1.1101(1.1032) | Xent 0.0154(0.0159) | Loss 1.1179(1.1111) | Error 0.0067(0.0052) Steps 542(545.88) | Grad Norm 1.4010(1.2554) | Total Time 10.00(10.00)\n",
      "Iter 1500 | Time 16.6792(16.6957) | Bit/dim 1.0988(1.1034) | Xent 0.0166(0.0168) | Loss 1.1071(1.1118) | Error 0.0056(0.0056) Steps 542(546.23) | Grad Norm 1.8559(1.3976) | Total Time 10.00(10.00)\n",
      "Iter 1510 | Time 17.0506(16.7490) | Bit/dim 1.1015(1.1020) | Xent 0.0144(0.0163) | Loss 1.1087(1.1102) | Error 0.0044(0.0054) Steps 548(546.62) | Grad Norm 2.4100(1.5535) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0023 | Time 68.6542, Epoch Time 1186.5596(1193.8222), Bit/dim 1.0965, Xent 0.0313, Loss 1.1122, Error 0.0196\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1520 | Time 16.5950(16.7708) | Bit/dim 1.1005(1.1015) | Xent 0.0097(0.0156) | Loss 1.1053(1.1093) | Error 0.0022(0.0053) Steps 542(546.64) | Grad Norm 1.3531(1.6050) | Total Time 10.00(10.00)\n",
      "Iter 1530 | Time 16.7087(16.7603) | Bit/dim 1.1036(1.1032) | Xent 0.0167(0.0145) | Loss 1.1119(1.1104) | Error 0.0078(0.0048) Steps 542(546.82) | Grad Norm 2.1613(1.7483) | Total Time 10.00(10.00)\n",
      "Iter 1540 | Time 16.6729(16.7882) | Bit/dim 1.1029(1.1028) | Xent 0.0308(0.0147) | Loss 1.1183(1.1102) | Error 0.0067(0.0047) Steps 548(546.82) | Grad Norm 0.7169(1.6697) | Total Time 10.00(10.00)\n",
      "Iter 1550 | Time 16.8786(16.8136) | Bit/dim 1.0918(1.1019) | Xent 0.0074(0.0153) | Loss 1.0955(1.1096) | Error 0.0033(0.0050) Steps 548(547.61) | Grad Norm 2.6873(1.8815) | Total Time 10.00(10.00)\n",
      "Iter 1560 | Time 16.8433(16.8318) | Bit/dim 1.1103(1.1015) | Xent 0.0058(0.0145) | Loss 1.1132(1.1087) | Error 0.0033(0.0047) Steps 542(546.75) | Grad Norm 1.8582(1.8662) | Total Time 10.00(10.00)\n",
      "Iter 1570 | Time 16.6992(16.8069) | Bit/dim 1.0924(1.1010) | Xent 0.0117(0.0152) | Loss 1.0982(1.1086) | Error 0.0056(0.0051) Steps 548(546.31) | Grad Norm 1.3061(1.6877) | Total Time 10.00(10.00)\n",
      "Iter 1580 | Time 16.4786(16.7666) | Bit/dim 1.1055(1.1018) | Xent 0.0102(0.0160) | Loss 1.1106(1.1098) | Error 0.0033(0.0050) Steps 548(545.99) | Grad Norm 4.6737(1.7964) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0024 | Time 68.2493, Epoch Time 1190.3587(1193.7183), Bit/dim 1.0958, Xent 0.0354, Loss 1.1135, Error 0.0202\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1590 | Time 16.7745(16.7749) | Bit/dim 1.0992(1.1023) | Xent 0.0101(0.0161) | Loss 1.1042(1.1104) | Error 0.0022(0.0052) Steps 542(546.19) | Grad Norm 1.4404(2.0038) | Total Time 10.00(10.00)\n",
      "Iter 1600 | Time 16.5304(16.7700) | Bit/dim 1.1075(1.1005) | Xent 0.0161(0.0167) | Loss 1.1156(1.1089) | Error 0.0067(0.0056) Steps 554(547.49) | Grad Norm 2.5832(2.0040) | Total Time 10.00(10.00)\n",
      "Iter 1610 | Time 16.6720(16.7524) | Bit/dim 1.1010(1.1019) | Xent 0.0096(0.0160) | Loss 1.1058(1.1099) | Error 0.0044(0.0052) Steps 548(547.12) | Grad Norm 0.6887(1.8295) | Total Time 10.00(10.00)\n",
      "Iter 1620 | Time 16.8101(16.8023) | Bit/dim 1.0923(1.1024) | Xent 0.0083(0.0160) | Loss 1.0964(1.1104) | Error 0.0022(0.0050) Steps 548(547.03) | Grad Norm 1.9053(1.7551) | Total Time 10.00(10.00)\n",
      "Iter 1630 | Time 17.0101(16.8313) | Bit/dim 1.1050(1.1026) | Xent 0.0143(0.0153) | Loss 1.1121(1.1102) | Error 0.0056(0.0048) Steps 548(546.53) | Grad Norm 0.7374(1.5804) | Total Time 10.00(10.00)\n",
      "Iter 1640 | Time 16.1639(16.7694) | Bit/dim 1.0863(1.1010) | Xent 0.0073(0.0153) | Loss 1.0900(1.1086) | Error 0.0011(0.0047) Steps 548(546.45) | Grad Norm 1.6074(1.4465) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0025 | Time 68.1468, Epoch Time 1190.1996(1193.6128), Bit/dim 1.0949, Xent 0.0315, Loss 1.1107, Error 0.0195\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n"
     ]
    }
   ],
   "source": [
    "%run -p train_cnf_conditional.py --data mnist --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 900 --save experiments/cnf_cond_bs900_wy_0_5_cont_lr_0_0001 --resume experiments/cnf_cond_bs900_wy_0_5/best_nll_checkpt.pth --conditional True --log_freq 10 --weight_y 0.5 --lr 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
