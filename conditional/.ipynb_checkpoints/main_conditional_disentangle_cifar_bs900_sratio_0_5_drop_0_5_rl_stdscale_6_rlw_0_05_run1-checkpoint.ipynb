{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0,1,2,3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_rl.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "GATES = [\"cnn1\", \"cnn2\", \"rnn\"]\n",
      "\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--gate', type=str, default='cnn1', choices=GATES)\n",
      "parser.add_argument('--scale', type=float, default=1.0)\n",
      "parser.add_argument('--scale_fac', type=float, default=1.0)\n",
      "parser.add_argument('--scale_std', type=float, default=1.0)\n",
      "parser.add_argument('--eta', default=0.1, type=float,\n",
      "                        help='tuning parameter that allows us to trade-off the competing goals of' \n",
      "                                'minimizing the prediction loss and maximizing the gate rewards ')\n",
      "parser.add_argument('--rl-weight', default=0.01, type=float,\n",
      "                        help='rl weight')\n",
      "\n",
      "parser.add_argument('--gamma', default=0.99, type=float,\n",
      "                        help='discount factor, default: (0.99)')\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "import lib.odenvp_conditional_rl as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol, \"scale\": args.scale, \"scale_fac\": args.scale_fac, \"scale_std\": args.scale_std, \"gate\": args.gate},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            # re-weight the gate rewards\n",
      "            normalized_eta = args.eta / len(logp_actions)\n",
      "            \n",
      "            # collect cumulative future rewards\n",
      "            R = - loss\n",
      "            cum_rewards = []\n",
      "            for r in nfe[::-1]:\n",
      "                R = -normalized_eta * r.view(-1,1) + args.gamma * R\n",
      "                cum_rewards.insert(0,R)\n",
      "            \n",
      "            # apply REINFORCE\n",
      "            rl_loss = 0\n",
      "            for lpa, r in zip(logp_actions, cum_rewards):\n",
      "                rl_loss = rl_loss - lpa.view(-1,1) * args.rl_weight * r\n",
      "                \n",
      "            loss = loss + rl_loss.mean()\n",
      "            \n",
      "            loss.backward()\n",
      "            \n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "            \n",
      "            for idx in range(len(model.module.transforms)):\n",
      "                for layer in model.module.transforms[idx].chain:\n",
      "                    if hasattr(layer, 'atol'):\n",
      "                        layer.odefunc.after_odeint()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'train': atol[tol_indx].mean()}, itr)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'train': rtol[tol_indx].mean()}, itr)\n",
      "                    \n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples, atol, rtol, logp_actions, nfe = model(fixed_z, reverse=True)\n",
      "            generated_samples = generated_samples.view(-1, *data_shape)\n",
      "            for tol_indx in range(len(atol)):\n",
      "                writer.add_scalars('atol_gen_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                writer.add_scalars('rtol_gen_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, condition_ratio=0.5, conditional=True, controlled_tol=False, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, eta=0.1, gamma=0.99, gate='cnn1', imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume='../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_6_rlw_0_05_run1/current_checkpt.pth', rl_weight=0.05, rtol=1e-05, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_6_rlw_0_05_run1', scale=1.0, scale_fac=1.0, scale_std=6.0, seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1450732\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc_rl.py:291: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Iter 4460 | Time 17.5219(18.4359) | Bit/dim 3.6623(3.6893) | Xent 0.7256(0.7126) | Loss 30.0366(34.7590) | Error 0.2444(0.2543) Steps 0(0.00) | Grad Norm 20.8113(25.5265) | Total Time 0.00(0.00)\n",
      "Iter 4470 | Time 18.2529(18.4976) | Bit/dim 3.6792(3.6877) | Xent 0.6610(0.7084) | Loss 30.6078(33.6770) | Error 0.2389(0.2513) Steps 0(0.00) | Grad Norm 17.2502(23.9723) | Total Time 0.00(0.00)\n",
      "Iter 4480 | Time 18.8421(18.5582) | Bit/dim 3.6596(3.6883) | Xent 0.7435(0.7068) | Loss 30.4208(32.8918) | Error 0.2544(0.2509) Steps 0(0.00) | Grad Norm 27.5603(24.1576) | Total Time 0.00(0.00)\n",
      "Iter 4490 | Time 18.3824(18.5815) | Bit/dim 3.6920(3.6870) | Xent 0.7209(0.7025) | Loss 30.1852(32.2941) | Error 0.2600(0.2504) Steps 0(0.00) | Grad Norm 31.1725(24.8808) | Total Time 0.00(0.00)\n",
      "Iter 4500 | Time 18.2338(18.6414) | Bit/dim 3.6514(3.6864) | Xent 0.7048(0.7030) | Loss 31.0605(31.8726) | Error 0.2544(0.2506) Steps 0(0.00) | Grad Norm 32.8524(26.8932) | Total Time 0.00(0.00)\n",
      "Iter 4510 | Time 17.7598(18.4557) | Bit/dim 3.6751(3.6882) | Xent 0.7276(0.7079) | Loss 30.5630(31.5154) | Error 0.2633(0.2524) Steps 0(0.00) | Grad Norm 21.5839(26.4228) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0082 | Time 97.6061, Epoch Time 1158.8397(1053.9536), Bit/dim 3.6905(best: inf), Xent 0.7993, Loss 4.0901, Error 0.2817(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4520 | Time 18.8307(18.3911) | Bit/dim 3.6466(3.6824) | Xent 0.6748(0.7064) | Loss 29.9092(34.1227) | Error 0.2433(0.2523) Steps 0(0.00) | Grad Norm 16.0572(25.1677) | Total Time 0.00(0.00)\n",
      "Iter 4530 | Time 19.6354(18.3946) | Bit/dim 3.6855(3.6849) | Xent 0.7627(0.7083) | Loss 30.4709(33.1783) | Error 0.2711(0.2517) Steps 0(0.00) | Grad Norm 34.7435(26.2768) | Total Time 0.00(0.00)\n",
      "Iter 4540 | Time 16.9505(18.4505) | Bit/dim 3.6944(3.6869) | Xent 0.6805(0.7059) | Loss 30.3332(32.4978) | Error 0.2344(0.2497) Steps 0(0.00) | Grad Norm 17.0315(26.0989) | Total Time 0.00(0.00)\n",
      "Iter 4550 | Time 17.5653(18.2644) | Bit/dim 3.6822(3.6870) | Xent 0.7030(0.7013) | Loss 30.7540(31.9088) | Error 0.2478(0.2494) Steps 0(0.00) | Grad Norm 36.5810(25.8147) | Total Time 0.00(0.00)\n",
      "Iter 4560 | Time 17.9879(18.1987) | Bit/dim 3.7013(3.6885) | Xent 0.7635(0.7041) | Loss 31.3655(31.5816) | Error 0.2811(0.2509) Steps 0(0.00) | Grad Norm 31.4667(26.7037) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0083 | Time 92.9607, Epoch Time 1114.0467(1055.7564), Bit/dim 3.6884(best: 3.6905), Xent 0.7776, Loss 4.0772, Error 0.2783(best: 0.2817)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4570 | Time 17.3831(18.1502) | Bit/dim 3.6742(3.6879) | Xent 0.6684(0.7076) | Loss 30.4758(34.9148) | Error 0.2378(0.2518) Steps 0(0.00) | Grad Norm 33.9592(28.1373) | Total Time 0.00(0.00)\n",
      "Iter 4580 | Time 17.2127(18.1661) | Bit/dim 3.7082(3.6893) | Xent 0.6615(0.7109) | Loss 29.2106(33.8480) | Error 0.2444(0.2524) Steps 0(0.00) | Grad Norm 18.4053(28.0545) | Total Time 0.00(0.00)\n",
      "Iter 4590 | Time 18.1673(18.1741) | Bit/dim 3.6929(3.6895) | Xent 0.6912(0.7076) | Loss 30.7495(32.9555) | Error 0.2511(0.2519) Steps 0(0.00) | Grad Norm 21.6292(27.7618) | Total Time 0.00(0.00)\n",
      "Iter 4600 | Time 20.3263(18.3504) | Bit/dim 3.6939(3.6861) | Xent 0.7272(0.7097) | Loss 30.5710(32.3098) | Error 0.2678(0.2521) Steps 0(0.00) | Grad Norm 28.9925(28.4583) | Total Time 0.00(0.00)\n",
      "Iter 4610 | Time 18.7058(18.3680) | Bit/dim 3.7283(3.6890) | Xent 0.6493(0.7047) | Loss 30.4437(31.8210) | Error 0.2378(0.2511) Steps 0(0.00) | Grad Norm 40.5512(28.3694) | Total Time 0.00(0.00)\n",
      "Iter 4620 | Time 18.2007(18.3543) | Bit/dim 3.6719(3.6860) | Xent 0.6510(0.7035) | Loss 30.6678(31.5419) | Error 0.2422(0.2504) Steps 0(0.00) | Grad Norm 17.2354(26.5968) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0084 | Time 95.3543, Epoch Time 1120.3439(1057.6940), Bit/dim 3.6829(best: 3.6884), Xent 0.7378, Loss 4.0518, Error 0.2619(best: 0.2783)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4630 | Time 18.3065(18.3499) | Bit/dim 3.6888(3.6876) | Xent 0.7439(0.6978) | Loss 30.2885(34.3445) | Error 0.2489(0.2481) Steps 0(0.00) | Grad Norm 31.6965(25.7005) | Total Time 0.00(0.00)\n",
      "Iter 4640 | Time 19.3115(18.3699) | Bit/dim 3.6496(3.6861) | Xent 0.6642(0.7013) | Loss 30.4222(33.4126) | Error 0.2278(0.2510) Steps 0(0.00) | Grad Norm 27.9161(26.6669) | Total Time 0.00(0.00)\n",
      "Iter 4650 | Time 21.3750(18.4451) | Bit/dim 3.7038(3.6874) | Xent 0.6872(0.6942) | Loss 31.5221(32.7313) | Error 0.2411(0.2480) Steps 0(0.00) | Grad Norm 19.4704(25.0876) | Total Time 0.00(0.00)\n",
      "Iter 4660 | Time 18.2579(18.2774) | Bit/dim 3.6565(3.6856) | Xent 0.6728(0.6906) | Loss 29.4900(32.1001) | Error 0.2367(0.2461) Steps 0(0.00) | Grad Norm 19.7484(23.8135) | Total Time 0.00(0.00)\n",
      "Iter 4670 | Time 18.8893(18.4108) | Bit/dim 3.6905(3.6827) | Xent 0.6871(0.6841) | Loss 31.0451(31.7329) | Error 0.2378(0.2437) Steps 0(0.00) | Grad Norm 31.7610(23.9585) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0085 | Time 92.7079, Epoch Time 1123.5626(1059.6700), Bit/dim 3.6803(best: 3.6829), Xent 0.7450, Loss 4.0529, Error 0.2601(best: 0.2619)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4680 | Time 19.3118(18.4795) | Bit/dim 3.6528(3.6757) | Xent 0.7067(0.6817) | Loss 31.2878(34.7598) | Error 0.2433(0.2429) Steps 0(0.00) | Grad Norm 35.8225(25.8194) | Total Time 0.00(0.00)\n",
      "Iter 4690 | Time 19.1136(18.4335) | Bit/dim 3.7134(3.6800) | Xent 0.7064(0.6771) | Loss 29.9124(33.5702) | Error 0.2711(0.2422) Steps 0(0.00) | Grad Norm 23.5623(25.2171) | Total Time 0.00(0.00)\n",
      "Iter 4700 | Time 19.6167(18.5162) | Bit/dim 3.7089(3.6816) | Xent 0.6560(0.6804) | Loss 30.9886(32.8175) | Error 0.2256(0.2432) Steps 0(0.00) | Grad Norm 23.7838(25.7904) | Total Time 0.00(0.00)\n",
      "Iter 4710 | Time 18.7613(18.4082) | Bit/dim 3.7008(3.6807) | Xent 0.7194(0.6855) | Loss 30.2855(32.1717) | Error 0.2467(0.2451) Steps 0(0.00) | Grad Norm 19.0752(25.9491) | Total Time 0.00(0.00)\n",
      "Iter 4720 | Time 24.6995(18.6148) | Bit/dim 3.7107(3.6811) | Xent 0.7621(0.6829) | Loss 32.1922(31.8200) | Error 0.2733(0.2438) Steps 0(0.00) | Grad Norm 32.4307(25.0021) | Total Time 0.00(0.00)\n",
      "Iter 4730 | Time 17.8923(18.5250) | Bit/dim 3.6783(3.6797) | Xent 0.7046(0.6835) | Loss 30.8242(31.4886) | Error 0.2478(0.2437) Steps 0(0.00) | Grad Norm 21.3197(24.9124) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0086 | Time 95.3548, Epoch Time 1132.6754(1061.8602), Bit/dim 3.6825(best: 3.6803), Xent 0.7518, Loss 4.0585, Error 0.2632(best: 0.2601)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4740 | Time 18.0615(18.6242) | Bit/dim 3.6627(3.6791) | Xent 0.6431(0.6765) | Loss 30.2570(34.0636) | Error 0.2278(0.2408) Steps 0(0.00) | Grad Norm 19.7117(24.6594) | Total Time 0.00(0.00)\n",
      "Iter 4750 | Time 17.2795(18.4991) | Bit/dim 3.6658(3.6781) | Xent 0.6729(0.6690) | Loss 30.0631(33.1205) | Error 0.2467(0.2387) Steps 0(0.00) | Grad Norm 14.4172(23.9283) | Total Time 0.00(0.00)\n",
      "Iter 4760 | Time 19.1547(18.4637) | Bit/dim 3.6709(3.6767) | Xent 0.6517(0.6687) | Loss 31.2269(32.4638) | Error 0.2267(0.2378) Steps 0(0.00) | Grad Norm 31.8367(24.9703) | Total Time 0.00(0.00)\n",
      "Iter 4770 | Time 20.7096(18.6323) | Bit/dim 3.6932(3.6769) | Xent 0.6781(0.6755) | Loss 31.1606(31.9549) | Error 0.2333(0.2400) Steps 0(0.00) | Grad Norm 23.9121(26.3619) | Total Time 0.00(0.00)\n",
      "Iter 4780 | Time 18.9277(18.7070) | Bit/dim 3.6432(3.6744) | Xent 0.7192(0.6818) | Loss 30.5180(31.5840) | Error 0.2489(0.2417) Steps 0(0.00) | Grad Norm 22.4311(26.1751) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0087 | Time 94.6028, Epoch Time 1136.7959(1064.1083), Bit/dim 3.6956(best: 3.6803), Xent 0.8104, Loss 4.1008, Error 0.2794(best: 0.2601)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4790 | Time 16.7682(18.5904) | Bit/dim 3.6961(3.6803) | Xent 0.6405(0.6871) | Loss 28.8162(34.8167) | Error 0.2222(0.2436) Steps 0(0.00) | Grad Norm 27.0095(27.9638) | Total Time 0.00(0.00)\n",
      "Iter 4800 | Time 20.3763(18.5276) | Bit/dim 3.6601(3.6830) | Xent 0.6578(0.6828) | Loss 30.4518(33.7823) | Error 0.2333(0.2422) Steps 0(0.00) | Grad Norm 19.7503(27.6480) | Total Time 0.00(0.00)\n",
      "Iter 4810 | Time 19.1186(18.4832) | Bit/dim 3.6571(3.6809) | Xent 0.6638(0.6728) | Loss 30.7857(32.9074) | Error 0.2356(0.2392) Steps 0(0.00) | Grad Norm 22.3129(27.2090) | Total Time 0.00(0.00)\n",
      "Iter 4820 | Time 17.9246(18.4234) | Bit/dim 3.7162(3.6819) | Xent 0.6700(0.6771) | Loss 29.6500(32.2362) | Error 0.2556(0.2410) Steps 0(0.00) | Grad Norm 29.7760(27.7383) | Total Time 0.00(0.00)\n",
      "Iter 4830 | Time 18.1021(18.7317) | Bit/dim 3.6563(3.6796) | Xent 0.6541(0.6776) | Loss 29.6931(31.7912) | Error 0.2367(0.2410) Steps 0(0.00) | Grad Norm 25.5797(27.3389) | Total Time 0.00(0.00)\n",
      "Iter 4840 | Time 17.6589(18.6545) | Bit/dim 3.7049(3.6783) | Xent 0.6839(0.6776) | Loss 30.7329(31.3941) | Error 0.2544(0.2404) Steps 0(0.00) | Grad Norm 25.4351(27.3359) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0088 | Time 95.4443, Epoch Time 1135.5467(1066.2514), Bit/dim 3.6763(best: 3.6803), Xent 0.7965, Loss 4.0745, Error 0.2799(best: 0.2601)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4850 | Time 17.8148(18.8339) | Bit/dim 3.6852(3.6794) | Xent 0.6368(0.6710) | Loss 30.0351(33.9960) | Error 0.2489(0.2395) Steps 0(0.00) | Grad Norm 18.0958(26.2919) | Total Time 0.00(0.00)\n",
      "Iter 4860 | Time 18.7968(18.7613) | Bit/dim 3.6698(3.6772) | Xent 0.7126(0.6689) | Loss 31.1080(33.1282) | Error 0.2600(0.2387) Steps 0(0.00) | Grad Norm 25.9054(24.7129) | Total Time 0.00(0.00)\n",
      "Iter 4870 | Time 20.6212(18.7701) | Bit/dim 3.6651(3.6727) | Xent 0.6451(0.6719) | Loss 30.7579(32.4706) | Error 0.2311(0.2382) Steps 0(0.00) | Grad Norm 17.8522(24.1875) | Total Time 0.00(0.00)\n",
      "Iter 4880 | Time 18.0023(18.7120) | Bit/dim 3.6641(3.6740) | Xent 0.7049(0.6761) | Loss 30.3757(31.9402) | Error 0.2456(0.2416) Steps 0(0.00) | Grad Norm 25.7365(24.2843) | Total Time 0.00(0.00)\n",
      "Iter 4890 | Time 19.2057(18.8229) | Bit/dim 3.6528(3.6734) | Xent 0.6813(0.6718) | Loss 30.6269(31.5600) | Error 0.2422(0.2403) Steps 0(0.00) | Grad Norm 22.2382(24.2168) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0089 | Time 94.1126, Epoch Time 1144.9103(1068.6112), Bit/dim 3.6675(best: 3.6763), Xent 0.7047, Loss 4.0198, Error 0.2487(best: 0.2601)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4900 | Time 18.4780(18.7072) | Bit/dim 3.6538(3.6735) | Xent 0.7325(0.6706) | Loss 30.5065(34.8467) | Error 0.2522(0.2397) Steps 0(0.00) | Grad Norm 36.5069(24.4946) | Total Time 0.00(0.00)\n",
      "Iter 4910 | Time 17.2838(18.6116) | Bit/dim 3.6736(3.6748) | Xent 0.6676(0.6655) | Loss 30.3948(33.6409) | Error 0.2456(0.2381) Steps 0(0.00) | Grad Norm 22.1310(23.4528) | Total Time 0.00(0.00)\n",
      "Iter 4920 | Time 18.3621(18.7210) | Bit/dim 3.6515(3.6698) | Xent 0.6815(0.6543) | Loss 31.5494(32.8792) | Error 0.2311(0.2338) Steps 0(0.00) | Grad Norm 18.9594(23.3280) | Total Time 0.00(0.00)\n",
      "Iter 4930 | Time 18.2298(18.6905) | Bit/dim 3.6837(3.6708) | Xent 0.6946(0.6557) | Loss 30.3319(32.2109) | Error 0.2467(0.2347) Steps 0(0.00) | Grad Norm 32.2850(24.4061) | Total Time 0.00(0.00)\n",
      "Iter 4940 | Time 17.1498(18.6751) | Bit/dim 3.6571(3.6693) | Xent 0.6099(0.6512) | Loss 30.3077(31.7295) | Error 0.2156(0.2324) Steps 0(0.00) | Grad Norm 34.5957(25.0029) | Total Time 0.00(0.00)\n",
      "Iter 4950 | Time 20.5158(18.7317) | Bit/dim 3.7014(3.6738) | Xent 0.6799(0.6574) | Loss 31.1862(31.4261) | Error 0.2389(0.2344) Steps 0(0.00) | Grad Norm 22.8952(25.5424) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0090 | Time 94.9376, Epoch Time 1142.6477(1070.8323), Bit/dim 3.6813(best: 3.6675), Xent 0.7469, Loss 4.0548, Error 0.2639(best: 0.2487)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4960 | Time 19.4688(18.6304) | Bit/dim 3.7055(3.6709) | Xent 0.5594(0.6460) | Loss 29.4265(33.7951) | Error 0.1978(0.2317) Steps 0(0.00) | Grad Norm 17.7727(24.5199) | Total Time 0.00(0.00)\n",
      "Iter 4970 | Time 19.2508(18.5528) | Bit/dim 3.6812(3.6723) | Xent 0.6589(0.6405) | Loss 30.5801(32.9249) | Error 0.2356(0.2291) Steps 0(0.00) | Grad Norm 31.2120(24.0288) | Total Time 0.00(0.00)\n",
      "Iter 4980 | Time 19.0470(18.6542) | Bit/dim 3.6470(3.6701) | Xent 0.7177(0.6485) | Loss 30.9152(32.3616) | Error 0.2433(0.2309) Steps 0(0.00) | Grad Norm 30.7597(24.1436) | Total Time 0.00(0.00)\n",
      "Iter 4990 | Time 18.6699(18.6095) | Bit/dim 3.6877(3.6716) | Xent 0.6103(0.6525) | Loss 30.9044(31.8720) | Error 0.2289(0.2339) Steps 0(0.00) | Grad Norm 24.6675(25.9556) | Total Time 0.00(0.00)\n",
      "Iter 5000 | Time 22.3328(18.7358) | Bit/dim 3.7045(3.6753) | Xent 0.6642(0.6632) | Loss 31.8172(31.5177) | Error 0.2378(0.2370) Steps 0(0.00) | Grad Norm 18.5467(26.7057) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0091 | Time 93.8918, Epoch Time 1136.1305(1072.7912), Bit/dim 3.6787(best: 3.6675), Xent 0.7379, Loss 4.0476, Error 0.2617(best: 0.2487)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5010 | Time 19.4575(18.6855) | Bit/dim 3.6688(3.6744) | Xent 0.6076(0.6545) | Loss 30.5506(34.8653) | Error 0.2133(0.2343) Steps 0(0.00) | Grad Norm 16.0354(26.1883) | Total Time 0.00(0.00)\n",
      "Iter 5020 | Time 19.8367(18.6268) | Bit/dim 3.6428(3.6719) | Xent 0.6677(0.6528) | Loss 30.6695(33.7041) | Error 0.2444(0.2342) Steps 0(0.00) | Grad Norm 28.7301(25.9379) | Total Time 0.00(0.00)\n",
      "Iter 5030 | Time 19.2946(18.6950) | Bit/dim 3.6664(3.6705) | Xent 0.6567(0.6470) | Loss 30.9935(32.7924) | Error 0.2467(0.2322) Steps 0(0.00) | Grad Norm 23.3636(24.3666) | Total Time 0.00(0.00)\n",
      "Iter 5040 | Time 19.3561(18.6603) | Bit/dim 3.6692(3.6697) | Xent 0.6297(0.6423) | Loss 30.9894(32.2084) | Error 0.2289(0.2303) Steps 0(0.00) | Grad Norm 41.0230(24.1814) | Total Time 0.00(0.00)\n",
      "Iter 5050 | Time 18.9278(18.7184) | Bit/dim 3.6537(3.6694) | Xent 0.6216(0.6388) | Loss 30.4392(31.7024) | Error 0.2211(0.2277) Steps 0(0.00) | Grad Norm 18.1511(23.6101) | Total Time 0.00(0.00)\n",
      "Iter 5060 | Time 17.6061(18.5635) | Bit/dim 3.6331(3.6691) | Xent 0.7317(0.6465) | Loss 30.3000(31.3232) | Error 0.2622(0.2306) Steps 0(0.00) | Grad Norm 30.8486(24.9526) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0092 | Time 95.8094, Epoch Time 1135.2777(1074.6658), Bit/dim 3.6670(best: 3.6675), Xent 0.7873, Loss 4.0606, Error 0.2700(best: 0.2487)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5070 | Time 19.0738(18.6020) | Bit/dim 3.6744(3.6716) | Xent 0.5942(0.6538) | Loss 31.1892(34.2484) | Error 0.2178(0.2332) Steps 0(0.00) | Grad Norm 16.4563(27.4399) | Total Time 0.00(0.00)\n",
      "Iter 5080 | Time 18.8884(18.6110) | Bit/dim 3.6575(3.6713) | Xent 0.5977(0.6546) | Loss 28.8808(33.2702) | Error 0.2067(0.2339) Steps 0(0.00) | Grad Norm 19.5526(27.8997) | Total Time 0.00(0.00)\n",
      "Iter 5090 | Time 17.5146(18.5861) | Bit/dim 3.6534(3.6735) | Xent 0.6741(0.6540) | Loss 29.5111(32.5422) | Error 0.2533(0.2345) Steps 0(0.00) | Grad Norm 16.4617(27.1371) | Total Time 0.00(0.00)\n",
      "Iter 5100 | Time 17.2232(18.6048) | Bit/dim 3.6696(3.6698) | Xent 0.5906(0.6579) | Loss 30.1908(32.0317) | Error 0.2133(0.2345) Steps 0(0.00) | Grad Norm 24.7561(26.3656) | Total Time 0.00(0.00)\n",
      "Iter 5110 | Time 17.6024(18.5484) | Bit/dim 3.6494(3.6724) | Xent 0.6690(0.6559) | Loss 30.5774(31.6319) | Error 0.2233(0.2337) Steps 0(0.00) | Grad Norm 37.5720(26.5054) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0093 | Time 95.9296, Epoch Time 1134.4217(1076.4585), Bit/dim 3.6653(best: 3.6670), Xent 0.7274, Loss 4.0290, Error 0.2524(best: 0.2487)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5120 | Time 18.1832(18.5227) | Bit/dim 3.7088(3.6723) | Xent 0.6240(0.6562) | Loss 30.9446(34.6163) | Error 0.2056(0.2331) Steps 0(0.00) | Grad Norm 21.7289(26.3740) | Total Time 0.00(0.00)\n",
      "Iter 5130 | Time 20.1089(18.7066) | Bit/dim 3.6212(3.6706) | Xent 0.6478(0.6571) | Loss 30.2111(33.5234) | Error 0.2300(0.2330) Steps 0(0.00) | Grad Norm 39.4679(26.9032) | Total Time 0.00(0.00)\n",
      "Iter 5140 | Time 18.2439(18.6837) | Bit/dim 3.6686(3.6695) | Xent 0.6433(0.6544) | Loss 29.4701(32.6840) | Error 0.2400(0.2332) Steps 0(0.00) | Grad Norm 22.1615(25.5454) | Total Time 0.00(0.00)\n",
      "Iter 5150 | Time 18.4714(18.4944) | Bit/dim 3.6456(3.6675) | Xent 0.6452(0.6503) | Loss 29.7791(32.0357) | Error 0.2344(0.2322) Steps 0(0.00) | Grad Norm 25.2166(25.5961) | Total Time 0.00(0.00)\n",
      "Iter 5160 | Time 20.0209(18.5835) | Bit/dim 3.6300(3.6643) | Xent 0.7049(0.6522) | Loss 29.9043(31.5606) | Error 0.2556(0.2327) Steps 0(0.00) | Grad Norm 26.7891(25.3438) | Total Time 0.00(0.00)\n",
      "Iter 5170 | Time 20.4757(18.6469) | Bit/dim 3.6521(3.6661) | Xent 0.6033(0.6496) | Loss 29.9910(31.3576) | Error 0.2144(0.2328) Steps 0(0.00) | Grad Norm 20.4949(24.6337) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0094 | Time 95.2333, Epoch Time 1141.0153(1078.3952), Bit/dim 3.6693(best: 3.6653), Xent 0.7400, Loss 4.0393, Error 0.2595(best: 0.2487)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5180 | Time 17.4130(18.5862) | Bit/dim 3.6666(3.6669) | Xent 0.5970(0.6427) | Loss 30.1057(34.1067) | Error 0.2189(0.2298) Steps 0(0.00) | Grad Norm 21.6021(24.7190) | Total Time 0.00(0.00)\n",
      "Iter 5190 | Time 18.7460(18.5731) | Bit/dim 3.6644(3.6665) | Xent 0.6533(0.6431) | Loss 31.1191(33.0610) | Error 0.2289(0.2295) Steps 0(0.00) | Grad Norm 20.4384(25.3298) | Total Time 0.00(0.00)\n",
      "Iter 5200 | Time 17.1646(18.6609) | Bit/dim 3.6615(3.6659) | Xent 0.7077(0.6444) | Loss 29.5607(32.3913) | Error 0.2467(0.2297) Steps 0(0.00) | Grad Norm 28.4604(26.9535) | Total Time 0.00(0.00)\n",
      "Iter 5210 | Time 18.9880(18.8159) | Bit/dim 3.6783(3.6678) | Xent 0.6363(0.6481) | Loss 30.9266(31.8593) | Error 0.2378(0.2304) Steps 0(0.00) | Grad Norm 21.9100(27.8621) | Total Time 0.00(0.00)\n",
      "Iter 5220 | Time 18.8267(18.8470) | Bit/dim 3.6621(3.6680) | Xent 0.5416(0.6414) | Loss 29.2403(31.4706) | Error 0.1933(0.2278) Steps 0(0.00) | Grad Norm 16.5586(25.8749) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0095 | Time 96.7690, Epoch Time 1153.5617(1080.6502), Bit/dim 3.6658(best: 3.6653), Xent 0.7055, Loss 4.0185, Error 0.2457(best: 0.2487)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5230 | Time 16.8296(18.8843) | Bit/dim 3.6684(3.6675) | Xent 0.6192(0.6402) | Loss 30.6393(34.7791) | Error 0.2111(0.2275) Steps 0(0.00) | Grad Norm 19.0896(25.4869) | Total Time 0.00(0.00)\n",
      "Iter 5240 | Time 17.0453(18.8694) | Bit/dim 3.6626(3.6675) | Xent 0.5972(0.6287) | Loss 30.3647(33.6310) | Error 0.2189(0.2238) Steps 0(0.00) | Grad Norm 32.9877(24.7060) | Total Time 0.00(0.00)\n",
      "Iter 5250 | Time 20.9268(18.7183) | Bit/dim 3.6738(3.6672) | Xent 0.6199(0.6241) | Loss 30.7244(32.7663) | Error 0.2167(0.2212) Steps 0(0.00) | Grad Norm 26.6645(23.4289) | Total Time 0.00(0.00)\n",
      "Iter 5260 | Time 19.4981(18.8499) | Bit/dim 3.6510(3.6638) | Xent 0.6561(0.6293) | Loss 30.7929(32.1331) | Error 0.2467(0.2229) Steps 0(0.00) | Grad Norm 28.0987(24.4846) | Total Time 0.00(0.00)\n",
      "Iter 5270 | Time 19.1393(18.6338) | Bit/dim 3.6751(3.6614) | Xent 0.5968(0.6358) | Loss 30.1335(31.6596) | Error 0.2133(0.2261) Steps 0(0.00) | Grad Norm 26.2826(25.7748) | Total Time 0.00(0.00)\n",
      "Iter 5280 | Time 18.8576(18.6708) | Bit/dim 3.6624(3.6643) | Xent 0.6394(0.6376) | Loss 30.4092(31.3555) | Error 0.2267(0.2269) Steps 0(0.00) | Grad Norm 17.5371(25.1091) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0096 | Time 95.7217, Epoch Time 1136.5470(1082.3271), Bit/dim 3.6676(best: 3.6653), Xent 0.7187, Loss 4.0269, Error 0.2499(best: 0.2457)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5290 | Time 17.7822(18.6688) | Bit/dim 3.6714(3.6636) | Xent 0.6246(0.6352) | Loss 31.1088(34.1696) | Error 0.2133(0.2263) Steps 0(0.00) | Grad Norm 28.4675(24.1649) | Total Time 0.00(0.00)\n",
      "Iter 5300 | Time 18.9731(18.6641) | Bit/dim 3.6691(3.6638) | Xent 0.5887(0.6330) | Loss 29.8954(33.1613) | Error 0.2133(0.2260) Steps 0(0.00) | Grad Norm 26.9577(24.3846) | Total Time 0.00(0.00)\n",
      "Iter 5310 | Time 18.4685(18.5929) | Bit/dim 3.6872(3.6655) | Xent 0.5299(0.6248) | Loss 30.3436(32.4595) | Error 0.1822(0.2243) Steps 0(0.00) | Grad Norm 17.6041(24.0716) | Total Time 0.00(0.00)\n",
      "Iter 5320 | Time 19.3590(18.5360) | Bit/dim 3.6450(3.6650) | Xent 0.6856(0.6353) | Loss 31.4860(31.9651) | Error 0.2433(0.2267) Steps 0(0.00) | Grad Norm 32.3200(26.0155) | Total Time 0.00(0.00)\n",
      "Iter 5330 | Time 19.4743(18.6835) | Bit/dim 3.6861(3.6674) | Xent 0.5879(0.6369) | Loss 31.0405(31.6497) | Error 0.2100(0.2274) Steps 0(0.00) | Grad Norm 16.9507(25.2915) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0097 | Time 96.2502, Epoch Time 1149.4936(1084.3421), Bit/dim 3.6642(best: 3.6653), Xent 0.7202, Loss 4.0243, Error 0.2557(best: 0.2457)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5340 | Time 19.6196(19.0806) | Bit/dim 3.6747(3.6660) | Xent 0.5839(0.6357) | Loss 29.6349(34.9310) | Error 0.1922(0.2264) Steps 0(0.00) | Grad Norm 15.4107(24.5766) | Total Time 0.00(0.00)\n",
      "Iter 5350 | Time 21.3923(19.0183) | Bit/dim 3.6553(3.6659) | Xent 0.6397(0.6277) | Loss 30.8397(33.7574) | Error 0.2344(0.2232) Steps 0(0.00) | Grad Norm 26.3335(24.0673) | Total Time 0.00(0.00)\n",
      "Iter 5360 | Time 18.3326(18.9635) | Bit/dim 3.6532(3.6626) | Xent 0.6065(0.6267) | Loss 29.6226(32.8610) | Error 0.2133(0.2241) Steps 0(0.00) | Grad Norm 25.6026(24.7729) | Total Time 0.00(0.00)\n",
      "Iter 5370 | Time 17.2783(18.9901) | Bit/dim 3.6547(3.6627) | Xent 0.5856(0.6275) | Loss 29.9423(32.2922) | Error 0.2156(0.2247) Steps 0(0.00) | Grad Norm 19.5239(25.6738) | Total Time 0.00(0.00)\n",
      "Iter 5380 | Time 17.5992(18.8124) | Bit/dim 3.6220(3.6612) | Xent 0.6348(0.6273) | Loss 28.7265(31.6027) | Error 0.2144(0.2240) Steps 0(0.00) | Grad Norm 17.0519(25.2049) | Total Time 0.00(0.00)\n",
      "Iter 5390 | Time 18.1480(18.7507) | Bit/dim 3.6499(3.6600) | Xent 0.5703(0.6221) | Loss 29.7199(31.3086) | Error 0.2078(0.2219) Steps 0(0.00) | Grad Norm 16.8728(23.5111) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0098 | Time 94.1948, Epoch Time 1146.8731(1086.2180), Bit/dim 3.6648(best: 3.6642), Xent 0.7038, Loss 4.0167, Error 0.2455(best: 0.2457)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5400 | Time 19.6261(18.9692) | Bit/dim 3.6475(3.6595) | Xent 0.5892(0.6167) | Loss 29.2824(33.9478) | Error 0.2156(0.2199) Steps 0(0.00) | Grad Norm 20.1019(22.6183) | Total Time 0.00(0.00)\n",
      "Iter 5410 | Time 19.3774(19.1244) | Bit/dim 3.6200(3.6559) | Xent 0.6295(0.6184) | Loss 29.9153(33.0384) | Error 0.2200(0.2189) Steps 0(0.00) | Grad Norm 15.0030(23.0345) | Total Time 0.00(0.00)\n",
      "Iter 5420 | Time 18.5426(19.0114) | Bit/dim 3.6706(3.6572) | Xent 0.6840(0.6228) | Loss 30.4336(32.2217) | Error 0.2489(0.2210) Steps 0(0.00) | Grad Norm 37.2987(24.2102) | Total Time 0.00(0.00)\n",
      "Iter 5430 | Time 17.1105(18.8975) | Bit/dim 3.6686(3.6596) | Xent 0.6024(0.6235) | Loss 30.1108(31.8127) | Error 0.2233(0.2222) Steps 0(0.00) | Grad Norm 32.5261(24.3423) | Total Time 0.00(0.00)\n",
      "Iter 5440 | Time 19.6654(18.8770) | Bit/dim 3.6648(3.6610) | Xent 0.6148(0.6259) | Loss 31.1899(31.4296) | Error 0.2111(0.2223) Steps 0(0.00) | Grad Norm 30.9175(26.2119) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0099 | Time 97.5700, Epoch Time 1164.1386(1088.5556), Bit/dim 3.6678(best: 3.6642), Xent 0.7056, Loss 4.0206, Error 0.2490(best: 0.2455)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5450 | Time 19.2781(18.8429) | Bit/dim 3.6374(3.6605) | Xent 0.6188(0.6211) | Loss 30.3639(34.6867) | Error 0.2256(0.2217) Steps 0(0.00) | Grad Norm 20.8805(25.0168) | Total Time 0.00(0.00)\n",
      "Iter 5460 | Time 17.9383(18.8627) | Bit/dim 3.6724(3.6601) | Xent 0.6320(0.6117) | Loss 30.0546(33.6406) | Error 0.2278(0.2176) Steps 0(0.00) | Grad Norm 21.5542(23.8666) | Total Time 0.00(0.00)\n",
      "Iter 5470 | Time 18.4026(18.9007) | Bit/dim 3.6176(3.6622) | Xent 0.6039(0.6092) | Loss 29.9678(32.7705) | Error 0.2033(0.2169) Steps 0(0.00) | Grad Norm 20.0185(24.1580) | Total Time 0.00(0.00)\n",
      "Iter 5480 | Time 18.9525(18.9965) | Bit/dim 3.6263(3.6600) | Xent 0.5718(0.6054) | Loss 29.7838(32.1513) | Error 0.1989(0.2159) Steps 0(0.00) | Grad Norm 22.0887(23.5206) | Total Time 0.00(0.00)\n",
      "Iter 5490 | Time 17.7932(19.0268) | Bit/dim 3.6871(3.6589) | Xent 0.6733(0.6072) | Loss 30.8680(31.6959) | Error 0.2411(0.2173) Steps 0(0.00) | Grad Norm 25.4671(22.7185) | Total Time 0.00(0.00)\n",
      "Iter 5500 | Time 18.2645(18.9610) | Bit/dim 3.6711(3.6592) | Xent 0.6329(0.6072) | Loss 29.4193(31.2671) | Error 0.2267(0.2164) Steps 0(0.00) | Grad Norm 27.8258(22.6852) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0100 | Time 96.5849, Epoch Time 1158.3463(1090.6494), Bit/dim 3.6634(best: 3.6642), Xent 0.7127, Loss 4.0198, Error 0.2468(best: 0.2455)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5510 | Time 18.8821(19.1564) | Bit/dim 3.6847(3.6591) | Xent 0.5669(0.6061) | Loss 29.4980(34.0986) | Error 0.1889(0.2143) Steps 0(0.00) | Grad Norm 23.0500(23.1168) | Total Time 0.00(0.00)\n",
      "Iter 5520 | Time 19.3340(19.1592) | Bit/dim 3.6469(3.6561) | Xent 0.6024(0.6020) | Loss 30.6872(33.1984) | Error 0.2156(0.2137) Steps 0(0.00) | Grad Norm 23.4674(23.3638) | Total Time 0.00(0.00)\n",
      "Iter 5530 | Time 18.3674(19.0686) | Bit/dim 3.6573(3.6572) | Xent 0.6654(0.6042) | Loss 30.8082(32.4884) | Error 0.2322(0.2148) Steps 0(0.00) | Grad Norm 23.9003(24.2485) | Total Time 0.00(0.00)\n",
      "Iter 5540 | Time 20.9673(19.1264) | Bit/dim 3.6865(3.6583) | Xent 0.6142(0.6089) | Loss 30.9580(31.8963) | Error 0.2244(0.2166) Steps 0(0.00) | Grad Norm 24.9225(23.4171) | Total Time 0.00(0.00)\n",
      "Iter 5550 | Time 18.7029(19.4991) | Bit/dim 3.6481(3.6589) | Xent 0.5762(0.6079) | Loss 29.2176(31.4744) | Error 0.2011(0.2155) Steps 0(0.00) | Grad Norm 15.2191(22.6729) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0101 | Time 95.3398, Epoch Time 1185.4294(1093.4928), Bit/dim 3.6584(best: 3.6634), Xent 0.6736, Loss 3.9952, Error 0.2343(best: 0.2455)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5560 | Time 18.6677(19.5862) | Bit/dim 3.6613(3.6569) | Xent 0.6272(0.5995) | Loss 30.3842(34.7313) | Error 0.2222(0.2129) Steps 0(0.00) | Grad Norm 41.1611(22.1098) | Total Time 0.00(0.00)\n",
      "Iter 5570 | Time 19.5235(19.4225) | Bit/dim 3.6185(3.6574) | Xent 0.6024(0.6048) | Loss 30.1427(33.5870) | Error 0.2267(0.2158) Steps 0(0.00) | Grad Norm 27.2187(24.3583) | Total Time 0.00(0.00)\n",
      "Iter 5580 | Time 17.7700(19.2969) | Bit/dim 3.6396(3.6582) | Xent 0.6385(0.5969) | Loss 30.3879(32.7113) | Error 0.2411(0.2132) Steps 0(0.00) | Grad Norm 21.8600(24.5859) | Total Time 0.00(0.00)\n",
      "Iter 5590 | Time 17.2150(19.2580) | Bit/dim 3.6635(3.6585) | Xent 0.6556(0.5947) | Loss 29.3762(32.0470) | Error 0.2433(0.2127) Steps 0(0.00) | Grad Norm 23.5613(23.6326) | Total Time 0.00(0.00)\n",
      "Iter 5600 | Time 19.8737(19.2617) | Bit/dim 3.6277(3.6560) | Xent 0.5558(0.5991) | Loss 29.7977(31.5688) | Error 0.1989(0.2141) Steps 0(0.00) | Grad Norm 23.0684(23.6233) | Total Time 0.00(0.00)\n",
      "Iter 5610 | Time 18.1524(19.2416) | Bit/dim 3.6415(3.6543) | Xent 0.5960(0.5993) | Loss 29.0890(31.1591) | Error 0.2167(0.2139) Steps 0(0.00) | Grad Norm 17.3005(24.0645) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0102 | Time 96.8725, Epoch Time 1170.7760(1095.8113), Bit/dim 3.6642(best: 3.6584), Xent 0.7007, Loss 4.0146, Error 0.2419(best: 0.2343)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5620 | Time 18.6006(19.3060) | Bit/dim 3.6669(3.6591) | Xent 0.5588(0.5967) | Loss 30.4974(34.1790) | Error 0.2078(0.2130) Steps 0(0.00) | Grad Norm 21.1550(23.6942) | Total Time 0.00(0.00)\n",
      "Iter 5630 | Time 19.4727(19.4368) | Bit/dim 3.7115(3.6590) | Xent 0.5962(0.5969) | Loss 30.7258(33.2070) | Error 0.2144(0.2128) Steps 0(0.00) | Grad Norm 33.9122(25.8382) | Total Time 0.00(0.00)\n",
      "Iter 5640 | Time 20.2157(19.4710) | Bit/dim 3.6570(3.6594) | Xent 0.6346(0.6028) | Loss 30.4049(32.4556) | Error 0.2233(0.2161) Steps 0(0.00) | Grad Norm 28.7729(26.5196) | Total Time 0.00(0.00)\n",
      "Iter 5650 | Time 20.2773(19.3650) | Bit/dim 3.6389(3.6579) | Xent 0.6039(0.6050) | Loss 30.7150(31.9093) | Error 0.2211(0.2170) Steps 0(0.00) | Grad Norm 21.3428(27.2190) | Total Time 0.00(0.00)\n",
      "Iter 5660 | Time 18.8634(19.1368) | Bit/dim 3.6388(3.6590) | Xent 0.6382(0.6115) | Loss 30.2140(31.4942) | Error 0.2367(0.2188) Steps 0(0.00) | Grad Norm 24.5739(26.1576) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0103 | Time 95.8967, Epoch Time 1176.9483(1098.2454), Bit/dim 3.6636(best: 3.6584), Xent 0.7219, Loss 4.0246, Error 0.2562(best: 0.2343)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5670 | Time 19.3034(19.2444) | Bit/dim 3.6571(3.6569) | Xent 0.6048(0.6044) | Loss 29.7692(34.8415) | Error 0.2233(0.2164) Steps 0(0.00) | Grad Norm 31.4282(25.5194) | Total Time 0.00(0.00)\n",
      "Iter 5680 | Time 21.5113(19.3006) | Bit/dim 3.6245(3.6530) | Xent 0.5898(0.5999) | Loss 30.8806(33.6125) | Error 0.2178(0.2153) Steps 0(0.00) | Grad Norm 17.8814(24.6226) | Total Time 0.00(0.00)\n",
      "Iter 5690 | Time 17.9326(19.2805) | Bit/dim 3.6513(3.6523) | Xent 0.6089(0.6009) | Loss 27.7466(32.5803) | Error 0.2211(0.2152) Steps 0(0.00) | Grad Norm 30.0920(25.5867) | Total Time 0.00(0.00)\n",
      "Iter 5700 | Time 18.2123(19.3037) | Bit/dim 3.6393(3.6559) | Xent 0.5700(0.5995) | Loss 29.4183(31.9719) | Error 0.2167(0.2150) Steps 0(0.00) | Grad Norm 17.6305(25.4027) | Total Time 0.00(0.00)\n",
      "Iter 5710 | Time 21.2499(19.5002) | Bit/dim 3.6683(3.6570) | Xent 0.6018(0.5962) | Loss 31.0362(31.5956) | Error 0.2122(0.2130) Steps 0(0.00) | Grad Norm 18.4164(24.5662) | Total Time 0.00(0.00)\n",
      "Iter 5720 | Time 21.6107(19.4316) | Bit/dim 3.6726(3.6556) | Xent 0.6690(0.5937) | Loss 30.4751(31.1692) | Error 0.2367(0.2118) Steps 0(0.00) | Grad Norm 23.5372(23.3761) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0104 | Time 96.3796, Epoch Time 1183.5560(1100.8047), Bit/dim 3.6526(best: 3.6584), Xent 0.6748, Loss 3.9900, Error 0.2354(best: 0.2343)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5730 | Time 19.3058(19.4805) | Bit/dim 3.6004(3.6533) | Xent 0.6285(0.5906) | Loss 30.6660(34.0801) | Error 0.2344(0.2113) Steps 0(0.00) | Grad Norm 28.3830(23.7622) | Total Time 0.00(0.00)\n",
      "Iter 5740 | Time 18.6765(19.2245) | Bit/dim 3.6326(3.6551) | Xent 0.5362(0.5902) | Loss 30.0013(33.1187) | Error 0.1978(0.2118) Steps 0(0.00) | Grad Norm 26.4322(24.7028) | Total Time 0.00(0.00)\n",
      "Iter 5750 | Time 18.6592(19.1591) | Bit/dim 3.6504(3.6542) | Xent 0.6299(0.5932) | Loss 31.3760(32.4638) | Error 0.2422(0.2136) Steps 0(0.00) | Grad Norm 43.6868(26.7136) | Total Time 0.00(0.00)\n",
      "Iter 5760 | Time 17.8675(19.0642) | Bit/dim 3.6767(3.6563) | Xent 0.6176(0.5991) | Loss 30.7817(31.8545) | Error 0.2122(0.2153) Steps 0(0.00) | Grad Norm 21.8332(26.6942) | Total Time 0.00(0.00)\n",
      "Iter 5770 | Time 19.5810(19.1935) | Bit/dim 3.6369(3.6567) | Xent 0.6252(0.6075) | Loss 29.6743(31.4398) | Error 0.2222(0.2176) Steps 0(0.00) | Grad Norm 20.6905(26.3204) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0105 | Time 95.8605, Epoch Time 1162.2837(1102.6491), Bit/dim 3.6535(best: 3.6526), Xent 0.7212, Loss 4.0140, Error 0.2439(best: 0.2343)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5780 | Time 19.7385(19.1532) | Bit/dim 3.6667(3.6558) | Xent 0.5262(0.6023) | Loss 30.1694(34.7101) | Error 0.1767(0.2148) Steps 0(0.00) | Grad Norm 17.9195(26.0616) | Total Time 0.00(0.00)\n",
      "Iter 5790 | Time 18.4398(19.0482) | Bit/dim 3.6269(3.6559) | Xent 0.5402(0.5963) | Loss 30.1467(33.6116) | Error 0.1856(0.2117) Steps 0(0.00) | Grad Norm 15.9933(24.9709) | Total Time 0.00(0.00)\n",
      "Iter 5800 | Time 19.1740(19.1374) | Bit/dim 3.6651(3.6529) | Xent 0.5635(0.5890) | Loss 30.8272(32.6896) | Error 0.1978(0.2095) Steps 0(0.00) | Grad Norm 14.1950(22.9340) | Total Time 0.00(0.00)\n",
      "Iter 5810 | Time 18.9970(19.2529) | Bit/dim 3.6482(3.6527) | Xent 0.5350(0.5889) | Loss 30.5080(32.0847) | Error 0.1911(0.2094) Steps 0(0.00) | Grad Norm 25.4565(22.7179) | Total Time 0.00(0.00)\n",
      "Iter 5820 | Time 18.5168(19.1927) | Bit/dim 3.6395(3.6535) | Xent 0.6236(0.5919) | Loss 30.5042(31.6711) | Error 0.2178(0.2105) Steps 0(0.00) | Grad Norm 33.7204(23.3524) | Total Time 0.00(0.00)\n",
      "Iter 5830 | Time 18.6447(19.1856) | Bit/dim 3.6329(3.6512) | Xent 0.5952(0.5954) | Loss 30.0646(31.3621) | Error 0.2289(0.2125) Steps 0(0.00) | Grad Norm 20.5660(23.2482) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0106 | Time 96.8598, Epoch Time 1170.8349(1104.6946), Bit/dim 3.6598(best: 3.6526), Xent 0.6928, Loss 4.0062, Error 0.2380(best: 0.2343)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5840 | Time 25.0710(19.3741) | Bit/dim 3.6397(3.6485) | Xent 0.5671(0.5907) | Loss 31.5530(34.1022) | Error 0.2100(0.2111) Steps 0(0.00) | Grad Norm 22.3849(22.1727) | Total Time 0.00(0.00)\n",
      "Iter 5850 | Time 20.1556(19.3170) | Bit/dim 3.6499(3.6473) | Xent 0.5898(0.5874) | Loss 30.3377(33.0617) | Error 0.1844(0.2094) Steps 0(0.00) | Grad Norm 33.8022(23.2095) | Total Time 0.00(0.00)\n",
      "Iter 5860 | Time 19.2569(19.1010) | Bit/dim 3.6512(3.6507) | Xent 0.5632(0.5870) | Loss 30.2309(32.2904) | Error 0.2056(0.2094) Steps 0(0.00) | Grad Norm 23.9017(23.6995) | Total Time 0.00(0.00)\n",
      "Iter 5870 | Time 19.5858(19.2430) | Bit/dim 3.6611(3.6517) | Xent 0.5461(0.5821) | Loss 31.2079(31.8352) | Error 0.1978(0.2088) Steps 0(0.00) | Grad Norm 18.0043(23.0426) | Total Time 0.00(0.00)\n",
      "Iter 5880 | Time 18.8832(19.3308) | Bit/dim 3.6456(3.6533) | Xent 0.5787(0.5826) | Loss 30.2715(31.4906) | Error 0.2089(0.2077) Steps 0(0.00) | Grad Norm 18.6783(23.7061) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0107 | Time 98.5897, Epoch Time 1186.2071(1107.1400), Bit/dim 3.6608(best: 3.6526), Xent 0.7033, Loss 4.0124, Error 0.2437(best: 0.2343)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5890 | Time 17.7472(19.4192) | Bit/dim 3.5985(3.6529) | Xent 0.5330(0.5790) | Loss 29.1424(34.6402) | Error 0.1844(0.2065) Steps 0(0.00) | Grad Norm 18.0361(22.9483) | Total Time 0.00(0.00)\n",
      "Iter 5900 | Time 20.1221(19.4805) | Bit/dim 3.6467(3.6530) | Xent 0.4777(0.5707) | Loss 29.3130(33.5225) | Error 0.1700(0.2044) Steps 0(0.00) | Grad Norm 13.7309(22.9820) | Total Time 0.00(0.00)\n",
      "Iter 5910 | Time 17.5723(19.5028) | Bit/dim 3.6456(3.6516) | Xent 0.5242(0.5681) | Loss 29.8044(32.6430) | Error 0.1856(0.2023) Steps 0(0.00) | Grad Norm 21.0744(23.1714) | Total Time 0.00(0.00)\n",
      "Iter 5920 | Time 20.0321(19.5707) | Bit/dim 3.6245(3.6494) | Xent 0.5758(0.5748) | Loss 31.2799(32.0606) | Error 0.2156(0.2051) Steps 0(0.00) | Grad Norm 25.1653(23.2886) | Total Time 0.00(0.00)\n",
      "Iter 5930 | Time 18.8258(19.5570) | Bit/dim 3.6454(3.6514) | Xent 0.6206(0.5873) | Loss 30.0109(31.7117) | Error 0.2256(0.2080) Steps 0(0.00) | Grad Norm 18.5955(23.4976) | Total Time 0.00(0.00)\n",
      "Iter 5940 | Time 17.4178(19.4131) | Bit/dim 3.6403(3.6521) | Xent 0.6121(0.5885) | Loss 30.1111(31.3077) | Error 0.2244(0.2089) Steps 0(0.00) | Grad Norm 20.4621(24.3591) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0108 | Time 97.4889, Epoch Time 1183.1996(1109.4218), Bit/dim 3.6480(best: 3.6526), Xent 0.6856, Loss 3.9908, Error 0.2347(best: 0.2343)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5950 | Time 18.1316(19.3695) | Bit/dim 3.6456(3.6503) | Xent 0.5430(0.5806) | Loss 30.0878(34.1366) | Error 0.1889(0.2072) Steps 0(0.00) | Grad Norm 18.3556(22.9437) | Total Time 0.00(0.00)\n",
      "Iter 5960 | Time 22.3877(19.5074) | Bit/dim 3.6415(3.6493) | Xent 0.5832(0.5696) | Loss 31.3511(33.1566) | Error 0.1844(0.2029) Steps 0(0.00) | Grad Norm 24.5080(21.7371) | Total Time 0.00(0.00)\n",
      "Iter 5970 | Time 18.3316(19.5504) | Bit/dim 3.6608(3.6458) | Xent 0.6021(0.5710) | Loss 30.5299(32.3973) | Error 0.2256(0.2024) Steps 0(0.00) | Grad Norm 24.9864(22.0598) | Total Time 0.00(0.00)\n",
      "Iter 5980 | Time 17.7837(19.6059) | Bit/dim 3.6382(3.6473) | Xent 0.5814(0.5758) | Loss 29.5084(31.8978) | Error 0.1944(0.2036) Steps 0(0.00) | Grad Norm 20.3608(23.3036) | Total Time 0.00(0.00)\n",
      "Iter 5990 | Time 20.1632(19.6471) | Bit/dim 3.6547(3.6477) | Xent 0.6403(0.5724) | Loss 29.7180(31.4759) | Error 0.2333(0.2031) Steps 0(0.00) | Grad Norm 32.4072(22.9140) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0109 | Time 97.6067, Epoch Time 1198.6650(1112.0991), Bit/dim 3.6472(best: 3.6480), Xent 0.6764, Loss 3.9854, Error 0.2339(best: 0.2343)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6000 | Time 20.9645(19.6863) | Bit/dim 3.6491(3.6484) | Xent 0.6053(0.5740) | Loss 31.0968(34.9141) | Error 0.2044(0.2034) Steps 0(0.00) | Grad Norm 24.0909(23.0381) | Total Time 0.00(0.00)\n",
      "Iter 6010 | Time 18.8890(19.6408) | Bit/dim 3.6614(3.6495) | Xent 0.5761(0.5661) | Loss 30.5385(33.7297) | Error 0.2233(0.2006) Steps 0(0.00) | Grad Norm 28.2786(23.0382) | Total Time 0.00(0.00)\n",
      "Iter 6020 | Time 18.8962(19.5761) | Bit/dim 3.6074(3.6479) | Xent 0.5957(0.5642) | Loss 30.2827(32.8066) | Error 0.2167(0.1993) Steps 0(0.00) | Grad Norm 30.4476(22.8386) | Total Time 0.00(0.00)\n",
      "Iter 6030 | Time 19.4186(19.5295) | Bit/dim 3.6687(3.6474) | Xent 0.5684(0.5676) | Loss 29.9984(32.1972) | Error 0.2044(0.2007) Steps 0(0.00) | Grad Norm 36.4145(24.1714) | Total Time 0.00(0.00)\n",
      "Iter 6040 | Time 18.8374(19.5155) | Bit/dim 3.6111(3.6449) | Xent 0.5151(0.5678) | Loss 29.6955(31.6526) | Error 0.1756(0.2020) Steps 0(0.00) | Grad Norm 16.5584(23.4952) | Total Time 0.00(0.00)\n",
      "Iter 6050 | Time 19.4952(19.6338) | Bit/dim 3.6383(3.6453) | Xent 0.5295(0.5651) | Loss 30.3633(31.2302) | Error 0.1789(0.2007) Steps 0(0.00) | Grad Norm 15.3350(22.8490) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0110 | Time 98.5862, Epoch Time 1193.9377(1114.5543), Bit/dim 3.6476(best: 3.6472), Xent 0.6919, Loss 3.9935, Error 0.2372(best: 0.2339)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6060 | Time 18.9436(19.4630) | Bit/dim 3.6384(3.6447) | Xent 0.5885(0.5639) | Loss 30.5885(34.2088) | Error 0.2089(0.2011) Steps 0(0.00) | Grad Norm 25.7399(23.4432) | Total Time 0.00(0.00)\n",
      "Iter 6070 | Time 18.8336(19.4028) | Bit/dim 3.6376(3.6455) | Xent 0.5562(0.5669) | Loss 29.9603(33.0551) | Error 0.2011(0.2038) Steps 0(0.00) | Grad Norm 33.0816(24.6076) | Total Time 0.00(0.00)\n",
      "Iter 6080 | Time 19.3851(19.5328) | Bit/dim 3.6667(3.6479) | Xent 0.5940(0.5739) | Loss 30.3683(32.4599) | Error 0.2156(0.2067) Steps 0(0.00) | Grad Norm 15.3876(25.6707) | Total Time 0.00(0.00)\n",
      "Iter 6090 | Time 19.3784(19.5322) | Bit/dim 3.6451(3.6462) | Xent 0.5380(0.5696) | Loss 30.5490(31.8049) | Error 0.1933(0.2046) Steps 0(0.00) | Grad Norm 13.3579(24.7507) | Total Time 0.00(0.00)\n",
      "Iter 6100 | Time 21.0114(19.6291) | Bit/dim 3.6459(3.6455) | Xent 0.5443(0.5665) | Loss 29.8916(31.3795) | Error 0.1978(0.2030) Steps 0(0.00) | Grad Norm 17.7907(22.8441) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0111 | Time 98.7425, Epoch Time 1188.7088(1116.7789), Bit/dim 3.6482(best: 3.6472), Xent 0.6787, Loss 3.9875, Error 0.2349(best: 0.2339)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6110 | Time 20.1782(19.6317) | Bit/dim 3.6625(3.6460) | Xent 0.6208(0.5637) | Loss 29.7737(34.7419) | Error 0.2278(0.2026) Steps 0(0.00) | Grad Norm 35.1022(22.8727) | Total Time 0.00(0.00)\n",
      "Iter 6120 | Time 19.3965(19.6723) | Bit/dim 3.6588(3.6442) | Xent 0.5290(0.5595) | Loss 30.2156(33.5438) | Error 0.1878(0.2010) Steps 0(0.00) | Grad Norm 25.0413(23.0766) | Total Time 0.00(0.00)\n",
      "Iter 6130 | Time 18.9192(19.5938) | Bit/dim 3.6480(3.6450) | Xent 0.5271(0.5591) | Loss 29.6329(32.5605) | Error 0.1978(0.2007) Steps 0(0.00) | Grad Norm 17.8907(23.5831) | Total Time 0.00(0.00)\n",
      "Iter 6140 | Time 20.4206(19.6437) | Bit/dim 3.6472(3.6451) | Xent 0.5644(0.5542) | Loss 30.3871(31.9591) | Error 0.2067(0.1985) Steps 0(0.00) | Grad Norm 17.0751(23.5422) | Total Time 0.00(0.00)\n",
      "Iter 6150 | Time 19.9322(19.6030) | Bit/dim 3.6264(3.6481) | Xent 0.5588(0.5513) | Loss 29.6256(31.5020) | Error 0.1956(0.1966) Steps 0(0.00) | Grad Norm 14.8612(22.4409) | Total Time 0.00(0.00)\n",
      "Iter 6160 | Time 18.9799(19.5685) | Bit/dim 3.6140(3.6422) | Xent 0.5859(0.5576) | Loss 30.3510(31.1461) | Error 0.1978(0.1992) Steps 0(0.00) | Grad Norm 37.0129(23.8402) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0112 | Time 96.6934, Epoch Time 1193.5219(1119.0812), Bit/dim 3.6501(best: 3.6472), Xent 0.6816, Loss 3.9909, Error 0.2329(best: 0.2339)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6170 | Time 20.4828(19.4771) | Bit/dim 3.6324(3.6445) | Xent 0.5521(0.5510) | Loss 29.8222(33.9263) | Error 0.1867(0.1961) Steps 0(0.00) | Grad Norm 24.1814(23.3770) | Total Time 0.00(0.00)\n",
      "Iter 6180 | Time 20.6793(19.5371) | Bit/dim 3.6667(3.6473) | Xent 0.5598(0.5507) | Loss 29.8130(32.9912) | Error 0.1944(0.1960) Steps 0(0.00) | Grad Norm 27.8830(23.8107) | Total Time 0.00(0.00)\n",
      "Iter 6190 | Time 18.9575(19.5306) | Bit/dim 3.6424(3.6458) | Xent 0.5558(0.5566) | Loss 29.6307(32.3024) | Error 0.2000(0.1993) Steps 0(0.00) | Grad Norm 20.3203(25.3348) | Total Time 0.00(0.00)\n",
      "Iter 6200 | Time 21.1469(19.7863) | Bit/dim 3.6650(3.6466) | Xent 0.5364(0.5598) | Loss 30.5735(31.8395) | Error 0.1811(0.1995) Steps 0(0.00) | Grad Norm 18.4603(24.4473) | Total Time 0.00(0.00)\n",
      "Iter 6210 | Time 19.9739(20.1130) | Bit/dim 3.6391(3.6461) | Xent 0.5644(0.5629) | Loss 30.5204(31.5359) | Error 0.2033(0.2010) Steps 0(0.00) | Grad Norm 26.1544(24.5468) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0113 | Time 96.4503, Epoch Time 1212.2426(1121.8760), Bit/dim 3.6457(best: 3.6472), Xent 0.6791, Loss 3.9852, Error 0.2362(best: 0.2329)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6220 | Time 19.0771(20.0082) | Bit/dim 3.6680(3.6448) | Xent 0.4878(0.5565) | Loss 31.3182(34.8843) | Error 0.1878(0.1990) Steps 0(0.00) | Grad Norm 17.1108(23.0597) | Total Time 0.00(0.00)\n",
      "Iter 6230 | Time 19.5977(19.8685) | Bit/dim 3.6412(3.6472) | Xent 0.5526(0.5499) | Loss 30.6237(33.6581) | Error 0.1933(0.1969) Steps 0(0.00) | Grad Norm 13.2399(22.1166) | Total Time 0.00(0.00)\n",
      "Iter 6240 | Time 20.2996(19.6898) | Bit/dim 3.6476(3.6443) | Xent 0.5097(0.5511) | Loss 30.2114(32.5968) | Error 0.1978(0.1974) Steps 0(0.00) | Grad Norm 21.8830(22.6483) | Total Time 0.00(0.00)\n",
      "Iter 6250 | Time 20.0868(19.8719) | Bit/dim 3.6316(3.6450) | Xent 0.5683(0.5462) | Loss 31.0082(32.0092) | Error 0.2022(0.1944) Steps 0(0.00) | Grad Norm 29.9477(22.8069) | Total Time 0.00(0.00)\n",
      "Iter 6260 | Time 19.9418(19.8425) | Bit/dim 3.6723(3.6453) | Xent 0.5567(0.5537) | Loss 30.6660(31.5575) | Error 0.2044(0.1972) Steps 0(0.00) | Grad Norm 33.9902(25.1431) | Total Time 0.00(0.00)\n",
      "Iter 6270 | Time 20.5036(19.7692) | Bit/dim 3.6785(3.6464) | Xent 0.5407(0.5515) | Loss 30.2810(31.1036) | Error 0.1900(0.1952) Steps 0(0.00) | Grad Norm 10.3112(23.5261) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0114 | Time 96.8663, Epoch Time 1197.6561(1124.1494), Bit/dim 3.6386(best: 3.6457), Xent 0.7205, Loss 3.9989, Error 0.2461(best: 0.2329)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6280 | Time 18.9675(19.7042) | Bit/dim 3.6425(3.6457) | Xent 0.5525(0.5500) | Loss 30.6784(34.0605) | Error 0.1956(0.1934) Steps 0(0.00) | Grad Norm 32.4150(24.9878) | Total Time 0.00(0.00)\n",
      "Iter 6290 | Time 22.5023(19.9504) | Bit/dim 3.6479(3.6447) | Xent 0.6091(0.5548) | Loss 29.8117(33.0805) | Error 0.2122(0.1957) Steps 0(0.00) | Grad Norm 26.6126(26.2670) | Total Time 0.00(0.00)\n",
      "Iter 6300 | Time 19.7165(19.8404) | Bit/dim 3.6385(3.6469) | Xent 0.5048(0.5547) | Loss 30.5826(32.2685) | Error 0.1756(0.1963) Steps 0(0.00) | Grad Norm 21.7955(26.7198) | Total Time 0.00(0.00)\n",
      "Iter 6310 | Time 19.2314(19.7247) | Bit/dim 3.6258(3.6451) | Xent 0.5443(0.5525) | Loss 30.9588(31.7252) | Error 0.2033(0.1953) Steps 0(0.00) | Grad Norm 15.9326(24.8813) | Total Time 0.00(0.00)\n",
      "Iter 6320 | Time 19.9630(19.6461) | Bit/dim 3.6133(3.6422) | Xent 0.5835(0.5532) | Loss 29.4779(31.3062) | Error 0.2289(0.1965) Steps 0(0.00) | Grad Norm 27.3842(24.4072) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0115 | Time 98.6627, Epoch Time 1200.9122(1126.4523), Bit/dim 3.6393(best: 3.6386), Xent 0.6892, Loss 3.9839, Error 0.2347(best: 0.2329)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6330 | Time 20.0859(19.7194) | Bit/dim 3.6618(3.6446) | Xent 0.5712(0.5463) | Loss 30.5411(34.7450) | Error 0.2000(0.1938) Steps 0(0.00) | Grad Norm 23.9683(23.4374) | Total Time 0.00(0.00)\n",
      "Iter 6340 | Time 19.3811(19.7402) | Bit/dim 3.6574(3.6470) | Xent 0.5073(0.5390) | Loss 30.3786(33.4341) | Error 0.1778(0.1897) Steps 0(0.00) | Grad Norm 12.9743(23.2487) | Total Time 0.00(0.00)\n",
      "Iter 6350 | Time 19.4234(19.7325) | Bit/dim 3.6290(3.6435) | Xent 0.5186(0.5328) | Loss 30.0539(32.5666) | Error 0.1844(0.1879) Steps 0(0.00) | Grad Norm 25.6697(22.8828) | Total Time 0.00(0.00)\n",
      "Iter 6360 | Time 18.6089(19.7040) | Bit/dim 3.6409(3.6419) | Xent 0.5782(0.5425) | Loss 29.9904(31.9777) | Error 0.2089(0.1924) Steps 0(0.00) | Grad Norm 27.5938(24.2385) | Total Time 0.00(0.00)\n",
      "Iter 6370 | Time 18.9877(19.7968) | Bit/dim 3.6064(3.6389) | Xent 0.5003(0.5385) | Loss 29.5581(31.4756) | Error 0.1711(0.1903) Steps 0(0.00) | Grad Norm 20.3808(22.7385) | Total Time 0.00(0.00)\n",
      "Iter 6380 | Time 19.9405(19.8300) | Bit/dim 3.6108(3.6398) | Xent 0.5542(0.5449) | Loss 29.9904(31.1028) | Error 0.1811(0.1928) Steps 0(0.00) | Grad Norm 25.8721(22.9402) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0116 | Time 98.5541, Epoch Time 1207.5667(1128.8857), Bit/dim 3.6395(best: 3.6386), Xent 0.6915, Loss 3.9852, Error 0.2331(best: 0.2329)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6390 | Time 19.2673(19.7854) | Bit/dim 3.6204(3.6405) | Xent 0.5202(0.5475) | Loss 29.6413(34.0786) | Error 0.1922(0.1945) Steps 0(0.00) | Grad Norm 27.7688(23.6384) | Total Time 0.00(0.00)\n",
      "Iter 6400 | Time 18.1158(19.6850) | Bit/dim 3.6253(3.6401) | Xent 0.4976(0.5463) | Loss 30.2732(33.0367) | Error 0.1767(0.1945) Steps 0(0.00) | Grad Norm 23.8113(23.7102) | Total Time 0.00(0.00)\n",
      "Iter 6410 | Time 18.4568(19.8404) | Bit/dim 3.6480(3.6433) | Xent 0.4806(0.5421) | Loss 28.5006(32.3162) | Error 0.1600(0.1927) Steps 0(0.00) | Grad Norm 14.6413(23.1640) | Total Time 0.00(0.00)\n",
      "Iter 6420 | Time 19.0442(19.8574) | Bit/dim 3.6325(3.6389) | Xent 0.5276(0.5409) | Loss 29.1041(31.6451) | Error 0.1922(0.1926) Steps 0(0.00) | Grad Norm 17.5147(23.0632) | Total Time 0.00(0.00)\n",
      "Iter 6430 | Time 19.1769(19.6775) | Bit/dim 3.6252(3.6383) | Xent 0.5368(0.5395) | Loss 28.5387(31.1829) | Error 0.1811(0.1918) Steps 0(0.00) | Grad Norm 17.6448(22.1843) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0117 | Time 99.5076, Epoch Time 1202.8701(1131.1053), Bit/dim 3.6378(best: 3.6386), Xent 0.6655, Loss 3.9705, Error 0.2248(best: 0.2329)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6440 | Time 19.5389(19.7264) | Bit/dim 3.6448(3.6390) | Xent 0.5249(0.5322) | Loss 30.1791(34.5459) | Error 0.1900(0.1894) Steps 0(0.00) | Grad Norm 12.4087(20.2175) | Total Time 0.00(0.00)\n",
      "Iter 6450 | Time 19.2638(19.7490) | Bit/dim 3.6634(3.6428) | Xent 0.5057(0.5320) | Loss 30.1607(33.4764) | Error 0.1800(0.1887) Steps 0(0.00) | Grad Norm 31.4373(21.5993) | Total Time 0.00(0.00)\n",
      "Iter 6460 | Time 18.8005(19.7877) | Bit/dim 3.6740(3.6448) | Xent 0.5412(0.5294) | Loss 30.7654(32.6305) | Error 0.2033(0.1884) Steps 0(0.00) | Grad Norm 39.0909(23.3554) | Total Time 0.00(0.00)\n",
      "Iter 6470 | Time 17.9905(19.7089) | Bit/dim 3.6053(3.6427) | Xent 0.4753(0.5297) | Loss 28.6513(31.8959) | Error 0.1778(0.1887) Steps 0(0.00) | Grad Norm 15.7196(23.9782) | Total Time 0.00(0.00)\n",
      "Iter 6480 | Time 20.3132(19.7676) | Bit/dim 3.6090(3.6390) | Xent 0.5034(0.5307) | Loss 29.7123(31.4166) | Error 0.1778(0.1884) Steps 0(0.00) | Grad Norm 19.3514(22.2991) | Total Time 0.00(0.00)\n",
      "Iter 6490 | Time 21.9536(19.8519) | Bit/dim 3.6500(3.6408) | Xent 0.6056(0.5353) | Loss 31.2799(31.0880) | Error 0.2111(0.1904) Steps 0(0.00) | Grad Norm 31.1983(22.9738) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0118 | Time 99.3716, Epoch Time 1208.2913(1133.4209), Bit/dim 3.6395(best: 3.6378), Xent 0.6849, Loss 3.9819, Error 0.2343(best: 0.2248)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6500 | Time 19.7865(19.7908) | Bit/dim 3.6163(3.6411) | Xent 0.5855(0.5350) | Loss 31.1017(34.0436) | Error 0.1967(0.1901) Steps 0(0.00) | Grad Norm 32.7714(23.4506) | Total Time 0.00(0.00)\n",
      "Iter 6510 | Time 20.6868(19.7565) | Bit/dim 3.6638(3.6415) | Xent 0.4966(0.5375) | Loss 30.8697(33.0145) | Error 0.1744(0.1894) Steps 0(0.00) | Grad Norm 24.0893(25.0968) | Total Time 0.00(0.00)\n",
      "Iter 6520 | Time 17.6294(19.8011) | Bit/dim 3.6270(3.6417) | Xent 0.5317(0.5344) | Loss 28.7483(32.2989) | Error 0.1989(0.1894) Steps 0(0.00) | Grad Norm 21.0195(25.0092) | Total Time 0.00(0.00)\n",
      "Iter 6530 | Time 21.1450(19.9724) | Bit/dim 3.6605(3.6447) | Xent 0.4994(0.5331) | Loss 31.0744(31.7810) | Error 0.1811(0.1897) Steps 0(0.00) | Grad Norm 20.0548(24.4458) | Total Time 0.00(0.00)\n",
      "Iter 6540 | Time 19.2037(20.0993) | Bit/dim 3.6343(3.6444) | Xent 0.5640(0.5360) | Loss 30.2038(31.4152) | Error 0.1833(0.1902) Steps 0(0.00) | Grad Norm 30.6554(25.2932) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0119 | Time 98.8724, Epoch Time 1216.8038(1135.9223), Bit/dim 3.6383(best: 3.6378), Xent 0.6718, Loss 3.9741, Error 0.2319(best: 0.2248)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6550 | Time 19.0463(19.9755) | Bit/dim 3.6689(3.6402) | Xent 0.5462(0.5330) | Loss 30.7877(34.8367) | Error 0.1933(0.1896) Steps 0(0.00) | Grad Norm 22.5033(25.0138) | Total Time 0.00(0.00)\n",
      "Iter 6560 | Time 18.7024(19.9419) | Bit/dim 3.6445(3.6410) | Xent 0.5131(0.5306) | Loss 29.3723(33.5581) | Error 0.1844(0.1894) Steps 0(0.00) | Grad Norm 23.4736(25.0286) | Total Time 0.00(0.00)\n",
      "Iter 6570 | Time 19.7694(19.9869) | Bit/dim 3.6199(3.6419) | Xent 0.5875(0.5292) | Loss 29.8990(32.5827) | Error 0.1989(0.1899) Steps 0(0.00) | Grad Norm 28.0017(24.9863) | Total Time 0.00(0.00)\n",
      "Iter 6580 | Time 19.0235(19.7935) | Bit/dim 3.6683(3.6396) | Xent 0.4764(0.5246) | Loss 30.4521(31.9443) | Error 0.1700(0.1883) Steps 0(0.00) | Grad Norm 17.3593(23.5876) | Total Time 0.00(0.00)\n",
      "Iter 6590 | Time 19.5168(19.5516) | Bit/dim 3.6767(3.6401) | Xent 0.6153(0.5309) | Loss 31.2235(31.5053) | Error 0.2144(0.1900) Steps 0(0.00) | Grad Norm 46.5702(24.4391) | Total Time 0.00(0.00)\n",
      "Iter 6600 | Time 19.7508(19.6081) | Bit/dim 3.6317(3.6414) | Xent 0.5180(0.5357) | Loss 29.4319(31.0862) | Error 0.1733(0.1916) Steps 0(0.00) | Grad Norm 18.3921(24.5486) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0120 | Time 98.2479, Epoch Time 1194.0817(1137.6671), Bit/dim 3.6485(best: 3.6378), Xent 0.6644, Loss 3.9807, Error 0.2253(best: 0.2248)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6610 | Time 20.3960(19.5243) | Bit/dim 3.6518(3.6406) | Xent 0.4634(0.5287) | Loss 31.1113(34.0154) | Error 0.1633(0.1891) Steps 0(0.00) | Grad Norm 21.5510(23.6669) | Total Time 0.00(0.00)\n",
      "Iter 6620 | Time 19.9784(19.6159) | Bit/dim 3.6322(3.6427) | Xent 0.4297(0.5215) | Loss 29.7065(33.0183) | Error 0.1489(0.1870) Steps 0(0.00) | Grad Norm 10.5707(22.7843) | Total Time 0.00(0.00)\n",
      "Iter 6630 | Time 21.3062(19.6995) | Bit/dim 3.6262(3.6419) | Xent 0.4720(0.5237) | Loss 29.9585(32.2441) | Error 0.1756(0.1871) Steps 0(0.00) | Grad Norm 14.6348(22.9322) | Total Time 0.00(0.00)\n",
      "Iter 6640 | Time 21.0760(19.7406) | Bit/dim 3.6499(3.6396) | Xent 0.4569(0.5156) | Loss 30.9173(31.6665) | Error 0.1544(0.1839) Steps 0(0.00) | Grad Norm 11.9695(21.9549) | Total Time 0.00(0.00)\n",
      "Iter 6650 | Time 20.1479(19.8448) | Bit/dim 3.6266(3.6377) | Xent 0.5206(0.5140) | Loss 30.0397(31.3311) | Error 0.1778(0.1821) Steps 0(0.00) | Grad Norm 15.3915(21.5994) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0121 | Time 101.4311, Epoch Time 1214.0524(1139.9587), Bit/dim 3.6364(best: 3.6378), Xent 0.7003, Loss 3.9865, Error 0.2374(best: 0.2248)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6660 | Time 18.9491(19.9173) | Bit/dim 3.6379(3.6356) | Xent 0.5130(0.5065) | Loss 29.8120(34.4496) | Error 0.1778(0.1789) Steps 0(0.00) | Grad Norm 18.3603(22.4386) | Total Time 0.00(0.00)\n",
      "Iter 6670 | Time 23.1656(20.1188) | Bit/dim 3.6091(3.6330) | Xent 0.5338(0.5033) | Loss 31.0685(33.2459) | Error 0.1822(0.1774) Steps 0(0.00) | Grad Norm 23.8073(22.6818) | Total Time 0.00(0.00)\n",
      "Iter 6680 | Time 19.4495(20.0646) | Bit/dim 3.6308(3.6329) | Xent 0.4715(0.5013) | Loss 30.0596(32.4326) | Error 0.1667(0.1764) Steps 0(0.00) | Grad Norm 21.7293(22.2446) | Total Time 0.00(0.00)\n",
      "Iter 6690 | Time 19.3967(19.9535) | Bit/dim 3.6545(3.6333) | Xent 0.5983(0.5106) | Loss 30.0774(31.7831) | Error 0.2133(0.1801) Steps 0(0.00) | Grad Norm 29.9934(21.8881) | Total Time 0.00(0.00)\n",
      "Iter 6700 | Time 19.9596(19.8675) | Bit/dim 3.6591(3.6338) | Xent 0.5230(0.5104) | Loss 31.2756(31.2575) | Error 0.1833(0.1808) Steps 0(0.00) | Grad Norm 32.1907(22.0164) | Total Time 0.00(0.00)\n",
      "Iter 6710 | Time 20.5912(19.8082) | Bit/dim 3.6468(3.6361) | Xent 0.5630(0.5138) | Loss 30.9768(31.0626) | Error 0.2056(0.1818) Steps 0(0.00) | Grad Norm 27.0092(21.7500) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0122 | Time 99.1407, Epoch Time 1209.2632(1142.0378), Bit/dim 3.6352(best: 3.6364), Xent 0.6833, Loss 3.9768, Error 0.2288(best: 0.2248)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6720 | Time 21.4679(19.8875) | Bit/dim 3.6483(3.6350) | Xent 0.5205(0.5121) | Loss 30.3094(33.8955) | Error 0.1944(0.1824) Steps 0(0.00) | Grad Norm 24.5799(22.6420) | Total Time 0.00(0.00)\n",
      "Iter 6730 | Time 19.0580(20.0008) | Bit/dim 3.6163(3.6359) | Xent 0.5285(0.5128) | Loss 29.6349(32.8600) | Error 0.1933(0.1828) Steps 0(0.00) | Grad Norm 22.6362(22.3679) | Total Time 0.00(0.00)\n",
      "Iter 6740 | Time 20.8498(20.1504) | Bit/dim 3.6872(3.6394) | Xent 0.5343(0.5123) | Loss 31.3995(32.1150) | Error 0.1911(0.1817) Steps 0(0.00) | Grad Norm 23.8234(22.3278) | Total Time 0.00(0.00)\n",
      "Iter 6750 | Time 19.2530(20.0860) | Bit/dim 3.6447(3.6356) | Xent 0.5658(0.5154) | Loss 31.0983(31.6317) | Error 0.2011(0.1827) Steps 0(0.00) | Grad Norm 22.6110(21.8509) | Total Time 0.00(0.00)\n",
      "Iter 6760 | Time 18.7383(20.2298) | Bit/dim 3.6321(3.6348) | Xent 0.4769(0.5121) | Loss 30.1209(31.2564) | Error 0.1744(0.1827) Steps 0(0.00) | Grad Norm 16.0500(21.4688) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0123 | Time 97.6237, Epoch Time 1233.1024(1144.7698), Bit/dim 3.6386(best: 3.6352), Xent 0.7459, Loss 4.0115, Error 0.2499(best: 0.2248)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6770 | Time 19.6810(20.2182) | Bit/dim 3.6192(3.6358) | Xent 0.5421(0.5154) | Loss 29.6684(34.6604) | Error 0.2067(0.1835) Steps 0(0.00) | Grad Norm 25.0450(22.6391) | Total Time 0.00(0.00)\n",
      "Iter 6780 | Time 19.8276(20.2450) | Bit/dim 3.6672(3.6371) | Xent 0.4577(0.5139) | Loss 29.4053(33.4604) | Error 0.1711(0.1835) Steps 0(0.00) | Grad Norm 24.8897(22.6566) | Total Time 0.00(0.00)\n",
      "Iter 6790 | Time 24.5365(20.4375) | Bit/dim 3.6480(3.6368) | Xent 0.4880(0.5102) | Loss 30.5706(32.6696) | Error 0.1767(0.1827) Steps 0(0.00) | Grad Norm 17.6150(21.8717) | Total Time 0.00(0.00)\n",
      "Iter 6800 | Time 19.7278(20.3155) | Bit/dim 3.6134(3.6356) | Xent 0.5292(0.5121) | Loss 30.8283(32.0128) | Error 0.1833(0.1847) Steps 0(0.00) | Grad Norm 27.1940(22.5676) | Total Time 0.00(0.00)\n",
      "Iter 6810 | Time 19.8746(20.2073) | Bit/dim 3.6318(3.6356) | Xent 0.5684(0.5169) | Loss 30.2741(31.5539) | Error 0.2144(0.1856) Steps 0(0.00) | Grad Norm 30.0009(22.9026) | Total Time 0.00(0.00)\n",
      "Iter 6820 | Time 18.7504(20.2913) | Bit/dim 3.6230(3.6328) | Xent 0.4994(0.5138) | Loss 30.3084(31.2362) | Error 0.1944(0.1847) Steps 0(0.00) | Grad Norm 27.9228(22.9562) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0124 | Time 99.9335, Epoch Time 1234.3366(1147.4568), Bit/dim 3.6346(best: 3.6352), Xent 0.6927, Loss 3.9810, Error 0.2316(best: 0.2248)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6830 | Time 19.5531(20.2476) | Bit/dim 3.6444(3.6327) | Xent 0.4864(0.5065) | Loss 30.1665(34.1031) | Error 0.1822(0.1831) Steps 0(0.00) | Grad Norm 26.4198(23.4109) | Total Time 0.00(0.00)\n",
      "Iter 6840 | Time 20.8908(20.1782) | Bit/dim 3.6502(3.6357) | Xent 0.4768(0.5053) | Loss 30.6704(33.0939) | Error 0.1744(0.1830) Steps 0(0.00) | Grad Norm 21.8946(23.8507) | Total Time 0.00(0.00)\n",
      "Iter 6850 | Time 19.7854(20.0868) | Bit/dim 3.5952(3.6342) | Xent 0.4577(0.5058) | Loss 29.1606(32.2538) | Error 0.1644(0.1823) Steps 0(0.00) | Grad Norm 15.1722(23.6479) | Total Time 0.00(0.00)\n",
      "Iter 6860 | Time 17.6830(20.0480) | Bit/dim 3.6156(3.6331) | Xent 0.4840(0.5026) | Loss 29.3747(31.6696) | Error 0.1756(0.1809) Steps 0(0.00) | Grad Norm 13.8088(23.2139) | Total Time 0.00(0.00)\n",
      "Iter 6870 | Time 19.3063(20.0380) | Bit/dim 3.6213(3.6296) | Xent 0.5050(0.4990) | Loss 29.7661(31.1976) | Error 0.1744(0.1782) Steps 0(0.00) | Grad Norm 21.9906(22.1665) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0125 | Time 99.5203, Epoch Time 1215.0984(1149.4860), Bit/dim 3.6328(best: 3.6346), Xent 0.6685, Loss 3.9671, Error 0.2291(best: 0.2248)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6880 | Time 18.6121(19.9083) | Bit/dim 3.6159(3.6302) | Xent 0.5027(0.4928) | Loss 29.4777(34.7330) | Error 0.1689(0.1751) Steps 0(0.00) | Grad Norm 24.3438(21.8515) | Total Time 0.00(0.00)\n",
      "Iter 6890 | Time 19.1154(19.9861) | Bit/dim 3.6621(3.6315) | Xent 0.5653(0.4929) | Loss 29.8758(33.4972) | Error 0.1856(0.1751) Steps 0(0.00) | Grad Norm 40.0417(22.0716) | Total Time 0.00(0.00)\n",
      "Iter 6900 | Time 19.4217(20.0848) | Bit/dim 3.6149(3.6324) | Xent 0.5431(0.5040) | Loss 30.1578(32.7032) | Error 0.1911(0.1792) Steps 0(0.00) | Grad Norm 20.1289(24.0417) | Total Time 0.00(0.00)\n",
      "Iter 6910 | Time 19.5728(20.1187) | Bit/dim 3.6505(3.6304) | Xent 0.5194(0.5107) | Loss 30.8509(32.0949) | Error 0.1856(0.1828) Steps 0(0.00) | Grad Norm 28.4735(24.9429) | Total Time 0.00(0.00)\n",
      "Iter 6920 | Time 19.3130(20.0956) | Bit/dim 3.6494(3.6344) | Xent 0.5512(0.5131) | Loss 30.8386(31.6099) | Error 0.2122(0.1846) Steps 0(0.00) | Grad Norm 29.5793(24.4758) | Total Time 0.00(0.00)\n",
      "Iter 6930 | Time 19.6096(20.0507) | Bit/dim 3.6409(3.6346) | Xent 0.5106(0.5165) | Loss 30.0693(31.2169) | Error 0.1878(0.1856) Steps 0(0.00) | Grad Norm 33.7513(24.9530) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0126 | Time 102.5390, Epoch Time 1226.8117(1151.8058), Bit/dim 3.6326(best: 3.6328), Xent 0.6837, Loss 3.9744, Error 0.2312(best: 0.2248)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6940 | Time 22.3389(20.2248) | Bit/dim 3.6103(3.6318) | Xent 0.5414(0.5126) | Loss 30.8338(34.2052) | Error 0.1900(0.1837) Steps 0(0.00) | Grad Norm 14.5418(23.8853) | Total Time 0.00(0.00)\n",
      "Iter 6950 | Time 21.4842(20.1713) | Bit/dim 3.6217(3.6319) | Xent 0.5134(0.5162) | Loss 30.8263(33.0947) | Error 0.1844(0.1848) Steps 0(0.00) | Grad Norm 24.0623(23.8173) | Total Time 0.00(0.00)\n",
      "Iter 6960 | Time 19.7753(20.1746) | Bit/dim 3.6598(3.6341) | Xent 0.4335(0.5027) | Loss 30.6061(32.3408) | Error 0.1556(0.1800) Steps 0(0.00) | Grad Norm 13.2883(21.7651) | Total Time 0.00(0.00)\n",
      "Iter 6970 | Time 20.7495(20.1895) | Bit/dim 3.6038(3.6343) | Xent 0.5161(0.5012) | Loss 30.2870(31.7839) | Error 0.1778(0.1793) Steps 0(0.00) | Grad Norm 23.4070(21.5184) | Total Time 0.00(0.00)\n",
      "Iter 6980 | Time 20.5122(20.2319) | Bit/dim 3.6077(3.6325) | Xent 0.5229(0.5028) | Loss 30.4043(31.3125) | Error 0.1900(0.1793) Steps 0(0.00) | Grad Norm 25.4680(23.0109) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0127 | Time 103.7505, Epoch Time 1233.9215(1154.2693), Bit/dim 3.6335(best: 3.6326), Xent 0.7033, Loss 3.9852, Error 0.2399(best: 0.2248)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6990 | Time 19.2202(20.1793) | Bit/dim 3.6586(3.6306) | Xent 0.5136(0.5099) | Loss 29.8950(34.8124) | Error 0.1756(0.1817) Steps 0(0.00) | Grad Norm 22.9834(23.5315) | Total Time 0.00(0.00)\n",
      "Iter 7000 | Time 19.8683(20.1119) | Bit/dim 3.6365(3.6326) | Xent 0.5531(0.5052) | Loss 30.0573(33.5404) | Error 0.1844(0.1802) Steps 0(0.00) | Grad Norm 26.2579(23.0336) | Total Time 0.00(0.00)\n",
      "Iter 7010 | Time 20.2868(20.1301) | Bit/dim 3.6288(3.6330) | Xent 0.4925(0.4983) | Loss 30.1569(32.5854) | Error 0.1789(0.1776) Steps 0(0.00) | Grad Norm 13.8035(22.9399) | Total Time 0.00(0.00)\n",
      "Iter 7020 | Time 19.6025(20.2392) | Bit/dim 3.6367(3.6326) | Xent 0.4941(0.4926) | Loss 30.1947(31.9313) | Error 0.1656(0.1758) Steps 0(0.00) | Grad Norm 24.0284(22.0354) | Total Time 0.00(0.00)\n",
      "Iter 7030 | Time 19.4583(20.1983) | Bit/dim 3.6253(3.6322) | Xent 0.4782(0.4931) | Loss 30.1980(31.4549) | Error 0.1767(0.1761) Steps 0(0.00) | Grad Norm 34.8715(22.6838) | Total Time 0.00(0.00)\n",
      "Iter 7040 | Time 21.5358(20.2141) | Bit/dim 3.6143(3.6283) | Xent 0.5237(0.4915) | Loss 30.5649(31.0876) | Error 0.1911(0.1752) Steps 0(0.00) | Grad Norm 18.2875(22.7340) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0128 | Time 99.5465, Epoch Time 1228.5389(1156.4973), Bit/dim 3.6359(best: 3.6326), Xent 0.6678, Loss 3.9699, Error 0.2258(best: 0.2248)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7050 | Time 21.6831(20.2321) | Bit/dim 3.6558(3.6256) | Xent 0.4683(0.4900) | Loss 29.4003(34.0294) | Error 0.1644(0.1742) Steps 0(0.00) | Grad Norm 26.8513(22.9896) | Total Time 0.00(0.00)\n",
      "Iter 7070 | Time 19.1399(20.4326) | Bit/dim 3.5925(3.6276) | Xent 0.5923(0.4914) | Loss 30.3071(32.2302) | Error 0.2044(0.1737) Steps 0(0.00) | Grad Norm 34.7152(23.2209) | Total Time 0.00(0.00)\n",
      "Iter 7080 | Time 18.1973(20.3265) | Bit/dim 3.6674(3.6282) | Xent 0.5076(0.4954) | Loss 29.8007(31.6782) | Error 0.1844(0.1751) Steps 0(0.00) | Grad Norm 20.0318(23.2786) | Total Time 0.00(0.00)\n",
      "Iter 7090 | Time 20.9533(20.2884) | Bit/dim 3.6267(3.6290) | Xent 0.4789(0.4934) | Loss 30.6238(31.1630) | Error 0.1711(0.1750) Steps 0(0.00) | Grad Norm 16.0420(22.8790) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0129 | Time 98.1597, Epoch Time 1236.9157(1158.9099), Bit/dim 3.6365(best: 3.6326), Xent 0.6702, Loss 3.9716, Error 0.2295(best: 0.2248)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7100 | Time 18.7634(20.1953) | Bit/dim 3.5951(3.6281) | Xent 0.5645(0.4998) | Loss 29.7691(34.5431) | Error 0.2089(0.1777) Steps 0(0.00) | Grad Norm 34.0792(22.1128) | Total Time 0.00(0.00)\n",
      "Iter 7110 | Time 20.8576(20.2165) | Bit/dim 3.6719(3.6313) | Xent 0.5370(0.4956) | Loss 31.2850(33.3544) | Error 0.1922(0.1759) Steps 0(0.00) | Grad Norm 36.8091(22.3933) | Total Time 0.00(0.00)\n",
      "Iter 7120 | Time 20.3477(20.1936) | Bit/dim 3.6183(3.6295) | Xent 0.5089(0.5006) | Loss 30.0674(32.5131) | Error 0.1778(0.1764) Steps 0(0.00) | Grad Norm 35.5851(23.1899) | Total Time 0.00(0.00)\n",
      "Iter 7130 | Time 20.0491(20.1877) | Bit/dim 3.6517(3.6331) | Xent 0.4674(0.4981) | Loss 29.6089(31.8982) | Error 0.1589(0.1760) Steps 0(0.00) | Grad Norm 24.9491(23.2550) | Total Time 0.00(0.00)\n",
      "Iter 7150 | Time 20.3840(20.2975) | Bit/dim 3.6521(3.6296) | Xent 0.5249(0.5009) | Loss 30.3937(31.1375) | Error 0.1733(0.1760) Steps 0(0.00) | Grad Norm 25.2324(23.1912) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0130 | Time 99.4310, Epoch Time 1228.0377(1160.9837), Bit/dim 3.6310(best: 3.6326), Xent 0.6807, Loss 3.9713, Error 0.2246(best: 0.2248)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7160 | Time 21.2971(20.2071) | Bit/dim 3.6274(3.6327) | Xent 0.4822(0.4926) | Loss 30.1796(34.0894) | Error 0.1644(0.1729) Steps 0(0.00) | Grad Norm 34.3856(23.5570) | Total Time 0.00(0.00)\n",
      "Iter 7170 | Time 20.3071(20.2223) | Bit/dim 3.6195(3.6325) | Xent 0.4144(0.4859) | Loss 29.7972(33.0233) | Error 0.1444(0.1707) Steps 0(0.00) | Grad Norm 20.0037(22.5577) | Total Time 0.00(0.00)\n",
      "Iter 7180 | Time 19.0253(20.1498) | Bit/dim 3.6242(3.6306) | Xent 0.4049(0.4802) | Loss 29.5120(32.1699) | Error 0.1400(0.1693) Steps 0(0.00) | Grad Norm 20.8045(22.9708) | Total Time 0.00(0.00)\n",
      "Iter 7190 | Time 19.8101(20.1629) | Bit/dim 3.6191(3.6279) | Xent 0.4650(0.4769) | Loss 29.9923(31.5885) | Error 0.1578(0.1693) Steps 0(0.00) | Grad Norm 29.8034(22.9398) | Total Time 0.00(0.00)\n",
      "Iter 7200 | Time 19.1445(20.1012) | Bit/dim 3.6341(3.6257) | Xent 0.5071(0.4870) | Loss 30.5564(31.2387) | Error 0.1789(0.1722) Steps 0(0.00) | Grad Norm 27.4304(24.6101) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0131 | Time 102.1770, Epoch Time 1222.7543(1162.8368), Bit/dim 3.6441(best: 3.6310), Xent 0.6688, Loss 3.9785, Error 0.2254(best: 0.2246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7210 | Time 21.1727(20.2039) | Bit/dim 3.6331(3.6275) | Xent 0.4864(0.4863) | Loss 30.4128(34.6865) | Error 0.1767(0.1718) Steps 0(0.00) | Grad Norm 26.9423(24.6050) | Total Time 0.00(0.00)\n",
      "Iter 7220 | Time 20.9551(20.1154) | Bit/dim 3.6300(3.6283) | Xent 0.5582(0.4896) | Loss 29.7766(33.3868) | Error 0.1978(0.1739) Steps 0(0.00) | Grad Norm 25.0082(24.4365) | Total Time 0.00(0.00)\n",
      "Iter 7230 | Time 19.3497(20.2689) | Bit/dim 3.6505(3.6305) | Xent 0.4120(0.4808) | Loss 29.7351(32.5466) | Error 0.1422(0.1712) Steps 0(0.00) | Grad Norm 17.0161(22.7367) | Total Time 0.00(0.00)\n",
      "Iter 7240 | Time 20.3184(20.1602) | Bit/dim 3.6177(3.6265) | Xent 0.5204(0.4839) | Loss 30.8952(31.9142) | Error 0.1767(0.1718) Steps 0(0.00) | Grad Norm 24.1056(23.5417) | Total Time 0.00(0.00)\n",
      "Iter 7250 | Time 20.0025(20.1934) | Bit/dim 3.6108(3.6269) | Xent 0.4842(0.4842) | Loss 30.6447(31.5314) | Error 0.1622(0.1711) Steps 0(0.00) | Grad Norm 22.6739(23.4235) | Total Time 0.00(0.00)\n",
      "Iter 7260 | Time 18.8701(20.1008) | Bit/dim 3.6389(3.6295) | Xent 0.4303(0.4817) | Loss 28.1772(31.1369) | Error 0.1522(0.1706) Steps 0(0.00) | Grad Norm 14.6978(22.5101) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0132 | Time 99.2295, Epoch Time 1227.7629(1164.7846), Bit/dim 3.6295(best: 3.6310), Xent 0.6622, Loss 3.9606, Error 0.2264(best: 0.2246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7270 | Time 21.0766(20.1947) | Bit/dim 3.6361(3.6293) | Xent 0.4336(0.4764) | Loss 29.8810(34.0732) | Error 0.1400(0.1690) Steps 0(0.00) | Grad Norm 27.0327(21.2611) | Total Time 0.00(0.00)\n",
      "Iter 7280 | Time 19.8997(20.0694) | Bit/dim 3.6199(3.6293) | Xent 0.4737(0.4847) | Loss 30.3365(32.9879) | Error 0.1644(0.1715) Steps 0(0.00) | Grad Norm 27.2656(23.7383) | Total Time 0.00(0.00)\n",
      "Iter 7290 | Time 20.3225(20.0617) | Bit/dim 3.6385(3.6291) | Xent 0.5389(0.4898) | Loss 30.1293(32.1902) | Error 0.1911(0.1727) Steps 0(0.00) | Grad Norm 21.0456(23.4371) | Total Time 0.00(0.00)\n",
      "Iter 7300 | Time 20.1864(20.1996) | Bit/dim 3.6114(3.6279) | Xent 0.4472(0.4830) | Loss 29.8960(31.6900) | Error 0.1633(0.1696) Steps 0(0.00) | Grad Norm 19.6245(22.5926) | Total Time 0.00(0.00)\n",
      "Iter 7310 | Time 20.6756(20.2069) | Bit/dim 3.6071(3.6254) | Xent 0.5004(0.4831) | Loss 30.5600(31.1668) | Error 0.1778(0.1698) Steps 0(0.00) | Grad Norm 31.1214(21.9852) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0133 | Time 99.0666, Epoch Time 1226.7948(1166.6449), Bit/dim 3.6302(best: 3.6295), Xent 0.6580, Loss 3.9592, Error 0.2209(best: 0.2246)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7320 | Time 21.4010(20.2219) | Bit/dim 3.6287(3.6242) | Xent 0.4949(0.4797) | Loss 30.3572(34.6775) | Error 0.1889(0.1708) Steps 0(0.00) | Grad Norm 22.5433(21.3467) | Total Time 0.00(0.00)\n",
      "Iter 7330 | Time 19.0802(20.2728) | Bit/dim 3.6075(3.6250) | Xent 0.4732(0.4841) | Loss 29.6344(33.5150) | Error 0.1767(0.1731) Steps 0(0.00) | Grad Norm 27.9015(24.4064) | Total Time 0.00(0.00)\n",
      "Iter 7340 | Time 19.8821(20.1530) | Bit/dim 3.6305(3.6279) | Xent 0.5124(0.4798) | Loss 30.8805(32.6104) | Error 0.1744(0.1709) Steps 0(0.00) | Grad Norm 18.0707(23.4125) | Total Time 0.00(0.00)\n",
      "Iter 7350 | Time 20.1711(20.1076) | Bit/dim 3.6262(3.6267) | Xent 0.4445(0.4784) | Loss 30.5204(31.9293) | Error 0.1633(0.1696) Steps 0(0.00) | Grad Norm 26.2585(22.6663) | Total Time 0.00(0.00)\n",
      "Iter 7360 | Time 26.0260(20.4192) | Bit/dim 3.6142(3.6268) | Xent 0.4872(0.4751) | Loss 29.0545(31.3645) | Error 0.1733(0.1693) Steps 0(0.00) | Grad Norm 23.4041(23.2450) | Total Time 0.00(0.00)\n",
      "Iter 7370 | Time 19.8141(20.2949) | Bit/dim 3.6467(3.6278) | Xent 0.5392(0.4784) | Loss 30.0541(30.9644) | Error 0.1756(0.1701) Steps 0(0.00) | Grad Norm 27.4689(23.3607) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0134 | Time 97.9643, Epoch Time 1232.0137(1168.6060), Bit/dim 3.6287(best: 3.6295), Xent 0.6885, Loss 3.9730, Error 0.2287(best: 0.2209)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7380 | Time 19.7131(20.2338) | Bit/dim 3.6519(3.6292) | Xent 0.5061(0.4814) | Loss 30.1371(33.8766) | Error 0.1878(0.1708) Steps 0(0.00) | Grad Norm 22.7137(23.5268) | Total Time 0.00(0.00)\n",
      "Iter 7390 | Time 19.6844(20.1128) | Bit/dim 3.6369(3.6282) | Xent 0.4941(0.4801) | Loss 30.0573(32.8717) | Error 0.1811(0.1711) Steps 0(0.00) | Grad Norm 18.4328(23.9026) | Total Time 0.00(0.00)\n",
      "Iter 7400 | Time 20.0048(20.1692) | Bit/dim 3.6147(3.6288) | Xent 0.4708(0.4737) | Loss 29.8874(32.1773) | Error 0.1633(0.1687) Steps 0(0.00) | Grad Norm 21.2841(23.3068) | Total Time 0.00(0.00)\n",
      "Iter 7410 | Time 20.0357(20.1823) | Bit/dim 3.6145(3.6242) | Xent 0.4045(0.4681) | Loss 29.8300(31.5349) | Error 0.1422(0.1668) Steps 0(0.00) | Grad Norm 15.1032(21.9608) | Total Time 0.00(0.00)\n",
      "Iter 7420 | Time 20.6526(20.3135) | Bit/dim 3.6282(3.6237) | Xent 0.4306(0.4660) | Loss 29.9342(31.0690) | Error 0.1656(0.1663) Steps 0(0.00) | Grad Norm 19.3537(20.8671) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0135 | Time 99.1803, Epoch Time 1236.4358(1170.6409), Bit/dim 3.6316(best: 3.6287), Xent 0.7010, Loss 3.9821, Error 0.2294(best: 0.2209)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7430 | Time 23.3465(20.5474) | Bit/dim 3.6275(3.6252) | Xent 0.4115(0.4597) | Loss 29.5614(34.5686) | Error 0.1533(0.1635) Steps 0(0.00) | Grad Norm 13.5920(21.0086) | Total Time 0.00(0.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_rl.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_6_rlw_0_05_run1 --seed 1 --lr 0.001 --conditional True --controlled_tol False --train_mode semisup --log_freq 10 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5 --scale_fac 1.0 --scale_std 6.0 --rl-weight 0.05\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
