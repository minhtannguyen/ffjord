{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0,1,2,3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_rl.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "GATES = [\"cnn1\", \"cnn2\", \"rnn\"]\n",
      "\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--gate', type=str, default='cnn1', choices=GATES)\n",
      "parser.add_argument('--scale', type=float, default=1.0)\n",
      "parser.add_argument('--scale_fac', type=float, default=1.0)\n",
      "parser.add_argument('--scale_std', type=float, default=1.0)\n",
      "parser.add_argument('--eta', default=0.1, type=float,\n",
      "                        help='tuning parameter that allows us to trade-off the competing goals of' \n",
      "                                'minimizing the prediction loss and maximizing the gate rewards ')\n",
      "parser.add_argument('--rl-weight', default=0.01, type=float,\n",
      "                        help='rl weight')\n",
      "\n",
      "parser.add_argument('--gamma', default=0.99, type=float,\n",
      "                        help='discount factor, default: (0.99)')\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "import lib.odenvp_conditional_rl as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol, \"scale\": args.scale, \"scale_fac\": args.scale_fac, \"scale_std\": args.scale_std, \"gate\": args.gate},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            # re-weight the gate rewards\n",
      "            normalized_eta = args.eta / len(logp_actions)\n",
      "            \n",
      "            # collect cumulative future rewards\n",
      "            R = - loss\n",
      "            cum_rewards = []\n",
      "            for r in nfe[::-1]:\n",
      "                R = -normalized_eta * r.view(-1,1) + args.gamma * R\n",
      "                cum_rewards.insert(0,R)\n",
      "            \n",
      "            # apply REINFORCE\n",
      "            rl_loss = 0\n",
      "            for lpa, r in zip(logp_actions, cum_rewards):\n",
      "                rl_loss = rl_loss - lpa.view(-1,1) * args.rl_weight * r\n",
      "                \n",
      "            loss = loss + rl_loss.mean()\n",
      "            \n",
      "            loss.backward()\n",
      "            \n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "            \n",
      "            for idx in range(len(model.module.transforms)):\n",
      "                for layer in model.module.transforms[idx].chain:\n",
      "                    if hasattr(layer, 'atol'):\n",
      "                        layer.odefunc.after_odeint()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'train': atol[tol_indx].mean()}, itr)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'train': rtol[tol_indx].mean()}, itr)\n",
      "                    \n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples, atol, rtol, logp_actions, nfe = model(fixed_z, reverse=True)\n",
      "            generated_samples = generated_samples.view(-1, *data_shape)\n",
      "            for tol_indx in range(len(atol)):\n",
      "                writer.add_scalars('atol_gen_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                writer.add_scalars('rtol_gen_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, condition_ratio=0.5, conditional=True, controlled_tol=False, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, eta=0.1, gamma=0.99, gate='cnn1', imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume=None, rl_weight=0.05, rtol=1e-05, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_6_rlw_0_05_run1', scale=1.0, scale_fac=1.0, scale_std=6.0, seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1450732\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc_rl.py:291: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Iter 0010 | Time 12.2438(29.8277) | Bit/dim 8.6902(8.9522) | Xent 2.2806(2.3001) | Loss 65.3205(66.2322) | Error 0.7956(0.8601) Steps 0(0.00) | Grad Norm 66.9255(84.8937) | Total Time 0.00(0.00)\n",
      "Iter 0020 | Time 12.2516(25.2713) | Bit/dim 8.4872(8.8631) | Xent 2.2264(2.2874) | Loss 63.8980(65.5772) | Error 0.7200(0.8319) Steps 0(0.00) | Grad Norm 27.8603(73.4580) | Total Time 0.00(0.00)\n",
      "Iter 0030 | Time 13.2841(21.9654) | Bit/dim 8.3932(8.7514) | Xent 2.1747(2.2634) | Loss 64.6328(64.8737) | Error 0.7589(0.8089) Steps 0(0.00) | Grad Norm 28.1457(60.3518) | Total Time 0.00(0.00)\n",
      "Iter 0040 | Time 13.4525(19.5038) | Bit/dim 8.1911(8.6233) | Xent 2.1135(2.2345) | Loss 61.5980(64.0807) | Error 0.7233(0.7911) Steps 0(0.00) | Grad Norm 17.1397(49.5569) | Total Time 0.00(0.00)\n",
      "Iter 0050 | Time 12.5426(17.7577) | Bit/dim 7.9736(8.4737) | Xent 2.1016(2.2019) | Loss 59.2445(63.0996) | Error 0.7111(0.7758) Steps 0(0.00) | Grad Norm 16.5063(41.1679) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0001 | Time 76.7278, Epoch Time 815.9937(815.9937), Bit/dim 7.7695(best: inf), Xent 2.0781, Loss 8.8085, Error 0.7001(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0060 | Time 12.7658(16.5118) | Bit/dim 7.6770(8.2912) | Xent 2.0791(2.1719) | Loss 57.2385(64.6415) | Error 0.7022(0.7591) Steps 0(0.00) | Grad Norm 17.1156(34.7419) | Total Time 0.00(0.00)\n",
      "Iter 0070 | Time 13.2235(15.6172) | Bit/dim 7.3716(8.0804) | Xent 2.0567(2.1461) | Loss 56.7826(62.6408) | Error 0.6744(0.7423) Steps 0(0.00) | Grad Norm 12.4854(29.3737) | Total Time 0.00(0.00)\n",
      "Iter 0080 | Time 13.1826(14.9178) | Bit/dim 7.1865(7.8636) | Xent 2.0699(2.1256) | Loss 55.4170(60.6989) | Error 0.6744(0.7266) Steps 0(0.00) | Grad Norm 10.2096(24.5500) | Total Time 0.00(0.00)\n",
      "Iter 0090 | Time 12.5000(14.4505) | Bit/dim 7.0826(7.6689) | Xent 2.0842(2.1128) | Loss 55.0928(59.0627) | Error 0.7244(0.7183) Steps 0(0.00) | Grad Norm 6.4414(20.2099) | Total Time 0.00(0.00)\n",
      "Iter 0100 | Time 13.9257(14.1586) | Bit/dim 7.0013(7.5061) | Xent 2.0627(2.1032) | Loss 54.8471(57.7856) | Error 0.6900(0.7124) Steps 0(0.00) | Grad Norm 10.3904(17.3149) | Total Time 0.00(0.00)\n",
      "Iter 0110 | Time 13.7924(14.0122) | Bit/dim 7.0033(7.3753) | Xent 2.0601(2.0947) | Loss 55.3264(56.8362) | Error 0.7233(0.7120) Steps 0(0.00) | Grad Norm 7.1833(14.6148) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0002 | Time 77.4561, Epoch Time 820.0994(816.1169), Bit/dim 6.9922(best: 7.7695), Xent 2.0583, Loss 8.0213, Error 0.6953(best: 0.7001)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0120 | Time 14.2652(14.1578) | Bit/dim 6.9412(7.2695) | Xent 2.0517(2.0853) | Loss 53.1381(58.7624) | Error 0.7067(0.7110) Steps 0(0.00) | Grad Norm 5.2872(12.7890) | Total Time 0.00(0.00)\n",
      "Iter 0130 | Time 14.3838(14.2001) | Bit/dim 6.9196(7.1828) | Xent 2.0408(2.0744) | Loss 53.9458(57.3531) | Error 0.6922(0.7062) Steps 0(0.00) | Grad Norm 9.0659(11.3784) | Total Time 0.00(0.00)\n",
      "Iter 0140 | Time 16.9011(14.2851) | Bit/dim 6.8951(7.1069) | Xent 2.0499(2.0647) | Loss 53.4148(56.2342) | Error 0.6878(0.7025) Steps 0(0.00) | Grad Norm 25.5051(11.8595) | Total Time 0.00(0.00)\n",
      "Iter 0150 | Time 15.3943(14.3995) | Bit/dim 6.7858(7.0356) | Xent 2.0155(2.0565) | Loss 51.3296(55.2197) | Error 0.7056(0.7001) Steps 0(0.00) | Grad Norm 12.7714(15.1854) | Total Time 0.00(0.00)\n",
      "Iter 0160 | Time 14.7350(14.4449) | Bit/dim 6.7045(6.9609) | Xent 2.0329(2.0515) | Loss 52.9876(54.5453) | Error 0.7067(0.6981) Steps 0(0.00) | Grad Norm 59.9489(22.7112) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0003 | Time 80.0534, Epoch Time 898.8037(818.5975), Bit/dim 6.6376(best: 6.9922), Xent 2.0365, Loss 7.6558, Error 0.7047(best: 0.6953)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0170 | Time 15.3406(14.5439) | Bit/dim 6.6174(6.8785) | Xent 2.0612(2.0526) | Loss 52.8081(57.0233) | Error 0.7289(0.7035) Steps 0(0.00) | Grad Norm 157.6722(44.3559) | Total Time 0.00(0.00)\n",
      "Iter 0180 | Time 16.4179(14.7235) | Bit/dim 6.4294(6.7767) | Xent 2.1360(2.0522) | Loss 50.5069(55.4448) | Error 0.7967(0.7115) Steps 0(0.00) | Grad Norm 253.4283(66.6731) | Total Time 0.00(0.00)\n",
      "Iter 0190 | Time 13.4041(14.6786) | Bit/dim 6.2223(6.6561) | Xent 2.0353(2.0513) | Loss 49.2900(54.0684) | Error 0.6778(0.7146) Steps 0(0.00) | Grad Norm 58.6626(81.8880) | Total Time 0.00(0.00)\n",
      "Iter 0200 | Time 15.2268(14.7882) | Bit/dim 6.3093(6.5655) | Xent 2.0543(2.1096) | Loss 49.8538(53.2019) | Error 0.7067(0.7271) Steps 0(0.00) | Grad Norm 169.0847(160.0645) | Total Time 0.00(0.00)\n",
      "Iter 0210 | Time 14.3315(14.9112) | Bit/dim 6.0884(6.4574) | Xent 2.0976(2.1031) | Loss 49.1029(52.0377) | Error 0.7278(0.7269) Steps 0(0.00) | Grad Norm 56.7391(137.0537) | Total Time 0.00(0.00)\n",
      "Iter 0220 | Time 13.5874(14.7291) | Bit/dim 5.8966(6.3268) | Xent 2.0412(2.0931) | Loss 48.0869(50.9084) | Error 0.6878(0.7246) Steps 0(0.00) | Grad Norm 28.6541(113.7027) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0004 | Time 78.7390, Epoch Time 914.6332(821.4786), Bit/dim 5.8929(best: 6.6376), Xent 2.0490, Loss 6.9174, Error 0.7078(best: 0.6953)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0230 | Time 13.7954(14.6678) | Bit/dim 5.8663(6.2122) | Xent 2.0742(2.0877) | Loss 46.5860(52.5225) | Error 0.7200(0.7251) Steps 0(0.00) | Grad Norm 94.5270(123.5589) | Total Time 0.00(0.00)\n",
      "Iter 0240 | Time 14.6993(14.5199) | Bit/dim 5.7941(6.1027) | Xent 2.0324(2.0721) | Loss 47.3929(50.8168) | Error 0.7344(0.7204) Steps 0(0.00) | Grad Norm 107.7447(114.6895) | Total Time 0.00(0.00)\n",
      "Iter 0250 | Time 13.2465(14.3941) | Bit/dim 5.7076(6.0073) | Xent 1.9996(2.0569) | Loss 45.6485(49.6067) | Error 0.6633(0.7105) Steps 0(0.00) | Grad Norm 57.8935(101.9788) | Total Time 0.00(0.00)\n",
      "Iter 0260 | Time 13.6423(14.2771) | Bit/dim 5.6590(5.9204) | Xent 1.9796(2.0393) | Loss 45.3121(48.6834) | Error 0.6956(0.7008) Steps 0(0.00) | Grad Norm 38.3832(90.1433) | Total Time 0.00(0.00)\n",
      "Iter 0270 | Time 13.8742(14.2527) | Bit/dim 5.6253(5.8513) | Xent 1.9379(2.0224) | Loss 42.6383(47.8974) | Error 0.6400(0.6924) Steps 0(0.00) | Grad Norm 50.8154(87.3492) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0005 | Time 79.6452, Epoch Time 874.7930(823.0780), Bit/dim 5.6307(best: 5.8929), Xent 1.9477, Loss 6.6046, Error 0.6460(best: 0.6953)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0280 | Time 14.1169(14.1941) | Bit/dim 5.5725(5.7905) | Xent 2.0102(2.0183) | Loss 46.0765(50.1491) | Error 0.7189(0.6974) Steps 0(0.00) | Grad Norm 150.2556(101.2769) | Total Time 0.00(0.00)\n",
      "Iter 0290 | Time 13.5733(14.0612) | Bit/dim 5.6193(5.7446) | Xent 1.9355(2.0057) | Loss 45.8384(48.8837) | Error 0.6478(0.6918) Steps 0(0.00) | Grad Norm 22.6632(91.7919) | Total Time 0.00(0.00)\n",
      "Iter 0300 | Time 13.3558(13.9806) | Bit/dim 5.5548(5.7040) | Xent 1.9268(1.9917) | Loss 44.6122(47.9508) | Error 0.6689(0.6844) Steps 0(0.00) | Grad Norm 96.8703(87.3259) | Total Time 0.00(0.00)\n",
      "Iter 0310 | Time 13.3202(13.9457) | Bit/dim 5.5587(5.6694) | Xent 2.0509(1.9918) | Loss 43.2491(47.2278) | Error 0.7233(0.6876) Steps 0(0.00) | Grad Norm 275.5309(120.3111) | Total Time 0.00(0.00)\n",
      "Iter 0320 | Time 13.7728(13.9738) | Bit/dim 5.5407(5.6381) | Xent 1.9521(1.9867) | Loss 44.6849(46.5993) | Error 0.6811(0.6865) Steps 0(0.00) | Grad Norm 58.2101(117.1424) | Total Time 0.00(0.00)\n",
      "Iter 0330 | Time 13.9879(13.9847) | Bit/dim 5.5658(5.6140) | Xent 1.9505(1.9842) | Loss 45.3137(46.1550) | Error 0.6600(0.6866) Steps 0(0.00) | Grad Norm 74.4939(113.6013) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0006 | Time 80.1264, Epoch Time 861.8597(824.2415), Bit/dim 5.5096(best: 5.6307), Xent 1.9562, Loss 6.4877, Error 0.6843(best: 0.6460)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0340 | Time 14.4707(14.0819) | Bit/dim 5.4914(5.5853) | Xent 1.9516(1.9743) | Loss 45.1136(48.4607) | Error 0.6544(0.6817) Steps 0(0.00) | Grad Norm 32.1376(95.5882) | Total Time 0.00(0.00)\n",
      "Iter 0350 | Time 14.2046(14.1529) | Bit/dim 5.4192(5.5490) | Xent 1.9075(1.9589) | Loss 44.3849(47.3930) | Error 0.6600(0.6755) Steps 0(0.00) | Grad Norm 50.6708(86.9633) | Total Time 0.00(0.00)\n",
      "Iter 0360 | Time 14.4644(14.1625) | Bit/dim 5.4170(5.5215) | Xent 2.0790(1.9718) | Loss 43.8467(46.5442) | Error 0.7556(0.6850) Steps 0(0.00) | Grad Norm 83.8591(96.3700) | Total Time 0.00(0.00)\n",
      "Iter 0370 | Time 14.4560(14.3406) | Bit/dim 5.3966(5.4897) | Xent 1.9496(1.9795) | Loss 43.4090(45.9120) | Error 0.6789(0.6886) Steps 0(0.00) | Grad Norm 57.3114(85.8520) | Total Time 0.00(0.00)\n",
      "Iter 0380 | Time 13.6580(14.4183) | Bit/dim 5.3508(5.4570) | Xent 1.9247(1.9716) | Loss 42.8849(45.3522) | Error 0.6589(0.6834) Steps 0(0.00) | Grad Norm 44.7809(74.4976) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0007 | Time 82.3439, Epoch Time 897.3070(826.4334), Bit/dim 5.3169(best: 5.5096), Xent 1.8898, Loss 6.2618, Error 0.6389(best: 0.6460)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0390 | Time 15.4858(14.5173) | Bit/dim 5.2703(5.4267) | Xent 1.8865(1.9596) | Loss 43.6039(47.9273) | Error 0.6422(0.6773) Steps 0(0.00) | Grad Norm 50.0415(73.2578) | Total Time 0.00(0.00)\n",
      "Iter 0400 | Time 14.9543(14.6223) | Bit/dim 5.3205(5.3891) | Xent 1.8945(1.9399) | Loss 44.5445(46.6965) | Error 0.6500(0.6681) Steps 0(0.00) | Grad Norm 75.2711(69.3252) | Total Time 0.00(0.00)\n",
      "Iter 0410 | Time 14.9338(14.6848) | Bit/dim 5.3385(5.3571) | Xent 1.8957(1.9337) | Loss 42.7855(45.8277) | Error 0.6611(0.6679) Steps 0(0.00) | Grad Norm 49.1320(85.0651) | Total Time 0.00(0.00)\n",
      "Iter 0420 | Time 17.8438(14.7882) | Bit/dim 5.2288(5.3218) | Xent 1.8726(1.9246) | Loss 43.5903(45.0894) | Error 0.6667(0.6657) Steps 0(0.00) | Grad Norm 73.2519(76.6052) | Total Time 0.00(0.00)\n",
      "Iter 0430 | Time 14.3816(14.8542) | Bit/dim 5.2272(5.2870) | Xent 1.9025(1.9177) | Loss 43.3296(44.3959) | Error 0.6744(0.6642) Steps 0(0.00) | Grad Norm 86.3835(70.0060) | Total Time 0.00(0.00)\n",
      "Iter 0440 | Time 14.6843(14.9191) | Bit/dim 5.2497(5.2564) | Xent 1.8578(1.9093) | Loss 42.4631(43.8752) | Error 0.6589(0.6633) Steps 0(0.00) | Grad Norm 97.0042(71.1432) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0008 | Time 81.3235, Epoch Time 922.9338(829.3284), Bit/dim 5.1470(best: 5.3169), Xent 1.8781, Loss 6.0861, Error 0.6482(best: 0.6389)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0450 | Time 15.3894(14.8214) | Bit/dim 5.1541(5.2290) | Xent 1.8836(1.9029) | Loss 41.0319(46.2531) | Error 0.6600(0.6616) Steps 0(0.00) | Grad Norm 88.8104(73.3084) | Total Time 0.00(0.00)\n",
      "Iter 0460 | Time 16.5774(14.9543) | Bit/dim 5.1729(5.2098) | Xent 2.0744(1.9474) | Loss 43.6418(45.4652) | Error 0.7556(0.6786) Steps 0(0.00) | Grad Norm 108.7906(105.7824) | Total Time 0.00(0.00)\n",
      "Iter 0470 | Time 15.1973(14.9983) | Bit/dim 5.1184(5.1848) | Xent 1.9647(1.9633) | Loss 41.6496(44.7020) | Error 0.6989(0.6872) Steps 0(0.00) | Grad Norm 29.4514(91.5609) | Total Time 0.00(0.00)\n",
      "Iter 0480 | Time 16.1679(15.0914) | Bit/dim 5.0595(5.1543) | Xent 1.9421(1.9640) | Loss 42.5741(43.9910) | Error 0.6567(0.6890) Steps 0(0.00) | Grad Norm 74.2952(80.8193) | Total Time 0.00(0.00)\n",
      "Iter 0490 | Time 15.7912(15.1659) | Bit/dim 5.0620(5.1245) | Xent 1.9103(1.9542) | Loss 42.5281(43.4713) | Error 0.6578(0.6850) Steps 0(0.00) | Grad Norm 22.9735(74.5428) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0009 | Time 83.1809, Epoch Time 934.1685(832.4736), Bit/dim 4.9824(best: 5.1470), Xent 1.8751, Loss 5.9199, Error 0.6396(best: 0.6389)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0500 | Time 15.0720(15.1981) | Bit/dim 5.0028(5.0921) | Xent 1.9728(1.9477) | Loss 40.7478(45.7592) | Error 0.6989(0.6817) Steps 0(0.00) | Grad Norm 158.1299(81.6093) | Total Time 0.00(0.00)\n",
      "Iter 0510 | Time 15.3244(15.2228) | Bit/dim 4.9801(5.0609) | Xent 1.8502(1.9306) | Loss 41.6830(44.5817) | Error 0.6356(0.6769) Steps 0(0.00) | Grad Norm 66.1229(79.1529) | Total Time 0.00(0.00)\n",
      "Iter 0520 | Time 16.4914(15.4285) | Bit/dim 4.9458(5.0296) | Xent 1.7952(1.9091) | Loss 41.9646(43.7296) | Error 0.6389(0.6693) Steps 0(0.00) | Grad Norm 67.0416(72.8636) | Total Time 0.00(0.00)\n",
      "Iter 0530 | Time 15.2207(15.6971) | Bit/dim 4.9202(5.0042) | Xent 1.9070(1.9035) | Loss 39.7749(43.1191) | Error 0.6611(0.6670) Steps 0(0.00) | Grad Norm 100.9135(82.0416) | Total Time 0.00(0.00)\n",
      "Iter 0540 | Time 16.3149(15.9521) | Bit/dim 4.9215(4.9904) | Xent 1.8703(1.9000) | Loss 41.7017(42.5913) | Error 0.6678(0.6651) Steps 0(0.00) | Grad Norm 89.6835(89.7933) | Total Time 0.00(0.00)\n",
      "Iter 0550 | Time 17.4655(16.0499) | Bit/dim 4.8358(4.9630) | Xent 1.9572(1.8932) | Loss 41.0578(42.1312) | Error 0.6889(0.6625) Steps 0(0.00) | Grad Norm 54.3478(81.7443) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0010 | Time 83.9074, Epoch Time 986.5261(837.0952), Bit/dim 4.8658(best: 4.9824), Xent 1.7934, Loss 5.7625, Error 0.6099(best: 0.6389)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0560 | Time 15.9780(15.9897) | Bit/dim 4.8352(4.9373) | Xent 1.7804(1.8775) | Loss 40.0160(44.4346) | Error 0.6222(0.6565) Steps 0(0.00) | Grad Norm 36.7547(87.8077) | Total Time 0.00(0.00)\n",
      "Iter 0570 | Time 15.1324(15.9853) | Bit/dim 4.8574(4.9232) | Xent 1.9232(1.8844) | Loss 40.1655(43.4167) | Error 0.6789(0.6630) Steps 0(0.00) | Grad Norm 106.7717(101.5128) | Total Time 0.00(0.00)\n",
      "Iter 0580 | Time 16.2147(15.9874) | Bit/dim 4.8574(4.8969) | Xent 1.8373(1.8768) | Loss 40.4678(42.7539) | Error 0.6222(0.6588) Steps 0(0.00) | Grad Norm 59.4072(90.5638) | Total Time 0.00(0.00)\n",
      "Iter 0590 | Time 15.3326(16.0874) | Bit/dim 4.8802(4.8755) | Xent 1.8365(1.8582) | Loss 39.2099(42.0793) | Error 0.6456(0.6528) Steps 0(0.00) | Grad Norm 157.9408(90.2586) | Total Time 0.00(0.00)\n",
      "Iter 0600 | Time 17.0009(16.0600) | Bit/dim 4.8020(4.8581) | Xent 1.8220(1.8506) | Loss 39.6477(41.5694) | Error 0.6333(0.6505) Steps 0(0.00) | Grad Norm 77.4256(94.6365) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0011 | Time 87.4526, Epoch Time 985.8010(841.5564), Bit/dim 4.7596(best: 4.8658), Xent 1.7947, Loss 5.6569, Error 0.6266(best: 0.6099)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0610 | Time 15.5515(16.0481) | Bit/dim 4.7725(4.8341) | Xent 1.8249(1.8441) | Loss 38.0518(44.3568) | Error 0.6322(0.6487) Steps 0(0.00) | Grad Norm 87.0165(91.6445) | Total Time 0.00(0.00)\n",
      "Iter 0620 | Time 15.8998(16.1925) | Bit/dim 4.8280(4.8332) | Xent 2.0478(1.8564) | Loss 41.5423(43.4274) | Error 0.7444(0.6555) Steps 0(0.00) | Grad Norm 244.7114(110.2533) | Total Time 0.00(0.00)\n",
      "Iter 0630 | Time 17.4724(16.1750) | Bit/dim 4.7831(4.8246) | Xent 1.7896(1.8561) | Loss 39.1445(42.5147) | Error 0.6333(0.6565) Steps 0(0.00) | Grad Norm 72.7799(103.4449) | Total Time 0.00(0.00)\n",
      "Iter 0640 | Time 16.3612(16.2482) | Bit/dim 4.7372(4.8029) | Xent 1.7360(1.8435) | Loss 40.3012(41.8308) | Error 0.6044(0.6510) Steps 0(0.00) | Grad Norm 29.9598(88.0382) | Total Time 0.00(0.00)\n",
      "Iter 0650 | Time 15.2847(16.1772) | Bit/dim 4.8582(4.7899) | Xent 1.8508(1.8286) | Loss 40.1041(41.2795) | Error 0.6656(0.6466) Steps 0(0.00) | Grad Norm 146.6966(88.2645) | Total Time 0.00(0.00)\n",
      "Iter 0660 | Time 15.7788(16.2154) | Bit/dim 4.7267(4.7869) | Xent 1.8521(1.8312) | Loss 40.1307(40.9934) | Error 0.6722(0.6480) Steps 0(0.00) | Grad Norm 59.6899(87.8767) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0012 | Time 87.0617, Epoch Time 1001.5497(846.3562), Bit/dim 4.7501(best: 4.7596), Xent 1.7526, Loss 5.6264, Error 0.6145(best: 0.6099)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0670 | Time 14.6111(16.2227) | Bit/dim 4.6981(4.7659) | Xent 1.7171(1.8102) | Loss 39.4486(43.1367) | Error 0.6278(0.6405) Steps 0(0.00) | Grad Norm 66.9586(78.2026) | Total Time 0.00(0.00)\n",
      "Iter 0680 | Time 16.0276(16.3510) | Bit/dim 4.6919(4.7523) | Xent 1.7689(1.7998) | Loss 39.3404(42.1775) | Error 0.6178(0.6369) Steps 0(0.00) | Grad Norm 104.8317(87.1519) | Total Time 0.00(0.00)\n",
      "Iter 0690 | Time 16.6819(16.4726) | Bit/dim 4.6879(4.7344) | Xent 1.7594(1.7911) | Loss 38.0596(41.3874) | Error 0.6133(0.6335) Steps 0(0.00) | Grad Norm 60.8352(82.2865) | Total Time 0.00(0.00)\n",
      "Iter 0700 | Time 17.6093(16.4715) | Bit/dim 4.6759(4.7142) | Xent 1.7316(1.7730) | Loss 39.0944(40.7460) | Error 0.6278(0.6265) Steps 0(0.00) | Grad Norm 62.7629(76.4685) | Total Time 0.00(0.00)\n",
      "Iter 0710 | Time 17.2677(16.6229) | Bit/dim 4.6447(4.6938) | Xent 1.6830(1.7540) | Loss 39.7758(40.3755) | Error 0.6133(0.6218) Steps 0(0.00) | Grad Norm 97.7249(71.0710) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0013 | Time 90.2760, Epoch Time 1023.7435(851.6778), Bit/dim 4.6477(best: 4.7501), Xent 1.8117, Loss 5.5536, Error 0.6520(best: 0.6099)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0720 | Time 17.7974(16.7033) | Bit/dim 4.6403(4.6887) | Xent 1.8533(1.7751) | Loss 39.2554(43.4093) | Error 0.6544(0.6309) Steps 0(0.00) | Grad Norm 131.6561(87.2683) | Total Time 0.00(0.00)\n",
      "Iter 0730 | Time 16.5095(16.6938) | Bit/dim 4.5898(4.6728) | Xent 1.7330(1.7649) | Loss 37.1950(42.1575) | Error 0.6300(0.6281) Steps 0(0.00) | Grad Norm 36.2626(78.7355) | Total Time 0.00(0.00)\n",
      "Iter 0740 | Time 15.9096(16.8896) | Bit/dim 4.5575(4.6538) | Xent 1.6559(1.7455) | Loss 38.9693(41.3235) | Error 0.5922(0.6212) Steps 0(0.00) | Grad Norm 22.6299(65.0873) | Total Time 0.00(0.00)\n",
      "Iter 0750 | Time 15.6469(17.0017) | Bit/dim 4.6225(4.6394) | Xent 1.6638(1.7302) | Loss 38.4498(40.5916) | Error 0.6133(0.6154) Steps 0(0.00) | Grad Norm 111.1391(66.7043) | Total Time 0.00(0.00)\n",
      "Iter 0760 | Time 17.3153(17.0258) | Bit/dim 4.6081(4.6293) | Xent 1.7004(1.7193) | Loss 39.8814(40.1504) | Error 0.6144(0.6137) Steps 0(0.00) | Grad Norm 103.4821(68.6797) | Total Time 0.00(0.00)\n",
      "Iter 0770 | Time 16.7975(16.9879) | Bit/dim 4.5882(4.6166) | Xent 1.7241(1.7154) | Loss 38.9905(39.7417) | Error 0.6122(0.6119) Steps 0(0.00) | Grad Norm 116.9118(72.2503) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0014 | Time 91.9178, Epoch Time 1051.7311(857.6794), Bit/dim 4.6331(best: 4.6477), Xent 1.6722, Loss 5.4692, Error 0.6048(best: 0.6099)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0780 | Time 18.5033(17.2619) | Bit/dim 4.6956(4.6301) | Xent 1.7432(1.7431) | Loss 38.0536(42.5747) | Error 0.5989(0.6201) Steps 0(0.00) | Grad Norm 93.1746(85.4092) | Total Time 0.00(0.00)\n",
      "Iter 0790 | Time 15.4430(17.1503) | Bit/dim 4.5618(4.6212) | Xent 1.6797(1.7459) | Loss 39.1860(41.6521) | Error 0.6122(0.6213) Steps 0(0.00) | Grad Norm 25.9319(80.4419) | Total Time 0.00(0.00)\n",
      "Iter 0800 | Time 17.1938(17.1125) | Bit/dim 4.5623(4.6056) | Xent 1.7221(1.7335) | Loss 38.2861(40.8169) | Error 0.5978(0.6144) Steps 0(0.00) | Grad Norm 65.5214(72.3766) | Total Time 0.00(0.00)\n",
      "Iter 0810 | Time 17.2087(17.0820) | Bit/dim 4.4996(4.5860) | Xent 1.6952(1.7137) | Loss 37.9487(40.1567) | Error 0.5989(0.6090) Steps 0(0.00) | Grad Norm 14.5328(65.2172) | Total Time 0.00(0.00)\n",
      "Iter 0820 | Time 19.9029(17.0602) | Bit/dim 4.5249(4.5685) | Xent 1.6751(1.7011) | Loss 37.6665(39.5634) | Error 0.5933(0.6058) Steps 0(0.00) | Grad Norm 67.9158(62.7517) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0015 | Time 88.8977, Epoch Time 1053.0476(863.5405), Bit/dim 4.5384(best: 4.6331), Xent 1.6062, Loss 5.3414, Error 0.5690(best: 0.6048)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0830 | Time 16.6049(17.1062) | Bit/dim 4.4989(4.5533) | Xent 1.6804(1.6854) | Loss 37.6255(42.5494) | Error 0.6089(0.6005) Steps 0(0.00) | Grad Norm 81.7431(61.9975) | Total Time 0.00(0.00)\n",
      "Iter 0840 | Time 16.5726(17.1526) | Bit/dim 4.5085(4.5393) | Xent 1.6766(1.6779) | Loss 37.6465(41.3440) | Error 0.6056(0.5998) Steps 0(0.00) | Grad Norm 58.7514(61.2772) | Total Time 0.00(0.00)\n",
      "Iter 0850 | Time 16.9742(17.1445) | Bit/dim 4.4678(4.5252) | Xent 1.6337(1.6639) | Loss 37.8816(40.4581) | Error 0.5789(0.5964) Steps 0(0.00) | Grad Norm 19.2986(54.1638) | Total Time 0.00(0.00)\n",
      "Iter 0860 | Time 16.2393(17.0445) | Bit/dim 4.5372(4.5212) | Xent 1.6395(1.6606) | Loss 37.3187(39.7774) | Error 0.5867(0.5972) Steps 0(0.00) | Grad Norm 59.7624(63.7507) | Total Time 0.00(0.00)\n",
      "Iter 0870 | Time 17.0745(17.1449) | Bit/dim 4.5180(4.5140) | Xent 1.6925(1.6708) | Loss 38.4101(39.5190) | Error 0.6256(0.6026) Steps 0(0.00) | Grad Norm 77.3163(67.7835) | Total Time 0.00(0.00)\n",
      "Iter 0880 | Time 19.3128(17.3139) | Bit/dim 4.4335(4.5017) | Xent 1.6255(1.6671) | Loss 37.8695(39.1737) | Error 0.5900(0.6034) Steps 0(0.00) | Grad Norm 46.5689(66.9385) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0016 | Time 90.6523, Epoch Time 1057.0979(869.3472), Bit/dim 4.4485(best: 4.5384), Xent 1.5381, Loss 5.2175, Error 0.5482(best: 0.5690)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0890 | Time 15.8878(17.3261) | Bit/dim 4.4441(4.4931) | Xent 1.5682(1.6504) | Loss 37.1154(41.6274) | Error 0.5622(0.5972) Steps 0(0.00) | Grad Norm 18.9164(63.1518) | Total Time 0.00(0.00)\n",
      "Iter 0900 | Time 17.5292(17.4331) | Bit/dim 4.4173(4.4753) | Xent 1.5794(1.6358) | Loss 36.3738(40.5314) | Error 0.5589(0.5890) Steps 0(0.00) | Grad Norm 65.4867(59.8844) | Total Time 0.00(0.00)\n",
      "Iter 0910 | Time 18.9589(17.5919) | Bit/dim 4.4218(4.4682) | Xent 1.6160(1.6368) | Loss 38.0744(39.9414) | Error 0.5778(0.5883) Steps 0(0.00) | Grad Norm 66.4914(66.5142) | Total Time 0.00(0.00)\n",
      "Iter 0920 | Time 16.6388(17.5586) | Bit/dim 4.3890(4.4625) | Xent 1.6637(1.6404) | Loss 35.8007(39.2694) | Error 0.6056(0.5890) Steps 0(0.00) | Grad Norm 42.5571(65.4944) | Total Time 0.00(0.00)\n",
      "Iter 0930 | Time 17.2918(17.4772) | Bit/dim 4.3957(4.4493) | Xent 1.6084(1.6316) | Loss 37.1990(38.7841) | Error 0.5956(0.5876) Steps 0(0.00) | Grad Norm 33.8000(56.5202) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0017 | Time 91.4675, Epoch Time 1072.2161(875.4332), Bit/dim 4.3964(best: 4.4485), Xent 1.5331, Loss 5.1630, Error 0.5545(best: 0.5482)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0940 | Time 18.5477(17.3991) | Bit/dim 4.4191(4.4335) | Xent 1.4945(1.6186) | Loss 37.2807(41.5412) | Error 0.5467(0.5856) Steps 0(0.00) | Grad Norm 44.2717(53.8196) | Total Time 0.00(0.00)\n",
      "Iter 0950 | Time 18.0364(17.3630) | Bit/dim 4.3494(4.4162) | Xent 1.4790(1.5974) | Loss 37.4858(40.2980) | Error 0.5111(0.5767) Steps 0(0.00) | Grad Norm 56.3046(49.9880) | Total Time 0.00(0.00)\n",
      "Iter 0960 | Time 16.6638(17.2734) | Bit/dim 4.4444(4.4163) | Xent 1.6385(1.6095) | Loss 36.0135(39.4145) | Error 0.5767(0.5804) Steps 0(0.00) | Grad Norm 51.6833(60.2113) | Total Time 0.00(0.00)\n",
      "Iter 0970 | Time 16.5117(17.4980) | Bit/dim 4.3452(4.4060) | Xent 1.6351(1.6029) | Loss 37.1789(38.9110) | Error 0.5867(0.5795) Steps 0(0.00) | Grad Norm 36.5321(54.1796) | Total Time 0.00(0.00)\n",
      "Iter 0980 | Time 16.9813(17.5052) | Bit/dim 4.3525(4.3900) | Xent 1.5843(1.5901) | Loss 36.8657(38.4016) | Error 0.5700(0.5768) Steps 0(0.00) | Grad Norm 41.3324(47.2426) | Total Time 0.00(0.00)\n",
      "Iter 0990 | Time 20.5799(17.5245) | Bit/dim 4.3287(4.3765) | Xent 1.6470(1.5885) | Loss 37.5973(38.0211) | Error 0.6111(0.5758) Steps 0(0.00) | Grad Norm 109.9843(47.0038) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0018 | Time 95.9061, Epoch Time 1075.2955(881.4291), Bit/dim 4.4712(best: 4.3964), Xent 1.6214, Loss 5.2819, Error 0.5785(best: 0.5482)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1000 | Time 17.3556(17.5553) | Bit/dim 4.3118(4.3692) | Xent 1.5199(1.5876) | Loss 37.8395(40.7478) | Error 0.5700(0.5746) Steps 0(0.00) | Grad Norm 41.8697(54.3001) | Total Time 0.00(0.00)\n",
      "Iter 1010 | Time 16.6515(17.4983) | Bit/dim 4.3303(4.3581) | Xent 1.5542(1.5822) | Loss 36.2392(39.6762) | Error 0.5789(0.5727) Steps 0(0.00) | Grad Norm 34.2406(53.2761) | Total Time 0.00(0.00)\n",
      "Iter 1020 | Time 18.2941(17.4914) | Bit/dim 4.3264(4.3494) | Xent 1.5700(1.5737) | Loss 37.2620(38.9586) | Error 0.5733(0.5700) Steps 0(0.00) | Grad Norm 87.0289(54.9533) | Total Time 0.00(0.00)\n",
      "Iter 1030 | Time 16.3437(17.3584) | Bit/dim 4.3167(4.3409) | Xent 1.5809(1.5658) | Loss 36.4457(38.3946) | Error 0.5700(0.5688) Steps 0(0.00) | Grad Norm 70.3285(56.4656) | Total Time 0.00(0.00)\n",
      "Iter 1040 | Time 17.0113(17.3920) | Bit/dim 4.3012(4.3304) | Xent 1.5495(1.5607) | Loss 35.8777(37.8887) | Error 0.5756(0.5687) Steps 0(0.00) | Grad Norm 23.1065(55.6918) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0019 | Time 92.0857, Epoch Time 1071.7660(887.1392), Bit/dim 4.2718(best: 4.3964), Xent 1.4626, Loss 5.0031, Error 0.5266(best: 0.5482)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1050 | Time 17.8871(17.6136) | Bit/dim 4.2396(4.3154) | Xent 1.4817(1.5454) | Loss 35.9623(40.6020) | Error 0.5433(0.5629) Steps 0(0.00) | Grad Norm 21.7318(50.9302) | Total Time 0.00(0.00)\n",
      "Iter 1060 | Time 18.2128(17.7882) | Bit/dim 4.2470(4.3018) | Xent 1.5327(1.5402) | Loss 37.0226(39.5408) | Error 0.5667(0.5600) Steps 0(0.00) | Grad Norm 44.5435(54.2537) | Total Time 0.00(0.00)\n",
      "Iter 1070 | Time 17.5144(17.7881) | Bit/dim 4.2393(4.2875) | Xent 1.4631(1.5306) | Loss 36.0180(38.6929) | Error 0.5156(0.5548) Steps 0(0.00) | Grad Norm 44.5221(51.4005) | Total Time 0.00(0.00)\n",
      "Iter 1080 | Time 18.9769(17.7138) | Bit/dim 4.2525(4.2914) | Xent 1.5739(1.5359) | Loss 36.8281(38.1776) | Error 0.5767(0.5566) Steps 0(0.00) | Grad Norm 40.0146(56.3362) | Total Time 0.00(0.00)\n",
      "Iter 1090 | Time 18.6156(17.6227) | Bit/dim 4.2274(4.2871) | Xent 1.5392(1.5381) | Loss 36.6426(37.6494) | Error 0.5622(0.5579) Steps 0(0.00) | Grad Norm 24.5624(53.6969) | Total Time 0.00(0.00)\n",
      "Iter 1100 | Time 17.3730(17.7227) | Bit/dim 4.2309(4.2747) | Xent 1.5180(1.5281) | Loss 35.8269(37.1857) | Error 0.5456(0.5556) Steps 0(0.00) | Grad Norm 31.2264(48.8540) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0020 | Time 89.8537, Epoch Time 1087.2512(893.1426), Bit/dim 4.2221(best: 4.2718), Xent 1.3981, Loss 4.9212, Error 0.5072(best: 0.5266)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1110 | Time 17.3982(17.8556) | Bit/dim 4.2390(4.2590) | Xent 1.4640(1.5107) | Loss 35.4807(39.6175) | Error 0.5378(0.5500) Steps 0(0.00) | Grad Norm 36.5157(43.5025) | Total Time 0.00(0.00)\n",
      "Iter 1120 | Time 19.7194(17.8038) | Bit/dim 4.2060(4.2428) | Xent 1.3460(1.4929) | Loss 35.4151(38.5727) | Error 0.4967(0.5427) Steps 0(0.00) | Grad Norm 45.1305(41.4898) | Total Time 0.00(0.00)\n",
      "Iter 1130 | Time 19.7459(17.8343) | Bit/dim 4.1900(4.2342) | Xent 1.4318(1.4770) | Loss 35.0929(37.8133) | Error 0.5278(0.5386) Steps 0(0.00) | Grad Norm 16.7104(44.6392) | Total Time 0.00(0.00)\n",
      "Iter 1140 | Time 18.0149(17.8883) | Bit/dim 4.2098(4.2248) | Xent 1.4894(1.4680) | Loss 35.0909(37.2716) | Error 0.5444(0.5359) Steps 0(0.00) | Grad Norm 89.1915(42.9482) | Total Time 0.00(0.00)\n",
      "Iter 1150 | Time 16.9816(17.8028) | Bit/dim 4.2086(4.2155) | Xent 1.7000(1.4804) | Loss 36.5083(36.7278) | Error 0.5844(0.5397) Steps 0(0.00) | Grad Norm 163.7697(50.8158) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0021 | Time 90.6016, Epoch Time 1091.9585(899.1071), Bit/dim 4.2405(best: 4.2221), Xent 1.7035, Loss 5.0923, Error 0.5927(best: 0.5072)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1160 | Time 18.7106(17.9402) | Bit/dim 4.2195(4.2349) | Xent 1.5981(1.5483) | Loss 36.1798(40.1478) | Error 0.5700(0.5579) Steps 0(0.00) | Grad Norm 62.0186(59.9652) | Total Time 0.00(0.00)\n",
      "Iter 1170 | Time 17.4420(17.8702) | Bit/dim 4.1940(4.2342) | Xent 1.4866(1.5403) | Loss 35.9038(39.0991) | Error 0.5489(0.5553) Steps 0(0.00) | Grad Norm 43.4832(53.7367) | Total Time 0.00(0.00)\n",
      "Iter 1180 | Time 16.4345(17.7888) | Bit/dim 4.1800(4.2247) | Xent 1.4331(1.5250) | Loss 33.9017(38.1582) | Error 0.5122(0.5500) Steps 0(0.00) | Grad Norm 26.4592(47.7683) | Total Time 0.00(0.00)\n",
      "Iter 1190 | Time 19.7341(17.7763) | Bit/dim 4.1625(4.2062) | Xent 1.6038(1.5046) | Loss 35.5590(37.3597) | Error 0.5722(0.5445) Steps 0(0.00) | Grad Norm 65.0477(44.0734) | Total Time 0.00(0.00)\n",
      "Iter 1200 | Time 16.7191(17.6607) | Bit/dim 4.1699(4.1910) | Xent 1.4725(1.4914) | Loss 35.4087(36.8735) | Error 0.5533(0.5407) Steps 0(0.00) | Grad Norm 51.8919(43.5696) | Total Time 0.00(0.00)\n",
      "Iter 1210 | Time 17.2745(17.6019) | Bit/dim 4.1380(4.1772) | Xent 1.3592(1.4766) | Loss 34.8070(36.3459) | Error 0.5133(0.5371) Steps 0(0.00) | Grad Norm 38.2478(42.4007) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0022 | Time 89.7029, Epoch Time 1077.6610(904.4637), Bit/dim 4.1400(best: 4.2221), Xent 1.3389, Loss 4.8094, Error 0.4807(best: 0.5072)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1220 | Time 16.2472(17.6944) | Bit/dim 4.1106(4.1618) | Xent 1.4149(1.4525) | Loss 35.1918(38.7633) | Error 0.5122(0.5282) Steps 0(0.00) | Grad Norm 25.6702(38.5924) | Total Time 0.00(0.00)\n",
      "Iter 1230 | Time 17.8904(17.5990) | Bit/dim 4.1650(4.1557) | Xent 1.4163(1.4422) | Loss 34.2371(37.7013) | Error 0.5156(0.5237) Steps 0(0.00) | Grad Norm 62.0674(41.8194) | Total Time 0.00(0.00)\n",
      "Iter 1240 | Time 17.8550(17.6209) | Bit/dim 4.1241(4.1464) | Xent 1.2969(1.4272) | Loss 34.6088(36.8296) | Error 0.4700(0.5186) Steps 0(0.00) | Grad Norm 23.8888(41.0541) | Total Time 0.00(0.00)\n",
      "Iter 1250 | Time 20.1897(17.7002) | Bit/dim 4.1087(4.1400) | Xent 1.3163(1.4123) | Loss 35.2730(36.3769) | Error 0.4800(0.5129) Steps 0(0.00) | Grad Norm 28.7605(37.4548) | Total Time 0.00(0.00)\n",
      "Iter 1260 | Time 16.2182(17.6209) | Bit/dim 4.1145(4.1306) | Xent 1.3800(1.4029) | Loss 34.0724(35.8828) | Error 0.5033(0.5101) Steps 0(0.00) | Grad Norm 28.2337(37.0574) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0023 | Time 89.3166, Epoch Time 1078.4652(909.6837), Bit/dim 4.1236(best: 4.1400), Xent 1.3322, Loss 4.7897, Error 0.4829(best: 0.4807)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1270 | Time 19.3860(17.6548) | Bit/dim 4.1020(4.1257) | Xent 1.3990(1.3952) | Loss 35.8927(38.9234) | Error 0.5311(0.5065) Steps 0(0.00) | Grad Norm 54.4868(39.8337) | Total Time 0.00(0.00)\n",
      "Iter 1280 | Time 16.6203(17.5336) | Bit/dim 4.1193(4.1221) | Xent 1.4214(1.3946) | Loss 33.8689(37.8686) | Error 0.5311(0.5073) Steps 0(0.00) | Grad Norm 29.0913(41.8604) | Total Time 0.00(0.00)\n",
      "Iter 1290 | Time 17.8754(17.4058) | Bit/dim 4.0886(4.1158) | Xent 1.3640(1.3855) | Loss 35.0960(37.1419) | Error 0.4900(0.5037) Steps 0(0.00) | Grad Norm 28.2065(40.6243) | Total Time 0.00(0.00)\n",
      "Iter 1300 | Time 16.5679(17.2965) | Bit/dim 4.0905(4.1064) | Xent 1.3679(1.3852) | Loss 34.3221(36.4484) | Error 0.5167(0.5037) Steps 0(0.00) | Grad Norm 27.9956(39.9195) | Total Time 0.00(0.00)\n",
      "Iter 1310 | Time 17.9155(17.5498) | Bit/dim 4.1044(4.1032) | Xent 1.3703(1.3706) | Loss 36.0233(35.9861) | Error 0.4944(0.4990) Steps 0(0.00) | Grad Norm 65.1519(40.1788) | Total Time 0.00(0.00)\n",
      "Iter 1320 | Time 16.9257(17.4927) | Bit/dim 4.0801(4.0939) | Xent 1.3458(1.3635) | Loss 35.1618(35.5279) | Error 0.4689(0.4944) Steps 0(0.00) | Grad Norm 39.8800(37.7298) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0024 | Time 91.3277, Epoch Time 1066.2636(914.3811), Bit/dim 4.0736(best: 4.1236), Xent 1.2689, Loss 4.7080, Error 0.4632(best: 0.4807)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1330 | Time 17.4942(17.5311) | Bit/dim 4.1018(4.0899) | Xent 1.3098(1.3524) | Loss 35.0903(38.2099) | Error 0.4711(0.4902) Steps 0(0.00) | Grad Norm 77.1598(39.4813) | Total Time 0.00(0.00)\n",
      "Iter 1340 | Time 16.5355(17.5492) | Bit/dim 4.0879(4.0862) | Xent 1.4158(1.3518) | Loss 33.3094(37.1925) | Error 0.5067(0.4896) Steps 0(0.00) | Grad Norm 51.1602(43.7149) | Total Time 0.00(0.00)\n",
      "Iter 1350 | Time 17.9901(17.5054) | Bit/dim 4.0937(4.0822) | Xent 1.3349(1.3513) | Loss 34.2195(36.5389) | Error 0.4878(0.4921) Steps 0(0.00) | Grad Norm 35.5290(43.0349) | Total Time 0.00(0.00)\n",
      "Iter 1360 | Time 17.1181(17.5169) | Bit/dim 4.0800(4.0784) | Xent 1.3745(1.3485) | Loss 34.8759(36.0349) | Error 0.4822(0.4893) Steps 0(0.00) | Grad Norm 38.8140(40.8148) | Total Time 0.00(0.00)\n",
      "Iter 1370 | Time 16.9287(17.4267) | Bit/dim 4.0248(4.0684) | Xent 1.3377(1.3430) | Loss 34.1463(35.6068) | Error 0.5044(0.4874) Steps 0(0.00) | Grad Norm 36.3208(38.6162) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0025 | Time 88.7545, Epoch Time 1068.0002(918.9897), Bit/dim 4.0380(best: 4.0736), Xent 1.2510, Loss 4.6635, Error 0.4551(best: 0.4632)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1380 | Time 18.2435(17.4193) | Bit/dim 4.0228(4.0620) | Xent 1.3179(1.3389) | Loss 34.8490(38.4616) | Error 0.4844(0.4856) Steps 0(0.00) | Grad Norm 39.7034(39.7620) | Total Time 0.00(0.00)\n",
      "Iter 1390 | Time 17.4204(17.4596) | Bit/dim 4.0487(4.0528) | Xent 1.3546(1.3316) | Loss 35.4132(37.3465) | Error 0.4956(0.4834) Steps 0(0.00) | Grad Norm 53.9462(39.2596) | Total Time 0.00(0.00)\n",
      "Iter 1400 | Time 16.5245(17.5081) | Bit/dim 4.0521(4.0482) | Xent 1.2791(1.3234) | Loss 34.3046(36.5222) | Error 0.4711(0.4830) Steps 0(0.00) | Grad Norm 34.2097(38.5235) | Total Time 0.00(0.00)\n",
      "Iter 1410 | Time 17.7502(17.5187) | Bit/dim 4.0433(4.0464) | Xent 1.2806(1.3253) | Loss 33.8790(35.9171) | Error 0.4811(0.4822) Steps 0(0.00) | Grad Norm 24.0842(39.8420) | Total Time 0.00(0.00)\n",
      "Iter 1420 | Time 16.8155(17.6857) | Bit/dim 4.0318(4.0429) | Xent 1.2935(1.3166) | Loss 34.8040(35.5201) | Error 0.4622(0.4791) Steps 0(0.00) | Grad Norm 31.1009(40.7482) | Total Time 0.00(0.00)\n",
      "Iter 1430 | Time 18.7894(17.6958) | Bit/dim 3.9999(4.0417) | Xent 1.3179(1.3184) | Loss 34.6810(35.2308) | Error 0.5022(0.4792) Steps 0(0.00) | Grad Norm 34.0775(40.0281) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0026 | Time 89.7286, Epoch Time 1080.7184(923.8415), Bit/dim 4.0310(best: 4.0380), Xent 1.2669, Loss 4.6644, Error 0.4622(best: 0.4551)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1440 | Time 16.4298(17.6478) | Bit/dim 4.0414(4.0355) | Xent 1.2321(1.3107) | Loss 33.6013(37.6128) | Error 0.4489(0.4751) Steps 0(0.00) | Grad Norm 35.6238(38.0901) | Total Time 0.00(0.00)\n",
      "Iter 1450 | Time 16.5480(17.5555) | Bit/dim 4.0092(4.0316) | Xent 1.2630(1.2991) | Loss 32.8685(36.5791) | Error 0.4733(0.4712) Steps 0(0.00) | Grad Norm 55.8360(37.0876) | Total Time 0.00(0.00)\n",
      "Iter 1460 | Time 17.9316(17.7724) | Bit/dim 3.9539(4.0231) | Xent 1.2745(1.2922) | Loss 32.9710(35.8770) | Error 0.4611(0.4694) Steps 0(0.00) | Grad Norm 28.2105(35.6044) | Total Time 0.00(0.00)\n",
      "Iter 1470 | Time 18.5563(17.6686) | Bit/dim 4.0066(4.0198) | Xent 1.2687(1.2862) | Loss 35.3752(35.4844) | Error 0.4611(0.4687) Steps 0(0.00) | Grad Norm 24.5784(35.5610) | Total Time 0.00(0.00)\n",
      "Iter 1480 | Time 16.8962(17.5597) | Bit/dim 4.0026(4.0169) | Xent 1.3701(1.2880) | Loss 31.9771(35.0098) | Error 0.4889(0.4680) Steps 0(0.00) | Grad Norm 53.7380(35.8184) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0027 | Time 89.0955, Epoch Time 1073.3524(928.3269), Bit/dim 4.0048(best: 4.0310), Xent 1.2057, Loss 4.6077, Error 0.4362(best: 0.4551)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1490 | Time 17.6866(17.5725) | Bit/dim 3.9727(4.0128) | Xent 1.4675(1.2873) | Loss 34.5494(37.9155) | Error 0.5200(0.4665) Steps 0(0.00) | Grad Norm 81.6007(38.1808) | Total Time 0.00(0.00)\n",
      "Iter 1500 | Time 17.3399(17.5491) | Bit/dim 4.0349(4.0190) | Xent 1.3564(1.3005) | Loss 33.7408(36.9498) | Error 0.4844(0.4683) Steps 0(0.00) | Grad Norm 53.4438(44.9179) | Total Time 0.00(0.00)\n",
      "Iter 1510 | Time 17.7552(17.5953) | Bit/dim 3.9876(4.0148) | Xent 1.2897(1.2996) | Loss 34.0398(36.2213) | Error 0.4511(0.4690) Steps 0(0.00) | Grad Norm 31.0217(44.4964) | Total Time 0.00(0.00)\n",
      "Iter 1520 | Time 17.8642(17.5842) | Bit/dim 4.0085(4.0109) | Xent 1.2764(1.2927) | Loss 33.7276(35.5988) | Error 0.4722(0.4680) Steps 0(0.00) | Grad Norm 31.5544(40.7968) | Total Time 0.00(0.00)\n",
      "Iter 1530 | Time 17.8373(17.6681) | Bit/dim 3.9998(4.0046) | Xent 1.1842(1.2770) | Loss 34.5975(35.1162) | Error 0.4267(0.4635) Steps 0(0.00) | Grad Norm 12.9386(39.5016) | Total Time 0.00(0.00)\n",
      "Iter 1540 | Time 16.5591(17.5629) | Bit/dim 4.0009(3.9999) | Xent 1.2638(1.2690) | Loss 32.3405(34.6313) | Error 0.4533(0.4592) Steps 0(0.00) | Grad Norm 33.1539(36.3742) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0028 | Time 88.6640, Epoch Time 1074.3824(932.7085), Bit/dim 3.9788(best: 4.0048), Xent 1.1917, Loss 4.5746, Error 0.4293(best: 0.4362)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1550 | Time 18.0340(17.6096) | Bit/dim 3.9873(3.9948) | Xent 1.1907(1.2574) | Loss 33.9014(37.2596) | Error 0.4333(0.4540) Steps 0(0.00) | Grad Norm 28.6489(34.4451) | Total Time 0.00(0.00)\n",
      "Iter 1560 | Time 17.4969(17.6261) | Bit/dim 3.9479(3.9882) | Xent 1.2082(1.2548) | Loss 32.2318(36.2057) | Error 0.4278(0.4537) Steps 0(0.00) | Grad Norm 36.6698(35.0820) | Total Time 0.00(0.00)\n",
      "Iter 1570 | Time 17.5819(17.5771) | Bit/dim 3.9631(3.9830) | Xent 1.1496(1.2385) | Loss 33.5390(35.5193) | Error 0.4033(0.4467) Steps 0(0.00) | Grad Norm 31.5564(31.6941) | Total Time 0.00(0.00)\n",
      "Iter 1580 | Time 17.8512(17.5544) | Bit/dim 3.9527(3.9773) | Xent 1.2890(1.2406) | Loss 33.7104(35.0623) | Error 0.4533(0.4467) Steps 0(0.00) | Grad Norm 33.8376(33.5942) | Total Time 0.00(0.00)\n",
      "Iter 1590 | Time 16.6758(17.4505) | Bit/dim 3.9936(3.9733) | Xent 1.1740(1.2360) | Loss 33.0442(34.5819) | Error 0.4189(0.4454) Steps 0(0.00) | Grad Norm 15.6754(33.4761) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0029 | Time 88.0444, Epoch Time 1066.8660(936.7333), Bit/dim 3.9611(best: 3.9788), Xent 1.1496, Loss 4.5359, Error 0.4214(best: 0.4293)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1600 | Time 16.6534(17.3470) | Bit/dim 3.9702(3.9710) | Xent 1.2035(1.2263) | Loss 32.7383(37.4248) | Error 0.4244(0.4410) Steps 0(0.00) | Grad Norm 32.4635(32.1432) | Total Time 0.00(0.00)\n",
      "Iter 1610 | Time 15.9163(17.2737) | Bit/dim 3.9546(3.9662) | Xent 1.1134(1.2102) | Loss 31.4184(36.2127) | Error 0.4022(0.4360) Steps 0(0.00) | Grad Norm 18.9231(30.4206) | Total Time 0.00(0.00)\n",
      "Iter 1620 | Time 17.5172(17.2288) | Bit/dim 3.9642(3.9645) | Xent 1.2609(1.2133) | Loss 33.3498(35.4570) | Error 0.4356(0.4370) Steps 0(0.00) | Grad Norm 35.7310(32.5202) | Total Time 0.00(0.00)\n",
      "Iter 1630 | Time 16.1196(17.2323) | Bit/dim 3.9531(3.9637) | Xent 1.2798(1.2218) | Loss 32.8010(34.9307) | Error 0.4589(0.4389) Steps 0(0.00) | Grad Norm 76.7447(38.8340) | Total Time 0.00(0.00)\n",
      "Iter 1640 | Time 17.2670(17.1417) | Bit/dim 3.9774(3.9633) | Xent 1.1934(1.2197) | Loss 32.7759(34.4901) | Error 0.4356(0.4372) Steps 0(0.00) | Grad Norm 29.9426(37.4442) | Total Time 0.00(0.00)\n",
      "Iter 1650 | Time 17.4190(17.1968) | Bit/dim 3.9314(3.9586) | Xent 1.1891(1.2246) | Loss 33.2677(34.2220) | Error 0.4311(0.4386) Steps 0(0.00) | Grad Norm 41.2112(40.2245) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0030 | Time 87.7038, Epoch Time 1046.0811(940.0137), Bit/dim 3.9635(best: 3.9611), Xent 1.1438, Loss 4.5355, Error 0.4111(best: 0.4214)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1660 | Time 17.5039(17.1823) | Bit/dim 3.9435(3.9557) | Xent 1.2228(1.2190) | Loss 32.9850(36.6127) | Error 0.4444(0.4366) Steps 0(0.00) | Grad Norm 45.7653(39.7102) | Total Time 0.00(0.00)\n",
      "Iter 1670 | Time 16.5951(17.1885) | Bit/dim 3.9238(3.9535) | Xent 1.1451(1.2039) | Loss 32.9567(35.5628) | Error 0.4189(0.4320) Steps 0(0.00) | Grad Norm 29.0135(36.3401) | Total Time 0.00(0.00)\n",
      "Iter 1680 | Time 21.0027(17.3200) | Bit/dim 3.9234(3.9497) | Xent 1.1551(1.1909) | Loss 32.3208(34.9426) | Error 0.4178(0.4267) Steps 0(0.00) | Grad Norm 23.1428(33.3538) | Total Time 0.00(0.00)\n",
      "Iter 1690 | Time 15.7343(17.2017) | Bit/dim 3.9751(3.9498) | Xent 1.1347(1.1862) | Loss 33.4776(34.4784) | Error 0.4167(0.4244) Steps 0(0.00) | Grad Norm 52.0801(35.8064) | Total Time 0.00(0.00)\n",
      "Iter 1700 | Time 17.0730(17.0787) | Bit/dim 3.9196(3.9446) | Xent 1.2153(1.1875) | Loss 32.6153(34.0286) | Error 0.4256(0.4254) Steps 0(0.00) | Grad Norm 34.5773(34.7428) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0031 | Time 88.5246, Epoch Time 1051.9504(943.3718), Bit/dim 3.9315(best: 3.9611), Xent 1.0908, Loss 4.4768, Error 0.3899(best: 0.4111)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1710 | Time 17.4773(17.1399) | Bit/dim 3.9356(3.9391) | Xent 1.1060(1.1796) | Loss 33.4268(36.9761) | Error 0.4033(0.4228) Steps 0(0.00) | Grad Norm 32.2804(34.8797) | Total Time 0.00(0.00)\n",
      "Iter 1720 | Time 16.7460(17.2150) | Bit/dim 3.9480(3.9383) | Xent 1.1431(1.1631) | Loss 33.0299(35.9294) | Error 0.4167(0.4155) Steps 0(0.00) | Grad Norm 30.1817(32.6299) | Total Time 0.00(0.00)\n",
      "Iter 1730 | Time 17.3172(17.2416) | Bit/dim 3.9300(3.9360) | Xent 1.1510(1.1649) | Loss 33.7257(35.1968) | Error 0.4100(0.4169) Steps 0(0.00) | Grad Norm 29.0951(33.4336) | Total Time 0.00(0.00)\n",
      "Iter 1740 | Time 16.7555(17.3010) | Bit/dim 3.9692(3.9346) | Xent 1.2250(1.1771) | Loss 32.9555(34.7074) | Error 0.4422(0.4223) Steps 0(0.00) | Grad Norm 40.0385(37.4624) | Total Time 0.00(0.00)\n",
      "Iter 1750 | Time 17.4719(17.3634) | Bit/dim 3.9657(3.9316) | Xent 1.1160(1.1719) | Loss 33.4204(34.2977) | Error 0.3989(0.4202) Steps 0(0.00) | Grad Norm 35.0409(35.8596) | Total Time 0.00(0.00)\n",
      "Iter 1760 | Time 16.4310(17.4442) | Bit/dim 3.9022(3.9266) | Xent 1.1248(1.1715) | Loss 31.8938(33.9830) | Error 0.4178(0.4208) Steps 0(0.00) | Grad Norm 25.0344(36.6447) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0032 | Time 87.5769, Epoch Time 1063.4412(946.9739), Bit/dim 3.9115(best: 3.9315), Xent 1.0892, Loss 4.4561, Error 0.3892(best: 0.3899)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1770 | Time 17.0606(17.4700) | Bit/dim 3.9008(3.9241) | Xent 1.1030(1.1621) | Loss 32.5950(36.5331) | Error 0.4056(0.4178) Steps 0(0.00) | Grad Norm 38.8205(34.5189) | Total Time 0.00(0.00)\n",
      "Iter 1780 | Time 16.6522(17.3186) | Bit/dim 3.9377(3.9225) | Xent 1.1697(1.1574) | Loss 32.7706(35.6044) | Error 0.4011(0.4149) Steps 0(0.00) | Grad Norm 36.2566(33.5432) | Total Time 0.00(0.00)\n",
      "Iter 1790 | Time 17.4287(17.4138) | Bit/dim 3.9207(3.9188) | Xent 1.2018(1.1607) | Loss 32.3312(34.8450) | Error 0.4389(0.4152) Steps 0(0.00) | Grad Norm 31.9281(37.1681) | Total Time 0.00(0.00)\n",
      "Iter 1800 | Time 17.0486(17.4854) | Bit/dim 3.9120(3.9186) | Xent 1.1480(1.1543) | Loss 32.3049(34.3769) | Error 0.4111(0.4133) Steps 0(0.00) | Grad Norm 41.8516(36.4660) | Total Time 0.00(0.00)\n",
      "Iter 1810 | Time 17.2348(17.3441) | Bit/dim 3.9149(3.9155) | Xent 1.1194(1.1464) | Loss 33.2653(33.8279) | Error 0.3767(0.4103) Steps 0(0.00) | Grad Norm 35.4001(35.4918) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0033 | Time 89.3693, Epoch Time 1069.4539(950.6483), Bit/dim 3.9065(best: 3.9115), Xent 1.0815, Loss 4.4473, Error 0.3879(best: 0.3892)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1820 | Time 17.0105(17.5426) | Bit/dim 3.9276(3.9154) | Xent 1.1180(1.1454) | Loss 33.5938(36.8087) | Error 0.3833(0.4112) Steps 0(0.00) | Grad Norm 44.9348(36.4526) | Total Time 0.00(0.00)\n",
      "Iter 1830 | Time 17.0701(17.4813) | Bit/dim 3.8637(3.9118) | Xent 1.1021(1.1347) | Loss 32.2760(35.7362) | Error 0.3756(0.4072) Steps 0(0.00) | Grad Norm 30.7341(34.4281) | Total Time 0.00(0.00)\n",
      "Iter 1840 | Time 17.6580(17.4592) | Bit/dim 3.8722(3.9079) | Xent 1.2507(1.1307) | Loss 30.6315(34.8670) | Error 0.4322(0.4052) Steps 0(0.00) | Grad Norm 53.7393(35.3857) | Total Time 0.00(0.00)\n",
      "Iter 1850 | Time 16.7851(17.3517) | Bit/dim 3.8894(3.9089) | Xent 1.1559(1.1312) | Loss 33.6087(34.3560) | Error 0.4078(0.4063) Steps 0(0.00) | Grad Norm 41.7622(35.2189) | Total Time 0.00(0.00)\n",
      "Iter 1860 | Time 16.6362(17.2084) | Bit/dim 3.8807(3.9041) | Xent 1.0626(1.1311) | Loss 31.8389(33.8503) | Error 0.3822(0.4060) Steps 0(0.00) | Grad Norm 43.6039(36.9415) | Total Time 0.00(0.00)\n",
      "Iter 1870 | Time 16.0315(17.1687) | Bit/dim 3.9024(3.9035) | Xent 1.1055(1.1316) | Loss 32.0502(33.6359) | Error 0.3878(0.4052) Steps 0(0.00) | Grad Norm 26.4284(35.1436) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0034 | Time 89.1676, Epoch Time 1049.5025(953.6139), Bit/dim 3.8903(best: 3.9065), Xent 1.0604, Loss 4.4205, Error 0.3797(best: 0.3879)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1880 | Time 17.0915(17.2398) | Bit/dim 3.9139(3.9020) | Xent 1.0597(1.1204) | Loss 32.7697(36.1617) | Error 0.3833(0.3996) Steps 0(0.00) | Grad Norm 45.8764(34.2401) | Total Time 0.00(0.00)\n",
      "Iter 1890 | Time 17.1795(17.2746) | Bit/dim 3.8864(3.8996) | Xent 1.0773(1.1109) | Loss 31.9185(35.1863) | Error 0.3878(0.3957) Steps 0(0.00) | Grad Norm 45.2495(33.6980) | Total Time 0.00(0.00)\n",
      "Iter 1900 | Time 16.8212(17.3256) | Bit/dim 3.8626(3.8953) | Xent 1.0970(1.1040) | Loss 31.0558(34.4703) | Error 0.3844(0.3937) Steps 0(0.00) | Grad Norm 25.4957(32.8724) | Total Time 0.00(0.00)\n",
      "Iter 1910 | Time 17.5310(17.2067) | Bit/dim 3.8772(3.8923) | Xent 1.1500(1.1066) | Loss 33.4612(34.0119) | Error 0.4133(0.3963) Steps 0(0.00) | Grad Norm 28.6266(32.4481) | Total Time 0.00(0.00)\n",
      "Iter 1920 | Time 17.1176(17.1995) | Bit/dim 3.8938(3.8886) | Xent 1.1599(1.1070) | Loss 32.8019(33.6519) | Error 0.4189(0.3960) Steps 0(0.00) | Grad Norm 52.6107(36.2381) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0035 | Time 86.8854, Epoch Time 1055.9459(956.6839), Bit/dim 3.8983(best: 3.8903), Xent 1.0792, Loss 4.4379, Error 0.3864(best: 0.3797)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1930 | Time 17.3998(17.3254) | Bit/dim 3.8888(3.8925) | Xent 1.1564(1.1108) | Loss 33.0078(36.7143) | Error 0.4322(0.3966) Steps 0(0.00) | Grad Norm 37.0740(38.0658) | Total Time 0.00(0.00)\n",
      "Iter 1940 | Time 18.1255(17.3607) | Bit/dim 3.8729(3.8880) | Xent 1.0584(1.1056) | Loss 31.3782(35.5643) | Error 0.3756(0.3963) Steps 0(0.00) | Grad Norm 19.3190(35.9225) | Total Time 0.00(0.00)\n",
      "Iter 1950 | Time 18.2633(17.3944) | Bit/dim 3.9129(3.8887) | Xent 1.0945(1.0988) | Loss 33.5740(34.7422) | Error 0.3867(0.3948) Steps 0(0.00) | Grad Norm 43.6378(35.8815) | Total Time 0.00(0.00)\n",
      "Iter 1960 | Time 18.5933(17.4814) | Bit/dim 3.8750(3.8856) | Xent 1.0860(1.0986) | Loss 32.3011(34.1218) | Error 0.3744(0.3931) Steps 0(0.00) | Grad Norm 45.0774(35.2844) | Total Time 0.00(0.00)\n",
      "Iter 1970 | Time 17.5737(17.4071) | Bit/dim 3.8650(3.8798) | Xent 1.0881(1.0994) | Loss 31.9287(33.6621) | Error 0.3789(0.3936) Steps 0(0.00) | Grad Norm 28.3884(34.8302) | Total Time 0.00(0.00)\n",
      "Iter 1980 | Time 16.0925(17.2947) | Bit/dim 3.8740(3.8803) | Xent 1.0777(1.0896) | Loss 32.7701(33.2123) | Error 0.3867(0.3903) Steps 0(0.00) | Grad Norm 48.9932(33.9907) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0036 | Time 89.3746, Epoch Time 1063.7084(959.8946), Bit/dim 3.8762(best: 3.8903), Xent 1.0594, Loss 4.4059, Error 0.3730(best: 0.3797)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1990 | Time 17.1775(17.2403) | Bit/dim 3.8783(3.8774) | Xent 1.0738(1.0823) | Loss 33.7354(35.8019) | Error 0.3811(0.3868) Steps 0(0.00) | Grad Norm 22.7087(32.3902) | Total Time 0.00(0.00)\n",
      "Iter 2000 | Time 20.2106(17.4508) | Bit/dim 3.8626(3.8762) | Xent 1.0766(1.0713) | Loss 33.1691(34.9873) | Error 0.3833(0.3808) Steps 0(0.00) | Grad Norm 33.4864(30.7243) | Total Time 0.00(0.00)\n",
      "Iter 2010 | Time 17.1450(17.3213) | Bit/dim 3.8880(3.8745) | Xent 1.0329(1.0691) | Loss 32.1636(34.2993) | Error 0.3667(0.3803) Steps 0(0.00) | Grad Norm 50.9372(30.9117) | Total Time 0.00(0.00)\n",
      "Iter 2020 | Time 17.1943(17.4444) | Bit/dim 3.9100(3.8726) | Xent 1.1511(1.0699) | Loss 33.2400(33.7918) | Error 0.4067(0.3802) Steps 0(0.00) | Grad Norm 65.1600(35.1750) | Total Time 0.00(0.00)\n",
      "Iter 2030 | Time 17.0237(17.5625) | Bit/dim 3.9323(3.8729) | Xent 1.1229(1.0795) | Loss 31.4672(33.4208) | Error 0.3844(0.3837) Steps 0(0.00) | Grad Norm 49.3603(38.5842) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0037 | Time 89.5673, Epoch Time 1076.0143(963.3782), Bit/dim 3.8718(best: 3.8762), Xent 1.0389, Loss 4.3912, Error 0.3698(best: 0.3730)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2040 | Time 17.4774(17.6349) | Bit/dim 3.8877(3.8728) | Xent 1.0485(1.0817) | Loss 32.4278(36.4637) | Error 0.3822(0.3861) Steps 0(0.00) | Grad Norm 33.5191(37.7956) | Total Time 0.00(0.00)\n",
      "Iter 2050 | Time 17.9622(17.5667) | Bit/dim 3.8704(3.8711) | Xent 1.0205(1.0742) | Loss 32.6636(35.3621) | Error 0.3644(0.3834) Steps 0(0.00) | Grad Norm 36.0319(36.9045) | Total Time 0.00(0.00)\n",
      "Iter 2060 | Time 16.1956(17.5145) | Bit/dim 3.8561(3.8686) | Xent 1.0854(1.0733) | Loss 32.1847(34.6583) | Error 0.4044(0.3837) Steps 0(0.00) | Grad Norm 22.2397(35.8223) | Total Time 0.00(0.00)\n",
      "Iter 2070 | Time 16.8156(17.4810) | Bit/dim 3.8455(3.8648) | Xent 1.0544(1.0594) | Loss 31.5979(33.9802) | Error 0.3722(0.3784) Steps 0(0.00) | Grad Norm 24.2723(32.8262) | Total Time 0.00(0.00)\n",
      "Iter 2080 | Time 18.2062(17.5335) | Bit/dim 3.8717(3.8656) | Xent 1.0175(1.0543) | Loss 32.1091(33.5955) | Error 0.3578(0.3779) Steps 0(0.00) | Grad Norm 35.5826(32.1653) | Total Time 0.00(0.00)\n",
      "Iter 2090 | Time 18.3171(17.6475) | Bit/dim 3.8073(3.8597) | Xent 1.1659(1.0584) | Loss 32.9570(33.3431) | Error 0.4044(0.3776) Steps 0(0.00) | Grad Norm 46.1615(32.0946) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0038 | Time 88.6788, Epoch Time 1071.9896(966.6365), Bit/dim 3.8528(best: 3.8718), Xent 1.0359, Loss 4.3707, Error 0.3759(best: 0.3698)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2100 | Time 17.5481(17.5207) | Bit/dim 3.8614(3.8587) | Xent 1.0451(1.0575) | Loss 32.5301(35.9539) | Error 0.3700(0.3781) Steps 0(0.00) | Grad Norm 41.6137(34.3786) | Total Time 0.00(0.00)\n",
      "Iter 2110 | Time 19.2085(17.5757) | Bit/dim 3.8530(3.8609) | Xent 1.0843(1.0574) | Loss 32.1465(34.9059) | Error 0.4000(0.3790) Steps 0(0.00) | Grad Norm 22.8867(35.6111) | Total Time 0.00(0.00)\n",
      "Iter 2120 | Time 18.5467(17.5170) | Bit/dim 3.8358(3.8581) | Xent 1.0709(1.0500) | Loss 32.8036(34.2322) | Error 0.3900(0.3775) Steps 0(0.00) | Grad Norm 25.4878(33.2994) | Total Time 0.00(0.00)\n",
      "Iter 2130 | Time 16.7842(17.3909) | Bit/dim 3.8312(3.8554) | Xent 1.0152(1.0520) | Loss 31.6734(33.7616) | Error 0.3833(0.3789) Steps 0(0.00) | Grad Norm 19.7626(31.7603) | Total Time 0.00(0.00)\n",
      "Iter 2140 | Time 17.0790(17.3337) | Bit/dim 3.8774(3.8536) | Xent 1.0725(1.0582) | Loss 31.5258(33.3744) | Error 0.3911(0.3801) Steps 0(0.00) | Grad Norm 42.4934(33.2309) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0039 | Time 89.1227, Epoch Time 1062.3289(969.5073), Bit/dim 3.8501(best: 3.8528), Xent 0.9761, Loss 4.3382, Error 0.3495(best: 0.3698)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2150 | Time 17.4734(17.4182) | Bit/dim 3.8375(3.8513) | Xent 1.0456(1.0555) | Loss 32.1364(36.4006) | Error 0.3644(0.3790) Steps 0(0.00) | Grad Norm 20.9059(32.2308) | Total Time 0.00(0.00)\n",
      "Iter 2160 | Time 17.1118(17.3740) | Bit/dim 3.8569(3.8515) | Xent 1.0297(1.0440) | Loss 32.4814(35.4016) | Error 0.3667(0.3749) Steps 0(0.00) | Grad Norm 40.4187(30.9127) | Total Time 0.00(0.00)\n",
      "Iter 2170 | Time 16.6330(17.4668) | Bit/dim 3.8471(3.8557) | Xent 1.0415(1.0426) | Loss 32.2951(34.5855) | Error 0.3689(0.3740) Steps 0(0.00) | Grad Norm 40.4716(33.0083) | Total Time 0.00(0.00)\n",
      "Iter 2180 | Time 16.3245(17.4036) | Bit/dim 3.8221(3.8537) | Xent 1.0235(1.0409) | Loss 32.0862(33.9422) | Error 0.3589(0.3735) Steps 0(0.00) | Grad Norm 30.4797(32.7241) | Total Time 0.00(0.00)\n",
      "Iter 2190 | Time 18.1808(17.4956) | Bit/dim 3.8109(3.8492) | Xent 1.0281(1.0400) | Loss 32.8666(33.5836) | Error 0.3622(0.3715) Steps 0(0.00) | Grad Norm 19.3361(33.4219) | Total Time 0.00(0.00)\n",
      "Iter 2200 | Time 17.4101(17.5014) | Bit/dim 3.8253(3.8456) | Xent 0.9822(1.0378) | Loss 31.6282(33.2073) | Error 0.3478(0.3693) Steps 0(0.00) | Grad Norm 19.2878(31.7601) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0040 | Time 88.8344, Epoch Time 1068.0656(972.4641), Bit/dim 3.8433(best: 3.8501), Xent 0.9723, Loss 4.3295, Error 0.3473(best: 0.3495)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2210 | Time 16.5256(17.3848) | Bit/dim 3.8464(3.8418) | Xent 0.9761(1.0224) | Loss 31.5810(35.5151) | Error 0.3478(0.3641) Steps 0(0.00) | Grad Norm 27.0393(30.4250) | Total Time 0.00(0.00)\n",
      "Iter 2220 | Time 16.7786(17.4696) | Bit/dim 3.8309(3.8390) | Xent 1.0980(1.0215) | Loss 31.2549(34.5637) | Error 0.4000(0.3644) Steps 0(0.00) | Grad Norm 31.9549(31.7190) | Total Time 0.00(0.00)\n",
      "Iter 2230 | Time 17.5381(17.4104) | Bit/dim 3.8475(3.8404) | Xent 1.0339(1.0225) | Loss 32.3520(33.9849) | Error 0.3678(0.3628) Steps 0(0.00) | Grad Norm 36.5191(34.3255) | Total Time 0.00(0.00)\n",
      "Iter 2240 | Time 17.6214(17.3214) | Bit/dim 3.8195(3.8381) | Xent 1.0073(1.0249) | Loss 30.4521(33.3400) | Error 0.3644(0.3648) Steps 0(0.00) | Grad Norm 27.0179(32.6592) | Total Time 0.00(0.00)\n",
      "Iter 2250 | Time 17.0563(17.2930) | Bit/dim 3.8320(3.8385) | Xent 1.0786(1.0325) | Loss 32.4437(33.1666) | Error 0.3867(0.3678) Steps 0(0.00) | Grad Norm 33.5975(34.2185) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0041 | Time 88.9322, Epoch Time 1056.8556(974.9958), Bit/dim 3.8431(best: 3.8433), Xent 1.1140, Loss 4.4001, Error 0.3890(best: 0.3473)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2260 | Time 16.2699(17.1730) | Bit/dim 3.8403(3.8394) | Xent 1.0565(1.0368) | Loss 31.6626(36.1372) | Error 0.3656(0.3678) Steps 0(0.00) | Grad Norm 56.5851(39.3429) | Total Time 0.00(0.00)\n",
      "Iter 2270 | Time 17.4479(17.3398) | Bit/dim 3.8436(3.8400) | Xent 1.0508(1.0394) | Loss 31.5051(35.1074) | Error 0.3689(0.3674) Steps 0(0.00) | Grad Norm 31.7930(38.9975) | Total Time 0.00(0.00)\n",
      "Iter 2280 | Time 17.6597(17.3395) | Bit/dim 3.8287(3.8395) | Xent 0.9642(1.0288) | Loss 31.9736(34.2756) | Error 0.3489(0.3653) Steps 0(0.00) | Grad Norm 19.0916(34.7296) | Total Time 0.00(0.00)\n",
      "Iter 2290 | Time 18.1874(17.5534) | Bit/dim 3.7969(3.8354) | Xent 1.0643(1.0235) | Loss 31.6737(33.7063) | Error 0.3722(0.3640) Steps 0(0.00) | Grad Norm 33.0656(33.8701) | Total Time 0.00(0.00)\n",
      "Iter 2300 | Time 18.8698(17.5578) | Bit/dim 3.8400(3.8328) | Xent 1.0146(1.0282) | Loss 31.7283(33.2947) | Error 0.3700(0.3668) Steps 0(0.00) | Grad Norm 25.6118(33.8180) | Total Time 0.00(0.00)\n",
      "Iter 2310 | Time 19.6567(17.6385) | Bit/dim 3.8153(3.8300) | Xent 0.9747(1.0154) | Loss 32.4125(33.0262) | Error 0.3533(0.3634) Steps 0(0.00) | Grad Norm 22.0531(30.7955) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0042 | Time 88.6642, Epoch Time 1075.6158(978.0144), Bit/dim 3.8250(best: 3.8431), Xent 0.9869, Loss 4.3185, Error 0.3520(best: 0.3473)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2320 | Time 17.9550(17.6350) | Bit/dim 3.8259(3.8287) | Xent 0.9244(1.0159) | Loss 31.7130(35.5978) | Error 0.3400(0.3643) Steps 0(0.00) | Grad Norm 16.2101(31.1366) | Total Time 0.00(0.00)\n",
      "Iter 2330 | Time 18.5443(17.6091) | Bit/dim 3.8087(3.8277) | Xent 0.9871(1.0084) | Loss 33.2685(34.6399) | Error 0.3578(0.3624) Steps 0(0.00) | Grad Norm 36.1344(32.9005) | Total Time 0.00(0.00)\n",
      "Iter 2340 | Time 17.6610(17.5025) | Bit/dim 3.8436(3.8283) | Xent 1.0297(1.0119) | Loss 32.8229(34.0397) | Error 0.3467(0.3631) Steps 0(0.00) | Grad Norm 57.1479(35.2285) | Total Time 0.00(0.00)\n",
      "Iter 2350 | Time 17.2954(17.4187) | Bit/dim 3.8242(3.8272) | Xent 1.0234(1.0137) | Loss 33.0101(33.5632) | Error 0.3567(0.3636) Steps 0(0.00) | Grad Norm 25.0611(34.4392) | Total Time 0.00(0.00)\n",
      "Iter 2360 | Time 16.8076(17.4838) | Bit/dim 3.7818(3.8252) | Xent 0.9445(1.0057) | Loss 31.5823(33.2373) | Error 0.3356(0.3601) Steps 0(0.00) | Grad Norm 31.4974(33.0735) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0043 | Time 88.0975, Epoch Time 1063.6118(980.5823), Bit/dim 3.8193(best: 3.8250), Xent 0.9681, Loss 4.3034, Error 0.3418(best: 0.3473)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2370 | Time 17.7407(17.4307) | Bit/dim 3.8286(3.8249) | Xent 0.9323(1.0003) | Loss 32.7884(36.1747) | Error 0.3378(0.3583) Steps 0(0.00) | Grad Norm 37.5970(34.0438) | Total Time 0.00(0.00)\n",
      "Iter 2380 | Time 17.4273(17.3261) | Bit/dim 3.8492(3.8267) | Xent 0.9945(0.9997) | Loss 32.9060(35.0812) | Error 0.3489(0.3570) Steps 0(0.00) | Grad Norm 26.3414(34.1820) | Total Time 0.00(0.00)\n",
      "Iter 2390 | Time 17.5251(17.4019) | Bit/dim 3.7929(3.8237) | Xent 1.0070(1.0067) | Loss 31.7802(34.3694) | Error 0.3689(0.3583) Steps 0(0.00) | Grad Norm 29.2030(33.5644) | Total Time 0.00(0.00)\n",
      "Iter 2400 | Time 16.8533(17.3036) | Bit/dim 3.8072(3.8210) | Xent 0.9715(1.0029) | Loss 32.2369(33.7270) | Error 0.3656(0.3573) Steps 0(0.00) | Grad Norm 16.4945(31.4421) | Total Time 0.00(0.00)\n",
      "Iter 2410 | Time 18.6187(17.3427) | Bit/dim 3.7971(3.8168) | Xent 1.0368(0.9967) | Loss 32.5194(33.1411) | Error 0.3733(0.3556) Steps 0(0.00) | Grad Norm 23.9229(30.7740) | Total Time 0.00(0.00)\n",
      "Iter 2420 | Time 16.4767(17.3707) | Bit/dim 3.7969(3.8140) | Xent 0.9107(0.9904) | Loss 32.1175(32.8897) | Error 0.3411(0.3545) Steps 0(0.00) | Grad Norm 13.2167(29.7066) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0044 | Time 89.5300, Epoch Time 1059.3870(982.9465), Bit/dim 3.8158(best: 3.8193), Xent 0.9438, Loss 4.2877, Error 0.3337(best: 0.3418)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2430 | Time 21.2752(17.5733) | Bit/dim 3.8095(3.8137) | Xent 0.9372(0.9841) | Loss 32.6683(35.5471) | Error 0.3344(0.3513) Steps 0(0.00) | Grad Norm 33.2617(30.4708) | Total Time 0.00(0.00)\n",
      "Iter 2440 | Time 19.6379(17.5032) | Bit/dim 3.8135(3.8124) | Xent 0.9803(0.9898) | Loss 32.3077(34.6795) | Error 0.3700(0.3527) Steps 0(0.00) | Grad Norm 24.8131(33.6992) | Total Time 0.00(0.00)\n",
      "Iter 2450 | Time 16.6833(17.4144) | Bit/dim 3.8157(3.8122) | Xent 0.9611(0.9933) | Loss 31.7378(34.0188) | Error 0.3222(0.3543) Steps 0(0.00) | Grad Norm 48.9004(35.3004) | Total Time 0.00(0.00)\n",
      "Iter 2460 | Time 17.4590(17.3970) | Bit/dim 3.8342(3.8140) | Xent 1.0300(0.9957) | Loss 32.9684(33.5889) | Error 0.3678(0.3553) Steps 0(0.00) | Grad Norm 41.3571(34.3341) | Total Time 0.00(0.00)\n",
      "Iter 2470 | Time 19.4676(17.4429) | Bit/dim 3.8161(3.8129) | Xent 1.0164(1.0003) | Loss 32.3519(33.1342) | Error 0.3689(0.3569) Steps 0(0.00) | Grad Norm 48.8099(33.9980) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0045 | Time 89.4900, Epoch Time 1066.0338(985.4391), Bit/dim 3.8201(best: 3.8158), Xent 0.9626, Loss 4.3014, Error 0.3374(best: 0.3337)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2480 | Time 15.5208(17.4735) | Bit/dim 3.7765(3.8124) | Xent 1.0042(1.0025) | Loss 30.1178(36.1614) | Error 0.3500(0.3587) Steps 0(0.00) | Grad Norm 24.7758(34.1708) | Total Time 0.00(0.00)\n",
      "Iter 2490 | Time 17.5227(17.4690) | Bit/dim 3.8311(3.8114) | Xent 1.0479(0.9957) | Loss 32.5551(35.0439) | Error 0.3878(0.3566) Steps 0(0.00) | Grad Norm 39.8504(32.3734) | Total Time 0.00(0.00)\n",
      "Iter 2500 | Time 16.6572(17.4765) | Bit/dim 3.7896(3.8119) | Xent 0.9846(0.9917) | Loss 31.8632(34.2010) | Error 0.3678(0.3559) Steps 0(0.00) | Grad Norm 15.5427(31.8819) | Total Time 0.00(0.00)\n",
      "Iter 2510 | Time 19.6849(17.6054) | Bit/dim 3.8082(3.8125) | Xent 0.9838(0.9816) | Loss 33.3260(33.6756) | Error 0.3500(0.3519) Steps 0(0.00) | Grad Norm 39.9754(30.5305) | Total Time 0.00(0.00)\n",
      "Iter 2520 | Time 17.5730(17.5133) | Bit/dim 3.8100(3.8092) | Xent 1.0304(0.9744) | Loss 31.3691(33.1381) | Error 0.3622(0.3484) Steps 0(0.00) | Grad Norm 50.5728(29.6476) | Total Time 0.00(0.00)\n",
      "Iter 2530 | Time 17.3077(17.4098) | Bit/dim 3.7791(3.8071) | Xent 0.8756(0.9678) | Loss 31.7731(32.8004) | Error 0.3122(0.3434) Steps 0(0.00) | Grad Norm 21.5698(29.0583) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0046 | Time 87.4957, Epoch Time 1068.7724(987.9391), Bit/dim 3.7991(best: 3.8158), Xent 0.9376, Loss 4.2679, Error 0.3342(best: 0.3337)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2540 | Time 17.5369(17.3399) | Bit/dim 3.7997(3.8052) | Xent 0.9647(0.9673) | Loss 30.6208(35.3155) | Error 0.3433(0.3433) Steps 0(0.00) | Grad Norm 29.9045(31.2435) | Total Time 0.00(0.00)\n",
      "Iter 2550 | Time 18.4767(17.4613) | Bit/dim 3.7929(3.8058) | Xent 0.9549(0.9681) | Loss 32.6058(34.4435) | Error 0.3500(0.3443) Steps 0(0.00) | Grad Norm 39.2367(32.5886) | Total Time 0.00(0.00)\n",
      "Iter 2560 | Time 17.5150(17.5597) | Bit/dim 3.8240(3.8020) | Xent 0.9613(0.9640) | Loss 31.8923(33.6668) | Error 0.3589(0.3439) Steps 0(0.00) | Grad Norm 38.9128(32.4198) | Total Time 0.00(0.00)\n",
      "Iter 2570 | Time 17.9047(17.6302) | Bit/dim 3.8269(3.8030) | Xent 1.0147(0.9647) | Loss 31.3487(33.2206) | Error 0.3667(0.3442) Steps 0(0.00) | Grad Norm 36.2611(33.8001) | Total Time 0.00(0.00)\n",
      "Iter 2580 | Time 18.6607(17.6248) | Bit/dim 3.7987(3.8015) | Xent 1.0037(0.9640) | Loss 32.7212(32.9078) | Error 0.3322(0.3434) Steps 0(0.00) | Grad Norm 50.0667(33.0386) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0047 | Time 88.4353, Epoch Time 1081.3049(990.7401), Bit/dim 3.8020(best: 3.7991), Xent 0.9500, Loss 4.2770, Error 0.3353(best: 0.3337)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2590 | Time 18.7492(17.6956) | Bit/dim 3.7597(3.7955) | Xent 0.8620(0.9655) | Loss 31.4801(36.0327) | Error 0.3033(0.3432) Steps 0(0.00) | Grad Norm 32.6940(32.4568) | Total Time 0.00(0.00)\n",
      "Iter 2600 | Time 17.7645(17.6865) | Bit/dim 3.7854(3.7961) | Xent 0.8573(0.9547) | Loss 30.3434(34.7525) | Error 0.3056(0.3396) Steps 0(0.00) | Grad Norm 19.6538(30.6388) | Total Time 0.00(0.00)\n",
      "Iter 2610 | Time 16.9910(17.5813) | Bit/dim 3.8185(3.7964) | Xent 0.9680(0.9610) | Loss 32.6035(33.9992) | Error 0.3300(0.3418) Steps 0(0.00) | Grad Norm 45.4942(31.0765) | Total Time 0.00(0.00)\n",
      "Iter 2620 | Time 17.1932(17.5721) | Bit/dim 3.7822(3.7963) | Xent 0.9717(0.9620) | Loss 31.4348(33.4568) | Error 0.3478(0.3434) Steps 0(0.00) | Grad Norm 28.6350(31.0217) | Total Time 0.00(0.00)\n",
      "Iter 2630 | Time 16.5088(17.4158) | Bit/dim 3.7770(3.7982) | Xent 0.9960(0.9587) | Loss 31.6711(32.9191) | Error 0.3656(0.3441) Steps 0(0.00) | Grad Norm 29.1631(31.2073) | Total Time 0.00(0.00)\n",
      "Iter 2640 | Time 18.1729(17.5694) | Bit/dim 3.8279(3.7981) | Xent 0.9585(0.9611) | Loss 31.0067(32.5496) | Error 0.3367(0.3438) Steps 0(0.00) | Grad Norm 30.8587(32.4630) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0048 | Time 88.4675, Epoch Time 1069.0886(993.0905), Bit/dim 3.8051(best: 3.7991), Xent 0.9577, Loss 4.2839, Error 0.3387(best: 0.3337)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2650 | Time 16.6675(17.4105) | Bit/dim 3.7509(3.7954) | Xent 0.9396(0.9600) | Loss 32.5724(35.1108) | Error 0.3444(0.3448) Steps 0(0.00) | Grad Norm 20.2819(31.9853) | Total Time 0.00(0.00)\n",
      "Iter 2660 | Time 18.3691(17.5758) | Bit/dim 3.7783(3.7942) | Xent 0.9047(0.9521) | Loss 32.0722(34.2212) | Error 0.3100(0.3416) Steps 0(0.00) | Grad Norm 28.1436(31.2055) | Total Time 0.00(0.00)\n",
      "Iter 2670 | Time 17.1684(17.4724) | Bit/dim 3.8089(3.7936) | Xent 0.9686(0.9395) | Loss 31.9790(33.5397) | Error 0.3533(0.3377) Steps 0(0.00) | Grad Norm 24.2674(29.0179) | Total Time 0.00(0.00)\n",
      "Iter 2680 | Time 17.0985(17.5190) | Bit/dim 3.7908(3.7942) | Xent 0.8820(0.9344) | Loss 31.8189(33.1008) | Error 0.2822(0.3338) Steps 0(0.00) | Grad Norm 14.2620(27.6148) | Total Time 0.00(0.00)\n",
      "Iter 2690 | Time 16.8167(17.4470) | Bit/dim 3.7414(3.7909) | Xent 1.0183(0.9410) | Loss 32.2264(32.7429) | Error 0.3656(0.3367) Steps 0(0.00) | Grad Norm 36.1838(27.9259) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0049 | Time 89.7696, Epoch Time 1066.6678(995.2978), Bit/dim 3.7858(best: 3.7991), Xent 0.9676, Loss 4.2696, Error 0.3423(best: 0.3337)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2700 | Time 16.9623(17.4563) | Bit/dim 3.8054(3.7909) | Xent 0.9957(0.9444) | Loss 32.2285(35.9662) | Error 0.3467(0.3376) Steps 0(0.00) | Grad Norm 44.2267(30.0323) | Total Time 0.00(0.00)\n",
      "Iter 2710 | Time 17.0217(17.4624) | Bit/dim 3.7715(3.7919) | Xent 0.9145(0.9495) | Loss 31.8906(34.8563) | Error 0.3356(0.3400) Steps 0(0.00) | Grad Norm 23.7879(33.3952) | Total Time 0.00(0.00)\n",
      "Iter 2720 | Time 16.6385(17.5357) | Bit/dim 3.7848(3.7935) | Xent 0.8654(0.9435) | Loss 31.7758(34.0416) | Error 0.3111(0.3371) Steps 0(0.00) | Grad Norm 43.7291(32.5084) | Total Time 0.00(0.00)\n",
      "Iter 2730 | Time 16.3472(17.4141) | Bit/dim 3.7616(3.7888) | Xent 0.9114(0.9353) | Loss 31.3554(33.3869) | Error 0.3144(0.3330) Steps 0(0.00) | Grad Norm 41.0002(31.5056) | Total Time 0.00(0.00)\n",
      "Iter 2740 | Time 17.6402(17.4488) | Bit/dim 3.7908(3.7866) | Xent 0.9373(0.9465) | Loss 31.4077(33.0472) | Error 0.3367(0.3369) Steps 0(0.00) | Grad Norm 21.5567(31.8734) | Total Time 0.00(0.00)\n",
      "Iter 2750 | Time 16.3566(17.3862) | Bit/dim 3.8181(3.7869) | Xent 0.9682(0.9488) | Loss 30.5799(32.6371) | Error 0.3478(0.3376) Steps 0(0.00) | Grad Norm 29.9083(32.4363) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0050 | Time 89.9193, Epoch Time 1064.8985(997.3859), Bit/dim 3.7920(best: 3.7858), Xent 0.9063, Loss 4.2451, Error 0.3234(best: 0.3337)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2760 | Time 16.3009(17.5142) | Bit/dim 3.7813(3.7840) | Xent 0.9084(0.9412) | Loss 31.9338(35.2402) | Error 0.3333(0.3351) Steps 0(0.00) | Grad Norm 41.3402(32.1293) | Total Time 0.00(0.00)\n",
      "Iter 2770 | Time 16.4269(17.5323) | Bit/dim 3.7893(3.7835) | Xent 0.9312(0.9424) | Loss 31.5118(34.1940) | Error 0.3278(0.3362) Steps 0(0.00) | Grad Norm 30.5458(31.1755) | Total Time 0.00(0.00)\n",
      "Iter 2780 | Time 16.0360(17.5433) | Bit/dim 3.7899(3.7837) | Xent 0.9597(0.9354) | Loss 31.2220(33.5916) | Error 0.3400(0.3344) Steps 0(0.00) | Grad Norm 49.8289(30.9951) | Total Time 0.00(0.00)\n",
      "Iter 2790 | Time 17.7496(17.4098) | Bit/dim 3.7740(3.7831) | Xent 0.9249(0.9304) | Loss 32.3208(33.1509) | Error 0.3267(0.3320) Steps 0(0.00) | Grad Norm 29.0666(30.4843) | Total Time 0.00(0.00)\n",
      "Iter 2800 | Time 17.6407(17.4435) | Bit/dim 3.7885(3.7837) | Xent 0.8623(0.9297) | Loss 30.8076(32.8305) | Error 0.2878(0.3323) Steps 0(0.00) | Grad Norm 34.9491(32.4880) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0051 | Time 88.4433, Epoch Time 1067.6481(999.4937), Bit/dim 3.7950(best: 3.7858), Xent 0.8995, Loss 4.2448, Error 0.3178(best: 0.3234)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2810 | Time 17.5354(17.3522) | Bit/dim 3.7927(3.7863) | Xent 0.9981(0.9294) | Loss 32.7630(35.9076) | Error 0.3644(0.3329) Steps 0(0.00) | Grad Norm 24.6852(31.4965) | Total Time 0.00(0.00)\n",
      "Iter 2820 | Time 17.3028(17.3591) | Bit/dim 3.7741(3.7861) | Xent 0.8980(0.9188) | Loss 31.3329(34.6558) | Error 0.3344(0.3290) Steps 0(0.00) | Grad Norm 38.6656(31.2983) | Total Time 0.00(0.00)\n",
      "Iter 2830 | Time 15.9955(17.3500) | Bit/dim 3.7594(3.7834) | Xent 0.8257(0.9155) | Loss 30.7175(33.7815) | Error 0.2989(0.3280) Steps 0(0.00) | Grad Norm 22.2241(31.6926) | Total Time 0.00(0.00)\n",
      "Iter 2840 | Time 17.4847(17.3970) | Bit/dim 3.7592(3.7808) | Xent 0.8760(0.9118) | Loss 31.3916(33.1789) | Error 0.3267(0.3277) Steps 0(0.00) | Grad Norm 19.6935(30.3924) | Total Time 0.00(0.00)\n",
      "Iter 2850 | Time 15.8487(17.2729) | Bit/dim 3.7685(3.7807) | Xent 0.9556(0.9068) | Loss 31.5841(32.6871) | Error 0.3478(0.3258) Steps 0(0.00) | Grad Norm 31.8183(28.5617) | Total Time 0.00(0.00)\n",
      "Iter 2860 | Time 17.4554(17.3549) | Bit/dim 3.7930(3.7777) | Xent 0.9013(0.9136) | Loss 32.3304(32.4199) | Error 0.3267(0.3271) Steps 0(0.00) | Grad Norm 21.1827(29.4942) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0052 | Time 90.8288, Epoch Time 1064.4365(1001.4420), Bit/dim 3.7840(best: 3.7858), Xent 0.9250, Loss 4.2465, Error 0.3307(best: 0.3178)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2870 | Time 17.7701(17.3881) | Bit/dim 3.7594(3.7774) | Xent 0.9134(0.9224) | Loss 32.1786(35.1988) | Error 0.3278(0.3307) Steps 0(0.00) | Grad Norm 69.4025(34.1102) | Total Time 0.00(0.00)\n",
      "Iter 2880 | Time 17.7318(17.3816) | Bit/dim 3.7528(3.7790) | Xent 0.9138(0.9247) | Loss 32.1954(34.2711) | Error 0.3322(0.3318) Steps 0(0.00) | Grad Norm 20.0879(33.9499) | Total Time 0.00(0.00)\n",
      "Iter 2890 | Time 17.1598(17.3556) | Bit/dim 3.7493(3.7789) | Xent 0.8774(0.9176) | Loss 32.0136(33.6194) | Error 0.3089(0.3292) Steps 0(0.00) | Grad Norm 15.8666(30.4559) | Total Time 0.00(0.00)\n",
      "Iter 2900 | Time 16.7011(17.5420) | Bit/dim 3.7701(3.7764) | Xent 0.9378(0.9139) | Loss 31.5758(33.0904) | Error 0.3178(0.3279) Steps 0(0.00) | Grad Norm 28.7379(28.8373) | Total Time 0.00(0.00)\n",
      "Iter 2910 | Time 16.4517(17.7724) | Bit/dim 3.7593(3.7738) | Xent 0.9214(0.9086) | Loss 32.1735(32.6153) | Error 0.3167(0.3253) Steps 0(0.00) | Grad Norm 18.8625(27.2196) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0053 | Time 89.4668, Epoch Time 1080.6553(1003.8184), Bit/dim 3.7671(best: 3.7840), Xent 0.8801, Loss 4.2071, Error 0.3149(best: 0.3178)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2920 | Time 21.5796(17.7907) | Bit/dim 3.7741(3.7742) | Xent 0.8919(0.9044) | Loss 31.9107(35.6344) | Error 0.3167(0.3241) Steps 0(0.00) | Grad Norm 34.0120(27.7825) | Total Time 0.00(0.00)\n",
      "Iter 2930 | Time 16.6801(17.5759) | Bit/dim 3.7558(3.7723) | Xent 0.8470(0.9019) | Loss 31.1145(34.6001) | Error 0.3000(0.3230) Steps 0(0.00) | Grad Norm 30.5940(28.1380) | Total Time 0.00(0.00)\n",
      "Iter 2940 | Time 18.2815(17.5165) | Bit/dim 3.7723(3.7704) | Xent 0.8674(0.8899) | Loss 31.8809(33.7780) | Error 0.3011(0.3182) Steps 0(0.00) | Grad Norm 35.8536(27.9691) | Total Time 0.00(0.00)\n",
      "Iter 2950 | Time 17.3348(17.4993) | Bit/dim 3.7543(3.7693) | Xent 0.9590(0.8926) | Loss 30.1531(33.1284) | Error 0.3444(0.3189) Steps 0(0.00) | Grad Norm 27.3129(27.3352) | Total Time 0.00(0.00)\n",
      "Iter 2960 | Time 18.6197(17.6178) | Bit/dim 3.7449(3.7662) | Xent 0.8556(0.8955) | Loss 31.8083(32.6789) | Error 0.3011(0.3194) Steps 0(0.00) | Grad Norm 34.3582(30.2561) | Total Time 0.00(0.00)\n",
      "Iter 2970 | Time 18.8819(17.7172) | Bit/dim 3.7610(3.7666) | Xent 0.9552(0.8941) | Loss 32.3799(32.3701) | Error 0.3344(0.3175) Steps 0(0.00) | Grad Norm 46.4364(29.8475) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0054 | Time 91.8830, Epoch Time 1077.6646(1006.0338), Bit/dim 3.7661(best: 3.7671), Xent 0.8715, Loss 4.2019, Error 0.3116(best: 0.3149)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2980 | Time 16.6184(17.6300) | Bit/dim 3.7396(3.7666) | Xent 0.9080(0.8923) | Loss 31.1488(35.0510) | Error 0.3178(0.3162) Steps 0(0.00) | Grad Norm 21.8849(29.2800) | Total Time 0.00(0.00)\n",
      "Iter 2990 | Time 17.8089(17.5442) | Bit/dim 3.7631(3.7675) | Xent 0.8747(0.8893) | Loss 31.4610(34.0239) | Error 0.3178(0.3161) Steps 0(0.00) | Grad Norm 36.8673(33.2566) | Total Time 0.00(0.00)\n",
      "Iter 3000 | Time 16.0517(17.4960) | Bit/dim 3.8111(3.7702) | Xent 0.8698(0.8901) | Loss 30.8788(33.2938) | Error 0.3178(0.3162) Steps 0(0.00) | Grad Norm 20.4889(33.7770) | Total Time 0.00(0.00)\n",
      "Iter 3010 | Time 17.7667(17.4196) | Bit/dim 3.7614(3.7653) | Xent 0.8766(0.8892) | Loss 32.2210(32.8554) | Error 0.2944(0.3156) Steps 0(0.00) | Grad Norm 22.5836(31.0160) | Total Time 0.00(0.00)\n",
      "Iter 3020 | Time 18.7649(17.3946) | Bit/dim 3.7630(3.7636) | Xent 0.9284(0.8881) | Loss 32.1128(32.5013) | Error 0.3378(0.3152) Steps 0(0.00) | Grad Norm 39.8279(30.9601) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0055 | Time 92.2691, Epoch Time 1061.9455(1007.7111), Bit/dim 3.7611(best: 3.7661), Xent 0.8936, Loss 4.2079, Error 0.3161(best: 0.3116)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3030 | Time 18.7960(17.3724) | Bit/dim 3.7622(3.7616) | Xent 0.9217(0.8943) | Loss 32.2983(35.5162) | Error 0.3267(0.3185) Steps 0(0.00) | Grad Norm 16.3721(32.0089) | Total Time 0.00(0.00)\n",
      "Iter 3040 | Time 17.6840(17.3706) | Bit/dim 3.7687(3.7612) | Xent 0.8282(0.8847) | Loss 31.4213(34.4331) | Error 0.2867(0.3148) Steps 0(0.00) | Grad Norm 18.6999(30.7715) | Total Time 0.00(0.00)\n",
      "Iter 3050 | Time 18.1139(17.4143) | Bit/dim 3.7618(3.7587) | Xent 0.7831(0.8760) | Loss 30.7248(33.5617) | Error 0.2789(0.3121) Steps 0(0.00) | Grad Norm 21.1345(28.4950) | Total Time 0.00(0.00)\n",
      "Iter 3060 | Time 19.6515(17.6100) | Bit/dim 3.7683(3.7580) | Xent 0.9457(0.8728) | Loss 33.5502(33.0371) | Error 0.3511(0.3124) Steps 0(0.00) | Grad Norm 39.0637(27.5817) | Total Time 0.00(0.00)\n",
      "Iter 3070 | Time 16.6332(17.5259) | Bit/dim 3.7786(3.7568) | Xent 0.9185(0.8746) | Loss 30.9060(32.5432) | Error 0.3244(0.3131) Steps 0(0.00) | Grad Norm 24.1035(28.6469) | Total Time 0.00(0.00)\n",
      "Iter 3080 | Time 18.6412(17.6519) | Bit/dim 3.7648(3.7559) | Xent 0.8841(0.8788) | Loss 31.4451(32.2348) | Error 0.2978(0.3130) Steps 0(0.00) | Grad Norm 33.2963(29.3511) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0056 | Time 92.3425, Epoch Time 1078.5548(1009.8364), Bit/dim 3.7483(best: 3.7611), Xent 0.8412, Loss 4.1689, Error 0.2963(best: 0.3116)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3090 | Time 17.2012(17.5783) | Bit/dim 3.7574(3.7574) | Xent 0.9081(0.8820) | Loss 31.8344(34.9702) | Error 0.3089(0.3135) Steps 0(0.00) | Grad Norm 31.5614(29.0264) | Total Time 0.00(0.00)\n",
      "Iter 3100 | Time 18.0086(17.5557) | Bit/dim 3.7374(3.7560) | Xent 0.8714(0.8869) | Loss 30.8568(34.0111) | Error 0.2911(0.3163) Steps 0(0.00) | Grad Norm 34.6656(29.7972) | Total Time 0.00(0.00)\n",
      "Iter 3110 | Time 16.6215(17.5421) | Bit/dim 3.7845(3.7546) | Xent 0.9219(0.8808) | Loss 31.8648(33.3239) | Error 0.3233(0.3139) Steps 0(0.00) | Grad Norm 36.9652(31.7763) | Total Time 0.00(0.00)\n",
      "Iter 3120 | Time 15.8112(17.5116) | Bit/dim 3.7289(3.7534) | Xent 0.8744(0.8837) | Loss 30.7276(32.8203) | Error 0.3056(0.3143) Steps 0(0.00) | Grad Norm 26.1633(31.4235) | Total Time 0.00(0.00)\n",
      "Iter 3130 | Time 17.7291(17.4320) | Bit/dim 3.7559(3.7527) | Xent 0.8410(0.8810) | Loss 31.1196(32.3736) | Error 0.3167(0.3135) Steps 0(0.00) | Grad Norm 27.4859(31.7103) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0057 | Time 90.7832, Epoch Time 1070.3987(1011.6533), Bit/dim 3.7530(best: 3.7483), Xent 0.8425, Loss 4.1742, Error 0.2976(best: 0.2963)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3140 | Time 17.9746(17.5168) | Bit/dim 3.7420(3.7502) | Xent 0.8413(0.8730) | Loss 32.1625(35.2244) | Error 0.2911(0.3095) Steps 0(0.00) | Grad Norm 33.7155(30.6024) | Total Time 0.00(0.00)\n",
      "Iter 3150 | Time 16.5305(17.5681) | Bit/dim 3.7507(3.7486) | Xent 0.8378(0.8666) | Loss 30.6828(34.1988) | Error 0.3056(0.3069) Steps 0(0.00) | Grad Norm 24.3939(29.1543) | Total Time 0.00(0.00)\n",
      "Iter 3160 | Time 17.3732(17.5430) | Bit/dim 3.7919(3.7524) | Xent 0.8663(0.8645) | Loss 31.8957(33.4753) | Error 0.3044(0.3077) Steps 0(0.00) | Grad Norm 49.7859(30.9356) | Total Time 0.00(0.00)\n",
      "Iter 3170 | Time 19.5227(17.4072) | Bit/dim 3.7440(3.7517) | Xent 0.8541(0.8584) | Loss 31.5282(32.9055) | Error 0.3300(0.3075) Steps 0(0.00) | Grad Norm 23.6849(30.5260) | Total Time 0.00(0.00)\n",
      "Iter 3180 | Time 17.4876(17.4529) | Bit/dim 3.7455(3.7514) | Xent 0.8851(0.8645) | Loss 32.0619(32.5443) | Error 0.3244(0.3100) Steps 0(0.00) | Grad Norm 23.5380(30.4066) | Total Time 0.00(0.00)\n",
      "Iter 3190 | Time 17.3162(17.3537) | Bit/dim 3.7696(3.7511) | Xent 0.8696(0.8627) | Loss 31.8527(32.1464) | Error 0.3000(0.3081) Steps 0(0.00) | Grad Norm 28.1537(29.7646) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0058 | Time 90.1703, Epoch Time 1063.3640(1013.2046), Bit/dim 3.7507(best: 3.7483), Xent 0.8504, Loss 4.1759, Error 0.3037(best: 0.2963)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3200 | Time 17.6546(17.2501) | Bit/dim 3.7447(3.7506) | Xent 0.8650(0.8582) | Loss 30.5118(34.8259) | Error 0.3000(0.3068) Steps 0(0.00) | Grad Norm 30.2710(28.7368) | Total Time 0.00(0.00)\n",
      "Iter 3210 | Time 19.0457(17.5300) | Bit/dim 3.7369(3.7475) | Xent 0.8075(0.8527) | Loss 31.3090(33.8734) | Error 0.3022(0.3034) Steps 0(0.00) | Grad Norm 18.7077(29.8632) | Total Time 0.00(0.00)\n",
      "Iter 3220 | Time 18.6432(17.8443) | Bit/dim 3.7557(3.7483) | Xent 0.8944(0.8531) | Loss 31.0256(33.2259) | Error 0.3144(0.3033) Steps 0(0.00) | Grad Norm 53.6106(30.6116) | Total Time 0.00(0.00)\n",
      "Iter 3230 | Time 18.6591(17.7493) | Bit/dim 3.7509(3.7489) | Xent 0.8590(0.8599) | Loss 31.8989(32.7067) | Error 0.3156(0.3073) Steps 0(0.00) | Grad Norm 24.6702(29.8237) | Total Time 0.00(0.00)\n",
      "Iter 3240 | Time 16.9495(17.6521) | Bit/dim 3.7433(3.7496) | Xent 0.9026(0.8629) | Loss 31.7546(32.3557) | Error 0.3311(0.3086) Steps 0(0.00) | Grad Norm 26.0289(29.3708) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0059 | Time 92.1739, Epoch Time 1085.4504(1015.3720), Bit/dim 3.7439(best: 3.7483), Xent 0.8564, Loss 4.1721, Error 0.3065(best: 0.2963)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3250 | Time 16.5023(17.5375) | Bit/dim 3.7655(3.7487) | Xent 0.8610(0.8561) | Loss 30.7991(35.3093) | Error 0.3078(0.3062) Steps 0(0.00) | Grad Norm 46.2919(29.1706) | Total Time 0.00(0.00)\n",
      "Iter 3260 | Time 17.1447(17.5799) | Bit/dim 3.7354(3.7476) | Xent 0.8328(0.8568) | Loss 30.2386(34.1848) | Error 0.2922(0.3062) Steps 0(0.00) | Grad Norm 32.9240(31.6512) | Total Time 0.00(0.00)\n",
      "Iter 3270 | Time 17.0150(17.5702) | Bit/dim 3.7390(3.7470) | Xent 0.8419(0.8547) | Loss 31.1934(33.4706) | Error 0.3033(0.3044) Steps 0(0.00) | Grad Norm 29.2635(30.7110) | Total Time 0.00(0.00)\n",
      "Iter 3280 | Time 17.9010(17.6264) | Bit/dim 3.7614(3.7502) | Xent 0.7753(0.8425) | Loss 30.8553(32.9363) | Error 0.2644(0.2997) Steps 0(0.00) | Grad Norm 24.6766(28.8718) | Total Time 0.00(0.00)\n",
      "Iter 3290 | Time 16.9759(17.5362) | Bit/dim 3.7282(3.7454) | Xent 0.9240(0.8547) | Loss 31.2629(32.4998) | Error 0.3256(0.3027) Steps 0(0.00) | Grad Norm 41.5795(31.7205) | Total Time 0.00(0.00)\n",
      "Iter 3300 | Time 17.5463(17.5332) | Bit/dim 3.7654(3.7492) | Xent 0.8921(0.8588) | Loss 31.7400(32.1941) | Error 0.3233(0.3049) Steps 0(0.00) | Grad Norm 19.4876(30.7926) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0060 | Time 91.7149, Epoch Time 1074.2583(1017.1386), Bit/dim 3.7444(best: 3.7439), Xent 0.8592, Loss 4.1740, Error 0.3075(best: 0.2963)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3310 | Time 15.7779(17.6128) | Bit/dim 3.7158(3.7471) | Xent 0.8479(0.8592) | Loss 30.4321(34.9002) | Error 0.2956(0.3064) Steps 0(0.00) | Grad Norm 20.9785(32.8505) | Total Time 0.00(0.00)\n",
      "Iter 3320 | Time 17.3504(17.7655) | Bit/dim 3.7494(3.7440) | Xent 0.8204(0.8493) | Loss 30.8994(33.9171) | Error 0.2922(0.3047) Steps 0(0.00) | Grad Norm 31.7587(30.6872) | Total Time 0.00(0.00)\n",
      "Iter 3330 | Time 16.7447(17.5514) | Bit/dim 3.7182(3.7394) | Xent 0.7705(0.8418) | Loss 31.8447(33.1746) | Error 0.2900(0.3016) Steps 0(0.00) | Grad Norm 21.2116(29.6698) | Total Time 0.00(0.00)\n",
      "Iter 3340 | Time 17.0618(17.6128) | Bit/dim 3.7567(3.7407) | Xent 0.7759(0.8412) | Loss 31.0714(32.6509) | Error 0.2644(0.3002) Steps 0(0.00) | Grad Norm 25.5914(29.7372) | Total Time 0.00(0.00)\n",
      "Iter 3350 | Time 17.5018(17.4805) | Bit/dim 3.7113(3.7431) | Xent 0.8509(0.8391) | Loss 31.1306(32.2350) | Error 0.3100(0.3000) Steps 0(0.00) | Grad Norm 21.6799(28.4856) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0061 | Time 91.9979, Epoch Time 1075.0096(1018.8747), Bit/dim 3.7320(best: 3.7439), Xent 0.8208, Loss 4.1424, Error 0.2929(best: 0.2963)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3360 | Time 15.5141(17.3679) | Bit/dim 3.7541(3.7408) | Xent 0.8157(0.8347) | Loss 31.1278(35.2693) | Error 0.2967(0.2969) Steps 0(0.00) | Grad Norm 19.8880(28.1930) | Total Time 0.00(0.00)\n",
      "Iter 3370 | Time 16.2401(17.3593) | Bit/dim 3.7399(3.7381) | Xent 0.8087(0.8261) | Loss 30.6417(34.1454) | Error 0.2956(0.2951) Steps 0(0.00) | Grad Norm 25.4075(26.8154) | Total Time 0.00(0.00)\n",
      "Iter 3380 | Time 17.5793(17.4985) | Bit/dim 3.7264(3.7343) | Xent 0.8803(0.8292) | Loss 30.6926(33.4489) | Error 0.3244(0.2962) Steps 0(0.00) | Grad Norm 47.4307(28.6693) | Total Time 0.00(0.00)\n",
      "Iter 3390 | Time 17.7334(17.4457) | Bit/dim 3.7503(3.7360) | Xent 0.8167(0.8309) | Loss 31.9758(32.8646) | Error 0.2833(0.2972) Steps 0(0.00) | Grad Norm 27.5059(28.2926) | Total Time 0.00(0.00)\n",
      "Iter 3400 | Time 17.2915(17.4058) | Bit/dim 3.7367(3.7381) | Xent 0.8087(0.8257) | Loss 30.6763(32.2712) | Error 0.2789(0.2938) Steps 0(0.00) | Grad Norm 25.0388(28.8718) | Total Time 0.00(0.00)\n",
      "Iter 3410 | Time 18.0491(17.3137) | Bit/dim 3.7351(3.7393) | Xent 0.8332(0.8278) | Loss 31.0759(31.9582) | Error 0.3011(0.2953) Steps 0(0.00) | Grad Norm 34.7125(28.3599) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0062 | Time 93.1861, Epoch Time 1064.8720(1020.2546), Bit/dim 3.7386(best: 3.7320), Xent 0.8640, Loss 4.1706, Error 0.3041(best: 0.2929)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3420 | Time 17.0378(17.4664) | Bit/dim 3.7453(3.7367) | Xent 0.8957(0.8342) | Loss 31.8830(34.8011) | Error 0.3178(0.2981) Steps 0(0.00) | Grad Norm 37.7256(29.5065) | Total Time 0.00(0.00)\n",
      "Iter 3430 | Time 16.5658(17.3254) | Bit/dim 3.7048(3.7373) | Xent 0.8014(0.8267) | Loss 30.7146(33.8543) | Error 0.2744(0.2953) Steps 0(0.00) | Grad Norm 23.7539(29.8142) | Total Time 0.00(0.00)\n",
      "Iter 3440 | Time 17.1001(17.3382) | Bit/dim 3.7432(3.7358) | Xent 0.8317(0.8247) | Loss 30.6402(33.0441) | Error 0.3011(0.2946) Steps 0(0.00) | Grad Norm 32.7803(28.5199) | Total Time 0.00(0.00)\n",
      "Iter 3450 | Time 16.5324(17.4430) | Bit/dim 3.7271(3.7335) | Xent 0.7706(0.8165) | Loss 30.9769(32.4843) | Error 0.2711(0.2909) Steps 0(0.00) | Grad Norm 22.4656(27.5152) | Total Time 0.00(0.00)\n",
      "Iter 3460 | Time 18.8859(17.5625) | Bit/dim 3.7607(3.7344) | Xent 0.8065(0.8211) | Loss 31.7578(32.1682) | Error 0.2678(0.2910) Steps 0(0.00) | Grad Norm 26.0858(27.2408) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0063 | Time 93.1171, Epoch Time 1077.2721(1021.9652), Bit/dim 3.7558(best: 3.7320), Xent 0.8651, Loss 4.1883, Error 0.3113(best: 0.2929)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3470 | Time 17.7183(17.5642) | Bit/dim 3.7389(3.7356) | Xent 0.8257(0.8254) | Loss 31.6617(35.3593) | Error 0.3078(0.2941) Steps 0(0.00) | Grad Norm 42.2321(30.5671) | Total Time 0.00(0.00)\n",
      "Iter 3480 | Time 18.7583(17.5751) | Bit/dim 3.6760(3.7363) | Xent 0.8848(0.8325) | Loss 30.7243(34.3612) | Error 0.3100(0.2980) Steps 0(0.00) | Grad Norm 36.6642(31.2902) | Total Time 0.00(0.00)\n",
      "Iter 3490 | Time 17.2274(17.6436) | Bit/dim 3.7309(3.7336) | Xent 0.7977(0.8313) | Loss 30.9702(33.4682) | Error 0.2756(0.2965) Steps 0(0.00) | Grad Norm 23.0707(32.2142) | Total Time 0.00(0.00)\n",
      "Iter 3500 | Time 16.9473(17.7212) | Bit/dim 3.7059(3.7334) | Xent 0.8400(0.8272) | Loss 31.1659(32.8931) | Error 0.3000(0.2941) Steps 0(0.00) | Grad Norm 34.1793(31.2993) | Total Time 0.00(0.00)\n",
      "Iter 3510 | Time 18.8036(17.8636) | Bit/dim 3.7521(3.7331) | Xent 0.7804(0.8228) | Loss 29.8528(32.4603) | Error 0.2722(0.2926) Steps 0(0.00) | Grad Norm 34.7207(30.2600) | Total Time 0.00(0.00)\n",
      "Iter 3520 | Time 17.7158(17.7621) | Bit/dim 3.7332(3.7334) | Xent 0.8800(0.8245) | Loss 31.1313(32.2019) | Error 0.3000(0.2923) Steps 0(0.00) | Grad Norm 45.7010(29.2163) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0064 | Time 94.0865, Epoch Time 1091.4881(1024.0509), Bit/dim 3.7309(best: 3.7320), Xent 0.8165, Loss 4.1391, Error 0.2900(best: 0.2929)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3530 | Time 18.0847(17.9070) | Bit/dim 3.7552(3.7308) | Xent 0.8240(0.8231) | Loss 31.5905(35.0481) | Error 0.2911(0.2916) Steps 0(0.00) | Grad Norm 29.5605(28.9818) | Total Time 0.00(0.00)\n",
      "Iter 3540 | Time 17.0984(17.8775) | Bit/dim 3.7782(3.7356) | Xent 0.8107(0.8130) | Loss 30.6534(34.0596) | Error 0.2856(0.2887) Steps 0(0.00) | Grad Norm 34.5497(29.3713) | Total Time 0.00(0.00)\n",
      "Iter 3550 | Time 18.0323(17.9656) | Bit/dim 3.7463(3.7331) | Xent 0.7346(0.8111) | Loss 30.7263(33.2959) | Error 0.2611(0.2891) Steps 0(0.00) | Grad Norm 22.4870(27.8869) | Total Time 0.00(0.00)\n",
      "Iter 3560 | Time 17.7662(17.9330) | Bit/dim 3.7126(3.7302) | Xent 0.8203(0.8132) | Loss 31.1382(32.7210) | Error 0.3022(0.2894) Steps 0(0.00) | Grad Norm 31.3276(29.8785) | Total Time 0.00(0.00)\n",
      "Iter 3570 | Time 19.1083(18.0110) | Bit/dim 3.7191(3.7270) | Xent 0.8731(0.8205) | Loss 32.1894(32.3273) | Error 0.2922(0.2936) Steps 0(0.00) | Grad Norm 22.9378(29.4639) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0065 | Time 92.5422, Epoch Time 1105.0537(1026.4809), Bit/dim 3.7253(best: 3.7309), Xent 0.8366, Loss 4.1436, Error 0.2962(best: 0.2900)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3580 | Time 17.7705(17.9515) | Bit/dim 3.7116(3.7266) | Xent 0.8300(0.8211) | Loss 30.8886(35.2946) | Error 0.2967(0.2946) Steps 0(0.00) | Grad Norm 31.3187(29.1031) | Total Time 0.00(0.00)\n",
      "Iter 3590 | Time 18.8016(17.9372) | Bit/dim 3.7355(3.7260) | Xent 0.7302(0.8155) | Loss 31.4675(34.2035) | Error 0.2611(0.2918) Steps 0(0.00) | Grad Norm 18.2339(29.3091) | Total Time 0.00(0.00)\n",
      "Iter 3600 | Time 19.4050(17.9354) | Bit/dim 3.7170(3.7260) | Xent 0.7899(0.8068) | Loss 31.6116(33.4578) | Error 0.2956(0.2882) Steps 0(0.00) | Grad Norm 26.7537(28.5868) | Total Time 0.00(0.00)\n",
      "Iter 3610 | Time 18.7056(17.9405) | Bit/dim 3.7683(3.7257) | Xent 0.7681(0.8072) | Loss 31.4147(32.9161) | Error 0.2744(0.2877) Steps 0(0.00) | Grad Norm 34.6998(28.8865) | Total Time 0.00(0.00)\n",
      "Iter 3620 | Time 17.9701(17.8112) | Bit/dim 3.7353(3.7262) | Xent 0.8508(0.8078) | Loss 31.5189(32.4614) | Error 0.2978(0.2876) Steps 0(0.00) | Grad Norm 27.4271(29.0238) | Total Time 0.00(0.00)\n",
      "Iter 3630 | Time 16.8910(17.7322) | Bit/dim 3.7448(3.7280) | Xent 0.8205(0.8084) | Loss 30.8721(32.0926) | Error 0.2711(0.2875) Steps 0(0.00) | Grad Norm 32.5229(29.5726) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0066 | Time 94.2831, Epoch Time 1086.9561(1028.2952), Bit/dim 3.7275(best: 3.7253), Xent 0.7913, Loss 4.1231, Error 0.2817(best: 0.2900)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3640 | Time 18.2164(17.6629) | Bit/dim 3.7667(3.7286) | Xent 0.8276(0.8061) | Loss 31.5095(34.7366) | Error 0.2911(0.2855) Steps 0(0.00) | Grad Norm 29.3887(29.6925) | Total Time 0.00(0.00)\n",
      "Iter 3650 | Time 17.5664(17.7187) | Bit/dim 3.7203(3.7252) | Xent 0.8390(0.8005) | Loss 31.8540(33.7886) | Error 0.2900(0.2837) Steps 0(0.00) | Grad Norm 31.2226(30.0462) | Total Time 0.00(0.00)\n",
      "Iter 3660 | Time 18.4370(17.6376) | Bit/dim 3.7269(3.7289) | Xent 0.7943(0.7972) | Loss 31.9831(33.0527) | Error 0.2833(0.2826) Steps 0(0.00) | Grad Norm 44.4700(30.0919) | Total Time 0.00(0.00)\n",
      "Iter 3670 | Time 17.5066(17.6707) | Bit/dim 3.6988(3.7288) | Xent 0.7457(0.7942) | Loss 31.1864(32.5608) | Error 0.2756(0.2825) Steps 0(0.00) | Grad Norm 20.1494(28.8599) | Total Time 0.00(0.00)\n",
      "Iter 3680 | Time 18.9482(17.7711) | Bit/dim 3.7186(3.7238) | Xent 0.8048(0.7993) | Loss 31.0502(32.2346) | Error 0.2978(0.2853) Steps 0(0.00) | Grad Norm 17.7175(27.6052) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0067 | Time 94.8395, Epoch Time 1089.4179(1030.1289), Bit/dim 3.7231(best: 3.7253), Xent 0.8077, Loss 4.1269, Error 0.2875(best: 0.2817)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3690 | Time 16.8862(17.8827) | Bit/dim 3.7383(3.7248) | Xent 0.8322(0.7991) | Loss 31.6955(35.3698) | Error 0.2844(0.2853) Steps 0(0.00) | Grad Norm 25.7975(27.3225) | Total Time 0.00(0.00)\n",
      "Iter 3700 | Time 18.3514(17.8563) | Bit/dim 3.7143(3.7241) | Xent 0.7867(0.7902) | Loss 31.5555(34.2280) | Error 0.2756(0.2829) Steps 0(0.00) | Grad Norm 28.6111(26.9534) | Total Time 0.00(0.00)\n",
      "Iter 3710 | Time 17.3370(17.8300) | Bit/dim 3.7185(3.7221) | Xent 0.7908(0.7918) | Loss 29.5212(33.3612) | Error 0.2611(0.2824) Steps 0(0.00) | Grad Norm 27.8828(27.9535) | Total Time 0.00(0.00)\n",
      "Iter 3720 | Time 18.8573(17.8801) | Bit/dim 3.7541(3.7209) | Xent 0.8339(0.7915) | Loss 31.1034(32.7858) | Error 0.3133(0.2831) Steps 0(0.00) | Grad Norm 35.8121(26.8921) | Total Time 0.00(0.00)\n",
      "Iter 3730 | Time 18.4163(17.9121) | Bit/dim 3.6978(3.7194) | Xent 0.8771(0.8002) | Loss 31.2122(32.3998) | Error 0.3144(0.2867) Steps 0(0.00) | Grad Norm 42.0685(29.0605) | Total Time 0.00(0.00)\n",
      "Iter 3740 | Time 17.7335(17.8613) | Bit/dim 3.7096(3.7200) | Xent 0.7570(0.7997) | Loss 31.7126(32.0932) | Error 0.2556(0.2863) Steps 0(0.00) | Grad Norm 19.9559(27.5067) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0068 | Time 94.1150, Epoch Time 1095.0801(1032.0774), Bit/dim 3.7223(best: 3.7231), Xent 0.8066, Loss 4.1256, Error 0.2858(best: 0.2817)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3750 | Time 19.3013(17.8914) | Bit/dim 3.7245(3.7182) | Xent 0.7462(0.7970) | Loss 31.7563(34.8044) | Error 0.2622(0.2847) Steps 0(0.00) | Grad Norm 26.3039(29.0956) | Total Time 0.00(0.00)\n",
      "Iter 3760 | Time 18.3766(17.9692) | Bit/dim 3.7157(3.7177) | Xent 0.7508(0.7992) | Loss 32.6315(33.8590) | Error 0.2889(0.2840) Steps 0(0.00) | Grad Norm 31.7775(30.5560) | Total Time 0.00(0.00)\n",
      "Iter 3770 | Time 18.1839(17.8467) | Bit/dim 3.7452(3.7164) | Xent 0.8355(0.7936) | Loss 31.9216(33.0652) | Error 0.2889(0.2817) Steps 0(0.00) | Grad Norm 32.2153(29.7229) | Total Time 0.00(0.00)\n",
      "Iter 3780 | Time 17.4380(17.8812) | Bit/dim 3.7444(3.7203) | Xent 0.7594(0.7911) | Loss 31.1715(32.5804) | Error 0.2678(0.2820) Steps 0(0.00) | Grad Norm 23.3480(28.0736) | Total Time 0.00(0.00)\n",
      "Iter 3790 | Time 18.4814(17.9412) | Bit/dim 3.7098(3.7195) | Xent 0.8529(0.7879) | Loss 32.6069(32.2240) | Error 0.3044(0.2807) Steps 0(0.00) | Grad Norm 15.5797(26.8706) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0069 | Time 93.7644, Epoch Time 1100.4138(1034.1275), Bit/dim 3.7151(best: 3.7223), Xent 0.7936, Loss 4.1119, Error 0.2805(best: 0.2817)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3800 | Time 18.3468(17.9986) | Bit/dim 3.7181(3.7192) | Xent 0.7200(0.7799) | Loss 30.9878(35.3825) | Error 0.2611(0.2767) Steps 0(0.00) | Grad Norm 23.1741(26.1688) | Total Time 0.00(0.00)\n",
      "Iter 3810 | Time 17.7821(18.0226) | Bit/dim 3.7111(3.7159) | Xent 0.7414(0.7761) | Loss 29.8942(34.1524) | Error 0.2767(0.2762) Steps 0(0.00) | Grad Norm 19.1572(25.6618) | Total Time 0.00(0.00)\n",
      "Iter 3820 | Time 17.5052(18.1247) | Bit/dim 3.7283(3.7178) | Xent 0.8208(0.7840) | Loss 31.5047(33.4548) | Error 0.2922(0.2782) Steps 0(0.00) | Grad Norm 20.0253(27.0038) | Total Time 0.00(0.00)\n",
      "Iter 3830 | Time 16.5265(18.0449) | Bit/dim 3.7211(3.7186) | Xent 0.7735(0.7809) | Loss 30.4960(32.8123) | Error 0.2567(0.2775) Steps 0(0.00) | Grad Norm 19.9811(25.7953) | Total Time 0.00(0.00)\n",
      "Iter 3840 | Time 17.6192(17.9576) | Bit/dim 3.7098(3.7146) | Xent 0.8174(0.7792) | Loss 31.9485(32.3650) | Error 0.2833(0.2765) Steps 0(0.00) | Grad Norm 31.6512(25.4920) | Total Time 0.00(0.00)\n",
      "Iter 3850 | Time 17.5425(17.8748) | Bit/dim 3.7029(3.7118) | Xent 0.7224(0.7745) | Loss 29.9302(31.9184) | Error 0.2622(0.2765) Steps 0(0.00) | Grad Norm 20.4215(25.7220) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0070 | Time 93.2749, Epoch Time 1098.1203(1036.0473), Bit/dim 3.7122(best: 3.7151), Xent 0.7858, Loss 4.1051, Error 0.2813(best: 0.2805)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3860 | Time 19.1057(17.9728) | Bit/dim 3.7452(3.7127) | Xent 0.7346(0.7750) | Loss 30.4775(34.6224) | Error 0.2578(0.2757) Steps 0(0.00) | Grad Norm 16.4969(25.2339) | Total Time 0.00(0.00)\n",
      "Iter 3870 | Time 18.0500(17.9727) | Bit/dim 3.7182(3.7126) | Xent 0.7531(0.7706) | Loss 31.2165(33.6566) | Error 0.2444(0.2751) Steps 0(0.00) | Grad Norm 17.2805(24.7685) | Total Time 0.00(0.00)\n",
      "Iter 3880 | Time 16.6958(17.9368) | Bit/dim 3.7048(3.7104) | Xent 0.7677(0.7840) | Loss 30.8983(33.0424) | Error 0.2678(0.2789) Steps 0(0.00) | Grad Norm 24.4287(29.0245) | Total Time 0.00(0.00)\n",
      "Iter 3890 | Time 17.9073(17.8832) | Bit/dim 3.7140(3.7146) | Xent 0.7937(0.7826) | Loss 31.0734(32.5448) | Error 0.2811(0.2788) Steps 0(0.00) | Grad Norm 39.0862(28.0254) | Total Time 0.00(0.00)\n",
      "Iter 3900 | Time 16.9269(17.9227) | Bit/dim 3.6983(3.7131) | Xent 0.7740(0.7827) | Loss 30.9155(32.2429) | Error 0.2711(0.2785) Steps 0(0.00) | Grad Norm 35.7921(29.2348) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0071 | Time 94.1725, Epoch Time 1097.8438(1037.9012), Bit/dim 3.7100(best: 3.7122), Xent 0.7899, Loss 4.1049, Error 0.2776(best: 0.2805)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3910 | Time 17.8025(17.8431) | Bit/dim 3.7076(3.7114) | Xent 0.8044(0.7798) | Loss 30.6493(35.3284) | Error 0.2778(0.2769) Steps 0(0.00) | Grad Norm 30.9296(28.6977) | Total Time 0.00(0.00)\n",
      "Iter 3920 | Time 17.0846(17.9335) | Bit/dim 3.7347(3.7078) | Xent 0.7375(0.7716) | Loss 30.9793(34.0726) | Error 0.2533(0.2737) Steps 0(0.00) | Grad Norm 22.0738(28.0982) | Total Time 0.00(0.00)\n",
      "Iter 3930 | Time 18.0473(17.9218) | Bit/dim 3.7206(3.7085) | Xent 0.7477(0.7718) | Loss 30.9576(33.2425) | Error 0.2689(0.2755) Steps 0(0.00) | Grad Norm 27.4825(28.7454) | Total Time 0.00(0.00)\n",
      "Iter 3940 | Time 18.5242(17.9158) | Bit/dim 3.6897(3.7045) | Xent 0.7952(0.7721) | Loss 30.5351(32.5695) | Error 0.2922(0.2753) Steps 0(0.00) | Grad Norm 26.7274(28.6366) | Total Time 0.00(0.00)\n",
      "Iter 3950 | Time 16.7152(17.8126) | Bit/dim 3.7482(3.7098) | Xent 0.7395(0.7669) | Loss 31.4248(32.0331) | Error 0.2789(0.2738) Steps 0(0.00) | Grad Norm 22.4833(27.8574) | Total Time 0.00(0.00)\n",
      "Iter 3960 | Time 17.2763(17.8987) | Bit/dim 3.7362(3.7127) | Xent 0.7481(0.7683) | Loss 31.2184(31.8428) | Error 0.2644(0.2744) Steps 0(0.00) | Grad Norm 15.8847(27.5594) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0072 | Time 93.2863, Epoch Time 1096.0242(1039.6449), Bit/dim 3.7161(best: 3.7100), Xent 0.7654, Loss 4.0987, Error 0.2668(best: 0.2776)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3970 | Time 17.8036(17.8546) | Bit/dim 3.7100(3.7131) | Xent 0.7774(0.7651) | Loss 31.4276(34.6415) | Error 0.2900(0.2723) Steps 0(0.00) | Grad Norm 24.5161(27.9116) | Total Time 0.00(0.00)\n",
      "Iter 3980 | Time 15.9093(17.7973) | Bit/dim 3.7399(3.7146) | Xent 0.7439(0.7674) | Loss 29.0867(33.6209) | Error 0.2756(0.2725) Steps 0(0.00) | Grad Norm 17.7533(27.4320) | Total Time 0.00(0.00)\n",
      "Iter 3990 | Time 20.5297(17.8835) | Bit/dim 3.7162(3.7115) | Xent 0.7462(0.7636) | Loss 30.5819(32.9020) | Error 0.2578(0.2702) Steps 0(0.00) | Grad Norm 20.7067(26.3246) | Total Time 0.00(0.00)\n",
      "Iter 4000 | Time 18.9842(18.0478) | Bit/dim 3.7046(3.7089) | Xent 0.7033(0.7548) | Loss 31.7658(32.3637) | Error 0.2378(0.2678) Steps 0(0.00) | Grad Norm 22.1787(25.6345) | Total Time 0.00(0.00)\n",
      "Iter 4010 | Time 18.5580(18.0499) | Bit/dim 3.7262(3.7067) | Xent 0.8372(0.7492) | Loss 32.0727(31.9125) | Error 0.2900(0.2665) Steps 0(0.00) | Grad Norm 15.8120(24.2044) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0073 | Time 94.7529, Epoch Time 1104.3872(1041.5871), Bit/dim 3.7108(best: 3.7100), Xent 0.8356, Loss 4.1286, Error 0.2954(best: 0.2668)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4020 | Time 17.9558(18.0098) | Bit/dim 3.7114(3.7057) | Xent 0.7525(0.7519) | Loss 31.3377(35.0702) | Error 0.2767(0.2681) Steps 0(0.00) | Grad Norm 37.8674(27.2604) | Total Time 0.00(0.00)\n",
      "Iter 4030 | Time 18.9073(18.0532) | Bit/dim 3.7435(3.7082) | Xent 0.7238(0.7547) | Loss 31.6410(34.0716) | Error 0.2544(0.2681) Steps 0(0.00) | Grad Norm 23.8984(28.2169) | Total Time 0.00(0.00)\n",
      "Iter 4040 | Time 18.3012(18.1069) | Bit/dim 3.7167(3.7060) | Xent 0.7056(0.7521) | Loss 31.3155(33.1979) | Error 0.2667(0.2670) Steps 0(0.00) | Grad Norm 13.1698(27.2918) | Total Time 0.00(0.00)\n",
      "Iter 4050 | Time 18.1035(18.2181) | Bit/dim 3.6765(3.7060) | Xent 0.7399(0.7438) | Loss 30.4364(32.5406) | Error 0.2578(0.2644) Steps 0(0.00) | Grad Norm 24.9307(25.2632) | Total Time 0.00(0.00)\n",
      "Iter 4060 | Time 17.2880(18.1345) | Bit/dim 3.6998(3.7046) | Xent 0.7232(0.7503) | Loss 30.7457(32.1630) | Error 0.2689(0.2684) Steps 0(0.00) | Grad Norm 24.7852(26.2873) | Total Time 0.00(0.00)\n",
      "Iter 4070 | Time 19.2199(18.1194) | Bit/dim 3.7091(3.7052) | Xent 0.7646(0.7493) | Loss 31.8849(31.8685) | Error 0.2711(0.2676) Steps 0(0.00) | Grad Norm 37.8248(26.7311) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0074 | Time 90.5762, Epoch Time 1105.1780(1043.4949), Bit/dim 3.7049(best: 3.7100), Xent 0.7580, Loss 4.0839, Error 0.2692(best: 0.2668)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4080 | Time 16.0193(17.9575) | Bit/dim 3.6819(3.7042) | Xent 0.7381(0.7458) | Loss 29.4490(34.6867) | Error 0.2689(0.2651) Steps 0(0.00) | Grad Norm 23.2581(27.6480) | Total Time 0.00(0.00)\n",
      "Iter 4090 | Time 17.6590(17.9136) | Bit/dim 3.6770(3.7028) | Xent 0.7536(0.7462) | Loss 30.9623(33.6713) | Error 0.2867(0.2660) Steps 0(0.00) | Grad Norm 24.4262(26.7880) | Total Time 0.00(0.00)\n",
      "Iter 4100 | Time 16.5848(17.9905) | Bit/dim 3.7241(3.7049) | Xent 0.7600(0.7552) | Loss 30.1083(32.9672) | Error 0.2800(0.2692) Steps 0(0.00) | Grad Norm 50.7530(29.6004) | Total Time 0.00(0.00)\n",
      "Iter 4110 | Time 18.1503(17.9953) | Bit/dim 3.7138(3.7052) | Xent 0.8354(0.7628) | Loss 30.9096(32.4589) | Error 0.2944(0.2727) Steps 0(0.00) | Grad Norm 30.9930(29.6044) | Total Time 0.00(0.00)\n",
      "Iter 4120 | Time 16.2770(17.9187) | Bit/dim 3.6860(3.7063) | Xent 0.7487(0.7672) | Loss 29.8435(32.0098) | Error 0.2856(0.2752) Steps 0(0.00) | Grad Norm 37.6639(30.2232) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0075 | Time 89.7364, Epoch Time 1090.2029(1044.8961), Bit/dim 3.7067(best: 3.7049), Xent 0.7612, Loss 4.0872, Error 0.2688(best: 0.2668)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4130 | Time 19.0390(18.0570) | Bit/dim 3.7185(3.7071) | Xent 0.7354(0.7639) | Loss 31.7426(35.2928) | Error 0.2500(0.2721) Steps 0(0.00) | Grad Norm 28.9601(29.2045) | Total Time 0.00(0.00)\n",
      "Iter 4140 | Time 18.4692(17.9291) | Bit/dim 3.6899(3.7072) | Xent 0.7595(0.7523) | Loss 31.7027(34.0267) | Error 0.2633(0.2683) Steps 0(0.00) | Grad Norm 22.5153(27.6412) | Total Time 0.00(0.00)\n",
      "Iter 4150 | Time 17.7439(17.8008) | Bit/dim 3.7219(3.7076) | Xent 0.6902(0.7424) | Loss 30.5000(33.2201) | Error 0.2589(0.2642) Steps 0(0.00) | Grad Norm 23.1716(25.4543) | Total Time 0.00(0.00)\n",
      "Iter 4160 | Time 18.1892(17.7580) | Bit/dim 3.7031(3.7045) | Xent 0.7654(0.7375) | Loss 30.9039(32.5820) | Error 0.2711(0.2622) Steps 0(0.00) | Grad Norm 25.9250(24.1752) | Total Time 0.00(0.00)\n",
      "Iter 4170 | Time 17.6898(17.6986) | Bit/dim 3.7074(3.7034) | Xent 0.7844(0.7370) | Loss 31.3420(32.1477) | Error 0.2700(0.2623) Steps 0(0.00) | Grad Norm 37.0948(25.4443) | Total Time 0.00(0.00)\n",
      "Iter 4180 | Time 17.4886(17.6226) | Bit/dim 3.6767(3.6979) | Xent 0.7117(0.7418) | Loss 29.8676(31.7299) | Error 0.2533(0.2639) Steps 0(0.00) | Grad Norm 24.9175(27.7402) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0076 | Time 92.1195, Epoch Time 1075.3596(1045.8100), Bit/dim 3.7042(best: 3.7049), Xent 0.8154, Loss 4.1119, Error 0.2841(best: 0.2668)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4190 | Time 16.9369(17.5536) | Bit/dim 3.6921(3.6965) | Xent 0.7907(0.7448) | Loss 29.7855(34.3694) | Error 0.2689(0.2638) Steps 0(0.00) | Grad Norm 29.9574(29.1912) | Total Time 0.00(0.00)\n",
      "Iter 4200 | Time 17.2422(17.4593) | Bit/dim 3.6790(3.6954) | Xent 0.6825(0.7370) | Loss 30.7349(33.4082) | Error 0.2467(0.2621) Steps 0(0.00) | Grad Norm 12.0018(27.6022) | Total Time 0.00(0.00)\n",
      "Iter 4210 | Time 19.0647(17.6123) | Bit/dim 3.6874(3.6955) | Xent 0.6981(0.7339) | Loss 31.2139(32.7746) | Error 0.2700(0.2612) Steps 0(0.00) | Grad Norm 21.2380(25.1442) | Total Time 0.00(0.00)\n",
      "Iter 4220 | Time 16.5361(17.6258) | Bit/dim 3.6796(3.6953) | Xent 0.7211(0.7279) | Loss 30.1546(32.1653) | Error 0.2367(0.2578) Steps 0(0.00) | Grad Norm 43.4022(25.5129) | Total Time 0.00(0.00)\n",
      "Iter 4230 | Time 17.1015(17.7505) | Bit/dim 3.6995(3.6990) | Xent 0.8094(0.7255) | Loss 31.2629(31.7931) | Error 0.2856(0.2575) Steps 0(0.00) | Grad Norm 29.8509(26.8219) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0077 | Time 89.1826, Epoch Time 1079.2343(1046.8127), Bit/dim 3.7042(best: 3.7042), Xent 0.8063, Loss 4.1074, Error 0.2809(best: 0.2668)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4240 | Time 17.5762(17.7527) | Bit/dim 3.7175(3.6949) | Xent 0.7569(0.7287) | Loss 30.6531(34.8598) | Error 0.2633(0.2579) Steps 0(0.00) | Grad Norm 30.1646(27.8298) | Total Time 0.00(0.00)\n",
      "Iter 4250 | Time 17.6546(17.6423) | Bit/dim 3.7219(3.6979) | Xent 0.7162(0.7227) | Loss 30.8554(33.7682) | Error 0.2544(0.2555) Steps 0(0.00) | Grad Norm 23.6131(26.5384) | Total Time 0.00(0.00)\n",
      "Iter 4260 | Time 17.6208(17.4550) | Bit/dim 3.6889(3.6984) | Xent 0.7528(0.7221) | Loss 30.5789(32.9059) | Error 0.2733(0.2564) Steps 0(0.00) | Grad Norm 36.6971(26.4868) | Total Time 0.00(0.00)\n",
      "Iter 4270 | Time 16.4990(17.4230) | Bit/dim 3.7189(3.6996) | Xent 0.7593(0.7261) | Loss 30.5054(32.2896) | Error 0.2711(0.2574) Steps 0(0.00) | Grad Norm 29.0887(26.5308) | Total Time 0.00(0.00)\n",
      "Iter 4280 | Time 17.8866(17.4909) | Bit/dim 3.6798(3.6991) | Xent 0.8113(0.7232) | Loss 31.2106(31.9137) | Error 0.2911(0.2572) Steps 0(0.00) | Grad Norm 22.3478(26.3354) | Total Time 0.00(0.00)\n",
      "Iter 4290 | Time 17.0320(17.5189) | Bit/dim 3.6914(3.6975) | Xent 0.7226(0.7202) | Loss 29.7430(31.5714) | Error 0.2789(0.2574) Steps 0(0.00) | Grad Norm 25.2424(25.9472) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0078 | Time 89.4824, Epoch Time 1060.9112(1047.2357), Bit/dim 3.6956(best: 3.7042), Xent 0.7657, Loss 4.0784, Error 0.2704(best: 0.2668)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4300 | Time 16.4106(17.5833) | Bit/dim 3.7079(3.6954) | Xent 0.6895(0.7154) | Loss 28.9366(34.3917) | Error 0.2300(0.2544) Steps 0(0.00) | Grad Norm 37.6664(26.1488) | Total Time 0.00(0.00)\n",
      "Iter 4310 | Time 18.5497(17.6178) | Bit/dim 3.6655(3.6954) | Xent 0.7054(0.7211) | Loss 30.6608(33.5148) | Error 0.2744(0.2567) Steps 0(0.00) | Grad Norm 42.5887(28.5228) | Total Time 0.00(0.00)\n",
      "Iter 4320 | Time 17.0232(17.6310) | Bit/dim 3.7039(3.6984) | Xent 0.7416(0.7306) | Loss 30.7583(32.7507) | Error 0.2622(0.2588) Steps 0(0.00) | Grad Norm 25.5768(29.5610) | Total Time 0.00(0.00)\n",
      "Iter 4330 | Time 17.6543(17.6492) | Bit/dim 3.6729(3.6957) | Xent 0.7305(0.7314) | Loss 30.7078(32.1549) | Error 0.2567(0.2599) Steps 0(0.00) | Grad Norm 19.9963(27.6075) | Total Time 0.00(0.00)\n",
      "Iter 4340 | Time 16.6894(17.7416) | Bit/dim 3.6741(3.6973) | Xent 0.7135(0.7323) | Loss 30.4586(31.8557) | Error 0.2500(0.2607) Steps 0(0.00) | Grad Norm 23.6737(27.1628) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0079 | Time 90.4810, Epoch Time 1086.8889(1048.4253), Bit/dim 3.6999(best: 3.6956), Xent 0.7422, Loss 4.0710, Error 0.2598(best: 0.2668)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4350 | Time 17.9727(17.7875) | Bit/dim 3.6788(3.6943) | Xent 0.7199(0.7263) | Loss 29.9951(34.9920) | Error 0.2756(0.2594) Steps 0(0.00) | Grad Norm 21.2384(25.9867) | Total Time 0.00(0.00)\n",
      "Iter 4360 | Time 17.9903(17.8771) | Bit/dim 3.6864(3.6935) | Xent 0.7645(0.7197) | Loss 31.9698(33.8802) | Error 0.2622(0.2566) Steps 0(0.00) | Grad Norm 31.5306(24.7419) | Total Time 0.00(0.00)\n",
      "Iter 4370 | Time 16.6263(17.7295) | Bit/dim 3.7133(3.6944) | Xent 0.7418(0.7162) | Loss 29.9418(33.0007) | Error 0.2633(0.2545) Steps 0(0.00) | Grad Norm 43.5275(26.4149) | Total Time 0.00(0.00)\n",
      "Iter 4380 | Time 16.8852(17.6772) | Bit/dim 3.6939(3.6916) | Xent 0.7565(0.7156) | Loss 29.4735(32.3052) | Error 0.2633(0.2546) Steps 0(0.00) | Grad Norm 28.0010(27.2744) | Total Time 0.00(0.00)\n",
      "Iter 4390 | Time 17.7047(17.7321) | Bit/dim 3.7477(3.6934) | Xent 0.8038(0.7249) | Loss 31.3348(31.9592) | Error 0.3022(0.2578) Steps 0(0.00) | Grad Norm 41.9690(28.2990) | Total Time 0.00(0.00)\n",
      "Iter 4400 | Time 17.2592(17.6612) | Bit/dim 3.6554(3.6968) | Xent 0.7446(0.7254) | Loss 30.2681(31.6495) | Error 0.2478(0.2574) Steps 0(0.00) | Grad Norm 15.8530(27.5061) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0080 | Time 90.1994, Epoch Time 1076.6008(1049.2706), Bit/dim 3.6979(best: 3.6956), Xent 0.7762, Loss 4.0860, Error 0.2704(best: 0.2598)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4410 | Time 16.9877(17.5897) | Bit/dim 3.7018(3.6929) | Xent 0.6604(0.7178) | Loss 29.9363(34.4402) | Error 0.2189(0.2549) Steps 0(0.00) | Grad Norm 16.0944(26.6716) | Total Time 0.00(0.00)\n",
      "Iter 4420 | Time 21.2741(17.7975) | Bit/dim 3.6817(3.6937) | Xent 0.6779(0.7073) | Loss 31.4514(33.3988) | Error 0.2433(0.2518) Steps 0(0.00) | Grad Norm 27.0939(24.6501) | Total Time 0.00(0.00)\n",
      "Iter 4430 | Time 17.5212(17.9397) | Bit/dim 3.7116(3.6946) | Xent 0.8064(0.7154) | Loss 31.8273(32.7636) | Error 0.2644(0.2546) Steps 0(0.00) | Grad Norm 47.6056(27.4589) | Total Time 0.00(0.00)\n",
      "Iter 4440 | Time 17.6923(18.0752) | Bit/dim 3.6694(3.6915) | Xent 0.7091(0.7187) | Loss 31.4598(32.2688) | Error 0.2467(0.2554) Steps 0(0.00) | Grad Norm 39.1091(28.1719) | Total Time 0.00(0.00)\n",
      "Iter 4450 | Time 18.5512(17.9111) | Bit/dim 3.6847(3.6898) | Xent 0.6558(0.7130) | Loss 30.9149(31.7265) | Error 0.2344(0.2538) Steps 0(0.00) | Grad Norm 25.9915(27.1965) | Total Time 0.00(0.00)\n",
      "validating...\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_rl.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_6_rlw_0_05_run1 --resume ../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_6_rlw_0_05_run1/current_checkpt.pth --seed 1 --lr 0.001 --conditional True --controlled_tol False --train_mode semisup --log_freq 10 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5 --scale_fac 1.0 --scale_std 6.0 --rl-weight 0.05\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
