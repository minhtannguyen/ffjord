{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='4,5,6,7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_2cond_nosep.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import torch.utils.data as data\n",
      "from torch.utils.data import Dataset\n",
      "\n",
      "from PIL import Image\n",
      "import os.path\n",
      "import errno\n",
      "import codecs\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"colormnist\", \"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "parser.add_argument(\"--y_class\", type=int, default=10)\n",
      "parser.add_argument(\"--y_color\", type=int, default=10)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "if args.controlled_tol:\n",
      "    import lib.odenvp_conditional_tol as odenvp\n",
      "else:\n",
      "    import lib.odenvp_conditional_2cond as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "class ColorMNIST(data.Dataset):\n",
      "    \"\"\"\n",
      "    ColorMNIST\n",
      "    \"\"\"\n",
      "    urls = [\n",
      "        'http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz',\n",
      "    ]\n",
      "    raw_folder = 'raw'\n",
      "    processed_folder = 'processed'\n",
      "    training_file = 'training.pt'\n",
      "    test_file = 'test.pt'\n",
      "\n",
      "    def __init__(self, root, train=True, transform=None, target_transform=None, download=False):\n",
      "        self.root = os.path.expanduser(root)\n",
      "        self.transform = transform\n",
      "        self.target_transform = target_transform\n",
      "        self.train = train  # training set or test set\n",
      "\n",
      "        if download:\n",
      "            self.download()\n",
      "\n",
      "        if not self._check_exists():\n",
      "            raise RuntimeError('Dataset not found.' +\n",
      "                               ' You can use download=True to download it')\n",
      "\n",
      "        if self.train:\n",
      "            self.train_data, self.train_labels = torch.load(\n",
      "                os.path.join(self.root, self.processed_folder, self.training_file))\n",
      "            \n",
      "            self.train_data = np.tile(self.train_data[:, :, :, np.newaxis], 3)\n",
      "        else:\n",
      "            self.test_data, self.test_labels = torch.load(\n",
      "                os.path.join(self.root, self.processed_folder, self.test_file))\n",
      "            \n",
      "            self.test_data = np.tile(self.test_data[:, :, :, np.newaxis], 3)\n",
      "        \n",
      "        self.pallette = [[31, 119, 180],\n",
      "                         [255, 127, 14],\n",
      "                         [44, 160, 44],\n",
      "                         [214, 39, 40],\n",
      "                         [148, 103, 189],\n",
      "                         [140, 86, 75],\n",
      "                         [227, 119, 194],\n",
      "                         [127, 127, 127],\n",
      "                         [188, 189, 34],\n",
      "                         [23, 190, 207]]\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        \"\"\"\n",
      "        Args:\n",
      "            index (int): Index\n",
      "\n",
      "        Returns:\n",
      "            tuple: (image, target) where target is index of the target class.\n",
      "        \"\"\"\n",
      "        if self.train:\n",
      "            img, target = self.train_data[index].copy(), self.train_labels[index]\n",
      "        else:\n",
      "            img, target = self.test_data[index].copy(), self.test_labels[index]\n",
      "        \n",
      "        # doing this so that it is consistent with all other datasets\n",
      "        # to return a PIL Image\n",
      "        y_color_digit = np.random.randint(0, args.y_color)\n",
      "        c_digit = self.pallette[y_color_digit]\n",
      "        \n",
      "        img[:, :, 0] = img[:, :, 0] / 255 * c_digit[0]\n",
      "        img[:, :, 1] = img[:, :, 1] / 255 * c_digit[1]\n",
      "        img[:, :, 2] = img[:, :, 2] / 255 * c_digit[2]\n",
      "        \n",
      "        img = Image.fromarray(img)\n",
      "\n",
      "        if self.transform is not None:\n",
      "            img = self.transform(img)\n",
      "\n",
      "        if self.target_transform is not None:\n",
      "            target = self.target_transform(target)\n",
      "\n",
      "        return img, [target,torch.from_numpy(np.array(y_color_digit))]\n",
      "\n",
      "    def __len__(self):\n",
      "        if self.train:\n",
      "            return len(self.train_data)\n",
      "        else:\n",
      "            return len(self.test_data)\n",
      "\n",
      "    def _check_exists(self):\n",
      "        return os.path.exists(os.path.join(self.root, self.processed_folder, self.training_file)) and \\\n",
      "            os.path.exists(os.path.join(self.root, self.processed_folder, self.test_file))\n",
      "\n",
      "    def download(self):\n",
      "        \"\"\"Download the MNIST data if it doesn't exist in processed_folder already.\"\"\"\n",
      "        from six.moves import urllib\n",
      "        import gzip\n",
      "\n",
      "        if self._check_exists():\n",
      "            return\n",
      "\n",
      "        # download files\n",
      "        try:\n",
      "            os.makedirs(os.path.join(self.root, self.raw_folder))\n",
      "            os.makedirs(os.path.join(self.root, self.processed_folder))\n",
      "        except OSError as e:\n",
      "            if e.errno == errno.EEXIST:\n",
      "                pass\n",
      "            else:\n",
      "                raise\n",
      "\n",
      "        for url in self.urls:\n",
      "            print('Downloading ' + url)\n",
      "            data = urllib.request.urlopen(url)\n",
      "            filename = url.rpartition('/')[2]\n",
      "            file_path = os.path.join(self.root, self.raw_folder, filename)\n",
      "            with open(file_path, 'wb') as f:\n",
      "                f.write(data.read())\n",
      "            with open(file_path.replace('.gz', ''), 'wb') as out_f, \\\n",
      "                    gzip.GzipFile(file_path) as zip_f:\n",
      "                out_f.write(zip_f.read())\n",
      "            os.unlink(file_path)\n",
      "\n",
      "        # process and save as torch files\n",
      "        print('Processing...')\n",
      "\n",
      "        training_set = (\n",
      "            read_image_file(os.path.join(self.root, self.raw_folder, 'train-images-idx3-ubyte')),\n",
      "            read_label_file(os.path.join(self.root, self.raw_folder, 'train-labels-idx1-ubyte'))\n",
      "        )\n",
      "        test_set = (\n",
      "            read_image_file(os.path.join(self.root, self.raw_folder, 't10k-images-idx3-ubyte')),\n",
      "            read_label_file(os.path.join(self.root, self.raw_folder, 't10k-labels-idx1-ubyte'))\n",
      "        )\n",
      "        with open(os.path.join(self.root, self.processed_folder, self.training_file), 'wb') as f:\n",
      "            torch.save(training_set, f)\n",
      "        with open(os.path.join(self.root, self.processed_folder, self.test_file), 'wb') as f:\n",
      "            torch.save(test_set, f)\n",
      "\n",
      "        print('Done!')\n",
      "\n",
      "    def __repr__(self):\n",
      "        fmt_str = 'Dataset ' + self.__class__.__name__ + '\\n'\n",
      "        fmt_str += '    Number of datapoints: {}\\n'.format(self.__len__())\n",
      "        tmp = 'train' if self.train is True else 'test'\n",
      "        fmt_str += '    Split: {}\\n'.format(tmp)\n",
      "        fmt_str += '    Root Location: {}\\n'.format(self.root)\n",
      "        tmp = '    Transforms (if any): '\n",
      "        fmt_str += '{0}{1}\\n'.format(tmp, self.transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
      "        tmp = '    Target Transforms (if any): '\n",
      "        fmt_str += '{0}{1}'.format(tmp, self.target_transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
      "        return fmt_str\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"colormnist\":\n",
      "        im_dim = 3\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = ColorMNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = ColorMNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, y_color, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "    y_onehot_color = thops.onehot(y_color, num_classes=model.module.y_color).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "    mean_color, logs_color = model.module._prior_color(y_onehot_color)\n",
      "    \n",
      "    mean_sup = mean + mean_color\n",
      "    logs_sup = 0.5 * torch.log(torch.exp(2.*logs) + torch.exp(2.*logs_color))\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean_sup, logs_sup, z[:, 0:dim_sup]).view(-1,1)\n",
      "\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "    \n",
      "    y_logits_color = model.module.project_color(zsup)\n",
      "    loss_xent_color = model.module.loss_class(y_logits_color, y_color.to(x.get_device()))\n",
      "    y_color_predicted = np.argmax(y_logits_color.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, loss_xent_color, y_predicted, y_color_predicted\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,\n",
      "            y_class = args.y_class,\n",
      "            y_color = args.y_color)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    xent_color_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    error_color_meter = utils.RunningAverageMeter(0.97)\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        xent_color_meter.set(checkpt['xent_train_color'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        error_color_meter.set(checkpt['error_train_color'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        \n",
      "        fixed_y_color = torch.from_numpy(np.arange(model.module.y_color)).repeat(model.module.y_color).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot_color = thops.onehot(fixed_y_color, num_classes=model.module.y_color)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            mean_color, logs_color = model.module._prior_color(fixed_y_onehot_color)\n",
      "            mean_sup = mean + mean_color\n",
      "            logs_sup = 0.5 * torch.log(torch.exp(2.*logs) + torch.exp(2.*logs_color))\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean_sup, logs_sup)\n",
      "            dim_unsup = np.prod(data_shape) - np.prod(fixed_z_sup.shape[1:])\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    best_error_score_color = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y_all) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            \n",
      "            y = y_all[0]\n",
      "            y_color = y_all[1]\n",
      "            \n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, loss_xent_color, y_predicted, y_color_predicted = compute_bits_per_dim_conditional(x, y, y_color, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * 0.5 * (loss_xent + loss_xent_color)\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  0.5 * (loss_xent + loss_xent_color)\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy()) \n",
      "                error_score_color = 1. - np.mean(y_color_predicted.astype(int) == y_color.numpy())\n",
      "                \n",
      "            else:\n",
      "                loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, loss_xent_color, error_score, error_score_color = loss, 0., 0., 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            loss.backward()\n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "                xent_color_meter.update(loss_xent_color.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "                xent_color_meter.update(loss_xent_color)\n",
      "            error_meter.update(error_score)\n",
      "            error_color_meter.update(error_score_color)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('xent_color', {'train_iter': xent_color_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('error_color', {'train_iter': error_color_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Xent Color {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) | Error Color {:.4f}({:.4f}) |\"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, xent_color_meter.val, xent_color_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, error_color_meter.val, error_color_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent_color', {'train_epoch': xent_color_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('error_color', {'train_epoch': error_color_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses_xent_color = []; losses = []\n",
      "                total_correct = 0\n",
      "                total_correct_color = 0\n",
      "                \n",
      "                for (x, y_all) in test_loader:\n",
      "                    y = y_all[0]\n",
      "                    y_color = y_all[1]\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, loss_xent_color, y_predicted, y_color_predicted = compute_bits_per_dim_conditional(x, y, y_color, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * 0.5 * (loss_xent + loss_xent_color)\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  0.5 * (loss_xent + loss_xent_color)\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                        total_correct_color += np.sum(y_color_predicted.astype(int) == y_color.numpy())\n",
      "                    else:\n",
      "                        loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent, loss_xent_color = loss, 0., 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                        losses_xent_color.append(loss_xent_color.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                        losses_xent_color.append(loss_xent_color)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss_xent_color = np.mean(losses_xent_color); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                error_score_color =  1. - total_correct_color / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('xent_color', {'validation': loss_xent_color}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                writer.add_scalars('error_color', {'validation': error_score_color}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Xent Color {:.4f}. Loss {:.4f}, Error {:.4f}(best: {:.4f}), Error Color {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss_xent_color, loss, error_score, best_error_score, error_score_color, best_error_score_color)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"error_color\": error_score_color,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"xent_color\": loss_xent_color,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"best_error_score_color\": best_error_score_color,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"error_train_color\": error_color_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"xent_train_color\": xent_color_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"error_color\": error_score_color,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"xent_color\": loss_xent_color,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"best_error_score_color\": best_error_score_color,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"error_train_color\": error_color_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"xent_train_color\": xent_color_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"error_color\": error_score_color,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"xent_color\": loss_xent_color,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"best_error_score_color\": best_error_score_color,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"error_train_color\": error_color_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"xent_train_color\": xent_color_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"error_color\": error_score_color,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"xent_color\": loss_xent_color,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"best_error_score_color\": best_error_score_color,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"error_train_color\": error_color_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"xent_train_color\": xent_color_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "                    if error_score_color < best_error_score_color:\n",
      "                        best_error_score_color = error_score_color\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"error_color\": error_score_color,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"xent_color\": loss_xent_color,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"best_error_score_color\": best_error_score_color,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"error_train_color\": error_color_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"xent_train_color\": xent_color_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_color_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, condition_ratio=0.5, conditional=True, controlled_tol=False, conv=True, data='colormnist', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.0001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume='../experiments_published/cnf_conditional_disentangle_colormnist_bs900_sratio_1_2th_drop_0_5_2cond_linear_nosep_run1/current_checkpt.pth', rtol=1e-05, save='../experiments_published/cnf_conditional_disentangle_colormnist_bs900_sratio_1_2th_drop_0_5_2cond_linear_nosep_run1', seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5, y_class=10, y_color=10)\n",
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=2352, bias=True)\n",
      "  (project_ycond_color): LinearZeros(in_features=10, out_features=2352, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1176, out_features=10, bias=True)\n",
      "  (project_color): LinearZeros(in_features=1176, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      "  (dropout_color): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 973670\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc.py:286: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "Iter 2910 | Time 44.5967(46.0124) | Bit/dim 1.1572(1.1692) | Xent 0.3009(0.3025) | Xent Color 0.0473(0.0530) | Loss 1.2442(1.2581) | Error 0.0911(0.0978) | Error Color 0.0144(0.0120) |Steps 548(550.58) | Grad Norm 1.0218(2.5475) | Total Time 10.00(10.00)\n",
      "Iter 2920 | Time 48.2202(45.7497) | Bit/dim 1.1555(1.1639) | Xent 0.2801(0.2996) | Xent Color 0.0388(0.0502) | Loss 1.2352(1.2513) | Error 0.0911(0.0967) | Error Color 0.0044(0.0110) |Steps 554(550.70) | Grad Norm 1.4543(2.2322) | Total Time 10.00(10.00)\n",
      "Iter 2930 | Time 46.5249(45.6879) | Bit/dim 1.1635(1.1603) | Xent 0.3066(0.2966) | Xent Color 0.0461(0.0492) | Loss 1.2516(1.2467) | Error 0.1044(0.0969) | Error Color 0.0100(0.0108) |Steps 554(550.46) | Grad Norm 1.4852(1.9751) | Total Time 10.00(10.00)\n",
      "Iter 2940 | Time 46.2764(45.4426) | Bit/dim 1.1496(1.1566) | Xent 0.3212(0.3008) | Xent Color 0.0371(0.0473) | Loss 1.2391(1.2436) | Error 0.1044(0.0973) | Error Color 0.0078(0.0102) |Steps 554(550.61) | Grad Norm 0.8782(1.7086) | Total Time 10.00(10.00)\n",
      "Iter 2950 | Time 44.2487(45.4179) | Bit/dim 1.1425(1.1545) | Xent 0.2784(0.2966) | Xent Color 0.0385(0.0461) | Loss 1.2217(1.2402) | Error 0.1000(0.0961) | Error Color 0.0089(0.0099) |Steps 548(551.29) | Grad Norm 0.8048(1.5010) | Total Time 10.00(10.00)\n",
      "Iter 2960 | Time 44.3361(45.4443) | Bit/dim 1.1628(1.1524) | Xent 0.2950(0.2932) | Xent Color 0.0610(0.0461) | Loss 1.2518(1.2372) | Error 0.0956(0.0944) | Error Color 0.0133(0.0097) |Steps 554(551.10) | Grad Norm 1.3438(1.4005) | Total Time 10.00(10.00)\n",
      "Iter 2970 | Time 45.6109(45.4277) | Bit/dim 1.1601(1.1518) | Xent 0.2940(0.2909) | Xent Color 0.0529(0.0457) | Loss 1.2468(1.2359) | Error 0.1067(0.0945) | Error Color 0.0122(0.0096) |Steps 548(550.93) | Grad Norm 0.8826(1.3008) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0045 | Time 134.0840, Epoch Time 3176.1041(2688.0677), Bit/dim 1.1477(best: inf), Xent 0.1794, Xent Color 0.0125. Loss 1.1957, Error 0.0550(best: inf), Error Color 0.0006(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2980 | Time 45.1356(45.3106) | Bit/dim 1.1374(1.1496) | Xent 0.2689(0.2931) | Xent Color 0.0429(0.0454) | Loss 1.2154(1.2343) | Error 0.0822(0.0939) | Error Color 0.0044(0.0093) |Steps 554(550.78) | Grad Norm 1.1570(1.2420) | Total Time 10.00(10.00)\n",
      "Iter 2990 | Time 46.5048(45.0883) | Bit/dim 1.1622(1.1493) | Xent 0.2589(0.2946) | Xent Color 0.0383(0.0442) | Loss 1.2365(1.2340) | Error 0.0811(0.0936) | Error Color 0.0067(0.0090) |Steps 554(550.71) | Grad Norm 1.1555(1.1829) | Total Time 10.00(10.00)\n",
      "Iter 3000 | Time 45.0842(45.0781) | Bit/dim 1.1300(1.1475) | Xent 0.2805(0.2925) | Xent Color 0.0500(0.0441) | Loss 1.2126(1.2316) | Error 0.0900(0.0930) | Error Color 0.0122(0.0090) |Steps 554(550.80) | Grad Norm 0.8116(1.1430) | Total Time 10.00(10.00)\n",
      "Iter 3010 | Time 43.9436(45.0612) | Bit/dim 1.1348(1.1457) | Xent 0.2813(0.2896) | Xent Color 0.0340(0.0439) | Loss 1.2137(1.2291) | Error 0.0978(0.0928) | Error Color 0.0056(0.0090) |Steps 554(551.34) | Grad Norm 0.6722(1.0795) | Total Time 10.00(10.00)\n",
      "Iter 3020 | Time 45.4998(44.9374) | Bit/dim 1.1479(1.1462) | Xent 0.2922(0.2895) | Xent Color 0.0414(0.0440) | Loss 1.2313(1.2296) | Error 0.0933(0.0936) | Error Color 0.0100(0.0089) |Steps 554(551.59) | Grad Norm 0.8282(1.0215) | Total Time 10.00(10.00)\n",
      "Iter 3030 | Time 43.9581(44.9722) | Bit/dim 1.1578(1.1463) | Xent 0.2399(0.2904) | Xent Color 0.0441(0.0434) | Loss 1.2288(1.2298) | Error 0.0744(0.0941) | Error Color 0.0100(0.0087) |Steps 554(551.63) | Grad Norm 0.8315(0.9837) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0046 | Time 122.3949, Epoch Time 3108.4863(2700.6803), Bit/dim 1.1433(best: 1.1477), Xent 0.1778, Xent Color 0.0122. Loss 1.1908, Error 0.0552(best: 0.0550), Error Color 0.0003(best: 0.0006)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3040 | Time 46.9544(45.0666) | Bit/dim 1.1404(1.1454) | Xent 0.2984(0.2899) | Xent Color 0.0455(0.0435) | Loss 1.2264(1.2288) | Error 0.0956(0.0938) | Error Color 0.0111(0.0090) |Steps 554(551.80) | Grad Norm 0.8716(0.9840) | Total Time 10.00(10.00)\n",
      "Iter 3050 | Time 43.3667(44.8255) | Bit/dim 1.1426(1.1445) | Xent 0.2917(0.2843) | Xent Color 0.0442(0.0427) | Loss 1.2266(1.2263) | Error 0.0911(0.0916) | Error Color 0.0111(0.0088) |Steps 548(551.41) | Grad Norm 1.8615(0.9949) | Total Time 10.00(10.00)\n",
      "Iter 3060 | Time 43.1283(44.8652) | Bit/dim 1.1420(1.1439) | Xent 0.2902(0.2886) | Xent Color 0.0465(0.0431) | Loss 1.2261(1.2268) | Error 0.0978(0.0927) | Error Color 0.0100(0.0091) |Steps 554(552.21) | Grad Norm 0.6077(0.9994) | Total Time 10.00(10.00)\n",
      "Iter 3070 | Time 46.9164(45.0094) | Bit/dim 1.1399(1.1435) | Xent 0.2629(0.2879) | Xent Color 0.0395(0.0431) | Loss 1.2155(1.2263) | Error 0.0900(0.0931) | Error Color 0.0100(0.0089) |Steps 560(552.69) | Grad Norm 1.0449(0.9964) | Total Time 10.00(10.00)\n",
      "Iter 3080 | Time 47.0530(44.9995) | Bit/dim 1.1411(1.1426) | Xent 0.2491(0.2873) | Xent Color 0.0352(0.0430) | Loss 1.2121(1.2252) | Error 0.0789(0.0925) | Error Color 0.0067(0.0087) |Steps 548(552.42) | Grad Norm 0.8691(1.0205) | Total Time 10.00(10.00)\n",
      "Iter 3090 | Time 45.2751(45.0260) | Bit/dim 1.1440(1.1428) | Xent 0.2873(0.2871) | Xent Color 0.0502(0.0433) | Loss 1.2284(1.2254) | Error 0.0956(0.0924) | Error Color 0.0100(0.0089) |Steps 554(552.68) | Grad Norm 2.1109(1.0246) | Total Time 10.00(10.00)\n",
      "Iter 3100 | Time 42.2916(45.2221) | Bit/dim 1.1286(1.1405) | Xent 0.2406(0.2866) | Xent Color 0.0390(0.0426) | Loss 1.1985(1.2228) | Error 0.0822(0.0925) | Error Color 0.0078(0.0087) |Steps 554(552.85) | Grad Norm 1.1728(1.0303) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0047 | Time 125.8068, Epoch Time 3124.4851(2713.3944), Bit/dim 1.1403(best: 1.1433), Xent 0.1772, Xent Color 0.0119. Loss 1.1876, Error 0.0544(best: 0.0550), Error Color 0.0003(best: 0.0003)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3110 | Time 45.6122(45.1262) | Bit/dim 1.1341(1.1403) | Xent 0.2838(0.2851) | Xent Color 0.0347(0.0423) | Loss 1.2138(1.2222) | Error 0.0911(0.0915) | Error Color 0.0056(0.0085) |Steps 554(553.00) | Grad Norm 0.9362(1.0818) | Total Time 10.00(10.00)\n",
      "Iter 3120 | Time 47.9597(45.3130) | Bit/dim 1.1524(1.1403) | Xent 0.2709(0.2853) | Xent Color 0.0351(0.0422) | Loss 1.2289(1.2221) | Error 0.0778(0.0916) | Error Color 0.0033(0.0085) |Steps 554(552.65) | Grad Norm 0.7870(1.0499) | Total Time 10.00(10.00)\n",
      "Iter 3130 | Time 45.7955(45.2156) | Bit/dim 1.1335(1.1384) | Xent 0.2836(0.2851) | Xent Color 0.0365(0.0419) | Loss 1.2135(1.2202) | Error 0.0833(0.0916) | Error Color 0.0044(0.0081) |Steps 554(552.06) | Grad Norm 1.0482(0.9932) | Total Time 10.00(10.00)\n",
      "Iter 3140 | Time 44.9203(45.4255) | Bit/dim 1.1259(1.1375) | Xent 0.2501(0.2800) | Xent Color 0.0434(0.0422) | Loss 1.1993(1.2180) | Error 0.0822(0.0895) | Error Color 0.0111(0.0085) |Steps 554(551.77) | Grad Norm 1.3702(0.9904) | Total Time 10.00(10.00)\n",
      "Iter 3150 | Time 42.9576(45.4013) | Bit/dim 1.1269(1.1372) | Xent 0.3036(0.2819) | Xent Color 0.0410(0.0419) | Loss 1.2130(1.2182) | Error 0.0933(0.0901) | Error Color 0.0067(0.0085) |Steps 548(552.23) | Grad Norm 0.8831(0.9665) | Total Time 10.00(10.00)\n",
      "Iter 3160 | Time 46.8467(45.3908) | Bit/dim 1.1454(1.1375) | Xent 0.2627(0.2798) | Xent Color 0.0422(0.0416) | Loss 1.2216(1.2179) | Error 0.0833(0.0899) | Error Color 0.0089(0.0083) |Steps 554(552.56) | Grad Norm 0.8109(0.9480) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0048 | Time 126.4597, Epoch Time 3152.3903(2726.5643), Bit/dim 1.1365(best: 1.1403), Xent 0.1741, Xent Color 0.0120. Loss 1.1831, Error 0.0537(best: 0.0544), Error Color 0.0004(best: 0.0003)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3170 | Time 44.5135(45.5262) | Bit/dim 1.1191(1.1371) | Xent 0.2848(0.2785) | Xent Color 0.0508(0.0426) | Loss 1.2031(1.2174) | Error 0.0867(0.0891) | Error Color 0.0111(0.0085) |Steps 554(553.25) | Grad Norm 0.8859(0.9763) | Total Time 10.00(10.00)\n",
      "Iter 3180 | Time 47.2692(45.7274) | Bit/dim 1.1407(1.1369) | Xent 0.2846(0.2771) | Xent Color 0.0306(0.0411) | Loss 1.2195(1.2164) | Error 0.0911(0.0887) | Error Color 0.0033(0.0080) |Steps 566(553.49) | Grad Norm 0.7260(0.9425) | Total Time 10.00(10.00)\n",
      "Iter 3190 | Time 46.9732(45.7544) | Bit/dim 1.1344(1.1364) | Xent 0.2892(0.2789) | Xent Color 0.0429(0.0410) | Loss 1.2175(1.2164) | Error 0.0822(0.0878) | Error Color 0.0111(0.0080) |Steps 554(553.76) | Grad Norm 0.8218(0.9301) | Total Time 10.00(10.00)\n",
      "Iter 3200 | Time 46.7898(45.7587) | Bit/dim 1.1402(1.1366) | Xent 0.3216(0.2817) | Xent Color 0.0417(0.0411) | Loss 1.2311(1.2173) | Error 0.1011(0.0896) | Error Color 0.0100(0.0082) |Steps 554(553.45) | Grad Norm 0.7538(0.9847) | Total Time 10.00(10.00)\n",
      "Iter 3210 | Time 45.6594(45.8780) | Bit/dim 1.1238(1.1352) | Xent 0.2737(0.2838) | Xent Color 0.0378(0.0418) | Loss 1.2017(1.2166) | Error 0.0789(0.0900) | Error Color 0.0089(0.0087) |Steps 554(553.82) | Grad Norm 0.5778(1.0222) | Total Time 10.00(10.00)\n",
      "Iter 3220 | Time 42.9285(45.7321) | Bit/dim 1.1225(1.1344) | Xent 0.2540(0.2857) | Xent Color 0.0360(0.0413) | Loss 1.1949(1.2161) | Error 0.0833(0.0911) | Error Color 0.0078(0.0085) |Steps 554(554.49) | Grad Norm 1.3265(1.0508) | Total Time 10.00(10.00)\n",
      "Iter 3230 | Time 46.7997(45.8323) | Bit/dim 1.1366(1.1342) | Xent 0.2677(0.2843) | Xent Color 0.0407(0.0415) | Loss 1.2137(1.2156) | Error 0.0711(0.0907) | Error Color 0.0078(0.0087) |Steps 554(554.08) | Grad Norm 1.1076(1.0443) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0049 | Time 129.4769, Epoch Time 3182.0773(2740.2297), Bit/dim 1.1330(best: 1.1365), Xent 0.1747, Xent Color 0.0113. Loss 1.1795, Error 0.0554(best: 0.0537), Error Color 0.0006(best: 0.0003)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3240 | Time 45.0388(45.8322) | Bit/dim 1.1273(1.1335) | Xent 0.2546(0.2831) | Xent Color 0.0461(0.0416) | Loss 1.2025(1.2147) | Error 0.0867(0.0899) | Error Color 0.0111(0.0088) |Steps 566(554.27) | Grad Norm 0.5965(1.0370) | Total Time 10.00(10.00)\n",
      "Iter 3250 | Time 46.4808(45.6311) | Bit/dim 1.1347(1.1339) | Xent 0.3284(0.2806) | Xent Color 0.0495(0.0422) | Loss 1.2292(1.2146) | Error 0.1078(0.0888) | Error Color 0.0122(0.0090) |Steps 548(554.15) | Grad Norm 0.9631(1.0300) | Total Time 10.00(10.00)\n",
      "Iter 3260 | Time 45.5892(45.5779) | Bit/dim 1.1186(1.1332) | Xent 0.3474(0.2844) | Xent Color 0.0437(0.0421) | Loss 1.2164(1.2148) | Error 0.1178(0.0903) | Error Color 0.0067(0.0087) |Steps 554(555.06) | Grad Norm 1.2551(1.0079) | Total Time 10.00(10.00)\n",
      "Iter 3270 | Time 45.8801(45.6739) | Bit/dim 1.1296(1.1319) | Xent 0.3231(0.2831) | Xent Color 0.0478(0.0415) | Loss 1.2223(1.2130) | Error 0.1111(0.0906) | Error Color 0.0100(0.0087) |Steps 554(554.47) | Grad Norm 0.7198(0.9831) | Total Time 10.00(10.00)\n",
      "Iter 3280 | Time 48.5413(45.7001) | Bit/dim 1.1426(1.1315) | Xent 0.3537(0.2830) | Xent Color 0.0401(0.0413) | Loss 1.2411(1.2126) | Error 0.1089(0.0908) | Error Color 0.0078(0.0083) |Steps 542(553.68) | Grad Norm 1.0452(1.0178) | Total Time 10.00(10.00)\n",
      "Iter 3290 | Time 42.4733(45.5931) | Bit/dim 1.1294(1.1302) | Xent 0.2377(0.2793) | Xent Color 0.0404(0.0412) | Loss 1.1989(1.2103) | Error 0.0744(0.0892) | Error Color 0.0044(0.0083) |Steps 554(553.04) | Grad Norm 0.6691(1.0383) | Total Time 10.00(10.00)\n",
      "Iter 3300 | Time 49.1017(45.6828) | Bit/dim 1.1263(1.1291) | Xent 0.2843(0.2788) | Xent Color 0.0387(0.0408) | Loss 1.2071(1.2090) | Error 0.1022(0.0893) | Error Color 0.0133(0.0082) |Steps 554(553.66) | Grad Norm 0.6526(1.0515) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0050 | Time 125.0130, Epoch Time 3157.1431(2752.7371), Bit/dim 1.1304(best: 1.1330), Xent 0.1718, Xent Color 0.0117. Loss 1.1763, Error 0.0544(best: 0.0537), Error Color 0.0004(best: 0.0003)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3310 | Time 43.3562(45.7540) | Bit/dim 1.1224(1.1296) | Xent 0.2479(0.2754) | Xent Color 0.0370(0.0399) | Loss 1.1936(1.2085) | Error 0.0822(0.0885) | Error Color 0.0078(0.0078) |Steps 548(553.64) | Grad Norm 0.8255(1.0694) | Total Time 10.00(10.00)\n",
      "Iter 3320 | Time 45.6126(45.8110) | Bit/dim 1.1075(1.1279) | Xent 0.3089(0.2768) | Xent Color 0.0503(0.0402) | Loss 1.1973(1.2071) | Error 0.1044(0.0889) | Error Color 0.0100(0.0080) |Steps 560(553.63) | Grad Norm 1.8501(1.0578) | Total Time 10.00(10.00)\n",
      "Iter 3330 | Time 45.0828(45.9875) | Bit/dim 1.1243(1.1271) | Xent 0.2709(0.2772) | Xent Color 0.0357(0.0399) | Loss 1.2009(1.2063) | Error 0.0800(0.0887) | Error Color 0.0033(0.0079) |Steps 566(554.77) | Grad Norm 1.2836(1.0729) | Total Time 10.00(10.00)\n",
      "Iter 3340 | Time 44.2785(45.7894) | Bit/dim 1.1331(1.1274) | Xent 0.3402(0.2818) | Xent Color 0.0438(0.0398) | Loss 1.2292(1.2078) | Error 0.0900(0.0894) | Error Color 0.0089(0.0077) |Steps 554(554.28) | Grad Norm 1.0037(1.0771) | Total Time 10.00(10.00)\n",
      "Iter 3350 | Time 45.8642(45.7741) | Bit/dim 1.1277(1.1270) | Xent 0.2394(0.2824) | Xent Color 0.0356(0.0398) | Loss 1.1965(1.2075) | Error 0.0789(0.0899) | Error Color 0.0067(0.0074) |Steps 560(553.52) | Grad Norm 0.8498(1.0794) | Total Time 10.00(10.00)\n",
      "Iter 3360 | Time 45.8910(45.8543) | Bit/dim 1.1230(1.1270) | Xent 0.2841(0.2793) | Xent Color 0.0378(0.0402) | Loss 1.2035(1.2069) | Error 0.0922(0.0887) | Error Color 0.0089(0.0077) |Steps 542(553.18) | Grad Norm 0.7075(1.1233) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0051 | Time 125.9703, Epoch Time 3173.6875(2765.3656), Bit/dim 1.1256(best: 1.1304), Xent 0.1697, Xent Color 0.0109. Loss 1.1708, Error 0.0509(best: 0.0537), Error Color 0.0003(best: 0.0003)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3370 | Time 44.7341(45.5679) | Bit/dim 1.1152(1.1264) | Xent 0.2440(0.2772) | Xent Color 0.0369(0.0395) | Loss 1.1854(1.2056) | Error 0.0811(0.0883) | Error Color 0.0056(0.0075) |Steps 542(553.18) | Grad Norm 1.6788(1.1036) | Total Time 10.00(10.00)\n",
      "Iter 3380 | Time 46.7659(45.7575) | Bit/dim 1.1162(1.1261) | Xent 0.2486(0.2742) | Xent Color 0.0390(0.0392) | Loss 1.1881(1.2044) | Error 0.0756(0.0866) | Error Color 0.0044(0.0074) |Steps 554(553.63) | Grad Norm 1.7640(1.1069) | Total Time 10.00(10.00)\n",
      "Iter 3390 | Time 46.6070(45.7991) | Bit/dim 1.1158(1.1251) | Xent 0.2858(0.2747) | Xent Color 0.0472(0.0396) | Loss 1.1991(1.2036) | Error 0.0989(0.0875) | Error Color 0.0100(0.0077) |Steps 548(553.71) | Grad Norm 1.3845(1.0863) | Total Time 10.00(10.00)\n",
      "Iter 3400 | Time 44.0705(45.8000) | Bit/dim 1.1463(1.1237) | Xent 0.2912(0.2775) | Xent Color 0.0412(0.0393) | Loss 1.2294(1.2029) | Error 0.0878(0.0883) | Error Color 0.0078(0.0078) |Steps 548(553.03) | Grad Norm 2.0637(1.1572) | Total Time 10.00(10.00)\n",
      "Iter 3410 | Time 45.7092(45.7520) | Bit/dim 1.1156(1.1226) | Xent 0.2736(0.2765) | Xent Color 0.0362(0.0391) | Loss 1.1931(1.2015) | Error 0.0867(0.0887) | Error Color 0.0067(0.0078) |Steps 560(553.17) | Grad Norm 1.8319(1.1802) | Total Time 10.00(10.00)\n",
      "Iter 3420 | Time 43.8327(45.6612) | Bit/dim 1.1298(1.1235) | Xent 0.2905(0.2768) | Xent Color 0.0441(0.0394) | Loss 1.2135(1.2025) | Error 0.0911(0.0885) | Error Color 0.0133(0.0079) |Steps 560(554.36) | Grad Norm 0.6226(1.1224) | Total Time 10.00(10.00)\n",
      "Iter 3430 | Time 47.8108(45.9882) | Bit/dim 1.1267(1.1234) | Xent 0.2743(0.2771) | Xent Color 0.0370(0.0393) | Loss 1.2045(1.2025) | Error 0.0878(0.0884) | Error Color 0.0056(0.0076) |Steps 542(552.70) | Grad Norm 0.9842(1.1355) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0052 | Time 127.1726, Epoch Time 3184.8458(2777.9500), Bit/dim 1.1216(best: 1.1256), Xent 0.1702, Xent Color 0.0104. Loss 1.1668, Error 0.0525(best: 0.0509), Error Color 0.0006(best: 0.0003)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3440 | Time 46.3287(46.2608) | Bit/dim 1.1156(1.1226) | Xent 0.2521(0.2750) | Xent Color 0.0322(0.0392) | Loss 1.1867(1.2011) | Error 0.0811(0.0883) | Error Color 0.0044(0.0075) |Steps 554(552.66) | Grad Norm 0.9748(1.0882) | Total Time 10.00(10.00)\n",
      "Iter 3450 | Time 43.4028(45.9704) | Bit/dim 1.1306(1.1219) | Xent 0.2527(0.2770) | Xent Color 0.0349(0.0392) | Loss 1.2025(1.2010) | Error 0.0789(0.0887) | Error Color 0.0044(0.0076) |Steps 542(552.19) | Grad Norm 0.5491(1.0438) | Total Time 10.00(10.00)\n",
      "Iter 3460 | Time 43.3268(45.7395) | Bit/dim 1.1207(1.1223) | Xent 0.2416(0.2761) | Xent Color 0.0351(0.0393) | Loss 1.1899(1.2011) | Error 0.0856(0.0885) | Error Color 0.0044(0.0078) |Steps 542(552.17) | Grad Norm 1.6304(1.1038) | Total Time 10.00(10.00)\n",
      "Iter 3470 | Time 45.1644(45.9369) | Bit/dim 1.1210(1.1220) | Xent 0.2557(0.2742) | Xent Color 0.0381(0.0385) | Loss 1.1944(1.2002) | Error 0.0833(0.0876) | Error Color 0.0067(0.0076) |Steps 548(552.05) | Grad Norm 0.9837(1.1228) | Total Time 10.00(10.00)\n",
      "Iter 3480 | Time 42.9167(46.0052) | Bit/dim 1.1204(1.1210) | Xent 0.2953(0.2737) | Xent Color 0.0455(0.0386) | Loss 1.2056(1.1991) | Error 0.0922(0.0874) | Error Color 0.0089(0.0077) |Steps 548(552.02) | Grad Norm 1.0178(1.1166) | Total Time 10.00(10.00)\n",
      "Iter 3490 | Time 46.6352(46.1318) | Bit/dim 1.1066(1.1200) | Xent 0.2575(0.2726) | Xent Color 0.0398(0.0382) | Loss 1.1809(1.1978) | Error 0.0778(0.0872) | Error Color 0.0067(0.0078) |Steps 542(551.55) | Grad Norm 0.5008(1.1239) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0053 | Time 123.2310, Epoch Time 3184.8332(2790.1565), Bit/dim 1.1190(best: 1.1216), Xent 0.1652, Xent Color 0.0104. Loss 1.1628, Error 0.0500(best: 0.0509), Error Color 0.0004(best: 0.0003)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3500 | Time 44.7238(46.1185) | Bit/dim 1.1203(1.1184) | Xent 0.2725(0.2728) | Xent Color 0.0393(0.0377) | Loss 1.1982(1.1960) | Error 0.1000(0.0878) | Error Color 0.0078(0.0074) |Steps 560(551.60) | Grad Norm 0.9378(1.1206) | Total Time 10.00(10.00)\n",
      "Iter 3510 | Time 45.8421(46.0620) | Bit/dim 1.1186(1.1183) | Xent 0.2782(0.2722) | Xent Color 0.0316(0.0379) | Loss 1.1960(1.1959) | Error 0.0967(0.0872) | Error Color 0.0089(0.0077) |Steps 548(552.17) | Grad Norm 1.7840(1.1681) | Total Time 10.00(10.00)\n",
      "Iter 3520 | Time 44.6847(46.0255) | Bit/dim 1.1040(1.1177) | Xent 0.2469(0.2708) | Xent Color 0.0365(0.0377) | Loss 1.1748(1.1948) | Error 0.0833(0.0863) | Error Color 0.0044(0.0073) |Steps 548(552.37) | Grad Norm 0.9416(1.1893) | Total Time 10.00(10.00)\n",
      "Iter 3530 | Time 48.9343(46.1974) | Bit/dim 1.1197(1.1177) | Xent 0.2870(0.2711) | Xent Color 0.0382(0.0368) | Loss 1.2010(1.1947) | Error 0.0878(0.0860) | Error Color 0.0056(0.0070) |Steps 554(552.18) | Grad Norm 1.1628(1.1603) | Total Time 10.00(10.00)\n",
      "Iter 3540 | Time 46.2458(46.4257) | Bit/dim 1.1087(1.1163) | Xent 0.2696(0.2707) | Xent Color 0.0346(0.0364) | Loss 1.1848(1.1931) | Error 0.0867(0.0861) | Error Color 0.0033(0.0068) |Steps 554(553.26) | Grad Norm 1.6925(1.1277) | Total Time 10.00(10.00)\n",
      "Iter 3550 | Time 43.0049(46.2358) | Bit/dim 1.1059(1.1155) | Xent 0.2575(0.2711) | Xent Color 0.0435(0.0377) | Loss 1.1812(1.1927) | Error 0.0867(0.0866) | Error Color 0.0111(0.0073) |Steps 560(553.45) | Grad Norm 2.1461(1.1976) | Total Time 10.00(10.00)\n",
      "Iter 3560 | Time 46.4615(46.0927) | Bit/dim 1.1119(1.1148) | Xent 0.2446(0.2680) | Xent Color 0.0343(0.0374) | Loss 1.1817(1.1912) | Error 0.0767(0.0859) | Error Color 0.0067(0.0073) |Steps 542(553.15) | Grad Norm 1.8430(1.2632) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0054 | Time 125.1690, Epoch Time 3197.0599(2802.3636), Bit/dim 1.1150(best: 1.1190), Xent 0.1640, Xent Color 0.0100. Loss 1.1585, Error 0.0513(best: 0.0500), Error Color 0.0002(best: 0.0003)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3570 | Time 43.9438(45.9681) | Bit/dim 1.1159(1.1140) | Xent 0.3006(0.2700) | Xent Color 0.0343(0.0377) | Loss 1.1996(1.1910) | Error 0.0989(0.0877) | Error Color 0.0033(0.0075) |Steps 560(554.01) | Grad Norm 1.0305(1.2015) | Total Time 10.00(10.00)\n",
      "Iter 3580 | Time 44.6398(45.6231) | Bit/dim 1.1127(1.1136) | Xent 0.2162(0.2681) | Xent Color 0.0332(0.0371) | Loss 1.1751(1.1899) | Error 0.0722(0.0874) | Error Color 0.0033(0.0073) |Steps 560(553.40) | Grad Norm 0.7903(1.2119) | Total Time 10.00(10.00)\n",
      "Iter 3590 | Time 43.7932(45.3900) | Bit/dim 1.1128(1.1124) | Xent 0.2475(0.2633) | Xent Color 0.0377(0.0370) | Loss 1.1841(1.1875) | Error 0.0833(0.0855) | Error Color 0.0078(0.0073) |Steps 548(553.06) | Grad Norm 0.7784(1.1573) | Total Time 10.00(10.00)\n",
      "Iter 3600 | Time 45.9871(45.4766) | Bit/dim 1.1105(1.1117) | Xent 0.2645(0.2668) | Xent Color 0.0331(0.0370) | Loss 1.1849(1.1877) | Error 0.0900(0.0866) | Error Color 0.0044(0.0070) |Steps 554(552.80) | Grad Norm 0.6511(1.1280) | Total Time 10.00(10.00)\n",
      "Iter 3610 | Time 46.9337(45.6041) | Bit/dim 1.1079(1.1113) | Xent 0.2656(0.2686) | Xent Color 0.0376(0.0371) | Loss 1.1837(1.1877) | Error 0.0856(0.0869) | Error Color 0.0067(0.0068) |Steps 542(551.58) | Grad Norm 1.6428(1.2082) | Total Time 10.00(10.00)\n",
      "Iter 3620 | Time 46.0727(45.6552) | Bit/dim 1.1140(1.1121) | Xent 0.3108(0.2717) | Xent Color 0.0368(0.0367) | Loss 1.2009(1.1892) | Error 0.0967(0.0880) | Error Color 0.0044(0.0066) |Steps 542(552.31) | Grad Norm 0.7215(1.1703) | Total Time 10.00(10.00)\n",
      "Iter 3630 | Time 44.6184(45.5666) | Bit/dim 1.1168(1.1129) | Xent 0.2531(0.2712) | Xent Color 0.0387(0.0365) | Loss 1.1898(1.1898) | Error 0.0811(0.0873) | Error Color 0.0067(0.0064) |Steps 548(551.97) | Grad Norm 0.9178(1.0993) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0055 | Time 126.7491, Epoch Time 3141.6979(2812.5436), Bit/dim 1.1121(best: 1.1150), Xent 0.1615, Xent Color 0.0104. Loss 1.1551, Error 0.0494(best: 0.0500), Error Color 0.0004(best: 0.0002)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3640 | Time 44.2269(45.4656) | Bit/dim 1.1202(1.1120) | Xent 0.2574(0.2718) | Xent Color 0.0365(0.0360) | Loss 1.1936(1.1889) | Error 0.0811(0.0878) | Error Color 0.0056(0.0062) |Steps 548(551.83) | Grad Norm 1.1260(1.1054) | Total Time 10.00(10.00)\n",
      "Iter 3650 | Time 44.4346(45.4087) | Bit/dim 1.1136(1.1111) | Xent 0.2438(0.2734) | Xent Color 0.0410(0.0366) | Loss 1.1848(1.1886) | Error 0.0856(0.0883) | Error Color 0.0078(0.0067) |Steps 554(552.40) | Grad Norm 1.1634(1.1608) | Total Time 10.00(10.00)\n",
      "Iter 3660 | Time 44.8296(45.5077) | Bit/dim 1.1056(1.1103) | Xent 0.2208(0.2727) | Xent Color 0.0320(0.0366) | Loss 1.1688(1.1876) | Error 0.0600(0.0873) | Error Color 0.0056(0.0069) |Steps 554(552.54) | Grad Norm 0.7295(1.1584) | Total Time 10.00(10.00)\n",
      "Iter 3670 | Time 46.5444(45.6783) | Bit/dim 1.0982(1.1090) | Xent 0.2865(0.2750) | Xent Color 0.0353(0.0364) | Loss 1.1786(1.1868) | Error 0.0844(0.0880) | Error Color 0.0033(0.0068) |Steps 554(552.91) | Grad Norm 0.8917(1.0976) | Total Time 10.00(10.00)\n",
      "Iter 3680 | Time 46.5785(45.7457) | Bit/dim 1.0992(1.1094) | Xent 0.2468(0.2717) | Xent Color 0.0385(0.0367) | Loss 1.1705(1.1866) | Error 0.0833(0.0881) | Error Color 0.0056(0.0071) |Steps 554(552.60) | Grad Norm 0.6173(1.0754) | Total Time 10.00(10.00)\n",
      "Iter 3690 | Time 45.2387(45.6764) | Bit/dim 1.1078(1.1089) | Xent 0.2686(0.2676) | Xent Color 0.0362(0.0369) | Loss 1.1840(1.1850) | Error 0.0878(0.0869) | Error Color 0.0056(0.0072) |Steps 560(552.80) | Grad Norm 1.1254(1.1059) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0056 | Time 126.5163, Epoch Time 3164.6985(2823.1083), Bit/dim 1.1070(best: 1.1121), Xent 0.1600, Xent Color 0.0095. Loss 1.1494, Error 0.0487(best: 0.0494), Error Color 0.0002(best: 0.0002)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3700 | Time 47.0685(45.7194) | Bit/dim 1.1093(1.1080) | Xent 0.2336(0.2664) | Xent Color 0.0412(0.0371) | Loss 1.1780(1.1838) | Error 0.0800(0.0865) | Error Color 0.0089(0.0074) |Steps 554(553.92) | Grad Norm 0.8545(1.0996) | Total Time 10.00(10.00)\n",
      "Iter 3710 | Time 45.6023(45.6426) | Bit/dim 1.0994(1.1074) | Xent 0.2361(0.2644) | Xent Color 0.0383(0.0366) | Loss 1.1679(1.1826) | Error 0.0833(0.0857) | Error Color 0.0100(0.0072) |Steps 554(553.22) | Grad Norm 1.3499(1.0683) | Total Time 10.00(10.00)\n",
      "Iter 3720 | Time 46.1309(45.6905) | Bit/dim 1.0978(1.1072) | Xent 0.2919(0.2681) | Xent Color 0.0404(0.0370) | Loss 1.1809(1.1834) | Error 0.0878(0.0847) | Error Color 0.0078(0.0074) |Steps 560(551.68) | Grad Norm 0.6971(1.1081) | Total Time 10.00(10.00)\n",
      "Iter 3730 | Time 47.0003(45.8137) | Bit/dim 1.1070(1.1066) | Xent 0.2629(0.2643) | Xent Color 0.0369(0.0364) | Loss 1.1819(1.1817) | Error 0.0756(0.0836) | Error Color 0.0067(0.0072) |Steps 554(551.37) | Grad Norm 1.6394(1.2254) | Total Time 10.00(10.00)\n",
      "Iter 3740 | Time 46.1920(45.8366) | Bit/dim 1.1016(1.1059) | Xent 0.2773(0.2648) | Xent Color 0.0403(0.0365) | Loss 1.1810(1.1813) | Error 0.0911(0.0841) | Error Color 0.0078(0.0071) |Steps 548(551.40) | Grad Norm 0.9992(1.2357) | Total Time 10.00(10.00)\n",
      "Iter 3750 | Time 45.7988(45.8247) | Bit/dim 1.1060(1.1052) | Xent 0.2384(0.2632) | Xent Color 0.0480(0.0369) | Loss 1.1776(1.1802) | Error 0.0722(0.0843) | Error Color 0.0111(0.0073) |Steps 548(551.37) | Grad Norm 1.5760(1.1977) | Total Time 10.00(10.00)\n",
      "Iter 3760 | Time 46.5074(45.7407) | Bit/dim 1.1046(1.1042) | Xent 0.2463(0.2615) | Xent Color 0.0452(0.0367) | Loss 1.1774(1.1788) | Error 0.0778(0.0838) | Error Color 0.0122(0.0074) |Steps 554(550.96) | Grad Norm 0.9563(1.1805) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0057 | Time 124.5193, Epoch Time 3166.9171(2833.4226), Bit/dim 1.1033(best: 1.1070), Xent 0.1616, Xent Color 0.0095. Loss 1.1461, Error 0.0509(best: 0.0487), Error Color 0.0001(best: 0.0002)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3770 | Time 45.1519(45.7513) | Bit/dim 1.1180(1.1041) | Xent 0.3102(0.2645) | Xent Color 0.0328(0.0361) | Loss 1.2038(1.1793) | Error 0.0944(0.0845) | Error Color 0.0056(0.0072) |Steps 560(551.89) | Grad Norm 1.4010(1.1514) | Total Time 10.00(10.00)\n",
      "Iter 3780 | Time 44.5590(45.7593) | Bit/dim 1.1153(1.1023) | Xent 0.2468(0.2610) | Xent Color 0.0347(0.0359) | Loss 1.1857(1.1766) | Error 0.0722(0.0829) | Error Color 0.0078(0.0072) |Steps 548(551.20) | Grad Norm 1.2594(1.1784) | Total Time 10.00(10.00)\n",
      "Iter 3790 | Time 47.1900(45.7826) | Bit/dim 1.1100(1.1017) | Xent 0.2578(0.2617) | Xent Color 0.0313(0.0356) | Loss 1.1823(1.1760) | Error 0.0856(0.0839) | Error Color 0.0056(0.0069) |Steps 542(551.58) | Grad Norm 0.7913(1.1492) | Total Time 10.00(10.00)\n",
      "Iter 3800 | Time 48.0026(45.6684) | Bit/dim 1.0912(1.1011) | Xent 0.2197(0.2600) | Xent Color 0.0346(0.0352) | Loss 1.1547(1.1749) | Error 0.0767(0.0846) | Error Color 0.0033(0.0064) |Steps 548(551.40) | Grad Norm 0.7276(1.1004) | Total Time 10.00(10.00)\n",
      "Iter 3810 | Time 46.5604(45.7113) | Bit/dim 1.1014(1.1015) | Xent 0.2758(0.2602) | Xent Color 0.0395(0.0357) | Loss 1.1803(1.1755) | Error 0.0889(0.0853) | Error Color 0.0089(0.0069) |Steps 554(551.60) | Grad Norm 1.3310(1.0717) | Total Time 10.00(10.00)\n",
      "Iter 3820 | Time 44.8709(45.6037) | Bit/dim 1.0860(1.1005) | Xent 0.2896(0.2669) | Xent Color 0.0322(0.0359) | Loss 1.1664(1.1762) | Error 0.0911(0.0866) | Error Color 0.0022(0.0069) |Steps 548(551.59) | Grad Norm 2.0174(1.1911) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0058 | Time 125.7895, Epoch Time 3163.1396(2843.3141), Bit/dim 1.1006(best: 1.1033), Xent 0.1593, Xent Color 0.0095. Loss 1.1428, Error 0.0492(best: 0.0487), Error Color 0.0005(best: 0.0001)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3830 | Time 44.7320(45.6026) | Bit/dim 1.0923(1.1002) | Xent 0.2453(0.2635) | Xent Color 0.0380(0.0356) | Loss 1.1632(1.1750) | Error 0.0956(0.0860) | Error Color 0.0067(0.0068) |Steps 548(550.60) | Grad Norm 1.1714(1.1810) | Total Time 10.00(10.00)\n",
      "Iter 3840 | Time 46.5870(45.6334) | Bit/dim 1.1022(1.0989) | Xent 0.2682(0.2646) | Xent Color 0.0428(0.0358) | Loss 1.1799(1.1740) | Error 0.0744(0.0859) | Error Color 0.0089(0.0068) |Steps 554(549.97) | Grad Norm 1.3101(1.1732) | Total Time 10.00(10.00)\n",
      "Iter 3850 | Time 48.6573(45.9439) | Bit/dim 1.1091(1.0986) | Xent 0.2471(0.2624) | Xent Color 0.0321(0.0357) | Loss 1.1789(1.1731) | Error 0.0889(0.0855) | Error Color 0.0044(0.0067) |Steps 554(550.09) | Grad Norm 1.2838(1.1055) | Total Time 10.00(10.00)\n",
      "Iter 3860 | Time 45.0610(45.7132) | Bit/dim 1.1138(1.0994) | Xent 0.2491(0.2610) | Xent Color 0.0320(0.0354) | Loss 1.1840(1.1736) | Error 0.0789(0.0851) | Error Color 0.0056(0.0068) |Steps 548(549.80) | Grad Norm 0.7647(1.1122) | Total Time 10.00(10.00)\n",
      "Iter 3870 | Time 46.6432(45.6951) | Bit/dim 1.0916(1.0983) | Xent 0.2416(0.2638) | Xent Color 0.0274(0.0353) | Loss 1.1589(1.1730) | Error 0.0900(0.0849) | Error Color 0.0033(0.0066) |Steps 554(549.86) | Grad Norm 1.3479(1.1427) | Total Time 10.00(10.00)\n",
      "Iter 3880 | Time 44.6312(45.6098) | Bit/dim 1.1003(1.0977) | Xent 0.2148(0.2621) | Xent Color 0.0381(0.0348) | Loss 1.1636(1.1720) | Error 0.0600(0.0840) | Error Color 0.0078(0.0065) |Steps 536(550.28) | Grad Norm 1.7124(1.2240) | Total Time 10.00(10.00)\n",
      "Iter 3890 | Time 44.4810(45.5506) | Bit/dim 1.0924(1.0961) | Xent 0.2825(0.2606) | Xent Color 0.0346(0.0357) | Loss 1.1717(1.1702) | Error 0.0844(0.0834) | Error Color 0.0067(0.0070) |Steps 554(550.56) | Grad Norm 1.1965(1.3338) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0059 | Time 123.5493, Epoch Time 3159.5802(2852.8020), Bit/dim 1.0970(best: 1.1006), Xent 0.1549, Xent Color 0.0088. Loss 1.1380, Error 0.0476(best: 0.0487), Error Color 0.0002(best: 0.0001)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3900 | Time 49.6180(45.8278) | Bit/dim 1.0845(1.0967) | Xent 0.2929(0.2638) | Xent Color 0.0312(0.0349) | Loss 1.1655(1.1713) | Error 0.0889(0.0839) | Error Color 0.0033(0.0065) |Steps 560(552.39) | Grad Norm 1.1491(1.2617) | Total Time 10.00(10.00)\n",
      "Iter 3910 | Time 45.7940(45.8167) | Bit/dim 1.0879(1.0961) | Xent 0.2334(0.2609) | Xent Color 0.0271(0.0347) | Loss 1.1530(1.1700) | Error 0.0733(0.0828) | Error Color 0.0011(0.0062) |Steps 560(553.33) | Grad Norm 1.3837(1.2150) | Total Time 10.00(10.00)\n",
      "Iter 3920 | Time 47.6715(45.7613) | Bit/dim 1.0974(1.0953) | Xent 0.2745(0.2598) | Xent Color 0.0308(0.0343) | Loss 1.1738(1.1688) | Error 0.0800(0.0830) | Error Color 0.0067(0.0062) |Steps 560(554.17) | Grad Norm 0.8039(1.1391) | Total Time 10.00(10.00)\n",
      "Iter 3930 | Time 47.6990(45.7065) | Bit/dim 1.0888(1.0947) | Xent 0.2613(0.2596) | Xent Color 0.0419(0.0339) | Loss 1.1647(1.1681) | Error 0.0911(0.0835) | Error Color 0.0089(0.0061) |Steps 560(555.42) | Grad Norm 1.4348(1.1163) | Total Time 10.00(10.00)\n",
      "Iter 3940 | Time 46.5502(45.8885) | Bit/dim 1.0913(1.0940) | Xent 0.2553(0.2601) | Xent Color 0.0299(0.0334) | Loss 1.1626(1.1674) | Error 0.0744(0.0835) | Error Color 0.0056(0.0059) |Steps 560(556.45) | Grad Norm 1.3338(1.1093) | Total Time 10.00(10.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_2cond_nosep.py --data colormnist --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/cnf_conditional_disentangle_colormnist_bs900_sratio_1_2th_drop_0_5_2cond_linear_nosep_run1 --resume ../experiments_published/cnf_conditional_disentangle_colormnist_bs900_sratio_1_2th_drop_0_5_2cond_linear_nosep_run1/current_checkpt.pth --seed 1 --lr 0.0001 --conditional True --controlled_tol False --train_mode semisup --log_freq 10 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5 --y_color 10 --y_class 10\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
