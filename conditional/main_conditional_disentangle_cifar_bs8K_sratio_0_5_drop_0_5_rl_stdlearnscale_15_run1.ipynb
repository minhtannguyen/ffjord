{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_rl.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "GATES = [\"cnn1\", \"cnn2\", \"rnn\"]\n",
      "\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--gate', type=str, default='cnn1', choices=GATES)\n",
      "parser.add_argument('--scale', type=float, default=1.0)\n",
      "parser.add_argument('--scale_fac', type=float, default=1.0)\n",
      "parser.add_argument('--scale_std', type=float, default=1.0)\n",
      "parser.add_argument('--eta', default=0.1, type=float,\n",
      "                        help='tuning parameter that allows us to trade-off the competing goals of' \n",
      "                                'minimizing the prediction loss and maximizing the gate rewards ')\n",
      "parser.add_argument('--rl-weight', default=0.01, type=float,\n",
      "                        help='rl weight')\n",
      "\n",
      "parser.add_argument('--gamma', default=0.99, type=float,\n",
      "                        help='discount factor, default: (0.99)')\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "import lib.odenvp_conditional_rl as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol, \"scale\": args.scale, \"scale_fac\": args.scale_fac, \"scale_std\": args.scale_std, \"gate\": args.gate},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            # re-weight the gate rewards\n",
      "            normalized_eta = args.eta / len(logp_actions)\n",
      "            \n",
      "            # collect cumulative future rewards\n",
      "            R = - loss\n",
      "            cum_rewards = []\n",
      "            for r in nfe[::-1]:\n",
      "                R = -normalized_eta * r.view(-1,1) + args.gamma * R\n",
      "                cum_rewards.insert(0,R)\n",
      "            \n",
      "            # apply REINFORCE\n",
      "            rl_loss = 0\n",
      "            for lpa, r in zip(logp_actions, cum_rewards):\n",
      "                rl_loss = rl_loss - lpa.view(-1,1) * args.rl_weight * r\n",
      "                \n",
      "            loss = loss + rl_loss.mean()\n",
      "            \n",
      "            loss.backward()\n",
      "            \n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "            \n",
      "            for idx in range(len(model.module.transforms)):\n",
      "                for layer in model.module.transforms[idx].chain:\n",
      "                    if hasattr(layer, 'atol'):\n",
      "                        layer.odefunc.after_odeint()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'train': atol[tol_indx].mean()}, itr)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'train': rtol[tol_indx].mean()}, itr)\n",
      "                    \n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples, atol, rtol, logp_actions, nfe = model(fixed_z, reverse=True)\n",
      "            generated_samples = generated_samples.view(-1, *data_shape)\n",
      "            for tol_indx in range(len(atol)):\n",
      "                writer.add_scalars('atol_gen_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                writer.add_scalars('rtol_gen_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=0.0001, autoencode=False, batch_norm=False, batch_size=8000, batch_size_schedule='', begin_epoch=1, condition_ratio=0.5, conditional=True, controlled_tol=True, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, eta=0.1, gamma=0.99, gate='cnn2', imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=1, lr=0.01, max_grad_norm=20.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume='../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_rl_stdlearnscale_15_run1/epoch_125_checkpt.pth', rl_weight=0.01, rtol=0.0001, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_rl_stdlearnscale_15_run1', scale=1.0, scale_fac=1.0, scale_std=15.0, seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=5000, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000.0, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1450886\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc_rl.py:291: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Iter 0751 | Time 125.4147(64.3321) | Bit/dim 3.8120(3.8959) | Xent 1.4208(1.5078) | Loss 12.1804(10.8071) | Error 0.5069(0.5372) Steps 0(0.00) | Grad Norm 7.9416(6.5173) | Total Time 0.00(0.00)\n",
      "Iter 0752 | Time 65.8417(64.3774) | Bit/dim 3.8132(3.8934) | Xent 1.3852(1.5041) | Loss 10.0302(10.7838) | Error 0.5004(0.5361) Steps 0(0.00) | Grad Norm 3.1324(6.4158) | Total Time 0.00(0.00)\n",
      "Iter 0753 | Time 62.9401(64.3342) | Bit/dim 3.7958(3.8905) | Xent 1.4272(1.5018) | Loss 10.0267(10.7611) | Error 0.5102(0.5353) Steps 0(0.00) | Grad Norm 6.1688(6.4084) | Total Time 0.00(0.00)\n",
      "Iter 0754 | Time 59.0237(64.1749) | Bit/dim 3.8106(3.8881) | Xent 1.4161(1.4992) | Loss 9.7939(10.7321) | Error 0.5085(0.5345) Steps 0(0.00) | Grad Norm 5.2471(6.3736) | Total Time 0.00(0.00)\n",
      "Iter 0755 | Time 60.2601(64.0575) | Bit/dim 3.7930(3.8852) | Xent 1.3810(1.4957) | Loss 9.9018(10.7072) | Error 0.4869(0.5331) Steps 0(0.00) | Grad Norm 2.8191(6.2669) | Total Time 0.00(0.00)\n",
      "Iter 0756 | Time 56.4290(63.8286) | Bit/dim 3.8076(3.8829) | Xent 1.3989(1.4928) | Loss 9.9052(10.6831) | Error 0.5055(0.5323) Steps 0(0.00) | Grad Norm 5.5625(6.2458) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0126 | Time 37.7256, Epoch Time 483.9954(399.6099), Bit/dim 3.8013(best: inf), Xent 1.3507, Loss 4.4766, Error 0.4805(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0757 | Time 78.9059(64.2809) | Bit/dim 3.8085(3.8807) | Xent 1.3866(1.4896) | Loss 13.3801(10.7640) | Error 0.4992(0.5313) Steps 0(0.00) | Grad Norm 5.0818(6.2109) | Total Time 0.00(0.00)\n",
      "Iter 0758 | Time 69.2480(64.4299) | Bit/dim 3.8061(3.8784) | Xent 1.3837(1.4864) | Loss 9.9806(10.7405) | Error 0.4946(0.5302) Steps 0(0.00) | Grad Norm 4.1654(6.1495) | Total Time 0.00(0.00)\n",
      "Iter 0759 | Time 58.8664(64.2630) | Bit/dim 3.8157(3.8766) | Xent 1.3817(1.4833) | Loss 9.7300(10.7102) | Error 0.4880(0.5289) Steps 0(0.00) | Grad Norm 4.6795(6.1054) | Total Time 0.00(0.00)\n",
      "Iter 0760 | Time 71.6102(64.4835) | Bit/dim 3.7985(3.8742) | Xent 1.3655(1.4797) | Loss 9.8338(10.6839) | Error 0.4931(0.5278) Steps 0(0.00) | Grad Norm 4.8161(6.0667) | Total Time 0.00(0.00)\n",
      "Iter 0761 | Time 58.2593(64.2967) | Bit/dim 3.7991(3.8720) | Xent 1.4063(1.4775) | Loss 9.8812(10.6598) | Error 0.5041(0.5271) Steps 0(0.00) | Grad Norm 8.1240(6.1284) | Total Time 0.00(0.00)\n",
      "Iter 0762 | Time 60.9481(64.1963) | Bit/dim 3.7871(3.8694) | Xent 1.4496(1.4767) | Loss 9.9918(10.6398) | Error 0.5204(0.5269) Steps 0(0.00) | Grad Norm 10.4694(6.2587) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0127 | Time 23.2215, Epoch Time 437.6349(400.7506), Bit/dim 3.8211(best: 3.8013), Xent 1.3672, Loss 4.5047, Error 0.4912(best: 0.4805)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0763 | Time 63.2652(64.1683) | Bit/dim 3.8111(3.8677) | Xent 1.3989(1.4744) | Loss 13.0154(10.7111) | Error 0.5028(0.5262) Steps 0(0.00) | Grad Norm 9.7831(6.3644) | Total Time 0.00(0.00)\n",
      "Iter 0764 | Time 61.6589(64.0931) | Bit/dim 3.8209(3.8663) | Xent 1.4062(1.4723) | Loss 9.8799(10.6861) | Error 0.4985(0.5254) Steps 0(0.00) | Grad Norm 11.1135(6.5069) | Total Time 0.00(0.00)\n",
      "Iter 0765 | Time 71.2854(64.3088) | Bit/dim 3.8109(3.8646) | Xent 1.5774(1.4755) | Loss 9.8405(10.6608) | Error 0.5529(0.5262) Steps 0(0.00) | Grad Norm 15.5329(6.7777) | Total Time 0.00(0.00)\n",
      "Iter 0766 | Time 61.4791(64.2239) | Bit/dim 3.8173(3.8632) | Xent 1.4786(1.4756) | Loss 10.0552(10.6426) | Error 0.5241(0.5261) Steps 0(0.00) | Grad Norm 14.4072(7.0065) | Total Time 0.00(0.00)\n",
      "Iter 0767 | Time 63.2662(64.1952) | Bit/dim 3.8149(3.8617) | Xent 1.4313(1.4742) | Loss 9.9321(10.6213) | Error 0.5214(0.5260) Steps 0(0.00) | Grad Norm 6.9382(7.0045) | Total Time 0.00(0.00)\n",
      "Iter 0768 | Time 63.9108(64.1867) | Bit/dim 3.7923(3.8597) | Xent 1.4170(1.4725) | Loss 10.1121(10.6060) | Error 0.5075(0.5254) Steps 0(0.00) | Grad Norm 5.1770(6.9497) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0128 | Time 23.2221, Epoch Time 423.9403(401.4463), Bit/dim 3.8123(best: 3.8013), Xent 1.3674, Loss 4.4960, Error 0.4913(best: 0.4805)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0769 | Time 61.4529(64.1047) | Bit/dim 3.8106(3.8582) | Xent 1.4306(1.4713) | Loss 13.3438(10.6881) | Error 0.5115(0.5250) Steps 0(0.00) | Grad Norm 7.9684(6.9802) | Total Time 0.00(0.00)\n",
      "Iter 0770 | Time 57.4390(63.9047) | Bit/dim 3.8025(3.8565) | Xent 1.3940(1.4689) | Loss 9.9438(10.6658) | Error 0.4988(0.5242) Steps 0(0.00) | Grad Norm 4.2090(6.8971) | Total Time 0.00(0.00)\n",
      "Iter 0771 | Time 52.8815(63.5740) | Bit/dim 3.7905(3.8545) | Xent 1.3958(1.4668) | Loss 9.6282(10.6347) | Error 0.5040(0.5236) Steps 0(0.00) | Grad Norm 4.1815(6.8156) | Total Time 0.00(0.00)\n",
      "Iter 0772 | Time 60.9547(63.4954) | Bit/dim 3.7949(3.8527) | Xent 1.3714(1.4639) | Loss 9.8974(10.6126) | Error 0.4980(0.5229) Steps 0(0.00) | Grad Norm 4.0225(6.7318) | Total Time 0.00(0.00)\n",
      "Iter 0773 | Time 56.8841(63.2971) | Bit/dim 3.7917(3.8509) | Xent 1.3733(1.4612) | Loss 9.5793(10.5816) | Error 0.4904(0.5219) Steps 0(0.00) | Grad Norm 3.1009(6.6229) | Total Time 0.00(0.00)\n",
      "Iter 0774 | Time 63.0202(63.2888) | Bit/dim 3.7899(3.8491) | Xent 1.3810(1.4588) | Loss 9.9849(10.5637) | Error 0.4972(0.5211) Steps 0(0.00) | Grad Norm 4.2446(6.5515) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0129 | Time 23.7419, Epoch Time 392.4487(401.1764), Bit/dim 3.7826(best: 3.8013), Xent 1.3165, Loss 4.4408, Error 0.4738(best: 0.4805)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0775 | Time 58.6099(63.1484) | Bit/dim 3.7769(3.8469) | Xent 1.3648(1.4560) | Loss 13.0284(10.6376) | Error 0.4859(0.5201) Steps 0(0.00) | Grad Norm 2.9418(6.4433) | Total Time 0.00(0.00)\n",
      "Iter 0776 | Time 57.7529(62.9865) | Bit/dim 3.7932(3.8453) | Xent 1.3792(1.4536) | Loss 9.9748(10.6177) | Error 0.4919(0.5192) Steps 0(0.00) | Grad Norm 3.8827(6.3664) | Total Time 0.00(0.00)\n",
      "Iter 0777 | Time 58.0399(62.8381) | Bit/dim 3.7919(3.8437) | Xent 1.3560(1.4507) | Loss 9.9556(10.5979) | Error 0.4890(0.5183) Steps 0(0.00) | Grad Norm 3.8426(6.2907) | Total Time 0.00(0.00)\n",
      "Iter 0778 | Time 65.8595(62.9288) | Bit/dim 3.7885(3.8420) | Xent 1.3556(1.4479) | Loss 9.9001(10.5769) | Error 0.4840(0.5173) Steps 0(0.00) | Grad Norm 3.3343(6.2020) | Total Time 0.00(0.00)\n",
      "Iter 0779 | Time 63.7114(62.9523) | Bit/dim 3.8022(3.8408) | Xent 1.3613(1.4453) | Loss 10.1032(10.5627) | Error 0.4979(0.5167) Steps 0(0.00) | Grad Norm 5.5593(6.1827) | Total Time 0.00(0.00)\n",
      "Iter 0780 | Time 62.6118(62.9420) | Bit/dim 3.7932(3.8394) | Xent 1.3883(1.4436) | Loss 9.9015(10.5429) | Error 0.5002(0.5162) Steps 0(0.00) | Grad Norm 8.9279(6.2651) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0130 | Time 23.2486, Epoch Time 406.3903(401.3328), Bit/dim 3.8159(best: 3.7826), Xent 1.3936, Loss 4.5127, Error 0.4985(best: 0.4738)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0781 | Time 56.1886(62.7394) | Bit/dim 3.8077(3.8385) | Xent 1.4358(1.4433) | Loss 13.1356(10.6207) | Error 0.5036(0.5158) Steps 0(0.00) | Grad Norm 11.6271(6.4260) | Total Time 0.00(0.00)\n",
      "Iter 0782 | Time 60.1212(62.6609) | Bit/dim 3.8379(3.8384) | Xent 1.6991(1.4510) | Loss 10.2779(10.6104) | Error 0.5702(0.5175) Steps 0(0.00) | Grad Norm 27.8007(7.0672) | Total Time 0.00(0.00)\n",
      "Iter 0783 | Time 68.6016(62.8391) | Bit/dim 3.8760(3.8396) | Xent 1.9561(1.4661) | Loss 10.5215(10.6077) | Error 0.6339(0.5210) Steps 0(0.00) | Grad Norm 21.8310(7.5101) | Total Time 0.00(0.00)\n",
      "Iter 0784 | Time 51.7002(62.5049) | Bit/dim 3.8763(3.8407) | Xent 1.6212(1.4708) | Loss 10.1852(10.5950) | Error 0.5713(0.5225) Steps 0(0.00) | Grad Norm 10.0893(7.5875) | Total Time 0.00(0.00)\n",
      "Iter 0785 | Time 62.4423(62.5031) | Bit/dim 3.9010(3.8425) | Xent 1.6874(1.4773) | Loss 10.4906(10.5919) | Error 0.5880(0.5244) Steps 0(0.00) | Grad Norm 11.8016(7.7139) | Total Time 0.00(0.00)\n",
      "Iter 0786 | Time 63.0719(62.5201) | Bit/dim 3.9311(3.8451) | Xent 1.6317(1.4819) | Loss 10.4615(10.5880) | Error 0.5939(0.5265) Steps 0(0.00) | Grad Norm 7.6615(7.7123) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0131 | Time 24.1768, Epoch Time 402.0578(401.3546), Bit/dim 3.9113(best: 3.7826), Xent 1.5005, Loss 4.6615, Error 0.5441(best: 0.4738)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0787 | Time 58.5131(62.3999) | Bit/dim 3.8988(3.8468) | Xent 1.5468(1.4839) | Loss 12.9821(10.6598) | Error 0.5534(0.5273) Steps 0(0.00) | Grad Norm 5.6544(7.6506) | Total Time 0.00(0.00)\n",
      "Iter 0788 | Time 62.4633(62.4018) | Bit/dim 3.8804(3.8478) | Xent 1.5146(1.4848) | Loss 10.1240(10.6437) | Error 0.5475(0.5279) Steps 0(0.00) | Grad Norm 5.0084(7.5713) | Total Time 0.00(0.00)\n",
      "Iter 0789 | Time 58.3817(62.2812) | Bit/dim 3.8874(3.8490) | Xent 1.4895(1.4849) | Loss 9.9188(10.6220) | Error 0.5375(0.5282) Steps 0(0.00) | Grad Norm 4.2036(7.4703) | Total Time 0.00(0.00)\n",
      "Iter 0790 | Time 58.6386(62.1719) | Bit/dim 3.8769(3.8498) | Xent 1.4863(1.4850) | Loss 10.1159(10.6068) | Error 0.5353(0.5284) Steps 0(0.00) | Grad Norm 3.9426(7.3645) | Total Time 0.00(0.00)\n",
      "Iter 0791 | Time 60.3513(62.1173) | Bit/dim 3.8585(3.8501) | Xent 1.5032(1.4855) | Loss 10.3515(10.5991) | Error 0.5497(0.5291) Steps 0(0.00) | Grad Norm 3.7039(7.2547) | Total Time 0.00(0.00)\n",
      "Iter 0792 | Time 59.3207(62.0334) | Bit/dim 3.8479(3.8500) | Xent 1.4548(1.4846) | Loss 10.0436(10.5825) | Error 0.5251(0.5290) Steps 0(0.00) | Grad Norm 3.3770(7.1383) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0132 | Time 24.1174, Epoch Time 397.9124(401.2513), Bit/dim 3.8670(best: 3.7826), Xent 1.3854, Loss 4.5597, Error 0.4976(best: 0.4738)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0793 | Time 66.0019(62.1525) | Bit/dim 3.8735(3.8507) | Xent 1.4423(1.4833) | Loss 13.3365(10.6651) | Error 0.5245(0.5288) Steps 0(0.00) | Grad Norm 4.1861(7.0498) | Total Time 0.00(0.00)\n",
      "Iter 0794 | Time 62.2665(62.1559) | Bit/dim 3.8335(3.8502) | Xent 1.4045(1.4810) | Loss 9.9676(10.6442) | Error 0.5008(0.5280) Steps 0(0.00) | Grad Norm 2.9481(6.9267) | Total Time 0.00(0.00)\n",
      "Iter 0795 | Time 64.0654(62.2132) | Bit/dim 3.8311(3.8496) | Xent 1.4380(1.4797) | Loss 10.2541(10.6325) | Error 0.5164(0.5276) Steps 0(0.00) | Grad Norm 3.4621(6.8228) | Total Time 0.00(0.00)\n",
      "Iter 0796 | Time 62.6350(62.2258) | Bit/dim 3.8331(3.8491) | Xent 1.4159(1.4778) | Loss 9.8820(10.6100) | Error 0.5086(0.5271) Steps 0(0.00) | Grad Norm 2.5251(6.6938) | Total Time 0.00(0.00)\n",
      "Iter 0797 | Time 73.0090(62.5493) | Bit/dim 3.8305(3.8486) | Xent 1.4063(1.4756) | Loss 10.0336(10.5927) | Error 0.5022(0.5263) Steps 0(0.00) | Grad Norm 2.4893(6.5677) | Total Time 0.00(0.00)\n",
      "Iter 0798 | Time 80.1026(63.0759) | Bit/dim 3.8231(3.8478) | Xent 1.4080(1.4736) | Loss 10.2001(10.5809) | Error 0.5037(0.5256) Steps 0(0.00) | Grad Norm 2.5838(6.4482) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0133 | Time 23.9660, Epoch Time 448.5156(402.6692), Bit/dim 3.8197(best: 3.7826), Xent 1.3442, Loss 4.4918, Error 0.4799(best: 0.4738)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0799 | Time 67.9924(63.2234) | Bit/dim 3.8199(3.8470) | Xent 1.3931(1.4712) | Loss 13.0671(10.6555) | Error 0.4948(0.5247) Steps 0(0.00) | Grad Norm 2.5558(6.3314) | Total Time 0.00(0.00)\n",
      "Iter 0800 | Time 74.3337(63.5567) | Bit/dim 3.8104(3.8459) | Xent 1.3729(1.4682) | Loss 10.0114(10.6361) | Error 0.4858(0.5235) Steps 0(0.00) | Grad Norm 2.2863(6.2101) | Total Time 0.00(0.00)\n",
      "Iter 0801 | Time 68.5163(63.7055) | Bit/dim 3.8110(3.8448) | Xent 1.3852(1.4657) | Loss 9.9288(10.6149) | Error 0.4905(0.5226) Steps 0(0.00) | Grad Norm 2.8466(6.1092) | Total Time 0.00(0.00)\n",
      "Iter 0802 | Time 65.1177(63.7479) | Bit/dim 3.8056(3.8436) | Xent 1.3801(1.4632) | Loss 9.3700(10.5776) | Error 0.4959(0.5218) Steps 0(0.00) | Grad Norm 2.5593(6.0027) | Total Time 0.00(0.00)\n",
      "Iter 0803 | Time 69.5430(63.9217) | Bit/dim 3.8036(3.8424) | Xent 1.3588(1.4600) | Loss 9.9094(10.5575) | Error 0.4814(0.5205) Steps 0(0.00) | Grad Norm 2.1725(5.8878) | Total Time 0.00(0.00)\n",
      "Iter 0804 | Time 72.6855(64.1846) | Bit/dim 3.8005(3.8412) | Xent 1.3595(1.4570) | Loss 9.8313(10.5357) | Error 0.4879(0.5196) Steps 0(0.00) | Grad Norm 2.6894(5.7918) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0134 | Time 23.0239, Epoch Time 456.9736(404.2983), Bit/dim 3.7959(best: 3.7826), Xent 1.3292, Loss 4.4605, Error 0.4725(best: 0.4738)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0805 | Time 73.1399(64.4533) | Bit/dim 3.8000(3.8399) | Xent 1.3495(1.4538) | Loss 13.2443(10.6170) | Error 0.4852(0.5185) Steps 0(0.00) | Grad Norm 3.7844(5.7316) | Total Time 0.00(0.00)\n",
      "Iter 0806 | Time 69.6448(64.6091) | Bit/dim 3.7956(3.8386) | Xent 1.3717(1.4513) | Loss 10.0500(10.6000) | Error 0.4951(0.5178) Steps 0(0.00) | Grad Norm 5.2720(5.7178) | Total Time 0.00(0.00)\n",
      "Iter 0807 | Time 71.9491(64.8293) | Bit/dim 3.7890(3.8371) | Xent 1.4004(1.4498) | Loss 9.7091(10.5733) | Error 0.4991(0.5173) Steps 0(0.00) | Grad Norm 6.7796(5.7497) | Total Time 0.00(0.00)\n",
      "Iter 0808 | Time 83.3160(65.3839) | Bit/dim 3.8007(3.8360) | Xent 1.3893(1.4480) | Loss 9.9049(10.5532) | Error 0.4988(0.5167) Steps 0(0.00) | Grad Norm 7.3341(5.7972) | Total Time 0.00(0.00)\n",
      "Iter 0809 | Time 71.9569(65.5810) | Bit/dim 3.7771(3.8343) | Xent 1.3823(1.4460) | Loss 9.8976(10.5335) | Error 0.4960(0.5161) Steps 0(0.00) | Grad Norm 6.8517(5.8288) | Total Time 0.00(0.00)\n",
      "Iter 0810 | Time 66.2906(65.6023) | Bit/dim 3.7729(3.8324) | Xent 1.4010(1.4447) | Loss 9.8890(10.5142) | Error 0.5046(0.5157) Steps 0(0.00) | Grad Norm 6.5871(5.8516) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0135 | Time 24.9104, Epoch Time 476.7608(406.4722), Bit/dim 3.7809(best: 3.7826), Xent 1.3734, Loss 4.4676, Error 0.4991(best: 0.4725)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0811 | Time 67.0802(65.6467) | Bit/dim 3.7782(3.8308) | Xent 1.4115(1.4437) | Loss 13.2621(10.5966) | Error 0.5109(0.5156) Steps 0(0.00) | Grad Norm 7.7649(5.9090) | Total Time 0.00(0.00)\n",
      "Iter 0812 | Time 66.9063(65.6845) | Bit/dim 3.7803(3.8293) | Xent 1.4208(1.4430) | Loss 9.7119(10.5701) | Error 0.5049(0.5153) Steps 0(0.00) | Grad Norm 8.1510(5.9762) | Total Time 0.00(0.00)\n",
      "Iter 0813 | Time 66.6987(65.7149) | Bit/dim 3.7678(3.8274) | Xent 1.4061(1.4419) | Loss 9.6079(10.5412) | Error 0.5075(0.5150) Steps 0(0.00) | Grad Norm 5.8858(5.9735) | Total Time 0.00(0.00)\n",
      "Iter 0814 | Time 77.0729(66.0556) | Bit/dim 3.7760(3.8259) | Xent 1.3703(1.4397) | Loss 10.0365(10.5261) | Error 0.4962(0.5145) Steps 0(0.00) | Grad Norm 4.1119(5.9177) | Total Time 0.00(0.00)\n",
      "Iter 0815 | Time 89.1464(66.7483) | Bit/dim 3.7750(3.8244) | Xent 1.4273(1.4394) | Loss 10.0747(10.5126) | Error 0.5177(0.5146) Steps 0(0.00) | Grad Norm 6.9798(5.9495) | Total Time 0.00(0.00)\n",
      "Iter 0816 | Time 69.1068(66.8191) | Bit/dim 3.7926(3.8234) | Xent 1.3521(1.4367) | Loss 9.9212(10.4948) | Error 0.4902(0.5138) Steps 0(0.00) | Grad Norm 6.0129(5.9514) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0136 | Time 24.8369, Epoch Time 476.4751(408.5723), Bit/dim 3.7952(best: 3.7809), Xent 1.3353, Loss 4.4629, Error 0.4730(best: 0.4725)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0817 | Time 73.3589(67.0153) | Bit/dim 3.7987(3.8227) | Xent 1.4201(1.4362) | Loss 13.3441(10.5803) | Error 0.5057(0.5136) Steps 0(0.00) | Grad Norm 6.8075(5.9771) | Total Time 0.00(0.00)\n",
      "Iter 0818 | Time 72.0838(67.1673) | Bit/dim 3.7785(3.8213) | Xent 1.3605(1.4340) | Loss 10.0617(10.5647) | Error 0.4878(0.5128) Steps 0(0.00) | Grad Norm 5.3964(5.9597) | Total Time 0.00(0.00)\n",
      "Iter 0819 | Time 78.6511(67.5119) | Bit/dim 3.7753(3.8200) | Xent 1.3647(1.4319) | Loss 10.0073(10.5480) | Error 0.4880(0.5121) Steps 0(0.00) | Grad Norm 4.1903(5.9066) | Total Time 0.00(0.00)\n",
      "Iter 0820 | Time 75.5188(67.7521) | Bit/dim 3.7909(3.8191) | Xent 1.3299(1.4288) | Loss 10.0544(10.5332) | Error 0.4786(0.5111) Steps 0(0.00) | Grad Norm 4.5246(5.8652) | Total Time 0.00(0.00)\n",
      "Iter 0821 | Time 66.1271(67.7033) | Bit/dim 3.7799(3.8179) | Xent 1.3306(1.4259) | Loss 9.8083(10.5115) | Error 0.4766(0.5100) Steps 0(0.00) | Grad Norm 4.8259(5.8340) | Total Time 0.00(0.00)\n",
      "Iter 0822 | Time 70.5438(67.7885) | Bit/dim 3.7949(3.8172) | Xent 1.3708(1.4242) | Loss 10.1076(10.4993) | Error 0.5000(0.5097) Steps 0(0.00) | Grad Norm 7.1944(5.8748) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0137 | Time 23.2050, Epoch Time 475.1314(410.5691), Bit/dim 3.7689(best: 3.7809), Xent 1.3739, Loss 4.4559, Error 0.4900(best: 0.4725)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0823 | Time 64.3050(67.6840) | Bit/dim 3.7720(3.8159) | Xent 1.4072(1.4237) | Loss 12.5281(10.5602) | Error 0.5079(0.5097) Steps 0(0.00) | Grad Norm 10.0410(5.9998) | Total Time 0.00(0.00)\n",
      "Iter 0824 | Time 75.0861(67.9061) | Bit/dim 3.7809(3.8148) | Xent 1.4444(1.4243) | Loss 10.1081(10.5466) | Error 0.5048(0.5095) Steps 0(0.00) | Grad Norm 14.4262(6.2526) | Total Time 0.00(0.00)\n",
      "Iter 0825 | Time 70.2810(67.9773) | Bit/dim 3.7771(3.8137) | Xent 1.4684(1.4257) | Loss 9.6699(10.5203) | Error 0.5168(0.5098) Steps 0(0.00) | Grad Norm 12.8426(6.4503) | Total Time 0.00(0.00)\n",
      "Iter 0826 | Time 72.4945(68.1129) | Bit/dim 3.7803(3.8127) | Xent 1.3768(1.4242) | Loss 9.9614(10.5036) | Error 0.4951(0.5093) Steps 0(0.00) | Grad Norm 3.5629(6.3637) | Total Time 0.00(0.00)\n",
      "Iter 0827 | Time 74.4333(68.3025) | Bit/dim 3.7967(3.8122) | Xent 1.3800(1.4229) | Loss 9.9503(10.4870) | Error 0.4922(0.5088) Steps 0(0.00) | Grad Norm 7.4731(6.3969) | Total Time 0.00(0.00)\n",
      "Iter 0828 | Time 67.6438(68.2827) | Bit/dim 3.7759(3.8111) | Xent 1.3714(1.4213) | Loss 9.9404(10.4706) | Error 0.4874(0.5082) Steps 0(0.00) | Grad Norm 5.4581(6.3688) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0138 | Time 23.5120, Epoch Time 463.2511(412.1495), Bit/dim 3.7798(best: 3.7689), Xent 1.3122, Loss 4.4359, Error 0.4740(best: 0.4725)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0829 | Time 80.3059(68.6434) | Bit/dim 3.7849(3.8103) | Xent 1.3453(1.4191) | Loss 13.4147(10.5589) | Error 0.4808(0.5073) Steps 0(0.00) | Grad Norm 4.0508(6.2992) | Total Time 0.00(0.00)\n",
      "Iter 0830 | Time 67.4165(68.6066) | Bit/dim 3.8123(3.8104) | Xent 1.3778(1.4178) | Loss 10.0825(10.5446) | Error 0.4909(0.5068) Steps 0(0.00) | Grad Norm 5.6652(6.2802) | Total Time 0.00(0.00)\n",
      "Iter 0831 | Time 71.6603(68.6982) | Bit/dim 3.7767(3.8094) | Xent 1.3368(1.4154) | Loss 9.8689(10.5243) | Error 0.4819(0.5061) Steps 0(0.00) | Grad Norm 5.3572(6.2525) | Total Time 0.00(0.00)\n",
      "Iter 0832 | Time 72.6940(68.8181) | Bit/dim 3.7885(3.8088) | Xent 1.3767(1.4142) | Loss 9.8336(10.5036) | Error 0.4959(0.5058) Steps 0(0.00) | Grad Norm 6.9118(6.2723) | Total Time 0.00(0.00)\n",
      "Iter 0833 | Time 64.1176(68.6771) | Bit/dim 3.7587(3.8073) | Xent 1.3614(1.4126) | Loss 9.7548(10.4812) | Error 0.4888(0.5053) Steps 0(0.00) | Grad Norm 5.3491(6.2446) | Total Time 0.00(0.00)\n",
      "Iter 0834 | Time 61.3526(68.4573) | Bit/dim 3.7727(3.8062) | Xent 1.3528(1.4108) | Loss 9.8556(10.4624) | Error 0.4919(0.5049) Steps 0(0.00) | Grad Norm 4.8576(6.2030) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0139 | Time 23.3188, Epoch Time 457.0791(413.4974), Bit/dim 3.7790(best: 3.7689), Xent 1.2985, Loss 4.4283, Error 0.4657(best: 0.4725)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0835 | Time 67.2149(68.4201) | Bit/dim 3.7739(3.8053) | Xent 1.3395(1.4087) | Loss 13.0826(10.5410) | Error 0.4809(0.5042) Steps 0(0.00) | Grad Norm 3.5557(6.1236) | Total Time 0.00(0.00)\n",
      "Iter 0836 | Time 66.6958(68.3683) | Bit/dim 3.7632(3.8040) | Xent 1.3247(1.4062) | Loss 9.7262(10.5165) | Error 0.4784(0.5034) Steps 0(0.00) | Grad Norm 3.3433(6.0402) | Total Time 0.00(0.00)\n",
      "Iter 0837 | Time 76.4741(68.6115) | Bit/dim 3.7760(3.8032) | Xent 1.3277(1.4038) | Loss 9.9847(10.5006) | Error 0.4791(0.5027) Steps 0(0.00) | Grad Norm 3.7231(5.9707) | Total Time 0.00(0.00)\n",
      "Iter 0838 | Time 70.7158(68.6746) | Bit/dim 3.7644(3.8020) | Xent 1.3279(1.4015) | Loss 9.7998(10.4796) | Error 0.4788(0.5019) Steps 0(0.00) | Grad Norm 4.3917(5.9233) | Total Time 0.00(0.00)\n",
      "Iter 0839 | Time 66.4918(68.6091) | Bit/dim 3.7700(3.8010) | Xent 1.3667(1.4005) | Loss 9.7674(10.4582) | Error 0.4862(0.5015) Steps 0(0.00) | Grad Norm 8.8013(6.0096) | Total Time 0.00(0.00)\n",
      "Iter 0840 | Time 65.4163(68.5134) | Bit/dim 3.7910(3.8007) | Xent 1.4921(1.4032) | Loss 9.9657(10.4434) | Error 0.5292(0.5023) Steps 0(0.00) | Grad Norm 15.5297(6.2952) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0140 | Time 23.4341, Epoch Time 452.6573(414.6722), Bit/dim 3.7996(best: 3.7689), Xent 1.5278, Loss 4.5635, Error 0.5414(best: 0.4657)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0841 | Time 72.9840(68.6475) | Bit/dim 3.8017(3.8008) | Xent 1.5793(1.4085) | Loss 12.8274(10.5149) | Error 0.5617(0.5041) Steps 0(0.00) | Grad Norm 14.9651(6.5553) | Total Time 0.00(0.00)\n",
      "Iter 0842 | Time 68.5098(68.6434) | Bit/dim 3.8099(3.8010) | Xent 1.4222(1.4089) | Loss 9.8444(10.4948) | Error 0.5197(0.5046) Steps 0(0.00) | Grad Norm 6.5666(6.5557) | Total Time 0.00(0.00)\n",
      "Iter 0843 | Time 69.0056(68.6542) | Bit/dim 3.8111(3.8013) | Xent 1.4618(1.4105) | Loss 10.0517(10.4815) | Error 0.5330(0.5054) Steps 0(0.00) | Grad Norm 5.4869(6.5236) | Total Time 0.00(0.00)\n",
      "Iter 0844 | Time 73.6734(68.8048) | Bit/dim 3.7975(3.8012) | Xent 1.4030(1.4103) | Loss 9.9149(10.4645) | Error 0.5032(0.5053) Steps 0(0.00) | Grad Norm 5.5608(6.4947) | Total Time 0.00(0.00)\n",
      "Iter 0845 | Time 82.3520(69.2112) | Bit/dim 3.7928(3.8010) | Xent 1.4288(1.4109) | Loss 10.0239(10.4513) | Error 0.5076(0.5054) Steps 0(0.00) | Grad Norm 6.3054(6.4890) | Total Time 0.00(0.00)\n",
      "Iter 0846 | Time 74.5238(69.3706) | Bit/dim 3.7663(3.7999) | Xent 1.3860(1.4101) | Loss 9.8562(10.4335) | Error 0.4952(0.5051) Steps 0(0.00) | Grad Norm 4.1795(6.4198) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0141 | Time 25.0882, Epoch Time 481.9402(416.6903), Bit/dim 3.7954(best: 3.7689), Xent 1.3369, Loss 4.4638, Error 0.4764(best: 0.4657)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0847 | Time 72.5808(69.4669) | Bit/dim 3.8019(3.8000) | Xent 1.3653(1.4088) | Loss 13.6243(10.5292) | Error 0.4894(0.5046) Steps 0(0.00) | Grad Norm 4.5798(6.3646) | Total Time 0.00(0.00)\n",
      "Iter 0848 | Time 71.3470(69.5233) | Bit/dim 3.7802(3.7994) | Xent 1.3506(1.4070) | Loss 9.7192(10.5049) | Error 0.4781(0.5038) Steps 0(0.00) | Grad Norm 3.4183(6.2762) | Total Time 0.00(0.00)\n",
      "Iter 0849 | Time 81.6681(69.8876) | Bit/dim 3.7835(3.7989) | Xent 1.3376(1.4049) | Loss 9.8860(10.4863) | Error 0.4774(0.5030) Steps 0(0.00) | Grad Norm 2.9647(6.1768) | Total Time 0.00(0.00)\n",
      "Iter 0850 | Time 65.5546(69.7577) | Bit/dim 3.7789(3.7983) | Xent 1.3588(1.4036) | Loss 9.8746(10.4680) | Error 0.4902(0.5027) Steps 0(0.00) | Grad Norm 4.4210(6.1241) | Total Time 0.00(0.00)\n",
      "Iter 0851 | Time 72.7626(69.8478) | Bit/dim 3.7736(3.7976) | Xent 1.3753(1.4027) | Loss 10.0033(10.4540) | Error 0.4932(0.5024) Steps 0(0.00) | Grad Norm 8.9240(6.2081) | Total Time 0.00(0.00)\n",
      "Iter 0852 | Time 71.5170(69.8979) | Bit/dim 3.8167(3.7981) | Xent 1.4513(1.4042) | Loss 9.8385(10.4356) | Error 0.5184(0.5029) Steps 0(0.00) | Grad Norm 13.0712(6.4140) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0142 | Time 23.0601, Epoch Time 474.6786(418.4299), Bit/dim 3.7829(best: 3.7689), Xent 1.3571, Loss 4.4614, Error 0.4791(best: 0.4657)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0853 | Time 64.7672(69.7440) | Bit/dim 3.7937(3.7980) | Xent 1.4074(1.4043) | Loss 13.1515(10.5170) | Error 0.4934(0.5026) Steps 0(0.00) | Grad Norm 10.7342(6.5436) | Total Time 0.00(0.00)\n",
      "Iter 0854 | Time 62.7541(69.5343) | Bit/dim 3.7732(3.7973) | Xent 1.3830(1.4036) | Loss 9.7785(10.4949) | Error 0.4858(0.5021) Steps 0(0.00) | Grad Norm 6.9440(6.5556) | Total Time 0.00(0.00)\n",
      "Iter 0855 | Time 67.5193(69.4738) | Bit/dim 3.7895(3.7970) | Xent 1.3665(1.4025) | Loss 9.9057(10.4772) | Error 0.4909(0.5017) Steps 0(0.00) | Grad Norm 6.4309(6.5519) | Total Time 0.00(0.00)\n",
      "Iter 0856 | Time 85.3987(69.9516) | Bit/dim 3.7805(3.7965) | Xent 1.3330(1.4004) | Loss 9.8367(10.4580) | Error 0.4808(0.5011) Steps 0(0.00) | Grad Norm 4.5815(6.4928) | Total Time 0.00(0.00)\n",
      "Iter 0857 | Time 74.4378(70.0861) | Bit/dim 3.7685(3.7957) | Xent 1.3590(1.3992) | Loss 9.9702(10.4434) | Error 0.4919(0.5008) Steps 0(0.00) | Grad Norm 4.5049(6.4332) | Total Time 0.00(0.00)\n",
      "Iter 0858 | Time 82.4109(70.4559) | Bit/dim 3.7911(3.7956) | Xent 1.3384(1.3974) | Loss 10.1259(10.4338) | Error 0.4866(0.5004) Steps 0(0.00) | Grad Norm 4.4533(6.3738) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0143 | Time 23.9613, Epoch Time 477.4832(420.2015), Bit/dim 3.7723(best: 3.7689), Xent 1.2979, Loss 4.4212, Error 0.4633(best: 0.4657)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0859 | Time 68.8928(70.4090) | Bit/dim 3.7729(3.7949) | Xent 1.3547(1.3961) | Loss 13.7113(10.5322) | Error 0.4876(0.5000) Steps 0(0.00) | Grad Norm 3.9494(6.3010) | Total Time 0.00(0.00)\n",
      "Iter 0860 | Time 64.1967(70.2226) | Bit/dim 3.7651(3.7940) | Xent 1.3226(1.3939) | Loss 9.4572(10.4999) | Error 0.4738(0.4992) Steps 0(0.00) | Grad Norm 3.0172(6.2025) | Total Time 0.00(0.00)\n",
      "Iter 0861 | Time 67.7743(70.1492) | Bit/dim 3.7708(3.7933) | Xent 1.3049(1.3912) | Loss 9.9889(10.4846) | Error 0.4700(0.4984) Steps 0(0.00) | Grad Norm 3.9529(6.1350) | Total Time 0.00(0.00)\n",
      "Iter 0862 | Time 63.5871(69.9523) | Bit/dim 3.7585(3.7923) | Xent 1.3366(1.3896) | Loss 9.9983(10.4700) | Error 0.4798(0.4978) Steps 0(0.00) | Grad Norm 2.8879(6.0376) | Total Time 0.00(0.00)\n",
      "Iter 0863 | Time 67.4583(69.8775) | Bit/dim 3.7678(3.7915) | Xent 1.3307(1.3878) | Loss 9.7980(10.4498) | Error 0.4702(0.4970) Steps 0(0.00) | Grad Norm 2.8201(5.9411) | Total Time 0.00(0.00)\n",
      "Iter 0864 | Time 71.6417(69.9304) | Bit/dim 3.7508(3.7903) | Xent 1.3005(1.3852) | Loss 9.8883(10.4330) | Error 0.4714(0.4962) Steps 0(0.00) | Grad Norm 2.9544(5.8515) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0144 | Time 23.9856, Epoch Time 443.2824(420.8939), Bit/dim 3.7584(best: 3.7689), Xent 1.2740, Loss 4.3954, Error 0.4563(best: 0.4633)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0865 | Time 67.9135(69.8699) | Bit/dim 3.7563(3.7893) | Xent 1.2965(1.3825) | Loss 13.4232(10.5227) | Error 0.4631(0.4952) Steps 0(0.00) | Grad Norm 2.9333(5.7639) | Total Time 0.00(0.00)\n",
      "Iter 0866 | Time 69.6186(69.8624) | Bit/dim 3.7528(3.7882) | Xent 1.2950(1.3799) | Loss 9.6625(10.4969) | Error 0.4685(0.4944) Steps 0(0.00) | Grad Norm 3.5510(5.6976) | Total Time 0.00(0.00)\n",
      "Iter 0867 | Time 72.2967(69.9354) | Bit/dim 3.7531(3.7871) | Xent 1.3574(1.3792) | Loss 9.7719(10.4751) | Error 0.4869(0.4942) Steps 0(0.00) | Grad Norm 7.4328(5.7496) | Total Time 0.00(0.00)\n",
      "Iter 0868 | Time 74.3023(70.0664) | Bit/dim 3.7705(3.7866) | Xent 1.4679(1.3819) | Loss 9.8808(10.4573) | Error 0.5211(0.4950) Steps 0(0.00) | Grad Norm 12.6724(5.9573) | Total Time 0.00(0.00)\n",
      "Iter 0869 | Time 80.3905(70.3761) | Bit/dim 3.7991(3.7870) | Xent 1.5136(1.3858) | Loss 10.1938(10.4494) | Error 0.5339(0.4962) Steps 0(0.00) | Grad Norm 16.1087(6.2618) | Total Time 0.00(0.00)\n",
      "Iter 0870 | Time 66.1604(70.2497) | Bit/dim 3.7759(3.7867) | Xent 1.4519(1.3878) | Loss 10.1064(10.4391) | Error 0.5250(0.4970) Steps 0(0.00) | Grad Norm 9.9545(6.3726) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0145 | Time 25.3014, Epoch Time 472.5822(422.4446), Bit/dim 3.7730(best: 3.7584), Xent 1.4072, Loss 4.4766, Error 0.5043(best: 0.4563)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0871 | Time 70.0177(70.2427) | Bit/dim 3.7690(3.7861) | Xent 1.4281(1.3890) | Loss 13.6302(10.5348) | Error 0.5100(0.4974) Steps 0(0.00) | Grad Norm 6.8732(6.3876) | Total Time 0.00(0.00)\n",
      "Iter 0872 | Time 80.7825(70.5589) | Bit/dim 3.7818(3.7860) | Xent 1.3615(1.3882) | Loss 10.0842(10.5213) | Error 0.4908(0.4972) Steps 0(0.00) | Grad Norm 3.9620(6.3149) | Total Time 0.00(0.00)\n",
      "Iter 0873 | Time 69.9394(70.5403) | Bit/dim 3.7803(3.7858) | Xent 1.4365(1.3896) | Loss 10.0756(10.5080) | Error 0.5124(0.4977) Steps 0(0.00) | Grad Norm 7.1173(6.3389) | Total Time 0.00(0.00)\n",
      "Iter 0874 | Time 63.4477(70.3275) | Bit/dim 3.7693(3.7853) | Xent 1.4035(1.3901) | Loss 9.9222(10.4904) | Error 0.5074(0.4980) Steps 0(0.00) | Grad Norm 6.0801(6.3312) | Total Time 0.00(0.00)\n",
      "Iter 0875 | Time 64.9184(70.1653) | Bit/dim 3.7578(3.7845) | Xent 1.3960(1.3902) | Loss 9.5381(10.4618) | Error 0.5065(0.4982) Steps 0(0.00) | Grad Norm 3.8427(6.2565) | Total Time 0.00(0.00)\n",
      "Iter 0876 | Time 66.2387(70.0475) | Bit/dim 3.7737(3.7842) | Xent 1.3448(1.3889) | Loss 9.9157(10.4454) | Error 0.4828(0.4978) Steps 0(0.00) | Grad Norm 3.9752(6.1881) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0146 | Time 23.2238, Epoch Time 454.6621(423.4111), Bit/dim 3.7600(best: 3.7584), Xent 1.3249, Loss 4.4225, Error 0.4775(best: 0.4563)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0877 | Time 68.2803(69.9944) | Bit/dim 3.7611(3.7835) | Xent 1.3756(1.3885) | Loss 13.2706(10.5302) | Error 0.4889(0.4975) Steps 0(0.00) | Grad Norm 4.6824(6.1429) | Total Time 0.00(0.00)\n",
      "Iter 0878 | Time 70.7926(70.0184) | Bit/dim 3.7807(3.7834) | Xent 1.3368(1.3869) | Loss 9.9823(10.5137) | Error 0.4821(0.4970) Steps 0(0.00) | Grad Norm 4.4430(6.0919) | Total Time 0.00(0.00)\n",
      "Iter 0879 | Time 85.8654(70.4938) | Bit/dim 3.7851(3.7835) | Xent 1.3378(1.3855) | Loss 9.9882(10.4980) | Error 0.4845(0.4966) Steps 0(0.00) | Grad Norm 3.4985(6.0141) | Total Time 0.00(0.00)\n",
      "Iter 0880 | Time 63.8319(70.2939) | Bit/dim 3.7683(3.7830) | Xent 1.3216(1.3835) | Loss 9.7393(10.4752) | Error 0.4800(0.4961) Steps 0(0.00) | Grad Norm 4.1097(5.9570) | Total Time 0.00(0.00)\n",
      "Iter 0881 | Time 75.3067(70.4443) | Bit/dim 3.7660(3.7825) | Xent 1.3005(1.3810) | Loss 9.9300(10.4589) | Error 0.4587(0.4950) Steps 0(0.00) | Grad Norm 5.8980(5.9552) | Total Time 0.00(0.00)\n",
      "Iter 0882 | Time 72.6634(70.5109) | Bit/dim 3.7539(3.7816) | Xent 1.3363(1.3797) | Loss 10.0282(10.4459) | Error 0.4765(0.4945) Steps 0(0.00) | Grad Norm 7.0675(5.9886) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0147 | Time 23.9229, Epoch Time 476.9207(425.0164), Bit/dim 3.7689(best: 3.7584), Xent 1.3176, Loss 4.4277, Error 0.4727(best: 0.4563)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0883 | Time 70.6086(70.5138) | Bit/dim 3.7676(3.7812) | Xent 1.3607(1.3791) | Loss 13.3795(10.5339) | Error 0.4841(0.4942) Steps 0(0.00) | Grad Norm 5.8571(5.9846) | Total Time 0.00(0.00)\n",
      "Iter 0884 | Time 63.9852(70.3180) | Bit/dim 3.7562(3.7805) | Xent 1.3430(1.3781) | Loss 9.8744(10.5142) | Error 0.4764(0.4936) Steps 0(0.00) | Grad Norm 3.8574(5.9208) | Total Time 0.00(0.00)\n",
      "Iter 0885 | Time 64.9747(70.1577) | Bit/dim 3.7603(3.7799) | Xent 1.3016(1.3758) | Loss 9.8592(10.4945) | Error 0.4644(0.4927) Steps 0(0.00) | Grad Norm 5.4547(5.9068) | Total Time 0.00(0.00)\n",
      "Iter 0886 | Time 65.1810(70.0084) | Bit/dim 3.7589(3.7792) | Xent 1.3137(1.3739) | Loss 9.8448(10.4750) | Error 0.4730(0.4922) Steps 0(0.00) | Grad Norm 6.3722(5.9208) | Total Time 0.00(0.00)\n",
      "Iter 0887 | Time 74.3342(70.1381) | Bit/dim 3.7734(3.7791) | Xent 1.3132(1.3721) | Loss 9.8338(10.4558) | Error 0.4692(0.4915) Steps 0(0.00) | Grad Norm 6.0445(5.9245) | Total Time 0.00(0.00)\n",
      "Iter 0888 | Time 63.8778(69.9503) | Bit/dim 3.7456(3.7781) | Xent 1.3025(1.3700) | Loss 9.8415(10.4374) | Error 0.4629(0.4906) Steps 0(0.00) | Grad Norm 3.8461(5.8621) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0148 | Time 24.5768, Epoch Time 443.2475(425.5633), Bit/dim 3.7450(best: 3.7584), Xent 1.2699, Loss 4.3799, Error 0.4537(best: 0.4563)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0889 | Time 88.7404(70.5140) | Bit/dim 3.7466(3.7771) | Xent 1.2917(1.3676) | Loss 13.4107(10.5266) | Error 0.4676(0.4899) Steps 0(0.00) | Grad Norm 3.3294(5.7862) | Total Time 0.00(0.00)\n",
      "Iter 0890 | Time 71.9737(70.5578) | Bit/dim 3.7598(3.7766) | Xent 1.3010(1.3656) | Loss 10.0553(10.5124) | Error 0.4669(0.4892) Steps 0(0.00) | Grad Norm 5.7913(5.7863) | Total Time 0.00(0.00)\n",
      "Iter 0891 | Time 71.9260(70.5989) | Bit/dim 3.7672(3.7763) | Xent 1.3390(1.3648) | Loss 9.7187(10.4886) | Error 0.4826(0.4890) Steps 0(0.00) | Grad Norm 7.1710(5.8279) | Total Time 0.00(0.00)\n",
      "Iter 0892 | Time 80.7422(70.9032) | Bit/dim 3.7619(3.7759) | Xent 1.3257(1.3637) | Loss 9.4636(10.4579) | Error 0.4736(0.4886) Steps 0(0.00) | Grad Norm 7.9153(5.8905) | Total Time 0.00(0.00)\n",
      "Iter 0893 | Time 72.6236(70.9548) | Bit/dim 3.7463(3.7750) | Xent 1.3430(1.3630) | Loss 10.0020(10.4442) | Error 0.4845(0.4884) Steps 0(0.00) | Grad Norm 7.9297(5.9517) | Total Time 0.00(0.00)\n",
      "Iter 0894 | Time 68.3060(70.8753) | Bit/dim 3.7576(3.7745) | Xent 1.3248(1.3619) | Loss 9.7919(10.4246) | Error 0.4764(0.4881) Steps 0(0.00) | Grad Norm 6.0315(5.9541) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0149 | Time 23.7470, Epoch Time 493.9534(427.6150), Bit/dim 3.7457(best: 3.7450), Xent 1.2594, Loss 4.3754, Error 0.4473(best: 0.4537)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0895 | Time 67.4009(70.7711) | Bit/dim 3.7548(3.7739) | Xent 1.2897(1.3597) | Loss 12.9175(10.4994) | Error 0.4670(0.4875) Steps 0(0.00) | Grad Norm 1.9018(5.8325) | Total Time 0.00(0.00)\n",
      "Iter 0896 | Time 64.7417(70.5902) | Bit/dim 3.7536(3.7733) | Xent 1.3311(1.3589) | Loss 9.6286(10.4733) | Error 0.4812(0.4873) Steps 0(0.00) | Grad Norm 5.1544(5.8121) | Total Time 0.00(0.00)\n",
      "Iter 0897 | Time 72.8122(70.6569) | Bit/dim 3.7467(3.7725) | Xent 1.3172(1.3576) | Loss 9.7885(10.4527) | Error 0.4696(0.4867) Steps 0(0.00) | Grad Norm 5.6149(5.8062) | Total Time 0.00(0.00)\n",
      "Iter 0898 | Time 70.1487(70.6416) | Bit/dim 3.7295(3.7712) | Xent 1.3138(1.3563) | Loss 9.9426(10.4374) | Error 0.4736(0.4863) Steps 0(0.00) | Grad Norm 3.4472(5.7355) | Total Time 0.00(0.00)\n",
      "Iter 0899 | Time 71.4331(70.6654) | Bit/dim 3.7410(3.7703) | Xent 1.2996(1.3546) | Loss 9.8602(10.4201) | Error 0.4619(0.4856) Steps 0(0.00) | Grad Norm 6.2192(5.7500) | Total Time 0.00(0.00)\n",
      "Iter 0900 | Time 69.8934(70.6422) | Bit/dim 3.7669(3.7702) | Xent 1.3450(1.3543) | Loss 9.8483(10.4030) | Error 0.4889(0.4857) Steps 0(0.00) | Grad Norm 7.7088(5.8087) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0150 | Time 24.4254, Epoch Time 456.6369(428.4857), Bit/dim 3.7498(best: 3.7450), Xent 1.2940, Loss 4.3967, Error 0.4657(best: 0.4473)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0901 | Time 66.0668(70.5050) | Bit/dim 3.7432(3.7694) | Xent 1.3463(1.3541) | Loss 13.6946(10.5017) | Error 0.4828(0.4856) Steps 0(0.00) | Grad Norm 6.6200(5.8331) | Total Time 0.00(0.00)\n",
      "Iter 0902 | Time 71.6449(70.5392) | Bit/dim 3.7849(3.7698) | Xent 1.3523(1.3540) | Loss 9.7315(10.4786) | Error 0.4912(0.4858) Steps 0(0.00) | Grad Norm 11.4185(6.0006) | Total Time 0.00(0.00)\n",
      "Iter 0903 | Time 63.7646(70.3359) | Bit/dim 3.8037(3.7709) | Xent 1.4387(1.3566) | Loss 9.9754(10.4635) | Error 0.5034(0.4863) Steps 0(0.00) | Grad Norm 13.6262(6.2294) | Total Time 0.00(0.00)\n",
      "Iter 0904 | Time 69.6921(70.3166) | Bit/dim 3.7831(3.7712) | Xent 1.6223(1.3645) | Loss 9.9963(10.4495) | Error 0.5584(0.4885) Steps 0(0.00) | Grad Norm 16.2265(6.5293) | Total Time 0.00(0.00)\n",
      "Iter 0905 | Time 67.0676(70.2191) | Bit/dim 3.9043(3.7752) | Xent 1.8995(1.3806) | Loss 10.4823(10.4505) | Error 0.6194(0.4924) Steps 0(0.00) | Grad Norm 23.4501(7.0369) | Total Time 0.00(0.00)\n",
      "Iter 0906 | Time 74.1255(70.3363) | Bit/dim 3.9525(3.7805) | Xent 1.5570(1.3859) | Loss 10.5231(10.4527) | Error 0.5691(0.4947) Steps 0(0.00) | Grad Norm 11.9430(7.1841) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0151 | Time 24.6512, Epoch Time 453.1468(429.2255), Bit/dim 3.9217(best: 3.7450), Xent 1.4542, Loss 4.6487, Error 0.5382(best: 0.4473)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0907 | Time 76.7491(70.5287) | Bit/dim 3.9278(3.7849) | Xent 1.5108(1.3896) | Loss 14.0369(10.5602) | Error 0.5500(0.4964) Steps 0(0.00) | Grad Norm 6.6011(7.1666) | Total Time 0.00(0.00)\n",
      "Iter 0908 | Time 65.3497(70.3733) | Bit/dim 3.8862(3.7880) | Xent 1.6086(1.3962) | Loss 10.5122(10.5587) | Error 0.5845(0.4990) Steps 0(0.00) | Grad Norm 7.7797(7.1850) | Total Time 0.00(0.00)\n",
      "Iter 0909 | Time 70.1801(70.3675) | Bit/dim 3.8496(3.7898) | Xent 1.5034(1.3994) | Loss 10.3319(10.5519) | Error 0.5433(0.5003) Steps 0(0.00) | Grad Norm 3.6212(7.0781) | Total Time 0.00(0.00)\n",
      "Iter 0910 | Time 72.4762(70.4308) | Bit/dim 3.8684(3.7922) | Xent 1.4705(1.4015) | Loss 10.0471(10.5368) | Error 0.5291(0.5012) Steps 0(0.00) | Grad Norm 3.4405(6.9690) | Total Time 0.00(0.00)\n",
      "Iter 0911 | Time 79.9558(70.7165) | Bit/dim 3.8567(3.7941) | Xent 1.4803(1.4039) | Loss 10.4838(10.5352) | Error 0.5415(0.5024) Steps 0(0.00) | Grad Norm 3.5190(6.8655) | Total Time 0.00(0.00)\n",
      "Iter 0912 | Time 67.7143(70.6265) | Bit/dim 3.8499(3.7958) | Xent 1.4526(1.4054) | Loss 9.8461(10.5145) | Error 0.5244(0.5031) Steps 0(0.00) | Grad Norm 3.2813(6.7580) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0152 | Time 24.5038, Epoch Time 472.5937(430.5266), Bit/dim 3.8478(best: 3.7450), Xent 1.3711, Loss 4.5333, Error 0.4920(best: 0.4473)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0913 | Time 82.5172(70.9832) | Bit/dim 3.8388(3.7971) | Xent 1.4207(1.4058) | Loss 14.3823(10.6306) | Error 0.5096(0.5033) Steps 0(0.00) | Grad Norm 3.0160(6.6457) | Total Time 0.00(0.00)\n",
      "Iter 0914 | Time 67.9896(70.8934) | Bit/dim 3.8418(3.7984) | Xent 1.3774(1.4050) | Loss 10.1078(10.6149) | Error 0.4966(0.5031) Steps 0(0.00) | Grad Norm 3.8340(6.5613) | Total Time 0.00(0.00)\n",
      "Iter 0915 | Time 64.5579(70.7033) | Bit/dim 3.8105(3.7988) | Xent 1.4199(1.4054) | Loss 9.9937(10.5962) | Error 0.5174(0.5035) Steps 0(0.00) | Grad Norm 2.4652(6.4385) | Total Time 0.00(0.00)\n",
      "Iter 0916 | Time 74.7508(70.8248) | Bit/dim 3.8126(3.7992) | Xent 1.4008(1.4053) | Loss 9.9557(10.5770) | Error 0.5044(0.5035) Steps 0(0.00) | Grad Norm 3.5021(6.3504) | Total Time 0.00(0.00)\n",
      "Iter 0917 | Time 79.1217(71.0737) | Bit/dim 3.8150(3.7997) | Xent 1.4054(1.4053) | Loss 10.2789(10.5681) | Error 0.5109(0.5037) Steps 0(0.00) | Grad Norm 5.7344(6.3319) | Total Time 0.00(0.00)\n",
      "Iter 0918 | Time 66.9092(70.9487) | Bit/dim 3.8091(3.8000) | Xent 1.4133(1.4055) | Loss 9.9281(10.5489) | Error 0.5188(0.5042) Steps 0(0.00) | Grad Norm 6.6521(6.3415) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0153 | Time 23.8968, Epoch Time 475.7133(431.8822), Bit/dim 3.8032(best: 3.7450), Xent 1.3537, Loss 4.4801, Error 0.4819(best: 0.4473)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0919 | Time 72.3508(70.9908) | Bit/dim 3.8097(3.8003) | Xent 1.3957(1.4052) | Loss 13.5798(10.6398) | Error 0.4959(0.5039) Steps 0(0.00) | Grad Norm 6.6595(6.3510) | Total Time 0.00(0.00)\n",
      "Iter 0920 | Time 66.9261(70.8688) | Bit/dim 3.7899(3.7999) | Xent 1.3532(1.4037) | Loss 9.9228(10.6183) | Error 0.4799(0.5032) Steps 0(0.00) | Grad Norm 5.7977(6.3344) | Total Time 0.00(0.00)\n",
      "Iter 0921 | Time 86.0581(71.3245) | Bit/dim 3.7870(3.7996) | Xent 1.3778(1.4029) | Loss 9.9475(10.5982) | Error 0.5011(0.5032) Steps 0(0.00) | Grad Norm 3.8801(6.2608) | Total Time 0.00(0.00)\n",
      "Iter 0922 | Time 74.8217(71.4294) | Bit/dim 3.7882(3.7992) | Xent 1.3636(1.4017) | Loss 10.1858(10.5858) | Error 0.4898(0.5028) Steps 0(0.00) | Grad Norm 2.7951(6.1568) | Total Time 0.00(0.00)\n",
      "Iter 0923 | Time 83.9109(71.8039) | Bit/dim 3.7752(3.7985) | Xent 1.3480(1.4001) | Loss 9.7723(10.5614) | Error 0.4856(0.5022) Steps 0(0.00) | Grad Norm 3.2922(6.0709) | Total Time 0.00(0.00)\n",
      "Iter 0924 | Time 76.3625(71.9406) | Bit/dim 3.7840(3.7981) | Xent 1.3431(1.3984) | Loss 9.8946(10.5414) | Error 0.4792(0.5016) Steps 0(0.00) | Grad Norm 3.1195(5.9824) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0154 | Time 25.0564, Epoch Time 501.0898(433.9584), Bit/dim 3.7803(best: 3.7450), Xent 1.3123, Loss 4.4364, Error 0.4693(best: 0.4473)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0925 | Time 67.4420(71.8057) | Bit/dim 3.7787(3.7975) | Xent 1.3504(1.3970) | Loss 13.5106(10.6305) | Error 0.4815(0.5010) Steps 0(0.00) | Grad Norm 5.1950(5.9587) | Total Time 0.00(0.00)\n",
      "Iter 0926 | Time 71.5448(71.7979) | Bit/dim 3.7806(3.7970) | Xent 1.3661(1.3960) | Loss 10.1152(10.6150) | Error 0.4845(0.5005) Steps 0(0.00) | Grad Norm 7.9498(6.0185) | Total Time 0.00(0.00)\n",
      "Iter 0927 | Time 70.5623(71.7608) | Bit/dim 3.8054(3.7972) | Xent 1.4003(1.3962) | Loss 10.1422(10.6008) | Error 0.5077(0.5007) Steps 0(0.00) | Grad Norm 10.7437(6.1602) | Total Time 0.00(0.00)\n",
      "Iter 0928 | Time 77.1413(71.9222) | Bit/dim 3.8094(3.7976) | Xent 1.4761(1.3986) | Loss 9.9989(10.5828) | Error 0.5260(0.5014) Steps 0(0.00) | Grad Norm 9.8777(6.2717) | Total Time 0.00(0.00)\n",
      "Iter 0929 | Time 78.2528(72.1121) | Bit/dim 3.7608(3.7965) | Xent 1.3572(1.3973) | Loss 9.7791(10.5587) | Error 0.4806(0.5008) Steps 0(0.00) | Grad Norm 4.2143(6.2100) | Total Time 0.00(0.00)\n",
      "Iter 0930 | Time 70.3878(72.0604) | Bit/dim 3.8053(3.7968) | Xent 1.4140(1.3978) | Loss 9.9157(10.5394) | Error 0.5079(0.5010) Steps 0(0.00) | Grad Norm 11.8471(6.3791) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0155 | Time 24.5102, Epoch Time 475.5351(435.2057), Bit/dim 3.7944(best: 3.7450), Xent 1.6438, Loss 4.6163, Error 0.5630(best: 0.4473)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0931 | Time 71.4490(72.0421) | Bit/dim 3.7871(3.7965) | Xent 1.6884(1.4065) | Loss 14.1726(10.6484) | Error 0.5785(0.5033) Steps 0(0.00) | Grad Norm 17.7601(6.7206) | Total Time 0.00(0.00)\n",
      "Iter 0932 | Time 80.3755(72.2921) | Bit/dim 3.8815(3.7990) | Xent 1.8769(1.4206) | Loss 11.0538(10.6605) | Error 0.6038(0.5064) Steps 0(0.00) | Grad Norm 28.0543(7.3606) | Total Time 0.00(0.00)\n",
      "Iter 0933 | Time 67.0622(72.1352) | Bit/dim 4.0718(3.8072) | Xent 1.9571(1.4367) | Loss 11.2561(10.6784) | Error 0.6211(0.5098) Steps 0(0.00) | Grad Norm 32.1035(8.1029) | Total Time 0.00(0.00)\n",
      "Iter 0934 | Time 73.7740(72.1843) | Bit/dim 4.0048(3.8131) | Xent 1.6825(1.4441) | Loss 10.9537(10.6867) | Error 0.5994(0.5125) Steps 0(0.00) | Grad Norm 9.0758(8.1321) | Total Time 0.00(0.00)\n",
      "Iter 0935 | Time 70.0383(72.1199) | Bit/dim 3.9820(3.8182) | Xent 1.6307(1.4497) | Loss 10.9154(10.6935) | Error 0.5845(0.5146) Steps 0(0.00) | Grad Norm 14.4734(8.3223) | Total Time 0.00(0.00)\n",
      "Iter 0936 | Time 80.3049(72.3655) | Bit/dim 4.0004(3.8237) | Xent 1.6169(1.4547) | Loss 10.9659(10.7017) | Error 0.5855(0.5168) Steps 0(0.00) | Grad Norm 7.0562(8.2843) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0156 | Time 27.2016, Epoch Time 486.6944(436.7504), Bit/dim 4.0257(best: 3.7450), Xent 1.5121, Loss 4.7817, Error 0.5475(best: 0.4473)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0937 | Time 76.0271(72.4753) | Bit/dim 4.0300(3.8298) | Xent 1.5714(1.4582) | Loss 17.0098(10.8909) | Error 0.5754(0.5185) Steps 0(0.00) | Grad Norm 4.3714(8.1669) | Total Time 0.00(0.00)\n",
      "Iter 0938 | Time 78.5655(72.6580) | Bit/dim 3.9950(3.8348) | Xent 1.5383(1.4606) | Loss 10.8176(10.8887) | Error 0.5599(0.5198) Steps 0(0.00) | Grad Norm 4.5524(8.0585) | Total Time 0.00(0.00)\n",
      "Iter 0939 | Time 72.6156(72.6568) | Bit/dim 3.9362(3.8378) | Xent 1.5184(1.4624) | Loss 10.9091(10.8893) | Error 0.5571(0.5209) Steps 0(0.00) | Grad Norm 4.9904(7.9664) | Total Time 0.00(0.00)\n",
      "Iter 0940 | Time 84.9599(73.0259) | Bit/dim 3.9410(3.8409) | Xent 1.4592(1.4623) | Loss 10.3163(10.8722) | Error 0.5225(0.5209) Steps 0(0.00) | Grad Norm 6616561954069920.0000(198496858622105.5000) | Total Time 0.00(0.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_rl.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 8000 --test_batch_size 5000 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_rl_stdlearnscale_15_run1 --resume ../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_rl_stdlearnscale_15_run1/epoch_125_checkpt.pth --seed 1 --conditional True --controlled_tol True --train_mode semisup --lr 0.01 --warmup_iters 1000 --atol 1e-4  --rtol 1e-4 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5 --scale_fac 1.0 --gate cnn2 --scale_std 15.0 --max_grad_norm 20.0\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
