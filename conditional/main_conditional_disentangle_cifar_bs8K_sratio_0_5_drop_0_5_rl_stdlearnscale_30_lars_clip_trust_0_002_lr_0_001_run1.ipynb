{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_rl_lars.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time, count_nfe_gate\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "import apex\n",
      "from apex.parallel.LARC import LARC\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "GATES = [\"cnn1\", \"cnn2\", \"rnn\"]\n",
      "\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--gate', type=str, default='cnn1', choices=GATES)\n",
      "parser.add_argument('--scale', type=float, default=1.0)\n",
      "parser.add_argument('--scale_fac', type=float, default=1.0)\n",
      "parser.add_argument('--scale_std', type=float, default=1.0)\n",
      "parser.add_argument('--eta', default=0.1, type=float,\n",
      "                        help='tuning parameter that allows us to trade-off the competing goals of' \n",
      "                                'minimizing the prediction loss and maximizing the gate rewards ')\n",
      "parser.add_argument('--rl-weight', default=0.01, type=float,\n",
      "                        help='rl weight')\n",
      "\n",
      "parser.add_argument('--gamma', default=0.99, type=float,\n",
      "                        help='discount factor, default: (0.99)')\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "# for lars\n",
      "parser.add_argument(\"--trust_coefficient\", type=float, default=0.02)\n",
      "parser.add_argument('--clip', type=eval, default=False, choices=[True, False])\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "import lib.odenvp_conditional_rl as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol, \"scale\": args.scale, \"scale_fac\": args.scale_fac, \"scale_std\": args.scale_std, \"gate\": args.gate},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    optimizer = LARC(optimizer, trust_coefficient=args.trust_coefficient, clip=args.clip, eps=1e-8)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.optim.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            # re-weight the gate rewards\n",
      "            normalized_eta = args.eta / len(logp_actions)\n",
      "            \n",
      "            # collect cumulative future rewards\n",
      "            R = - loss\n",
      "            cum_rewards = []\n",
      "            for r in nfe[::-1]:\n",
      "                R = -normalized_eta * r.view(-1,1) + args.gamma * R\n",
      "                cum_rewards.insert(0,R)\n",
      "            \n",
      "            # apply REINFORCE\n",
      "            rl_loss = 0\n",
      "            for lpa, r in zip(logp_actions, cum_rewards):\n",
      "                rl_loss = rl_loss - lpa.view(-1,1) * args.rl_weight * r\n",
      "                \n",
      "            loss = loss + rl_loss.mean()\n",
      "            \n",
      "            loss.backward()\n",
      "            \n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe_gate(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            for idx in range(len(model.module.transforms)):\n",
      "                for layer in model.module.transforms[idx].chain:\n",
      "                    if hasattr(layer, 'atol'):\n",
      "                        layer.odefunc.after_odeint()\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'train': atol[tol_indx].mean()}, itr)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'train': rtol[tol_indx].mean()}, itr)\n",
      "                    \n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples, atol, rtol, logp_actions, nfe = model(fixed_z, reverse=True)\n",
      "            generated_samples = generated_samples.view(-1, *data_shape)\n",
      "            for tol_indx in range(len(atol)):\n",
      "                writer.add_scalars('atol_gen_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                writer.add_scalars('rtol_gen_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=0.0001, autoencode=False, batch_norm=False, batch_size=8000, batch_size_schedule='', begin_epoch=1, clip=True, condition_ratio=0.5, conditional=True, controlled_tol=True, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, eta=0.1, gamma=0.99, gate='cnn2', imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=1, lr=0.001, max_grad_norm=20.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume=None, rl_weight=0.01, rtol=0.0001, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_rl_stdlearnscale_30_lars_clip_trust_0_002_lr_0_001_run1', scale=1.0, scale_fac=1.0, scale_std=30.0, seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=5000, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', trust_coefficient=0.002, val_freq=1, warmup_iters=1000.0, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1450886\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc_rl.py:291: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Iter 0001 | Time 95.6065(95.6065) | Bit/dim 8.8973(8.8973) | Xent 2.3026(2.3026) | Loss 20.9452(20.9452) | Error 0.8982(0.8982) Steps 430(430.00) | Grad Norm 27.4585(27.4585) | Total Time 0.00(0.00)\n",
      "Iter 0002 | Time 40.3078(93.9475) | Bit/dim 8.8890(8.8971) | Xent 2.3015(2.3026) | Loss 20.9091(20.9442) | Error 0.7848(0.8948) Steps 478(431.44) | Grad Norm 27.3406(27.4549) | Total Time 0.00(0.00)\n",
      "Iter 0003 | Time 39.3677(92.3101) | Bit/dim 8.8823(8.8966) | Xent 2.3005(2.3025) | Loss 20.9456(20.9442) | Error 0.7759(0.8913) Steps 496(433.38) | Grad Norm 26.7793(27.4347) | Total Time 0.00(0.00)\n",
      "Iter 0004 | Time 38.2372(90.6879) | Bit/dim 8.8468(8.8951) | Xent 2.2993(2.3024) | Loss 20.3730(20.9271) | Error 0.7771(0.8879) Steps 466(434.36) | Grad Norm 25.8043(27.3858) | Total Time 0.00(0.00)\n",
      "Iter 0005 | Time 36.6863(89.0679) | Bit/dim 8.8732(8.8945) | Xent 2.2982(2.3023) | Loss 20.6773(20.9196) | Error 0.7857(0.8848) Steps 448(434.76) | Grad Norm 25.1634(27.3191) | Total Time 0.00(0.00)\n",
      "Iter 0006 | Time 33.6788(87.4062) | Bit/dim 8.8147(8.8921) | Xent 2.2965(2.3021) | Loss 20.5143(20.9074) | Error 0.7737(0.8815) Steps 454(435.34) | Grad Norm 24.7031(27.2406) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0001 | Time 34.6681, Epoch Time 335.1032(335.1032), Bit/dim 8.7890(best: inf), Xent 2.2946, Loss 9.9363, Error 0.7642(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0007 | Time 46.2525(86.1716) | Bit/dim 8.8025(8.8894) | Xent 2.2948(2.3019) | Loss 23.1050(20.9733) | Error 0.7745(0.8782) Steps 466(436.26) | Grad Norm 23.7842(27.1369) | Total Time 0.00(0.00)\n",
      "Iter 0008 | Time 37.8654(84.7224) | Bit/dim 8.7666(8.8857) | Xent 2.2932(2.3016) | Loss 20.6356(20.9632) | Error 0.7685(0.8750) Steps 454(436.79) | Grad Norm 22.6648(27.0028) | Total Time 0.00(0.00)\n",
      "Iter 0009 | Time 35.3112(83.2401) | Bit/dim 8.7560(8.8818) | Xent 2.2906(2.3013) | Loss 20.0925(20.9371) | Error 0.7695(0.8718) Steps 442(436.95) | Grad Norm 21.3059(26.8319) | Total Time 0.00(0.00)\n",
      "Iter 0010 | Time 34.9851(81.7924) | Bit/dim 8.7289(8.8772) | Xent 2.2889(2.3009) | Loss 20.3742(20.9202) | Error 0.7815(0.8691) Steps 454(437.46) | Grad Norm 20.4355(26.6400) | Total Time 0.00(0.00)\n",
      "Iter 0011 | Time 35.0744(80.3909) | Bit/dim 8.7236(8.8726) | Xent 2.2858(2.3005) | Loss 20.3879(20.9042) | Error 0.7670(0.8660) Steps 430(437.24) | Grad Norm 19.1541(26.4154) | Total Time 0.00(0.00)\n",
      "Iter 0012 | Time 34.5587(79.0159) | Bit/dim 8.6609(8.8663) | Xent 2.2832(2.2999) | Loss 20.3865(20.8887) | Error 0.7618(0.8629) Steps 448(437.56) | Grad Norm 17.8548(26.1586) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0002 | Time 18.8795, Epoch Time 259.0322(332.8210), Bit/dim 8.6333(best: 8.7890), Xent 2.2791, Loss 9.7728, Error 0.7499(best: 0.7642)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0013 | Time 37.5857(77.7730) | Bit/dim 8.6879(8.8609) | Xent 2.2789(2.2993) | Loss 22.6436(20.9413) | Error 0.7536(0.8596) Steps 472(438.59) | Grad Norm 15.8322(25.8488) | Total Time 0.00(0.00)\n",
      "Iter 0014 | Time 34.7413(76.4821) | Bit/dim 8.6145(8.8535) | Xent 2.2765(2.2986) | Loss 19.9989(20.9131) | Error 0.7675(0.8569) Steps 442(438.70) | Grad Norm 14.6658(25.5133) | Total Time 0.00(0.00)\n",
      "Iter 0015 | Time 34.1086(75.2109) | Bit/dim 8.6319(8.8469) | Xent 2.2723(2.2978) | Loss 19.6502(20.8752) | Error 0.7591(0.8539) Steps 454(439.15) | Grad Norm 12.9544(25.1365) | Total Time 0.00(0.00)\n",
      "Iter 0016 | Time 36.9589(74.0633) | Bit/dim 8.5340(8.8375) | Xent 2.2680(2.2969) | Loss 20.1276(20.8528) | Error 0.7499(0.8508) Steps 418(438.52) | Grad Norm 12.4010(24.7545) | Total Time 0.00(0.00)\n",
      "Iter 0017 | Time 37.1990(72.9574) | Bit/dim 8.5305(8.8283) | Xent 2.2628(2.2959) | Loss 19.7965(20.8211) | Error 0.7510(0.8478) Steps 448(438.80) | Grad Norm 10.8813(24.3383) | Total Time 0.00(0.00)\n",
      "Iter 0018 | Time 33.4564(71.7724) | Bit/dim 8.5285(8.8193) | Xent 2.2572(2.2948) | Loss 20.2210(20.8031) | Error 0.7394(0.8446) Steps 454(439.26) | Grad Norm 9.6752(23.8984) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0003 | Time 17.5498, Epoch Time 247.4246(330.2592), Bit/dim 8.4948(best: 8.6333), Xent 2.2504, Loss 9.6200, Error 0.7360(best: 0.7499)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0019 | Time 39.1078(70.7924) | Bit/dim 8.4934(8.8095) | Xent 2.2517(2.2935) | Loss 22.7142(20.8604) | Error 0.7459(0.8416) Steps 472(440.24) | Grad Norm 8.5150(23.4369) | Total Time 0.00(0.00)\n",
      "Iter 0020 | Time 38.4019(69.8207) | Bit/dim 8.4831(8.7997) | Xent 2.2469(2.2921) | Loss 19.9319(20.8325) | Error 0.7446(0.8387) Steps 442(440.30) | Grad Norm 7.5000(22.9588) | Total Time 0.00(0.00)\n",
      "Iter 0021 | Time 38.2108(68.8724) | Bit/dim 8.4961(8.7906) | Xent 2.2390(2.2905) | Loss 19.3883(20.7892) | Error 0.7364(0.8356) Steps 442(440.35) | Grad Norm 6.5649(22.4669) | Total Time 0.00(0.00)\n",
      "Iter 0022 | Time 35.3518(67.8668) | Bit/dim 8.4598(8.7807) | Xent 2.2306(2.2887) | Loss 19.9197(20.7631) | Error 0.7412(0.8328) Steps 448(440.58) | Grad Norm 6.4382(21.9861) | Total Time 0.00(0.00)\n",
      "Iter 0023 | Time 32.8389(66.8160) | Bit/dim 8.4325(8.7702) | Xent 2.2244(2.2868) | Loss 19.2233(20.7169) | Error 0.7440(0.8301) Steps 418(439.90) | Grad Norm 6.1066(21.5097) | Total Time 0.00(0.00)\n",
      "Iter 0024 | Time 33.8483(65.8269) | Bit/dim 8.4228(8.7598) | Xent 2.2196(2.2847) | Loss 19.8903(20.6921) | Error 0.7492(0.8277) Steps 442(439.96) | Grad Norm 6.4525(21.0580) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0004 | Time 18.9738, Epoch Time 252.6542(327.9310), Bit/dim 8.4015(best: 8.4948), Xent 2.2078, Loss 9.5054, Error 0.7346(best: 0.7360)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0025 | Time 39.1352(65.0262) | Bit/dim 8.4048(8.7492) | Xent 2.2070(2.2824) | Loss 22.5818(20.7488) | Error 0.7410(0.8251) Steps 418(439.30) | Grad Norm 6.7875(20.6299) | Total Time 0.00(0.00)\n",
      "Iter 0026 | Time 36.3161(64.1649) | Bit/dim 8.4112(8.7390) | Xent 2.2042(2.2801) | Loss 19.8308(20.7213) | Error 0.7504(0.8229) Steps 472(440.28) | Grad Norm 7.0576(20.2227) | Total Time 0.00(0.00)\n",
      "Iter 0027 | Time 36.5519(63.3365) | Bit/dim 8.3932(8.7287) | Xent 2.1966(2.2776) | Loss 19.8906(20.6964) | Error 0.7546(0.8208) Steps 466(441.06) | Grad Norm 7.3494(19.8365) | Total Time 0.00(0.00)\n",
      "Iter 0028 | Time 37.2261(62.5532) | Bit/dim 8.3738(8.7180) | Xent 2.1915(2.2750) | Loss 19.7001(20.6665) | Error 0.7475(0.8186) Steps 466(441.80) | Grad Norm 7.6538(19.4710) | Total Time 0.00(0.00)\n",
      "Iter 0029 | Time 38.5561(61.8333) | Bit/dim 8.3383(8.7066) | Xent 2.1825(2.2722) | Loss 19.9045(20.6436) | Error 0.7421(0.8163) Steps 466(442.53) | Grad Norm 7.5286(19.1127) | Total Time 0.00(0.00)\n",
      "Iter 0030 | Time 32.3775(60.9496) | Bit/dim 8.3293(8.6953) | Xent 2.1737(2.2692) | Loss 19.4814(20.6088) | Error 0.7489(0.8143) Steps 430(442.15) | Grad Norm 7.9121(18.7767) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0005 | Time 17.9889, Epoch Time 254.0940(325.7159), Bit/dim 8.3075(best: 8.4015), Xent 2.1612, Loss 9.3881, Error 0.7350(best: 0.7346)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0031 | Time 34.8197(60.1657) | Bit/dim 8.3010(8.6835) | Xent 2.1688(2.2662) | Loss 21.9543(20.6491) | Error 0.7494(0.8123) Steps 424(441.61) | Grad Norm 7.6208(18.4421) | Total Time 0.00(0.00)\n",
      "Iter 0032 | Time 38.4232(59.5134) | Bit/dim 8.3033(8.6721) | Xent 2.1619(2.2631) | Loss 19.4640(20.6136) | Error 0.7488(0.8104) Steps 424(441.08) | Grad Norm 7.5253(18.1145) | Total Time 0.00(0.00)\n",
      "Iter 0033 | Time 38.2196(58.8746) | Bit/dim 8.2767(8.6602) | Xent 2.1430(2.2595) | Loss 19.1582(20.5699) | Error 0.7364(0.8082) Steps 430(440.75) | Grad Norm 7.0567(17.7828) | Total Time 0.00(0.00)\n",
      "Iter 0034 | Time 34.0844(58.1309) | Bit/dim 8.2528(8.6480) | Xent 2.1447(2.2561) | Loss 19.2032(20.5289) | Error 0.7374(0.8061) Steps 430(440.43) | Grad Norm 6.8685(17.4554) | Total Time 0.00(0.00)\n",
      "Iter 0035 | Time 36.8844(57.4935) | Bit/dim 8.2528(8.6361) | Xent 2.1354(2.2524) | Loss 19.5099(20.4983) | Error 0.7216(0.8036) Steps 466(441.19) | Grad Norm 6.4905(17.1264) | Total Time 0.00(0.00)\n",
      "Iter 0036 | Time 33.7551(56.7813) | Bit/dim 8.2204(8.6237) | Xent 2.1337(2.2489) | Loss 19.2257(20.4602) | Error 0.7324(0.8014) Steps 430(440.86) | Grad Norm 5.9572(16.7914) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0006 | Time 18.5839, Epoch Time 250.6439(323.4637), Bit/dim 8.1916(best: 8.3075), Xent 2.1152, Loss 9.2493, Error 0.7177(best: 0.7346)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0037 | Time 35.6307(56.1468) | Bit/dim 8.2048(8.6111) | Xent 2.1169(2.2449) | Loss 20.7161(20.4678) | Error 0.7271(0.7992) Steps 454(441.25) | Grad Norm 5.3521(16.4482) | Total Time 0.00(0.00)\n",
      "Iter 0038 | Time 34.8066(55.5066) | Bit/dim 8.1831(8.5982) | Xent 2.1163(2.2411) | Loss 18.9911(20.4235) | Error 0.7239(0.7969) Steps 442(441.27) | Grad Norm 4.7491(16.0972) | Total Time 0.00(0.00)\n",
      "Iter 0039 | Time 37.5058(54.9666) | Bit/dim 8.1541(8.5849) | Xent 2.1054(2.2370) | Loss 18.5922(20.3686) | Error 0.7231(0.7947) Steps 472(442.20) | Grad Norm 4.3997(15.7463) | Total Time 0.00(0.00)\n",
      "Iter 0040 | Time 35.8360(54.3927) | Bit/dim 8.1354(8.5714) | Xent 2.1032(2.2330) | Loss 18.9718(20.3267) | Error 0.7299(0.7928) Steps 430(441.83) | Grad Norm 4.2345(15.4009) | Total Time 0.00(0.00)\n",
      "Iter 0041 | Time 33.4226(53.7636) | Bit/dim 8.1202(8.5579) | Xent 2.0969(2.2289) | Loss 19.2553(20.2945) | Error 0.7170(0.7905) Steps 430(441.48) | Grad Norm 4.3447(15.0692) | Total Time 0.00(0.00)\n",
      "Iter 0042 | Time 36.5129(53.2461) | Bit/dim 8.0976(8.5441) | Xent 2.0875(2.2246) | Loss 18.7575(20.2484) | Error 0.7147(0.7882) Steps 442(441.49) | Grad Norm 4.6051(14.7553) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0007 | Time 18.4725, Epoch Time 248.5749(321.2171), Bit/dim 8.0675(best: 8.1916), Xent 2.0760, Loss 9.1056, Error 0.7080(best: 0.7177)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0043 | Time 34.5854(52.6862) | Bit/dim 8.0889(8.5304) | Xent 2.0818(2.2204) | Loss 21.3796(20.2824) | Error 0.7121(0.7859) Steps 406(440.43) | Grad Norm 5.0484(14.4641) | Total Time 0.00(0.00)\n",
      "Iter 0044 | Time 36.3286(52.1955) | Bit/dim 8.0443(8.5158) | Xent 2.0732(2.2159) | Loss 18.6951(20.2348) | Error 0.7192(0.7839) Steps 448(440.65) | Grad Norm 4.8908(14.1769) | Total Time 0.00(0.00)\n",
      "Iter 0045 | Time 39.3468(51.8100) | Bit/dim 8.0256(8.5011) | Xent 2.0750(2.2117) | Loss 19.1102(20.2010) | Error 0.7123(0.7818) Steps 430(440.33) | Grad Norm 5.3081(13.9108) | Total Time 0.00(0.00)\n",
      "Iter 0046 | Time 35.6128(51.3241) | Bit/dim 8.0117(8.4865) | Xent 2.0615(2.2072) | Loss 18.5908(20.1527) | Error 0.7080(0.7796) Steps 484(441.64) | Grad Norm 4.7973(13.6374) | Total Time 0.00(0.00)\n",
      "Iter 0047 | Time 35.2058(50.8406) | Bit/dim 7.9748(8.4711) | Xent 2.0706(2.2031) | Loss 18.6893(20.1088) | Error 0.7072(0.7774) Steps 442(441.65) | Grad Norm 4.9076(13.3755) | Total Time 0.00(0.00)\n",
      "Iter 0048 | Time 39.8269(50.5102) | Bit/dim 7.9664(8.4560) | Xent 2.0688(2.1991) | Loss 18.9222(20.0732) | Error 0.7084(0.7753) Steps 442(441.66) | Grad Norm 4.9918(13.1240) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0008 | Time 18.6582, Epoch Time 255.4199(319.2432), Bit/dim 7.9242(best: 8.0675), Xent 2.0465, Loss 8.9474, Error 0.6933(best: 0.7080)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0049 | Time 33.8951(50.0117) | Bit/dim 7.9447(8.4406) | Xent 2.0620(2.1950) | Loss 20.7027(20.0921) | Error 0.7075(0.7733) Steps 430(441.31) | Grad Norm 4.7681(12.8734) | Total Time 0.00(0.00)\n",
      "Iter 0050 | Time 37.8088(49.6456) | Bit/dim 7.8961(8.4243) | Xent 2.0459(2.1905) | Loss 18.6138(20.0477) | Error 0.6929(0.7709) Steps 436(441.16) | Grad Norm 4.4068(12.6194) | Total Time 0.00(0.00)\n",
      "Iter 0051 | Time 35.7923(49.2300) | Bit/dim 7.8900(8.4083) | Xent 2.0501(2.1863) | Loss 18.3873(19.9979) | Error 0.6941(0.7686) Steps 418(440.46) | Grad Norm 4.2645(12.3687) | Total Time 0.00(0.00)\n",
      "Iter 0052 | Time 34.7366(48.7952) | Bit/dim 7.8439(8.3913) | Xent 2.0304(2.1816) | Loss 17.5485(19.9244) | Error 0.6909(0.7663) Steps 442(440.51) | Grad Norm 3.9866(12.1172) | Total Time 0.00(0.00)\n",
      "Iter 0053 | Time 36.6997(48.4324) | Bit/dim 7.8159(8.3741) | Xent 2.0496(2.1776) | Loss 18.4135(19.8791) | Error 0.7020(0.7643) Steps 478(441.63) | Grad Norm 4.1827(11.8792) | Total Time 0.00(0.00)\n",
      "Iter 0054 | Time 35.2450(48.0367) | Bit/dim 7.8013(8.3569) | Xent 2.0383(2.1735) | Loss 18.2257(19.8295) | Error 0.6839(0.7619) Steps 418(440.92) | Grad Norm 4.6328(11.6618) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0009 | Time 18.0634, Epoch Time 247.9348(317.1039), Bit/dim 7.7616(best: 7.9242), Xent 2.0260, Loss 8.7746, Error 0.6819(best: 0.6933)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0055 | Time 36.3680(47.6867) | Bit/dim 7.7772(8.3395) | Xent 2.0300(2.1692) | Loss 20.8509(19.8602) | Error 0.6845(0.7596) Steps 460(441.50) | Grad Norm 4.8965(11.4589) | Total Time 0.00(0.00)\n",
      "Iter 0056 | Time 35.4451(47.3194) | Bit/dim 7.7301(8.3212) | Xent 2.0326(2.1651) | Loss 18.2590(19.8121) | Error 0.6939(0.7576) Steps 442(441.51) | Grad Norm 4.5696(11.2522) | Total Time 0.00(0.00)\n",
      "Iter 0057 | Time 36.0797(46.9822) | Bit/dim 7.7146(8.3030) | Xent 2.0267(2.1609) | Loss 17.7725(19.7509) | Error 0.6880(0.7555) Steps 460(442.06) | Grad Norm 4.6589(11.0544) | Total Time 0.00(0.00)\n",
      "Iter 0058 | Time 35.9198(46.6504) | Bit/dim 7.6792(8.2843) | Xent 2.0211(2.1567) | Loss 17.9885(19.6981) | Error 0.6803(0.7533) Steps 436(441.88) | Grad Norm 4.3931(10.8545) | Total Time 0.00(0.00)\n",
      "Iter 0059 | Time 35.4850(46.3154) | Bit/dim 7.6449(8.2651) | Xent 2.0225(2.1527) | Loss 18.0640(19.6490) | Error 0.6803(0.7511) Steps 460(442.43) | Grad Norm 4.3555(10.6596) | Total Time 0.00(0.00)\n",
      "Iter 0060 | Time 37.5914(46.0537) | Bit/dim 7.6243(8.2459) | Xent 2.0233(2.1488) | Loss 17.9860(19.5991) | Error 0.6873(0.7492) Steps 466(443.13) | Grad Norm 4.2283(10.4666) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0010 | Time 19.4425, Epoch Time 252.1791(315.1562), Bit/dim 7.5887(best: 7.7616), Xent 2.0080, Loss 8.5927, Error 0.6724(best: 0.6819)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0061 | Time 37.6417(45.8013) | Bit/dim 7.5983(8.2265) | Xent 2.0161(2.1448) | Loss 20.3628(19.6221) | Error 0.6791(0.7471) Steps 460(443.64) | Grad Norm 4.2723(10.2808) | Total Time 0.00(0.00)\n",
      "Iter 0062 | Time 41.0859(45.6599) | Bit/dim 7.5663(8.2067) | Xent 2.0196(2.1411) | Loss 17.5911(19.5611) | Error 0.6811(0.7451) Steps 442(443.59) | Grad Norm 4.0041(10.0925) | Total Time 0.00(0.00)\n",
      "Iter 0063 | Time 38.5770(45.4474) | Bit/dim 7.5337(8.1865) | Xent 2.0070(2.1371) | Loss 17.2565(19.4920) | Error 0.6704(0.7428) Steps 460(444.08) | Grad Norm 3.9540(9.9083) | Total Time 0.00(0.00)\n",
      "Iter 0064 | Time 37.8539(45.2196) | Bit/dim 7.5043(8.1660) | Xent 2.0074(2.1332) | Loss 17.8392(19.4424) | Error 0.6791(0.7409) Steps 436(443.84) | Grad Norm 3.9747(9.7303) | Total Time 0.00(0.00)\n",
      "Iter 0065 | Time 35.0253(44.9137) | Bit/dim 7.4765(8.1453) | Xent 2.0172(2.1297) | Loss 17.7144(19.3906) | Error 0.6779(0.7390) Steps 454(444.15) | Grad Norm 3.8586(9.5542) | Total Time 0.00(0.00)\n",
      "Iter 0066 | Time 41.7257(44.8181) | Bit/dim 7.4512(8.1245) | Xent 2.0000(2.1258) | Loss 17.7102(19.3402) | Error 0.6723(0.7370) Steps 466(444.80) | Grad Norm 3.7562(9.3802) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0011 | Time 19.6495, Epoch Time 267.1752(313.7167), Bit/dim 7.4233(best: 7.5887), Xent 1.9972, Loss 8.4219, Error 0.6647(best: 0.6724)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0067 | Time 37.4666(44.5976) | Bit/dim 7.4247(8.1035) | Xent 2.0093(2.1223) | Loss 19.9103(19.3573) | Error 0.6799(0.7353) Steps 442(444.72) | Grad Norm 3.7539(9.2115) | Total Time 0.00(0.00)\n",
      "Iter 0068 | Time 38.4248(44.4124) | Bit/dim 7.4032(8.0825) | Xent 2.0120(2.1190) | Loss 17.8841(19.3131) | Error 0.6744(0.7335) Steps 508(446.62) | Grad Norm 3.7225(9.0468) | Total Time 0.00(0.00)\n",
      "Iter 0069 | Time 35.8842(44.1565) | Bit/dim 7.3763(8.0613) | Xent 1.9950(2.1153) | Loss 17.6104(19.2620) | Error 0.6627(0.7314) Steps 442(446.48) | Grad Norm 3.6637(8.8853) | Total Time 0.00(0.00)\n",
      "Iter 0070 | Time 40.0050(44.0320) | Bit/dim 7.3531(8.0401) | Xent 2.0038(2.1119) | Loss 17.6858(19.2147) | Error 0.6724(0.7296) Steps 448(446.52) | Grad Norm 3.5276(8.7246) | Total Time 0.00(0.00)\n",
      "Iter 0071 | Time 37.6457(43.8404) | Bit/dim 7.3276(8.0187) | Xent 2.0015(2.1086) | Loss 17.3368(19.1584) | Error 0.6747(0.7280) Steps 460(446.93) | Grad Norm 3.3661(8.5638) | Total Time 0.00(0.00)\n",
      "Iter 0072 | Time 36.0896(43.6079) | Bit/dim 7.3027(7.9972) | Xent 2.0081(2.1056) | Loss 17.4014(19.1057) | Error 0.6704(0.7262) Steps 454(447.14) | Grad Norm 3.5096(8.4122) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0012 | Time 19.3589, Epoch Time 260.8077(312.1295), Bit/dim 7.2833(best: 7.4233), Xent 1.9952, Loss 8.2809, Error 0.6641(best: 0.6647)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0073 | Time 41.1777(43.5350) | Bit/dim 7.2845(7.9758) | Xent 2.0105(2.1027) | Loss 19.1998(19.1085) | Error 0.6739(0.7247) Steps 490(448.42) | Grad Norm 3.3511(8.2604) | Total Time 0.00(0.00)\n",
      "Iter 0074 | Time 41.1647(43.4639) | Bit/dim 7.2633(7.9544) | Xent 1.9951(2.0995) | Loss 17.4365(19.0583) | Error 0.6720(0.7231) Steps 430(447.87) | Grad Norm 3.2664(8.1105) | Total Time 0.00(0.00)\n",
      "Iter 0075 | Time 38.9592(43.3287) | Bit/dim 7.2434(7.9331) | Xent 2.0003(2.0965) | Loss 16.6526(18.9861) | Error 0.6666(0.7214) Steps 442(447.70) | Grad Norm 2.8973(7.9541) | Total Time 0.00(0.00)\n",
      "Iter 0076 | Time 37.5003(43.1539) | Bit/dim 7.2308(7.9120) | Xent 2.0112(2.0940) | Loss 17.3289(18.9364) | Error 0.6859(0.7203) Steps 478(448.61) | Grad Norm 2.8325(7.8005) | Total Time 0.00(0.00)\n",
      "Iter 0077 | Time 38.6414(43.0185) | Bit/dim 7.2104(7.8910) | Xent 2.0064(2.0914) | Loss 17.0014(18.8784) | Error 0.6695(0.7188) Steps 490(449.85) | Grad Norm 2.6228(7.6452) | Total Time 0.00(0.00)\n",
      "Iter 0078 | Time 38.2995(42.8769) | Bit/dim 7.1875(7.8699) | Xent 2.0074(2.0888) | Loss 17.2086(18.8283) | Error 0.6749(0.7175) Steps 454(449.97) | Grad Norm 2.6636(7.4957) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0013 | Time 19.3029, Epoch Time 271.1846(310.9011), Bit/dim 7.1781(best: 7.2833), Xent 1.9988, Loss 8.1775, Error 0.6656(best: 0.6641)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0079 | Time 42.3205(42.8602) | Bit/dim 7.1798(7.8492) | Xent 2.0060(2.0863) | Loss 19.8384(18.8586) | Error 0.6761(0.7162) Steps 520(452.07) | Grad Norm 2.4296(7.3437) | Total Time 0.00(0.00)\n",
      "Iter 0080 | Time 39.1464(42.7488) | Bit/dim 7.1703(7.8288) | Xent 2.0104(2.0841) | Loss 17.3215(18.8125) | Error 0.6741(0.7150) Steps 442(451.77) | Grad Norm 2.3635(7.1943) | Total Time 0.00(0.00)\n",
      "Iter 0081 | Time 38.7578(42.6291) | Bit/dim 7.1500(7.8085) | Xent 1.9980(2.0815) | Loss 17.0804(18.7605) | Error 0.6764(0.7138) Steps 448(451.66) | Grad Norm 2.2354(7.0456) | Total Time 0.00(0.00)\n",
      "Iter 0082 | Time 36.1611(42.4350) | Bit/dim 7.1368(7.7883) | Xent 2.0173(2.0796) | Loss 16.8351(18.7028) | Error 0.6912(0.7131) Steps 436(451.19) | Grad Norm 2.1265(6.8980) | Total Time 0.00(0.00)\n",
      "Iter 0083 | Time 38.3279(42.3118) | Bit/dim 7.1252(7.7684) | Xent 2.0127(2.0776) | Loss 16.8537(18.6473) | Error 0.6847(0.7123) Steps 430(450.55) | Grad Norm 2.3660(6.7620) | Total Time 0.00(0.00)\n",
      "Iter 0084 | Time 37.7655(42.1754) | Bit/dim 7.1115(7.7487) | Xent 2.0176(2.0758) | Loss 16.9665(18.5969) | Error 0.6826(0.7114) Steps 478(451.38) | Grad Norm 1.8371(6.6143) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0014 | Time 19.6453, Epoch Time 267.9813(309.6135), Bit/dim 7.1062(best: 7.1781), Xent 2.0073, Loss 8.1098, Error 0.6735(best: 0.6641)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0085 | Time 39.1789(42.0855) | Bit/dim 7.1037(7.7294) | Xent 2.0108(2.0738) | Loss 19.1120(18.6123) | Error 0.6873(0.7107) Steps 472(451.99) | Grad Norm 1.9224(6.4735) | Total Time 0.00(0.00)\n",
      "Iter 0086 | Time 36.4284(41.9158) | Bit/dim 7.0946(7.7103) | Xent 2.0138(2.0720) | Loss 16.8117(18.5583) | Error 0.6783(0.7097) Steps 478(452.77) | Grad Norm 1.7287(6.3312) | Total Time 0.00(0.00)\n",
      "Iter 0087 | Time 41.2465(41.8957) | Bit/dim 7.0943(7.6918) | Xent 2.0269(2.0707) | Loss 17.2210(18.5182) | Error 0.6935(0.7092) Steps 502(454.25) | Grad Norm 1.5442(6.1876) | Total Time 0.00(0.00)\n",
      "Iter 0088 | Time 41.1666(41.8739) | Bit/dim 7.0755(7.6733) | Xent 2.0173(2.0691) | Loss 16.8590(18.4684) | Error 0.6905(0.7087) Steps 460(454.42) | Grad Norm 1.7308(6.0539) | Total Time 0.00(0.00)\n",
      "Iter 0089 | Time 42.3909(41.8894) | Bit/dim 7.0652(7.6551) | Xent 2.0171(2.0675) | Loss 16.6655(18.4143) | Error 0.6813(0.7078) Steps 496(455.67) | Grad Norm 1.5640(5.9192) | Total Time 0.00(0.00)\n",
      "Iter 0090 | Time 41.0491(41.8642) | Bit/dim 7.0685(7.6375) | Xent 2.0275(2.0663) | Loss 16.8825(18.3684) | Error 0.6970(0.7075) Steps 508(457.24) | Grad Norm 1.3239(5.7813) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0015 | Time 20.1054, Epoch Time 277.5266(308.6509), Bit/dim 7.0585(best: 7.1062), Xent 2.0137, Loss 8.0654, Error 0.6793(best: 0.6641)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0091 | Time 39.1266(41.7820) | Bit/dim 7.0592(7.6202) | Xent 2.0228(2.0650) | Loss 18.8817(18.3838) | Error 0.6924(0.7071) Steps 472(457.68) | Grad Norm 1.4146(5.6503) | Total Time 0.00(0.00)\n",
      "Iter 0092 | Time 41.6829(41.7791) | Bit/dim 7.0477(7.6030) | Xent 2.0198(2.0636) | Loss 16.5589(18.3290) | Error 0.6891(0.7065) Steps 466(457.93) | Grad Norm 1.4045(5.5229) | Total Time 0.00(0.00)\n",
      "Iter 0093 | Time 37.1356(41.6398) | Bit/dim 7.0409(7.5861) | Xent 2.0219(2.0624) | Loss 16.8318(18.2841) | Error 0.6934(0.7061) Steps 448(457.63) | Grad Norm 1.2598(5.3950) | Total Time 0.00(0.00)\n",
      "Iter 0094 | Time 38.3076(41.5398) | Bit/dim 7.0315(7.5695) | Xent 2.0232(2.0612) | Loss 16.7663(18.2386) | Error 0.6896(0.7056) Steps 490(458.61) | Grad Norm 1.8296(5.2881) | Total Time 0.00(0.00)\n",
      "Iter 0095 | Time 38.4093(41.4459) | Bit/dim 7.0411(7.5536) | Xent 2.0195(2.0600) | Loss 16.9175(18.1989) | Error 0.6947(0.7053) Steps 508(460.09) | Grad Norm 1.6340(5.1785) | Total Time 0.00(0.00)\n",
      "Iter 0096 | Time 37.7182(41.3341) | Bit/dim 7.0300(7.5379) | Xent 2.0247(2.0589) | Loss 16.7218(18.1546) | Error 0.6925(0.7049) Steps 478(460.62) | Grad Norm 1.0194(5.0537) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0016 | Time 19.8322, Epoch Time 267.9278(307.4292), Bit/dim 7.0256(best: 7.0585), Xent 2.0141, Loss 8.0327, Error 0.6784(best: 0.6641)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0097 | Time 38.7784(41.2574) | Bit/dim 7.0322(7.5228) | Xent 2.0229(2.0578) | Loss 19.5352(18.1960) | Error 0.6870(0.7044) Steps 466(460.79) | Grad Norm 1.4158(4.9445) | Total Time 0.00(0.00)\n",
      "Iter 0098 | Time 37.4486(41.1431) | Bit/dim 7.0197(7.5077) | Xent 2.0142(2.0565) | Loss 16.5758(18.1474) | Error 0.6887(0.7039) Steps 472(461.12) | Grad Norm 2.5296(4.8721) | Total Time 0.00(0.00)\n",
      "Iter 0099 | Time 37.4774(41.0332) | Bit/dim 7.0146(7.4929) | Xent 2.0202(2.0554) | Loss 16.8000(18.1070) | Error 0.7004(0.7038) Steps 466(461.27) | Grad Norm 2.0119(4.7863) | Total Time 0.00(0.00)\n",
      "Iter 0100 | Time 38.6683(40.9622) | Bit/dim 7.0108(7.4784) | Xent 2.0263(2.0545) | Loss 16.7706(18.0669) | Error 0.6892(0.7034) Steps 466(461.41) | Grad Norm 1.2415(4.6799) | Total Time 0.00(0.00)\n",
      "Iter 0101 | Time 38.1783(40.8787) | Bit/dim 6.9970(7.4640) | Xent 2.0285(2.0538) | Loss 16.9094(18.0322) | Error 0.6951(0.7031) Steps 454(461.19) | Grad Norm 1.9681(4.5986) | Total Time 0.00(0.00)\n",
      "Iter 0102 | Time 36.7581(40.7551) | Bit/dim 7.0053(7.4502) | Xent 2.0130(2.0525) | Loss 16.3581(17.9820) | Error 0.6844(0.7026) Steps 478(461.69) | Grad Norm 1.3358(4.5007) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0017 | Time 18.9792, Epoch Time 261.9536(306.0649), Bit/dim 7.0008(best: 7.0256), Xent 2.0071, Loss 8.0043, Error 0.6734(best: 0.6641)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0103 | Time 42.8446(40.8178) | Bit/dim 7.0028(7.4368) | Xent 2.0241(2.0517) | Loss 19.2202(18.0191) | Error 0.6815(0.7019) Steps 442(461.10) | Grad Norm 2.4975(4.4406) | Total Time 0.00(0.00)\n",
      "Iter 0104 | Time 36.4100(40.6855) | Bit/dim 6.9967(7.4236) | Xent 2.0188(2.0507) | Loss 16.4894(17.9732) | Error 0.6910(0.7016) Steps 460(461.07) | Grad Norm 1.0108(4.3377) | Total Time 0.00(0.00)\n",
      "Iter 0105 | Time 38.5118(40.6203) | Bit/dim 6.9887(7.4105) | Xent 2.0145(2.0496) | Loss 16.6535(17.9336) | Error 0.6912(0.7013) Steps 484(461.76) | Grad Norm 1.4449(4.2509) | Total Time 0.00(0.00)\n",
      "Iter 0106 | Time 38.7152(40.5632) | Bit/dim 6.9797(7.3976) | Xent 2.0149(2.0486) | Loss 16.6045(17.8938) | Error 0.6844(0.7008) Steps 466(461.88) | Grad Norm 1.2913(4.1621) | Total Time 0.00(0.00)\n",
      "Iter 0107 | Time 39.5963(40.5342) | Bit/dim 6.9872(7.3853) | Xent 2.0054(2.0473) | Loss 16.8062(17.8611) | Error 0.6830(0.7002) Steps 460(461.83) | Grad Norm 1.6642(4.0872) | Total Time 0.00(0.00)\n",
      "Iter 0108 | Time 41.7634(40.5710) | Bit/dim 6.9795(7.3731) | Xent 2.0034(2.0460) | Loss 16.8428(17.8306) | Error 0.6754(0.6995) Steps 502(463.03) | Grad Norm 1.2202(4.0012) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0018 | Time 20.1381, Epoch Time 273.7691(305.0961), Bit/dim 6.9789(best: 7.0008), Xent 1.9966, Loss 7.9772, Error 0.6691(best: 0.6641)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0109 | Time 42.6802(40.6343) | Bit/dim 6.9703(7.3610) | Xent 1.9980(2.0445) | Loss 19.5896(17.8833) | Error 0.6783(0.6989) Steps 490(463.84) | Grad Norm 2.8052(3.9653) | Total Time 0.00(0.00)\n",
      "Iter 0110 | Time 37.7417(40.5475) | Bit/dim 6.9713(7.3493) | Xent 1.9961(2.0431) | Loss 16.1479(17.8313) | Error 0.6874(0.6985) Steps 454(463.55) | Grad Norm 1.0837(3.8789) | Total Time 0.00(0.00)\n",
      "Iter 0111 | Time 39.8623(40.5270) | Bit/dim 6.9629(7.3378) | Xent 2.0006(2.0418) | Loss 16.7798(17.7997) | Error 0.6690(0.6976) Steps 454(463.26) | Grad Norm 1.7192(3.8141) | Total Time 0.00(0.00)\n",
      "Iter 0112 | Time 37.4654(40.4351) | Bit/dim 6.9647(7.3266) | Xent 2.0054(2.0407) | Loss 16.9033(17.7728) | Error 0.6785(0.6971) Steps 484(463.88) | Grad Norm 2.1551(3.7643) | Total Time 0.00(0.00)\n",
      "Iter 0113 | Time 38.2939(40.3709) | Bit/dim 6.9704(7.3159) | Xent 2.0128(2.0399) | Loss 16.5806(17.7371) | Error 0.6854(0.6967) Steps 466(463.95) | Grad Norm 2.1552(3.7160) | Total Time 0.00(0.00)\n",
      "Iter 0114 | Time 38.1567(40.3045) | Bit/dim 6.9647(7.3053) | Xent 2.0048(2.0388) | Loss 16.7263(17.7068) | Error 0.6735(0.6960) Steps 442(463.29) | Grad Norm 5.4475(3.7680) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0019 | Time 19.4607, Epoch Time 269.8374(304.0383), Bit/dim 6.9567(best: 6.9789), Xent 1.9862, Loss 7.9498, Error 0.6627(best: 0.6641)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0115 | Time 42.9733(40.3845) | Bit/dim 6.9624(7.2951) | Xent 1.9984(2.0376) | Loss 19.5006(17.7606) | Error 0.6720(0.6953) Steps 436(462.47) | Grad Norm 1.6159(3.7034) | Total Time 0.00(0.00)\n",
      "Iter 0116 | Time 39.5070(40.3582) | Bit/dim 6.9521(7.2848) | Xent 1.9915(2.0362) | Loss 16.4426(17.7210) | Error 0.6814(0.6949) Steps 490(463.29) | Grad Norm 2.9888(3.6820) | Total Time 0.00(0.00)\n",
      "Iter 0117 | Time 38.9873(40.3171) | Bit/dim 6.9519(7.2748) | Xent 2.0065(2.0353) | Loss 16.4874(17.6840) | Error 0.6798(0.6944) Steps 496(464.28) | Grad Norm 3.3652(3.6725) | Total Time 0.00(0.00)\n",
      "Iter 0118 | Time 42.1848(40.3731) | Bit/dim 6.9370(7.2646) | Xent 1.9819(2.0337) | Loss 16.3943(17.6453) | Error 0.6638(0.6935) Steps 520(465.95) | Grad Norm 1.7798(3.6157) | Total Time 0.00(0.00)\n",
      "Iter 0119 | Time 36.8739(40.2681) | Bit/dim 6.9358(7.2548) | Xent 1.9929(2.0325) | Loss 16.6879(17.6166) | Error 0.6731(0.6929) Steps 466(465.95) | Grad Norm 2.0237(3.5679) | Total Time 0.00(0.00)\n",
      "Iter 0120 | Time 36.4719(40.1542) | Bit/dim 6.9257(7.2449) | Xent 1.9912(2.0313) | Loss 16.7413(17.5903) | Error 0.6691(0.6922) Steps 460(465.77) | Grad Norm 2.0632(3.5228) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0020 | Time 20.3671, Epoch Time 273.1697(303.1123), Bit/dim 6.9319(best: 6.9567), Xent 1.9784, Loss 7.9211, Error 0.6632(best: 0.6627)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0121 | Time 38.8980(40.1166) | Bit/dim 6.9285(7.2354) | Xent 1.9897(2.0300) | Loss 19.1532(17.6372) | Error 0.6647(0.6914) Steps 460(465.60) | Grad Norm 1.5852(3.4647) | Total Time 0.00(0.00)\n",
      "Iter 0122 | Time 41.5191(40.1586) | Bit/dim 6.9229(7.2260) | Xent 1.9961(2.0290) | Loss 16.8248(17.6129) | Error 0.6731(0.6908) Steps 496(466.51) | Grad Norm 3.4750(3.4650) | Total Time 0.00(0.00)\n",
      "Iter 0123 | Time 40.3244(40.1636) | Bit/dim 6.9302(7.2172) | Xent 1.9916(2.0279) | Loss 16.8166(17.5890) | Error 0.6680(0.6901) Steps 490(467.21) | Grad Norm 1.6178(3.4096) | Total Time 0.00(0.00)\n",
      "Iter 0124 | Time 44.2398(40.2859) | Bit/dim 6.9149(7.2081) | Xent 1.9828(2.0265) | Loss 16.5272(17.5571) | Error 0.6674(0.6894) Steps 454(466.82) | Grad Norm 3.4568(3.4110) | Total Time 0.00(0.00)\n",
      "Iter 0125 | Time 37.8121(40.2117) | Bit/dim 6.9051(7.1990) | Xent 1.9969(2.0256) | Loss 15.9408(17.5086) | Error 0.6743(0.6890) Steps 466(466.79) | Grad Norm 2.4872(3.3833) | Total Time 0.00(0.00)\n",
      "Iter 0126 | Time 39.2549(40.1830) | Bit/dim 6.9046(7.1902) | Xent 1.9743(2.0241) | Loss 16.5833(17.4809) | Error 0.6699(0.6884) Steps 460(466.59) | Grad Norm 2.9025(3.3688) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0021 | Time 19.7653, Epoch Time 277.7574(302.3516), Bit/dim 6.9046(best: 6.9319), Xent 1.9740, Loss 7.8916, Error 0.6604(best: 0.6627)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0127 | Time 39.8333(40.1725) | Bit/dim 6.8971(7.1814) | Xent 1.9913(2.0231) | Loss 19.2131(17.5328) | Error 0.6731(0.6880) Steps 442(465.85) | Grad Norm 3.4479(3.3712) | Total Time 0.00(0.00)\n",
      "Iter 0128 | Time 42.0037(40.2274) | Bit/dim 6.9037(7.1730) | Xent 1.9770(2.0217) | Loss 16.5696(17.5039) | Error 0.6644(0.6872) Steps 454(465.50) | Grad Norm 2.1207(3.3337) | Total Time 0.00(0.00)\n",
      "Iter 0129 | Time 41.2061(40.2568) | Bit/dim 6.8983(7.1648) | Xent 1.9856(2.0206) | Loss 16.5151(17.4743) | Error 0.6715(0.6868) Steps 484(466.05) | Grad Norm 2.6159(3.3122) | Total Time 0.00(0.00)\n",
      "Iter 0130 | Time 38.3204(40.1987) | Bit/dim 6.8824(7.1563) | Xent 1.9788(2.0194) | Loss 16.2134(17.4364) | Error 0.6673(0.6862) Steps 478(466.41) | Grad Norm 1.4634(3.2567) | Total Time 0.00(0.00)\n",
      "Iter 0131 | Time 40.2314(40.1997) | Bit/dim 6.8739(7.1479) | Xent 1.9777(2.0181) | Loss 16.6793(17.4137) | Error 0.6699(0.6857) Steps 508(467.66) | Grad Norm 1.3035(3.1981) | Total Time 0.00(0.00)\n",
      "Iter 0132 | Time 38.6889(40.1543) | Bit/dim 6.8707(7.1395) | Xent 1.9742(2.0168) | Loss 16.6514(17.3909) | Error 0.6562(0.6848) Steps 502(468.69) | Grad Norm 2.3318(3.1721) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0022 | Time 19.8938, Epoch Time 276.1630(301.5659), Bit/dim 6.8706(best: 6.9046), Xent 1.9666, Loss 7.8539, Error 0.6535(best: 0.6604)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0133 | Time 38.7786(40.1131) | Bit/dim 6.8730(7.1316) | Xent 1.9796(2.0157) | Loss 19.0312(17.4401) | Error 0.6600(0.6841) Steps 460(468.43) | Grad Norm 1.3029(3.1160) | Total Time 0.00(0.00)\n",
      "Iter 0134 | Time 40.6953(40.1305) | Bit/dim 6.8668(7.1236) | Xent 1.9866(2.0148) | Loss 16.5560(17.4136) | Error 0.6690(0.6836) Steps 478(468.71) | Grad Norm 1.5956(3.0704) | Total Time 0.00(0.00)\n",
      "Iter 0135 | Time 37.7733(40.0598) | Bit/dim 6.8581(7.1156) | Xent 1.9726(2.0136) | Loss 16.4354(17.3842) | Error 0.6674(0.6831) Steps 454(468.27) | Grad Norm 1.1144(3.0117) | Total Time 0.00(0.00)\n",
      "Iter 0136 | Time 37.2067(39.9742) | Bit/dim 6.8410(7.1074) | Xent 1.9590(2.0119) | Loss 16.4207(17.3553) | Error 0.6573(0.6824) Steps 472(468.38) | Grad Norm 1.4686(2.9654) | Total Time 0.00(0.00)\n",
      "Iter 0137 | Time 39.9410(39.9732) | Bit/dim 6.8407(7.0994) | Xent 1.9821(2.0110) | Loss 16.2885(17.3233) | Error 0.6681(0.6819) Steps 472(468.49) | Grad Norm 1.3972(2.9184) | Total Time 0.00(0.00)\n",
      "Iter 0138 | Time 39.2243(39.9508) | Bit/dim 6.8300(7.0913) | Xent 1.9732(2.0099) | Loss 16.4180(17.2961) | Error 0.6615(0.6813) Steps 454(468.06) | Grad Norm 2.1191(2.8944) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0023 | Time 19.6171, Epoch Time 269.2221(300.5956), Bit/dim 6.8304(best: 6.8706), Xent 1.9608, Loss 7.8108, Error 0.6484(best: 0.6535)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0139 | Time 39.8223(39.9469) | Bit/dim 6.8276(7.0834) | Xent 1.9632(2.0085) | Loss 19.2019(17.3533) | Error 0.6644(0.6808) Steps 466(468.00) | Grad Norm 5.8513(2.9831) | Total Time 0.00(0.00)\n",
      "Iter 0140 | Time 42.5834(40.0260) | Bit/dim 6.8209(7.0755) | Xent 1.9757(2.0075) | Loss 16.6805(17.3331) | Error 0.6681(0.6804) Steps 478(468.30) | Grad Norm 12.5992(3.2716) | Total Time 0.00(0.00)\n",
      "Iter 0141 | Time 38.2520(39.9728) | Bit/dim 6.8228(7.0680) | Xent 1.9929(2.0071) | Loss 16.1154(17.2966) | Error 0.6929(0.6808) Steps 472(468.41) | Grad Norm 22.1779(3.8388) | Total Time 0.00(0.00)\n",
      "Iter 0142 | Time 37.3592(39.8944) | Bit/dim 6.8119(7.0603) | Xent 1.9907(2.0066) | Loss 15.8481(17.2531) | Error 0.6921(0.6811) Steps 448(467.80) | Grad Norm 20.6312(4.3426) | Total Time 0.00(0.00)\n",
      "Iter 0143 | Time 45.4322(40.0605) | Bit/dim 6.7929(7.0523) | Xent 1.9713(2.0055) | Loss 16.7119(17.2369) | Error 0.6618(0.6806) Steps 460(467.56) | Grad Norm 9.6728(4.5025) | Total Time 0.00(0.00)\n",
      "Iter 0144 | Time 38.6099(40.0170) | Bit/dim 6.7878(7.0443) | Xent 1.9768(2.0047) | Loss 16.2306(17.2067) | Error 0.6744(0.6804) Steps 484(468.06) | Grad Norm 9.0730(4.6396) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0024 | Time 19.9463, Epoch Time 277.7641(299.9107), Bit/dim 6.7893(best: 6.8304), Xent 1.9783, Loss 7.7784, Error 0.6862(best: 0.6484)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0145 | Time 40.1574(40.0212) | Bit/dim 6.7898(7.0367) | Xent 1.9873(2.0041) | Loss 18.6908(17.2512) | Error 0.6972(0.6809) Steps 484(468.53) | Grad Norm 23.5197(5.2060) | Total Time 0.00(0.00)\n",
      "Iter 0146 | Time 38.9556(39.9892) | Bit/dim 6.7730(7.0288) | Xent 1.9669(2.0030) | Loss 15.8606(17.2095) | Error 0.6596(0.6802) Steps 472(468.64) | Grad Norm 13.2524(5.4474) | Total Time 0.00(0.00)\n",
      "Iter 0147 | Time 42.9869(40.0792) | Bit/dim 6.7665(7.0209) | Xent 1.9664(2.0019) | Loss 16.1083(17.1765) | Error 0.6560(0.6795) Steps 478(468.92) | Grad Norm 6.0814(5.4664) | Total Time 0.00(0.00)\n",
      "Iter 0148 | Time 43.3152(40.1762) | Bit/dim 6.7541(7.0129) | Xent 1.9971(2.0018) | Loss 16.5128(17.1566) | Error 0.6910(0.6799) Steps 526(470.63) | Grad Norm 23.0021(5.9925) | Total Time 0.00(0.00)\n",
      "Iter 0149 | Time 39.2676(40.1490) | Bit/dim 6.7493(7.0050) | Xent 1.9635(2.0006) | Loss 16.2853(17.1304) | Error 0.6681(0.6795) Steps 478(470.85) | Grad Norm 20.5814(6.4301) | Total Time 0.00(0.00)\n",
      "Iter 0150 | Time 41.2818(40.1830) | Bit/dim 6.7479(6.9973) | Xent 1.9582(1.9994) | Loss 16.1224(17.1002) | Error 0.6606(0.6789) Steps 490(471.43) | Grad Norm 8.7930(6.5010) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0025 | Time 20.0764, Epoch Time 282.2779(299.3817), Bit/dim 6.7346(best: 6.7893), Xent 1.9465, Loss 7.7078, Error 0.6528(best: 0.6484)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0151 | Time 42.4829(40.2520) | Bit/dim 6.7256(6.9891) | Xent 1.9601(1.9982) | Loss 19.2840(17.1657) | Error 0.6635(0.6785) Steps 520(472.88) | Grad Norm 6.6412(6.5052) | Total Time 0.00(0.00)\n",
      "Iter 0152 | Time 41.2344(40.2814) | Bit/dim 6.7239(6.9812) | Xent 1.9794(1.9976) | Loss 15.8259(17.1255) | Error 0.6707(0.6782) Steps 508(473.94) | Grad Norm 15.0701(6.7622) | Total Time 0.00(0.00)\n",
      "Iter 0153 | Time 42.7186(40.3546) | Bit/dim 6.7232(6.9734) | Xent 1.9706(1.9968) | Loss 16.0285(17.0926) | Error 0.6785(0.6783) Steps 484(474.24) | Grad Norm 20.1288(7.1632) | Total Time 0.00(0.00)\n",
      "Iter 0154 | Time 39.6857(40.3345) | Bit/dim 6.7025(6.9653) | Xent 1.9604(1.9957) | Loss 15.7468(17.0522) | Error 0.6727(0.6781) Steps 496(474.89) | Grad Norm 20.3113(7.5576) | Total Time 0.00(0.00)\n",
      "Iter 0155 | Time 38.0736(40.2667) | Bit/dim 6.6921(6.9571) | Xent 1.9685(1.9949) | Loss 15.6180(17.0092) | Error 0.6743(0.6780) Steps 478(474.98) | Grad Norm 13.1864(7.7265) | Total Time 0.00(0.00)\n",
      "Iter 0156 | Time 44.8347(40.4037) | Bit/dim 6.6717(6.9485) | Xent 1.9354(1.9931) | Loss 16.0104(16.9792) | Error 0.6404(0.6768) Steps 520(476.34) | Grad Norm 3.8543(7.6103) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0026 | Time 19.8453, Epoch Time 284.6617(298.9401), Bit/dim 6.6683(best: 6.7346), Xent 1.9414, Loss 7.6390, Error 0.6541(best: 0.6484)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0157 | Time 41.7195(40.4432) | Bit/dim 6.6703(6.9402) | Xent 1.9445(1.9917) | Loss 18.5638(17.0268) | Error 0.6558(0.6762) Steps 502(477.11) | Grad Norm 7.4901(7.6067) | Total Time 0.00(0.00)\n",
      "Iter 0158 | Time 41.8596(40.4857) | Bit/dim 6.6600(6.9318) | Xent 1.9646(1.9908) | Loss 16.1169(16.9995) | Error 0.6659(0.6759) Steps 454(476.41) | Grad Norm 15.0070(7.8287) | Total Time 0.00(0.00)\n",
      "Iter 0159 | Time 42.0057(40.5313) | Bit/dim 6.6436(6.9231) | Xent 1.9626(1.9900) | Loss 16.0508(16.9710) | Error 0.6735(0.6758) Steps 490(476.82) | Grad Norm 26.7938(8.3977) | Total Time 0.00(0.00)\n",
      "Iter 0160 | Time 40.3015(40.5244) | Bit/dim 6.6255(6.9142) | Xent 1.9571(1.9890) | Loss 15.7046(16.9330) | Error 0.6774(0.6759) Steps 502(477.58) | Grad Norm 18.8094(8.7100) | Total Time 0.00(0.00)\n",
      "Iter 0161 | Time 45.6216(40.6773) | Bit/dim 6.6126(6.9052) | Xent 1.9598(1.9881) | Loss 16.3642(16.9160) | Error 0.6565(0.6753) Steps 490(477.95) | Grad Norm 16.7844(8.9523) | Total Time 0.00(0.00)\n",
      "Iter 0162 | Time 40.6684(40.6770) | Bit/dim 6.6010(6.8960) | Xent 1.9524(1.9871) | Loss 15.4694(16.8726) | Error 0.6720(0.6752) Steps 508(478.85) | Grad Norm 13.1145(9.0771) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0027 | Time 20.7992, Epoch Time 288.7748(298.6351), Bit/dim 6.5861(best: 6.6683), Xent 1.9333, Loss 7.5528, Error 0.6461(best: 0.6484)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0163 | Time 42.3796(40.7281) | Bit/dim 6.5851(6.8867) | Xent 1.9342(1.9855) | Loss 18.7147(16.9278) | Error 0.6567(0.6746) Steps 514(479.90) | Grad Norm 7.6660(9.0348) | Total Time 0.00(0.00)\n",
      "Iter 0164 | Time 42.3685(40.7773) | Bit/dim 6.5693(6.8772) | Xent 1.9509(1.9844) | Loss 15.9344(16.8980) | Error 0.6634(0.6743) Steps 508(480.75) | Grad Norm 23.7152(9.4752) | Total Time 0.00(0.00)\n",
      "Iter 0165 | Time 43.0020(40.8441) | Bit/dim 6.5595(6.8677) | Xent 1.9835(1.9844) | Loss 15.8750(16.8673) | Error 0.6966(0.6750) Steps 520(481.92) | Grad Norm 38.1992(10.3369) | Total Time 0.00(0.00)\n",
      "Iter 0166 | Time 39.3029(40.7978) | Bit/dim 6.5434(6.8579) | Xent 1.9459(1.9833) | Loss 15.6924(16.8321) | Error 0.6575(0.6744) Steps 478(481.81) | Grad Norm 14.2671(10.4548) | Total Time 0.00(0.00)\n",
      "Iter 0167 | Time 43.5996(40.8819) | Bit/dim 6.5561(6.8489) | Xent 2.0478(1.9852) | Loss 16.1176(16.8107) | Error 0.7266(0.6760) Steps 472(481.51) | Grad Norm 72.6043(12.3193) | Total Time 0.00(0.00)\n",
      "Iter 0168 | Time 41.1906(40.8911) | Bit/dim 6.5400(6.8396) | Xent 2.0392(1.9868) | Loss 15.6189(16.7749) | Error 0.7224(0.6774) Steps 496(481.95) | Grad Norm 64.9939(13.8996) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0028 | Time 20.9127, Epoch Time 288.8233(298.3408), Bit/dim 6.5012(best: 6.5861), Xent 1.9506, Loss 7.4764, Error 0.6753(best: 0.6461)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0169 | Time 40.8649(40.8904) | Bit/dim 6.4938(6.8292) | Xent 1.9736(1.9864) | Loss 18.3825(16.8231) | Error 0.6881(0.6777) Steps 514(482.91) | Grad Norm 36.3397(14.5728) | Total Time 0.00(0.00)\n",
      "Iter 0170 | Time 43.5411(40.9699) | Bit/dim 6.5032(6.8195) | Xent 1.9430(1.9851) | Loss 15.6510(16.7880) | Error 0.6787(0.6778) Steps 508(483.66) | Grad Norm 35.4372(15.1987) | Total Time 0.00(0.00)\n",
      "Iter 0171 | Time 41.2427(40.9781) | Bit/dim 6.4693(6.8089) | Xent 1.9653(1.9845) | Loss 15.6615(16.7542) | Error 0.6771(0.6777) Steps 496(484.03) | Grad Norm 39.0949(15.9156) | Total Time 0.00(0.00)\n",
      "Iter 0172 | Time 48.0500(41.1902) | Bit/dim 6.4879(6.7993) | Xent 1.9558(1.9837) | Loss 16.0106(16.7319) | Error 0.6720(0.6776) Steps 484(484.03) | Grad Norm 29.0438(16.3094) | Total Time 0.00(0.00)\n",
      "Iter 0173 | Time 43.2910(41.2532) | Bit/dim 6.4529(6.7889) | Xent 1.9981(1.9841) | Loss 15.3169(16.6894) | Error 0.7069(0.6784) Steps 484(484.03) | Grad Norm 47.4078(17.2424) | Total Time 0.00(0.00)\n",
      "Iter 0174 | Time 42.7201(41.2973) | Bit/dim 6.4588(6.7790) | Xent 1.9485(1.9830) | Loss 15.4705(16.6529) | Error 0.6701(0.6782) Steps 478(483.85) | Grad Norm 32.1474(17.6895) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0029 | Time 20.9454, Epoch Time 297.1336(298.3046), Bit/dim 6.4355(best: 6.5012), Xent 2.0117, Loss 7.4413, Error 0.7091(best: 0.6461)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0175 | Time 41.7200(41.3099) | Bit/dim 6.4302(6.7686) | Xent 2.0228(1.9842) | Loss 18.3698(16.7044) | Error 0.7109(0.6792) Steps 496(484.21) | Grad Norm 54.3669(18.7898) | Total Time 0.00(0.00)\n",
      "Iter 0176 | Time 42.8911(41.3574) | Bit/dim 6.4635(6.7594) | Xent 2.0011(1.9847) | Loss 15.7523(16.6758) | Error 0.7009(0.6798) Steps 502(484.75) | Grad Norm 56.1906(19.9119) | Total Time 0.00(0.00)\n",
      "Iter 0177 | Time 42.9491(41.4051) | Bit/dim 6.3914(6.7484) | Xent 1.9577(1.9839) | Loss 15.5912(16.6433) | Error 0.6939(0.6802) Steps 478(484.54) | Grad Norm 30.5614(20.2314) | Total Time 0.00(0.00)\n",
      "Iter 0178 | Time 41.3158(41.4024) | Bit/dim 6.4269(6.7387) | Xent 1.9459(1.9828) | Loss 15.2700(16.6021) | Error 0.6731(0.6800) Steps 472(484.17) | Grad Norm 37.7900(20.7581) | Total Time 0.00(0.00)\n",
      "Iter 0179 | Time 41.3326(41.4003) | Bit/dim 6.3907(6.7283) | Xent 1.9453(1.9816) | Loss 15.2051(16.5602) | Error 0.6576(0.6794) Steps 484(484.16) | Grad Norm 39.7190(21.3269) | Total Time 0.00(0.00)\n",
      "Iter 0180 | Time 40.1640(41.3633) | Bit/dim 6.3500(6.7169) | Xent 1.9432(1.9805) | Loss 15.1054(16.5165) | Error 0.6480(0.6784) Steps 502(484.70) | Grad Norm 13.1578(21.0819) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0030 | Time 20.9931, Epoch Time 287.2905(297.9741), Bit/dim 6.3457(best: 6.4355), Xent 1.9669, Loss 7.3292, Error 0.6931(best: 0.6461)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0181 | Time 42.2749(41.3906) | Bit/dim 6.3373(6.7055) | Xent 1.9569(1.9798) | Loss 18.1883(16.5667) | Error 0.6925(0.6788) Steps 496(485.04) | Grad Norm 42.7613(21.7322) | Total Time 0.00(0.00)\n",
      "Iter 0182 | Time 41.7299(41.4008) | Bit/dim 6.3070(6.6936) | Xent 1.9275(1.9782) | Loss 15.5089(16.5349) | Error 0.6598(0.6783) Steps 490(485.19) | Grad Norm 18.1336(21.6243) | Total Time 0.00(0.00)\n",
      "Iter 0183 | Time 38.9827(41.3282) | Bit/dim 6.3617(6.6836) | Xent 2.0464(1.9803) | Loss 15.4074(16.5011) | Error 0.7309(0.6798) Steps 478(484.97) | Grad Norm 83.8842(23.4921) | Total Time 0.00(0.00)\n",
      "Iter 0184 | Time 46.8249(41.4931) | Bit/dim 6.3552(6.6738) | Xent 1.9804(1.9803) | Loss 15.6306(16.4750) | Error 0.6947(0.6803) Steps 520(486.02) | Grad Norm 62.1918(24.6531) | Total Time 0.00(0.00)\n",
      "Iter 0185 | Time 45.5927(41.6161) | Bit/dim 6.2953(6.6624) | Xent 2.0827(1.9833) | Loss 15.5687(16.4478) | Error 0.7578(0.6826) Steps 532(487.40) | Grad Norm 98.1071(26.8567) | Total Time 0.00(0.00)\n",
      "Iter 0186 | Time 44.7091(41.7089) | Bit/dim 6.3271(6.6524) | Xent 2.0452(1.9852) | Loss 15.5418(16.4206) | Error 0.7490(0.6846) Steps 508(488.02) | Grad Norm 100.3080(29.0602) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0031 | Time 21.2916, Epoch Time 297.5295(297.9608), Bit/dim 6.2985(best: 6.3457), Xent 1.9679, Loss 7.2825, Error 0.7005(best: 0.6461)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0187 | Time 36.9712(41.5668) | Bit/dim 6.2943(6.6416) | Xent 1.9795(1.9850) | Loss 17.8763(16.4643) | Error 0.7083(0.6853) Steps 484(487.90) | Grad Norm 46.6977(29.5894) | Total Time 0.00(0.00)\n",
      "Iter 0188 | Time 43.7544(41.6324) | Bit/dim 6.2306(6.6293) | Xent 1.9621(1.9843) | Loss 15.2487(16.4278) | Error 0.6821(0.6852) Steps 508(488.50) | Grad Norm 58.1256(30.4454) | Total Time 0.00(0.00)\n",
      "Iter 0189 | Time 47.9917(41.8232) | Bit/dim 6.2795(6.6188) | Xent 1.9984(1.9848) | Loss 15.6509(16.4045) | Error 0.6897(0.6854) Steps 466(487.83) | Grad Norm 53.3883(31.1337) | Total Time 0.00(0.00)\n",
      "Iter 0190 | Time 41.3375(41.8086) | Bit/dim 6.2139(6.6067) | Xent 1.9471(1.9836) | Loss 14.7791(16.3557) | Error 0.6753(0.6851) Steps 508(488.43) | Grad Norm 39.7615(31.3926) | Total Time 0.00(0.00)\n",
      "Iter 0191 | Time 41.7305(41.8063) | Bit/dim 6.2022(6.5945) | Xent 1.9454(1.9825) | Loss 14.8472(16.3105) | Error 0.6740(0.6847) Steps 460(487.58) | Grad Norm 34.3563(31.4815) | Total Time 0.00(0.00)\n",
      "Iter 0192 | Time 45.1935(41.9079) | Bit/dim 6.2161(6.5832) | Xent 2.0163(1.9835) | Loss 15.4936(16.2860) | Error 0.6981(0.6851) Steps 520(488.55) | Grad Norm 65.6808(32.5075) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0032 | Time 20.7699, Epoch Time 293.9262(297.8398), Bit/dim 6.1674(best: 6.2985), Xent 1.9269, Loss 7.1309, Error 0.6491(best: 0.6461)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0193 | Time 40.8504(41.8762) | Bit/dim 6.1670(6.5707) | Xent 1.9476(1.9824) | Loss 17.8327(16.3324) | Error 0.6627(0.6845) Steps 496(488.77) | Grad Norm 32.5595(32.5090) | Total Time 0.00(0.00)\n",
      "Iter 0194 | Time 43.2735(41.9181) | Bit/dim 6.1982(6.5595) | Xent 2.3017(1.9920) | Loss 15.6660(16.3124) | Error 0.8015(0.6880) Steps 490(488.81) | Grad Norm 121.2752(35.1720) | Total Time 0.00(0.00)\n",
      "Iter 0195 | Time 47.4406(42.0838) | Bit/dim 6.2490(6.5502) | Xent 2.3629(2.0031) | Loss 15.7839(16.2965) | Error 0.8017(0.6914) Steps 502(489.21) | Grad Norm 129.3056(37.9960) | Total Time 0.00(0.00)\n",
      "Iter 0196 | Time 45.9357(42.1993) | Bit/dim 6.2075(6.5399) | Xent 1.9248(2.0008) | Loss 15.1996(16.2636) | Error 0.6546(0.6903) Steps 514(489.95) | Grad Norm 35.7086(37.9274) | Total Time 0.00(0.00)\n",
      "Iter 0197 | Time 43.7585(42.2461) | Bit/dim 6.1991(6.5297) | Xent 2.3192(2.0103) | Loss 15.3032(16.2348) | Error 0.7694(0.6927) Steps 538(491.39) | Grad Norm 133.4694(40.7936) | Total Time 0.00(0.00)\n",
      "Iter 0198 | Time 47.3322(42.3987) | Bit/dim 6.4050(6.5259) | Xent 2.4888(2.0247) | Loss 16.1134(16.2312) | Error 0.7610(0.6947) Steps 478(490.99) | Grad Norm 156.4600(44.2636) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0033 | Time 20.5036, Epoch Time 304.7337(298.0466), Bit/dim 6.4288(best: 6.1674), Xent 2.1972, Loss 7.5274, Error 0.7404(best: 0.6461)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0199 | Time 43.5947(42.4346) | Bit/dim 6.4251(6.5229) | Xent 2.2079(2.0302) | Loss 18.6689(16.3043) | Error 0.7381(0.6960) Steps 532(492.22) | Grad Norm 107.3190(46.1553) | Total Time 0.00(0.00)\n",
      "Iter 0200 | Time 40.7586(42.3843) | Bit/dim 6.2493(6.5147) | Xent 2.0177(2.0298) | Loss 15.2421(16.2724) | Error 0.7057(0.6963) Steps 484(491.97) | Grad Norm 89.3065(47.4498) | Total Time 0.00(0.00)\n",
      "Iter 0201 | Time 47.9087(42.5500) | Bit/dim 6.1228(6.5030) | Xent 2.3267(2.0387) | Loss 15.4319(16.2472) | Error 0.8104(0.6997) Steps 502(492.28) | Grad Norm 116.4591(49.5201) | Total Time 0.00(0.00)\n",
      "Iter 0202 | Time 43.4141(42.5759) | Bit/dim 6.1342(6.4919) | Xent 2.3587(2.0483) | Loss 15.7057(16.2310) | Error 0.8260(0.7035) Steps 514(492.93) | Grad Norm 132.9336(52.0225) | Total Time 0.00(0.00)\n",
      "Iter 0203 | Time 44.8253(42.6434) | Bit/dim 6.2143(6.4836) | Xent 1.9672(2.0459) | Loss 15.0009(16.1941) | Error 0.6847(0.7029) Steps 532(494.10) | Grad Norm 37.5570(51.5886) | Total Time 0.00(0.00)\n",
      "Iter 0204 | Time 41.6170(42.6126) | Bit/dim 6.1111(6.4724) | Xent 2.1393(2.0487) | Loss 15.2000(16.1643) | Error 0.7511(0.7044) Steps 490(493.98) | Grad Norm 81.2413(52.4781) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0034 | Time 20.6675, Epoch Time 298.4314(298.0581), Bit/dim 6.1360(best: 6.1674), Xent 2.1962, Loss 7.2341, Error 0.7327(best: 0.6461)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0205 | Time 47.9121(42.7716) | Bit/dim 6.1310(6.4621) | Xent 2.2114(2.0536) | Loss 18.0133(16.2197) | Error 0.7366(0.7054) Steps 550(495.66) | Grad Norm 78.7025(53.2649) | Total Time 0.00(0.00)\n",
      "Iter 0206 | Time 44.3999(42.8205) | Bit/dim 6.1388(6.4524) | Xent 2.0337(2.0530) | Loss 15.2417(16.1904) | Error 0.7349(0.7062) Steps 496(495.67) | Grad Norm 55.9302(53.3448) | Total Time 0.00(0.00)\n",
      "Iter 0207 | Time 47.9526(42.9744) | Bit/dim 6.1208(6.4425) | Xent 1.9578(2.0501) | Loss 15.0354(16.1557) | Error 0.6671(0.7051) Steps 526(496.58) | Grad Norm 41.5462(52.9909) | Total Time 0.00(0.00)\n",
      "Iter 0208 | Time 44.1474(43.0096) | Bit/dim 6.0632(6.4311) | Xent 2.1076(2.0518) | Loss 14.9907(16.1208) | Error 0.7746(0.7072) Steps 508(496.92) | Grad Norm 74.6077(53.6394) | Total Time 0.00(0.00)\n",
      "Iter 0209 | Time 42.3396(42.9895) | Bit/dim 6.0735(6.4204) | Xent 2.0203(2.0509) | Loss 14.7405(16.0794) | Error 0.7272(0.7078) Steps 490(496.71) | Grad Norm 34.4022(53.0623) | Total Time 0.00(0.00)\n",
      "Iter 0210 | Time 45.3468(43.0602) | Bit/dim 6.0421(6.4090) | Xent 1.9727(2.0485) | Loss 14.7859(16.0406) | Error 0.6690(0.7066) Steps 508(497.05) | Grad Norm 41.9295(52.7283) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0035 | Time 19.7873, Epoch Time 307.9443(298.3547), Bit/dim 6.1253(best: 6.1360), Xent 2.0146, Loss 7.1326, Error 0.7338(best: 0.6461)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0211 | Time 45.5682(43.1355) | Bit/dim 6.1212(6.4004) | Xent 2.0191(2.0477) | Loss 18.2538(16.1070) | Error 0.7325(0.7074) Steps 514(497.56) | Grad Norm 110.0844(54.4490) | Total Time 0.00(0.00)\n",
      "Iter 0212 | Time 44.0998(43.1644) | Bit/dim 6.1321(6.3924) | Xent 2.0091(2.0465) | Loss 15.0282(16.0746) | Error 0.7354(0.7082) Steps 520(498.23) | Grad Norm 84.2242(55.3422) | Total Time 0.00(0.00)\n",
      "Iter 0213 | Time 43.2896(43.1682) | Bit/dim 6.0365(6.3817) | Xent 2.1783(2.0505) | Loss 14.8852(16.0389) | Error 0.7840(0.7105) Steps 490(497.99) | Grad Norm 123.1859(57.3775) | Total Time 0.00(0.00)\n",
      "Iter 0214 | Time 48.0799(43.3155) | Bit/dim 6.0074(6.3705) | Xent 2.1635(2.0539) | Loss 15.0305(16.0087) | Error 0.7809(0.7126) Steps 508(498.29) | Grad Norm 129.3558(59.5369) | Total Time 0.00(0.00)\n",
      "Iter 0215 | Time 44.8372(43.3612) | Bit/dim 6.0008(6.3594) | Xent 1.9881(2.0519) | Loss 14.8780(15.9748) | Error 0.6947(0.7121) Steps 538(499.48) | Grad Norm 51.0573(59.2825) | Total Time 0.00(0.00)\n",
      "Iter 0216 | Time 43.8984(43.3773) | Bit/dim 5.9861(6.3482) | Xent 1.9760(2.0496) | Loss 14.7749(15.9388) | Error 0.6890(0.7114) Steps 508(499.73) | Grad Norm 40.8405(58.7292) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0036 | Time 20.1685, Epoch Time 305.5199(298.5697), Bit/dim 6.0116(best: 6.1253), Xent 2.1382, Loss 7.0807, Error 0.7842(best: 0.6461)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0217 | Time 44.9646(43.4249) | Bit/dim 6.0116(6.3381) | Xent 2.1467(2.0525) | Loss 17.6090(15.9889) | Error 0.7857(0.7136) Steps 520(500.34) | Grad Norm 153.0820(61.5598) | Total Time 0.00(0.00)\n",
      "Iter 0218 | Time 45.4123(43.4845) | Bit/dim 5.9922(6.3277) | Xent 2.1279(2.0548) | Loss 14.6485(15.9487) | Error 0.7748(0.7154) Steps 514(500.75) | Grad Norm 136.7749(63.8163) | Total Time 0.00(0.00)\n",
      "Iter 0219 | Time 44.6844(43.5205) | Bit/dim 5.9549(6.3165) | Xent 2.0346(2.0542) | Loss 14.9497(15.9187) | Error 0.7265(0.7158) Steps 508(500.97) | Grad Norm 63.6571(63.8115) | Total Time 0.00(0.00)\n",
      "Iter 0220 | Time 42.3180(43.4844) | Bit/dim 5.9440(6.3053) | Xent 2.0061(2.0527) | Loss 14.8264(15.8859) | Error 0.7079(0.7155) Steps 514(501.36) | Grad Norm 57.5159(63.6226) | Total Time 0.00(0.00)\n",
      "Iter 0221 | Time 43.2748(43.4781) | Bit/dim 5.9659(6.2952) | Xent 2.0978(2.0541) | Loss 15.0564(15.8610) | Error 0.7580(0.7168) Steps 496(501.20) | Grad Norm 127.8598(65.5497) | Total Time 0.00(0.00)\n",
      "Iter 0222 | Time 42.5858(43.4514) | Bit/dim 5.9507(6.2848) | Xent 2.1015(2.0555) | Loss 14.5086(15.8205) | Error 0.7530(0.7179) Steps 496(501.04) | Grad Norm 114.7268(67.0250) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0037 | Time 19.2652, Epoch Time 298.6384(298.5717), Bit/dim 5.9331(best: 6.0116), Xent 2.0058, Loss 6.9360, Error 0.7078(best: 0.6461)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0223 | Time 41.9461(43.4062) | Bit/dim 5.9336(6.2743) | Xent 2.0151(2.0543) | Loss 17.1873(15.8615) | Error 0.7101(0.7177) Steps 484(500.53) | Grad Norm 79.7031(67.4054) | Total Time 0.00(0.00)\n",
      "Iter 0224 | Time 44.2774(43.4324) | Bit/dim 5.9127(6.2634) | Xent 1.9888(2.0523) | Loss 14.4456(15.8190) | Error 0.7000(0.7171) Steps 520(501.12) | Grad Norm 73.5441(67.5895) | Total Time 0.00(0.00)\n",
      "Iter 0225 | Time 42.9252(43.4171) | Bit/dim 5.9047(6.2527) | Xent 2.0898(2.0535) | Loss 14.3774(15.7757) | Error 0.7443(0.7179) Steps 496(500.96) | Grad Norm 104.4197(68.6945) | Total Time 0.00(0.00)\n",
      "Iter 0226 | Time 45.9841(43.4941) | Bit/dim 5.9059(6.2423) | Xent 2.0596(2.0536) | Loss 14.7700(15.7456) | Error 0.7395(0.7186) Steps 490(500.63) | Grad Norm 98.1495(69.5781) | Total Time 0.00(0.00)\n",
      "Iter 0227 | Time 43.8250(43.5041) | Bit/dim 5.9083(6.2323) | Xent 2.0192(2.0526) | Loss 14.6378(15.7123) | Error 0.7110(0.7184) Steps 472(499.77) | Grad Norm 91.6427(70.2400) | Total Time 0.00(0.00)\n",
      "Iter 0228 | Time 46.6476(43.5984) | Bit/dim 5.9124(6.2227) | Xent 2.0191(2.0516) | Loss 14.6255(15.6797) | Error 0.7105(0.7181) Steps 526(500.56) | Grad Norm 86.4190(70.7254) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0038 | Time 19.8800, Epoch Time 301.4641(298.6585), Bit/dim 5.8942(best: 5.9331), Xent 2.0217, Loss 6.9051, Error 0.7057(best: 0.6461)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0229 | Time 43.2079(43.5867) | Bit/dim 5.8835(6.2125) | Xent 2.0430(2.0513) | Loss 17.3930(15.7311) | Error 0.7156(0.7181) Steps 508(500.78) | Grad Norm 88.3981(71.2556) | Total Time 0.00(0.00)\n",
      "Iter 0230 | Time 43.4690(43.5831) | Bit/dim 5.8735(6.2023) | Xent 2.0105(2.0501) | Loss 14.4665(15.6932) | Error 0.6879(0.7171) Steps 502(500.82) | Grad Norm 72.3566(71.2886) | Total Time 0.00(0.00)\n",
      "Iter 0231 | Time 44.4832(43.6101) | Bit/dim 5.8768(6.1925) | Xent 2.0487(2.0501) | Loss 14.7088(15.6637) | Error 0.7336(0.7176) Steps 526(501.58) | Grad Norm 94.3244(71.9797) | Total Time 0.00(0.00)\n",
      "Iter 0232 | Time 45.4244(43.6646) | Bit/dim 5.8734(6.1830) | Xent 2.0181(2.0491) | Loss 14.6064(15.6319) | Error 0.7203(0.7177) Steps 478(500.87) | Grad Norm 82.7486(72.3028) | Total Time 0.00(0.00)\n",
      "Iter 0233 | Time 46.1165(43.7381) | Bit/dim 5.8618(6.1733) | Xent 2.0051(2.0478) | Loss 14.5812(15.6004) | Error 0.6839(0.7167) Steps 502(500.90) | Grad Norm 62.8410(72.0189) | Total Time 0.00(0.00)\n",
      "Iter 0234 | Time 42.9721(43.7151) | Bit/dim 5.8568(6.1638) | Xent 1.9883(2.0460) | Loss 14.5464(15.5688) | Error 0.6718(0.7154) Steps 508(501.12) | Grad Norm 49.2686(71.3364) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0039 | Time 19.9627, Epoch Time 301.6364(298.7478), Bit/dim 5.8664(best: 5.8942), Xent 2.0295, Loss 6.8812, Error 0.7235(best: 0.6461)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0235 | Time 44.3367(43.7338) | Bit/dim 5.8683(6.1550) | Xent 2.0512(2.0462) | Loss 17.4389(15.6249) | Error 0.7294(0.7158) Steps 508(501.32) | Grad Norm 91.6797(71.9467) | Total Time 0.00(0.00)\n",
      "Iter 0236 | Time 45.7940(43.7956) | Bit/dim 5.8605(6.1461) | Xent 2.0235(2.0455) | Loss 14.7113(15.5975) | Error 0.7185(0.7159) Steps 508(501.52) | Grad Norm 79.8208(72.1829) | Total Time 0.00(0.00)\n",
      "Iter 0237 | Time 45.0801(43.8341) | Bit/dim 5.8397(6.1369) | Xent 1.9900(2.0438) | Loss 14.6554(15.5692) | Error 0.6695(0.7145) Steps 496(501.36) | Grad Norm 51.6559(71.5671) | Total Time 0.00(0.00)\n",
      "Iter 0238 | Time 44.7935(43.8629) | Bit/dim 5.8232(6.1275) | Xent 1.9830(2.0420) | Loss 14.7112(15.5435) | Error 0.6614(0.7129) Steps 532(502.28) | Grad Norm 46.3854(70.8117) | Total Time 0.00(0.00)\n",
      "Iter 0239 | Time 41.8468(43.8024) | Bit/dim 5.8329(6.1187) | Xent 2.0033(2.0408) | Loss 13.9693(15.4963) | Error 0.7054(0.7126) Steps 514(502.63) | Grad Norm 78.0597(71.0291) | Total Time 0.00(0.00)\n",
      "Iter 0240 | Time 44.3752(43.8196) | Bit/dim 5.8385(6.1103) | Xent 1.9966(2.0395) | Loss 14.0376(15.4525) | Error 0.6964(0.7122) Steps 496(502.43) | Grad Norm 68.4458(70.9516) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0040 | Time 20.3260, Epoch Time 302.4098(298.8577), Bit/dim 5.8229(best: 5.8664), Xent 1.9754, Loss 6.8106, Error 0.6681(best: 0.6461)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0241 | Time 42.5212(43.7807) | Bit/dim 5.8267(6.1018) | Xent 1.9925(2.0381) | Loss 16.8215(15.4936) | Error 0.6769(0.7111) Steps 490(502.06) | Grad Norm 49.2044(70.2992) | Total Time 0.00(0.00)\n",
      "Iter 0242 | Time 44.9417(43.8155) | Bit/dim 5.8140(6.0932) | Xent 1.9768(2.0363) | Loss 14.2173(15.4553) | Error 0.6665(0.7098) Steps 514(502.41) | Grad Norm 43.5969(69.4981) | Total Time 0.00(0.00)\n",
      "Iter 0243 | Time 46.5061(43.8962) | Bit/dim 5.8238(6.0851) | Xent 2.0044(2.0353) | Loss 14.3806(15.4230) | Error 0.7014(0.7095) Steps 508(502.58) | Grad Norm 70.1908(69.5189) | Total Time 0.00(0.00)\n",
      "Iter 0244 | Time 45.2886(43.9380) | Bit/dim 5.8138(6.0769) | Xent 1.9982(2.0342) | Loss 14.0698(15.3824) | Error 0.6947(0.7091) Steps 496(502.38) | Grad Norm 60.2499(69.2408) | Total Time 0.00(0.00)\n",
      "Iter 0245 | Time 45.6387(43.9890) | Bit/dim 5.7908(6.0683) | Xent 1.9791(2.0325) | Loss 14.4936(15.3558) | Error 0.6685(0.7079) Steps 460(501.11) | Grad Norm 46.2275(68.5504) | Total Time 0.00(0.00)\n",
      "Iter 0246 | Time 45.1906(44.0251) | Bit/dim 5.7734(6.0595) | Xent 1.9636(2.0305) | Loss 14.2075(15.3213) | Error 0.6584(0.7064) Steps 514(501.50) | Grad Norm 39.9949(67.6938) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0041 | Time 20.2614, Epoch Time 306.0968(299.0749), Bit/dim 5.8012(best: 5.8229), Xent 1.9719, Loss 6.7871, Error 0.6762(best: 0.6461)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0247 | Time 45.2362(44.0614) | Bit/dim 5.8077(6.0519) | Xent 1.9783(2.0289) | Loss 17.3603(15.3825) | Error 0.6855(0.7057) Steps 496(501.33) | Grad Norm 60.4649(67.4769) | Total Time 0.00(0.00)\n",
      "Iter 0248 | Time 45.8877(44.1162) | Bit/dim 5.8000(6.0444) | Xent 1.9824(2.0275) | Loss 14.4490(15.3545) | Error 0.6847(0.7051) Steps 508(501.53) | Grad Norm 53.4709(67.0567) | Total Time 0.00(0.00)\n",
      "Iter 0249 | Time 43.8219(44.1073) | Bit/dim 5.7725(6.0362) | Xent 1.9771(2.0260) | Loss 14.1915(15.3196) | Error 0.6735(0.7042) Steps 472(500.65) | Grad Norm 48.4247(66.4978) | Total Time 0.00(0.00)\n",
      "Iter 0250 | Time 44.0949(44.1070) | Bit/dim 5.7695(6.0282) | Xent 1.9713(2.0244) | Loss 14.1481(15.2845) | Error 0.6654(0.7030) Steps 520(501.23) | Grad Norm 44.6817(65.8433) | Total Time 0.00(0.00)\n",
      "Iter 0251 | Time 46.7566(44.1865) | Bit/dim 5.7700(6.0205) | Xent 1.9807(2.0230) | Loss 14.2092(15.2522) | Error 0.6885(0.7026) Steps 508(501.43) | Grad Norm 48.0599(65.3098) | Total Time 0.00(0.00)\n",
      "Iter 0252 | Time 43.8049(44.1750) | Bit/dim 5.7528(6.0125) | Xent 1.9722(2.0215) | Loss 14.0008(15.2147) | Error 0.6838(0.7020) Steps 520(501.99) | Grad Norm 42.7045(64.6316) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0042 | Time 20.5153, Epoch Time 306.0488(299.2841), Bit/dim 5.7572(best: 5.8012), Xent 1.9623, Loss 6.7384, Error 0.6559(best: 0.6461)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0253 | Time 43.1908(44.1455) | Bit/dim 5.7352(6.0041) | Xent 1.9692(2.0200) | Loss 16.6148(15.2567) | Error 0.6779(0.7013) Steps 508(502.17) | Grad Norm 44.5322(64.0286) | Total Time 0.00(0.00)\n",
      "Iter 0254 | Time 41.4466(44.0645) | Bit/dim 5.7501(5.9965) | Xent 1.9722(2.0185) | Loss 14.0806(15.2214) | Error 0.6696(0.7003) Steps 496(501.98) | Grad Norm 38.9620(63.2766) | Total Time 0.00(0.00)\n",
      "Iter 0255 | Time 44.0738(44.0648) | Bit/dim 5.7653(5.9896) | Xent 1.9604(2.0168) | Loss 14.1026(15.1878) | Error 0.6736(0.6995) Steps 472(501.08) | Grad Norm 47.2208(62.7950) | Total Time 0.00(0.00)\n",
      "Iter 0256 | Time 44.0622(44.0647) | Bit/dim 5.7352(5.9819) | Xent 1.9783(2.0156) | Loss 13.9747(15.1514) | Error 0.6804(0.6990) Steps 496(500.93) | Grad Norm 42.3125(62.1805) | Total Time 0.00(0.00)\n",
      "Iter 0257 | Time 41.4031(43.9849) | Bit/dim 5.7496(5.9750) | Xent 1.9747(2.0144) | Loss 13.8976(15.1138) | Error 0.6667(0.6980) Steps 496(500.78) | Grad Norm 43.0226(61.6057) | Total Time 0.00(0.00)\n",
      "Iter 0258 | Time 44.1002(43.9883) | Bit/dim 5.7292(5.9676) | Xent 1.9694(2.0130) | Loss 14.4174(15.0929) | Error 0.6593(0.6968) Steps 484(500.28) | Grad Norm 39.3021(60.9366) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0043 | Time 20.8058, Epoch Time 294.9248(299.1533), Bit/dim 5.7328(best: 5.7572), Xent 1.9473, Loss 6.7064, Error 0.6597(best: 0.6461)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0259 | Time 42.0265(43.9295) | Bit/dim 5.7246(5.9603) | Xent 1.9587(2.0114) | Loss 16.9589(15.1489) | Error 0.6759(0.6962) Steps 490(499.97) | Grad Norm 44.6509(60.4481) | Total Time 0.00(0.00)\n",
      "Iter 0260 | Time 42.9112(43.8989) | Bit/dim 5.7182(5.9530) | Xent 1.9957(2.0109) | Loss 14.2410(15.1217) | Error 0.6899(0.6960) Steps 490(499.67) | Grad Norm 43.3431(59.9349) | Total Time 0.00(0.00)\n",
      "Iter 0261 | Time 45.0646(43.9339) | Bit/dim 5.7101(5.9458) | Xent 1.9564(2.0093) | Loss 14.1900(15.0937) | Error 0.6546(0.6948) Steps 484(499.20) | Grad Norm 29.0065(59.0071) | Total Time 0.00(0.00)\n",
      "Iter 0262 | Time 45.8168(43.9904) | Bit/dim 5.7088(5.9387) | Xent 1.9696(2.0081) | Loss 14.3769(15.0722) | Error 0.6676(0.6939) Steps 526(500.01) | Grad Norm 24.1963(57.9627) | Total Time 0.00(0.00)\n",
      "Iter 0263 | Time 44.8987(44.0176) | Bit/dim 5.7188(5.9321) | Xent 2.0741(2.0101) | Loss 14.4444(15.0534) | Error 0.7632(0.6960) Steps 490(499.71) | Grad Norm 77.6352(58.5529) | Total Time 0.00(0.00)\n",
      "Iter 0264 | Time 43.8988(44.0141) | Bit/dim 5.7182(5.9256) | Xent 2.0353(2.0109) | Loss 14.0864(15.0244) | Error 0.7298(0.6970) Steps 508(499.96) | Grad Norm 66.0904(58.7790) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0044 | Time 20.2117, Epoch Time 300.3661(299.1897), Bit/dim 5.7124(best: 5.7328), Xent 1.9725, Loss 6.6986, Error 0.6809(best: 0.6461)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0265 | Time 39.8719(43.8898) | Bit/dim 5.7302(5.9198) | Xent 1.9838(2.0100) | Loss 16.7414(15.0759) | Error 0.6912(0.6969) Steps 490(499.66) | Grad Norm 54.4663(58.6497) | Total Time 0.00(0.00)\n",
      "Iter 0266 | Time 44.7155(43.9146) | Bit/dim 5.7044(5.9133) | Xent 1.9901(2.0094) | Loss 14.5994(15.0616) | Error 0.6980(0.6969) Steps 514(500.09) | Grad Norm 60.5768(58.7075) | Total Time 0.00(0.00)\n",
      "Iter 0267 | Time 45.5976(43.9651) | Bit/dim 5.7017(5.9070) | Xent 1.9779(2.0085) | Loss 14.2544(15.0374) | Error 0.6779(0.6963) Steps 520(500.68) | Grad Norm 59.3061(58.7254) | Total Time 0.00(0.00)\n",
      "Iter 0268 | Time 44.4174(43.9786) | Bit/dim 5.6812(5.9002) | Xent 1.9595(2.0070) | Loss 14.1542(15.0109) | Error 0.6720(0.6956) Steps 496(500.54) | Grad Norm 30.0823(57.8661) | Total Time 0.00(0.00)\n",
      "Iter 0269 | Time 43.7967(43.9732) | Bit/dim 5.6955(5.8941) | Xent 1.9588(2.0056) | Loss 14.2897(14.9892) | Error 0.6680(0.6948) Steps 484(500.05) | Grad Norm 28.3470(56.9806) | Total Time 0.00(0.00)\n",
      "Iter 0270 | Time 41.2293(43.8909) | Bit/dim 5.7061(5.8884) | Xent 1.9821(2.0049) | Loss 14.1845(14.9651) | Error 0.6835(0.6944) Steps 514(500.47) | Grad Norm 63.8674(57.1872) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0045 | Time 19.9884, Epoch Time 295.6773(299.0843), Bit/dim 5.6908(best: 5.7124), Xent 1.9581, Loss 6.6699, Error 0.6621(best: 0.6461)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0271 | Time 42.2852(43.8427) | Bit/dim 5.6884(5.8824) | Xent 1.9689(2.0038) | Loss 17.0767(15.0284) | Error 0.6650(0.6936) Steps 484(499.97) | Grad Norm 49.7790(56.9649) | Total Time 0.00(0.00)\n",
      "Iter 0272 | Time 40.4534(43.7410) | Bit/dim 5.6979(5.8769) | Xent 1.9800(2.0031) | Loss 14.3038(15.0067) | Error 0.7000(0.6937) Steps 496(499.85) | Grad Norm 61.3216(57.0956) | Total Time 0.00(0.00)\n",
      "Iter 0273 | Time 41.9258(43.6866) | Bit/dim 5.6761(5.8709) | Xent 1.9802(2.0024) | Loss 13.8896(14.9732) | Error 0.6910(0.6937) Steps 502(499.92) | Grad Norm 40.3270(56.5926) | Total Time 0.00(0.00)\n",
      "Iter 0274 | Time 40.4752(43.5902) | Bit/dim 5.6627(5.8646) | Xent 1.9754(2.0016) | Loss 14.0505(14.9455) | Error 0.6884(0.6935) Steps 478(499.26) | Grad Norm 36.4930(55.9896) | Total Time 0.00(0.00)\n",
      "Iter 0275 | Time 43.2311(43.5794) | Bit/dim 5.6879(5.8593) | Xent 2.0406(2.0028) | Loss 14.3176(14.9267) | Error 0.7155(0.6942) Steps 490(498.98) | Grad Norm 59.9923(56.1097) | Total Time 0.00(0.00)\n",
      "Iter 0276 | Time 41.5912(43.5198) | Bit/dim 5.6638(5.8534) | Xent 1.9863(2.0023) | Loss 14.2135(14.9053) | Error 0.6936(0.6941) Steps 478(498.35) | Grad Norm 39.3813(55.6078) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0046 | Time 20.6958, Epoch Time 286.3166(298.7013), Bit/dim 5.6852(best: 5.6908), Xent 2.0414, Loss 6.7059, Error 0.7386(best: 0.6461)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0277 | Time 44.0788(43.5366) | Bit/dim 5.6718(5.8480) | Xent 2.0535(2.0038) | Loss 17.0389(14.9693) | Error 0.7450(0.6957) Steps 508(498.64) | Grad Norm 69.2180(56.0161) | Total Time 0.00(0.00)\n",
      "Iter 0278 | Time 40.5443(43.4468) | Bit/dim 5.6774(5.8429) | Xent 2.0460(2.0051) | Loss 13.9871(14.9398) | Error 0.7424(0.6971) Steps 466(497.66) | Grad Norm 60.9806(56.1651) | Total Time 0.00(0.00)\n",
      "Iter 0279 | Time 43.2112(43.4397) | Bit/dim 5.7215(5.8392) | Xent 2.0046(2.0051) | Loss 14.3419(14.9219) | Error 0.7336(0.6982) Steps 496(497.61) | Grad Norm 81.9059(56.9373) | Total Time 0.00(0.00)\n",
      "Iter 0280 | Time 40.5585(43.3533) | Bit/dim 5.6764(5.8343) | Xent 2.0036(2.0050) | Loss 14.0713(14.8964) | Error 0.7079(0.6985) Steps 496(497.56) | Grad Norm 51.8706(56.7853) | Total Time 0.00(0.00)\n",
      "Iter 0281 | Time 40.4620(43.2666) | Bit/dim 5.7087(5.8306) | Xent 2.0167(2.0054) | Loss 14.2025(14.8756) | Error 0.7089(0.6988) Steps 466(496.62) | Grad Norm 86.2743(57.6699) | Total Time 0.00(0.00)\n",
      "Iter 0282 | Time 42.2845(43.2371) | Bit/dim 5.7517(5.8282) | Xent 2.0395(2.0064) | Loss 14.3703(14.8604) | Error 0.7074(0.6990) Steps 490(496.42) | Grad Norm 97.0745(58.8521) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0047 | Time 20.4869, Epoch Time 287.3974(298.3622), Bit/dim 5.6527(best: 5.6852), Xent 1.9595, Loss 6.6325, Error 0.6661(best: 0.6461)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0283 | Time 40.9960(43.1699) | Bit/dim 5.6417(5.8226) | Xent 1.9733(2.0054) | Loss 16.6075(14.9128) | Error 0.6746(0.6983) Steps 502(496.59) | Grad Norm 29.7997(57.9805) | Total Time 0.00(0.00)\n",
      "Iter 0284 | Time 40.7694(43.0978) | Bit/dim 5.8154(5.8224) | Xent 2.0703(2.0073) | Loss 14.3337(14.8954) | Error 0.7684(0.7004) Steps 490(496.39) | Grad Norm 119.5229(59.8268) | Total Time 0.00(0.00)\n",
      "Iter 0285 | Time 41.6820(43.0554) | Bit/dim 5.9464(5.8261) | Xent 2.1520(2.0117) | Loss 14.8758(14.8948) | Error 0.7905(0.7031) Steps 478(495.84) | Grad Norm 161.0950(62.8648) | Total Time 0.00(0.00)\n",
      "Iter 0286 | Time 39.9855(42.9633) | Bit/dim 5.8051(5.8255) | Xent 2.0232(2.0120) | Loss 14.2766(14.8763) | Error 0.7379(0.7041) Steps 484(495.48) | Grad Norm 98.3127(63.9283) | Total Time 0.00(0.00)\n",
      "Iter 0287 | Time 40.4335(42.8874) | Bit/dim 5.6405(5.8199) | Xent 1.9728(2.0108) | Loss 14.0566(14.8517) | Error 0.6663(0.7030) Steps 484(495.14) | Grad Norm 7.3213(62.2301) | Total Time 0.00(0.00)\n",
      "Iter 0288 | Time 41.7525(42.8533) | Bit/dim 5.6981(5.8163) | Xent 1.9927(2.0103) | Loss 13.8523(14.8217) | Error 0.6769(0.7022) Steps 508(495.52) | Grad Norm 58.3682(62.1142) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0048 | Time 20.3985, Epoch Time 281.9093(297.8686), Bit/dim 5.6921(best: 5.6527), Xent 1.9685, Loss 6.6764, Error 0.6549(best: 0.6461)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0289 | Time 41.6793(42.8181) | Bit/dim 5.6923(5.8126) | Xent 1.9869(2.0096) | Loss 16.9895(14.8868) | Error 0.6815(0.7016) Steps 520(496.26) | Grad Norm 50.7008(61.7718) | Total Time 0.00(0.00)\n",
      "Iter 0290 | Time 42.4365(42.8067) | Bit/dim 5.6245(5.8069) | Xent 1.9679(2.0084) | Loss 14.1915(14.8659) | Error 0.6775(0.7009) Steps 460(495.17) | Grad Norm 12.3407(60.2889) | Total Time 0.00(0.00)\n",
      "Iter 0291 | Time 40.4454(42.7358) | Bit/dim 5.6763(5.8030) | Xent 1.9564(2.0068) | Loss 14.0819(14.8424) | Error 0.6674(0.6999) Steps 502(495.38) | Grad Norm 30.7009(59.4012) | Total Time 0.00(0.00)\n",
      "Iter 0292 | Time 42.0370(42.7149) | Bit/dim 5.6370(5.7980) | Xent 1.9623(2.0055) | Loss 13.9795(14.8165) | Error 0.6701(0.6990) Steps 508(495.75) | Grad Norm 11.6807(57.9696) | Total Time 0.00(0.00)\n",
      "Iter 0293 | Time 40.7638(42.6563) | Bit/dim 5.6393(5.7933) | Xent 1.9692(2.0044) | Loss 13.9094(14.7893) | Error 0.6754(0.6983) Steps 460(494.68) | Grad Norm 20.3916(56.8423) | Total Time 0.00(0.00)\n",
      "Iter 0294 | Time 43.3136(42.6760) | Bit/dim 5.6175(5.7880) | Xent 1.9513(2.0028) | Loss 13.8293(14.7605) | Error 0.6587(0.6971) Steps 496(494.72) | Grad Norm 13.3479(55.5374) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0049 | Time 20.2026, Epoch Time 286.8860(297.5391), Bit/dim 5.6381(best: 5.6527), Xent 1.9648, Loss 6.6205, Error 0.6722(best: 0.6461)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0295 | Time 42.8902(42.6825) | Bit/dim 5.6395(5.7835) | Xent 1.9794(2.0021) | Loss 16.6564(14.8174) | Error 0.6896(0.6969) Steps 478(494.22) | Grad Norm 28.9203(54.7389) | Total Time 0.00(0.00)\n",
      "Iter 0296 | Time 43.1118(42.6954) | Bit/dim 5.6141(5.7785) | Xent 1.9436(2.0003) | Loss 14.0750(14.7951) | Error 0.6620(0.6958) Steps 472(493.55) | Grad Norm 6.7651(53.2997) | Total Time 0.00(0.00)\n",
      "Iter 0297 | Time 46.8781(42.8208) | Bit/dim 5.6302(5.7740) | Xent 1.9823(1.9998) | Loss 14.2117(14.7776) | Error 0.6954(0.6958) Steps 496(493.63) | Grad Norm 29.2196(52.5773) | Total Time 0.00(0.00)\n",
      "Iter 0298 | Time 41.9621(42.7951) | Bit/dim 5.6340(5.7698) | Xent 1.9456(1.9982) | Loss 13.7613(14.7471) | Error 0.6587(0.6947) Steps 490(493.52) | Grad Norm 25.6662(51.7700) | Total Time 0.00(0.00)\n",
      "Iter 0299 | Time 39.5674(42.6982) | Bit/dim 5.6175(5.7652) | Xent 1.9319(1.9962) | Loss 13.7061(14.7159) | Error 0.6578(0.6936) Steps 496(493.59) | Grad Norm 17.8181(50.7514) | Total Time 0.00(0.00)\n",
      "Iter 0300 | Time 41.8157(42.6718) | Bit/dim 5.6187(5.7608) | Xent 1.9699(1.9954) | Loss 14.1169(14.6979) | Error 0.6824(0.6933) Steps 478(493.12) | Grad Norm 30.6178(50.1474) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0050 | Time 19.8684, Epoch Time 291.7591(297.3657), Bit/dim 5.6235(best: 5.6381), Xent 1.9398, Loss 6.5934, Error 0.6461(best: 0.6461)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0301 | Time 45.2812(42.7500) | Bit/dim 5.6128(5.7564) | Xent 1.9698(1.9946) | Loss 15.7989(14.7309) | Error 0.6723(0.6926) Steps 502(493.39) | Grad Norm 42.8574(49.9287) | Total Time 0.00(0.00)\n",
      "Iter 0302 | Time 42.7072(42.7488) | Bit/dim 5.5886(5.7514) | Xent 1.9384(1.9929) | Loss 13.8905(14.7057) | Error 0.6530(0.6914) Steps 502(493.65) | Grad Norm 19.7626(49.0237) | Total Time 0.00(0.00)\n",
      "Iter 0303 | Time 38.6570(42.6260) | Bit/dim 5.6819(5.7493) | Xent 2.0540(1.9948) | Loss 14.1513(14.6891) | Error 0.7379(0.6928) Steps 466(492.82) | Grad Norm 112.8483(50.9385) | Total Time 0.00(0.00)\n",
      "Iter 0304 | Time 51.5293(42.8931) | Bit/dim 5.7000(5.7478) | Xent 2.0518(1.9965) | Loss 14.2431(14.6757) | Error 0.7275(0.6939) Steps 526(493.81) | Grad Norm 117.2315(52.9273) | Total Time 0.00(0.00)\n",
      "Iter 0305 | Time 55.1296(43.2602) | Bit/dim 5.6554(5.7450) | Xent 1.9317(1.9945) | Loss 14.3226(14.6651) | Error 0.6613(0.6929) Steps 508(494.24) | Grad Norm 34.5357(52.3755) | Total Time 0.00(0.00)\n",
      "Iter 0306 | Time 49.8926(43.4592) | Bit/dim 5.6508(5.7422) | Xent 2.0233(1.9954) | Loss 13.9724(14.6443) | Error 0.7238(0.6938) Steps 496(494.29) | Grad Norm 86.9574(53.4130) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0051 | Time 20.1997, Epoch Time 319.2917(298.0235), Bit/dim 5.7514(best: 5.6235), Xent 1.9969, Loss 6.7498, Error 0.6934(best: 0.6461)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0307 | Time 46.1927(43.5412) | Bit/dim 5.7476(5.7424) | Xent 2.0311(1.9965) | Loss 16.5745(14.7022) | Error 0.7134(0.6944) Steps 496(494.34) | Grad Norm 90.8915(54.5373) | Total Time 0.00(0.00)\n",
      "Iter 0308 | Time 45.2475(43.5924) | Bit/dim 5.6995(5.7411) | Xent 2.0307(1.9975) | Loss 14.2963(14.6901) | Error 0.7290(0.6954) Steps 466(493.49) | Grad Norm 52.1064(54.4644) | Total Time 0.00(0.00)\n",
      "Iter 0309 | Time 46.4569(43.6783) | Bit/dim 5.6231(5.7375) | Xent 1.9709(1.9967) | Loss 13.8159(14.6638) | Error 0.6850(0.6951) Steps 508(493.93) | Grad Norm 43.2382(54.1276) | Total Time 0.00(0.00)\n",
      "Iter 0310 | Time 50.3815(43.8794) | Bit/dim 5.6625(5.7353) | Xent 1.9739(1.9960) | Loss 14.1413(14.6482) | Error 0.6901(0.6950) Steps 478(493.45) | Grad Norm 45.5499(53.8703) | Total Time 0.00(0.00)\n",
      "Iter 0311 | Time 45.2325(43.9200) | Bit/dim 5.6627(5.7331) | Xent 2.0003(1.9961) | Loss 14.0705(14.6308) | Error 0.7100(0.6954) Steps 484(493.17) | Grad Norm 41.2514(53.4917) | Total Time 0.00(0.00)\n",
      "Iter 0312 | Time 48.7626(44.0653) | Bit/dim 5.6198(5.7297) | Xent 1.9383(1.9944) | Loss 13.9019(14.6090) | Error 0.6664(0.6946) Steps 478(492.71) | Grad Norm 28.0233(52.7277) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0052 | Time 20.0284, Epoch Time 318.3505(298.6333), Bit/dim 5.6079(best: 5.6235), Xent 1.9352, Loss 6.5755, Error 0.6510(best: 0.6461)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0313 | Time 48.4888(44.1980) | Bit/dim 5.6155(5.7263) | Xent 1.9440(1.9929) | Loss 16.4875(14.6653) | Error 0.6670(0.6937) Steps 502(492.99) | Grad Norm 24.7969(51.8897) | Total Time 0.00(0.00)\n",
      "Iter 0314 | Time 46.9933(44.2818) | Bit/dim 5.6243(5.7232) | Xent 1.9325(1.9911) | Loss 13.6549(14.6350) | Error 0.6479(0.6924) Steps 484(492.72) | Grad Norm 12.5856(50.7106) | Total Time 0.00(0.00)\n",
      "Iter 0315 | Time 46.1954(44.3392) | Bit/dim 5.5823(5.7190) | Xent 1.9391(1.9895) | Loss 14.0136(14.6164) | Error 0.6675(0.6916) Steps 496(492.82) | Grad Norm 21.5411(49.8355) | Total Time 0.00(0.00)\n",
      "Iter 0316 | Time 47.1435(44.4234) | Bit/dim 5.6196(5.7160) | Xent 1.9926(1.9896) | Loss 14.1976(14.6038) | Error 0.6891(0.6915) Steps 496(492.92) | Grad Norm 46.4289(49.7333) | Total Time 0.00(0.00)\n",
      "Iter 0317 | Time 48.6015(44.5487) | Bit/dim 5.5975(5.7125) | Xent 1.9462(1.9883) | Loss 14.1436(14.5900) | Error 0.6606(0.6906) Steps 466(492.11) | Grad Norm 21.2070(48.8775) | Total Time 0.00(0.00)\n",
      "Iter 0318 | Time 43.3219(44.5119) | Bit/dim 5.5961(5.7090) | Xent 2.0828(1.9911) | Loss 13.7167(14.5638) | Error 0.7531(0.6925) Steps 454(490.96) | Grad Norm 97.3408(50.3314) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0053 | Time 20.5587, Epoch Time 317.0574(299.1860), Bit/dim 5.6909(best: 5.6079), Xent 2.0913, Loss 6.7366, Error 0.7628(best: 0.6461)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0319 | Time 46.8976(44.5835) | Bit/dim 5.7042(5.7088) | Xent 2.1070(1.9946) | Loss 16.9703(14.6360) | Error 0.7632(0.6946) Steps 526(492.02) | Grad Norm 93.6970(51.6324) | Total Time 0.00(0.00)\n",
      "Iter 0320 | Time 48.7059(44.7072) | Bit/dim 5.7476(5.7100) | Xent 1.9387(1.9929) | Loss 14.2265(14.6237) | Error 0.6607(0.6936) Steps 502(492.32) | Grad Norm 21.3273(50.7232) | Total Time 0.00(0.00)\n",
      "Iter 0321 | Time 47.5241(44.7917) | Bit/dim 5.5900(5.7064) | Xent 1.9929(1.9929) | Loss 14.0807(14.6074) | Error 0.6969(0.6937) Steps 496(492.43) | Grad Norm 65.4435(51.1649) | Total Time 0.00(0.00)\n",
      "Iter 0322 | Time 47.9324(44.8859) | Bit/dim 5.6364(5.7043) | Xent 1.9309(1.9911) | Loss 14.0389(14.5904) | Error 0.6613(0.6927) Steps 460(491.45) | Grad Norm 39.7162(50.8214) | Total Time 0.00(0.00)\n",
      "Iter 0323 | Time 50.5126(45.0547) | Bit/dim 5.7205(5.7048) | Xent 2.0079(1.9916) | Loss 14.0246(14.5734) | Error 0.7051(0.6931) Steps 478(491.05) | Grad Norm 81.6284(51.7456) | Total Time 0.00(0.00)\n",
      "Iter 0324 | Time 44.7088(45.0443) | Bit/dim 5.7547(5.7063) | Xent 1.9548(1.9905) | Loss 14.1472(14.5606) | Error 0.6900(0.6930) Steps 478(490.66) | Grad Norm 61.2175(52.0298) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0054 | Time 19.6816, Epoch Time 322.2917(299.8792), Bit/dim 5.6708(best: 5.6079), Xent 1.9672, Loss 6.6544, Error 0.6893(best: 0.6461)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0325 | Time 48.3602(45.1438) | Bit/dim 5.6639(5.7050) | Xent 1.9771(1.9901) | Loss 17.1061(14.6370) | Error 0.6976(0.6931) Steps 466(489.92) | Grad Norm 44.5973(51.8068) | Total Time 0.00(0.00)\n",
      "Iter 0326 | Time 45.2116(45.1458) | Bit/dim 5.6007(5.7019) | Xent 1.9559(1.9891) | Loss 13.3781(14.5992) | Error 0.6686(0.6924) Steps 490(489.92) | Grad Norm 36.1962(51.3385) | Total Time 0.00(0.00)\n",
      "Iter 0327 | Time 45.3186(45.1510) | Bit/dim 5.5691(5.6979) | Xent 2.0076(1.9896) | Loss 13.7611(14.5741) | Error 0.7211(0.6933) Steps 478(489.56) | Grad Norm 52.9776(51.3876) | Total Time 0.00(0.00)\n",
      "Iter 0328 | Time 47.2313(45.2134) | Bit/dim 5.5626(5.6938) | Xent 1.9731(1.9891) | Loss 13.7923(14.5506) | Error 0.6972(0.6934) Steps 496(489.76) | Grad Norm 39.2189(51.0226) | Total Time 0.00(0.00)\n",
      "Iter 0329 | Time 51.9411(45.4152) | Bit/dim 5.5925(5.6908) | Xent 1.9489(1.9879) | Loss 13.8935(14.5309) | Error 0.6623(0.6924) Steps 508(490.30) | Grad Norm 30.3821(50.4034) | Total Time 0.00(0.00)\n",
      "Iter 0330 | Time 44.5745(45.3900) | Bit/dim 5.5724(5.6872) | Xent 1.9647(1.9872) | Loss 13.6198(14.5036) | Error 0.6820(0.6921) Steps 484(490.11) | Grad Norm 26.5056(49.6864) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0055 | Time 20.6529, Epoch Time 319.1772(300.4581), Bit/dim 5.5918(best: 5.6079), Xent 1.9534, Loss 6.5685, Error 0.6775(best: 0.6461)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0331 | Time 47.0921(45.4411) | Bit/dim 5.5966(5.6845) | Xent 1.9690(1.9867) | Loss 16.9622(14.5773) | Error 0.6864(0.6920) Steps 490(490.11) | Grad Norm 45.1607(49.5507) | Total Time 0.00(0.00)\n",
      "Iter 0332 | Time 53.2949(45.6767) | Bit/dim 5.5936(5.6818) | Xent 1.9521(1.9856) | Loss 14.0796(14.5624) | Error 0.6761(0.6915) Steps 466(489.39) | Grad Norm 39.2766(49.2424) | Total Time 0.00(0.00)\n",
      "Iter 0333 | Time 47.5090(45.7317) | Bit/dim 5.5482(5.6778) | Xent 1.9596(1.9848) | Loss 13.7138(14.5369) | Error 0.6759(0.6910) Steps 532(490.67) | Grad Norm 21.3449(48.4055) | Total Time 0.00(0.00)\n",
      "Iter 0334 | Time 48.0164(45.8002) | Bit/dim 5.5500(5.6740) | Xent 1.9437(1.9836) | Loss 13.6082(14.5091) | Error 0.6564(0.6900) Steps 508(491.19) | Grad Norm 17.8966(47.4902) | Total Time 0.00(0.00)\n",
      "Iter 0335 | Time 49.7447(45.9185) | Bit/dim 5.5424(5.6700) | Xent 1.9815(1.9836) | Loss 13.9588(14.4926) | Error 0.6941(0.6901) Steps 472(490.61) | Grad Norm 40.2443(47.2729) | Total Time 0.00(0.00)\n",
      "Iter 0336 | Time 47.1549(45.9556) | Bit/dim 5.5332(5.6659) | Xent 1.9403(1.9823) | Loss 13.7194(14.4694) | Error 0.6643(0.6893) Steps 490(490.59) | Grad Norm 26.9806(46.6641) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0056 | Time 21.3557, Epoch Time 330.1833(301.3499), Bit/dim 5.5342(best: 5.5918), Xent 1.9457, Loss 6.5071, Error 0.6636(best: 0.6461)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0337 | Time 46.1922(45.9627) | Bit/dim 5.5329(5.6619) | Xent 1.9772(1.9821) | Loss 15.7291(14.5072) | Error 0.6811(0.6891) Steps 514(491.29) | Grad Norm 27.8563(46.0999) | Total Time 0.00(0.00)\n",
      "Iter 0338 | Time 50.6305(46.1028) | Bit/dim 5.5350(5.6581) | Xent 1.9664(1.9816) | Loss 13.8794(14.4883) | Error 0.6734(0.6886) Steps 484(491.08) | Grad Norm 25.7485(45.4893) | Total Time 0.00(0.00)\n",
      "Iter 0339 | Time 49.2284(46.1965) | Bit/dim 5.5272(5.6542) | Xent 1.9349(1.9802) | Loss 13.6686(14.4637) | Error 0.6630(0.6878) Steps 502(491.40) | Grad Norm 14.4207(44.5573) | Total Time 0.00(0.00)\n",
      "Iter 0340 | Time 50.9143(46.3381) | Bit/dim 5.4988(5.6495) | Xent 1.9435(1.9791) | Loss 14.0396(14.4510) | Error 0.6736(0.6874) Steps 496(491.54) | Grad Norm 21.5405(43.8668) | Total Time 0.00(0.00)\n",
      "Iter 0341 | Time 50.2074(46.4541) | Bit/dim 5.5069(5.6452) | Xent 1.9297(1.9776) | Loss 14.0302(14.4384) | Error 0.6600(0.6866) Steps 496(491.67) | Grad Norm 13.1418(42.9450) | Total Time 0.00(0.00)\n",
      "Iter 0342 | Time 48.7702(46.5236) | Bit/dim 5.5112(5.6412) | Xent 1.9346(1.9764) | Loss 13.0642(14.3972) | Error 0.6587(0.6858) Steps 472(491.08) | Grad Norm 19.7488(42.2491) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0057 | Time 21.1126, Epoch Time 332.7718(302.2925), Bit/dim 5.5014(best: 5.5342), Xent 1.9213, Loss 6.4620, Error 0.6459(best: 0.6461)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0343 | Time 46.3725(46.5191) | Bit/dim 5.5121(5.6373) | Xent 1.9319(1.9750) | Loss 16.9840(14.4748) | Error 0.6661(0.6852) Steps 526(492.13) | Grad Norm 24.9988(41.7316) | Total Time 0.00(0.00)\n",
      "Iter 0344 | Time 48.7702(46.5866) | Bit/dim 5.5051(5.6334) | Xent 1.9710(1.9749) | Loss 13.8478(14.4560) | Error 0.6827(0.6851) Steps 496(492.25) | Grad Norm 31.9101(41.4370) | Total Time 0.00(0.00)\n",
      "Iter 0345 | Time 46.1272(46.5728) | Bit/dim 5.4901(5.6291) | Xent 1.9234(1.9734) | Loss 13.8281(14.4371) | Error 0.6526(0.6841) Steps 526(493.26) | Grad Norm 21.6177(40.8424) | Total Time 0.00(0.00)\n",
      "Iter 0346 | Time 49.6712(46.6658) | Bit/dim 5.4848(5.6247) | Xent 1.9367(1.9723) | Loss 13.9033(14.4211) | Error 0.6633(0.6835) Steps 520(494.06) | Grad Norm 39.9342(40.8151) | Total Time 0.00(0.00)\n",
      "Iter 0347 | Time 48.9806(46.7352) | Bit/dim 5.4763(5.6203) | Xent 1.9356(1.9712) | Loss 13.6705(14.3986) | Error 0.6711(0.6831) Steps 538(495.38) | Grad Norm 15.3961(40.0526) | Total Time 0.00(0.00)\n",
      "Iter 0348 | Time 45.9985(46.7131) | Bit/dim 5.4801(5.6161) | Xent 1.9245(1.9698) | Loss 13.9500(14.3851) | Error 0.6559(0.6823) Steps 520(496.12) | Grad Norm 22.5849(39.5285) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0058 | Time 21.1291, Epoch Time 322.9080(302.9110), Bit/dim 5.4718(best: 5.5014), Xent 1.9256, Loss 6.4346, Error 0.6663(best: 0.6459)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0349 | Time 50.1475(46.8162) | Bit/dim 5.4656(5.6116) | Xent 1.9389(1.9688) | Loss 16.9531(14.4622) | Error 0.6799(0.6822) Steps 502(496.30) | Grad Norm 31.3065(39.2819) | Total Time 0.00(0.00)\n",
      "Iter 0350 | Time 47.9351(46.8497) | Bit/dim 5.5078(5.6085) | Xent 1.9619(1.9686) | Loss 13.8855(14.4449) | Error 0.6844(0.6823) Steps 466(495.39) | Grad Norm 54.4473(39.7368) | Total Time 0.00(0.00)\n",
      "Iter 0351 | Time 48.4392(46.8974) | Bit/dim 5.4687(5.6043) | Xent 1.9131(1.9670) | Loss 13.6737(14.4217) | Error 0.6484(0.6813) Steps 514(495.95) | Grad Norm 19.3855(39.1263) | Total Time 0.00(0.00)\n",
      "Iter 0352 | Time 43.9865(46.8101) | Bit/dim 5.5323(5.6021) | Xent 2.0564(1.9696) | Loss 13.8722(14.4052) | Error 0.7304(0.6828) Steps 490(495.77) | Grad Norm 100.7747(40.9758) | Total Time 0.00(0.00)\n",
      "Iter 0353 | Time 46.9555(46.8145) | Bit/dim 5.5606(5.6009) | Xent 2.0218(1.9712) | Loss 13.7928(14.3869) | Error 0.7185(0.6838) Steps 502(495.95) | Grad Norm 84.6617(42.2863) | Total Time 0.00(0.00)\n",
      "Iter 0354 | Time 47.8013(46.8441) | Bit/dim 5.5148(5.5983) | Xent 2.0098(1.9724) | Loss 13.7068(14.3665) | Error 0.7250(0.6851) Steps 478(495.42) | Grad Norm 80.1649(43.4227) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0059 | Time 21.6558, Epoch Time 322.7338(303.5057), Bit/dim 5.5108(best: 5.4718), Xent 2.0136, Loss 6.5176, Error 0.7286(best: 0.6459)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0355 | Time 49.4839(46.9233) | Bit/dim 5.5246(5.5961) | Xent 2.0508(1.9747) | Loss 17.1207(14.4491) | Error 0.7364(0.6866) Steps 496(495.43) | Grad Norm 88.2631(44.7679) | Total Time 0.00(0.00)\n",
      "Iter 0356 | Time 50.2084(47.0218) | Bit/dim 5.4474(5.5916) | Xent 1.9240(1.9732) | Loss 13.9003(14.4326) | Error 0.6690(0.6861) Steps 478(494.91) | Grad Norm 20.2867(44.0335) | Total Time 0.00(0.00)\n",
      "Iter 0357 | Time 44.4493(46.9446) | Bit/dim 5.4884(5.5885) | Xent 2.0135(1.9744) | Loss 13.6751(14.4099) | Error 0.7103(0.6868) Steps 502(495.12) | Grad Norm 62.4132(44.5849) | Total Time 0.00(0.00)\n",
      "Iter 0358 | Time 53.6129(47.1447) | Bit/dim 5.4579(5.5846) | Xent 2.0152(1.9756) | Loss 13.5668(14.3846) | Error 0.7210(0.6878) Steps 532(496.23) | Grad Norm 40.9373(44.4754) | Total Time 0.00(0.00)\n",
      "Iter 0359 | Time 43.5276(47.0362) | Bit/dim 5.4679(5.5811) | Xent 1.9630(1.9752) | Loss 13.4030(14.3552) | Error 0.6795(0.6876) Steps 472(495.50) | Grad Norm 50.3451(44.6515) | Total Time 0.00(0.00)\n",
      "Iter 0360 | Time 47.8398(47.0603) | Bit/dim 5.4949(5.5785) | Xent 1.9639(1.9749) | Loss 13.6670(14.3345) | Error 0.7024(0.6880) Steps 508(495.88) | Grad Norm 53.9675(44.9310) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0060 | Time 20.6186, Epoch Time 325.1826(304.1560), Bit/dim 5.4283(best: 5.4718), Xent 1.9314, Loss 6.3940, Error 0.6547(best: 0.6459)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0361 | Time 50.0652(47.1504) | Bit/dim 5.4268(5.5740) | Xent 1.9550(1.9743) | Loss 16.8700(14.4106) | Error 0.6750(0.6876) Steps 526(496.78) | Grad Norm 16.6696(44.0832) | Total Time 0.00(0.00)\n",
      "Iter 0362 | Time 46.0124(47.1163) | Bit/dim 5.4685(5.5708) | Xent 1.9692(1.9742) | Loss 13.5649(14.3852) | Error 0.6921(0.6878) Steps 478(496.22) | Grad Norm 47.9545(44.1993) | Total Time 0.00(0.00)\n",
      "Iter 0363 | Time 44.4472(47.0362) | Bit/dim 5.4361(5.5668) | Xent 1.9475(1.9734) | Loss 13.5477(14.3601) | Error 0.6741(0.6874) Steps 496(496.21) | Grad Norm 30.6838(43.7938) | Total Time 0.00(0.00)\n",
      "Iter 0364 | Time 47.6735(47.0553) | Bit/dim 5.4311(5.5627) | Xent 1.9523(1.9727) | Loss 12.9699(14.3184) | Error 0.6784(0.6871) Steps 490(496.02) | Grad Norm 30.3329(43.3900) | Total Time 0.00(0.00)\n",
      "Iter 0365 | Time 52.3677(47.2147) | Bit/dim 5.4601(5.5596) | Xent 1.9322(1.9715) | Loss 13.7513(14.3014) | Error 0.6638(0.6864) Steps 472(495.30) | Grad Norm 28.6516(42.9479) | Total Time 0.00(0.00)\n",
      "Iter 0366 | Time 51.7634(47.3512) | Bit/dim 5.4111(5.5551) | Xent 1.9463(1.9708) | Loss 13.7887(14.2860) | Error 0.6730(0.6860) Steps 478(494.78) | Grad Norm 26.1032(42.4425) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0061 | Time 19.7205, Epoch Time 328.2504(304.8788), Bit/dim 5.4126(best: 5.4283), Xent 1.9306, Loss 6.3779, Error 0.6495(best: 0.6459)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0367 | Time 47.8951(47.3675) | Bit/dim 5.4124(5.5509) | Xent 1.9444(1.9700) | Loss 16.1825(14.3429) | Error 0.6683(0.6854) Steps 496(494.82) | Grad Norm 28.2565(42.0169) | Total Time 0.00(0.00)\n",
      "Iter 0368 | Time 46.7335(47.3485) | Bit/dim 5.4047(5.5465) | Xent 1.9611(1.9697) | Loss 13.5387(14.3188) | Error 0.6827(0.6854) Steps 514(495.40) | Grad Norm 30.5178(41.6720) | Total Time 0.00(0.00)\n",
      "Iter 0369 | Time 50.0760(47.4303) | Bit/dim 5.4026(5.5422) | Xent 1.9396(1.9688) | Loss 13.6129(14.2976) | Error 0.6609(0.6846) Steps 520(496.13) | Grad Norm 27.2337(41.2388) | Total Time 0.00(0.00)\n",
      "Iter 0370 | Time 49.6416(47.4966) | Bit/dim 5.3933(5.5377) | Xent 1.9224(1.9674) | Loss 13.6543(14.2783) | Error 0.6529(0.6837) Steps 472(495.41) | Grad Norm 15.3933(40.4635) | Total Time 0.00(0.00)\n",
      "Iter 0371 | Time 49.5958(47.5596) | Bit/dim 5.3894(5.5332) | Xent 1.9397(1.9666) | Loss 13.5144(14.2554) | Error 0.6645(0.6831) Steps 490(495.25) | Grad Norm 25.2425(40.0068) | Total Time 0.00(0.00)\n",
      "Iter 0372 | Time 49.1356(47.6069) | Bit/dim 5.3880(5.5289) | Xent 1.9197(1.9652) | Loss 13.5409(14.2339) | Error 0.6699(0.6827) Steps 514(495.81) | Grad Norm 13.0246(39.1974) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0062 | Time 21.8986, Epoch Time 330.6308(305.6514), Bit/dim 5.3911(best: 5.4126), Xent 1.9068, Loss 6.3445, Error 0.6441(best: 0.6459)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0373 | Time 47.2600(47.5965) | Bit/dim 5.3887(5.5247) | Xent 1.9158(1.9637) | Loss 16.0681(14.2890) | Error 0.6551(0.6819) Steps 514(496.36) | Grad Norm 26.8071(38.8257) | Total Time 0.00(0.00)\n",
      "Iter 0374 | Time 46.8970(47.5755) | Bit/dim 5.3820(5.5204) | Xent 1.9168(1.9623) | Loss 13.5670(14.2673) | Error 0.6467(0.6808) Steps 466(495.45) | Grad Norm 13.8000(38.0749) | Total Time 0.00(0.00)\n",
      "Iter 0375 | Time 47.3279(47.5681) | Bit/dim 5.3941(5.5166) | Xent 1.9303(1.9613) | Loss 13.6769(14.2496) | Error 0.6594(0.6802) Steps 472(494.74) | Grad Norm 38.9101(38.0999) | Total Time 0.00(0.00)\n",
      "Iter 0376 | Time 50.6741(47.6612) | Bit/dim 5.3837(5.5126) | Xent 1.9325(1.9604) | Loss 13.3070(14.2213) | Error 0.6647(0.6797) Steps 484(494.42) | Grad Norm 33.2230(37.9536) | Total Time 0.00(0.00)\n",
      "Iter 0377 | Time 47.6966(47.6623) | Bit/dim 5.3704(5.5084) | Xent 1.8903(1.9583) | Loss 13.3134(14.1941) | Error 0.6450(0.6787) Steps 484(494.11) | Grad Norm 19.9109(37.4123) | Total Time 0.00(0.00)\n",
      "Iter 0378 | Time 44.5793(47.5698) | Bit/dim 5.3395(5.5033) | Xent 1.9109(1.9569) | Loss 13.4281(14.1711) | Error 0.6507(0.6778) Steps 502(494.34) | Grad Norm 21.8910(36.9467) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0063 | Time 20.8686, Epoch Time 321.1373(306.1160), Bit/dim 5.3525(best: 5.3911), Xent 1.9000, Loss 6.3025, Error 0.6511(best: 0.6441)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0379 | Time 46.8767(47.5490) | Bit/dim 5.3494(5.4987) | Xent 1.9351(1.9563) | Loss 16.4867(14.2406) | Error 0.6734(0.6777) Steps 478(493.85) | Grad Norm 29.2712(36.7164) | Total Time 0.00(0.00)\n",
      "Iter 0380 | Time 47.7359(47.5546) | Bit/dim 5.3130(5.4931) | Xent 1.9180(1.9551) | Loss 13.4201(14.2159) | Error 0.6629(0.6773) Steps 502(494.10) | Grad Norm 23.2326(36.3119) | Total Time 0.00(0.00)\n",
      "Iter 0381 | Time 46.8524(47.5336) | Bit/dim 5.3388(5.4885) | Xent 1.8906(1.9532) | Loss 12.9396(14.1777) | Error 0.6407(0.6762) Steps 520(494.88) | Grad Norm 16.6959(35.7234) | Total Time 0.00(0.00)\n",
      "Iter 0382 | Time 43.5895(47.4152) | Bit/dim 5.3318(5.4838) | Xent 1.9379(1.9527) | Loss 13.2075(14.1486) | Error 0.6900(0.6766) Steps 496(494.91) | Grad Norm 32.6446(35.6311) | Total Time 0.00(0.00)\n",
      "Iter 0383 | Time 48.0337(47.4338) | Bit/dim 5.3056(5.4784) | Xent 1.9263(1.9519) | Loss 13.2680(14.1221) | Error 0.6760(0.6766) Steps 520(495.66) | Grad Norm 19.3722(35.1433) | Total Time 0.00(0.00)\n",
      "Iter 0384 | Time 44.9467(47.3592) | Bit/dim 5.3455(5.4744) | Xent 1.9391(1.9515) | Loss 13.3271(14.0983) | Error 0.6730(0.6765) Steps 478(495.13) | Grad Norm 28.8007(34.9530) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0064 | Time 19.7562, Epoch Time 313.8720(306.3486), Bit/dim 5.3148(best: 5.3525), Xent 1.9319, Loss 6.2807, Error 0.6623(best: 0.6441)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0385 | Time 47.9504(47.3769) | Bit/dim 5.3308(5.4701) | Xent 1.9686(1.9521) | Loss 16.2355(14.1624) | Error 0.6836(0.6767) Steps 514(495.70) | Grad Norm 30.9307(34.8324) | Total Time 0.00(0.00)\n",
      "Iter 0386 | Time 47.2154(47.3721) | Bit/dim 5.2849(5.4646) | Xent 1.8896(1.9502) | Loss 13.2012(14.1336) | Error 0.6515(0.6759) Steps 466(494.81) | Grad Norm 19.0488(34.3589) | Total Time 0.00(0.00)\n",
      "Iter 0387 | Time 49.7214(47.4425) | Bit/dim 5.3144(5.4601) | Xent 1.9086(1.9489) | Loss 13.2905(14.1083) | Error 0.6634(0.6755) Steps 490(494.66) | Grad Norm 37.5353(34.4542) | Total Time 0.00(0.00)\n",
      "Iter 0388 | Time 50.1527(47.5239) | Bit/dim 5.3126(5.4556) | Xent 1.9147(1.9479) | Loss 12.9546(14.0737) | Error 0.6661(0.6753) Steps 502(494.88) | Grad Norm 33.6212(34.4292) | Total Time 0.00(0.00)\n",
      "Iter 0389 | Time 50.5569(47.6148) | Bit/dim 5.2693(5.4501) | Xent 1.9324(1.9474) | Loss 13.4535(14.0551) | Error 0.6798(0.6754) Steps 490(494.74) | Grad Norm 25.9156(34.1738) | Total Time 0.00(0.00)\n",
      "Iter 0390 | Time 49.1702(47.6615) | Bit/dim 5.2836(5.4451) | Xent 1.9252(1.9468) | Loss 13.4198(14.0360) | Error 0.6683(0.6752) Steps 514(495.31) | Grad Norm 20.3735(33.7597) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0065 | Time 20.1689, Epoch Time 330.5310(307.0741), Bit/dim 5.2648(best: 5.3148), Xent 1.9117, Loss 6.2206, Error 0.6382(best: 0.6441)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0391 | Time 50.9109(47.7590) | Bit/dim 5.2747(5.4400) | Xent 1.9254(1.9461) | Loss 15.9782(14.0943) | Error 0.6705(0.6750) Steps 496(495.33) | Grad Norm 18.9192(33.3145) | Total Time 0.00(0.00)\n",
      "Iter 0392 | Time 47.5498(47.7527) | Bit/dim 5.2849(5.4353) | Xent 1.9429(1.9460) | Loss 13.2700(14.0695) | Error 0.6843(0.6753) Steps 502(495.53) | Grad Norm 42.4310(33.5880) | Total Time 0.00(0.00)\n",
      "Iter 0393 | Time 49.5232(47.8058) | Bit/dim 5.2689(5.4303) | Xent 1.9507(1.9462) | Loss 13.2490(14.0449) | Error 0.6786(0.6754) Steps 466(494.65) | Grad Norm 22.5179(33.2559) | Total Time 0.00(0.00)\n",
      "Iter 0394 | Time 50.8981(47.8986) | Bit/dim 5.2947(5.4262) | Xent 1.9206(1.9454) | Loss 13.2654(14.0215) | Error 0.6679(0.6752) Steps 496(494.69) | Grad Norm 42.7093(33.5395) | Total Time 0.00(0.00)\n",
      "Iter 0395 | Time 47.2430(47.8789) | Bit/dim 5.2516(5.4210) | Xent 1.8988(1.9440) | Loss 13.0590(13.9927) | Error 0.6419(0.6742) Steps 490(494.55) | Grad Norm 14.2395(32.9605) | Total Time 0.00(0.00)\n",
      "Iter 0396 | Time 55.8132(48.1170) | Bit/dim 5.3277(5.4182) | Xent 1.9478(1.9441) | Loss 13.3627(13.9738) | Error 0.6830(0.6745) Steps 538(495.85) | Grad Norm 71.5788(34.1191) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0066 | Time 21.0934, Epoch Time 338.7387(308.0241), Bit/dim 5.3071(best: 5.2648), Xent 1.8974, Loss 6.2558, Error 0.6422(best: 0.6382)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0397 | Time 49.5822(48.1609) | Bit/dim 5.3048(5.4148) | Xent 1.9420(1.9441) | Loss 15.9244(14.0323) | Error 0.6680(0.6743) Steps 514(496.40) | Grad Norm 58.9912(34.8652) | Total Time 0.00(0.00)\n",
      "Iter 0398 | Time 48.7082(48.1773) | Bit/dim 5.2429(5.4096) | Xent 1.9416(1.9440) | Loss 12.9681(14.0004) | Error 0.6838(0.6745) Steps 490(496.20) | Grad Norm 39.1491(34.9938) | Total Time 0.00(0.00)\n",
      "Iter 0399 | Time 46.0902(48.1147) | Bit/dim 5.2721(5.4055) | Xent 1.8889(1.9423) | Loss 13.2775(13.9787) | Error 0.6472(0.6737) Steps 490(496.02) | Grad Norm 34.5265(34.9797) | Total Time 0.00(0.00)\n",
      "Iter 0400 | Time 47.9375(48.1094) | Bit/dim 5.2493(5.4008) | Xent 1.9916(1.9438) | Loss 13.0651(13.9513) | Error 0.7074(0.6747) Steps 508(496.38) | Grad Norm 35.1147(34.9838) | Total Time 0.00(0.00)\n",
      "Iter 0401 | Time 49.8036(48.1602) | Bit/dim 5.2359(5.3959) | Xent 1.9444(1.9438) | Loss 13.1393(13.9269) | Error 0.6845(0.6750) Steps 490(496.19) | Grad Norm 24.0914(34.6570) | Total Time 0.00(0.00)\n",
      "Iter 0402 | Time 45.2401(48.0726) | Bit/dim 5.2646(5.3919) | Xent 1.9528(1.9441) | Loss 13.2402(13.9063) | Error 0.6816(0.6752) Steps 460(495.10) | Grad Norm 44.9766(34.9666) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0067 | Time 21.0631, Epoch Time 324.3409(308.5136), Bit/dim 5.2520(best: 5.2648), Xent 1.9765, Loss 6.2402, Error 0.6963(best: 0.6382)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0403 | Time 49.4545(48.1141) | Bit/dim 5.2581(5.3879) | Xent 2.0038(1.9459) | Loss 16.1433(13.9734) | Error 0.7132(0.6764) Steps 484(494.77) | Grad Norm 42.4055(35.1898) | Total Time 0.00(0.00)\n",
      "Iter 0404 | Time 50.5012(48.1857) | Bit/dim 5.2233(5.3830) | Xent 1.9153(1.9450) | Loss 13.1704(13.9493) | Error 0.6610(0.6759) Steps 514(495.35) | Grad Norm 22.1943(34.7999) | Total Time 0.00(0.00)\n",
      "Iter 0405 | Time 49.0594(48.2119) | Bit/dim 5.2134(5.3779) | Xent 1.9138(1.9440) | Loss 13.2981(13.9298) | Error 0.6638(0.6755) Steps 490(495.18) | Grad Norm 20.2588(34.3637) | Total Time 0.00(0.00)\n",
      "Iter 0406 | Time 51.2168(48.3020) | Bit/dim 5.2579(5.3743) | Xent 1.8973(1.9426) | Loss 13.3891(13.9136) | Error 0.6526(0.6749) Steps 478(494.67) | Grad Norm 24.0250(34.0535) | Total Time 0.00(0.00)\n",
      "Iter 0407 | Time 53.6524(48.4626) | Bit/dim 5.2256(5.3698) | Xent 1.8993(1.9413) | Loss 13.2374(13.8933) | Error 0.6567(0.6743) Steps 556(496.51) | Grad Norm 19.8429(33.6272) | Total Time 0.00(0.00)\n",
      "Iter 0408 | Time 56.1792(48.6941) | Bit/dim 5.2236(5.3655) | Xent 1.9277(1.9409) | Loss 13.4035(13.8786) | Error 0.6685(0.6741) Steps 496(496.49) | Grad Norm 36.7710(33.7215) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0068 | Time 21.1844, Epoch Time 347.5283(309.6840), Bit/dim 5.2206(best: 5.2520), Xent 1.8938, Loss 6.1675, Error 0.6433(best: 0.6382)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0409 | Time 48.3119(48.6826) | Bit/dim 5.2398(5.3617) | Xent 1.9169(1.9402) | Loss 15.8316(13.9372) | Error 0.6616(0.6738) Steps 502(496.66) | Grad Norm 36.5586(33.8066) | Total Time 0.00(0.00)\n",
      "Iter 0410 | Time 50.6781(48.7425) | Bit/dim 5.1885(5.3565) | Xent 1.9299(1.9399) | Loss 12.8215(13.9037) | Error 0.6735(0.6738) Steps 532(497.72) | Grad Norm 26.5040(33.5875) | Total Time 0.00(0.00)\n",
      "Iter 0411 | Time 49.0556(48.7518) | Bit/dim 5.2123(5.3522) | Xent 1.8961(1.9386) | Loss 13.0732(13.8788) | Error 0.6643(0.6735) Steps 526(498.57) | Grad Norm 24.9309(33.3278) | Total Time 0.00(0.00)\n",
      "Iter 0412 | Time 55.8143(48.9637) | Bit/dim 5.1953(5.3475) | Xent 1.9001(1.9374) | Loss 13.2247(13.8592) | Error 0.6600(0.6731) Steps 490(498.31) | Grad Norm 24.7302(33.0699) | Total Time 0.00(0.00)\n",
      "Iter 0413 | Time 49.5996(48.9828) | Bit/dim 5.2031(5.3431) | Xent 1.9211(1.9369) | Loss 13.0969(13.8363) | Error 0.6635(0.6728) Steps 484(497.88) | Grad Norm 23.6420(32.7871) | Total Time 0.00(0.00)\n",
      "Iter 0414 | Time 49.4800(48.9977) | Bit/dim 5.1934(5.3386) | Xent 1.8876(1.9355) | Loss 13.0550(13.8129) | Error 0.6480(0.6720) Steps 526(498.72) | Grad Norm 16.5604(32.3003) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0069 | Time 20.6777, Epoch Time 339.5322(310.5794), Bit/dim 5.1761(best: 5.2206), Xent 1.8937, Loss 6.1229, Error 0.6551(best: 0.6382)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0415 | Time 50.2337(49.0348) | Bit/dim 5.1785(5.3338) | Xent 1.9043(1.9345) | Loss 15.7996(13.8725) | Error 0.6609(0.6717) Steps 502(498.82) | Grad Norm 25.0056(32.0814) | Total Time 0.00(0.00)\n",
      "Iter 0416 | Time 50.6828(49.0842) | Bit/dim 5.1564(5.3285) | Xent 1.8945(1.9333) | Loss 13.0577(13.8480) | Error 0.6538(0.6712) Steps 466(497.84) | Grad Norm 16.8499(31.6245) | Total Time 0.00(0.00)\n",
      "Iter 0417 | Time 52.3079(49.1809) | Bit/dim 5.1686(5.3237) | Xent 1.9611(1.9342) | Loss 13.4276(13.8354) | Error 0.6860(0.6716) Steps 496(497.78) | Grad Norm 36.7750(31.7790) | Total Time 0.00(0.00)\n",
      "Iter 0418 | Time 54.3023(49.3346) | Bit/dim 5.1774(5.3193) | Xent 1.9068(1.9333) | Loss 12.6759(13.8006) | Error 0.6603(0.6713) Steps 532(498.81) | Grad Norm 22.5839(31.5032) | Total Time 0.00(0.00)\n",
      "Iter 0419 | Time 52.9716(49.4437) | Bit/dim 5.1569(5.3144) | Xent 1.9286(1.9332) | Loss 13.0722(13.7788) | Error 0.6806(0.6715) Steps 496(498.73) | Grad Norm 34.1502(31.5826) | Total Time 0.00(0.00)\n",
      "Iter 0420 | Time 48.7360(49.4225) | Bit/dim 5.1737(5.3102) | Xent 1.9320(1.9332) | Loss 13.1913(13.7611) | Error 0.6725(0.6716) Steps 538(499.90) | Grad Norm 32.9966(31.6250) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0070 | Time 20.6415, Epoch Time 345.7055(311.6332), Bit/dim 5.1533(best: 5.1761), Xent 1.8855, Loss 6.0961, Error 0.6348(best: 0.6382)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0421 | Time 46.3486(49.3303) | Bit/dim 5.1469(5.3053) | Xent 1.9059(1.9323) | Loss 15.7323(13.8203) | Error 0.6635(0.6713) Steps 484(499.43) | Grad Norm 19.6250(31.2650) | Total Time 0.00(0.00)\n",
      "Iter 0422 | Time 50.0223(49.3510) | Bit/dim 5.1579(5.3009) | Xent 1.8898(1.9311) | Loss 13.0009(13.7957) | Error 0.6474(0.6706) Steps 502(499.50) | Grad Norm 17.9771(30.8663) | Total Time 0.00(0.00)\n",
      "Iter 0423 | Time 50.4961(49.3854) | Bit/dim 5.1401(5.2961) | Xent 1.8972(1.9300) | Loss 13.2061(13.7780) | Error 0.6575(0.6702) Steps 544(500.84) | Grad Norm 19.0198(30.5110) | Total Time 0.00(0.00)\n",
      "Iter 0424 | Time 49.7513(49.3963) | Bit/dim 5.1451(5.2915) | Xent 1.8813(1.9286) | Loss 12.9341(13.7527) | Error 0.6467(0.6695) Steps 484(500.33) | Grad Norm 11.4882(29.9403) | Total Time 0.00(0.00)\n",
      "Iter 0425 | Time 50.0268(49.4153) | Bit/dim 5.1633(5.2877) | Xent 1.9066(1.9279) | Loss 13.2101(13.7364) | Error 0.6634(0.6693) Steps 532(501.28) | Grad Norm 23.9040(29.7592) | Total Time 0.00(0.00)\n",
      "Iter 0426 | Time 50.5788(49.4502) | Bit/dim 5.1247(5.2828) | Xent 1.8809(1.9265) | Loss 13.0498(13.7158) | Error 0.6546(0.6689) Steps 496(501.13) | Grad Norm 14.5525(29.3030) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0071 | Time 21.4393, Epoch Time 335.0634(312.3361), Bit/dim 5.1453(best: 5.1533), Xent 1.8980, Loss 6.0943, Error 0.6734(best: 0.6348)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0427 | Time 46.7611(49.3695) | Bit/dim 5.1402(5.2785) | Xent 1.9184(1.9263) | Loss 15.4307(13.7673) | Error 0.6707(0.6689) Steps 484(500.61) | Grad Norm 36.8187(29.5285) | Total Time 0.00(0.00)\n",
      "Iter 0428 | Time 51.4596(49.4322) | Bit/dim 5.1403(5.2744) | Xent 1.9227(1.9262) | Loss 13.0517(13.7458) | Error 0.6720(0.6690) Steps 508(500.83) | Grad Norm 26.6316(29.4416) | Total Time 0.00(0.00)\n",
      "Iter 0429 | Time 51.0940(49.4820) | Bit/dim 5.1494(5.2706) | Xent 1.8790(1.9247) | Loss 13.1853(13.7290) | Error 0.6467(0.6684) Steps 508(501.05) | Grad Norm 24.1440(29.2826) | Total Time 0.00(0.00)\n",
      "Iter 0430 | Time 53.9738(49.6168) | Bit/dim 5.1218(5.2662) | Xent 1.9353(1.9251) | Loss 12.9367(13.7052) | Error 0.6741(0.6685) Steps 472(500.18) | Grad Norm 28.8521(29.2697) | Total Time 0.00(0.00)\n",
      "Iter 0431 | Time 48.3300(49.5782) | Bit/dim 5.1026(5.2613) | Xent 1.8668(1.9233) | Loss 12.7988(13.6780) | Error 0.6410(0.6677) Steps 478(499.51) | Grad Norm 17.9778(28.9309) | Total Time 0.00(0.00)\n",
      "Iter 0432 | Time 51.3855(49.6324) | Bit/dim 5.1088(5.2567) | Xent 1.9325(1.9236) | Loss 13.1727(13.6629) | Error 0.6724(0.6679) Steps 538(500.67) | Grad Norm 41.3194(29.3026) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0072 | Time 20.0635, Epoch Time 339.1916(313.1418), Bit/dim 5.1192(best: 5.1453), Xent 1.9106, Loss 6.0745, Error 0.6705(best: 0.6348)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0433 | Time 52.5401(49.7196) | Bit/dim 5.1221(5.2527) | Xent 1.9287(1.9237) | Loss 15.9647(13.7319) | Error 0.6721(0.6680) Steps 490(500.35) | Grad Norm 39.4720(29.6077) | Total Time 0.00(0.00)\n",
      "Iter 0434 | Time 48.5808(49.6855) | Bit/dim 5.0906(5.2478) | Xent 1.8609(1.9219) | Loss 13.1307(13.7139) | Error 0.6492(0.6674) Steps 502(500.40) | Grad Norm 8.6996(28.9804) | Total Time 0.00(0.00)\n",
      "Iter 0435 | Time 54.6376(49.8340) | Bit/dim 5.1045(5.2435) | Xent 1.9005(1.9212) | Loss 12.9225(13.6901) | Error 0.6587(0.6672) Steps 490(500.08) | Grad Norm 23.2264(28.8078) | Total Time 0.00(0.00)\n",
      "Iter 0436 | Time 50.0435(49.8403) | Bit/dim 5.0957(5.2391) | Xent 1.9042(1.9207) | Loss 13.3318(13.6794) | Error 0.6658(0.6671) Steps 532(501.04) | Grad Norm 17.3859(28.4652) | Total Time 0.00(0.00)\n",
      "Iter 0437 | Time 50.6528(49.8647) | Bit/dim 5.0911(5.2346) | Xent 1.8756(1.9194) | Loss 12.8037(13.6531) | Error 0.6511(0.6666) Steps 514(501.43) | Grad Norm 25.6908(28.3819) | Total Time 0.00(0.00)\n",
      "Iter 0438 | Time 47.2595(49.7865) | Bit/dim 5.0756(5.2299) | Xent 1.9034(1.9189) | Loss 12.8484(13.6290) | Error 0.6695(0.6667) Steps 502(501.45) | Grad Norm 17.1551(28.0451) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0073 | Time 21.3012, Epoch Time 340.6097(313.9658), Bit/dim 5.0825(best: 5.1192), Xent 1.8546, Loss 6.0098, Error 0.6183(best: 0.6348)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0439 | Time 50.6853(49.8135) | Bit/dim 5.0855(5.2255) | Xent 1.8699(1.9174) | Loss 15.4665(13.6841) | Error 0.6405(0.6659) Steps 550(502.90) | Grad Norm 18.1355(27.7478) | Total Time 0.00(0.00)\n",
      "Iter 0440 | Time 50.2902(49.8278) | Bit/dim 5.0896(5.2214) | Xent 1.9291(1.9178) | Loss 12.9083(13.6608) | Error 0.6759(0.6662) Steps 454(501.44) | Grad Norm 34.1240(27.9391) | Total Time 0.00(0.00)\n",
      "Iter 0441 | Time 51.2196(49.8696) | Bit/dim 5.0706(5.2169) | Xent 1.8765(1.9165) | Loss 12.8249(13.6357) | Error 0.6499(0.6657) Steps 514(501.81) | Grad Norm 17.6665(27.6309) | Total Time 0.00(0.00)\n",
      "Iter 0442 | Time 53.2866(49.9721) | Bit/dim 5.0946(5.2132) | Xent 1.8580(1.9148) | Loss 12.8351(13.6117) | Error 0.6380(0.6649) Steps 508(502.00) | Grad Norm 33.5061(27.8072) | Total Time 0.00(0.00)\n",
      "Iter 0443 | Time 55.6846(50.1434) | Bit/dim 5.0781(5.2092) | Xent 1.9049(1.9145) | Loss 12.9559(13.5921) | Error 0.6714(0.6651) Steps 538(503.08) | Grad Norm 24.1006(27.6960) | Total Time 0.00(0.00)\n",
      "Iter 0444 | Time 49.8288(50.1340) | Bit/dim 5.0764(5.2052) | Xent 1.8903(1.9137) | Loss 12.7257(13.5661) | Error 0.6600(0.6650) Steps 502(503.05) | Grad Norm 22.5342(27.5411) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0074 | Time 20.8857, Epoch Time 347.7342(314.9789), Bit/dim 5.0620(best: 5.0825), Xent 1.9131, Loss 6.0185, Error 0.6659(best: 0.6183)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0445 | Time 51.4214(50.1726) | Bit/dim 5.0749(5.2013) | Xent 1.9327(1.9143) | Loss 15.7819(13.6325) | Error 0.6805(0.6654) Steps 478(502.30) | Grad Norm 25.1801(27.4703) | Total Time 0.00(0.00)\n",
      "Iter 0446 | Time 48.7868(50.1311) | Bit/dim 5.0592(5.1970) | Xent 1.9157(1.9144) | Loss 12.7426(13.6058) | Error 0.6641(0.6654) Steps 496(502.11) | Grad Norm 25.5453(27.4126) | Total Time 0.00(0.00)\n",
      "Iter 0447 | Time 46.6439(50.0264) | Bit/dim 5.0717(5.1933) | Xent 1.8712(1.9131) | Loss 12.6852(13.5782) | Error 0.6547(0.6651) Steps 502(502.10) | Grad Norm 21.4511(27.2337) | Total Time 0.00(0.00)\n",
      "Iter 0448 | Time 51.3943(50.0675) | Bit/dim 5.0683(5.1895) | Xent 1.8660(1.9117) | Loss 12.9597(13.5597) | Error 0.6465(0.6645) Steps 508(502.28) | Grad Norm 28.4060(27.2689) | Total Time 0.00(0.00)\n",
      "Iter 0449 | Time 47.9975(50.0054) | Bit/dim 5.0525(5.1854) | Xent 1.9045(1.9114) | Loss 12.9005(13.5399) | Error 0.6640(0.6645) Steps 520(502.81) | Grad Norm 24.2541(27.1784) | Total Time 0.00(0.00)\n",
      "Iter 0450 | Time 52.6209(50.0838) | Bit/dim 5.0386(5.1810) | Xent 1.8902(1.9108) | Loss 12.9138(13.5211) | Error 0.6601(0.6644) Steps 466(501.71) | Grad Norm 20.9746(26.9923) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0075 | Time 21.1742, Epoch Time 335.8692(315.6056), Bit/dim 5.0510(best: 5.0620), Xent 1.8772, Loss 5.9896, Error 0.6352(best: 0.6183)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0451 | Time 47.4793(50.0057) | Bit/dim 5.0625(5.1775) | Xent 1.9013(1.9105) | Loss 15.7788(13.5888) | Error 0.6659(0.6644) Steps 484(501.18) | Grad Norm 26.3676(26.9736) | Total Time 0.00(0.00)\n",
      "Iter 0452 | Time 50.5197(50.0211) | Bit/dim 5.0519(5.1737) | Xent 1.9118(1.9106) | Loss 12.7120(13.5625) | Error 0.6626(0.6644) Steps 502(501.20) | Grad Norm 19.5737(26.7516) | Total Time 0.00(0.00)\n",
      "Iter 0453 | Time 46.7813(49.9239) | Bit/dim 5.0614(5.1703) | Xent 1.8902(1.9099) | Loss 12.7224(13.5373) | Error 0.6607(0.6642) Steps 490(500.86) | Grad Norm 33.3425(26.9493) | Total Time 0.00(0.00)\n",
      "Iter 0454 | Time 47.9741(49.8654) | Bit/dim 5.0041(5.1653) | Xent 1.8642(1.9086) | Loss 12.7650(13.5142) | Error 0.6467(0.6637) Steps 520(501.44) | Grad Norm 3.4701(26.2449) | Total Time 0.00(0.00)\n",
      "Iter 0455 | Time 50.3479(49.8799) | Bit/dim 5.0267(5.1612) | Xent 1.8625(1.9072) | Loss 12.8088(13.4930) | Error 0.6536(0.6634) Steps 514(501.82) | Grad Norm 21.7621(26.1105) | Total Time 0.00(0.00)\n",
      "Iter 0456 | Time 55.3210(50.0431) | Bit/dim 5.0244(5.1571) | Xent 1.9046(1.9071) | Loss 12.6214(13.4669) | Error 0.6745(0.6637) Steps 514(502.18) | Grad Norm 24.7166(26.0686) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0076 | Time 21.0718, Epoch Time 335.4266(316.2002), Bit/dim 5.0087(best: 5.0510), Xent 1.8387, Loss 5.9281, Error 0.6205(best: 0.6183)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0457 | Time 51.5582(50.0886) | Bit/dim 5.0104(5.1527) | Xent 1.8416(1.9051) | Loss 14.4303(13.4958) | Error 0.6302(0.6627) Steps 520(502.72) | Grad Norm 11.9336(25.6446) | Total Time 0.00(0.00)\n",
      "Iter 0458 | Time 47.3505(50.0064) | Bit/dim 5.0082(5.1483) | Xent 1.8599(1.9038) | Loss 12.6321(13.4698) | Error 0.6422(0.6621) Steps 514(503.05) | Grad Norm 9.7651(25.1682) | Total Time 0.00(0.00)\n",
      "Iter 0459 | Time 53.3427(50.1065) | Bit/dim 5.0088(5.1442) | Xent 1.8889(1.9033) | Loss 12.9241(13.4535) | Error 0.6501(0.6618) Steps 526(503.74) | Grad Norm 11.7767(24.7665) | Total Time 0.00(0.00)\n",
      "Iter 0460 | Time 51.9635(50.1622) | Bit/dim 5.0143(5.1403) | Xent 1.8447(1.9016) | Loss 13.0301(13.4408) | Error 0.6349(0.6610) Steps 508(503.87) | Grad Norm 10.4853(24.3380) | Total Time 0.00(0.00)\n",
      "Iter 0461 | Time 49.3928(50.1392) | Bit/dim 4.9872(5.1357) | Xent 1.8704(1.9006) | Loss 12.6162(13.4160) | Error 0.6587(0.6609) Steps 508(503.99) | Grad Norm 18.8173(24.1724) | Total Time 0.00(0.00)\n",
      "Iter 0462 | Time 52.2546(50.2026) | Bit/dim 5.0037(5.1317) | Xent 1.8761(1.8999) | Loss 12.7797(13.3969) | Error 0.6475(0.6605) Steps 544(505.19) | Grad Norm 24.2169(24.1737) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0077 | Time 22.1379, Epoch Time 343.9831(317.0337), Bit/dim 4.9843(best: 5.0087), Xent 1.8291, Loss 5.8989, Error 0.6244(best: 0.6183)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0463 | Time 50.9230(50.2242) | Bit/dim 4.9778(5.1271) | Xent 1.8400(1.8981) | Loss 15.5094(13.4603) | Error 0.6465(0.6601) Steps 502(505.10) | Grad Norm 10.5529(23.7651) | Total Time 0.00(0.00)\n",
      "Iter 0464 | Time 50.2027(50.2236) | Bit/dim 4.9867(5.1229) | Xent 1.8421(1.8964) | Loss 12.7866(13.4401) | Error 0.6409(0.6595) Steps 532(505.91) | Grad Norm 8.9518(23.3207) | Total Time 0.00(0.00)\n",
      "Iter 0465 | Time 51.2216(50.2535) | Bit/dim 4.9704(5.1183) | Xent 1.8526(1.8951) | Loss 12.8289(13.4218) | Error 0.6464(0.6591) Steps 538(506.87) | Grad Norm 9.0897(22.8938) | Total Time 0.00(0.00)\n",
      "Iter 0466 | Time 51.9904(50.3056) | Bit/dim 4.9863(5.1143) | Xent 1.8909(1.8950) | Loss 12.9330(13.4071) | Error 0.6663(0.6593) Steps 484(506.18) | Grad Norm 31.3680(23.1480) | Total Time 0.00(0.00)\n",
      "Iter 0467 | Time 50.0773(50.2988) | Bit/dim 4.9869(5.1105) | Xent 1.8463(1.8935) | Loss 12.4322(13.3779) | Error 0.6402(0.6587) Steps 508(506.24) | Grad Norm 18.2414(23.0008) | Total Time 0.00(0.00)\n",
      "Iter 0468 | Time 49.3669(50.2708) | Bit/dim 4.9566(5.1059) | Xent 1.8431(1.8920) | Loss 12.6868(13.3571) | Error 0.6355(0.6580) Steps 526(506.83) | Grad Norm 5.2136(22.4672) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0078 | Time 20.7770, Epoch Time 340.5193(317.7383), Bit/dim 4.9568(best: 4.9843), Xent 1.8137, Loss 5.8637, Error 0.6171(best: 0.6183)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0469 | Time 51.5462(50.3091) | Bit/dim 4.9710(5.1019) | Xent 1.8339(1.8903) | Loss 15.3393(13.4166) | Error 0.6345(0.6573) Steps 520(507.22) | Grad Norm 8.9690(22.0622) | Total Time 0.00(0.00)\n",
      "Iter 0470 | Time 54.2672(50.4278) | Bit/dim 4.9605(5.0976) | Xent 1.8174(1.8881) | Loss 12.5268(13.3899) | Error 0.6262(0.6564) Steps 490(506.71) | Grad Norm 13.0456(21.7917) | Total Time 0.00(0.00)\n",
      "Iter 0471 | Time 54.4881(50.5496) | Bit/dim 4.9456(5.0931) | Xent 1.8470(1.8869) | Loss 12.4657(13.3622) | Error 0.6455(0.6561) Steps 514(506.93) | Grad Norm 23.6935(21.8488) | Total Time 0.00(0.00)\n",
      "Iter 0472 | Time 48.0447(50.4745) | Bit/dim 4.9686(5.0893) | Xent 1.8892(1.8869) | Loss 12.7093(13.3426) | Error 0.6556(0.6561) Steps 508(506.96) | Grad Norm 31.0659(22.1253) | Total Time 0.00(0.00)\n",
      "Iter 0473 | Time 49.8283(50.4551) | Bit/dim 4.9434(5.0849) | Xent 1.8525(1.8859) | Loss 12.5365(13.3184) | Error 0.6438(0.6557) Steps 496(506.63) | Grad Norm 14.1229(21.8852) | Total Time 0.00(0.00)\n",
      "Iter 0474 | Time 54.0717(50.5636) | Bit/dim 4.9579(5.0811) | Xent 1.8744(1.8855) | Loss 12.6690(13.2989) | Error 0.6605(0.6558) Steps 550(507.93) | Grad Norm 30.4101(22.1410) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0079 | Time 20.5962, Epoch Time 348.6960(318.6670), Bit/dim 4.9530(best: 4.9568), Xent 1.8320, Loss 5.8690, Error 0.6332(best: 0.6171)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0475 | Time 49.0478(50.5181) | Bit/dim 4.9537(5.0773) | Xent 1.8800(1.8854) | Loss 15.1972(13.3559) | Error 0.6564(0.6559) Steps 508(507.93) | Grad Norm 34.6070(22.5150) | Total Time 0.00(0.00)\n",
      "Iter 0476 | Time 49.3880(50.4842) | Bit/dim 4.9393(5.0732) | Xent 1.8326(1.8838) | Loss 12.2494(13.3227) | Error 0.6342(0.6552) Steps 520(508.30) | Grad Norm 8.0952(22.0824) | Total Time 0.00(0.00)\n",
      "Iter 0477 | Time 53.3057(50.5689) | Bit/dim 4.9737(5.0702) | Xent 1.9050(1.8844) | Loss 12.8296(13.3079) | Error 0.6651(0.6555) Steps 526(508.83) | Grad Norm 55.2285(23.0768) | Total Time 0.00(0.00)\n",
      "Iter 0478 | Time 54.1172(50.6753) | Bit/dim 4.9304(5.0660) | Xent 1.8542(1.8835) | Loss 12.6478(13.2881) | Error 0.6525(0.6554) Steps 520(509.16) | Grad Norm 24.4799(23.1188) | Total Time 0.00(0.00)\n",
      "Iter 0479 | Time 51.9152(50.7125) | Bit/dim 5.0045(5.0641) | Xent 1.9203(1.8846) | Loss 12.9046(13.2766) | Error 0.6795(0.6561) Steps 532(509.85) | Grad Norm 64.0581(24.3470) | Total Time 0.00(0.00)\n",
      "Iter 0480 | Time 55.3383(50.8513) | Bit/dim 5.0178(5.0628) | Xent 1.9832(1.8876) | Loss 12.3659(13.2493) | Error 0.7070(0.6577) Steps 490(509.25) | Grad Norm 65.9609(25.5954) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0080 | Time 21.0456, Epoch Time 350.4556(319.6207), Bit/dim 4.9243(best: 4.9530), Xent 1.8003, Loss 5.8245, Error 0.6149(best: 0.6171)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0481 | Time 53.9148(50.9432) | Bit/dim 4.9213(5.0585) | Xent 1.8403(1.8862) | Loss 15.3767(13.3131) | Error 0.6434(0.6572) Steps 520(509.57) | Grad Norm 12.0034(25.1877) | Total Time 0.00(0.00)\n",
      "Iter 0482 | Time 54.5436(51.0512) | Bit/dim 4.9829(5.0562) | Xent 1.9096(1.8869) | Loss 12.5352(13.2897) | Error 0.6755(0.6578) Steps 550(510.79) | Grad Norm 56.9340(26.1401) | Total Time 0.00(0.00)\n",
      "Iter 0483 | Time 50.6184(51.0382) | Bit/dim 5.0049(5.0547) | Xent 1.9291(1.8881) | Loss 12.9610(13.2799) | Error 0.6784(0.6584) Steps 502(510.52) | Grad Norm 45.6188(26.7244) | Total Time 0.00(0.00)\n",
      "Iter 0484 | Time 47.3897(50.9288) | Bit/dim 5.0128(5.0534) | Xent 1.8439(1.8868) | Loss 12.6574(13.2612) | Error 0.6431(0.6579) Steps 496(510.09) | Grad Norm 19.9024(26.5198) | Total Time 0.00(0.00)\n",
      "Iter 0485 | Time 50.0269(50.9017) | Bit/dim 4.9268(5.0496) | Xent 1.8355(1.8853) | Loss 12.3883(13.2350) | Error 0.6398(0.6574) Steps 484(509.30) | Grad Norm 17.5991(26.2522) | Total Time 0.00(0.00)\n",
      "Iter 0486 | Time 57.5575(51.1014) | Bit/dim 5.0214(5.0488) | Xent 1.9477(1.8871) | Loss 12.7102(13.2193) | Error 0.6729(0.6579) Steps 514(509.45) | Grad Norm 36.7219(26.5662) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0081 | Time 21.3572, Epoch Time 351.4721(320.5762), Bit/dim 4.9496(best: 4.9243), Xent 1.8205, Loss 5.8598, Error 0.6248(best: 0.6149)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0487 | Time 50.8034(51.0924) | Bit/dim 4.9545(5.0460) | Xent 1.8561(1.8862) | Loss 15.4761(13.2870) | Error 0.6465(0.6575) Steps 502(509.22) | Grad Norm 22.5968(26.4472) | Total Time 0.00(0.00)\n",
      "Iter 0488 | Time 51.0676(51.0917) | Bit/dim 4.9137(5.0420) | Xent 1.8665(1.8856) | Loss 12.4031(13.2605) | Error 0.6609(0.6576) Steps 490(508.65) | Grad Norm 19.5252(26.2395) | Total Time 0.00(0.00)\n",
      "Iter 0489 | Time 53.6616(51.1688) | Bit/dim 4.9565(5.0394) | Xent 1.9114(1.8864) | Loss 12.6075(13.2409) | Error 0.6786(0.6583) Steps 514(508.81) | Grad Norm 41.5918(26.7001) | Total Time 0.00(0.00)\n",
      "Iter 0490 | Time 55.9583(51.3125) | Bit/dim 4.9469(5.0367) | Xent 1.9131(1.8872) | Loss 12.7711(13.2268) | Error 0.6659(0.6585) Steps 496(508.42) | Grad Norm 42.2160(27.1655) | Total Time 0.00(0.00)\n",
      "Iter 0491 | Time 50.7222(51.2948) | Bit/dim 4.9222(5.0332) | Xent 1.8676(1.8866) | Loss 12.3558(13.2006) | Error 0.6570(0.6584) Steps 526(508.95) | Grad Norm 9.3220(26.6302) | Total Time 0.00(0.00)\n",
      "Iter 0492 | Time 57.5019(51.4810) | Bit/dim 4.9271(5.0300) | Xent 1.8467(1.8854) | Loss 12.6159(13.1831) | Error 0.6447(0.6580) Steps 532(509.64) | Grad Norm 21.9966(26.4912) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0082 | Time 20.9608, Epoch Time 356.5068(321.6541), Bit/dim 4.9006(best: 4.9243), Xent 1.8504, Loss 5.8258, Error 0.6423(best: 0.6149)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0493 | Time 48.7019(51.3976) | Bit/dim 4.8924(5.0259) | Xent 1.8943(1.8857) | Loss 15.4345(13.2506) | Error 0.6686(0.6583) Steps 526(510.13) | Grad Norm 15.6943(26.1673) | Total Time 0.00(0.00)\n",
      "Iter 0494 | Time 48.0117(51.2960) | Bit/dim 4.9059(5.0223) | Xent 1.8672(1.8851) | Loss 12.4170(13.2256) | Error 0.6541(0.6582) Steps 508(510.07) | Grad Norm 8.8195(25.6469) | Total Time 0.00(0.00)\n",
      "Iter 0495 | Time 48.8656(51.2231) | Bit/dim 4.9001(5.0186) | Xent 1.8718(1.8847) | Loss 12.5732(13.2061) | Error 0.6607(0.6583) Steps 502(509.83) | Grad Norm 20.9425(25.5058) | Total Time 0.00(0.00)\n",
      "Iter 0496 | Time 51.4900(51.2311) | Bit/dim 4.9173(5.0156) | Xent 1.8544(1.8838) | Loss 12.3204(13.1795) | Error 0.6552(0.6582) Steps 472(508.69) | Grad Norm 25.0704(25.4927) | Total Time 0.00(0.00)\n",
      "Iter 0497 | Time 48.4213(51.1468) | Bit/dim 4.8735(5.0113) | Xent 1.8448(1.8826) | Loss 12.2526(13.1517) | Error 0.6470(0.6579) Steps 466(507.41) | Grad Norm 7.6839(24.9584) | Total Time 0.00(0.00)\n",
      "Iter 0498 | Time 53.3914(51.2142) | Bit/dim 4.8750(5.0073) | Xent 1.8076(1.8804) | Loss 12.3722(13.1283) | Error 0.6215(0.6568) Steps 496(507.07) | Grad Norm 11.3120(24.5490) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0083 | Time 21.8512, Epoch Time 336.7055(322.1057), Bit/dim 4.8873(best: 4.9006), Xent 1.8100, Loss 5.7923, Error 0.6261(best: 0.6149)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0499 | Time 53.5527(51.2843) | Bit/dim 4.8623(5.0029) | Xent 1.8198(1.8786) | Loss 14.7318(13.1764) | Error 0.6370(0.6562) Steps 520(507.46) | Grad Norm 19.8051(24.4067) | Total Time 0.00(0.00)\n",
      "Iter 0500 | Time 51.5640(51.2927) | Bit/dim 4.8938(4.9996) | Xent 1.8675(1.8782) | Loss 12.3628(13.1520) | Error 0.6526(0.6561) Steps 538(508.37) | Grad Norm 24.7638(24.4174) | Total Time 0.00(0.00)\n",
      "Iter 0501 | Time 53.9553(51.3726) | Bit/dim 4.8843(4.9962) | Xent 1.8249(1.8766) | Loss 12.6271(13.1363) | Error 0.6379(0.6555) Steps 514(508.54) | Grad Norm 15.4835(24.1494) | Total Time 0.00(0.00)\n",
      "Iter 0502 | Time 52.3352(51.4015) | Bit/dim 4.8852(4.9928) | Xent 1.8216(1.8750) | Loss 12.2181(13.1087) | Error 0.6350(0.6549) Steps 478(507.62) | Grad Norm 31.2154(24.3614) | Total Time 0.00(0.00)\n",
      "Iter 0503 | Time 52.4548(51.4331) | Bit/dim 4.8752(4.9893) | Xent 1.8612(1.8746) | Loss 12.1010(13.0785) | Error 0.6575(0.6550) Steps 508(507.64) | Grad Norm 18.9298(24.1984) | Total Time 0.00(0.00)\n",
      "Iter 0504 | Time 52.4111(51.4624) | Bit/dim 4.9003(4.9866) | Xent 1.8605(1.8742) | Loss 12.7091(13.0674) | Error 0.6516(0.6549) Steps 526(508.19) | Grad Norm 33.0193(24.4631) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0084 | Time 21.2499, Epoch Time 353.3704(323.0436), Bit/dim 4.8508(best: 4.8873), Xent 1.7961, Loss 5.7488, Error 0.6243(best: 0.6149)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0505 | Time 58.6989(51.6795) | Bit/dim 4.8660(4.9830) | Xent 1.8151(1.8724) | Loss 15.6083(13.1436) | Error 0.6274(0.6541) Steps 520(508.54) | Grad Norm 14.5802(24.1666) | Total Time 0.00(0.00)\n",
      "Iter 0506 | Time 46.1550(51.5138) | Bit/dim 4.9346(4.9816) | Xent 1.8996(1.8732) | Loss 12.3192(13.1189) | Error 0.6587(0.6542) Steps 484(507.81) | Grad Norm 58.1226(25.1853) | Total Time 0.00(0.00)\n",
      "Iter 0507 | Time 52.8535(51.5540) | Bit/dim 4.8909(4.9789) | Xent 1.8707(1.8731) | Loss 12.4153(13.0978) | Error 0.6609(0.6544) Steps 526(508.35) | Grad Norm 49.8008(25.9237) | Total Time 0.00(0.00)\n",
      "Iter 0508 | Time 51.4322(51.5503) | Bit/dim 4.8528(4.9751) | Xent 1.8166(1.8714) | Loss 12.3752(13.0761) | Error 0.6402(0.6540) Steps 508(508.34) | Grad Norm 23.4464(25.8494) | Total Time 0.00(0.00)\n",
      "Iter 0509 | Time 48.3270(51.4536) | Bit/dim 4.8524(4.9714) | Xent 1.8045(1.8694) | Loss 12.4587(13.0576) | Error 0.6305(0.6533) Steps 496(507.97) | Grad Norm 24.7934(25.8177) | Total Time 0.00(0.00)\n",
      "Iter 0510 | Time 55.1717(51.5652) | Bit/dim 4.8490(4.9677) | Xent 1.8561(1.8690) | Loss 12.4283(13.0387) | Error 0.6515(0.6532) Steps 526(508.51) | Grad Norm 33.2271(26.0400) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0085 | Time 21.6317, Epoch Time 350.2168(323.8588), Bit/dim 4.8454(best: 4.8508), Xent 1.7874, Loss 5.7391, Error 0.6155(best: 0.6149)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0511 | Time 57.2110(51.7345) | Bit/dim 4.8415(4.9639) | Xent 1.8219(1.8676) | Loss 15.6029(13.1156) | Error 0.6430(0.6529) Steps 538(509.40) | Grad Norm 27.5340(26.0848) | Total Time 0.00(0.00)\n",
      "Iter 0512 | Time 51.1989(51.7185) | Bit/dim 4.8512(4.9606) | Xent 1.8105(1.8659) | Loss 12.2239(13.0889) | Error 0.6355(0.6524) Steps 526(509.89) | Grad Norm 29.0640(26.1742) | Total Time 0.00(0.00)\n",
      "Iter 0513 | Time 54.0806(51.7893) | Bit/dim 4.8605(4.9576) | Xent 1.8433(1.8652) | Loss 12.4810(13.0706) | Error 0.6390(0.6520) Steps 490(509.30) | Grad Norm 27.4454(26.2123) | Total Time 0.00(0.00)\n",
      "Iter 0514 | Time 55.0786(51.8880) | Bit/dim 4.8296(4.9537) | Xent 1.7979(1.8632) | Loss 12.2534(13.0461) | Error 0.6209(0.6511) Steps 550(510.52) | Grad Norm 16.6206(25.9246) | Total Time 0.00(0.00)\n",
      "Iter 0515 | Time 51.2979(51.8703) | Bit/dim 4.8196(4.9497) | Xent 1.8512(1.8628) | Loss 12.4608(13.0286) | Error 0.6496(0.6510) Steps 538(511.34) | Grad Norm 24.2700(25.8749) | Total Time 0.00(0.00)\n",
      "Iter 0516 | Time 57.4159(52.0367) | Bit/dim 4.8085(4.9455) | Xent 1.8114(1.8613) | Loss 12.4486(13.0112) | Error 0.6371(0.6506) Steps 472(510.16) | Grad Norm 21.0135(25.7291) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0086 | Time 20.3904, Epoch Time 362.7127(325.0244), Bit/dim 4.8313(best: 4.8454), Xent 1.7853, Loss 5.7240, Error 0.6143(best: 0.6149)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0517 | Time 55.6895(52.1462) | Bit/dim 4.8363(4.9422) | Xent 1.8139(1.8599) | Loss 15.5238(13.0865) | Error 0.6261(0.6499) Steps 520(510.46) | Grad Norm 30.8170(25.8817) | Total Time 0.00(0.00)\n",
      "Iter 0518 | Time 49.5680(52.0689) | Bit/dim 4.8070(4.9381) | Xent 1.7821(1.8575) | Loss 12.0609(13.0558) | Error 0.6193(0.6489) Steps 508(510.38) | Grad Norm 15.8055(25.5795) | Total Time 0.00(0.00)\n",
      "Iter 0519 | Time 55.1189(52.1604) | Bit/dim 4.8001(4.9340) | Xent 1.8020(1.8559) | Loss 12.1896(13.0298) | Error 0.6260(0.6483) Steps 514(510.49) | Grad Norm 14.8771(25.2584) | Total Time 0.00(0.00)\n",
      "Iter 0520 | Time 51.8770(52.1519) | Bit/dim 4.8266(4.9308) | Xent 1.8300(1.8551) | Loss 12.3226(13.0086) | Error 0.6484(0.6483) Steps 532(511.14) | Grad Norm 28.4222(25.3533) | Total Time 0.00(0.00)\n",
      "Iter 0521 | Time 51.8796(52.1437) | Bit/dim 4.8251(4.9276) | Xent 1.7964(1.8533) | Loss 12.4013(12.9904) | Error 0.6255(0.6476) Steps 520(511.40) | Grad Norm 28.8495(25.4582) | Total Time 0.00(0.00)\n",
      "Iter 0522 | Time 52.8553(52.1651) | Bit/dim 4.8020(4.9238) | Xent 1.7836(1.8513) | Loss 12.0217(12.9613) | Error 0.6274(0.6470) Steps 490(510.76) | Grad Norm 8.4938(24.9493) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0087 | Time 21.7170, Epoch Time 354.5809(325.9111), Bit/dim 4.8040(best: 4.8313), Xent 1.7526, Loss 5.6803, Error 0.6126(best: 0.6143)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0523 | Time 51.4668(52.1441) | Bit/dim 4.8066(4.9203) | Xent 1.7822(1.8492) | Loss 15.1421(13.0267) | Error 0.6244(0.6463) Steps 526(511.22) | Grad Norm 24.9522(24.9493) | Total Time 0.00(0.00)\n",
      "Iter 0524 | Time 52.4980(52.1547) | Bit/dim 4.8271(4.9175) | Xent 1.8037(1.8478) | Loss 12.1323(12.9999) | Error 0.6316(0.6459) Steps 490(510.58) | Grad Norm 33.2043(25.1970) | Total Time 0.00(0.00)\n",
      "Iter 0525 | Time 48.5227(52.0458) | Bit/dim 4.7880(4.9136) | Xent 1.7913(1.8461) | Loss 12.1982(12.9758) | Error 0.6250(0.6452) Steps 520(510.86) | Grad Norm 13.8005(24.8551) | Total Time 0.00(0.00)\n",
      "Iter 0526 | Time 54.2199(52.1110) | Bit/dim 4.8604(4.9120) | Xent 1.9273(1.8486) | Loss 12.3948(12.9584) | Error 0.6776(0.6462) Steps 520(511.14) | Grad Norm 70.4372(26.2226) | Total Time 0.00(0.00)\n",
      "Iter 0527 | Time 52.6335(52.1267) | Bit/dim 4.7995(4.9087) | Xent 1.8090(1.8474) | Loss 11.7293(12.9215) | Error 0.6466(0.6462) Steps 526(511.58) | Grad Norm 38.0169(26.5764) | Total Time 0.00(0.00)\n",
      "Iter 0528 | Time 52.7656(52.1458) | Bit/dim 4.8326(4.9064) | Xent 1.8846(1.8485) | Loss 12.4103(12.9062) | Error 0.6721(0.6470) Steps 502(511.30) | Grad Norm 41.9163(27.0366) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0088 | Time 20.6830, Epoch Time 348.5933(326.5916), Bit/dim 4.8251(best: 4.8040), Xent 1.8576, Loss 5.7539, Error 0.6658(best: 0.6126)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0529 | Time 49.5523(52.0680) | Bit/dim 4.8235(4.9039) | Xent 1.8814(1.8495) | Loss 14.9937(12.9688) | Error 0.6698(0.6477) Steps 520(511.56) | Grad Norm 34.5923(27.2633) | Total Time 0.00(0.00)\n",
      "Iter 0530 | Time 53.7029(52.1171) | Bit/dim 4.7958(4.9006) | Xent 1.8129(1.8484) | Loss 12.1398(12.9440) | Error 0.6418(0.6475) Steps 508(511.45) | Grad Norm 35.9470(27.5238) | Total Time 0.00(0.00)\n",
      "Iter 0531 | Time 49.9533(52.0522) | Bit/dim 4.8239(4.8983) | Xent 1.9051(1.8501) | Loss 12.5722(12.9328) | Error 0.6805(0.6485) Steps 550(512.61) | Grad Norm 44.9907(28.0478) | Total Time 0.00(0.00)\n",
      "Iter 0532 | Time 53.8727(52.1068) | Bit/dim 4.7834(4.8949) | Xent 1.8892(1.8512) | Loss 12.3420(12.9151) | Error 0.6704(0.6491) Steps 502(512.29) | Grad Norm 41.4910(28.4511) | Total Time 0.00(0.00)\n",
      "Iter 0533 | Time 48.9476(52.0120) | Bit/dim 4.7763(4.8913) | Xent 1.7944(1.8495) | Loss 12.2547(12.8953) | Error 0.6312(0.6486) Steps 532(512.88) | Grad Norm 10.9712(27.9267) | Total Time 0.00(0.00)\n",
      "Iter 0534 | Time 51.0466(51.9831) | Bit/dim 4.7934(4.8884) | Xent 1.9094(1.8513) | Loss 12.1064(12.8716) | Error 0.6691(0.6492) Steps 502(512.55) | Grad Norm 56.6812(28.7893) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0089 | Time 20.7913, Epoch Time 343.8773(327.1102), Bit/dim 4.7690(best: 4.8040), Xent 1.7408, Loss 5.6394, Error 0.6040(best: 0.6126)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0535 | Time 59.4205(52.2062) | Bit/dim 4.7751(4.8850) | Xent 1.7908(1.8495) | Loss 14.5453(12.9218) | Error 0.6258(0.6485) Steps 502(512.24) | Grad Norm 14.0052(28.3458) | Total Time 0.00(0.00)\n",
      "Iter 0536 | Time 55.4858(52.3046) | Bit/dim 4.8359(4.8835) | Xent 1.9471(1.8525) | Loss 12.4316(12.9071) | Error 0.6853(0.6496) Steps 490(511.57) | Grad Norm 74.8927(29.7422) | Total Time 0.00(0.00)\n",
      "Iter 0537 | Time 48.0047(52.1756) | Bit/dim 4.8346(4.8821) | Xent 2.0011(1.8569) | Loss 12.3763(12.8912) | Error 0.7092(0.6514) Steps 508(511.46) | Grad Norm 70.3134(30.9593) | Total Time 0.00(0.00)\n",
      "Iter 0538 | Time 53.9106(52.2276) | Bit/dim 4.7997(4.8796) | Xent 1.8266(1.8560) | Loss 12.2077(12.8707) | Error 0.6535(0.6515) Steps 532(512.08) | Grad Norm 19.9549(30.6292) | Total Time 0.00(0.00)\n",
      "Iter 0539 | Time 58.3228(52.4105) | Bit/dim 4.7953(4.8771) | Xent 1.9232(1.8580) | Loss 12.2602(12.8524) | Error 0.6879(0.6526) Steps 502(511.78) | Grad Norm 65.8893(31.6870) | Total Time 0.00(0.00)\n",
      "Iter 0540 | Time 58.0341(52.5792) | Bit/dim 4.7838(4.8743) | Xent 1.9596(1.8611) | Loss 12.4140(12.8392) | Error 0.6990(0.6540) Steps 502(511.48) | Grad Norm 55.9035(32.4135) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0090 | Time 22.1818, Epoch Time 371.3781(328.4382), Bit/dim 4.8309(best: 4.7690), Xent 1.9375, Loss 5.7996, Error 0.7006(best: 0.6040)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0541 | Time 59.6013(52.7898) | Bit/dim 4.8391(4.8732) | Xent 1.9674(1.8643) | Loss 15.5750(12.9213) | Error 0.6987(0.6553) Steps 526(511.92) | Grad Norm 42.8035(32.7252) | Total Time 0.00(0.00)\n",
      "Iter 0542 | Time 52.1374(52.7703) | Bit/dim 4.7967(4.8709) | Xent 1.8648(1.8643) | Loss 12.3828(12.9051) | Error 0.6686(0.6557) Steps 532(512.52) | Grad Norm 22.3328(32.4134) | Total Time 0.00(0.00)\n",
      "Iter 0543 | Time 54.9235(52.8349) | Bit/dim 4.8079(4.8690) | Xent 1.8905(1.8651) | Loss 12.2819(12.8864) | Error 0.6718(0.6562) Steps 490(511.85) | Grad Norm 58.6889(33.2017) | Total Time 0.00(0.00)\n",
      "Iter 0544 | Time 61.3968(53.0917) | Bit/dim 4.8397(4.8681) | Xent 1.9054(1.8663) | Loss 12.3890(12.8715) | Error 0.6739(0.6567) Steps 514(511.91) | Grad Norm 40.6483(33.4251) | Total Time 0.00(0.00)\n",
      "Iter 0545 | Time 53.3840(53.1005) | Bit/dim 4.8427(4.8674) | Xent 1.8606(1.8661) | Loss 12.2784(12.8537) | Error 0.6586(0.6568) Steps 550(513.05) | Grad Norm 24.4875(33.1570) | Total Time 0.00(0.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_rl_lars.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 8000 --test_batch_size 5000 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_rl_stdlearnscale_30_lars_clip_trust_0_002_lr_0_001_run1 --seed 1 --conditional True --controlled_tol True --train_mode semisup --lr 0.001 --warmup_iters 1000 --atol 1e-4  --rtol 1e-4 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5 --scale_fac 1.0 --gate cnn2 --scale_std 30.0 --max_grad_norm 20.0 --trust_coefficient 0.002 --clip True\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
