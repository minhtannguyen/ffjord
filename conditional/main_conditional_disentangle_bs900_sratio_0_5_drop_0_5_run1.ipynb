{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0,1,2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "if args.controlled_tol:\n",
      "    import lib.odenvp_conditional_tol as odenvp\n",
      "else:\n",
      "    import lib.odenvp_conditional as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            loss.backward()\n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, condition_ratio=0.5, conditional=True, controlled_tol=False, conv=True, data='mnist', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume=None, rtol=1e-05, save='../experiments_published/cnf_conditional_disentangle_bs900_sratio_0_5_drop_0_5_run1', seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5)\n",
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=784, bias=True)\n",
      "  (project_class): LinearZeros(in_features=392, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 814778\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc.py:286: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "Iter 0010 | Time 7.7607(16.9105) | Bit/dim 17.8044(19.1340) | Xent 2.2847(2.3008) | Loss 18.9468(20.2844) | Error 0.8256(0.8789) Steps 410(410.00) | Grad Norm 128.4063(141.9667) | Total Time 10.00(10.00)\n",
      "Iter 0020 | Time 7.7398(14.4959) | Bit/dim 14.6429(18.3129) | Xent 2.2441(2.2917) | Loss 15.7650(19.4588) | Error 0.8089(0.8733) Steps 410(410.00) | Grad Norm 86.8975(132.4465) | Total Time 10.00(10.00)\n",
      "Iter 0030 | Time 7.5206(12.7014) | Bit/dim 12.0006(16.9470) | Xent 2.1910(2.2710) | Loss 13.0961(18.0825) | Error 0.5667(0.8199) Steps 410(410.00) | Grad Norm 43.8048(113.5045) | Total Time 10.00(10.00)\n",
      "Iter 0040 | Time 7.7312(11.4092) | Bit/dim 10.2547(15.3755) | Xent 2.1103(2.2375) | Loss 11.3099(16.4942) | Error 0.3278(0.7071) Steps 410(410.00) | Grad Norm 21.3557(91.3318) | Total Time 10.00(10.00)\n",
      "Iter 0050 | Time 7.6337(10.4511) | Bit/dim 8.6883(13.7819) | Xent 2.0407(2.1932) | Loss 9.7086(14.8785) | Error 0.2900(0.6014) Steps 410(410.00) | Grad Norm 14.7231(71.6773) | Total Time 10.00(10.00)\n",
      "Iter 0060 | Time 7.6232(9.7057) | Bit/dim 7.3956(12.2564) | Xent 1.9715(2.1426) | Loss 8.3813(13.3277) | Error 0.2889(0.5239) Steps 410(410.00) | Grad Norm 15.0888(56.8265) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0001 | Time 40.1251, Epoch Time 575.0941(575.0941), Bit/dim 6.5042(best: inf), Xent 1.9308, Loss 7.4695, Error 0.2381(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0070 | Time 7.5922(9.1668) | Bit/dim 6.0730(10.7776) | Xent 1.9408(2.0919) | Loss 7.0435(11.8236) | Error 0.3144(0.4642) Steps 410(410.00) | Grad Norm 11.9606(45.4041) | Total Time 10.00(10.00)\n",
      "Iter 0080 | Time 7.7161(8.7988) | Bit/dim 4.8334(9.3482) | Xent 1.9221(2.0473) | Loss 5.7945(10.3718) | Error 0.3067(0.4217) Steps 410(410.00) | Grad Norm 10.4450(36.3724) | Total Time 10.00(10.00)\n",
      "Iter 0090 | Time 8.3163(8.5285) | Bit/dim 3.8789(8.0097) | Xent 1.9500(2.0167) | Loss 4.8539(9.0180) | Error 0.3411(0.3955) Steps 416(410.18) | Grad Norm 8.6074(29.3267) | Total Time 10.00(10.00)\n",
      "Iter 0100 | Time 8.1409(8.4362) | Bit/dim 3.1575(6.8099) | Xent 1.9883(2.0036) | Loss 4.1517(7.8117) | Error 0.3489(0.3815) Steps 422(413.01) | Grad Norm 5.4347(23.3886) | Total Time 10.00(10.00)\n",
      "Iter 0110 | Time 8.2455(8.3768) | Bit/dim 2.7866(5.7934) | Xent 2.0403(2.0077) | Loss 3.8068(6.7973) | Error 0.4022(0.3835) Steps 422(415.37) | Grad Norm 3.6986(18.4028) | Total Time 10.00(10.00)\n",
      "Iter 0120 | Time 8.7264(8.4220) | Bit/dim 2.5574(4.9661) | Xent 2.0692(2.0211) | Loss 3.5919(5.9767) | Error 0.3878(0.3911) Steps 440(420.06) | Grad Norm 2.4308(14.3340) | Total Time 10.00(10.00)\n",
      "Iter 0130 | Time 9.1126(8.5522) | Bit/dim 2.4366(4.3120) | Xent 2.0954(2.0376) | Loss 3.4843(5.3308) | Error 0.4622(0.4036) Steps 440(425.29) | Grad Norm 1.8226(11.1095) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0002 | Time 39.3304, Epoch Time 596.1937(575.7270), Bit/dim 2.3854(best: 6.5042), Xent 2.0843, Loss 3.4275, Error 0.3437(best: 0.2381)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0140 | Time 8.8681(8.6184) | Bit/dim 2.3551(3.8038) | Xent 2.1027(2.0533) | Loss 3.4065(4.8304) | Error 0.4622(0.4154) Steps 440(429.15) | Grad Norm 1.4623(8.6207) | Total Time 10.00(10.00)\n",
      "Iter 0150 | Time 8.6740(8.6624) | Bit/dim 2.2827(3.4100) | Xent 2.0920(2.0638) | Loss 3.3286(4.4419) | Error 0.4522(0.4218) Steps 440(432.00) | Grad Norm 1.2306(6.7053) | Total Time 10.00(10.00)\n",
      "Iter 0160 | Time 8.6319(8.6998) | Bit/dim 2.2621(3.1117) | Xent 2.0726(2.0676) | Loss 3.2984(4.1455) | Error 0.4089(0.4222) Steps 440(434.10) | Grad Norm 1.1329(5.2502) | Total Time 10.00(10.00)\n",
      "Iter 0170 | Time 8.7005(8.7327) | Bit/dim 2.2636(2.8845) | Xent 2.0550(2.0667) | Loss 3.2912(3.9179) | Error 0.4189(0.4223) Steps 440(435.65) | Grad Norm 1.0791(4.1560) | Total Time 10.00(10.00)\n",
      "Iter 0180 | Time 9.2975(8.7648) | Bit/dim 2.2081(2.7108) | Xent 2.0220(2.0595) | Loss 3.2191(3.7406) | Error 0.3756(0.4167) Steps 440(436.79) | Grad Norm 1.0358(3.3387) | Total Time 10.00(10.00)\n",
      "Iter 0190 | Time 8.5028(8.7417) | Bit/dim 2.2065(2.5803) | Xent 1.9975(2.0463) | Loss 3.2053(3.6035) | Error 0.3867(0.4105) Steps 434(436.65) | Grad Norm 1.0117(2.7349) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0003 | Time 39.1387, Epoch Time 631.2091(577.3915), Bit/dim 2.1951(best: 2.3854), Xent 1.9499, Loss 3.1700, Error 0.2884(best: 0.2381)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0200 | Time 8.5840(8.6664) | Bit/dim 2.2087(2.4815) | Xent 1.9610(2.0283) | Loss 3.1892(3.4957) | Error 0.3911(0.4034) Steps 434(435.95) | Grad Norm 1.0599(2.2857) | Total Time 10.00(10.00)\n",
      "Iter 0210 | Time 8.3406(8.5904) | Bit/dim 2.1759(2.4046) | Xent 1.9188(2.0049) | Loss 3.1353(3.4071) | Error 0.3611(0.3946) Steps 428(434.00) | Grad Norm 1.0035(1.9531) | Total Time 10.00(10.00)\n",
      "Iter 0220 | Time 8.0936(8.5379) | Bit/dim 2.1875(2.3486) | Xent 1.8770(1.9750) | Loss 3.1260(3.3361) | Error 0.3456(0.3827) Steps 428(432.43) | Grad Norm 0.9849(1.7052) | Total Time 10.00(10.00)\n",
      "Iter 0230 | Time 8.2872(8.4796) | Bit/dim 2.1803(2.3045) | Xent 1.8142(1.9376) | Loss 3.0874(3.2734) | Error 0.3367(0.3685) Steps 428(431.26) | Grad Norm 1.0310(1.5315) | Total Time 10.00(10.00)\n",
      "Iter 0240 | Time 8.3862(8.4761) | Bit/dim 2.1849(2.2718) | Xent 1.7307(1.8959) | Loss 3.0503(3.2197) | Error 0.3167(0.3584) Steps 428(430.41) | Grad Norm 1.0607(1.4013) | Total Time 10.00(10.00)\n",
      "Iter 0250 | Time 8.1783(8.4646) | Bit/dim 2.1719(2.2460) | Xent 1.6714(1.8464) | Loss 3.0076(3.1692) | Error 0.2978(0.3457) Steps 428(429.77) | Grad Norm 1.0605(1.3104) | Total Time 10.00(10.00)\n",
      "Iter 0260 | Time 8.7001(8.4203) | Bit/dim 2.1776(2.2257) | Xent 1.5897(1.7890) | Loss 2.9725(3.1202) | Error 0.2989(0.3339) Steps 428(429.31) | Grad Norm 1.1051(1.2518) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0004 | Time 38.2025, Epoch Time 607.7800(578.3032), Bit/dim 2.1677(best: 2.1951), Xent 1.5092, Loss 2.9223, Error 0.2230(best: 0.2381)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0270 | Time 8.0959(8.4084) | Bit/dim 2.2025(2.2121) | Xent 1.4714(1.7206) | Loss 2.9382(3.0724) | Error 0.2578(0.3188) Steps 428(428.96) | Grad Norm 1.0835(1.2144) | Total Time 10.00(10.00)\n",
      "Iter 0280 | Time 8.3793(8.3952) | Bit/dim 2.1712(2.2009) | Xent 1.3813(1.6450) | Loss 2.8618(3.0235) | Error 0.2567(0.3057) Steps 428(428.71) | Grad Norm 1.0621(1.1882) | Total Time 10.00(10.00)\n",
      "Iter 0290 | Time 8.5858(8.3827) | Bit/dim 2.1642(2.1922) | Xent 1.3145(1.5665) | Loss 2.8215(2.9754) | Error 0.2678(0.2946) Steps 434(428.88) | Grad Norm 1.2405(1.1782) | Total Time 10.00(10.00)\n",
      "Iter 0300 | Time 8.5671(8.4250) | Bit/dim 2.1729(2.1828) | Xent 1.1926(1.4829) | Loss 2.7693(2.9242) | Error 0.2233(0.2814) Steps 434(430.22) | Grad Norm 1.2465(1.1922) | Total Time 10.00(10.00)\n",
      "Iter 0310 | Time 8.5114(8.4352) | Bit/dim 2.1557(2.1742) | Xent 1.1539(1.4023) | Loss 2.7327(2.8753) | Error 0.2344(0.2715) Steps 434(431.22) | Grad Norm 1.1433(1.1851) | Total Time 10.00(10.00)\n",
      "Iter 0320 | Time 8.6277(8.4572) | Bit/dim 2.1372(2.1651) | Xent 1.0957(1.3295) | Loss 2.6850(2.8299) | Error 0.2400(0.2636) Steps 434(431.95) | Grad Norm 1.1793(1.1597) | Total Time 10.00(10.00)\n",
      "Iter 0330 | Time 8.4383(8.4704) | Bit/dim 2.1195(2.1526) | Xent 1.0306(1.2506) | Loss 2.6348(2.7778) | Error 0.2311(0.2509) Steps 434(432.49) | Grad Norm 0.9239(1.1912) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0005 | Time 39.5130, Epoch Time 611.5264(579.2999), Bit/dim 2.1127(best: 2.1677), Xent 0.9306, Loss 2.5780, Error 0.1705(best: 0.2230)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0340 | Time 8.3646(8.4617) | Bit/dim 2.1040(2.1415) | Xent 0.9508(1.1811) | Loss 2.5794(2.7321) | Error 0.2344(0.2438) Steps 440(434.04) | Grad Norm 1.3091(1.2150) | Total Time 10.00(10.00)\n",
      "Iter 0350 | Time 8.5871(8.5108) | Bit/dim 2.0695(2.1261) | Xent 0.9412(1.1204) | Loss 2.5401(2.6863) | Error 0.2167(0.2355) Steps 440(435.60) | Grad Norm 0.9576(1.2450) | Total Time 10.00(10.00)\n",
      "Iter 0360 | Time 8.3042(8.5132) | Bit/dim 2.0681(2.1117) | Xent 0.8675(1.0593) | Loss 2.5019(2.6413) | Error 0.1889(0.2281) Steps 440(436.76) | Grad Norm 1.0756(1.2214) | Total Time 10.00(10.00)\n",
      "Iter 0370 | Time 8.9416(8.6758) | Bit/dim 2.0414(2.0957) | Xent 0.8643(1.0099) | Loss 2.4736(2.6006) | Error 0.2067(0.2236) Steps 458(441.36) | Grad Norm 1.8511(1.2360) | Total Time 10.00(10.00)\n",
      "Iter 0380 | Time 8.9912(8.7894) | Bit/dim 2.0404(2.0810) | Xent 0.7899(0.9601) | Loss 2.4354(2.5610) | Error 0.1922(0.2170) Steps 458(445.73) | Grad Norm 1.1806(1.3022) | Total Time 10.00(10.00)\n",
      "Iter 0390 | Time 9.3083(8.9485) | Bit/dim 2.0199(2.0663) | Xent 0.7951(0.9184) | Loss 2.4174(2.5255) | Error 0.2189(0.2128) Steps 464(449.64) | Grad Norm 2.0304(1.3984) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0006 | Time 40.2934, Epoch Time 644.7628(581.2637), Bit/dim 2.0052(best: 2.1127), Xent 0.6777, Loss 2.3440, Error 0.1478(best: 0.1705)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0400 | Time 9.5635(9.0914) | Bit/dim 2.0133(2.0523) | Xent 0.7698(0.8810) | Loss 2.3982(2.4928) | Error 0.1889(0.2093) Steps 464(453.41) | Grad Norm 1.4570(1.4270) | Total Time 10.00(10.00)\n",
      "Iter 0410 | Time 9.2413(9.1922) | Bit/dim 2.0057(2.0377) | Xent 0.6861(0.8472) | Loss 2.3487(2.4613) | Error 0.1800(0.2059) Steps 464(456.19) | Grad Norm 2.1548(1.5457) | Total Time 10.00(10.00)\n",
      "Iter 0420 | Time 9.4002(9.2557) | Bit/dim 1.9652(2.0242) | Xent 0.7517(0.8183) | Loss 2.3410(2.4333) | Error 0.2133(0.2033) Steps 464(458.24) | Grad Norm 2.2103(1.7491) | Total Time 10.00(10.00)\n",
      "Iter 0430 | Time 9.3222(9.2731) | Bit/dim 1.9603(2.0124) | Xent 0.7059(0.7942) | Loss 2.3132(2.4095) | Error 0.1878(0.2012) Steps 464(459.75) | Grad Norm 1.7059(1.7873) | Total Time 10.00(10.00)\n",
      "Iter 0440 | Time 9.4904(9.3069) | Bit/dim 1.9714(1.9988) | Xent 0.6948(0.7685) | Loss 2.3188(2.3830) | Error 0.1822(0.1981) Steps 470(461.72) | Grad Norm 6.6859(2.2204) | Total Time 10.00(10.00)\n",
      "Iter 0450 | Time 9.3811(9.3619) | Bit/dim 1.9393(1.9848) | Xent 0.6952(0.7460) | Loss 2.2869(2.3578) | Error 0.1744(0.1934) Steps 482(463.68) | Grad Norm 4.7396(2.8470) | Total Time 10.00(10.00)\n",
      "Iter 0460 | Time 9.5525(9.4006) | Bit/dim 1.9289(1.9704) | Xent 0.6974(0.7238) | Loss 2.2776(2.3323) | Error 0.1933(0.1893) Steps 476(466.03) | Grad Norm 2.2078(2.8452) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0007 | Time 42.2788, Epoch Time 680.8021(584.2499), Bit/dim 1.9158(best: 2.0052), Xent 0.5371, Loss 2.1843, Error 0.1281(best: 0.1478)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0470 | Time 9.4733(9.4469) | Bit/dim 1.8931(1.9537) | Xent 0.6254(0.7071) | Loss 2.2058(2.3072) | Error 0.1789(0.1879) Steps 476(468.64) | Grad Norm 2.7148(2.7617) | Total Time 10.00(10.00)\n",
      "Iter 0480 | Time 9.5100(9.4869) | Bit/dim 1.8799(1.9389) | Xent 0.6396(0.6915) | Loss 2.1997(2.2846) | Error 0.1700(0.1866) Steps 476(470.74) | Grad Norm 3.3007(2.8628) | Total Time 10.00(10.00)\n",
      "Iter 0490 | Time 9.3211(9.5145) | Bit/dim 1.8470(1.9189) | Xent 0.6571(0.6763) | Loss 2.1756(2.2570) | Error 0.1778(0.1837) Steps 476(472.57) | Grad Norm 2.6238(2.9571) | Total Time 10.00(10.00)\n",
      "Iter 0500 | Time 9.6718(9.5058) | Bit/dim 1.8447(1.8968) | Xent 0.6147(0.6637) | Loss 2.1521(2.2286) | Error 0.1689(0.1828) Steps 482(473.37) | Grad Norm 5.7316(3.4785) | Total Time 10.00(10.00)\n",
      "Iter 0510 | Time 9.8607(9.6046) | Bit/dim 1.7745(1.8693) | Xent 0.6124(0.6533) | Loss 2.0807(2.1960) | Error 0.1722(0.1821) Steps 482(475.05) | Grad Norm 4.0806(3.8939) | Total Time 10.00(10.00)\n",
      "Iter 0520 | Time 10.0983(9.6701) | Bit/dim 1.7185(1.8416) | Xent 0.5786(0.6409) | Loss 2.0078(2.1620) | Error 0.1600(0.1802) Steps 488(477.21) | Grad Norm 6.5949(4.3447) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0008 | Time 42.7699, Epoch Time 698.0493(587.6639), Bit/dim 1.7254(best: 1.9158), Xent 0.4516, Loss 1.9512, Error 0.1119(best: 0.1281)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0530 | Time 10.2152(9.7622) | Bit/dim 1.7291(1.8138) | Xent 0.5702(0.6235) | Loss 2.0142(2.1255) | Error 0.1578(0.1757) Steps 488(479.44) | Grad Norm 4.7454(4.5697) | Total Time 10.00(10.00)\n",
      "Iter 0540 | Time 10.4753(9.8640) | Bit/dim 1.6904(1.7840) | Xent 0.5908(0.6109) | Loss 1.9858(2.0894) | Error 0.1689(0.1741) Steps 500(482.38) | Grad Norm 5.6536(4.6893) | Total Time 10.00(10.00)\n",
      "Iter 0550 | Time 10.0529(9.9337) | Bit/dim 1.6613(1.7563) | Xent 0.5674(0.6008) | Loss 1.9450(2.0567) | Error 0.1633(0.1731) Steps 488(484.73) | Grad Norm 2.9185(4.4935) | Total Time 10.00(10.00)\n",
      "Iter 0560 | Time 9.7327(9.9956) | Bit/dim 1.6475(1.7283) | Xent 0.5620(0.5954) | Loss 1.9286(2.0260) | Error 0.1589(0.1731) Steps 488(487.12) | Grad Norm 5.9889(4.8376) | Total Time 10.00(10.00)\n",
      "Iter 0570 | Time 10.2734(10.0442) | Bit/dim 1.6128(1.7043) | Xent 0.6066(0.5824) | Loss 1.9162(1.9955) | Error 0.1833(0.1699) Steps 500(488.93) | Grad Norm 4.0636(5.2908) | Total Time 10.00(10.00)\n",
      "Iter 0580 | Time 10.1069(10.0879) | Bit/dim 1.6300(1.6837) | Xent 0.5157(0.5683) | Loss 1.8879(1.9678) | Error 0.1522(0.1673) Steps 488(489.92) | Grad Norm 8.3901(6.0872) | Total Time 10.00(10.00)\n",
      "Iter 0590 | Time 10.5891(10.1345) | Bit/dim 1.6024(1.6641) | Xent 0.5095(0.5622) | Loss 1.8571(1.9452) | Error 0.1511(0.1671) Steps 494(491.50) | Grad Norm 6.4140(6.1799) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0009 | Time 45.4261, Epoch Time 732.3926(592.0057), Bit/dim 1.5888(best: 1.7254), Xent 0.3969, Loss 1.7872, Error 0.1034(best: 0.1119)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0600 | Time 10.4618(10.1843) | Bit/dim 1.6082(1.6439) | Xent 0.4792(0.5562) | Loss 1.8478(1.9220) | Error 0.1522(0.1652) Steps 500(493.15) | Grad Norm 3.0083(6.2195) | Total Time 10.00(10.00)\n",
      "Iter 0610 | Time 10.4705(10.2476) | Bit/dim 1.5494(1.6260) | Xent 0.5402(0.5485) | Loss 1.8195(1.9003) | Error 0.1733(0.1643) Steps 506(495.80) | Grad Norm 3.7465(6.6134) | Total Time 10.00(10.00)\n",
      "Iter 0620 | Time 10.6662(10.3524) | Bit/dim 1.5419(1.6095) | Xent 0.5279(0.5406) | Loss 1.8059(1.8798) | Error 0.1556(0.1619) Steps 512(499.15) | Grad Norm 7.4987(6.3057) | Total Time 10.00(10.00)\n",
      "Iter 0630 | Time 10.4638(10.4263) | Bit/dim 1.5607(1.5951) | Xent 0.5534(0.5361) | Loss 1.8374(1.8632) | Error 0.1633(0.1618) Steps 512(501.93) | Grad Norm 8.1919(6.6453) | Total Time 10.00(10.00)\n",
      "Iter 0640 | Time 10.4448(10.4570) | Bit/dim 1.5252(1.5822) | Xent 0.4648(0.5270) | Loss 1.7576(1.8457) | Error 0.1322(0.1598) Steps 512(503.67) | Grad Norm 11.7329(7.0392) | Total Time 10.00(10.00)\n",
      "Iter 0650 | Time 10.3350(10.4832) | Bit/dim 1.5296(1.5690) | Xent 0.5037(0.5207) | Loss 1.7815(1.8293) | Error 0.1567(0.1585) Steps 518(506.03) | Grad Norm 7.9097(7.0734) | Total Time 10.00(10.00)\n",
      "Iter 0660 | Time 10.7125(10.5252) | Bit/dim 1.5408(1.5589) | Xent 0.5861(0.5156) | Loss 1.8339(1.8167) | Error 0.1933(0.1575) Steps 512(507.15) | Grad Norm 15.5525(7.5402) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0010 | Time 46.1413, Epoch Time 758.1015(596.9886), Bit/dim 1.5356(best: 1.5888), Xent 0.3442, Loss 1.7078, Error 0.0961(best: 0.1034)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0670 | Time 10.3967(10.5400) | Bit/dim 1.5186(1.5499) | Xent 0.4968(0.5091) | Loss 1.7670(1.8045) | Error 0.1511(0.1548) Steps 506(508.45) | Grad Norm 7.0335(8.1003) | Total Time 10.00(10.00)\n",
      "Iter 0680 | Time 10.6929(10.5713) | Bit/dim 1.4903(1.5365) | Xent 0.5008(0.5073) | Loss 1.7407(1.7902) | Error 0.1567(0.1548) Steps 512(509.35) | Grad Norm 3.7130(7.1232) | Total Time 10.00(10.00)\n",
      "Iter 0690 | Time 10.4294(10.5620) | Bit/dim 1.5063(1.5255) | Xent 0.4292(0.4963) | Loss 1.7209(1.7736) | Error 0.1322(0.1515) Steps 512(510.49) | Grad Norm 6.1855(6.2372) | Total Time 10.00(10.00)\n",
      "Iter 0700 | Time 10.9863(10.6013) | Bit/dim 1.4968(1.5283) | Xent 0.5120(0.4990) | Loss 1.7528(1.7778) | Error 0.1644(0.1528) Steps 512(511.09) | Grad Norm 6.8936(9.0561) | Total Time 10.00(10.00)\n",
      "Iter 0710 | Time 10.6877(10.6318) | Bit/dim 1.4917(1.5212) | Xent 0.5192(0.4909) | Loss 1.7513(1.7666) | Error 0.1589(0.1506) Steps 518(512.14) | Grad Norm 5.9417(8.8703) | Total Time 10.00(10.00)\n",
      "Iter 0720 | Time 10.4574(10.6119) | Bit/dim 1.4570(1.5108) | Xent 0.4603(0.4854) | Loss 1.6871(1.7535) | Error 0.1411(0.1494) Steps 518(512.87) | Grad Norm 5.2914(8.5654) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0011 | Time 47.2471, Epoch Time 764.7458(602.0213), Bit/dim 1.4721(best: 1.5356), Xent 0.3273, Loss 1.6358, Error 0.0906(best: 0.0961)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0730 | Time 10.6244(10.6292) | Bit/dim 1.4790(1.5024) | Xent 0.4312(0.4838) | Loss 1.6945(1.7443) | Error 0.1311(0.1491) Steps 518(513.56) | Grad Norm 11.8874(8.8063) | Total Time 10.00(10.00)\n",
      "Iter 0740 | Time 11.2661(10.6524) | Bit/dim 1.4532(1.4936) | Xent 0.4406(0.4756) | Loss 1.6735(1.7315) | Error 0.1422(0.1481) Steps 518(514.72) | Grad Norm 10.5013(8.7425) | Total Time 10.00(10.00)\n",
      "Iter 0750 | Time 10.9833(10.6789) | Bit/dim 1.4666(1.4871) | Xent 0.4971(0.4766) | Loss 1.7151(1.7254) | Error 0.1433(0.1486) Steps 518(515.43) | Grad Norm 6.1042(9.2022) | Total Time 10.00(10.00)\n",
      "Iter 0760 | Time 10.4297(10.6576) | Bit/dim 1.4647(1.4804) | Xent 0.4756(0.4695) | Loss 1.7025(1.7152) | Error 0.1567(0.1475) Steps 518(516.09) | Grad Norm 12.1656(9.2688) | Total Time 10.00(10.00)\n",
      "Iter 0770 | Time 10.4740(10.6493) | Bit/dim 1.4425(1.4745) | Xent 0.4744(0.4636) | Loss 1.6797(1.7063) | Error 0.1511(0.1463) Steps 518(516.29) | Grad Norm 4.2587(9.4817) | Total Time 10.00(10.00)\n",
      "Iter 0780 | Time 10.8261(10.6629) | Bit/dim 1.4253(1.4687) | Xent 0.5087(0.4638) | Loss 1.6796(1.7006) | Error 0.1567(0.1460) Steps 524(516.54) | Grad Norm 5.1921(9.7740) | Total Time 10.00(10.00)\n",
      "Iter 0790 | Time 10.5269(10.6866) | Bit/dim 1.4783(1.4713) | Xent 0.4002(0.4632) | Loss 1.6784(1.7029) | Error 0.1311(0.1477) Steps 512(516.29) | Grad Norm 11.4165(11.4068) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0012 | Time 47.9416, Epoch Time 768.1553(607.0053), Bit/dim 1.4913(best: 1.4721), Xent 0.2803, Loss 1.6315, Error 0.0844(best: 0.0906)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0800 | Time 10.6917(10.6619) | Bit/dim 1.4344(1.4675) | Xent 0.3895(0.4619) | Loss 1.6292(1.6984) | Error 0.1211(0.1461) Steps 518(515.63) | Grad Norm 6.9994(11.5268) | Total Time 10.00(10.00)\n",
      "Iter 0810 | Time 10.4536(10.6738) | Bit/dim 1.4407(1.4596) | Xent 0.3995(0.4540) | Loss 1.6404(1.6866) | Error 0.1311(0.1436) Steps 518(516.89) | Grad Norm 8.8067(10.1613) | Total Time 10.00(10.00)\n",
      "Iter 0820 | Time 10.6377(10.6845) | Bit/dim 1.4455(1.4540) | Xent 0.4192(0.4527) | Loss 1.6550(1.6803) | Error 0.1300(0.1428) Steps 524(518.13) | Grad Norm 11.9956(10.4798) | Total Time 10.00(10.00)\n",
      "Iter 0830 | Time 11.0439(10.6861) | Bit/dim 1.4032(1.4492) | Xent 0.4229(0.4474) | Loss 1.6146(1.6729) | Error 0.1311(0.1414) Steps 524(518.73) | Grad Norm 12.0125(10.7097) | Total Time 10.00(10.00)\n",
      "Iter 0840 | Time 10.8099(10.6952) | Bit/dim 1.3976(1.4412) | Xent 0.4209(0.4458) | Loss 1.6081(1.6641) | Error 0.1444(0.1401) Steps 518(517.91) | Grad Norm 3.8752(9.9717) | Total Time 10.00(10.00)\n",
      "Iter 0850 | Time 10.9277(10.6721) | Bit/dim 1.4276(1.4357) | Xent 0.4514(0.4437) | Loss 1.6533(1.6575) | Error 0.1433(0.1391) Steps 512(516.57) | Grad Norm 14.7309(9.9199) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0013 | Time 47.0413, Epoch Time 766.3976(611.7871), Bit/dim 1.4312(best: 1.4721), Xent 0.2636, Loss 1.5630, Error 0.0783(best: 0.0844)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0860 | Time 10.6515(10.6779) | Bit/dim 1.4129(1.4302) | Xent 0.4543(0.4439) | Loss 1.6401(1.6522) | Error 0.1389(0.1396) Steps 524(517.81) | Grad Norm 13.4475(9.3349) | Total Time 10.00(10.00)\n",
      "Iter 0870 | Time 10.7418(10.7691) | Bit/dim 1.4580(1.4336) | Xent 0.3610(0.4453) | Loss 1.6385(1.6563) | Error 0.1111(0.1402) Steps 524(519.56) | Grad Norm 13.3285(12.3496) | Total Time 10.00(10.00)\n",
      "Iter 0880 | Time 11.0286(10.7661) | Bit/dim 1.4129(1.4331) | Xent 0.4184(0.4390) | Loss 1.6221(1.6526) | Error 0.1411(0.1389) Steps 524(520.40) | Grad Norm 4.5734(12.2369) | Total Time 10.00(10.00)\n",
      "Iter 0890 | Time 10.4425(10.7599) | Bit/dim 1.3991(1.4255) | Xent 0.4832(0.4379) | Loss 1.6408(1.6445) | Error 0.1511(0.1377) Steps 512(521.16) | Grad Norm 12.3061(10.9794) | Total Time 10.00(10.00)\n",
      "Iter 0900 | Time 10.9063(10.8092) | Bit/dim 1.3895(1.4213) | Xent 0.3895(0.4373) | Loss 1.5842(1.6399) | Error 0.1233(0.1373) Steps 524(522.96) | Grad Norm 7.5961(11.1243) | Total Time 10.00(10.00)\n",
      "Iter 0910 | Time 10.9221(10.8254) | Bit/dim 1.4138(1.4170) | Xent 0.4483(0.4328) | Loss 1.6380(1.6334) | Error 0.1500(0.1359) Steps 524(524.02) | Grad Norm 13.1742(11.2335) | Total Time 10.00(10.00)\n",
      "Iter 0920 | Time 10.6267(10.8230) | Bit/dim 1.4369(1.4127) | Xent 0.3996(0.4292) | Loss 1.6367(1.6273) | Error 0.1256(0.1349) Steps 512(523.36) | Grad Norm 18.4861(11.2416) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0014 | Time 46.5776, Epoch Time 778.5005(616.7885), Bit/dim 1.3898(best: 1.4312), Xent 0.2513, Loss 1.5154, Error 0.0740(best: 0.0783)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0930 | Time 10.7993(10.8551) | Bit/dim 1.3821(1.4082) | Xent 0.4441(0.4252) | Loss 1.6041(1.6209) | Error 0.1500(0.1333) Steps 524(523.47) | Grad Norm 8.3865(11.4594) | Total Time 10.00(10.00)\n",
      "Iter 0940 | Time 10.7987(10.8944) | Bit/dim 1.3893(1.4061) | Xent 0.3917(0.4230) | Loss 1.5852(1.6175) | Error 0.1222(0.1332) Steps 524(523.82) | Grad Norm 5.5175(11.7086) | Total Time 10.00(10.00)\n",
      "Iter 0950 | Time 10.6345(10.9458) | Bit/dim 1.3762(1.4080) | Xent 0.4077(0.4220) | Loss 1.5800(1.6190) | Error 0.1278(0.1319) Steps 518(524.03) | Grad Norm 3.0058(12.9338) | Total Time 10.00(10.00)\n",
      "Iter 0960 | Time 11.2495(10.9806) | Bit/dim 1.4273(1.4128) | Xent 0.4122(0.4190) | Loss 1.6334(1.6223) | Error 0.1367(0.1305) Steps 542(525.68) | Grad Norm 18.1045(14.5107) | Total Time 10.00(10.00)\n",
      "Iter 0970 | Time 10.9376(10.9272) | Bit/dim 1.3539(1.4061) | Xent 0.4118(0.4128) | Loss 1.5599(1.6125) | Error 0.1344(0.1293) Steps 530(525.48) | Grad Norm 6.7034(12.7787) | Total Time 10.00(10.00)\n",
      "Iter 0980 | Time 10.7925(10.9385) | Bit/dim 1.4019(1.4007) | Xent 0.4278(0.4052) | Loss 1.6158(1.6033) | Error 0.1233(0.1272) Steps 530(526.28) | Grad Norm 16.9457(11.5678) | Total Time 10.00(10.00)\n",
      "Iter 0990 | Time 11.1531(11.0030) | Bit/dim 1.3761(1.3931) | Xent 0.3610(0.4015) | Loss 1.5566(1.5939) | Error 0.1244(0.1263) Steps 524(528.73) | Grad Norm 5.6389(10.7823) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0015 | Time 47.2558, Epoch Time 788.9738(621.9541), Bit/dim 1.3560(best: 1.3898), Xent 0.2516, Loss 1.4819, Error 0.0735(best: 0.0740)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1000 | Time 10.6464(11.0320) | Bit/dim 1.4090(1.3893) | Xent 0.3657(0.3994) | Loss 1.5918(1.5890) | Error 0.1222(0.1264) Steps 518(529.49) | Grad Norm 20.2216(11.2197) | Total Time 10.00(10.00)\n",
      "Iter 1010 | Time 11.8031(11.0777) | Bit/dim 1.3884(1.3878) | Xent 0.3568(0.3945) | Loss 1.5668(1.5850) | Error 0.1156(0.1242) Steps 542(530.73) | Grad Norm 18.4483(11.7677) | Total Time 10.00(10.00)\n",
      "Iter 1020 | Time 11.5104(11.0677) | Bit/dim 1.3509(1.3837) | Xent 0.4084(0.3936) | Loss 1.5551(1.5804) | Error 0.1344(0.1244) Steps 542(530.29) | Grad Norm 7.3332(11.6073) | Total Time 10.00(10.00)\n",
      "Iter 1030 | Time 10.7798(11.0489) | Bit/dim 1.3648(1.3776) | Xent 0.3821(0.3861) | Loss 1.5559(1.5706) | Error 0.1122(0.1215) Steps 524(530.73) | Grad Norm 8.2039(9.9622) | Total Time 10.00(10.00)\n",
      "Iter 1040 | Time 10.8942(11.1028) | Bit/dim 1.3548(1.3743) | Xent 0.3798(0.3832) | Loss 1.5447(1.5659) | Error 0.1211(0.1214) Steps 524(531.40) | Grad Norm 5.5806(10.8538) | Total Time 10.00(10.00)\n",
      "Iter 1050 | Time 10.6675(11.1206) | Bit/dim 1.3804(1.3750) | Xent 0.3734(0.3837) | Loss 1.5671(1.5668) | Error 0.1289(0.1223) Steps 518(531.08) | Grad Norm 12.8907(11.4049) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0016 | Time 46.5436, Epoch Time 796.5819(627.1929), Bit/dim 1.3570(best: 1.3560), Xent 0.2232, Loss 1.4686, Error 0.0666(best: 0.0735)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1060 | Time 11.5737(11.1292) | Bit/dim 1.3649(1.3700) | Xent 0.3811(0.3845) | Loss 1.5555(1.5622) | Error 0.1222(0.1219) Steps 518(530.57) | Grad Norm 5.8713(10.4265) | Total Time 10.00(10.00)\n",
      "Iter 1070 | Time 11.5370(11.1411) | Bit/dim 1.3244(1.3672) | Xent 0.3879(0.3802) | Loss 1.5183(1.5573) | Error 0.1033(0.1206) Steps 542(531.35) | Grad Norm 11.7232(10.3513) | Total Time 10.00(10.00)\n",
      "Iter 1080 | Time 11.6686(11.1787) | Bit/dim 1.3644(1.3637) | Xent 0.3920(0.3820) | Loss 1.5604(1.5547) | Error 0.1278(0.1199) Steps 554(533.00) | Grad Norm 23.1758(10.0628) | Total Time 10.00(10.00)\n",
      "Iter 1090 | Time 10.7587(11.1813) | Bit/dim 1.3494(1.3619) | Xent 0.3622(0.3773) | Loss 1.5305(1.5505) | Error 0.1000(0.1190) Steps 524(532.99) | Grad Norm 8.1487(10.4633) | Total Time 10.00(10.00)\n",
      "Iter 1100 | Time 11.8702(11.2093) | Bit/dim 1.4393(1.3761) | Xent 0.3879(0.3765) | Loss 1.6333(1.5643) | Error 0.1111(0.1195) Steps 554(533.63) | Grad Norm 32.7480(13.6817) | Total Time 10.00(10.00)\n",
      "Iter 1110 | Time 10.5703(11.2083) | Bit/dim 1.3656(1.3828) | Xent 0.3659(0.3880) | Loss 1.5485(1.5768) | Error 0.1200(0.1239) Steps 524(532.76) | Grad Norm 8.9204(13.8946) | Total Time 10.00(10.00)\n",
      "Iter 1120 | Time 11.4959(11.2260) | Bit/dim 1.3624(1.3787) | Xent 0.3620(0.3844) | Loss 1.5434(1.5709) | Error 0.1167(0.1225) Steps 554(535.66) | Grad Norm 4.0861(12.1245) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0017 | Time 47.5669, Epoch Time 803.5599(632.4839), Bit/dim 1.3510(best: 1.3560), Xent 0.2215, Loss 1.4617, Error 0.0654(best: 0.0666)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1130 | Time 11.3424(11.2565) | Bit/dim 1.3162(1.3696) | Xent 0.4077(0.3828) | Loss 1.5201(1.5610) | Error 0.1256(0.1224) Steps 548(537.03) | Grad Norm 4.5827(10.3427) | Total Time 10.00(10.00)\n",
      "Iter 1140 | Time 11.3770(11.3542) | Bit/dim 1.3453(1.3629) | Xent 0.3417(0.3784) | Loss 1.5161(1.5521) | Error 0.1056(0.1206) Steps 554(540.31) | Grad Norm 3.5661(8.8295) | Total Time 10.00(10.00)\n",
      "Iter 1150 | Time 11.6081(11.3879) | Bit/dim 1.3389(1.3572) | Xent 0.3355(0.3693) | Loss 1.5066(1.5419) | Error 0.1067(0.1180) Steps 548(539.93) | Grad Norm 12.0901(8.7198) | Total Time 10.00(10.00)\n",
      "Iter 1160 | Time 11.6616(11.3539) | Bit/dim 1.3309(1.3561) | Xent 0.4086(0.3640) | Loss 1.5352(1.5381) | Error 0.1389(0.1171) Steps 554(539.26) | Grad Norm 23.2549(9.8119) | Total Time 10.00(10.00)\n",
      "Iter 1170 | Time 11.7494(11.3043) | Bit/dim 1.3984(1.3708) | Xent 0.3334(0.3622) | Loss 1.5651(1.5519) | Error 0.1133(0.1163) Steps 554(538.65) | Grad Norm 22.9499(13.1134) | Total Time 10.00(10.00)\n",
      "Iter 1180 | Time 11.3883(11.2440) | Bit/dim 1.3444(1.3710) | Xent 0.3443(0.3571) | Loss 1.5166(1.5495) | Error 0.1022(0.1134) Steps 548(536.86) | Grad Norm 8.9376(13.3749) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0018 | Time 46.4610, Epoch Time 808.6903(637.7701), Bit/dim 1.3355(best: 1.3510), Xent 0.2072, Loss 1.4391, Error 0.0621(best: 0.0654)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1190 | Time 11.3143(11.2401) | Bit/dim 1.3692(1.3652) | Xent 0.3859(0.3531) | Loss 1.5622(1.5418) | Error 0.1167(0.1114) Steps 524(535.69) | Grad Norm 10.5387(12.3955) | Total Time 10.00(10.00)\n",
      "Iter 1200 | Time 11.2566(11.2491) | Bit/dim 1.3467(1.3580) | Xent 0.2801(0.3414) | Loss 1.4868(1.5287) | Error 0.0944(0.1087) Steps 518(536.02) | Grad Norm 6.7677(10.9675) | Total Time 10.00(10.00)\n",
      "Iter 1210 | Time 11.8432(11.3467) | Bit/dim 1.3478(1.3518) | Xent 0.2946(0.3375) | Loss 1.4951(1.5205) | Error 0.0833(0.1069) Steps 548(538.25) | Grad Norm 8.5594(9.4649) | Total Time 10.00(10.00)\n",
      "Iter 1220 | Time 11.3327(11.3787) | Bit/dim 1.3301(1.3472) | Xent 0.3942(0.3365) | Loss 1.5272(1.5155) | Error 0.1189(0.1061) Steps 548(538.93) | Grad Norm 14.4611(9.7475) | Total Time 10.00(10.00)\n",
      "Iter 1230 | Time 10.4958(11.3588) | Bit/dim 1.3850(1.3485) | Xent 0.3659(0.3389) | Loss 1.5680(1.5179) | Error 0.1133(0.1066) Steps 512(538.60) | Grad Norm 22.2438(11.0468) | Total Time 10.00(10.00)\n",
      "Iter 1240 | Time 11.2451(11.3170) | Bit/dim 1.3469(1.3597) | Xent 0.3258(0.3366) | Loss 1.5098(1.5280) | Error 0.1133(0.1062) Steps 542(537.36) | Grad Norm 12.6892(13.0481) | Total Time 10.00(10.00)\n",
      "Iter 1250 | Time 11.5947(11.2918) | Bit/dim 1.3400(1.3559) | Xent 0.2913(0.3292) | Loss 1.4857(1.5205) | Error 0.0867(0.1041) Steps 554(536.86) | Grad Norm 5.6842(12.1185) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0019 | Time 48.5296, Epoch Time 812.7308(643.0189), Bit/dim 1.3204(best: 1.3355), Xent 0.1892, Loss 1.4151, Error 0.0560(best: 0.0621)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1260 | Time 11.8097(11.3688) | Bit/dim 1.3002(1.3488) | Xent 0.2861(0.3229) | Loss 1.4433(1.5102) | Error 0.0911(0.1019) Steps 542(538.68) | Grad Norm 4.1336(10.3856) | Total Time 10.00(10.00)\n",
      "Iter 1270 | Time 11.2070(11.3433) | Bit/dim 1.3213(1.3420) | Xent 0.3397(0.3192) | Loss 1.4911(1.5016) | Error 0.1022(0.0999) Steps 536(538.12) | Grad Norm 4.6779(8.8519) | Total Time 10.00(10.00)\n",
      "Iter 1280 | Time 11.5100(11.3973) | Bit/dim 1.3341(1.3363) | Xent 0.3148(0.3139) | Loss 1.4915(1.4932) | Error 0.1011(0.0992) Steps 542(539.60) | Grad Norm 7.1064(7.8759) | Total Time 10.00(10.00)\n",
      "Iter 1290 | Time 11.6648(11.4324) | Bit/dim 1.3371(1.3324) | Xent 0.3351(0.3168) | Loss 1.5046(1.4907) | Error 0.1178(0.1006) Steps 542(541.17) | Grad Norm 12.4083(7.7302) | Total Time 10.00(10.00)\n",
      "Iter 1300 | Time 11.4295(11.4234) | Bit/dim 1.3438(1.3317) | Xent 0.3462(0.3127) | Loss 1.5169(1.4880) | Error 0.1122(0.0995) Steps 542(541.28) | Grad Norm 17.1004(8.8017) | Total Time 10.00(10.00)\n",
      "Iter 1310 | Time 11.7532(11.4672) | Bit/dim 1.3207(1.3319) | Xent 0.2690(0.3100) | Loss 1.4552(1.4869) | Error 0.0844(0.0984) Steps 548(541.90) | Grad Norm 5.0173(9.7611) | Total Time 10.00(10.00)\n",
      "Iter 1320 | Time 11.3518(11.4578) | Bit/dim 1.3266(1.3303) | Xent 0.3419(0.3095) | Loss 1.4975(1.4850) | Error 0.1089(0.0978) Steps 542(541.60) | Grad Norm 6.6338(10.1321) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0020 | Time 48.9658, Epoch Time 821.5429(648.3747), Bit/dim 1.3247(best: 1.3204), Xent 0.1811, Loss 1.4152, Error 0.0557(best: 0.0560)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1330 | Time 11.5778(11.4951) | Bit/dim 1.3372(1.3265) | Xent 0.2374(0.3012) | Loss 1.4559(1.4771) | Error 0.0800(0.0954) Steps 554(543.79) | Grad Norm 6.1684(9.0606) | Total Time 10.00(10.00)\n",
      "Iter 1340 | Time 11.7266(11.5463) | Bit/dim 1.3221(1.3243) | Xent 0.2746(0.2964) | Loss 1.4594(1.4725) | Error 0.0867(0.0930) Steps 536(544.55) | Grad Norm 7.8102(8.2712) | Total Time 10.00(10.00)\n",
      "Iter 1350 | Time 11.5850(11.5557) | Bit/dim 1.2961(1.3227) | Xent 0.2728(0.2970) | Loss 1.4326(1.4713) | Error 0.0833(0.0929) Steps 548(544.56) | Grad Norm 6.6659(9.5882) | Total Time 10.00(10.00)\n",
      "Iter 1360 | Time 11.7175(11.5586) | Bit/dim 1.3033(1.3230) | Xent 0.2980(0.2908) | Loss 1.4522(1.4684) | Error 0.0889(0.0907) Steps 542(543.35) | Grad Norm 9.2366(10.3344) | Total Time 10.00(10.00)\n",
      "Iter 1370 | Time 11.7568(11.6008) | Bit/dim 1.3158(1.3219) | Xent 0.2328(0.2879) | Loss 1.4322(1.4659) | Error 0.0644(0.0902) Steps 554(544.48) | Grad Norm 8.6395(10.2328) | Total Time 10.00(10.00)\n",
      "Iter 1380 | Time 12.1840(11.6068) | Bit/dim 1.3315(1.3178) | Xent 0.2332(0.2857) | Loss 1.4481(1.4606) | Error 0.0756(0.0904) Steps 554(545.84) | Grad Norm 14.0280(9.8055) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0021 | Time 48.8582, Epoch Time 830.1098(653.8267), Bit/dim 1.3267(best: 1.3204), Xent 0.1754, Loss 1.4144, Error 0.0505(best: 0.0557)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1390 | Time 11.6663(11.5600) | Bit/dim 1.3852(1.3260) | Xent 0.3880(0.2861) | Loss 1.5792(1.4691) | Error 0.1178(0.0894) Steps 548(545.43) | Grad Norm 32.2932(11.8998) | Total Time 10.00(10.00)\n",
      "Iter 1400 | Time 11.8806(11.5253) | Bit/dim 1.3216(1.3315) | Xent 0.2311(0.2806) | Loss 1.4372(1.4717) | Error 0.0656(0.0880) Steps 566(545.59) | Grad Norm 7.0914(13.5553) | Total Time 10.00(10.00)\n",
      "Iter 1410 | Time 11.3302(11.5324) | Bit/dim 1.3341(1.3276) | Xent 0.2785(0.2797) | Loss 1.4734(1.4674) | Error 0.0811(0.0880) Steps 548(546.36) | Grad Norm 7.6733(12.0980) | Total Time 10.00(10.00)\n",
      "Iter 1420 | Time 11.8440(11.5836) | Bit/dim 1.3375(1.3229) | Xent 0.2703(0.2818) | Loss 1.4726(1.4638) | Error 0.0878(0.0877) Steps 548(548.22) | Grad Norm 4.5915(10.5516) | Total Time 10.00(10.00)\n",
      "Iter 1430 | Time 11.5129(11.6043) | Bit/dim 1.3081(1.3178) | Xent 0.2675(0.2751) | Loss 1.4418(1.4553) | Error 0.0867(0.0861) Steps 560(549.75) | Grad Norm 11.4502(9.7612) | Total Time 10.00(10.00)\n",
      "Iter 1440 | Time 11.7874(11.6226) | Bit/dim 1.2848(1.3141) | Xent 0.2458(0.2653) | Loss 1.4077(1.4467) | Error 0.0778(0.0837) Steps 554(550.81) | Grad Norm 9.5757(9.7927) | Total Time 10.00(10.00)\n",
      "Iter 1450 | Time 12.2924(11.6514) | Bit/dim 1.3210(1.3109) | Xent 0.2713(0.2627) | Loss 1.4567(1.4422) | Error 0.0767(0.0831) Steps 554(551.30) | Grad Norm 11.5596(10.0128) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0022 | Time 48.9133, Epoch Time 831.0558(659.1436), Bit/dim 1.3403(best: 1.3204), Xent 0.1329, Loss 1.4067, Error 0.0422(best: 0.0505)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1460 | Time 11.6476(11.6648) | Bit/dim 1.2847(1.3103) | Xent 0.2405(0.2613) | Loss 1.4050(1.4409) | Error 0.0800(0.0816) Steps 554(552.15) | Grad Norm 12.9084(9.9672) | Total Time 10.00(10.00)\n",
      "Iter 1470 | Time 11.8297(11.7111) | Bit/dim 1.3311(1.3095) | Xent 0.2685(0.2597) | Loss 1.4653(1.4393) | Error 0.0889(0.0809) Steps 560(552.54) | Grad Norm 19.3233(10.3699) | Total Time 10.00(10.00)\n",
      "Iter 1480 | Time 11.8037(11.6944) | Bit/dim 1.2766(1.3087) | Xent 0.2426(0.2567) | Loss 1.3979(1.4370) | Error 0.0711(0.0810) Steps 548(552.91) | Grad Norm 8.2397(10.7766) | Total Time 10.00(10.00)\n",
      "Iter 1490 | Time 11.7316(11.6581) | Bit/dim 1.3040(1.3052) | Xent 0.2032(0.2470) | Loss 1.4056(1.4288) | Error 0.0589(0.0780) Steps 548(552.52) | Grad Norm 4.0162(9.2668) | Total Time 10.00(10.00)\n",
      "Iter 1500 | Time 11.1991(11.6134) | Bit/dim 1.2906(1.3011) | Xent 0.2338(0.2436) | Loss 1.4075(1.4229) | Error 0.0667(0.0766) Steps 548(551.94) | Grad Norm 3.1227(8.1120) | Total Time 10.00(10.00)\n",
      "Iter 1510 | Time 11.7581(11.6631) | Bit/dim 1.3589(1.3129) | Xent 0.2897(0.2463) | Loss 1.5037(1.4361) | Error 0.0933(0.0787) Steps 554(552.34) | Grad Norm 26.8882(11.6685) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0023 | Time 49.8743, Epoch Time 834.2062(664.3955), Bit/dim 1.2945(best: 1.3204), Xent 0.1325, Loss 1.3607, Error 0.0399(best: 0.0422)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1520 | Time 11.7509(11.6412) | Bit/dim 1.2824(1.3141) | Xent 0.2544(0.2502) | Loss 1.4096(1.4392) | Error 0.0678(0.0796) Steps 548(551.92) | Grad Norm 4.7988(12.2304) | Total Time 10.00(10.00)\n",
      "Iter 1530 | Time 11.7408(11.6494) | Bit/dim 1.2977(1.3110) | Xent 0.2295(0.2426) | Loss 1.4124(1.4322) | Error 0.0733(0.0775) Steps 554(552.29) | Grad Norm 7.8055(10.8830) | Total Time 10.00(10.00)\n",
      "Iter 1540 | Time 11.6017(11.6015) | Bit/dim 1.2770(1.3050) | Xent 0.2041(0.2359) | Loss 1.3791(1.4230) | Error 0.0600(0.0747) Steps 548(551.62) | Grad Norm 3.9231(9.7580) | Total Time 10.00(10.00)\n",
      "Iter 1550 | Time 11.6096(11.6103) | Bit/dim 1.2854(1.3013) | Xent 0.1940(0.2315) | Loss 1.3824(1.4170) | Error 0.0656(0.0737) Steps 548(550.96) | Grad Norm 8.7068(8.8684) | Total Time 10.00(10.00)\n",
      "Iter 1560 | Time 11.6627(11.6193) | Bit/dim 1.3037(1.2986) | Xent 0.2012(0.2272) | Loss 1.4043(1.4123) | Error 0.0656(0.0720) Steps 560(551.18) | Grad Norm 12.4674(8.9489) | Total Time 10.00(10.00)\n",
      "Iter 1570 | Time 11.3274(11.5703) | Bit/dim 1.3165(1.2936) | Xent 0.2423(0.2243) | Loss 1.4376(1.4057) | Error 0.0733(0.0709) Steps 548(550.34) | Grad Norm 16.8499(8.4379) | Total Time 10.00(10.00)\n",
      "Iter 1580 | Time 11.6689(11.5669) | Bit/dim 1.2665(1.2931) | Xent 0.2502(0.2248) | Loss 1.3916(1.4055) | Error 0.0833(0.0709) Steps 560(550.81) | Grad Norm 13.7444(9.4347) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0024 | Time 50.4188, Epoch Time 829.8362(669.3587), Bit/dim 1.2844(best: 1.2945), Xent 0.1213, Loss 1.3451, Error 0.0366(best: 0.0399)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1590 | Time 11.8048(11.6074) | Bit/dim 1.3635(1.3060) | Xent 0.1927(0.2291) | Loss 1.4599(1.4206) | Error 0.0644(0.0728) Steps 560(550.73) | Grad Norm 15.8235(12.0842) | Total Time 10.00(10.00)\n",
      "Iter 1600 | Time 11.2786(11.5567) | Bit/dim 1.3007(1.3215) | Xent 0.2743(0.2366) | Loss 1.4379(1.4398) | Error 0.0878(0.0752) Steps 554(551.29) | Grad Norm 9.2819(13.3902) | Total Time 10.00(10.00)\n",
      "Iter 1610 | Time 11.2663(11.5400) | Bit/dim 1.2883(1.3240) | Xent 0.2215(0.2313) | Loss 1.3990(1.4397) | Error 0.0511(0.0728) Steps 548(551.12) | Grad Norm 9.5569(13.4847) | Total Time 10.00(10.00)\n",
      "Iter 1620 | Time 11.2615(11.5377) | Bit/dim 1.3018(1.3159) | Xent 0.2159(0.2260) | Loss 1.4098(1.4290) | Error 0.0689(0.0724) Steps 548(551.20) | Grad Norm 7.1098(12.0683) | Total Time 10.00(10.00)\n",
      "Iter 1630 | Time 11.8049(11.5865) | Bit/dim 1.2963(1.3079) | Xent 0.2575(0.2251) | Loss 1.4251(1.4205) | Error 0.0767(0.0717) Steps 554(550.54) | Grad Norm 8.4750(10.5744) | Total Time 10.00(10.00)\n",
      "Iter 1640 | Time 11.4689(11.5630) | Bit/dim 1.2704(1.3016) | Xent 0.1913(0.2190) | Loss 1.3660(1.4112) | Error 0.0589(0.0693) Steps 548(549.88) | Grad Norm 6.2893(9.4614) | Total Time 10.00(10.00)\n",
      "Iter 1650 | Time 11.5030(11.5433) | Bit/dim 1.2898(1.2958) | Xent 0.1831(0.2110) | Loss 1.3814(1.4014) | Error 0.0689(0.0673) Steps 548(549.38) | Grad Norm 14.0543(8.5266) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0025 | Time 48.9957, Epoch Time 826.0873(674.0605), Bit/dim 1.2858(best: 1.2844), Xent 0.1017, Loss 1.3366, Error 0.0309(best: 0.0366)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1660 | Time 11.8107(11.5606) | Bit/dim 1.3066(1.2988) | Xent 0.2068(0.2090) | Loss 1.4100(1.4034) | Error 0.0678(0.0665) Steps 554(549.70) | Grad Norm 13.3771(10.4454) | Total Time 10.00(10.00)\n",
      "Iter 1670 | Time 11.8467(11.5859) | Bit/dim 1.2792(1.2949) | Xent 0.1872(0.2057) | Loss 1.3727(1.3977) | Error 0.0633(0.0654) Steps 548(549.39) | Grad Norm 8.7972(10.4630) | Total Time 10.00(10.00)\n",
      "Iter 1680 | Time 11.4973(11.5637) | Bit/dim 1.2608(1.2886) | Xent 0.2580(0.2015) | Loss 1.3898(1.3893) | Error 0.0833(0.0640) Steps 548(549.02) | Grad Norm 6.8083(9.7878) | Total Time 10.00(10.00)\n",
      "Iter 1690 | Time 11.4643(11.5688) | Bit/dim 1.2777(1.2833) | Xent 0.1604(0.2001) | Loss 1.3579(1.3833) | Error 0.0556(0.0637) Steps 548(548.59) | Grad Norm 11.3212(8.9338) | Total Time 10.00(10.00)\n",
      "Iter 1700 | Time 12.0599(11.5685) | Bit/dim 1.2685(1.2804) | Xent 0.2289(0.2012) | Loss 1.3830(1.3810) | Error 0.0722(0.0636) Steps 548(548.44) | Grad Norm 12.0023(8.6439) | Total Time 10.00(10.00)\n",
      "Iter 1710 | Time 11.5014(11.5453) | Bit/dim 1.2787(1.2794) | Xent 0.2294(0.1985) | Loss 1.3934(1.3787) | Error 0.0700(0.0627) Steps 548(547.70) | Grad Norm 10.1815(9.4996) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0026 | Time 49.1538, Epoch Time 826.5032(678.6338), Bit/dim 1.2703(best: 1.2844), Xent 0.0881, Loss 1.3144, Error 0.0292(best: 0.0309)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1720 | Time 11.4352(11.5246) | Bit/dim 1.2309(1.2755) | Xent 0.2433(0.1936) | Loss 1.3526(1.3724) | Error 0.0767(0.0612) Steps 542(546.55) | Grad Norm 4.3650(8.3558) | Total Time 10.00(10.00)\n",
      "Iter 1730 | Time 11.9880(11.4932) | Bit/dim 1.2598(1.2727) | Xent 0.1804(0.1948) | Loss 1.3501(1.3702) | Error 0.0589(0.0605) Steps 548(545.70) | Grad Norm 9.0066(7.8472) | Total Time 10.00(10.00)\n",
      "Iter 1740 | Time 11.3893(11.5145) | Bit/dim 1.2608(1.2727) | Xent 0.2207(0.1953) | Loss 1.3711(1.3703) | Error 0.0767(0.0601) Steps 548(546.30) | Grad Norm 10.4601(8.2091) | Total Time 10.00(10.00)\n",
      "Iter 1750 | Time 12.2079(11.5473) | Bit/dim 1.5603(1.3068) | Xent 0.1120(0.1926) | Loss 1.6163(1.4031) | Error 0.0411(0.0600) Steps 572(547.32) | Grad Norm 11.1328(11.6183) | Total Time 10.00(10.00)\n",
      "Iter 1760 | Time 11.9000(11.6283) | Bit/dim 1.4056(1.3330) | Xent 0.1953(0.1989) | Loss 1.5033(1.4324) | Error 0.0544(0.0616) Steps 548(549.76) | Grad Norm 10.2110(11.7173) | Total Time 10.00(10.00)\n",
      "Iter 1770 | Time 11.8060(11.6128) | Bit/dim 1.2968(1.3253) | Xent 0.2599(0.1999) | Loss 1.4268(1.4253) | Error 0.0811(0.0622) Steps 548(548.98) | Grad Norm 11.2585(10.9976) | Total Time 10.00(10.00)\n",
      "Iter 1780 | Time 11.1596(11.5373) | Bit/dim 1.2590(1.3144) | Xent 0.2317(0.1957) | Loss 1.3748(1.4122) | Error 0.0656(0.0609) Steps 542(547.62) | Grad Norm 7.3833(9.7143) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0027 | Time 49.3375, Epoch Time 827.0824(683.0873), Bit/dim 1.2678(best: 1.2703), Xent 0.0966, Loss 1.3161, Error 0.0318(best: 0.0292)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1790 | Time 11.3686(11.5058) | Bit/dim 1.2628(1.3019) | Xent 0.1992(0.1947) | Loss 1.3624(1.3992) | Error 0.0633(0.0607) Steps 542(546.48) | Grad Norm 2.7056(8.0554) | Total Time 10.00(10.00)\n",
      "Iter 1800 | Time 11.5084(11.5342) | Bit/dim 1.2581(1.2909) | Xent 0.1635(0.1873) | Loss 1.3398(1.3845) | Error 0.0489(0.0584) Steps 548(546.88) | Grad Norm 4.3343(6.7792) | Total Time 10.00(10.00)\n",
      "Iter 1810 | Time 11.2055(11.5039) | Bit/dim 1.2583(1.2815) | Xent 0.2026(0.1847) | Loss 1.3596(1.3738) | Error 0.0567(0.0577) Steps 542(546.05) | Grad Norm 3.7165(6.0193) | Total Time 10.00(10.00)\n",
      "Iter 1820 | Time 11.3649(11.4950) | Bit/dim 1.2630(1.2761) | Xent 0.2063(0.1833) | Loss 1.3661(1.3677) | Error 0.0556(0.0574) Steps 548(545.32) | Grad Norm 10.7245(6.1097) | Total Time 10.00(10.00)\n",
      "Iter 1830 | Time 11.4235(11.4492) | Bit/dim 1.2473(1.2709) | Xent 0.1439(0.1804) | Loss 1.3192(1.3611) | Error 0.0511(0.0561) Steps 542(544.61) | Grad Norm 2.8331(6.1596) | Total Time 10.00(10.00)\n",
      "Iter 1840 | Time 11.3771(11.4188) | Bit/dim 1.2597(1.2676) | Xent 0.1679(0.1777) | Loss 1.3436(1.3564) | Error 0.0444(0.0550) Steps 542(543.92) | Grad Norm 3.4830(6.7133) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0028 | Time 49.1197, Epoch Time 818.4926(687.1494), Bit/dim 1.2435(best: 1.2678), Xent 0.0812, Loss 1.2841, Error 0.0256(best: 0.0292)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1850 | Time 11.4082(11.4189) | Bit/dim 1.2534(1.2641) | Xent 0.2071(0.1755) | Loss 1.3570(1.3519) | Error 0.0567(0.0534) Steps 542(543.99) | Grad Norm 2.8842(6.8226) | Total Time 10.00(10.00)\n",
      "Iter 1860 | Time 11.2563(11.4227) | Bit/dim 1.2499(1.2616) | Xent 0.1751(0.1725) | Loss 1.3375(1.3479) | Error 0.0511(0.0528) Steps 548(544.31) | Grad Norm 6.4335(7.1765) | Total Time 10.00(10.00)\n",
      "Iter 1870 | Time 12.6430(11.4570) | Bit/dim 1.3190(1.2859) | Xent 0.3561(0.1805) | Loss 1.4971(1.3762) | Error 0.1144(0.0553) Steps 578(546.09) | Grad Norm 33.8190(10.7059) | Total Time 10.00(10.00)\n",
      "Iter 1880 | Time 11.4523(11.4892) | Bit/dim 1.3174(1.3081) | Xent 0.2387(0.1866) | Loss 1.4367(1.4013) | Error 0.0789(0.0582) Steps 554(546.83) | Grad Norm 28.2282(12.3364) | Total Time 10.00(10.00)\n",
      "Iter 1890 | Time 11.4345(11.5415) | Bit/dim 1.3298(1.3107) | Xent 0.1262(0.1847) | Loss 1.3929(1.4030) | Error 0.0389(0.0582) Steps 560(549.32) | Grad Norm 9.5366(12.4315) | Total Time 10.00(10.00)\n",
      "Iter 1900 | Time 11.3839(11.5390) | Bit/dim 1.2604(1.3005) | Xent 0.1852(0.1798) | Loss 1.3530(1.3904) | Error 0.0544(0.0570) Steps 548(550.55) | Grad Norm 4.2296(11.2164) | Total Time 10.00(10.00)\n",
      "Iter 1910 | Time 11.3142(11.5237) | Bit/dim 1.2547(1.2894) | Xent 0.1512(0.1738) | Loss 1.3303(1.3763) | Error 0.0467(0.0552) Steps 554(550.51) | Grad Norm 6.3836(9.7888) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0029 | Time 49.2226, Epoch Time 825.8536(691.3106), Bit/dim 1.2516(best: 1.2435), Xent 0.0753, Loss 1.2892, Error 0.0244(best: 0.0256)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1920 | Time 11.2742(11.5191) | Bit/dim 1.2641(1.2797) | Xent 0.1844(0.1683) | Loss 1.3562(1.3638) | Error 0.0500(0.0536) Steps 548(550.59) | Grad Norm 4.5083(8.2961) | Total Time 10.00(10.00)\n",
      "Iter 1930 | Time 11.7392(11.5166) | Bit/dim 1.2420(1.2702) | Xent 0.1767(0.1647) | Loss 1.3304(1.3526) | Error 0.0522(0.0523) Steps 548(551.00) | Grad Norm 5.1777(7.0285) | Total Time 10.00(10.00)\n",
      "Iter 1940 | Time 11.3346(11.4967) | Bit/dim 1.2393(1.2623) | Xent 0.1677(0.1636) | Loss 1.3231(1.3441) | Error 0.0489(0.0512) Steps 554(551.79) | Grad Norm 5.2849(6.3216) | Total Time 10.00(10.00)\n",
      "Iter 1950 | Time 11.8171(11.4942) | Bit/dim 1.2572(1.2589) | Xent 0.2119(0.1618) | Loss 1.3632(1.3398) | Error 0.0656(0.0509) Steps 554(551.75) | Grad Norm 13.0896(6.5054) | Total Time 10.00(10.00)\n",
      "Iter 1960 | Time 11.4425(11.4655) | Bit/dim 1.2486(1.2560) | Xent 0.1454(0.1589) | Loss 1.3213(1.3354) | Error 0.0444(0.0505) Steps 548(550.76) | Grad Norm 6.1769(7.2024) | Total Time 10.00(10.00)\n",
      "Iter 1970 | Time 11.3409(11.4517) | Bit/dim 1.2520(1.2536) | Xent 0.1340(0.1533) | Loss 1.3190(1.3303) | Error 0.0400(0.0492) Steps 554(550.23) | Grad Norm 7.7393(7.2779) | Total Time 10.00(10.00)\n",
      "Iter 1980 | Time 11.4319(11.4516) | Bit/dim 1.2566(1.2529) | Xent 0.1460(0.1521) | Loss 1.3296(1.3289) | Error 0.0511(0.0485) Steps 542(549.76) | Grad Norm 19.7873(7.8679) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0030 | Time 49.4358, Epoch Time 820.3938(695.1831), Bit/dim 1.3062(best: 1.2435), Xent 0.0706, Loss 1.3415, Error 0.0226(best: 0.0244)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1990 | Time 11.3629(11.5169) | Bit/dim 1.2606(1.2631) | Xent 0.1339(0.1551) | Loss 1.3275(1.3407) | Error 0.0389(0.0491) Steps 548(551.85) | Grad Norm 8.3898(10.2874) | Total Time 10.00(10.00)\n",
      "Iter 2000 | Time 11.5545(11.5374) | Bit/dim 1.2513(1.2652) | Xent 0.1767(0.1569) | Loss 1.3396(1.3437) | Error 0.0533(0.0495) Steps 554(552.40) | Grad Norm 13.9278(11.1577) | Total Time 10.00(10.00)\n",
      "Iter 2010 | Time 11.4952(11.5437) | Bit/dim 1.2572(1.2631) | Xent 0.1740(0.1583) | Loss 1.3442(1.3423) | Error 0.0600(0.0502) Steps 554(552.63) | Grad Norm 7.5745(11.1387) | Total Time 10.00(10.00)\n",
      "Iter 2020 | Time 11.3954(11.5264) | Bit/dim 1.2626(1.2612) | Xent 0.1270(0.1571) | Loss 1.3261(1.3397) | Error 0.0467(0.0497) Steps 548(551.99) | Grad Norm 9.6361(11.3135) | Total Time 10.00(10.00)\n",
      "Iter 2030 | Time 12.0754(11.5065) | Bit/dim 1.2852(1.2586) | Xent 0.1729(0.1536) | Loss 1.3717(1.3353) | Error 0.0578(0.0488) Steps 554(551.12) | Grad Norm 15.3060(11.0295) | Total Time 10.00(10.00)\n",
      "Iter 2040 | Time 11.1998(11.4838) | Bit/dim 1.2541(1.2575) | Xent 0.1622(0.1567) | Loss 1.3352(1.3358) | Error 0.0467(0.0491) Steps 548(550.44) | Grad Norm 10.8265(10.9816) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0031 | Time 49.6173, Epoch Time 825.7671(699.1006), Bit/dim 1.2373(best: 1.2435), Xent 0.0684, Loss 1.2715, Error 0.0212(best: 0.0226)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2050 | Time 11.7701(11.4977) | Bit/dim 1.2385(1.2536) | Xent 0.1249(0.1536) | Loss 1.3009(1.3304) | Error 0.0411(0.0478) Steps 554(550.47) | Grad Norm 9.3585(10.9659) | Total Time 10.00(10.00)\n",
      "Iter 2060 | Time 11.4104(11.5372) | Bit/dim 1.2643(1.2564) | Xent 0.1629(0.1509) | Loss 1.3458(1.3318) | Error 0.0378(0.0468) Steps 554(551.21) | Grad Norm 7.8653(11.5636) | Total Time 10.00(10.00)\n",
      "Iter 2070 | Time 11.5106(11.5591) | Bit/dim 1.2309(1.2515) | Xent 0.1063(0.1459) | Loss 1.2841(1.3245) | Error 0.0300(0.0455) Steps 548(551.32) | Grad Norm 2.8294(10.5730) | Total Time 10.00(10.00)\n",
      "Iter 2080 | Time 11.1404(11.5597) | Bit/dim 1.2660(1.2508) | Xent 0.1853(0.1462) | Loss 1.3586(1.3239) | Error 0.0600(0.0456) Steps 548(551.40) | Grad Norm 21.4587(11.4905) | Total Time 10.00(10.00)\n",
      "Iter 2090 | Time 11.3347(11.5217) | Bit/dim 1.2599(1.2506) | Xent 0.1690(0.1456) | Loss 1.3444(1.3234) | Error 0.0589(0.0457) Steps 548(551.17) | Grad Norm 20.3790(11.2161) | Total Time 10.00(10.00)\n",
      "Iter 2100 | Time 12.0833(11.5193) | Bit/dim 1.2582(1.2496) | Xent 0.1398(0.1465) | Loss 1.3281(1.3228) | Error 0.0444(0.0455) Steps 554(551.43) | Grad Norm 12.5002(11.5192) | Total Time 10.00(10.00)\n",
      "Iter 2110 | Time 11.4042(11.5384) | Bit/dim 1.2642(1.2490) | Xent 0.1584(0.1460) | Loss 1.3434(1.3220) | Error 0.0611(0.0456) Steps 554(552.09) | Grad Norm 8.8643(11.2659) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0032 | Time 49.6026, Epoch Time 828.5016(702.9826), Bit/dim 1.2342(best: 1.2373), Xent 0.0684, Loss 1.2684, Error 0.0227(best: 0.0212)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2120 | Time 11.3645(11.5465) | Bit/dim 1.2301(1.2466) | Xent 0.1610(0.1455) | Loss 1.3106(1.3194) | Error 0.0467(0.0453) Steps 554(552.13) | Grad Norm 7.9466(10.8951) | Total Time 10.00(10.00)\n",
      "Iter 2130 | Time 11.5475(11.5184) | Bit/dim 1.2254(1.2435) | Xent 0.1220(0.1419) | Loss 1.2864(1.3144) | Error 0.0322(0.0437) Steps 554(552.01) | Grad Norm 6.1718(9.8620) | Total Time 10.00(10.00)\n",
      "Iter 2140 | Time 11.2679(11.4923) | Bit/dim 1.2153(1.2378) | Xent 0.1424(0.1416) | Loss 1.2865(1.3086) | Error 0.0433(0.0436) Steps 548(552.18) | Grad Norm 2.5218(8.1897) | Total Time 10.00(10.00)\n",
      "Iter 2150 | Time 11.5297(11.5031) | Bit/dim 1.2123(1.2342) | Xent 0.1011(0.1379) | Loss 1.2628(1.3031) | Error 0.0300(0.0427) Steps 554(552.37) | Grad Norm 2.6901(7.2717) | Total Time 10.00(10.00)\n",
      "Iter 2160 | Time 11.6374(11.5358) | Bit/dim 1.2266(1.2328) | Xent 0.1674(0.1373) | Loss 1.3103(1.3015) | Error 0.0456(0.0424) Steps 554(552.64) | Grad Norm 3.6046(6.5854) | Total Time 10.00(10.00)\n",
      "Iter 2170 | Time 11.4650(11.5292) | Bit/dim 1.2351(1.2296) | Xent 0.1595(0.1371) | Loss 1.3148(1.2982) | Error 0.0433(0.0427) Steps 554(553.00) | Grad Norm 5.4530(5.8284) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0033 | Time 49.4097, Epoch Time 825.0086(706.6434), Bit/dim 1.2926(best: 1.2342), Xent 0.0680, Loss 1.3266, Error 0.0220(best: 0.0212)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2180 | Time 11.3135(11.5354) | Bit/dim 1.2537(1.2328) | Xent 0.1674(0.1360) | Loss 1.3374(1.3008) | Error 0.0489(0.0422) Steps 548(553.09) | Grad Norm 7.6250(7.3096) | Total Time 10.00(10.00)\n",
      "Iter 2190 | Time 11.5857(11.6363) | Bit/dim 1.2978(1.2624) | Xent 0.1803(0.1431) | Loss 1.3879(1.3340) | Error 0.0556(0.0450) Steps 560(555.85) | Grad Norm 21.0181(10.7623) | Total Time 10.00(10.00)\n",
      "Iter 2200 | Time 11.2608(11.6800) | Bit/dim 1.2610(1.2746) | Xent 0.1262(0.1448) | Loss 1.3241(1.3470) | Error 0.0378(0.0450) Steps 548(556.55) | Grad Norm 9.5121(11.7824) | Total Time 10.00(10.00)\n",
      "Iter 2210 | Time 11.0991(11.6285) | Bit/dim 1.2202(1.2673) | Xent 0.0998(0.1438) | Loss 1.2701(1.3392) | Error 0.0322(0.0453) Steps 548(555.20) | Grad Norm 7.1926(10.8810) | Total Time 10.00(10.00)\n",
      "Iter 2220 | Time 11.5119(11.6214) | Bit/dim 1.2264(1.2581) | Xent 0.1333(0.1388) | Loss 1.2930(1.3275) | Error 0.0467(0.0441) Steps 554(554.61) | Grad Norm 3.9518(9.3137) | Total Time 10.00(10.00)\n",
      "Iter 2230 | Time 11.5613(11.6151) | Bit/dim 1.2330(1.2501) | Xent 0.1572(0.1406) | Loss 1.3116(1.3204) | Error 0.0433(0.0448) Steps 554(553.97) | Grad Norm 2.8874(7.8859) | Total Time 10.00(10.00)\n",
      "Iter 2240 | Time 11.7465(11.6383) | Bit/dim 1.2283(1.2431) | Xent 0.1241(0.1407) | Loss 1.2904(1.3135) | Error 0.0333(0.0440) Steps 554(553.95) | Grad Norm 4.3491(7.0472) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0034 | Time 49.9335, Epoch Time 835.4306(710.5070), Bit/dim 1.2160(best: 1.2342), Xent 0.0673, Loss 1.2496, Error 0.0230(best: 0.0212)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2250 | Time 11.9155(11.6164) | Bit/dim 1.2075(1.2377) | Xent 0.1400(0.1377) | Loss 1.2775(1.3065) | Error 0.0444(0.0428) Steps 554(553.81) | Grad Norm 3.3635(6.6109) | Total Time 10.00(10.00)\n",
      "Iter 2260 | Time 11.8025(11.6293) | Bit/dim 1.2336(1.2334) | Xent 0.1270(0.1342) | Loss 1.2971(1.3005) | Error 0.0322(0.0415) Steps 554(554.46) | Grad Norm 10.9471(6.5722) | Total Time 10.00(10.00)\n",
      "Iter 2270 | Time 11.6784(11.6506) | Bit/dim 1.2259(1.2304) | Xent 0.0992(0.1313) | Loss 1.2755(1.2961) | Error 0.0300(0.0405) Steps 554(554.34) | Grad Norm 11.5278(7.4390) | Total Time 10.00(10.00)\n",
      "Iter 2280 | Time 11.5652(11.6356) | Bit/dim 1.2319(1.2258) | Xent 0.1129(0.1276) | Loss 1.2883(1.2896) | Error 0.0344(0.0398) Steps 554(554.25) | Grad Norm 11.7045(7.1633) | Total Time 10.00(10.00)\n",
      "Iter 2290 | Time 11.4410(11.6023) | Bit/dim 1.2407(1.2242) | Xent 0.1237(0.1261) | Loss 1.3025(1.2872) | Error 0.0400(0.0394) Steps 554(554.19) | Grad Norm 13.7058(8.1731) | Total Time 10.00(10.00)\n",
      "Iter 2300 | Time 11.5998(11.7063) | Bit/dim 1.3019(1.2442) | Xent 0.1557(0.1289) | Loss 1.3798(1.3086) | Error 0.0467(0.0398) Steps 554(556.41) | Grad Norm 19.5930(11.6475) | Total Time 10.00(10.00)\n",
      "Iter 2310 | Time 12.0082(11.7828) | Bit/dim 1.3090(1.2626) | Xent 0.1655(0.1310) | Loss 1.3917(1.3281) | Error 0.0478(0.0407) Steps 554(558.03) | Grad Norm 19.0666(13.2845) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0035 | Time 50.2665, Epoch Time 839.5557(714.3785), Bit/dim 1.2411(best: 1.2160), Xent 0.0622, Loss 1.2722, Error 0.0198(best: 0.0212)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2320 | Time 11.3880(11.7322) | Bit/dim 1.2601(1.2608) | Xent 0.1568(0.1319) | Loss 1.3385(1.3268) | Error 0.0544(0.0413) Steps 566(558.51) | Grad Norm 18.7849(13.1818) | Total Time 10.00(10.00)\n",
      "Iter 2330 | Time 11.8817(11.7063) | Bit/dim 1.2339(1.2540) | Xent 0.1334(0.1298) | Loss 1.3006(1.3189) | Error 0.0433(0.0401) Steps 554(557.46) | Grad Norm 7.1281(11.8830) | Total Time 10.00(10.00)\n",
      "Iter 2340 | Time 11.3331(11.6940) | Bit/dim 1.2150(1.2455) | Xent 0.1000(0.1264) | Loss 1.2650(1.3087) | Error 0.0311(0.0393) Steps 554(556.55) | Grad Norm 5.7707(9.9571) | Total Time 10.00(10.00)\n",
      "Iter 2350 | Time 11.4302(11.6693) | Bit/dim 1.2193(1.2374) | Xent 0.1169(0.1253) | Loss 1.2777(1.3000) | Error 0.0333(0.0392) Steps 554(555.88) | Grad Norm 2.7651(8.3756) | Total Time 10.00(10.00)\n",
      "Iter 2360 | Time 11.4211(11.6609) | Bit/dim 1.2193(1.2317) | Xent 0.1235(0.1263) | Loss 1.2810(1.2949) | Error 0.0356(0.0393) Steps 554(555.39) | Grad Norm 8.5377(7.5927) | Total Time 10.00(10.00)\n",
      "Iter 2370 | Time 11.6133(11.6097) | Bit/dim 1.2361(1.2292) | Xent 0.1150(0.1269) | Loss 1.2936(1.2926) | Error 0.0400(0.0397) Steps 554(555.02) | Grad Norm 9.5554(7.5289) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0036 | Time 49.5066, Epoch Time 830.3389(717.8573), Bit/dim 1.2111(best: 1.2160), Xent 0.0589, Loss 1.2406, Error 0.0199(best: 0.0198)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2380 | Time 11.5372(11.6017) | Bit/dim 1.3061(1.2337) | Xent 0.1119(0.1255) | Loss 1.3621(1.2964) | Error 0.0356(0.0392) Steps 560(555.27) | Grad Norm 14.0919(9.3522) | Total Time 10.00(10.00)\n",
      "Iter 2390 | Time 11.6888(11.6666) | Bit/dim 1.2841(1.2424) | Xent 0.1411(0.1266) | Loss 1.3547(1.3058) | Error 0.0478(0.0403) Steps 554(556.21) | Grad Norm 34.1798(11.4156) | Total Time 10.00(10.00)\n",
      "Iter 2400 | Time 11.7322(11.7159) | Bit/dim 1.2618(1.2510) | Xent 0.1291(0.1282) | Loss 1.3263(1.3151) | Error 0.0422(0.0406) Steps 560(557.02) | Grad Norm 11.9215(12.5921) | Total Time 10.00(10.00)\n",
      "Iter 2410 | Time 11.5704(11.6874) | Bit/dim 1.2204(1.2446) | Xent 0.1302(0.1293) | Loss 1.2855(1.3093) | Error 0.0389(0.0408) Steps 554(556.40) | Grad Norm 9.3606(11.6918) | Total Time 10.00(10.00)\n",
      "Iter 2420 | Time 11.4212(11.6551) | Bit/dim 1.2146(1.2377) | Xent 0.1283(0.1273) | Loss 1.2787(1.3013) | Error 0.0422(0.0401) Steps 554(556.09) | Grad Norm 5.7098(10.0813) | Total Time 10.00(10.00)\n",
      "Iter 2430 | Time 11.6786(11.6352) | Bit/dim 1.2019(1.2301) | Xent 0.1142(0.1235) | Loss 1.2590(1.2919) | Error 0.0311(0.0388) Steps 554(555.54) | Grad Norm 3.4787(8.3535) | Total Time 10.00(10.00)\n",
      "Iter 2440 | Time 11.4220(11.5983) | Bit/dim 1.2087(1.2246) | Xent 0.1176(0.1219) | Loss 1.2675(1.2855) | Error 0.0333(0.0389) Steps 554(555.13) | Grad Norm 3.3487(6.8825) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0037 | Time 49.5163, Epoch Time 834.2120(721.3479), Bit/dim 1.1984(best: 1.2111), Xent 0.0530, Loss 1.2249, Error 0.0167(best: 0.0198)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2450 | Time 11.6146(11.6233) | Bit/dim 1.2054(1.2194) | Xent 0.0819(0.1206) | Loss 1.2464(1.2797) | Error 0.0256(0.0385) Steps 554(554.84) | Grad Norm 3.0276(6.1201) | Total Time 10.00(10.00)\n",
      "Iter 2460 | Time 11.5704(11.6365) | Bit/dim 1.2068(1.2142) | Xent 0.1073(0.1213) | Loss 1.2605(1.2748) | Error 0.0289(0.0381) Steps 554(554.62) | Grad Norm 3.5143(5.5845) | Total Time 10.00(10.00)\n",
      "Iter 2470 | Time 11.3026(11.6237) | Bit/dim 1.1851(1.2116) | Xent 0.1327(0.1193) | Loss 1.2515(1.2712) | Error 0.0433(0.0376) Steps 554(554.45) | Grad Norm 3.0393(5.8829) | Total Time 10.00(10.00)\n",
      "Iter 2480 | Time 11.5810(11.6166) | Bit/dim 1.2089(1.2095) | Xent 0.1419(0.1176) | Loss 1.2799(1.2683) | Error 0.0433(0.0368) Steps 554(554.34) | Grad Norm 7.1834(5.9560) | Total Time 10.00(10.00)\n",
      "Iter 2490 | Time 11.5559(11.6125) | Bit/dim 1.1897(1.2072) | Xent 0.1238(0.1167) | Loss 1.2516(1.2655) | Error 0.0311(0.0359) Steps 554(554.25) | Grad Norm 6.9770(5.4067) | Total Time 10.00(10.00)\n",
      "Iter 2500 | Time 11.5245(11.6091) | Bit/dim 1.2009(1.2086) | Xent 0.0947(0.1133) | Loss 1.2482(1.2652) | Error 0.0300(0.0350) Steps 554(554.35) | Grad Norm 11.7820(6.6758) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0038 | Time 50.4826, Epoch Time 831.9606(724.6663), Bit/dim 1.2269(best: 1.1984), Xent 0.0593, Loss 1.2565, Error 0.0192(best: 0.0167)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2510 | Time 11.1953(11.5979) | Bit/dim 1.4290(1.2180) | Xent 0.1737(0.1184) | Loss 1.5159(1.2772) | Error 0.0544(0.0366) Steps 536(554.06) | Grad Norm 30.4334(8.9368) | Total Time 10.00(10.00)\n",
      "Iter 2520 | Time 11.7842(11.6553) | Bit/dim 1.2489(1.2367) | Xent 0.1406(0.1205) | Loss 1.3192(1.2970) | Error 0.0478(0.0378) Steps 566(556.93) | Grad Norm 20.2499(10.9871) | Total Time 10.00(10.00)\n",
      "Iter 2530 | Time 11.4747(11.6373) | Bit/dim 1.2162(1.2351) | Xent 0.1008(0.1215) | Loss 1.2666(1.2958) | Error 0.0322(0.0379) Steps 560(556.66) | Grad Norm 13.7458(10.7692) | Total Time 10.00(10.00)\n",
      "Iter 2540 | Time 11.4587(11.6318) | Bit/dim 1.2193(1.2336) | Xent 0.0817(0.1189) | Loss 1.2602(1.2931) | Error 0.0244(0.0372) Steps 560(557.02) | Grad Norm 8.0012(11.0434) | Total Time 10.00(10.00)\n",
      "Iter 2550 | Time 11.6115(11.6262) | Bit/dim 1.2263(1.2277) | Xent 0.0895(0.1168) | Loss 1.2711(1.2861) | Error 0.0278(0.0364) Steps 554(556.54) | Grad Norm 11.0252(10.3560) | Total Time 10.00(10.00)\n",
      "Iter 2560 | Time 11.5933(11.6318) | Bit/dim 1.2185(1.2265) | Xent 0.1143(0.1181) | Loss 1.2756(1.2855) | Error 0.0344(0.0361) Steps 554(556.04) | Grad Norm 10.2199(11.0573) | Total Time 10.00(10.00)\n",
      "Iter 2570 | Time 11.4989(11.6189) | Bit/dim 1.2004(1.2202) | Xent 0.1047(0.1178) | Loss 1.2527(1.2791) | Error 0.0344(0.0366) Steps 554(555.82) | Grad Norm 4.1287(9.5329) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0039 | Time 49.8907, Epoch Time 833.5134(727.9317), Bit/dim 1.1988(best: 1.1984), Xent 0.0486, Loss 1.2231, Error 0.0152(best: 0.0167)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2580 | Time 11.2967(11.6077) | Bit/dim 1.1886(1.2134) | Xent 0.0997(0.1148) | Loss 1.2384(1.2709) | Error 0.0267(0.0353) Steps 554(555.35) | Grad Norm 2.1482(7.8790) | Total Time 10.00(10.00)\n",
      "Iter 2590 | Time 11.5790(11.5793) | Bit/dim 1.1975(1.2074) | Xent 0.0956(0.1125) | Loss 1.2453(1.2637) | Error 0.0256(0.0346) Steps 554(554.99) | Grad Norm 2.2146(6.5488) | Total Time 10.00(10.00)\n",
      "Iter 2600 | Time 11.7294(11.5766) | Bit/dim 1.2271(1.2052) | Xent 0.0981(0.1088) | Loss 1.2762(1.2596) | Error 0.0289(0.0333) Steps 560(554.91) | Grad Norm 14.7521(6.7762) | Total Time 10.00(10.00)\n",
      "Iter 2610 | Time 11.4236(11.6310) | Bit/dim 1.2266(1.2149) | Xent 0.0901(0.1085) | Loss 1.2716(1.2691) | Error 0.0333(0.0335) Steps 554(555.90) | Grad Norm 8.8459(9.0587) | Total Time 10.00(10.00)\n",
      "Iter 2620 | Time 11.9510(11.6632) | Bit/dim 1.2214(1.2133) | Xent 0.1430(0.1128) | Loss 1.2929(1.2698) | Error 0.0500(0.0346) Steps 554(555.57) | Grad Norm 29.4115(9.7442) | Total Time 10.00(10.00)\n",
      "Iter 2630 | Time 11.6683(11.7208) | Bit/dim 1.3599(1.2342) | Xent 0.0759(0.1100) | Loss 1.3979(1.2892) | Error 0.0200(0.0338) Steps 560(556.63) | Grad Norm 9.6876(11.7583) | Total Time 10.00(10.00)\n",
      "Iter 2640 | Time 11.9516(11.7319) | Bit/dim 1.2393(1.2435) | Xent 0.1271(0.1151) | Loss 1.3029(1.3010) | Error 0.0456(0.0353) Steps 554(556.45) | Grad Norm 14.8010(11.7550) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0040 | Time 50.3500, Epoch Time 837.5127(731.2191), Bit/dim 1.2137(best: 1.1984), Xent 0.0522, Loss 1.2398, Error 0.0174(best: 0.0152)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2650 | Time 11.5624(11.7093) | Bit/dim 1.2120(1.2355) | Xent 0.1053(0.1123) | Loss 1.2647(1.2917) | Error 0.0333(0.0346) Steps 560(556.43) | Grad Norm 9.6204(10.2781) | Total Time 10.00(10.00)\n",
      "Iter 2660 | Time 11.3138(11.6472) | Bit/dim 1.1905(1.2254) | Xent 0.0939(0.1095) | Loss 1.2374(1.2801) | Error 0.0289(0.0334) Steps 554(555.93) | Grad Norm 3.3326(8.5103) | Total Time 10.00(10.00)\n",
      "Iter 2670 | Time 11.3294(11.6311) | Bit/dim 1.1750(1.2177) | Xent 0.1145(0.1111) | Loss 1.2323(1.2733) | Error 0.0389(0.0342) Steps 554(555.42) | Grad Norm 5.1615(7.1438) | Total Time 10.00(10.00)\n",
      "Iter 2680 | Time 11.4385(11.6161) | Bit/dim 1.2023(1.2126) | Xent 0.0989(0.1081) | Loss 1.2517(1.2667) | Error 0.0322(0.0332) Steps 554(555.05) | Grad Norm 4.6859(6.1975) | Total Time 10.00(10.00)\n",
      "Iter 2690 | Time 11.4571(11.6026) | Bit/dim 1.1960(1.2089) | Xent 0.0987(0.1058) | Loss 1.2454(1.2618) | Error 0.0311(0.0332) Steps 554(554.77) | Grad Norm 7.4896(5.8217) | Total Time 10.00(10.00)\n",
      "Iter 2700 | Time 11.9369(11.6346) | Bit/dim 1.2187(1.2054) | Xent 0.0892(0.1064) | Loss 1.2633(1.2586) | Error 0.0344(0.0330) Steps 560(554.75) | Grad Norm 13.2624(6.7209) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0041 | Time 50.3372, Epoch Time 830.3778(734.1939), Bit/dim 1.1849(best: 1.1984), Xent 0.0463, Loss 1.2080, Error 0.0148(best: 0.0152)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2710 | Time 11.7607(11.6035) | Bit/dim 1.1817(1.2012) | Xent 0.1026(0.1045) | Loss 1.2330(1.2534) | Error 0.0322(0.0319) Steps 554(554.55) | Grad Norm 4.8743(6.3212) | Total Time 10.00(10.00)\n",
      "Iter 2720 | Time 11.3968(11.5836) | Bit/dim 1.1959(1.1975) | Xent 0.0982(0.1028) | Loss 1.2450(1.2489) | Error 0.0300(0.0315) Steps 554(554.41) | Grad Norm 2.5368(5.5200) | Total Time 10.00(10.00)\n",
      "Iter 2730 | Time 11.5355(11.6484) | Bit/dim 1.2192(1.2155) | Xent 0.1806(0.1056) | Loss 1.3094(1.2683) | Error 0.0556(0.0325) Steps 554(555.31) | Grad Norm 5.7605(8.2454) | Total Time 10.00(10.00)\n",
      "Iter 2740 | Time 11.9720(11.6712) | Bit/dim 1.2038(1.2213) | Xent 0.1286(0.1111) | Loss 1.2682(1.2769) | Error 0.0400(0.0339) Steps 554(555.61) | Grad Norm 4.9119(8.6982) | Total Time 10.00(10.00)\n",
      "Iter 2750 | Time 11.5947(11.6597) | Bit/dim 1.2143(1.2180) | Xent 0.1322(0.1110) | Loss 1.2804(1.2734) | Error 0.0456(0.0339) Steps 554(556.28) | Grad Norm 6.4863(7.6986) | Total Time 10.00(10.00)\n",
      "Iter 2760 | Time 12.1923(11.6462) | Bit/dim 1.2162(1.2125) | Xent 0.0958(0.1121) | Loss 1.2641(1.2685) | Error 0.0289(0.0343) Steps 560(556.03) | Grad Norm 13.6287(7.5675) | Total Time 10.00(10.00)\n",
      "Iter 2770 | Time 12.1348(11.6839) | Bit/dim 1.2797(1.2349) | Xent 0.1391(0.1086) | Loss 1.3493(1.2892) | Error 0.0389(0.0331) Steps 554(555.75) | Grad Norm 10.5292(9.9275) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0042 | Time 49.3856, Epoch Time 835.8805(737.2445), Bit/dim 1.3142(best: 1.1849), Xent 0.0573, Loss 1.3428, Error 0.0173(best: 0.0148)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2800 | Time 11.7235(11.6636) | Bit/dim 1.2059(1.2281) | Xent 0.0938(0.1084) | Loss 1.2528(1.2823) | Error 0.0333(0.0330) Steps 560(556.87) | Grad Norm 9.3095(9.9888) | Total Time 10.00(10.00)\n",
      "Iter 2810 | Time 11.7852(11.6470) | Bit/dim 1.2312(1.2212) | Xent 0.0913(0.1069) | Loss 1.2768(1.2746) | Error 0.0289(0.0321) Steps 566(556.81) | Grad Norm 12.1328(9.8766) | Total Time 10.00(10.00)\n",
      "Iter 2820 | Time 11.6202(11.6144) | Bit/dim 1.2075(1.2157) | Xent 0.0784(0.1034) | Loss 1.2467(1.2673) | Error 0.0256(0.0312) Steps 560(556.25) | Grad Norm 7.9817(10.5956) | Total Time 10.00(10.00)\n",
      "Iter 2830 | Time 12.0300(11.5930) | Bit/dim 1.1897(1.2089) | Xent 0.1056(0.1002) | Loss 1.2425(1.2590) | Error 0.0333(0.0306) Steps 554(555.82) | Grad Norm 3.2226(9.3718) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0043 | Time 50.0458, Epoch Time 1061.3039(746.9663), Bit/dim 1.1766(best: 1.1849), Xent 0.0471, Loss 1.2002, Error 0.0156(best: 0.0148)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2840 | Time 11.5392(11.5599) | Bit/dim 1.1822(1.2038) | Xent 0.1343(0.0982) | Loss 1.2493(1.2529) | Error 0.0367(0.0301) Steps 554(555.34) | Grad Norm 6.6830(8.1528) | Total Time 10.00(10.00)\n",
      "Iter 2850 | Time 11.4705(11.5682) | Bit/dim 1.1765(1.1988) | Xent 0.1063(0.0983) | Loss 1.2296(1.2480) | Error 0.0244(0.0302) Steps 554(554.99) | Grad Norm 3.9664(6.9497) | Total Time 10.00(10.00)\n",
      "Iter 2860 | Time 11.6337(11.5632) | Bit/dim 1.1832(1.1943) | Xent 0.1301(0.0980) | Loss 1.2483(1.2433) | Error 0.0389(0.0299) Steps 554(554.88) | Grad Norm 5.6984(6.3535) | Total Time 10.00(10.00)\n",
      "Iter 2870 | Time 12.5581(11.6046) | Bit/dim 1.2526(1.1955) | Xent 0.1013(0.0974) | Loss 1.3033(1.2441) | Error 0.0300(0.0296) Steps 584(556.05) | Grad Norm 37.8843(8.0489) | Total Time 10.00(10.00)\n",
      "Iter 2880 | Time 12.1285(11.7224) | Bit/dim 1.2801(1.2252) | Xent 0.1496(0.1020) | Loss 1.3549(1.2762) | Error 0.0411(0.0312) Steps 566(557.89) | Grad Norm 13.8821(11.0128) | Total Time 10.00(10.00)\n",
      "Iter 2890 | Time 11.4606(11.6679) | Bit/dim 1.2339(1.2343) | Xent 0.0887(0.1018) | Loss 1.2782(1.2852) | Error 0.0256(0.0318) Steps 554(557.32) | Grad Norm 13.0666(11.5600) | Total Time 10.00(10.00)\n",
      "Iter 2900 | Time 11.6782(11.6945) | Bit/dim 1.2460(1.2376) | Xent 0.1282(0.1051) | Loss 1.3101(1.2901) | Error 0.0456(0.0329) Steps 554(557.25) | Grad Norm 23.2370(12.3878) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0044 | Time 50.7926, Epoch Time 837.1606(749.6721), Bit/dim 1.1971(best: 1.1766), Xent 0.0476, Loss 1.2209, Error 0.0160(best: 0.0148)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2910 | Time 11.8295(11.6975) | Bit/dim 1.2124(1.2316) | Xent 0.0954(0.1019) | Loss 1.2601(1.2825) | Error 0.0289(0.0322) Steps 560(557.93) | Grad Norm 11.6770(12.0311) | Total Time 10.00(10.00)\n",
      "Iter 2920 | Time 11.6870(11.7170) | Bit/dim 1.1814(1.2251) | Xent 0.0728(0.1012) | Loss 1.2178(1.2757) | Error 0.0211(0.0320) Steps 554(557.79) | Grad Norm 4.3422(11.6350) | Total Time 10.00(10.00)\n",
      "Iter 2930 | Time 11.3957(11.6659) | Bit/dim 1.1886(1.2166) | Xent 0.0788(0.1011) | Loss 1.2280(1.2672) | Error 0.0244(0.0318) Steps 554(556.93) | Grad Norm 8.4993(10.3720) | Total Time 10.00(10.00)\n",
      "Iter 2940 | Time 11.4300(11.6514) | Bit/dim 1.1767(1.2075) | Xent 0.0818(0.0988) | Loss 1.2176(1.2570) | Error 0.0278(0.0309) Steps 554(556.30) | Grad Norm 5.8448(8.9127) | Total Time 10.00(10.00)\n",
      "Iter 2950 | Time 11.2929(11.6530) | Bit/dim 1.1612(1.2000) | Xent 0.1548(0.0996) | Loss 1.2387(1.2498) | Error 0.0444(0.0312) Steps 554(555.70) | Grad Norm 2.9644(7.4354) | Total Time 10.00(10.00)\n",
      "Iter 2960 | Time 11.5118(11.6291) | Bit/dim 1.1668(1.1948) | Xent 0.0799(0.0967) | Loss 1.2068(1.2432) | Error 0.0211(0.0301) Steps 554(555.40) | Grad Norm 3.2145(6.2685) | Total Time 10.00(10.00)\n",
      "Iter 2970 | Time 11.6565(11.6252) | Bit/dim 1.1882(1.1922) | Xent 0.0809(0.0955) | Loss 1.2286(1.2399) | Error 0.0289(0.0301) Steps 554(555.21) | Grad Norm 12.4758(5.7841) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0045 | Time 50.7765, Epoch Time 833.5758(752.1892), Bit/dim 1.1997(best: 1.1766), Xent 0.0466, Loss 1.2230, Error 0.0167(best: 0.0148)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2980 | Time 11.6211(11.6521) | Bit/dim 1.1928(1.1965) | Xent 0.1092(0.0965) | Loss 1.2474(1.2447) | Error 0.0289(0.0302) Steps 554(555.80) | Grad Norm 5.6873(8.0908) | Total Time 10.00(10.00)\n",
      "Iter 2990 | Time 12.0297(11.7007) | Bit/dim 1.2684(1.2132) | Xent 0.0790(0.0972) | Loss 1.3079(1.2618) | Error 0.0267(0.0300) Steps 560(557.07) | Grad Norm 17.6770(10.4773) | Total Time 10.00(10.00)\n",
      "Iter 3000 | Time 11.6586(11.7412) | Bit/dim 1.2085(1.2166) | Xent 0.1079(0.0986) | Loss 1.2625(1.2659) | Error 0.0344(0.0302) Steps 560(557.40) | Grad Norm 17.3964(11.2787) | Total Time 10.00(10.00)\n",
      "Iter 3010 | Time 10.9291(11.7645) | Bit/dim 1.2563(1.2218) | Xent 0.0814(0.0983) | Loss 1.2970(1.2709) | Error 0.0300(0.0306) Steps 530(558.00) | Grad Norm 14.4084(12.3984) | Total Time 10.00(10.00)\n",
      "Iter 3020 | Time 11.5831(11.7475) | Bit/dim 1.2004(1.2187) | Xent 0.1170(0.0972) | Loss 1.2588(1.2673) | Error 0.0333(0.0303) Steps 560(558.38) | Grad Norm 2.1255(11.9872) | Total Time 10.00(10.00)\n",
      "Iter 3030 | Time 11.9616(11.7509) | Bit/dim 1.2057(1.2180) | Xent 0.0692(0.0974) | Loss 1.2403(1.2667) | Error 0.0256(0.0303) Steps 560(558.03) | Grad Norm 10.4443(12.3262) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0046 | Time 50.3561, Epoch Time 842.5936(754.9014), Bit/dim 1.2044(best: 1.1766), Xent 0.0466, Loss 1.2277, Error 0.0150(best: 0.0148)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3040 | Time 11.5649(11.7232) | Bit/dim 1.1865(1.2144) | Xent 0.0975(0.0971) | Loss 1.2352(1.2629) | Error 0.0333(0.0300) Steps 554(556.97) | Grad Norm 2.9708(11.8360) | Total Time 10.00(10.00)\n",
      "Iter 3050 | Time 11.7169(11.6840) | Bit/dim 1.1908(1.2082) | Xent 0.1107(0.0981) | Loss 1.2462(1.2573) | Error 0.0344(0.0302) Steps 560(556.98) | Grad Norm 9.8088(10.9713) | Total Time 10.00(10.00)\n",
      "Iter 3060 | Time 11.4911(11.6490) | Bit/dim 1.1848(1.2005) | Xent 0.0819(0.0948) | Loss 1.2258(1.2479) | Error 0.0222(0.0292) Steps 554(556.20) | Grad Norm 2.4958(9.4414) | Total Time 10.00(10.00)\n",
      "Iter 3070 | Time 11.5971(11.6368) | Bit/dim 1.1830(1.1944) | Xent 0.1073(0.0923) | Loss 1.2366(1.2405) | Error 0.0344(0.0285) Steps 554(555.62) | Grad Norm 1.4802(7.5075) | Total Time 10.00(10.00)\n",
      "Iter 3080 | Time 11.5338(11.6172) | Bit/dim 1.1844(1.1904) | Xent 0.0855(0.0885) | Loss 1.2271(1.2347) | Error 0.0333(0.0276) Steps 554(555.84) | Grad Norm 3.3951(6.1225) | Total Time 10.00(10.00)\n",
      "Iter 3090 | Time 11.4660(11.6195) | Bit/dim 1.1766(1.1860) | Xent 0.1203(0.0909) | Loss 1.2368(1.2314) | Error 0.0389(0.0279) Steps 554(555.52) | Grad Norm 2.8530(5.1925) | Total Time 10.00(10.00)\n",
      "Iter 3100 | Time 11.3349(11.5929) | Bit/dim 1.1795(1.1829) | Xent 0.1079(0.0911) | Loss 1.2334(1.2285) | Error 0.0311(0.0287) Steps 560(555.30) | Grad Norm 2.3828(4.6844) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0047 | Time 50.3637, Epoch Time 829.6180(757.1429), Bit/dim 1.1635(best: 1.1766), Xent 0.0432, Loss 1.1851, Error 0.0129(best: 0.0148)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3110 | Time 11.4299(11.6014) | Bit/dim 1.1726(1.1809) | Xent 0.0848(0.0886) | Loss 1.2150(1.2252) | Error 0.0289(0.0279) Steps 560(555.80) | Grad Norm 4.7086(4.5893) | Total Time 10.00(10.00)\n",
      "Iter 3120 | Time 11.7107(11.5849) | Bit/dim 1.1693(1.1780) | Xent 0.1037(0.0896) | Loss 1.2211(1.2228) | Error 0.0311(0.0278) Steps 560(555.95) | Grad Norm 7.7295(4.5550) | Total Time 10.00(10.00)\n",
      "Iter 3130 | Time 12.6950(11.6414) | Bit/dim 1.2593(1.1842) | Xent 0.1083(0.0888) | Loss 1.3135(1.2286) | Error 0.0289(0.0280) Steps 590(557.47) | Grad Norm 43.0046(7.2284) | Total Time 10.00(10.00)\n",
      "Iter 3140 | Time 11.9190(11.6498) | Bit/dim 1.2105(1.2020) | Xent 0.1022(0.0915) | Loss 1.2616(1.2478) | Error 0.0300(0.0283) Steps 560(557.39) | Grad Norm 9.1992(8.4307) | Total Time 10.00(10.00)\n",
      "Iter 3150 | Time 12.2776(11.6947) | Bit/dim 1.2020(1.2044) | Xent 0.0757(0.0937) | Loss 1.2398(1.2513) | Error 0.0222(0.0293) Steps 560(557.30) | Grad Norm 13.9263(9.3998) | Total Time 10.00(10.00)\n",
      "Iter 3160 | Time 11.4557(11.6970) | Bit/dim 1.1936(1.2023) | Xent 0.1058(0.0927) | Loss 1.2465(1.2486) | Error 0.0267(0.0293) Steps 554(557.19) | Grad Norm 9.1504(9.6184) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0048 | Time 51.0208, Epoch Time 836.5534(759.5252), Bit/dim 1.1838(best: 1.1635), Xent 0.0412, Loss 1.2044, Error 0.0144(best: 0.0129)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3170 | Time 11.4000(11.6649) | Bit/dim 1.1821(1.1967) | Xent 0.0837(0.0922) | Loss 1.2239(1.2428) | Error 0.0300(0.0287) Steps 554(557.10) | Grad Norm 6.5365(9.5347) | Total Time 10.00(10.00)\n",
      "Iter 3180 | Time 11.5637(11.6900) | Bit/dim 1.1643(1.1932) | Xent 0.0977(0.0901) | Loss 1.2132(1.2382) | Error 0.0322(0.0279) Steps 554(557.51) | Grad Norm 11.3757(10.1485) | Total Time 10.00(10.00)\n",
      "Iter 3190 | Time 11.9173(11.6736) | Bit/dim 1.2047(1.1902) | Xent 0.0908(0.0895) | Loss 1.2501(1.2350) | Error 0.0344(0.0278) Steps 554(556.87) | Grad Norm 10.4657(9.3767) | Total Time 10.00(10.00)\n",
      "Iter 3200 | Time 11.8251(11.7184) | Bit/dim 1.2544(1.1984) | Xent 0.0988(0.0906) | Loss 1.3038(1.2437) | Error 0.0322(0.0282) Steps 560(557.98) | Grad Norm 16.7040(11.2662) | Total Time 10.00(10.00)\n",
      "Iter 3210 | Time 11.9718(11.7251) | Bit/dim 1.2700(1.2109) | Xent 0.0859(0.0911) | Loss 1.3129(1.2564) | Error 0.0233(0.0284) Steps 554(557.76) | Grad Norm 15.8998(12.3199) | Total Time 10.00(10.00)\n",
      "Iter 3220 | Time 11.2969(11.7474) | Bit/dim 1.2198(1.2098) | Xent 0.1091(0.0897) | Loss 1.2743(1.2547) | Error 0.0367(0.0281) Steps 554(558.29) | Grad Norm 23.6043(12.6034) | Total Time 10.00(10.00)\n",
      "Iter 3230 | Time 10.8167(11.7592) | Bit/dim 1.2447(1.2092) | Xent 0.0802(0.0883) | Loss 1.2848(1.2533) | Error 0.0200(0.0273) Steps 536(558.26) | Grad Norm 15.1235(12.7355) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0049 | Time 51.1817, Epoch Time 842.1378(762.0035), Bit/dim 1.1893(best: 1.1635), Xent 0.0431, Loss 1.2109, Error 0.0135(best: 0.0129)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3240 | Time 11.5512(11.7425) | Bit/dim 1.1855(1.2046) | Xent 0.0872(0.0906) | Loss 1.2291(1.2499) | Error 0.0267(0.0277) Steps 560(558.54) | Grad Norm 11.9687(12.0611) | Total Time 10.00(10.00)\n",
      "Iter 3250 | Time 11.3907(11.6675) | Bit/dim 1.1808(1.1978) | Xent 0.0779(0.0878) | Loss 1.2198(1.2417) | Error 0.0233(0.0270) Steps 560(557.66) | Grad Norm 4.5050(10.4439) | Total Time 10.00(10.00)\n",
      "Iter 3260 | Time 11.8140(11.6598) | Bit/dim 1.1731(1.1922) | Xent 0.0725(0.0888) | Loss 1.2094(1.2366) | Error 0.0167(0.0268) Steps 554(557.00) | Grad Norm 7.5812(9.4174) | Total Time 10.00(10.00)\n",
      "Iter 3270 | Time 11.3191(11.6189) | Bit/dim 1.1580(1.1864) | Xent 0.0430(0.0859) | Loss 1.1796(1.2293) | Error 0.0178(0.0265) Steps 560(557.17) | Grad Norm 2.9136(7.8601) | Total Time 10.00(10.00)\n",
      "Iter 3280 | Time 11.6625(11.6135) | Bit/dim 1.1714(1.1812) | Xent 0.0730(0.0860) | Loss 1.2079(1.2242) | Error 0.0244(0.0268) Steps 554(557.24) | Grad Norm 5.4254(6.7734) | Total Time 10.00(10.00)\n",
      "Iter 3290 | Time 11.5719(11.6109) | Bit/dim 1.1607(1.1781) | Xent 0.0879(0.0865) | Loss 1.2047(1.2214) | Error 0.0322(0.0276) Steps 554(556.39) | Grad Norm 4.5688(5.8530) | Total Time 10.00(10.00)\n",
      "Iter 3300 | Time 12.2011(11.6115) | Bit/dim 1.1702(1.1762) | Xent 0.0828(0.0867) | Loss 1.2116(1.2196) | Error 0.0233(0.0271) Steps 554(556.69) | Grad Norm 2.0948(5.2107) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0050 | Time 50.2452, Epoch Time 829.5138(764.0289), Bit/dim 1.1648(best: 1.1635), Xent 0.0463, Loss 1.1880, Error 0.0154(best: 0.0129)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3310 | Time 12.0262(11.6065) | Bit/dim 1.1754(1.1739) | Xent 0.0540(0.0847) | Loss 1.2024(1.2162) | Error 0.0189(0.0269) Steps 572(557.33) | Grad Norm 10.1565(5.0921) | Total Time 10.00(10.00)\n",
      "Iter 3320 | Time 11.8166(11.6276) | Bit/dim 1.1732(1.1724) | Xent 0.0863(0.0860) | Loss 1.2163(1.2154) | Error 0.0256(0.0264) Steps 560(558.35) | Grad Norm 7.9308(5.6259) | Total Time 10.00(10.00)\n",
      "Iter 3330 | Time 11.6581(11.6479) | Bit/dim 1.1665(1.1697) | Xent 0.0907(0.0854) | Loss 1.2118(1.2123) | Error 0.0278(0.0263) Steps 554(557.83) | Grad Norm 2.5136(5.3603) | Total Time 10.00(10.00)\n",
      "Iter 3340 | Time 11.5779(11.6357) | Bit/dim 1.1822(1.1686) | Xent 0.1102(0.0832) | Loss 1.2373(1.2102) | Error 0.0356(0.0259) Steps 554(558.05) | Grad Norm 8.0547(5.0335) | Total Time 10.00(10.00)\n",
      "Iter 3350 | Time 11.9101(11.6521) | Bit/dim 1.2686(1.1741) | Xent 0.0583(0.0849) | Loss 1.2977(1.2166) | Error 0.0167(0.0259) Steps 560(557.81) | Grad Norm 21.1597(7.1739) | Total Time 10.00(10.00)\n",
      "Iter 3360 | Time 12.0042(11.7537) | Bit/dim 1.3555(1.2059) | Xent 0.1245(0.0869) | Loss 1.4178(1.2493) | Error 0.0433(0.0263) Steps 566(560.38) | Grad Norm 8.3254(9.9259) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0051 | Time 51.5428, Epoch Time 840.3127(766.3174), Bit/dim 1.2188(best: 1.1635), Xent 0.0412, Loss 1.2394, Error 0.0129(best: 0.0129)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3370 | Time 11.4339(11.7701) | Bit/dim 1.1925(1.2167) | Xent 0.1276(0.0891) | Loss 1.2562(1.2613) | Error 0.0300(0.0265) Steps 560(562.67) | Grad Norm 5.4455(8.7508) | Total Time 10.00(10.00)\n",
      "Iter 3380 | Time 11.5148(11.7413) | Bit/dim 1.1676(1.2090) | Xent 0.0647(0.0872) | Loss 1.1999(1.2526) | Error 0.0222(0.0263) Steps 554(561.24) | Grad Norm 2.1586(7.4441) | Total Time 10.00(10.00)\n",
      "Iter 3390 | Time 11.9462(11.7253) | Bit/dim 1.1704(1.1998) | Xent 0.0813(0.0871) | Loss 1.2111(1.2433) | Error 0.0222(0.0261) Steps 560(560.51) | Grad Norm 2.3626(6.2774) | Total Time 10.00(10.00)\n",
      "Iter 3400 | Time 11.5763(11.7040) | Bit/dim 1.1666(1.1912) | Xent 0.0931(0.0861) | Loss 1.2132(1.2343) | Error 0.0256(0.0255) Steps 560(560.23) | Grad Norm 2.3245(5.3772) | Total Time 10.00(10.00)\n",
      "Iter 3410 | Time 11.6778(11.7109) | Bit/dim 1.1568(1.1835) | Xent 0.0593(0.0863) | Loss 1.1865(1.2266) | Error 0.0189(0.0253) Steps 554(558.97) | Grad Norm 2.2720(4.5296) | Total Time 10.00(10.00)\n",
      "Iter 3420 | Time 11.9398(11.7226) | Bit/dim 1.1793(1.1789) | Xent 0.1149(0.0860) | Loss 1.2368(1.2219) | Error 0.0333(0.0255) Steps 566(558.38) | Grad Norm 1.4922(4.4440) | Total Time 10.00(10.00)\n",
      "Iter 3430 | Time 11.9125(11.7499) | Bit/dim 1.1587(1.1760) | Xent 0.0831(0.0831) | Loss 1.2003(1.2175) | Error 0.0289(0.0253) Steps 560(559.28) | Grad Norm 8.4548(5.1967) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0052 | Time 51.1402, Epoch Time 840.2773(768.5362), Bit/dim 1.1554(best: 1.1635), Xent 0.0384, Loss 1.1747, Error 0.0119(best: 0.0129)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3440 | Time 12.0326(11.7601) | Bit/dim 1.1754(1.1727) | Xent 0.1000(0.0844) | Loss 1.2254(1.2149) | Error 0.0333(0.0258) Steps 566(559.46) | Grad Norm 12.7480(5.6814) | Total Time 10.00(10.00)\n",
      "Iter 3450 | Time 11.9085(11.7622) | Bit/dim 1.4377(1.2078) | Xent 0.0792(0.0822) | Loss 1.4773(1.2489) | Error 0.0256(0.0255) Steps 560(560.49) | Grad Norm 7.5173(8.3369) | Total Time 10.00(10.00)\n",
      "Iter 3460 | Time 11.9992(11.7749) | Bit/dim 1.2344(1.2348) | Xent 0.0740(0.0845) | Loss 1.2715(1.2771) | Error 0.0222(0.0266) Steps 572(561.83) | Grad Norm 7.3626(7.7191) | Total Time 10.00(10.00)\n",
      "Iter 3470 | Time 11.0410(11.7328) | Bit/dim 1.2932(1.2340) | Xent 0.0788(0.0847) | Loss 1.3326(1.2763) | Error 0.0278(0.0271) Steps 548(561.14) | Grad Norm 18.7444(9.4452) | Total Time 10.00(10.00)\n",
      "Iter 3480 | Time 11.3993(11.6710) | Bit/dim 1.2620(1.2400) | Xent 0.1212(0.0899) | Loss 1.3226(1.2850) | Error 0.0289(0.0281) Steps 542(558.02) | Grad Norm 14.7219(11.1600) | Total Time 10.00(10.00)\n",
      "Iter 3490 | Time 11.6490(11.6773) | Bit/dim 1.1742(1.2316) | Xent 0.0786(0.0913) | Loss 1.2135(1.2773) | Error 0.0289(0.0285) Steps 566(559.73) | Grad Norm 3.0909(11.0501) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0053 | Time 50.7068, Epoch Time 838.6549(770.6397), Bit/dim 1.2042(best: 1.1554), Xent 0.0408, Loss 1.2246, Error 0.0136(best: 0.0119)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3500 | Time 11.3174(11.7096) | Bit/dim 1.1778(1.2234) | Xent 0.1091(0.0922) | Loss 1.2323(1.2695) | Error 0.0311(0.0286) Steps 566(560.28) | Grad Norm 5.2833(11.4983) | Total Time 10.00(10.00)\n",
      "Iter 3510 | Time 11.4738(11.7084) | Bit/dim 1.1749(1.2128) | Xent 0.0839(0.0914) | Loss 1.2169(1.2585) | Error 0.0233(0.0285) Steps 566(561.13) | Grad Norm 4.0711(10.5051) | Total Time 10.00(10.00)\n",
      "Iter 3520 | Time 11.4932(11.6708) | Bit/dim 1.1845(1.2026) | Xent 0.0974(0.0892) | Loss 1.2332(1.2472) | Error 0.0233(0.0276) Steps 560(561.27) | Grad Norm 4.3232(9.2749) | Total Time 10.00(10.00)\n",
      "Iter 3530 | Time 11.8521(11.6636) | Bit/dim 1.1800(1.1929) | Xent 0.0840(0.0863) | Loss 1.2220(1.2361) | Error 0.0289(0.0268) Steps 554(560.14) | Grad Norm 2.9002(7.8958) | Total Time 10.00(10.00)\n",
      "Iter 3540 | Time 11.7072(11.6539) | Bit/dim 1.1542(1.1835) | Xent 0.0923(0.0867) | Loss 1.2003(1.2269) | Error 0.0322(0.0269) Steps 560(561.18) | Grad Norm 1.5706(6.4030) | Total Time 10.00(10.00)\n",
      "Iter 3550 | Time 11.7236(11.6739) | Bit/dim 1.1755(1.1781) | Xent 0.1070(0.0837) | Loss 1.2290(1.2200) | Error 0.0322(0.0261) Steps 560(561.45) | Grad Norm 2.4536(5.5204) | Total Time 10.00(10.00)\n",
      "Iter 3560 | Time 11.6409(11.6819) | Bit/dim 1.1606(1.1737) | Xent 0.0896(0.0817) | Loss 1.2054(1.2146) | Error 0.0256(0.0256) Steps 566(561.68) | Grad Norm 8.9346(5.2255) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0054 | Time 51.2910, Epoch Time 837.4725(772.6447), Bit/dim 1.1684(best: 1.1554), Xent 0.0357, Loss 1.1863, Error 0.0107(best: 0.0119)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3570 | Time 12.3948(11.7381) | Bit/dim 1.2279(1.1840) | Xent 0.1053(0.0793) | Loss 1.2806(1.2236) | Error 0.0289(0.0248) Steps 584(563.59) | Grad Norm 25.4841(8.1913) | Total Time 10.00(10.00)\n",
      "Iter 3580 | Time 11.6102(11.7541) | Bit/dim 1.1530(1.1825) | Xent 0.0536(0.0797) | Loss 1.1798(1.2223) | Error 0.0200(0.0252) Steps 566(565.12) | Grad Norm 7.3593(8.0381) | Total Time 10.00(10.00)\n",
      "Iter 3590 | Time 10.9452(11.6721) | Bit/dim 1.1655(1.1779) | Xent 0.0539(0.0780) | Loss 1.1924(1.2169) | Error 0.0189(0.0247) Steps 548(563.03) | Grad Norm 5.2089(7.1591) | Total Time 10.00(10.00)\n",
      "Iter 3600 | Time 11.9533(11.6814) | Bit/dim 1.1654(1.1732) | Xent 0.0572(0.0743) | Loss 1.1940(1.2103) | Error 0.0200(0.0238) Steps 572(563.20) | Grad Norm 9.7284(6.7629) | Total Time 10.00(10.00)\n",
      "Iter 3610 | Time 12.0230(11.7068) | Bit/dim 1.2408(1.1810) | Xent 0.0657(0.0758) | Loss 1.2737(1.2189) | Error 0.0178(0.0238) Steps 572(563.52) | Grad Norm 16.8090(9.2059) | Total Time 10.00(10.00)\n",
      "Iter 3620 | Time 11.3590(11.7301) | Bit/dim 1.3077(1.2008) | Xent 0.0700(0.0760) | Loss 1.3427(1.2388) | Error 0.0256(0.0234) Steps 548(563.83) | Grad Norm 16.0486(11.3406) | Total Time 10.00(10.00)\n",
      "Iter 3630 | Time 12.2547(11.7604) | Bit/dim 1.1875(1.2037) | Xent 0.1062(0.0788) | Loss 1.2406(1.2431) | Error 0.0322(0.0242) Steps 572(563.60) | Grad Norm 8.6342(11.6156) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0055 | Time 51.5407, Epoch Time 840.9468(774.6938), Bit/dim 1.1740(best: 1.1554), Xent 0.0478, Loss 1.1979, Error 0.0159(best: 0.0107)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3640 | Time 11.6461(11.7673) | Bit/dim 1.1678(1.1982) | Xent 0.0571(0.0799) | Loss 1.1964(1.2381) | Error 0.0189(0.0248) Steps 572(564.68) | Grad Norm 2.3455(10.8698) | Total Time 10.00(10.00)\n",
      "Iter 3650 | Time 11.6667(11.7792) | Bit/dim 1.1811(1.1914) | Xent 0.0834(0.0786) | Loss 1.2228(1.2307) | Error 0.0233(0.0245) Steps 560(564.48) | Grad Norm 15.0536(9.9459) | Total Time 10.00(10.00)\n",
      "Iter 3660 | Time 11.5460(11.7754) | Bit/dim 1.1667(1.1866) | Xent 0.0864(0.0793) | Loss 1.2099(1.2262) | Error 0.0278(0.0246) Steps 560(565.01) | Grad Norm 7.1037(9.7984) | Total Time 10.00(10.00)\n",
      "Iter 3670 | Time 11.8223(11.7826) | Bit/dim 1.2245(1.1881) | Xent 0.0451(0.0786) | Loss 1.2471(1.2274) | Error 0.0111(0.0239) Steps 548(564.31) | Grad Norm 14.7803(10.9328) | Total Time 10.00(10.00)\n",
      "Iter 3680 | Time 12.0863(11.8098) | Bit/dim 1.1917(1.1916) | Xent 0.1020(0.0780) | Loss 1.2426(1.2306) | Error 0.0311(0.0238) Steps 572(564.53) | Grad Norm 26.3482(12.0431) | Total Time 10.00(10.00)\n",
      "Iter 3690 | Time 12.1496(11.8046) | Bit/dim 1.1639(1.1870) | Xent 0.0609(0.0765) | Loss 1.1943(1.2252) | Error 0.0233(0.0237) Steps 572(565.43) | Grad Norm 9.5497(11.3297) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0056 | Time 51.0894, Epoch Time 844.8314(776.7979), Bit/dim 1.1610(best: 1.1554), Xent 0.0359, Loss 1.1790, Error 0.0115(best: 0.0107)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3700 | Time 11.3972(11.7902) | Bit/dim 1.1513(1.1804) | Xent 0.0861(0.0776) | Loss 1.1943(1.2192) | Error 0.0233(0.0234) Steps 554(565.32) | Grad Norm 3.7538(10.3656) | Total Time 10.00(10.00)\n",
      "Iter 3710 | Time 11.8310(11.8025) | Bit/dim 1.1588(1.1759) | Xent 0.0757(0.0761) | Loss 1.1967(1.2139) | Error 0.0233(0.0231) Steps 560(565.79) | Grad Norm 3.3990(9.0843) | Total Time 10.00(10.00)\n",
      "Iter 3720 | Time 11.8058(11.7680) | Bit/dim 1.1626(1.1711) | Xent 0.0698(0.0739) | Loss 1.1975(1.2081) | Error 0.0222(0.0226) Steps 554(565.18) | Grad Norm 5.4734(7.8735) | Total Time 10.00(10.00)\n",
      "Iter 3730 | Time 12.2963(11.7679) | Bit/dim 1.1626(1.1690) | Xent 0.0454(0.0716) | Loss 1.1853(1.2048) | Error 0.0156(0.0223) Steps 578(565.29) | Grad Norm 9.7317(8.2868) | Total Time 10.00(10.00)\n",
      "Iter 3740 | Time 11.4464(11.7800) | Bit/dim 1.1607(1.1674) | Xent 0.0858(0.0725) | Loss 1.2036(1.2036) | Error 0.0300(0.0230) Steps 566(565.79) | Grad Norm 2.9060(8.1542) | Total Time 10.00(10.00)\n",
      "Iter 3750 | Time 11.7306(11.8122) | Bit/dim 1.1684(1.1688) | Xent 0.0784(0.0721) | Loss 1.2076(1.2048) | Error 0.0244(0.0225) Steps 560(566.14) | Grad Norm 9.7715(9.0158) | Total Time 10.00(10.00)\n",
      "Iter 3760 | Time 11.7664(11.8018) | Bit/dim 1.2474(1.1781) | Xent 0.0929(0.0731) | Loss 1.2938(1.2146) | Error 0.0278(0.0231) Steps 572(565.98) | Grad Norm 17.6002(10.8478) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0057 | Time 52.9700, Epoch Time 844.9186(778.8415), Bit/dim 1.2114(best: 1.1554), Xent 0.0379, Loss 1.2304, Error 0.0131(best: 0.0107)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3770 | Time 12.1062(11.7523) | Bit/dim 1.2124(1.1862) | Xent 0.0964(0.0735) | Loss 1.2606(1.2229) | Error 0.0267(0.0234) Steps 578(564.99) | Grad Norm 9.8413(11.8559) | Total Time 10.00(10.00)\n",
      "Iter 3780 | Time 12.0269(11.8081) | Bit/dim 1.1823(1.1839) | Xent 0.0809(0.0737) | Loss 1.2228(1.2208) | Error 0.0244(0.0229) Steps 566(566.05) | Grad Norm 18.0287(11.6571) | Total Time 10.00(10.00)\n",
      "Iter 3790 | Time 11.6798(11.8412) | Bit/dim 1.1971(1.1831) | Xent 0.0687(0.0736) | Loss 1.2315(1.2199) | Error 0.0200(0.0230) Steps 566(567.43) | Grad Norm 13.3369(11.8246) | Total Time 10.00(10.00)\n",
      "Iter 3800 | Time 11.4928(11.8353) | Bit/dim 1.1858(1.1832) | Xent 0.0692(0.0740) | Loss 1.2204(1.2202) | Error 0.0156(0.0229) Steps 560(567.95) | Grad Norm 16.9470(12.0075) | Total Time 10.00(10.00)\n",
      "Iter 3810 | Time 11.8146(11.8474) | Bit/dim 1.1511(1.1789) | Xent 0.0596(0.0725) | Loss 1.1809(1.2152) | Error 0.0178(0.0231) Steps 572(568.35) | Grad Norm 8.5940(11.0084) | Total Time 10.00(10.00)\n",
      "Iter 3820 | Time 12.0084(11.8503) | Bit/dim 1.1612(1.1741) | Xent 0.0758(0.0736) | Loss 1.1991(1.2109) | Error 0.0189(0.0233) Steps 572(568.31) | Grad Norm 5.8242(9.7775) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0058 | Time 51.7028, Epoch Time 850.0849(780.9788), Bit/dim 1.1506(best: 1.1554), Xent 0.0387, Loss 1.1700, Error 0.0141(best: 0.0107)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3830 | Time 12.2263(11.8620) | Bit/dim 1.1622(1.1711) | Xent 0.0779(0.0734) | Loss 1.2012(1.2078) | Error 0.0233(0.0231) Steps 566(568.30) | Grad Norm 4.9592(8.6976) | Total Time 10.00(10.00)\n",
      "Iter 3840 | Time 11.9127(11.8652) | Bit/dim 1.1437(1.1660) | Xent 0.0467(0.0726) | Loss 1.1670(1.2023) | Error 0.0167(0.0227) Steps 566(566.72) | Grad Norm 2.6094(7.1395) | Total Time 10.00(10.00)\n",
      "Iter 3850 | Time 11.8191(11.8455) | Bit/dim 1.1581(1.1629) | Xent 0.0961(0.0738) | Loss 1.2062(1.1998) | Error 0.0256(0.0229) Steps 572(566.92) | Grad Norm 3.2802(6.0022) | Total Time 10.00(10.00)\n",
      "Iter 3860 | Time 11.6252(11.8202) | Bit/dim 1.1566(1.1604) | Xent 0.0718(0.0730) | Loss 1.1925(1.1969) | Error 0.0211(0.0227) Steps 566(566.53) | Grad Norm 2.9277(5.2237) | Total Time 10.00(10.00)\n",
      "Iter 3870 | Time 11.7800(11.8009) | Bit/dim 1.1524(1.1577) | Xent 0.0662(0.0737) | Loss 1.1855(1.1945) | Error 0.0267(0.0231) Steps 572(565.74) | Grad Norm 2.4420(4.4386) | Total Time 10.00(10.00)\n",
      "Iter 3880 | Time 11.9570(11.8226) | Bit/dim 1.1531(1.1550) | Xent 0.0465(0.0727) | Loss 1.1764(1.1913) | Error 0.0133(0.0225) Steps 566(567.63) | Grad Norm 1.8026(4.1021) | Total Time 10.00(10.00)\n",
      "Iter 3890 | Time 11.8630(11.8060) | Bit/dim 1.1641(1.1539) | Xent 0.0494(0.0734) | Loss 1.1888(1.1906) | Error 0.0167(0.0223) Steps 572(566.23) | Grad Norm 6.4328(4.0642) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0059 | Time 51.0138, Epoch Time 845.1040(782.9026), Bit/dim 1.1458(best: 1.1506), Xent 0.0339, Loss 1.1627, Error 0.0121(best: 0.0107)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3900 | Time 12.1689(11.8454) | Bit/dim 1.1464(1.1509) | Xent 0.0654(0.0720) | Loss 1.1791(1.1869) | Error 0.0211(0.0220) Steps 584(567.79) | Grad Norm 5.1976(4.2643) | Total Time 10.00(10.00)\n",
      "Iter 3910 | Time 11.8733(11.8337) | Bit/dim 1.1697(1.1517) | Xent 0.0581(0.0708) | Loss 1.1988(1.1871) | Error 0.0222(0.0218) Steps 572(568.09) | Grad Norm 11.7942(4.9261) | Total Time 10.00(10.00)\n",
      "Iter 3920 | Time 11.4635(11.8856) | Bit/dim 1.3531(1.1671) | Xent 0.0507(0.0733) | Loss 1.3784(1.2038) | Error 0.0189(0.0230) Steps 554(568.82) | Grad Norm 16.6812(8.3926) | Total Time 10.00(10.00)\n",
      "Iter 3930 | Time 11.5133(11.8065) | Bit/dim 1.2860(1.2142) | Xent 0.0979(0.0754) | Loss 1.3350(1.2519) | Error 0.0300(0.0237) Steps 560(567.52) | Grad Norm 6.0947(7.8522) | Total Time 10.00(10.00)\n",
      "Iter 3940 | Time 11.8596(11.7811) | Bit/dim 1.2441(1.2139) | Xent 0.0515(0.0764) | Loss 1.2699(1.2521) | Error 0.0167(0.0243) Steps 566(568.03) | Grad Norm 13.1406(8.3386) | Total Time 10.00(10.00)\n",
      "Iter 3950 | Time 11.0694(11.7455) | Bit/dim 1.1884(1.2110) | Xent 0.0692(0.0805) | Loss 1.2230(1.2512) | Error 0.0189(0.0252) Steps 548(567.29) | Grad Norm 9.2248(9.9221) | Total Time 10.00(10.00)\n",
      "Iter 3960 | Time 11.7504(11.7761) | Bit/dim 1.1543(1.2025) | Xent 0.0907(0.0783) | Loss 1.1996(1.2417) | Error 0.0200(0.0247) Steps 560(567.44) | Grad Norm 7.3141(9.5548) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0060 | Time 50.6856, Epoch Time 844.0986(784.7385), Bit/dim 1.1577(best: 1.1458), Xent 0.0332, Loss 1.1743, Error 0.0122(best: 0.0107)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3970 | Time 11.9214(11.7759) | Bit/dim 1.1438(1.1936) | Xent 0.0682(0.0772) | Loss 1.1778(1.2322) | Error 0.0211(0.0245) Steps 578(567.15) | Grad Norm 6.9486(9.3659) | Total Time 10.00(10.00)\n",
      "Iter 3980 | Time 12.3726(11.8014) | Bit/dim 1.1748(1.1877) | Xent 0.0752(0.0749) | Loss 1.2124(1.2251) | Error 0.0200(0.0235) Steps 578(568.12) | Grad Norm 10.5203(10.2776) | Total Time 10.00(10.00)\n",
      "Iter 3990 | Time 11.8335(11.7599) | Bit/dim 1.1721(1.1863) | Xent 0.0554(0.0742) | Loss 1.1998(1.2235) | Error 0.0167(0.0228) Steps 560(566.76) | Grad Norm 15.9545(11.1143) | Total Time 10.00(10.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle.py --data mnist --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/cnf_conditional_disentangle_bs900_sratio_0_5_drop_0_5_run1 --seed 1 --lr 0.001 --conditional True --controlled_tol False --train_mode semisup --log_freq 10 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
