{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_rl.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "GATES = [\"cnn1\", \"cnn2\", \"rnn\"]\n",
      "\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--gate', type=str, default='cnn1', choices=GATES)\n",
      "parser.add_argument('--scale', type=float, default=1.0)\n",
      "parser.add_argument('--scale_fac', type=float, default=1.0)\n",
      "parser.add_argument('--scale_std', type=float, default=1.0)\n",
      "parser.add_argument('--eta', default=0.1, type=float,\n",
      "                        help='tuning parameter that allows us to trade-off the competing goals of' \n",
      "                                'minimizing the prediction loss and maximizing the gate rewards ')\n",
      "parser.add_argument('--rl-weight', default=0.01, type=float,\n",
      "                        help='rl weight')\n",
      "\n",
      "parser.add_argument('--gamma', default=0.99, type=float,\n",
      "                        help='discount factor, default: (0.99)')\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "import lib.odenvp_conditional_rl as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol, \"scale\": args.scale, \"scale_fac\": args.scale_fac, \"scale_std\": args.scale_std, \"gate\": args.gate},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            # re-weight the gate rewards\n",
      "            normalized_eta = args.eta / len(logp_actions)\n",
      "            \n",
      "            # collect cumulative future rewards\n",
      "            R = - loss\n",
      "            cum_rewards = []\n",
      "            for r in nfe[::-1]:\n",
      "                R = -normalized_eta * r.view(-1,1) + args.gamma * R\n",
      "                cum_rewards.insert(0,R)\n",
      "            \n",
      "            # apply REINFORCE\n",
      "            rl_loss = 0\n",
      "            for lpa, r in zip(logp_actions, cum_rewards):\n",
      "                rl_loss = rl_loss - lpa.view(-1,1) * args.rl_weight * r\n",
      "                \n",
      "            loss = loss + rl_loss.mean()\n",
      "            \n",
      "            loss.backward()\n",
      "            \n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "            \n",
      "            for idx in range(len(model.module.transforms)):\n",
      "                for layer in model.module.transforms[idx].chain:\n",
      "                    if hasattr(layer, 'atol'):\n",
      "                        layer.odefunc.after_odeint()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'train': atol[tol_indx].mean()}, itr)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'train': rtol[tol_indx].mean()}, itr)\n",
      "                    \n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples, atol, rtol, logp_actions, nfe = model(fixed_z, reverse=True)\n",
      "            generated_samples = generated_samples.view(-1, *data_shape)\n",
      "            for tol_indx in range(len(atol)):\n",
      "                writer.add_scalars('atol_gen_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                writer.add_scalars('rtol_gen_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=0.0001, autoencode=False, batch_norm=False, batch_size=8000, batch_size_schedule='', begin_epoch=1, condition_ratio=0.5, conditional=True, controlled_tol=True, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, eta=0.1, gamma=0.99, gate='cnn1', imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=1, lr=0.01, max_grad_norm=20.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume='../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_rl_stdscale_3_run1/epoch_86_checkpt.pth', rl_weight=0.01, rtol=0.0001, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_rl_stdscale_3_run1', scale=1.0, scale_fac=1.0, scale_std=3.0, seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=5000, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000.0, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1450732\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc_rl.py:291: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Iter 0517 | Time 119.5119(72.9342) | Bit/dim 3.9130(3.9783) | Xent 1.4150(1.5240) | Loss 13.6761(11.9543) | Error 0.5146(0.5485) Steps 0(0.00) | Grad Norm 6.0502(8.6745) | Total Time 0.00(0.00)\n",
      "Iter 0518 | Time 72.6033(72.9243) | Bit/dim 3.9141(3.9763) | Xent 1.3968(1.5202) | Loss 11.0103(11.9260) | Error 0.5054(0.5472) Steps 0(0.00) | Grad Norm 7.0501(8.6258) | Total Time 0.00(0.00)\n",
      "Iter 0519 | Time 63.1613(72.6314) | Bit/dim 3.9115(3.9744) | Xent 1.4452(1.5180) | Loss 11.1226(11.9019) | Error 0.5255(0.5466) Steps 0(0.00) | Grad Norm 8.3776(8.6183) | Total Time 0.00(0.00)\n",
      "Iter 0520 | Time 60.1330(72.2564) | Bit/dim 3.9111(3.9725) | Xent 1.4712(1.5166) | Loss 10.7745(11.8681) | Error 0.5309(0.5461) Steps 0(0.00) | Grad Norm 11.8804(8.7162) | Total Time 0.00(0.00)\n",
      "Iter 0521 | Time 63.2636(71.9867) | Bit/dim 3.9224(3.9710) | Xent 1.5154(1.5165) | Loss 11.0452(11.8434) | Error 0.5464(0.5461) Steps 0(0.00) | Grad Norm 18.4327(9.0077) | Total Time 0.00(0.00)\n",
      "Iter 0522 | Time 71.8472(71.9825) | Bit/dim 3.9223(3.9695) | Xent 1.6044(1.5192) | Loss 11.2077(11.8243) | Error 0.5686(0.5468) Steps 0(0.00) | Grad Norm 16.9721(9.2466) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0087 | Time 42.7022, Epoch Time 512.1694(446.1081), Bit/dim 3.9283(best: inf), Xent 1.4227, Loss 4.6396, Error 0.5174(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0523 | Time 76.4353(72.1161) | Bit/dim 3.9329(3.9684) | Xent 1.4688(1.5177) | Loss 15.2028(11.9257) | Error 0.5284(0.5462) Steps 0(0.00) | Grad Norm 7.4571(9.1929) | Total Time 0.00(0.00)\n",
      "Iter 0524 | Time 65.3939(71.9144) | Bit/dim 3.9315(3.9673) | Xent 1.4445(1.5155) | Loss 11.0864(11.9005) | Error 0.5196(0.5454) Steps 0(0.00) | Grad Norm 11.1602(9.2520) | Total Time 0.00(0.00)\n",
      "Iter 0525 | Time 65.5660(71.7239) | Bit/dim 3.9192(3.9659) | Xent 1.4623(1.5139) | Loss 10.8646(11.8694) | Error 0.5305(0.5450) Steps 0(0.00) | Grad Norm 8.6656(9.2344) | Total Time 0.00(0.00)\n",
      "Iter 0526 | Time 67.7836(71.6057) | Bit/dim 3.9095(3.9642) | Xent 1.4947(1.5133) | Loss 10.9989(11.8433) | Error 0.5316(0.5446) Steps 0(0.00) | Grad Norm 10.7457(9.2797) | Total Time 0.00(0.00)\n",
      "Iter 0527 | Time 65.0616(71.4094) | Bit/dim 3.9391(3.9634) | Xent 1.5001(1.5129) | Loss 11.1261(11.8218) | Error 0.5528(0.5448) Steps 0(0.00) | Grad Norm 9.8317(9.2963) | Total Time 0.00(0.00)\n",
      "Iter 0528 | Time 65.8646(71.2431) | Bit/dim 3.8967(3.9614) | Xent 1.4334(1.5105) | Loss 10.9749(11.7964) | Error 0.5145(0.5439) Steps 0(0.00) | Grad Norm 4.4094(9.1497) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0088 | Time 25.1946, Epoch Time 447.3456(446.1452), Bit/dim 3.9065(best: 3.9283), Xent 1.4279, Loss 4.6205, Error 0.5159(best: 0.5174)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0529 | Time 65.1029(71.0589) | Bit/dim 3.8939(3.9594) | Xent 1.4898(1.5099) | Loss 14.7996(11.8865) | Error 0.5399(0.5438) Steps 0(0.00) | Grad Norm 8.5931(9.1330) | Total Time 0.00(0.00)\n",
      "Iter 0530 | Time 58.6657(70.6871) | Bit/dim 3.9135(3.9580) | Xent 1.4409(1.5078) | Loss 10.8908(11.8566) | Error 0.5169(0.5430) Steps 0(0.00) | Grad Norm 5.6494(9.0285) | Total Time 0.00(0.00)\n",
      "Iter 0531 | Time 58.1434(70.3107) | Bit/dim 3.9062(3.9565) | Xent 1.4183(1.5051) | Loss 10.6422(11.8202) | Error 0.5121(0.5421) Steps 0(0.00) | Grad Norm 4.7218(8.8993) | Total Time 0.00(0.00)\n",
      "Iter 0532 | Time 62.8717(70.0876) | Bit/dim 3.8804(3.9542) | Xent 1.4240(1.5027) | Loss 11.0617(11.7974) | Error 0.5204(0.5414) Steps 0(0.00) | Grad Norm 5.0706(8.7844) | Total Time 0.00(0.00)\n",
      "Iter 0533 | Time 65.1715(69.9401) | Bit/dim 3.9101(3.9529) | Xent 1.4321(1.5006) | Loss 10.9349(11.7715) | Error 0.5105(0.5405) Steps 0(0.00) | Grad Norm 7.7136(8.7523) | Total Time 0.00(0.00)\n",
      "Iter 0534 | Time 64.8420(69.7872) | Bit/dim 3.9047(3.9514) | Xent 1.4272(1.4984) | Loss 11.0963(11.7513) | Error 0.5145(0.5397) Steps 0(0.00) | Grad Norm 10.4510(8.8032) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0089 | Time 26.7974, Epoch Time 417.7566(445.2935), Bit/dim 3.8924(best: 3.9065), Xent 1.4173, Loss 4.6011, Error 0.5073(best: 0.5159)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0535 | Time 59.7528(69.4861) | Bit/dim 3.8999(3.9499) | Xent 1.4582(1.4972) | Loss 15.4789(11.8631) | Error 0.5186(0.5391) Steps 0(0.00) | Grad Norm 12.2481(8.9066) | Total Time 0.00(0.00)\n",
      "Iter 0536 | Time 60.5400(69.2177) | Bit/dim 3.8970(3.9483) | Xent 1.4537(1.4959) | Loss 11.0393(11.8384) | Error 0.5225(0.5386) Steps 0(0.00) | Grad Norm 13.6659(9.0494) | Total Time 0.00(0.00)\n",
      "Iter 0537 | Time 67.9306(69.1791) | Bit/dim 3.9101(3.9471) | Xent 1.4577(1.4947) | Loss 10.7544(11.8059) | Error 0.5328(0.5384) Steps 0(0.00) | Grad Norm 10.5342(9.0939) | Total Time 0.00(0.00)\n",
      "Iter 0538 | Time 62.7784(68.9871) | Bit/dim 3.8865(3.9453) | Xent 1.3966(1.4918) | Loss 11.0017(11.7817) | Error 0.5097(0.5375) Steps 0(0.00) | Grad Norm 5.1425(8.9754) | Total Time 0.00(0.00)\n",
      "Iter 0539 | Time 62.3557(68.7882) | Bit/dim 3.8780(3.9433) | Xent 1.4387(1.4902) | Loss 10.6273(11.7471) | Error 0.5191(0.5370) Steps 0(0.00) | Grad Norm 12.2127(9.0725) | Total Time 0.00(0.00)\n",
      "Iter 0540 | Time 65.0046(68.6747) | Bit/dim 3.8973(3.9419) | Xent 1.5169(1.4910) | Loss 11.1480(11.7291) | Error 0.5463(0.5373) Steps 0(0.00) | Grad Norm 14.0917(9.2231) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0090 | Time 25.6176, Epoch Time 420.0807(444.5372), Bit/dim 3.9073(best: 3.8924), Xent 1.3581, Loss 4.5864, Error 0.4892(best: 0.5073)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0541 | Time 60.2784(68.4228) | Bit/dim 3.8989(3.9406) | Xent 1.3923(1.4880) | Loss 15.1650(11.8322) | Error 0.5089(0.5364) Steps 0(0.00) | Grad Norm 7.9895(9.1860) | Total Time 0.00(0.00)\n",
      "Iter 0542 | Time 69.3480(68.4505) | Bit/dim 3.9002(3.9394) | Xent 1.4491(1.4869) | Loss 11.1959(11.8131) | Error 0.5202(0.5359) Steps 0(0.00) | Grad Norm 12.1112(9.2738) | Total Time 0.00(0.00)\n",
      "Iter 0543 | Time 66.5997(68.3950) | Bit/dim 3.9053(3.9384) | Xent 1.4368(1.4854) | Loss 11.0588(11.7905) | Error 0.5201(0.5355) Steps 0(0.00) | Grad Norm 11.5712(9.3427) | Total Time 0.00(0.00)\n",
      "Iter 0544 | Time 63.8844(68.2597) | Bit/dim 3.8904(3.9370) | Xent 1.4048(1.4829) | Loss 10.9837(11.7663) | Error 0.5049(0.5345) Steps 0(0.00) | Grad Norm 5.2282(9.2193) | Total Time 0.00(0.00)\n",
      "Iter 0545 | Time 62.4163(68.0844) | Bit/dim 3.8794(3.9352) | Xent 1.4064(1.4806) | Loss 11.0876(11.7459) | Error 0.5102(0.5338) Steps 0(0.00) | Grad Norm 5.3780(9.1040) | Total Time 0.00(0.00)\n",
      "Iter 0546 | Time 63.1406(67.9361) | Bit/dim 3.8894(3.9339) | Xent 1.4116(1.4786) | Loss 11.0135(11.7240) | Error 0.5131(0.5332) Steps 0(0.00) | Grad Norm 6.5241(9.0267) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0091 | Time 25.2704, Epoch Time 427.0674(444.0131), Bit/dim 3.8928(best: 3.8924), Xent 1.3814, Loss 4.5835, Error 0.5057(best: 0.4892)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0547 | Time 65.4117(67.8603) | Bit/dim 3.8898(3.9325) | Xent 1.4127(1.4766) | Loss 14.8190(11.8168) | Error 0.5114(0.5325) Steps 0(0.00) | Grad Norm 9.1880(9.0315) | Total Time 0.00(0.00)\n",
      "Iter 0548 | Time 59.6818(67.6150) | Bit/dim 3.9038(3.9317) | Xent 1.4674(1.4763) | Loss 11.0943(11.7951) | Error 0.5242(0.5323) Steps 0(0.00) | Grad Norm 13.9003(9.1776) | Total Time 0.00(0.00)\n",
      "Iter 0549 | Time 63.8515(67.5021) | Bit/dim 3.9354(3.9318) | Xent 1.6546(1.4817) | Loss 11.1449(11.7756) | Error 0.5770(0.5336) Steps 0(0.00) | Grad Norm 24.6738(9.6424) | Total Time 0.00(0.00)\n",
      "Iter 0550 | Time 56.2441(67.1643) | Bit/dim 3.9463(3.9322) | Xent 1.5539(1.4838) | Loss 11.1118(11.7557) | Error 0.5635(0.5345) Steps 0(0.00) | Grad Norm 14.0971(9.7761) | Total Time 0.00(0.00)\n",
      "Iter 0551 | Time 62.3651(67.0204) | Bit/dim 3.9189(3.9318) | Xent 1.4620(1.4832) | Loss 11.0928(11.7358) | Error 0.5296(0.5344) Steps 0(0.00) | Grad Norm 5.9680(9.6618) | Total Time 0.00(0.00)\n",
      "Iter 0552 | Time 66.9477(67.0182) | Bit/dim 3.9217(3.9315) | Xent 1.4749(1.4829) | Loss 11.0249(11.7145) | Error 0.5356(0.5344) Steps 0(0.00) | Grad Norm 8.7359(9.6341) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0092 | Time 25.2734, Epoch Time 415.5072(443.1579), Bit/dim 3.9228(best: 3.8924), Xent 1.4393, Loss 4.6424, Error 0.5272(best: 0.4892)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0553 | Time 56.3253(66.6974) | Bit/dim 3.9185(3.9311) | Xent 1.4655(1.4824) | Loss 14.2978(11.7920) | Error 0.5341(0.5344) Steps 0(0.00) | Grad Norm 8.8358(9.6101) | Total Time 0.00(0.00)\n",
      "Iter 0554 | Time 65.1389(66.6506) | Bit/dim 3.9159(3.9307) | Xent 1.4497(1.4814) | Loss 10.9029(11.7653) | Error 0.5325(0.5344) Steps 0(0.00) | Grad Norm 5.8176(9.4963) | Total Time 0.00(0.00)\n",
      "Iter 0555 | Time 64.5943(66.5889) | Bit/dim 3.8940(3.9296) | Xent 1.4594(1.4808) | Loss 10.6816(11.7328) | Error 0.5252(0.5341) Steps 0(0.00) | Grad Norm 8.0061(9.4516) | Total Time 0.00(0.00)\n",
      "Iter 0556 | Time 59.2185(66.3678) | Bit/dim 3.9309(3.9296) | Xent 1.4884(1.4810) | Loss 11.0392(11.7120) | Error 0.5373(0.5342) Steps 0(0.00) | Grad Norm 15.6549(9.6377) | Total Time 0.00(0.00)\n",
      "Iter 0557 | Time 57.1148(66.0902) | Bit/dim 3.9380(3.9299) | Xent 1.4790(1.4809) | Loss 11.1622(11.6955) | Error 0.5304(0.5341) Steps 0(0.00) | Grad Norm 16.0786(9.8310) | Total Time 0.00(0.00)\n",
      "Iter 0558 | Time 61.2116(65.9439) | Bit/dim 3.9053(3.9291) | Xent 1.4514(1.4800) | Loss 10.9970(11.6746) | Error 0.5291(0.5339) Steps 0(0.00) | Grad Norm 9.1762(9.8113) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0093 | Time 25.4560, Epoch Time 404.9683(442.0122), Bit/dim 3.9198(best: 3.8924), Xent 1.3721, Loss 4.6058, Error 0.4973(best: 0.4892)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0559 | Time 62.7810(65.8490) | Bit/dim 3.9222(3.9289) | Xent 1.4304(1.4786) | Loss 14.8725(11.7705) | Error 0.5154(0.5334) Steps 0(0.00) | Grad Norm 9.1742(9.7922) | Total Time 0.00(0.00)\n",
      "Iter 0560 | Time 59.6893(65.6642) | Bit/dim 3.8859(3.9276) | Xent 1.4431(1.4775) | Loss 10.9850(11.7469) | Error 0.5200(0.5330) Steps 0(0.00) | Grad Norm 10.6843(9.8190) | Total Time 0.00(0.00)\n",
      "Iter 0561 | Time 59.5721(65.4814) | Bit/dim 3.9034(3.9269) | Xent 1.4028(1.4753) | Loss 11.1963(11.7304) | Error 0.5012(0.5320) Steps 0(0.00) | Grad Norm 5.9319(9.7024) | Total Time 0.00(0.00)\n",
      "Iter 0562 | Time 66.0704(65.4991) | Bit/dim 3.9136(3.9265) | Xent 1.4202(1.4736) | Loss 10.9734(11.7077) | Error 0.5141(0.5315) Steps 0(0.00) | Grad Norm 9.2701(9.6894) | Total Time 0.00(0.00)\n",
      "Iter 0563 | Time 56.0946(65.2170) | Bit/dim 3.8824(3.9252) | Xent 1.4458(1.4728) | Loss 10.9425(11.6847) | Error 0.5282(0.5314) Steps 0(0.00) | Grad Norm 8.5513(9.6552) | Total Time 0.00(0.00)\n",
      "Iter 0564 | Time 66.7117(65.2618) | Bit/dim 3.8785(3.9238) | Xent 1.3860(1.4702) | Loss 10.9750(11.6635) | Error 0.5021(0.5305) Steps 0(0.00) | Grad Norm 4.7748(9.5088) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0094 | Time 24.8533, Epoch Time 411.7295(441.1037), Bit/dim 3.8890(best: 3.8924), Xent 1.3964, Loss 4.5872, Error 0.5095(best: 0.4892)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0565 | Time 59.8927(65.1007) | Bit/dim 3.8913(3.9228) | Xent 1.4658(1.4700) | Loss 14.5131(11.7489) | Error 0.5334(0.5306) Steps 0(0.00) | Grad Norm 9.9589(9.5223) | Total Time 0.00(0.00)\n",
      "Iter 0566 | Time 58.5133(64.9031) | Bit/dim 3.8661(3.9211) | Xent 1.4306(1.4689) | Loss 10.8667(11.7225) | Error 0.5139(0.5301) Steps 0(0.00) | Grad Norm 8.4196(9.4893) | Total Time 0.00(0.00)\n",
      "Iter 0567 | Time 68.7386(65.0182) | Bit/dim 3.8729(3.9197) | Xent 1.4031(1.4669) | Loss 10.8552(11.6965) | Error 0.5040(0.5293) Steps 0(0.00) | Grad Norm 7.8044(9.4387) | Total Time 0.00(0.00)\n",
      "Iter 0568 | Time 58.1881(64.8133) | Bit/dim 3.8671(3.9181) | Xent 1.3950(1.4647) | Loss 10.3392(11.6557) | Error 0.4962(0.5283) Steps 0(0.00) | Grad Norm 6.9403(9.3638) | Total Time 0.00(0.00)\n",
      "Iter 0569 | Time 68.4024(64.9210) | Bit/dim 3.8856(3.9171) | Xent 1.3841(1.4623) | Loss 10.8549(11.6317) | Error 0.4984(0.5274) Steps 0(0.00) | Grad Norm 4.3350(9.2129) | Total Time 0.00(0.00)\n",
      "Iter 0570 | Time 63.0637(64.8652) | Bit/dim 3.8684(3.9156) | Xent 1.3837(1.4600) | Loss 10.7738(11.6060) | Error 0.4996(0.5266) Steps 0(0.00) | Grad Norm 5.3167(9.0960) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0095 | Time 25.5304, Epoch Time 418.0670(440.4126), Bit/dim 3.8716(best: 3.8890), Xent 1.3233, Loss 4.5332, Error 0.4830(best: 0.4892)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0571 | Time 56.8389(64.6244) | Bit/dim 3.8755(3.9144) | Xent 1.3647(1.4571) | Loss 14.8674(11.7038) | Error 0.4938(0.5256) Steps 0(0.00) | Grad Norm 4.4161(8.9556) | Total Time 0.00(0.00)\n",
      "Iter 0572 | Time 58.5372(64.4418) | Bit/dim 3.8712(3.9131) | Xent 1.3743(1.4546) | Loss 10.9434(11.6810) | Error 0.5022(0.5249) Steps 0(0.00) | Grad Norm 7.9367(8.9250) | Total Time 0.00(0.00)\n",
      "Iter 0573 | Time 64.1933(64.4344) | Bit/dim 3.8583(3.9115) | Xent 1.4867(1.4556) | Loss 10.6360(11.6497) | Error 0.5286(0.5250) Steps 0(0.00) | Grad Norm 16.3961(9.1492) | Total Time 0.00(0.00)\n",
      "Iter 0574 | Time 65.8946(64.4782) | Bit/dim 3.9145(3.9116) | Xent 1.6465(1.4613) | Loss 11.1812(11.6356) | Error 0.5684(0.5263) Steps 0(0.00) | Grad Norm 27.2011(9.6907) | Total Time 0.00(0.00)\n",
      "Iter 0575 | Time 62.0406(64.4051) | Bit/dim 3.8698(3.9103) | Xent 1.4361(1.4605) | Loss 10.9890(11.6162) | Error 0.5161(0.5260) Steps 0(0.00) | Grad Norm 11.8304(9.7549) | Total Time 0.00(0.00)\n",
      "Iter 0576 | Time 59.8151(64.2674) | Bit/dim 3.8928(3.9098) | Xent 1.5007(1.4617) | Loss 11.0553(11.5994) | Error 0.5448(0.5266) Steps 0(0.00) | Grad Norm 14.3888(9.8939) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0096 | Time 27.5052, Epoch Time 410.6000(439.5182), Bit/dim 3.9037(best: 3.8716), Xent 1.4360, Loss 4.6217, Error 0.5258(best: 0.4830)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0577 | Time 58.6458(64.0987) | Bit/dim 3.8992(3.9095) | Xent 1.4877(1.4625) | Loss 15.1041(11.7045) | Error 0.5359(0.5268) Steps 0(0.00) | Grad Norm 9.8673(9.8931) | Total Time 0.00(0.00)\n",
      "Iter 0578 | Time 61.0286(64.0066) | Bit/dim 3.8773(3.9085) | Xent 1.4479(1.4621) | Loss 10.7750(11.6766) | Error 0.5248(0.5268) Steps 0(0.00) | Grad Norm 7.1807(9.8118) | Total Time 0.00(0.00)\n",
      "Iter 0579 | Time 60.3631(63.8973) | Bit/dim 3.9107(3.9086) | Xent 1.4559(1.4619) | Loss 10.6280(11.6452) | Error 0.5280(0.5268) Steps 0(0.00) | Grad Norm 8.5118(9.7728) | Total Time 0.00(0.00)\n",
      "Iter 0580 | Time 63.6635(63.8903) | Bit/dim 3.8710(3.9075) | Xent 1.4462(1.4614) | Loss 11.0840(11.6283) | Error 0.5188(0.5266) Steps 0(0.00) | Grad Norm 7.8847(9.7161) | Total Time 0.00(0.00)\n",
      "Iter 0581 | Time 62.1872(63.8392) | Bit/dim 3.8765(3.9065) | Xent 1.4209(1.4602) | Loss 10.9603(11.6083) | Error 0.5157(0.5262) Steps 0(0.00) | Grad Norm 5.8856(9.6012) | Total Time 0.00(0.00)\n",
      "Iter 0582 | Time 56.4841(63.6185) | Bit/dim 3.9054(3.9065) | Xent 1.3995(1.4584) | Loss 10.9568(11.5888) | Error 0.5191(0.5260) Steps 0(0.00) | Grad Norm 6.6000(9.5112) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0097 | Time 24.9556, Epoch Time 402.8754(438.4190), Bit/dim 3.8715(best: 3.8716), Xent 1.3382, Loss 4.5405, Error 0.4882(best: 0.4830)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0583 | Time 58.8839(63.4765) | Bit/dim 3.8678(3.9053) | Xent 1.3717(1.4558) | Loss 14.6731(11.6813) | Error 0.4975(0.5252) Steps 0(0.00) | Grad Norm 4.7000(9.3668) | Total Time 0.00(0.00)\n",
      "Iter 0584 | Time 64.8554(63.5179) | Bit/dim 3.8579(3.9039) | Xent 1.3839(1.4536) | Loss 10.9287(11.6587) | Error 0.4980(0.5244) Steps 0(0.00) | Grad Norm 5.9961(9.2657) | Total Time 0.00(0.00)\n",
      "Iter 0585 | Time 64.5790(63.5497) | Bit/dim 3.8798(3.9032) | Xent 1.3936(1.4518) | Loss 10.8889(11.6356) | Error 0.5042(0.5238) Steps 0(0.00) | Grad Norm 9.9544(9.2864) | Total Time 0.00(0.00)\n",
      "Iter 0586 | Time 57.9601(63.3820) | Bit/dim 3.8901(3.9028) | Xent 1.4343(1.4513) | Loss 11.1217(11.6202) | Error 0.5146(0.5235) Steps 0(0.00) | Grad Norm 14.9063(9.4550) | Total Time 0.00(0.00)\n",
      "Iter 0587 | Time 62.7435(63.3629) | Bit/dim 3.8822(3.9022) | Xent 1.5194(1.4534) | Loss 11.0025(11.6017) | Error 0.5351(0.5238) Steps 0(0.00) | Grad Norm 16.0987(9.6543) | Total Time 0.00(0.00)\n",
      "Iter 0588 | Time 61.4006(63.3040) | Bit/dim 3.8788(3.9015) | Xent 1.4151(1.4522) | Loss 10.9550(11.5823) | Error 0.5108(0.5234) Steps 0(0.00) | Grad Norm 8.0079(9.6049) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0098 | Time 24.2367, Epoch Time 410.5725(437.5836), Bit/dim 3.8620(best: 3.8715), Xent 1.3522, Loss 4.5381, Error 0.4963(best: 0.4830)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0589 | Time 62.0273(63.2657) | Bit/dim 3.8526(3.9000) | Xent 1.4020(1.4507) | Loss 14.3798(11.6662) | Error 0.5110(0.5231) Steps 0(0.00) | Grad Norm 7.9568(9.5554) | Total Time 0.00(0.00)\n",
      "Iter 0590 | Time 59.1089(63.1410) | Bit/dim 3.8717(3.8992) | Xent 1.4276(1.4500) | Loss 10.9738(11.6454) | Error 0.5121(0.5227) Steps 0(0.00) | Grad Norm 10.3476(9.5792) | Total Time 0.00(0.00)\n",
      "Iter 0591 | Time 58.8129(63.0111) | Bit/dim 3.8558(3.8979) | Xent 1.3891(1.4482) | Loss 10.4954(11.6109) | Error 0.5001(0.5221) Steps 0(0.00) | Grad Norm 4.9907(9.4416) | Total Time 0.00(0.00)\n",
      "Iter 0592 | Time 67.3024(63.1399) | Bit/dim 3.8496(3.8964) | Xent 1.4032(1.4468) | Loss 10.8327(11.5876) | Error 0.5050(0.5215) Steps 0(0.00) | Grad Norm 6.5321(9.3543) | Total Time 0.00(0.00)\n",
      "Iter 0593 | Time 62.3664(63.1167) | Bit/dim 3.8510(3.8951) | Xent 1.3791(1.4448) | Loss 10.7370(11.5621) | Error 0.4960(0.5208) Steps 0(0.00) | Grad Norm 4.9004(9.2207) | Total Time 0.00(0.00)\n",
      "Iter 0594 | Time 69.4205(63.3058) | Bit/dim 3.8494(3.8937) | Xent 1.3760(1.4427) | Loss 10.8982(11.5421) | Error 0.4926(0.5199) Steps 0(0.00) | Grad Norm 4.9806(9.0935) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0099 | Time 25.8902, Epoch Time 420.8098(437.0803), Bit/dim 3.8499(best: 3.8620), Xent 1.3090, Loss 4.5044, Error 0.4773(best: 0.4830)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0595 | Time 66.7350(63.4087) | Bit/dim 3.8421(3.8921) | Xent 1.3454(1.4398) | Loss 14.8367(11.6410) | Error 0.4871(0.5190) Steps 0(0.00) | Grad Norm 3.8882(8.9373) | Total Time 0.00(0.00)\n",
      "Iter 0596 | Time 59.4647(63.2904) | Bit/dim 3.8357(3.8904) | Xent 1.3631(1.4375) | Loss 10.8689(11.6178) | Error 0.4889(0.5181) Steps 0(0.00) | Grad Norm 3.1544(8.7638) | Total Time 0.00(0.00)\n",
      "Iter 0597 | Time 68.0473(63.4331) | Bit/dim 3.8409(3.8890) | Xent 1.3551(1.4350) | Loss 10.7890(11.5930) | Error 0.4915(0.5173) Steps 0(0.00) | Grad Norm 5.0482(8.6523) | Total Time 0.00(0.00)\n",
      "Iter 0598 | Time 64.6233(63.4688) | Bit/dim 3.8380(3.8874) | Xent 1.3248(1.4317) | Loss 10.6761(11.5654) | Error 0.4784(0.5161) Steps 0(0.00) | Grad Norm 5.2387(8.5499) | Total Time 0.00(0.00)\n",
      "Iter 0599 | Time 60.1225(63.3684) | Bit/dim 3.8442(3.8861) | Xent 1.3472(1.4292) | Loss 10.6754(11.5387) | Error 0.4864(0.5152) Steps 0(0.00) | Grad Norm 4.4055(8.4256) | Total Time 0.00(0.00)\n",
      "Iter 0600 | Time 66.2836(63.4558) | Bit/dim 3.8516(3.8851) | Xent 1.3280(1.4262) | Loss 10.7471(11.5150) | Error 0.4871(0.5144) Steps 0(0.00) | Grad Norm 2.6993(8.2538) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0100 | Time 25.2641, Epoch Time 426.5792(436.7653), Bit/dim 3.8309(best: 3.8499), Xent 1.2850, Loss 4.4734, Error 0.4691(best: 0.4773)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0601 | Time 65.6341(63.5212) | Bit/dim 3.8358(3.8836) | Xent 1.3249(1.4231) | Loss 14.3454(11.5999) | Error 0.4758(0.5132) Steps 0(0.00) | Grad Norm 3.3579(8.1069) | Total Time 0.00(0.00)\n",
      "Iter 0602 | Time 64.1258(63.5393) | Bit/dim 3.8308(3.8820) | Xent 1.3380(1.4206) | Loss 10.6469(11.5713) | Error 0.4811(0.5122) Steps 0(0.00) | Grad Norm 7.5191(8.0893) | Total Time 0.00(0.00)\n",
      "Iter 0603 | Time 61.3790(63.4745) | Bit/dim 3.8293(3.8805) | Xent 1.3809(1.4194) | Loss 10.9052(11.5513) | Error 0.4954(0.5117) Steps 0(0.00) | Grad Norm 14.2777(8.2749) | Total Time 0.00(0.00)\n",
      "Iter 0604 | Time 57.6782(63.3006) | Bit/dim 3.8476(3.8795) | Xent 1.4624(1.4207) | Loss 10.8481(11.5302) | Error 0.5202(0.5120) Steps 0(0.00) | Grad Norm 20.9705(8.6558) | Total Time 0.00(0.00)\n",
      "Iter 0605 | Time 64.4792(63.3360) | Bit/dim 3.8895(3.8798) | Xent 1.6118(1.4264) | Loss 11.0576(11.5161) | Error 0.5655(0.5136) Steps 0(0.00) | Grad Norm 18.8062(8.9603) | Total Time 0.00(0.00)\n",
      "Iter 0606 | Time 66.5815(63.4333) | Bit/dim 3.9786(3.8827) | Xent 1.6808(1.4340) | Loss 11.3275(11.5104) | Error 0.5766(0.5155) Steps 0(0.00) | Grad Norm 25.0447(9.4429) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0101 | Time 25.1298, Epoch Time 420.7106(436.2837), Bit/dim 3.9873(best: 3.8309), Xent 1.6189, Loss 4.7967, Error 0.5796(best: 0.4691)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0607 | Time 64.8642(63.4763) | Bit/dim 3.9866(3.8859) | Xent 1.6820(1.4415) | Loss 14.6021(11.6032) | Error 0.5989(0.5180) Steps 0(0.00) | Grad Norm 14.0101(9.5799) | Total Time 0.00(0.00)\n",
      "Iter 0608 | Time 57.7365(63.3041) | Bit/dim 3.9820(3.8887) | Xent 1.6197(1.4468) | Loss 11.0729(11.5872) | Error 0.5941(0.5203) Steps 0(0.00) | Grad Norm 20.4586(9.9062) | Total Time 0.00(0.00)\n",
      "Iter 0609 | Time 67.0154(63.4154) | Bit/dim 4.0215(3.8927) | Xent 1.6747(1.4537) | Loss 11.4824(11.5841) | Error 0.6053(0.5228) Steps 0(0.00) | Grad Norm 16.9532(10.1176) | Total Time 0.00(0.00)\n",
      "Iter 0610 | Time 55.8153(63.1874) | Bit/dim 4.0605(3.8978) | Xent 1.6794(1.4604) | Loss 11.5793(11.5840) | Error 0.5994(0.5251) Steps 0(0.00) | Grad Norm 14.6474(10.2535) | Total Time 0.00(0.00)\n",
      "Iter 0611 | Time 58.1809(63.0372) | Bit/dim 4.0882(3.9035) | Xent 1.7494(1.4691) | Loss 11.6834(11.5869) | Error 0.6064(0.5276) Steps 0(0.00) | Grad Norm 17.5486(10.4724) | Total Time 0.00(0.00)\n",
      "Iter 0612 | Time 63.9291(63.0640) | Bit/dim 4.0220(3.9070) | Xent 1.6142(1.4735) | Loss 11.4396(11.5825) | Error 0.5729(0.5289) Steps 0(0.00) | Grad Norm 14.1455(10.5826) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0102 | Time 25.7705, Epoch Time 409.8318(435.4901), Bit/dim 4.0811(best: 3.8309), Xent 1.5404, Loss 4.8513, Error 0.5559(best: 0.4691)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0613 | Time 65.9381(63.1502) | Bit/dim 4.0831(3.9123) | Xent 1.5834(1.4768) | Loss 15.7002(11.7060) | Error 0.5629(0.5299) Steps 0(0.00) | Grad Norm 10.3528(10.5757) | Total Time 0.00(0.00)\n",
      "Iter 0614 | Time 66.8752(63.2619) | Bit/dim 4.0225(3.9156) | Xent 1.5100(1.4778) | Loss 11.1994(11.6908) | Error 0.5465(0.5304) Steps 0(0.00) | Grad Norm 6.5448(10.4548) | Total Time 0.00(0.00)\n",
      "Iter 0615 | Time 67.0822(63.3766) | Bit/dim 4.0048(3.9183) | Xent 1.5452(1.4798) | Loss 11.3902(11.6818) | Error 0.5549(0.5312) Steps 0(0.00) | Grad Norm 10.0767(10.4434) | Total Time 0.00(0.00)\n",
      "Iter 0616 | Time 64.2934(63.4041) | Bit/dim 4.0093(3.9210) | Xent 1.5143(1.4808) | Loss 11.3207(11.6710) | Error 0.5410(0.5315) Steps 0(0.00) | Grad Norm 9.5480(10.4166) | Total Time 0.00(0.00)\n",
      "Iter 0617 | Time 61.0079(63.3322) | Bit/dim 3.9855(3.9230) | Xent 1.5063(1.4816) | Loss 11.4082(11.6631) | Error 0.5516(0.5321) Steps 0(0.00) | Grad Norm 7.4854(10.3286) | Total Time 0.00(0.00)\n",
      "Iter 0618 | Time 62.8023(63.3163) | Bit/dim 3.9673(3.9243) | Xent 1.5119(1.4825) | Loss 10.9669(11.6422) | Error 0.5437(0.5324) Steps 0(0.00) | Grad Norm 7.0457(10.2301) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0103 | Time 25.6493, Epoch Time 429.2257(435.3022), Bit/dim 3.9475(best: 3.8309), Xent 1.4601, Loss 4.6776, Error 0.5357(best: 0.4691)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0619 | Time 59.0866(63.1894) | Bit/dim 3.9502(3.9251) | Xent 1.5049(1.4832) | Loss 14.9074(11.7402) | Error 0.5489(0.5329) Steps 0(0.00) | Grad Norm 6.5391(10.1194) | Total Time 0.00(0.00)\n",
      "Iter 0620 | Time 66.0555(63.2754) | Bit/dim 3.9576(3.9260) | Xent 1.4538(1.4823) | Loss 10.9120(11.7153) | Error 0.5297(0.5328) Steps 0(0.00) | Grad Norm 5.4719(9.9800) | Total Time 0.00(0.00)\n",
      "Iter 0621 | Time 63.7508(63.2896) | Bit/dim 3.9539(3.9269) | Xent 1.4785(1.4822) | Loss 11.1592(11.6987) | Error 0.5388(0.5330) Steps 0(0.00) | Grad Norm 7.0078(9.8908) | Total Time 0.00(0.00)\n",
      "Iter 0622 | Time 70.5759(63.5082) | Bit/dim 3.9322(3.9270) | Xent 1.4510(1.4812) | Loss 11.0835(11.6802) | Error 0.5220(0.5327) Steps 0(0.00) | Grad Norm 4.6037(9.7322) | Total Time 0.00(0.00)\n",
      "Iter 0623 | Time 64.4398(63.5362) | Bit/dim 3.9085(3.9265) | Xent 1.4338(1.4798) | Loss 10.9986(11.6597) | Error 0.5196(0.5323) Steps 0(0.00) | Grad Norm 5.1534(9.5948) | Total Time 0.00(0.00)\n",
      "Iter 0624 | Time 63.0113(63.5204) | Bit/dim 3.8991(3.9257) | Xent 1.4278(1.4782) | Loss 11.0677(11.6420) | Error 0.5159(0.5318) Steps 0(0.00) | Grad Norm 4.3648(9.4379) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0104 | Time 26.4668, Epoch Time 429.1778(435.1184), Bit/dim 3.9060(best: 3.8309), Xent 1.3862, Loss 4.5990, Error 0.4988(best: 0.4691)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0625 | Time 68.2740(63.6630) | Bit/dim 3.8998(3.9249) | Xent 1.4083(1.4761) | Loss 15.4472(11.7561) | Error 0.5056(0.5310) Steps 0(0.00) | Grad Norm 6.3005(9.3438) | Total Time 0.00(0.00)\n",
      "Iter 0626 | Time 57.5206(63.4788) | Bit/dim 3.8964(3.9240) | Xent 1.4161(1.4743) | Loss 10.6109(11.7218) | Error 0.5079(0.5303) Steps 0(0.00) | Grad Norm 4.8313(9.2084) | Total Time 0.00(0.00)\n",
      "Iter 0627 | Time 65.4011(63.5364) | Bit/dim 3.8870(3.9229) | Xent 1.4022(1.4722) | Loss 11.0452(11.7015) | Error 0.5031(0.5295) Steps 0(0.00) | Grad Norm 3.9733(9.0514) | Total Time 0.00(0.00)\n",
      "Iter 0628 | Time 67.8910(63.6671) | Bit/dim 3.8853(3.9218) | Xent 1.4028(1.4701) | Loss 11.1558(11.6851) | Error 0.5052(0.5288) Steps 0(0.00) | Grad Norm 5.4453(8.9432) | Total Time 0.00(0.00)\n",
      "Iter 0629 | Time 60.1950(63.5629) | Bit/dim 3.8774(3.9205) | Xent 1.3784(1.4673) | Loss 10.8270(11.6594) | Error 0.4946(0.5277) Steps 0(0.00) | Grad Norm 4.2401(8.8021) | Total Time 0.00(0.00)\n",
      "Iter 0630 | Time 62.3747(63.5273) | Bit/dim 3.8572(3.9186) | Xent 1.3836(1.4648) | Loss 10.8890(11.6363) | Error 0.4959(0.5268) Steps 0(0.00) | Grad Norm 3.1489(8.6325) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0105 | Time 25.9931, Epoch Time 423.6262(434.7737), Bit/dim 3.8586(best: 3.8309), Xent 1.3374, Loss 4.5273, Error 0.4836(best: 0.4691)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0631 | Time 68.9605(63.6903) | Bit/dim 3.8466(3.9164) | Xent 1.3610(1.4617) | Loss 14.9159(11.7347) | Error 0.4905(0.5257) Steps 0(0.00) | Grad Norm 4.7477(8.5160) | Total Time 0.00(0.00)\n",
      "Iter 0632 | Time 62.7789(63.6629) | Bit/dim 3.8569(3.9146) | Xent 1.3954(1.4597) | Loss 10.6778(11.7030) | Error 0.4954(0.5248) Steps 0(0.00) | Grad Norm 6.6009(8.4585) | Total Time 0.00(0.00)\n",
      "Iter 0633 | Time 58.9408(63.5213) | Bit/dim 3.8670(3.9132) | Xent 1.4281(1.4588) | Loss 10.8107(11.6762) | Error 0.5092(0.5243) Steps 0(0.00) | Grad Norm 12.2485(8.5722) | Total Time 0.00(0.00)\n",
      "Iter 0634 | Time 57.3544(63.3362) | Bit/dim 3.8623(3.9117) | Xent 1.6203(1.4636) | Loss 10.9454(11.6543) | Error 0.5697(0.5257) Steps 0(0.00) | Grad Norm 21.7520(8.9676) | Total Time 0.00(0.00)\n",
      "Iter 0635 | Time 62.6715(63.3163) | Bit/dim 3.8928(3.9111) | Xent 1.6464(1.4691) | Loss 11.2751(11.6429) | Error 0.5775(0.5272) Steps 0(0.00) | Grad Norm 24.1320(9.4225) | Total Time 0.00(0.00)\n",
      "Iter 0636 | Time 67.2598(63.4346) | Bit/dim 3.8681(3.9098) | Xent 1.4600(1.4688) | Loss 11.0368(11.6247) | Error 0.5317(0.5274) Steps 0(0.00) | Grad Norm 7.6173(9.3684) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0106 | Time 26.6858, Epoch Time 420.5906(434.3482), Bit/dim 3.8690(best: 3.8309), Xent 1.4689, Loss 4.6035, Error 0.5320(best: 0.4691)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0637 | Time 66.8388(63.5367) | Bit/dim 3.8680(3.9085) | Xent 1.5026(1.4699) | Loss 15.0202(11.7266) | Error 0.5397(0.5277) Steps 0(0.00) | Grad Norm 6.4094(9.2796) | Total Time 0.00(0.00)\n",
      "Iter 0638 | Time 63.0445(63.5220) | Bit/dim 3.8590(3.9071) | Xent 1.4775(1.4701) | Loss 11.1103(11.7081) | Error 0.5349(0.5280) Steps 0(0.00) | Grad Norm 4.7221(9.1429) | Total Time 0.00(0.00)\n",
      "Iter 0639 | Time 65.1197(63.5699) | Bit/dim 3.8548(3.9055) | Xent 1.4259(1.4688) | Loss 10.9533(11.6854) | Error 0.5155(0.5276) Steps 0(0.00) | Grad Norm 4.2937(8.9974) | Total Time 0.00(0.00)\n",
      "Iter 0640 | Time 64.6402(63.6020) | Bit/dim 3.8539(3.9039) | Xent 1.3959(1.4666) | Loss 10.8367(11.6600) | Error 0.4964(0.5266) Steps 0(0.00) | Grad Norm 4.2054(8.8537) | Total Time 0.00(0.00)\n",
      "Iter 0641 | Time 61.3455(63.5343) | Bit/dim 3.8507(3.9023) | Xent 1.4196(1.4652) | Loss 10.4688(11.6242) | Error 0.5196(0.5264) Steps 0(0.00) | Grad Norm 4.4481(8.7215) | Total Time 0.00(0.00)\n",
      "Iter 0642 | Time 58.6130(63.3867) | Bit/dim 3.8546(3.9009) | Xent 1.4128(1.4636) | Loss 10.8812(11.6020) | Error 0.5135(0.5260) Steps 0(0.00) | Grad Norm 3.8172(8.5744) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0107 | Time 24.7292, Epoch Time 420.2450(433.9251), Bit/dim 3.8441(best: 3.8309), Xent 1.3459, Loss 4.5170, Error 0.4897(best: 0.4691)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0643 | Time 59.0642(63.2570) | Bit/dim 3.8441(3.8992) | Xent 1.4033(1.4618) | Loss 14.7142(11.6953) | Error 0.5041(0.5254) Steps 0(0.00) | Grad Norm 3.8510(8.4327) | Total Time 0.00(0.00)\n",
      "Iter 0644 | Time 57.5987(63.0873) | Bit/dim 3.8439(3.8976) | Xent 1.3956(1.4598) | Loss 10.7941(11.6683) | Error 0.5009(0.5247) Steps 0(0.00) | Grad Norm 7.4648(8.4036) | Total Time 0.00(0.00)\n",
      "Iter 0645 | Time 63.7939(63.1085) | Bit/dim 3.8529(3.8962) | Xent 1.4190(1.4586) | Loss 10.8730(11.6444) | Error 0.5119(0.5243) Steps 0(0.00) | Grad Norm 9.8262(8.4463) | Total Time 0.00(0.00)\n",
      "Iter 0646 | Time 69.9288(63.3131) | Bit/dim 3.8441(3.8946) | Xent 1.4225(1.4575) | Loss 10.7057(11.6163) | Error 0.5179(0.5241) Steps 0(0.00) | Grad Norm 10.2011(8.4989) | Total Time 0.00(0.00)\n",
      "Iter 0647 | Time 64.2595(63.3415) | Bit/dim 3.8425(3.8931) | Xent 1.3726(1.4549) | Loss 10.9323(11.5957) | Error 0.4979(0.5233) Steps 0(0.00) | Grad Norm 6.8081(8.4482) | Total Time 0.00(0.00)\n",
      "Iter 0648 | Time 70.7264(63.5630) | Bit/dim 3.8321(3.8913) | Xent 1.3435(1.4516) | Loss 10.9577(11.5766) | Error 0.4826(0.5221) Steps 0(0.00) | Grad Norm 3.5758(8.3020) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0108 | Time 25.6319, Epoch Time 426.7508(433.7099), Bit/dim 3.8334(best: 3.8309), Xent 1.3291, Loss 4.4980, Error 0.4753(best: 0.4691)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0649 | Time 66.7881(63.6598) | Bit/dim 3.8275(3.8893) | Xent 1.3576(1.4488) | Loss 14.9961(11.6792) | Error 0.4861(0.5210) Steps 0(0.00) | Grad Norm 4.8484(8.1984) | Total Time 0.00(0.00)\n",
      "Iter 0650 | Time 58.0517(63.4915) | Bit/dim 3.8181(3.8872) | Xent 1.3559(1.4460) | Loss 10.6586(11.6486) | Error 0.4975(0.5203) Steps 0(0.00) | Grad Norm 3.8884(8.0691) | Total Time 0.00(0.00)\n",
      "Iter 0651 | Time 62.6480(63.4662) | Bit/dim 3.8216(3.8852) | Xent 1.3597(1.4434) | Loss 10.7307(11.6210) | Error 0.4962(0.5196) Steps 0(0.00) | Grad Norm 5.0095(7.9773) | Total Time 0.00(0.00)\n",
      "Iter 0652 | Time 61.5292(63.4081) | Bit/dim 3.8297(3.8836) | Xent 1.3362(1.4402) | Loss 10.7353(11.5945) | Error 0.4785(0.5183) Steps 0(0.00) | Grad Norm 3.5108(7.8433) | Total Time 0.00(0.00)\n",
      "Iter 0653 | Time 69.7258(63.5976) | Bit/dim 3.8102(3.8814) | Xent 1.3472(1.4374) | Loss 10.6777(11.5670) | Error 0.4838(0.5173) Steps 0(0.00) | Grad Norm 3.9292(7.7259) | Total Time 0.00(0.00)\n",
      "Iter 0654 | Time 58.1910(63.4354) | Bit/dim 3.8195(3.8795) | Xent 1.3288(1.4341) | Loss 10.6934(11.5407) | Error 0.4761(0.5161) Steps 0(0.00) | Grad Norm 2.2508(7.5617) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0109 | Time 26.1428, Epoch Time 419.0526(433.2701), Bit/dim 3.8220(best: 3.8309), Xent 1.2899, Loss 4.4670, Error 0.4684(best: 0.4691)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0655 | Time 61.2114(63.3687) | Bit/dim 3.8157(3.8776) | Xent 1.3349(1.4312) | Loss 14.9671(11.6435) | Error 0.4805(0.5150) Steps 0(0.00) | Grad Norm 3.3951(7.4367) | Total Time 0.00(0.00)\n",
      "Iter 0656 | Time 64.9017(63.4147) | Bit/dim 3.8100(3.8756) | Xent 1.3165(1.4277) | Loss 10.8310(11.6192) | Error 0.4726(0.5137) Steps 0(0.00) | Grad Norm 2.5588(7.2903) | Total Time 0.00(0.00)\n",
      "Iter 0657 | Time 62.2067(63.3785) | Bit/dim 3.8036(3.8734) | Xent 1.3252(1.4246) | Loss 10.4687(11.5846) | Error 0.4736(0.5125) Steps 0(0.00) | Grad Norm 3.8008(7.1857) | Total Time 0.00(0.00)\n",
      "Iter 0658 | Time 58.7373(63.2392) | Bit/dim 3.7983(3.8712) | Xent 1.3245(1.4216) | Loss 10.2033(11.5432) | Error 0.4739(0.5114) Steps 0(0.00) | Grad Norm 5.3964(7.1320) | Total Time 0.00(0.00)\n",
      "Iter 0659 | Time 61.7557(63.1947) | Bit/dim 3.8214(3.8697) | Xent 1.3691(1.4201) | Loss 10.8885(11.5236) | Error 0.4900(0.5107) Steps 0(0.00) | Grad Norm 9.8504(7.2135) | Total Time 0.00(0.00)\n",
      "Iter 0660 | Time 63.3634(63.1998) | Bit/dim 3.8272(3.8684) | Xent 1.4752(1.4217) | Loss 10.8362(11.5029) | Error 0.5135(0.5108) Steps 0(0.00) | Grad Norm 22.7059(7.6783) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0110 | Time 26.3986, Epoch Time 414.5656(432.7090), Bit/dim 3.8615(best: 3.8220), Xent 1.7284, Loss 4.7257, Error 0.5811(best: 0.4684)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0661 | Time 72.2942(63.4726) | Bit/dim 3.8687(3.8684) | Xent 1.7609(1.4319) | Loss 15.0222(11.6085) | Error 0.5839(0.5130) Steps 0(0.00) | Grad Norm 27.0129(8.2583) | Total Time 0.00(0.00)\n",
      "Iter 0662 | Time 61.8544(63.4241) | Bit/dim 3.8347(3.8674) | Xent 1.3828(1.4304) | Loss 10.5646(11.5772) | Error 0.5025(0.5127) Steps 0(0.00) | Grad Norm 5.7347(8.1826) | Total Time 0.00(0.00)\n",
      "Iter 0663 | Time 62.0963(63.3842) | Bit/dim 3.8566(3.8671) | Xent 1.5601(1.4343) | Loss 10.9715(11.5590) | Error 0.5485(0.5138) Steps 0(0.00) | Grad Norm 16.5425(8.4334) | Total Time 0.00(0.00)\n",
      "Iter 0664 | Time 57.8676(63.2187) | Bit/dim 3.8150(3.8655) | Xent 1.4523(1.4349) | Loss 10.9144(11.5397) | Error 0.5114(0.5137) Steps 0(0.00) | Grad Norm 7.3639(8.4013) | Total Time 0.00(0.00)\n",
      "Iter 0665 | Time 64.6575(63.2619) | Bit/dim 3.8323(3.8645) | Xent 1.5036(1.4369) | Loss 10.9648(11.5224) | Error 0.5357(0.5143) Steps 0(0.00) | Grad Norm 8.1147(8.3927) | Total Time 0.00(0.00)\n",
      "Iter 0666 | Time 61.8786(63.2204) | Bit/dim 3.8364(3.8637) | Xent 1.4059(1.4360) | Loss 10.7819(11.5002) | Error 0.4990(0.5139) Steps 0(0.00) | Grad Norm 6.3288(8.3308) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0111 | Time 26.7675, Epoch Time 423.3865(432.4293), Bit/dim 3.8318(best: 3.8220), Xent 1.3369, Loss 4.5003, Error 0.4769(best: 0.4684)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0667 | Time 60.2396(63.1310) | Bit/dim 3.8329(3.8627) | Xent 1.3868(1.4345) | Loss 15.1688(11.6103) | Error 0.4985(0.5134) Steps 0(0.00) | Grad Norm 4.2346(8.2079) | Total Time 0.00(0.00)\n",
      "Iter 0668 | Time 65.2544(63.1947) | Bit/dim 3.8299(3.8618) | Xent 1.3740(1.4327) | Loss 10.5340(11.5780) | Error 0.4939(0.5128) Steps 0(0.00) | Grad Norm 6.7962(8.1656) | Total Time 0.00(0.00)\n",
      "Iter 0669 | Time 62.2188(63.1654) | Bit/dim 3.8355(3.8610) | Xent 1.3691(1.4308) | Loss 10.6757(11.5509) | Error 0.4844(0.5120) Steps 0(0.00) | Grad Norm 7.1806(8.1360) | Total Time 0.00(0.00)\n",
      "Iter 0670 | Time 64.7281(63.2123) | Bit/dim 3.8421(3.8604) | Xent 1.3479(1.4283) | Loss 10.5517(11.5210) | Error 0.4848(0.5112) Steps 0(0.00) | Grad Norm 6.1146(8.0754) | Total Time 0.00(0.00)\n",
      "Iter 0671 | Time 60.7274(63.1377) | Bit/dim 3.8338(3.8596) | Xent 1.3321(1.4254) | Loss 10.4278(11.4882) | Error 0.4768(0.5101) Steps 0(0.00) | Grad Norm 4.7770(7.9764) | Total Time 0.00(0.00)\n",
      "Iter 0672 | Time 66.0460(63.2250) | Bit/dim 3.8343(3.8588) | Xent 1.3358(1.4227) | Loss 10.7644(11.4664) | Error 0.4791(0.5092) Steps 0(0.00) | Grad Norm 3.8044(7.8513) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0112 | Time 24.8017, Epoch Time 420.0193(432.0570), Bit/dim 3.8189(best: 3.8220), Xent 1.2851, Loss 4.4615, Error 0.4642(best: 0.4684)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0673 | Time 63.0532(63.2198) | Bit/dim 3.8311(3.8580) | Xent 1.3184(1.4196) | Loss 14.8972(11.5694) | Error 0.4721(0.5081) Steps 0(0.00) | Grad Norm 4.4441(7.7491) | Total Time 0.00(0.00)\n",
      "Iter 0674 | Time 90.9995(64.0532) | Bit/dim 3.8208(3.8569) | Xent 1.3178(1.4165) | Loss 10.7251(11.5440) | Error 0.4719(0.5070) Steps 0(0.00) | Grad Norm 6.3648(7.7075) | Total Time 0.00(0.00)\n",
      "Iter 0675 | Time 65.8756(64.1079) | Bit/dim 3.8232(3.8559) | Xent 1.3367(1.4141) | Loss 10.7730(11.5209) | Error 0.4820(0.5063) Steps 0(0.00) | Grad Norm 6.9434(7.6846) | Total Time 0.00(0.00)\n",
      "Iter 0676 | Time 65.2773(64.1430) | Bit/dim 3.8185(3.8548) | Xent 1.3330(1.4117) | Loss 10.4962(11.4902) | Error 0.4754(0.5053) Steps 0(0.00) | Grad Norm 6.5193(7.6496) | Total Time 0.00(0.00)\n",
      "Iter 0677 | Time 63.0516(64.1102) | Bit/dim 3.8011(3.8532) | Xent 1.3312(1.4093) | Loss 10.8524(11.4710) | Error 0.4776(0.5045) Steps 0(0.00) | Grad Norm 4.9443(7.5685) | Total Time 0.00(0.00)\n",
      "Iter 0678 | Time 61.3789(64.0283) | Bit/dim 3.8105(3.8519) | Xent 1.3100(1.4063) | Loss 10.4055(11.4391) | Error 0.4734(0.5036) Steps 0(0.00) | Grad Norm 3.1138(7.4348) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0113 | Time 25.6307, Epoch Time 450.9836(432.6248), Bit/dim 3.8126(best: 3.8189), Xent 1.2600, Loss 4.4426, Error 0.4575(best: 0.4642)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0679 | Time 63.9619(64.0263) | Bit/dim 3.7958(3.8502) | Xent 1.3040(1.4033) | Loss 14.9096(11.5432) | Error 0.4657(0.5024) Steps 0(0.00) | Grad Norm 4.3936(7.3436) | Total Time 0.00(0.00)\n",
      "Iter 0680 | Time 64.9850(64.0551) | Bit/dim 3.8020(3.8487) | Xent 1.3275(1.4010) | Loss 10.6132(11.5153) | Error 0.4809(0.5018) Steps 0(0.00) | Grad Norm 4.8474(7.2687) | Total Time 0.00(0.00)\n",
      "Iter 0681 | Time 61.5499(63.9799) | Bit/dim 3.7905(3.8470) | Xent 1.2853(1.3975) | Loss 10.4255(11.4826) | Error 0.4691(0.5008) Steps 0(0.00) | Grad Norm 3.3648(7.1516) | Total Time 0.00(0.00)\n",
      "Iter 0682 | Time 61.8459(63.9159) | Bit/dim 3.7971(3.8455) | Xent 1.2960(1.3945) | Loss 10.4458(11.4515) | Error 0.4698(0.4999) Steps 0(0.00) | Grad Norm 2.5163(7.0125) | Total Time 0.00(0.00)\n",
      "Iter 0683 | Time 65.6490(63.9679) | Bit/dim 3.7909(3.8439) | Xent 1.2751(1.3909) | Loss 10.6346(11.4270) | Error 0.4460(0.4983) Steps 0(0.00) | Grad Norm 3.0448(6.8935) | Total Time 0.00(0.00)\n",
      "Iter 0684 | Time 69.8986(64.1458) | Bit/dim 3.7945(3.8424) | Xent 1.3117(1.3885) | Loss 10.5262(11.4000) | Error 0.4751(0.4976) Steps 0(0.00) | Grad Norm 3.5953(6.7946) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0114 | Time 24.9133, Epoch Time 428.3485(432.4965), Bit/dim 3.7881(best: 3.8126), Xent 1.2656, Loss 4.4209, Error 0.4577(best: 0.4575)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0685 | Time 65.7833(64.1949) | Bit/dim 3.8020(3.8412) | Xent 1.3151(1.3863) | Loss 14.7540(11.5006) | Error 0.4738(0.4968) Steps 0(0.00) | Grad Norm 5.6830(6.7612) | Total Time 0.00(0.00)\n",
      "Iter 0686 | Time 61.8486(64.1245) | Bit/dim 3.7955(3.8398) | Xent 1.3433(1.3850) | Loss 10.5851(11.4731) | Error 0.4805(0.4964) Steps 0(0.00) | Grad Norm 12.3610(6.9292) | Total Time 0.00(0.00)\n",
      "Iter 0687 | Time 60.3798(64.0122) | Bit/dim 3.8247(3.8393) | Xent 1.6946(1.3943) | Loss 11.0571(11.4606) | Error 0.5853(0.4990) Steps 0(0.00) | Grad Norm 23.9581(7.4401) | Total Time 0.00(0.00)\n",
      "Iter 0688 | Time 64.9788(64.0412) | Bit/dim 3.8446(3.8395) | Xent 1.6736(1.4027) | Loss 11.3164(11.4563) | Error 0.5894(0.5017) Steps 0(0.00) | Grad Norm 15.0691(7.6690) | Total Time 0.00(0.00)\n",
      "Iter 0689 | Time 64.9435(64.0683) | Bit/dim 3.9331(3.8423) | Xent 1.5626(1.4075) | Loss 10.9214(11.4403) | Error 0.5540(0.5033) Steps 0(0.00) | Grad Norm 18.4212(7.9915) | Total Time 0.00(0.00)\n",
      "Iter 0690 | Time 68.0593(64.1880) | Bit/dim 3.9272(3.8449) | Xent 2.1512(1.4298) | Loss 11.7992(11.4510) | Error 0.6855(0.5088) Steps 0(0.00) | Grad Norm 26.3559(8.5425) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0115 | Time 26.7018, Epoch Time 428.3312(432.3716), Bit/dim 4.0019(best: 3.7881), Xent 1.8886, Loss 4.9462, Error 0.6501(best: 0.4575)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0691 | Time 65.9337(64.2404) | Bit/dim 3.9968(3.8494) | Xent 1.9779(1.4462) | Loss 16.1882(11.5931) | Error 0.6598(0.5133) Steps 0(0.00) | Grad Norm 17.5494(8.8127) | Total Time 0.00(0.00)\n",
      "Iter 0692 | Time 63.5903(64.2209) | Bit/dim 4.0495(3.8554) | Xent 1.8531(1.4584) | Loss 11.7607(11.5982) | Error 0.6482(0.5173) Steps 0(0.00) | Grad Norm 12.5777(8.9256) | Total Time 0.00(0.00)\n",
      "Iter 0693 | Time 67.3741(64.3155) | Bit/dim 4.0417(3.8610) | Xent 1.6952(1.4655) | Loss 11.6497(11.5997) | Error 0.6038(0.5199) Steps 0(0.00) | Grad Norm 13.4318(9.0608) | Total Time 0.00(0.00)\n",
      "Iter 0694 | Time 65.3461(64.3464) | Bit/dim 4.0684(3.8672) | Xent 1.6353(1.4706) | Loss 11.4725(11.5959) | Error 0.5891(0.5220) Steps 0(0.00) | Grad Norm 9.4496(9.0725) | Total Time 0.00(0.00)\n",
      "Iter 0695 | Time 73.0728(64.6082) | Bit/dim 4.0922(3.8740) | Xent 1.5631(1.4734) | Loss 11.4619(11.5919) | Error 0.5707(0.5235) Steps 0(0.00) | Grad Norm 8.7915(9.0640) | Total Time 0.00(0.00)\n",
      "Iter 0696 | Time 73.0060(64.8601) | Bit/dim 4.0581(3.8795) | Xent 1.5297(1.4751) | Loss 11.3670(11.5851) | Error 0.5608(0.5246) Steps 0(0.00) | Grad Norm 5.5586(8.9589) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0116 | Time 27.7286, Epoch Time 452.2005(432.9665), Bit/dim 4.0361(best: 3.7881), Xent 1.4734, Loss 4.7728, Error 0.5418(best: 0.4575)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0697 | Time 68.5942(64.9721) | Bit/dim 4.0328(3.8841) | Xent 1.5296(1.4767) | Loss 17.9574(11.7763) | Error 0.5559(0.5255) Steps 0(0.00) | Grad Norm 7.2026(8.9062) | Total Time 0.00(0.00)\n",
      "Iter 0698 | Time 69.2978(65.1019) | Bit/dim 4.0192(3.8882) | Xent 1.5298(1.4783) | Loss 11.6521(11.7726) | Error 0.5609(0.5266) Steps 0(0.00) | Grad Norm 4.9785(8.7883) | Total Time 0.00(0.00)\n",
      "Iter 0699 | Time 71.3778(65.2902) | Bit/dim 3.9700(3.8906) | Xent 1.5210(1.4796) | Loss 11.2829(11.7579) | Error 0.5517(0.5273) Steps 0(0.00) | Grad Norm 5.7776(8.6980) | Total Time 0.00(0.00)\n",
      "Iter 0700 | Time 65.7092(65.3027) | Bit/dim 3.9615(3.8927) | Xent 1.5098(1.4805) | Loss 11.3420(11.7454) | Error 0.5500(0.5280) Steps 0(0.00) | Grad Norm 5.8757(8.6134) | Total Time 0.00(0.00)\n",
      "Iter 0701 | Time 66.0844(65.3262) | Bit/dim 3.9503(3.8945) | Xent 1.5032(1.4812) | Loss 11.3933(11.7348) | Error 0.5446(0.5285) Steps 0(0.00) | Grad Norm 6.0870(8.5376) | Total Time 0.00(0.00)\n",
      "Iter 0702 | Time 74.5040(65.6015) | Bit/dim 3.9287(3.8955) | Xent 1.4692(1.4808) | Loss 11.4748(11.7270) | Error 0.5306(0.5286) Steps 0(0.00) | Grad Norm 3.8014(8.3955) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0117 | Time 28.3922, Epoch Time 459.9227(433.7751), Bit/dim 3.9299(best: 3.7881), Xent 1.4062, Loss 4.6330, Error 0.5082(best: 0.4575)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0703 | Time 66.6376(65.6326) | Bit/dim 3.9354(3.8967) | Xent 1.4514(1.4799) | Loss 16.5379(11.8714) | Error 0.5269(0.5285) Steps 0(0.00) | Grad Norm 4.4455(8.2770) | Total Time 0.00(0.00)\n",
      "Iter 0704 | Time 69.6606(65.7534) | Bit/dim 3.9193(3.8974) | Xent 1.4530(1.4791) | Loss 11.2248(11.8520) | Error 0.5268(0.5285) Steps 0(0.00) | Grad Norm 4.3942(8.1605) | Total Time 0.00(0.00)\n",
      "Iter 0705 | Time 67.8690(65.8169) | Bit/dim 3.8998(3.8974) | Xent 1.4200(1.4774) | Loss 11.2948(11.8353) | Error 0.5164(0.5281) Steps 0(0.00) | Grad Norm 3.9848(8.0352) | Total Time 0.00(0.00)\n",
      "Iter 0706 | Time 73.8608(66.0582) | Bit/dim 3.8989(3.8975) | Xent 1.4311(1.4760) | Loss 10.9677(11.8092) | Error 0.5184(0.5278) Steps 0(0.00) | Grad Norm 6.2324(7.9811) | Total Time 0.00(0.00)\n",
      "Iter 0707 | Time 72.1256(66.2403) | Bit/dim 3.9065(3.8978) | Xent 1.4975(1.4766) | Loss 11.0695(11.7870) | Error 0.5391(0.5282) Steps 0(0.00) | Grad Norm 12.6577(8.1214) | Total Time 0.00(0.00)\n",
      "Iter 0708 | Time 65.3740(66.2143) | Bit/dim 3.9255(3.8986) | Xent 1.6860(1.4829) | Loss 11.5128(11.7788) | Error 0.6068(0.5305) Steps 0(0.00) | Grad Norm 27.4846(8.7023) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0118 | Time 29.6204, Epoch Time 461.0182(434.5924), Bit/dim 3.9362(best: 3.7881), Xent 1.4849, Loss 4.6786, Error 0.5316(best: 0.4575)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0709 | Time 74.6862(66.4684) | Bit/dim 3.9323(3.8996) | Xent 1.5349(1.4845) | Loss 16.7155(11.9269) | Error 0.5461(0.5310) Steps 0(0.00) | Grad Norm 14.7316(8.8832) | Total Time 0.00(0.00)\n",
      "Iter 0710 | Time 69.2808(66.5528) | Bit/dim 3.9079(3.8998) | Xent 1.4340(1.4829) | Loss 11.2311(11.9060) | Error 0.5175(0.5306) Steps 0(0.00) | Grad Norm 5.3907(8.7784) | Total Time 0.00(0.00)\n",
      "Iter 0711 | Time 64.8804(66.5026) | Bit/dim 3.8910(3.8996) | Xent 1.4647(1.4824) | Loss 11.1848(11.8844) | Error 0.5277(0.5305) Steps 0(0.00) | Grad Norm 7.2810(8.7335) | Total Time 0.00(0.00)\n",
      "Iter 0712 | Time 72.1519(66.6721) | Bit/dim 3.9047(3.8997) | Xent 1.4324(1.4809) | Loss 11.1198(11.8615) | Error 0.5165(0.5301) Steps 0(0.00) | Grad Norm 5.5724(8.6387) | Total Time 0.00(0.00)\n",
      "Iter 0713 | Time 72.1549(66.8366) | Bit/dim 3.8728(3.8989) | Xent 1.4249(1.4792) | Loss 10.8633(11.8315) | Error 0.5091(0.5295) Steps 0(0.00) | Grad Norm 3.5511(8.4860) | Total Time 0.00(0.00)\n",
      "Iter 0714 | Time 65.6612(66.8013) | Bit/dim 3.8749(3.8982) | Xent 1.4305(1.4778) | Loss 11.0473(11.8080) | Error 0.5206(0.5292) Steps 0(0.00) | Grad Norm 4.5063(8.3667) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0119 | Time 26.9007, Epoch Time 461.6354(435.4037), Bit/dim 3.8659(best: 3.7881), Xent 1.3555, Loss 4.5437, Error 0.4829(best: 0.4575)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0715 | Time 70.2864(66.9059) | Bit/dim 3.8548(3.8969) | Xent 1.3932(1.4752) | Loss 15.7702(11.9269) | Error 0.5005(0.5283) Steps 0(0.00) | Grad Norm 5.2829(8.2741) | Total Time 0.00(0.00)\n",
      "Iter 0716 | Time 73.5050(67.1038) | Bit/dim 3.8699(3.8961) | Xent 1.4094(1.4732) | Loss 10.7911(11.8928) | Error 0.5120(0.5278) Steps 0(0.00) | Grad Norm 5.4161(8.1884) | Total Time 0.00(0.00)\n",
      "Iter 0717 | Time 64.9010(67.0378) | Bit/dim 3.8425(3.8945) | Xent 1.3952(1.4709) | Loss 10.8948(11.8628) | Error 0.4961(0.5269) Steps 0(0.00) | Grad Norm 5.6267(8.1115) | Total Time 0.00(0.00)\n",
      "Iter 0718 | Time 65.1242(66.9804) | Bit/dim 3.8524(3.8932) | Xent 1.4227(1.4695) | Loss 11.1061(11.8401) | Error 0.5134(0.5265) Steps 0(0.00) | Grad Norm 9.8173(8.1627) | Total Time 0.00(0.00)\n",
      "Iter 0719 | Time 66.8906(66.9777) | Bit/dim 3.8412(3.8917) | Xent 1.3719(1.4665) | Loss 10.8137(11.8094) | Error 0.4900(0.5254) Steps 0(0.00) | Grad Norm 6.7969(8.1217) | Total Time 0.00(0.00)\n",
      "Iter 0720 | Time 66.7911(66.9721) | Bit/dim 3.8373(3.8900) | Xent 1.3466(1.4629) | Loss 10.9040(11.7822) | Error 0.4794(0.5240) Steps 0(0.00) | Grad Norm 3.0718(7.9702) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0120 | Time 27.1390, Epoch Time 450.3674(435.8526), Bit/dim 3.8314(best: 3.7881), Xent 1.3164, Loss 4.4896, Error 0.4737(best: 0.4575)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0721 | Time 75.2018(67.2190) | Bit/dim 3.8275(3.8882) | Xent 1.3479(1.4595) | Loss 15.6768(11.8990) | Error 0.4871(0.5229) Steps 0(0.00) | Grad Norm 5.8519(7.9067) | Total Time 0.00(0.00)\n",
      "Iter 0722 | Time 63.2599(67.1002) | Bit/dim 3.8397(3.8867) | Xent 1.3515(1.4562) | Loss 10.9094(11.8693) | Error 0.4790(0.5216) Steps 0(0.00) | Grad Norm 7.9742(7.9087) | Total Time 0.00(0.00)\n",
      "Iter 0723 | Time 71.5434(67.2335) | Bit/dim 3.8201(3.8847) | Xent 1.3516(1.4531) | Loss 10.7377(11.8354) | Error 0.4854(0.5205) Steps 0(0.00) | Grad Norm 5.9284(7.8493) | Total Time 0.00(0.00)\n",
      "Iter 0724 | Time 67.4304(67.2394) | Bit/dim 3.8180(3.8827) | Xent 1.3482(1.4500) | Loss 10.7588(11.8031) | Error 0.4882(0.5195) Steps 0(0.00) | Grad Norm 3.5523(7.7204) | Total Time 0.00(0.00)\n",
      "Iter 0725 | Time 65.9747(67.2014) | Bit/dim 3.8146(3.8807) | Xent 1.3546(1.4471) | Loss 10.6102(11.7673) | Error 0.4824(0.5184) Steps 0(0.00) | Grad Norm 5.1768(7.6441) | Total Time 0.00(0.00)\n",
      "Iter 0726 | Time 67.0467(67.1968) | Bit/dim 3.8032(3.8783) | Xent 1.3729(1.4449) | Loss 10.7161(11.7358) | Error 0.4966(0.5178) Steps 0(0.00) | Grad Norm 6.7290(7.6166) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0121 | Time 26.8340, Epoch Time 453.4258(436.3798), Bit/dim 3.8037(best: 3.7881), Xent 1.3155, Loss 4.4614, Error 0.4717(best: 0.4575)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0727 | Time 63.9471(67.0993) | Bit/dim 3.8074(3.8762) | Xent 1.3659(1.4425) | Loss 16.1156(11.8672) | Error 0.4914(0.5170) Steps 0(0.00) | Grad Norm 7.4368(7.6112) | Total Time 0.00(0.00)\n",
      "Iter 0728 | Time 68.1036(67.1294) | Bit/dim 3.8160(3.8744) | Xent 1.4015(1.4413) | Loss 10.7766(11.8344) | Error 0.5082(0.5167) Steps 0(0.00) | Grad Norm 9.5602(7.6697) | Total Time 0.00(0.00)\n",
      "Iter 0729 | Time 73.7663(67.3285) | Bit/dim 3.8143(3.8726) | Xent 1.3865(1.4396) | Loss 10.7304(11.8013) | Error 0.4981(0.5162) Steps 0(0.00) | Grad Norm 8.6966(7.7005) | Total Time 0.00(0.00)\n",
      "Iter 0730 | Time 68.5186(67.3643) | Bit/dim 3.7831(3.8699) | Xent 1.3129(1.4358) | Loss 10.7566(11.7700) | Error 0.4676(0.5147) Steps 0(0.00) | Grad Norm 3.6486(7.5790) | Total Time 0.00(0.00)\n",
      "Iter 0731 | Time 72.8428(67.5286) | Bit/dim 3.7905(3.8675) | Xent 1.3581(1.4335) | Loss 10.7543(11.7395) | Error 0.4826(0.5137) Steps 0(0.00) | Grad Norm 4.9182(7.4991) | Total Time 0.00(0.00)\n",
      "Iter 0732 | Time 75.0910(67.7555) | Bit/dim 3.7877(3.8651) | Xent 1.3182(1.4300) | Loss 10.7040(11.7085) | Error 0.4796(0.5127) Steps 0(0.00) | Grad Norm 6.0882(7.4568) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0122 | Time 27.3737, Epoch Time 465.3919(437.2502), Bit/dim 3.7944(best: 3.7881), Xent 1.2793, Loss 4.4340, Error 0.4614(best: 0.4575)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0733 | Time 68.0258(67.7636) | Bit/dim 3.7920(3.8629) | Xent 1.3037(1.4262) | Loss 15.2469(11.8146) | Error 0.4749(0.5116) Steps 0(0.00) | Grad Norm 3.0335(7.3241) | Total Time 0.00(0.00)\n",
      "Iter 0734 | Time 66.9888(67.7403) | Bit/dim 3.7839(3.8606) | Xent 1.3043(1.4226) | Loss 10.4998(11.7752) | Error 0.4701(0.5103) Steps 0(0.00) | Grad Norm 4.0189(7.2250) | Total Time 0.00(0.00)\n",
      "Iter 0735 | Time 69.7625(67.8010) | Bit/dim 3.7825(3.8582) | Xent 1.3368(1.4200) | Loss 10.8561(11.7476) | Error 0.4801(0.5094) Steps 0(0.00) | Grad Norm 6.2924(7.1970) | Total Time 0.00(0.00)\n",
      "Iter 0736 | Time 66.7892(67.7707) | Bit/dim 3.7819(3.8559) | Xent 1.3518(1.4180) | Loss 10.8384(11.7203) | Error 0.4850(0.5087) Steps 0(0.00) | Grad Norm 8.7641(7.2440) | Total Time 0.00(0.00)\n",
      "Iter 0737 | Time 65.5300(67.7034) | Bit/dim 3.8012(3.8543) | Xent 1.3383(1.4156) | Loss 10.8680(11.6947) | Error 0.4828(0.5079) Steps 0(0.00) | Grad Norm 13.1870(7.4223) | Total Time 0.00(0.00)\n",
      "Iter 0738 | Time 67.5819(67.6998) | Bit/dim 3.8214(3.8533) | Xent 1.4197(1.4157) | Loss 10.6704(11.6640) | Error 0.5095(0.5080) Steps 0(0.00) | Grad Norm 17.2631(7.7175) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0123 | Time 26.4671, Epoch Time 446.8713(437.5388), Bit/dim 3.8115(best: 3.7881), Xent 1.4020, Loss 4.5125, Error 0.5014(best: 0.4575)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0739 | Time 68.5560(67.7255) | Bit/dim 3.8183(3.8523) | Xent 1.4491(1.4167) | Loss 15.5588(11.7809) | Error 0.5125(0.5081) Steps 0(0.00) | Grad Norm 14.8928(7.9328) | Total Time 0.00(0.00)\n",
      "Iter 0740 | Time 66.2273(67.6805) | Bit/dim 3.8230(3.8514) | Xent 1.4772(1.4185) | Loss 10.8489(11.7529) | Error 0.5364(0.5089) Steps 0(0.00) | Grad Norm 10.6288(8.0136) | Total Time 0.00(0.00)\n",
      "Iter 0741 | Time 67.7807(67.6835) | Bit/dim 3.7950(3.8497) | Xent 1.3680(1.4170) | Loss 10.5860(11.7179) | Error 0.4969(0.5086) Steps 0(0.00) | Grad Norm 6.3279(7.9631) | Total Time 0.00(0.00)\n",
      "Iter 0742 | Time 73.8818(67.8695) | Bit/dim 3.7931(3.8480) | Xent 1.3847(1.4160) | Loss 10.8721(11.6925) | Error 0.5020(0.5084) Steps 0(0.00) | Grad Norm 6.1074(7.9074) | Total Time 0.00(0.00)\n",
      "Iter 0743 | Time 70.3901(67.9451) | Bit/dim 3.7808(3.8460) | Xent 1.3721(1.4147) | Loss 10.7875(11.6654) | Error 0.4965(0.5080) Steps 0(0.00) | Grad Norm 6.2091(7.8565) | Total Time 0.00(0.00)\n",
      "Iter 0744 | Time 68.7182(67.9683) | Bit/dim 3.8092(3.8449) | Xent 1.3562(1.4130) | Loss 10.8452(11.6408) | Error 0.4878(0.5074) Steps 0(0.00) | Grad Norm 5.3297(7.7807) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0124 | Time 26.8902, Epoch Time 458.4296(438.1655), Bit/dim 3.7936(best: 3.7881), Xent 1.2907, Loss 4.4390, Error 0.4629(best: 0.4575)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0745 | Time 63.0503(67.8208) | Bit/dim 3.7978(3.8435) | Xent 1.3226(1.4103) | Loss 15.5461(11.7579) | Error 0.4739(0.5064) Steps 0(0.00) | Grad Norm 4.7980(7.6912) | Total Time 0.00(0.00)\n",
      "Iter 0746 | Time 67.2638(67.8040) | Bit/dim 3.8044(3.8423) | Xent 1.3208(1.4076) | Loss 10.7118(11.7265) | Error 0.4800(0.5056) Steps 0(0.00) | Grad Norm 5.0001(7.6104) | Total Time 0.00(0.00)\n",
      "Iter 0747 | Time 68.5780(67.8273) | Bit/dim 3.7899(3.8407) | Xent 1.2957(1.4042) | Loss 10.7435(11.6970) | Error 0.4689(0.5045) Steps 0(0.00) | Grad Norm 2.9882(7.4718) | Total Time 0.00(0.00)\n",
      "Iter 0748 | Time 70.7729(67.9156) | Bit/dim 3.7924(3.8393) | Xent 1.2887(1.4007) | Loss 10.7982(11.6701) | Error 0.4595(0.5032) Steps 0(0.00) | Grad Norm 3.6082(7.3559) | Total Time 0.00(0.00)\n",
      "Iter 0749 | Time 70.3398(67.9884) | Bit/dim 3.7959(3.8380) | Xent 1.2883(1.3974) | Loss 10.6545(11.6396) | Error 0.4617(0.5019) Steps 0(0.00) | Grad Norm 3.0094(7.2255) | Total Time 0.00(0.00)\n",
      "Iter 0750 | Time 68.7921(68.0125) | Bit/dim 3.7908(3.8365) | Xent 1.2969(1.3944) | Loss 10.6570(11.6101) | Error 0.4649(0.5008) Steps 0(0.00) | Grad Norm 3.8767(7.1250) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0125 | Time 26.1335, Epoch Time 450.6054(438.5387), Bit/dim 3.7910(best: 3.7881), Xent 1.2489, Loss 4.4155, Error 0.4546(best: 0.4575)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0751 | Time 70.2911(68.0808) | Bit/dim 3.7782(3.8348) | Xent 1.2846(1.3911) | Loss 15.4089(11.7241) | Error 0.4673(0.4998) Steps 0(0.00) | Grad Norm 4.1494(7.0357) | Total Time 0.00(0.00)\n",
      "Iter 0752 | Time 65.8358(68.0135) | Bit/dim 3.7942(3.8336) | Xent 1.3012(1.3884) | Loss 10.7083(11.6936) | Error 0.4688(0.4989) Steps 0(0.00) | Grad Norm 6.8081(7.0289) | Total Time 0.00(0.00)\n",
      "Iter 0753 | Time 61.8734(67.8293) | Bit/dim 3.7815(3.8320) | Xent 1.3556(1.3874) | Loss 10.8104(11.6671) | Error 0.4811(0.4983) Steps 0(0.00) | Grad Norm 9.0395(7.0892) | Total Time 0.00(0.00)\n",
      "Iter 0754 | Time 67.5557(67.8211) | Bit/dim 3.7785(3.8304) | Xent 1.3790(1.3871) | Loss 10.8535(11.6427) | Error 0.4900(0.4981) Steps 0(0.00) | Grad Norm 9.9488(7.1750) | Total Time 0.00(0.00)\n",
      "Iter 0755 | Time 69.0469(67.8578) | Bit/dim 3.7786(3.8289) | Xent 1.3513(1.3861) | Loss 10.2974(11.6024) | Error 0.4850(0.4977) Steps 0(0.00) | Grad Norm 6.9054(7.1669) | Total Time 0.00(0.00)\n",
      "Iter 0756 | Time 70.8511(67.9476) | Bit/dim 3.7859(3.8276) | Xent 1.2997(1.3835) | Loss 10.4036(11.5664) | Error 0.4679(0.4968) Steps 0(0.00) | Grad Norm 5.0163(7.1024) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0126 | Time 26.8650, Epoch Time 448.2407(438.8298), Bit/dim 3.7780(best: 3.7881), Xent 1.2992, Loss 4.4275, Error 0.4710(best: 0.4546)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0757 | Time 65.0377(67.8603) | Bit/dim 3.7838(3.8263) | Xent 1.3331(1.3820) | Loss 14.9018(11.6665) | Error 0.4822(0.4964) Steps 0(0.00) | Grad Norm 7.7979(7.1233) | Total Time 0.00(0.00)\n",
      "Iter 0758 | Time 63.2003(67.7205) | Bit/dim 3.7990(3.8254) | Xent 1.3317(1.3804) | Loss 10.6103(11.6348) | Error 0.4842(0.4960) Steps 0(0.00) | Grad Norm 8.8391(7.1747) | Total Time 0.00(0.00)\n",
      "Iter 0759 | Time 67.1911(67.7047) | Bit/dim 3.7741(3.8239) | Xent 1.3048(1.3782) | Loss 10.6531(11.6053) | Error 0.4606(0.4949) Steps 0(0.00) | Grad Norm 6.3151(7.1490) | Total Time 0.00(0.00)\n",
      "Iter 0760 | Time 60.4370(67.4866) | Bit/dim 3.7986(3.8231) | Xent 1.2968(1.3757) | Loss 10.3926(11.5689) | Error 0.4666(0.4941) Steps 0(0.00) | Grad Norm 4.6285(7.0733) | Total Time 0.00(0.00)\n",
      "Iter 0761 | Time 61.4462(67.3054) | Bit/dim 3.7711(3.8216) | Xent 1.2708(1.3726) | Loss 10.5979(11.5398) | Error 0.4533(0.4929) Steps 0(0.00) | Grad Norm 4.5824(6.9986) | Total Time 0.00(0.00)\n",
      "Iter 0762 | Time 66.2306(67.2732) | Bit/dim 3.7741(3.8202) | Xent 1.3509(1.3719) | Loss 10.6142(11.5120) | Error 0.4826(0.4926) Steps 0(0.00) | Grad Norm 7.9165(7.0261) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0127 | Time 26.7515, Epoch Time 426.3223(438.4546), Bit/dim 3.7953(best: 3.7780), Xent 1.3128, Loss 4.4517, Error 0.4786(best: 0.4546)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0763 | Time 63.0540(67.1466) | Bit/dim 3.8066(3.8197) | Xent 1.3550(1.3714) | Loss 15.5807(11.6341) | Error 0.4964(0.4927) Steps 0(0.00) | Grad Norm 12.4887(7.1900) | Total Time 0.00(0.00)\n",
      "Iter 0764 | Time 63.9295(67.0501) | Bit/dim 3.8400(3.8204) | Xent 1.4780(1.3746) | Loss 10.9697(11.6142) | Error 0.5220(0.4936) Steps 0(0.00) | Grad Norm 22.5946(7.6522) | Total Time 0.00(0.00)\n",
      "Iter 0765 | Time 72.2297(67.2055) | Bit/dim 3.8233(3.8204) | Xent 1.6089(1.3817) | Loss 10.9722(11.5949) | Error 0.5581(0.4955) Steps 0(0.00) | Grad Norm 19.7358(8.0147) | Total Time 0.00(0.00)\n",
      "Iter 0766 | Time 69.0462(67.2607) | Bit/dim 3.8545(3.8215) | Xent 1.7281(1.3920) | Loss 11.1058(11.5802) | Error 0.6170(0.4991) Steps 0(0.00) | Grad Norm 12.8126(8.1586) | Total Time 0.00(0.00)\n",
      "Iter 0767 | Time 69.0527(67.3145) | Bit/dim 3.8497(3.8223) | Xent 1.4151(1.3927) | Loss 10.8183(11.5574) | Error 0.5068(0.4994) Steps 0(0.00) | Grad Norm 6.0167(8.0944) | Total Time 0.00(0.00)\n",
      "Iter 0768 | Time 73.4545(67.4987) | Bit/dim 3.8679(3.8237) | Xent 1.5774(1.3983) | Loss 11.1421(11.5449) | Error 0.5644(0.5013) Steps 0(0.00) | Grad Norm 11.6134(8.1999) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0128 | Time 26.3465, Epoch Time 453.3505(438.9015), Bit/dim 3.8520(best: 3.7780), Xent 1.4345, Loss 4.5692, Error 0.5205(best: 0.4546)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0769 | Time 64.9672(67.4227) | Bit/dim 3.8559(3.8246) | Xent 1.4765(1.4006) | Loss 15.6325(11.6675) | Error 0.5291(0.5022) Steps 0(0.00) | Grad Norm 6.8824(8.1604) | Total Time 0.00(0.00)\n",
      "Iter 0770 | Time 67.7129(67.4314) | Bit/dim 3.8344(3.8249) | Xent 1.4373(1.4017) | Loss 10.7945(11.6414) | Error 0.5177(0.5026) Steps 0(0.00) | Grad Norm 4.6562(8.0553) | Total Time 0.00(0.00)\n",
      "Iter 0771 | Time 74.4792(67.6428) | Bit/dim 3.8560(3.8259) | Xent 1.4394(1.4029) | Loss 10.8121(11.6165) | Error 0.5229(0.5032) Steps 0(0.00) | Grad Norm 8.5258(8.0694) | Total Time 0.00(0.00)\n",
      "Iter 0772 | Time 62.0897(67.4763) | Bit/dim 3.8403(3.8263) | Xent 1.3929(1.4026) | Loss 10.7173(11.5895) | Error 0.5064(0.5033) Steps 0(0.00) | Grad Norm 4.4598(7.9611) | Total Time 0.00(0.00)\n",
      "Iter 0773 | Time 68.6452(67.5113) | Bit/dim 3.8362(3.8266) | Xent 1.3944(1.4023) | Loss 10.5491(11.5583) | Error 0.4950(0.5031) Steps 0(0.00) | Grad Norm 5.5200(7.8879) | Total Time 0.00(0.00)\n",
      "Iter 0774 | Time 67.7391(67.5182) | Bit/dim 3.8216(3.8264) | Xent 1.4035(1.4023) | Loss 10.9556(11.5402) | Error 0.5030(0.5031) Steps 0(0.00) | Grad Norm 6.3814(7.8427) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0129 | Time 26.6138, Epoch Time 448.4393(439.1876), Bit/dim 3.8408(best: 3.7780), Xent 1.3384, Loss 4.5100, Error 0.4874(best: 0.4546)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0775 | Time 64.5445(67.4289) | Bit/dim 3.8349(3.8267) | Xent 1.3719(1.4014) | Loss 15.9425(11.6723) | Error 0.4924(0.5027) Steps 0(0.00) | Grad Norm 6.5683(7.8044) | Total Time 0.00(0.00)\n",
      "Iter 0776 | Time 63.1972(67.3020) | Bit/dim 3.8300(3.8268) | Xent 1.3580(1.4001) | Loss 10.7672(11.6451) | Error 0.4866(0.5023) Steps 0(0.00) | Grad Norm 4.1576(7.6950) | Total Time 0.00(0.00)\n",
      "Iter 0777 | Time 63.3765(67.1842) | Bit/dim 3.8161(3.8265) | Xent 1.3654(1.3991) | Loss 10.6955(11.6166) | Error 0.4914(0.5019) Steps 0(0.00) | Grad Norm 3.4913(7.5689) | Total Time 0.00(0.00)\n",
      "Iter 0778 | Time 65.8857(67.1453) | Bit/dim 3.8018(3.8257) | Xent 1.3672(1.3981) | Loss 10.8689(11.5942) | Error 0.4952(0.5017) Steps 0(0.00) | Grad Norm 5.1144(7.4953) | Total Time 0.00(0.00)\n",
      "Iter 0779 | Time 74.3654(67.3619) | Bit/dim 3.8060(3.8251) | Xent 1.3245(1.3959) | Loss 10.7856(11.5700) | Error 0.4744(0.5009) Steps 0(0.00) | Grad Norm 4.2463(7.3978) | Total Time 0.00(0.00)\n",
      "Iter 0780 | Time 64.9638(67.2899) | Bit/dim 3.8021(3.8245) | Xent 1.3181(1.3936) | Loss 10.5985(11.5408) | Error 0.4677(0.4999) Steps 0(0.00) | Grad Norm 2.6996(7.2569) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0130 | Time 26.8713, Epoch Time 439.0031(439.1821), Bit/dim 3.8021(best: 3.7780), Xent 1.2987, Loss 4.4515, Error 0.4685(best: 0.4546)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0781 | Time 70.2193(67.3778) | Bit/dim 3.8044(3.8239) | Xent 1.3470(1.3922) | Loss 15.4810(11.6590) | Error 0.4752(0.4992) Steps 0(0.00) | Grad Norm 4.8952(7.1860) | Total Time 0.00(0.00)\n",
      "Iter 0782 | Time 71.9237(67.5142) | Bit/dim 3.7966(3.8230) | Xent 1.3298(1.3903) | Loss 10.9361(11.6373) | Error 0.4799(0.4986) Steps 0(0.00) | Grad Norm 5.9965(7.1503) | Total Time 0.00(0.00)\n",
      "Iter 0783 | Time 68.2448(67.5361) | Bit/dim 3.7917(3.8221) | Xent 1.3474(1.3890) | Loss 10.7674(11.6112) | Error 0.4832(0.4981) Steps 0(0.00) | Grad Norm 6.4886(7.1305) | Total Time 0.00(0.00)\n",
      "Iter 0784 | Time 63.0779(67.4024) | Bit/dim 3.7905(3.8211) | Xent 1.3298(1.3873) | Loss 10.6349(11.5819) | Error 0.4799(0.4976) Steps 0(0.00) | Grad Norm 6.0041(7.0967) | Total Time 0.00(0.00)\n",
      "Iter 0785 | Time 67.6183(67.4088) | Bit/dim 3.7860(3.8201) | Xent 1.3202(1.3852) | Loss 10.7868(11.5581) | Error 0.4792(0.4970) Steps 0(0.00) | Grad Norm 3.6041(6.9919) | Total Time 0.00(0.00)\n",
      "Iter 0786 | Time 63.2935(67.2854) | Bit/dim 3.7709(3.8186) | Xent 1.3207(1.3833) | Loss 10.5851(11.5289) | Error 0.4768(0.4964) Steps 0(0.00) | Grad Norm 3.4279(6.8850) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0131 | Time 26.6419, Epoch Time 447.1050(439.4197), Bit/dim 3.7728(best: 3.7780), Xent 1.2674, Loss 4.4065, Error 0.4527(best: 0.4546)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0787 | Time 75.2817(67.5253) | Bit/dim 3.7693(3.8171) | Xent 1.3057(1.3810) | Loss 15.7139(11.6544) | Error 0.4712(0.4957) Steps 0(0.00) | Grad Norm 2.7501(6.7609) | Total Time 0.00(0.00)\n",
      "Iter 0788 | Time 63.9118(67.4169) | Bit/dim 3.7709(3.8157) | Xent 1.3138(1.3790) | Loss 10.6903(11.6255) | Error 0.4785(0.4952) Steps 0(0.00) | Grad Norm 3.4305(6.6610) | Total Time 0.00(0.00)\n",
      "Iter 0789 | Time 68.6264(67.4531) | Bit/dim 3.7738(3.8145) | Xent 1.3129(1.3770) | Loss 10.4952(11.5916) | Error 0.4738(0.4945) Steps 0(0.00) | Grad Norm 3.3759(6.5625) | Total Time 0.00(0.00)\n",
      "Iter 0790 | Time 66.9055(67.4367) | Bit/dim 3.7639(3.8130) | Xent 1.2870(1.3743) | Loss 10.5417(11.5601) | Error 0.4605(0.4935) Steps 0(0.00) | Grad Norm 2.5261(6.4414) | Total Time 0.00(0.00)\n",
      "Iter 0791 | Time 61.2645(67.2516) | Bit/dim 3.7742(3.8118) | Xent 1.2919(1.3718) | Loss 10.6343(11.5323) | Error 0.4604(0.4925) Steps 0(0.00) | Grad Norm 4.0683(6.3702) | Total Time 0.00(0.00)\n",
      "Iter 0792 | Time 71.7399(67.3862) | Bit/dim 3.7671(3.8105) | Xent 1.3163(1.3701) | Loss 10.6976(11.5073) | Error 0.4718(0.4919) Steps 0(0.00) | Grad Norm 8.1464(6.4235) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0132 | Time 26.3848, Epoch Time 450.1093(439.7404), Bit/dim 3.7675(best: 3.7728), Xent 1.3634, Loss 4.4492, Error 0.4830(best: 0.4527)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0793 | Time 66.4549(67.3583) | Bit/dim 3.7745(3.8094) | Xent 1.3910(1.3708) | Loss 52.3235(12.7318) | Error 0.4856(0.4917) Steps 0(0.00) | Grad Norm 13.9236(6.6485) | Total Time 0.00(0.00)\n",
      "Iter 0794 | Time 76.3433(67.6278) | Bit/dim 3.7955(3.8090) | Xent 1.5014(1.3747) | Loss 10.7781(12.6732) | Error 0.5224(0.4926) Steps 0(0.00) | Grad Norm 19.3978(7.0310) | Total Time 0.00(0.00)\n",
      "Iter 0795 | Time 64.6557(67.5387) | Bit/dim 3.7836(3.8082) | Xent 1.5345(1.3795) | Loss 10.9409(12.6212) | Error 0.5421(0.4941) Steps 0(0.00) | Grad Norm 12.6716(7.2002) | Total Time 0.00(0.00)\n",
      "Iter 0796 | Time 61.9029(67.3696) | Bit/dim 3.7828(3.8074) | Xent 1.3802(1.3795) | Loss 10.7321(12.5645) | Error 0.4965(0.4942) Steps 0(0.00) | Grad Norm 8.1543(7.2288) | Total Time 0.00(0.00)\n",
      "Iter 0797 | Time 67.4039(67.3706) | Bit/dim 3.8066(3.8074) | Xent 1.4375(1.3812) | Loss 10.8351(12.5126) | Error 0.5191(0.4949) Steps 0(0.00) | Grad Norm 11.9514(7.3705) | Total Time 0.00(0.00)\n",
      "Iter 0798 | Time 65.8892(67.3262) | Bit/dim 3.7868(3.8068) | Xent 1.4284(1.3827) | Loss 10.8931(12.4641) | Error 0.5077(0.4953) Steps 0(0.00) | Grad Norm 8.4353(7.4024) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0133 | Time 25.8045, Epoch Time 444.1292(439.8721), Bit/dim 3.7908(best: 3.7675), Xent 1.3564, Loss 4.4690, Error 0.4923(best: 0.4527)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0799 | Time 70.0967(67.4093) | Bit/dim 3.7842(3.8061) | Xent 1.3900(1.3829) | Loss 15.0102(12.5404) | Error 0.5012(0.4955) Steps 0(0.00) | Grad Norm 6.7330(7.3823) | Total Time 0.00(0.00)\n",
      "Iter 0800 | Time 64.7760(67.3303) | Bit/dim 3.7897(3.8056) | Xent 1.3625(1.3823) | Loss 10.6435(12.4835) | Error 0.4931(0.4954) Steps 0(0.00) | Grad Norm 3.3428(7.2612) | Total Time 0.00(0.00)\n",
      "Iter 0801 | Time 70.8023(67.4344) | Bit/dim 3.7906(3.8052) | Xent 1.3338(1.3808) | Loss 10.7411(12.4313) | Error 0.4814(0.4950) Steps 0(0.00) | Grad Norm 5.4300(7.2062) | Total Time 0.00(0.00)\n",
      "Iter 0802 | Time 66.0181(67.3919) | Bit/dim 3.7938(3.8048) | Xent 1.3221(1.3790) | Loss 10.6125(12.3767) | Error 0.4761(0.4944) Steps 0(0.00) | Grad Norm 4.6424(7.1293) | Total Time 0.00(0.00)\n",
      "Iter 0803 | Time 67.7276(67.4020) | Bit/dim 3.7927(3.8045) | Xent 1.3131(1.3771) | Loss 10.6704(12.3255) | Error 0.4664(0.4936) Steps 0(0.00) | Grad Norm 4.0018(7.0355) | Total Time 0.00(0.00)\n",
      "Iter 0804 | Time 63.4174(67.2825) | Bit/dim 3.7871(3.8040) | Xent 1.2856(1.3743) | Loss 10.3195(12.2653) | Error 0.4651(0.4927) Steps 0(0.00) | Grad Norm 2.9171(6.9119) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0134 | Time 27.0537, Epoch Time 446.3048(440.0651), Bit/dim 3.7833(best: 3.7675), Xent 1.2595, Loss 4.4130, Error 0.4537(best: 0.4527)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0805 | Time 69.1976(67.3399) | Bit/dim 3.7940(3.8037) | Xent 1.2879(1.3717) | Loss 15.5017(12.3624) | Error 0.4605(0.4918) Steps 0(0.00) | Grad Norm 4.7344(6.8466) | Total Time 0.00(0.00)\n",
      "Iter 0806 | Time 70.8522(67.4453) | Bit/dim 3.7896(3.8032) | Xent 1.2869(1.3692) | Loss 10.7725(12.3147) | Error 0.4645(0.4909) Steps 0(0.00) | Grad Norm 4.0622(6.7631) | Total Time 0.00(0.00)\n",
      "Iter 0807 | Time 63.9064(67.3391) | Bit/dim 3.7775(3.8025) | Xent 1.2789(1.3665) | Loss 10.5695(12.2624) | Error 0.4619(0.4901) Steps 0(0.00) | Grad Norm 3.1304(6.6541) | Total Time 0.00(0.00)\n",
      "Iter 0808 | Time 64.9138(67.2664) | Bit/dim 3.7544(3.8010) | Xent 1.2795(1.3639) | Loss 10.4865(12.2091) | Error 0.4574(0.4891) Steps 0(0.00) | Grad Norm 2.6866(6.5351) | Total Time 0.00(0.00)\n",
      "Iter 0809 | Time 68.9430(67.3167) | Bit/dim 3.7656(3.8000) | Xent 1.2894(1.3616) | Loss 10.6411(12.1621) | Error 0.4577(0.4882) Steps 0(0.00) | Grad Norm 4.9252(6.4868) | Total Time 0.00(0.00)\n",
      "Iter 0810 | Time 67.5971(67.3251) | Bit/dim 3.7655(3.7989) | Xent 1.2939(1.3596) | Loss 10.5119(12.1125) | Error 0.4701(0.4876) Steps 0(0.00) | Grad Norm 5.5773(6.4595) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0135 | Time 26.1025, Epoch Time 447.2542(440.2807), Bit/dim 3.7645(best: 3.7675), Xent 1.2458, Loss 4.3874, Error 0.4511(best: 0.4527)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0811 | Time 64.4117(67.2377) | Bit/dim 3.7570(3.7977) | Xent 1.2765(1.3571) | Loss 14.8276(12.1940) | Error 0.4636(0.4869) Steps 0(0.00) | Grad Norm 6.1330(6.4497) | Total Time 0.00(0.00)\n",
      "Iter 0812 | Time 61.7030(67.0716) | Bit/dim 3.7566(3.7964) | Xent 1.3023(1.3555) | Loss 10.6970(12.1491) | Error 0.4719(0.4864) Steps 0(0.00) | Grad Norm 5.6007(6.4242) | Total Time 0.00(0.00)\n",
      "Iter 0813 | Time 63.4721(66.9637) | Bit/dim 3.7482(3.7950) | Xent 1.2609(1.3526) | Loss 10.6565(12.1043) | Error 0.4520(0.4854) Steps 0(0.00) | Grad Norm 2.5717(6.3086) | Total Time 0.00(0.00)\n",
      "Iter 0814 | Time 74.7397(67.1969) | Bit/dim 3.7634(3.7940) | Xent 1.2704(1.3502) | Loss 10.4254(12.0539) | Error 0.4559(0.4845) Steps 0(0.00) | Grad Norm 4.8848(6.2659) | Total Time 0.00(0.00)\n",
      "Iter 0815 | Time 69.4987(67.2660) | Bit/dim 3.7556(3.7929) | Xent 1.2855(1.3482) | Loss 10.3887(12.0040) | Error 0.4649(0.4839) Steps 0(0.00) | Grad Norm 6.0130(6.2583) | Total Time 0.00(0.00)\n",
      "Iter 0816 | Time 61.7556(67.1007) | Bit/dim 3.7539(3.7917) | Xent 1.2818(1.3462) | Loss 10.6392(11.9630) | Error 0.4553(0.4831) Steps 0(0.00) | Grad Norm 5.5156(6.2361) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0136 | Time 27.6408, Epoch Time 439.2668(440.2503), Bit/dim 3.7566(best: 3.7645), Xent 1.2548, Loss 4.3840, Error 0.4451(best: 0.4511)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0817 | Time 71.1849(67.2232) | Bit/dim 3.7607(3.7908) | Xent 1.3102(1.3451) | Loss 14.7125(12.0455) | Error 0.4676(0.4826) Steps 0(0.00) | Grad Norm 7.5976(6.2769) | Total Time 0.00(0.00)\n",
      "Iter 0818 | Time 68.1370(67.2506) | Bit/dim 3.7542(3.7897) | Xent 1.2532(1.3424) | Loss 10.6156(12.0026) | Error 0.4490(0.4816) Steps 0(0.00) | Grad Norm 5.1026(6.2417) | Total Time 0.00(0.00)\n",
      "Iter 0819 | Time 68.3291(67.2830) | Bit/dim 3.7510(3.7885) | Xent 1.2722(1.3403) | Loss 10.6378(11.9617) | Error 0.4555(0.4808) Steps 0(0.00) | Grad Norm 5.2110(6.2108) | Total Time 0.00(0.00)\n",
      "Iter 0820 | Time 69.0027(67.3346) | Bit/dim 3.7389(3.7870) | Xent 1.2982(1.3390) | Loss 10.5247(11.9186) | Error 0.4637(0.4803) Steps 0(0.00) | Grad Norm 5.6177(6.1930) | Total Time 0.00(0.00)\n",
      "Iter 0821 | Time 66.5362(67.3106) | Bit/dim 3.7497(3.7859) | Xent 1.2863(1.3374) | Loss 10.7224(11.8827) | Error 0.4633(0.4798) Steps 0(0.00) | Grad Norm 6.1127(6.1906) | Total Time 0.00(0.00)\n",
      "Iter 0822 | Time 66.9532(67.2999) | Bit/dim 3.7506(3.7849) | Xent 1.2627(1.3352) | Loss 10.4878(11.8408) | Error 0.4634(0.4793) Steps 0(0.00) | Grad Norm 6.0381(6.1860) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0137 | Time 26.9588, Epoch Time 452.9687(440.6319), Bit/dim 3.7489(best: 3.7566), Xent 1.2421, Loss 4.3700, Error 0.4436(best: 0.4451)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0823 | Time 61.6896(67.1316) | Bit/dim 3.7478(3.7837) | Xent 1.2623(1.3330) | Loss 14.6252(11.9244) | Error 0.4517(0.4785) Steps 0(0.00) | Grad Norm 6.1360(6.1845) | Total Time 0.00(0.00)\n",
      "Iter 0824 | Time 68.0903(67.1603) | Bit/dim 3.7579(3.7830) | Xent 1.2628(1.3309) | Loss 10.6387(11.8858) | Error 0.4589(0.4779) Steps 0(0.00) | Grad Norm 5.5725(6.1661) | Total Time 0.00(0.00)\n",
      "Iter 0825 | Time 63.2579(67.0433) | Bit/dim 3.7401(3.7817) | Xent 1.2600(1.3288) | Loss 10.4051(11.8414) | Error 0.4489(0.4770) Steps 0(0.00) | Grad Norm 5.2181(6.1377) | Total Time 0.00(0.00)\n",
      "Iter 0826 | Time 74.6760(67.2723) | Bit/dim 3.7277(3.7801) | Xent 1.2293(1.3258) | Loss 10.6392(11.8053) | Error 0.4386(0.4759) Steps 0(0.00) | Grad Norm 5.1650(6.1085) | Total Time 0.00(0.00)\n",
      "Iter 0827 | Time 64.4618(67.1879) | Bit/dim 3.7409(3.7789) | Xent 1.2783(1.3244) | Loss 10.5684(11.7682) | Error 0.4550(0.4752) Steps 0(0.00) | Grad Norm 6.5922(6.1230) | Total Time 0.00(0.00)\n",
      "Iter 0828 | Time 64.5644(67.1092) | Bit/dim 3.7503(3.7780) | Xent 1.2611(1.3225) | Loss 10.4070(11.7274) | Error 0.4556(0.4747) Steps 0(0.00) | Grad Norm 8.3644(6.1903) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0138 | Time 26.2140, Epoch Time 439.1075(440.5862), Bit/dim 3.7574(best: 3.7489), Xent 1.2289, Loss 4.3718, Error 0.4402(best: 0.4436)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0829 | Time 64.7424(67.0382) | Bit/dim 3.7764(3.7780) | Xent 1.2693(1.3209) | Loss 14.8791(11.8219) | Error 0.4549(0.4741) Steps 0(0.00) | Grad Norm 8.7440(6.2669) | Total Time 0.00(0.00)\n",
      "Iter 0830 | Time 63.5811(66.9345) | Bit/dim 3.7456(3.7770) | Xent 1.2188(1.3178) | Loss 10.2576(11.7750) | Error 0.4386(0.4730) Steps 0(0.00) | Grad Norm 4.7781(6.2222) | Total Time 0.00(0.00)\n",
      "Iter 0831 | Time 68.2964(66.9754) | Bit/dim 3.7338(3.7757) | Xent 1.2373(1.3154) | Loss 10.4920(11.7365) | Error 0.4365(0.4719) Steps 0(0.00) | Grad Norm 4.4788(6.1699) | Total Time 0.00(0.00)\n",
      "Iter 0832 | Time 65.4393(66.9293) | Bit/dim 3.7538(3.7751) | Xent 1.2437(1.3132) | Loss 10.7103(11.7057) | Error 0.4520(0.4713) Steps 0(0.00) | Grad Norm 8.0796(6.2272) | Total Time 0.00(0.00)\n",
      "Iter 0833 | Time 60.9184(66.7490) | Bit/dim 3.7505(3.7743) | Xent 1.2527(1.3114) | Loss 10.6514(11.6741) | Error 0.4535(0.4708) Steps 0(0.00) | Grad Norm 8.6425(6.2997) | Total Time 0.00(0.00)\n",
      "Iter 0834 | Time 66.3105(66.7358) | Bit/dim 3.7534(3.7737) | Xent 1.2740(1.3103) | Loss 10.3705(11.6350) | Error 0.4605(0.4705) Steps 0(0.00) | Grad Norm 8.8095(6.3750) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0139 | Time 26.2870, Epoch Time 431.4323(440.3115), Bit/dim 3.7552(best: 3.7489), Xent 1.4289, Loss 4.4696, Error 0.5107(best: 0.4402)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0835 | Time 62.5110(66.6091) | Bit/dim 3.7527(3.7731) | Xent 1.4785(1.3154) | Loss 15.1695(11.7410) | Error 0.5230(0.4720) Steps 0(0.00) | Grad Norm 15.8954(6.6606) | Total Time 0.00(0.00)\n",
      "Iter 0836 | Time 75.7990(66.8848) | Bit/dim 3.8537(3.7755) | Xent 1.7846(1.3294) | Loss 11.3691(11.7299) | Error 0.5864(0.4755) Steps 0(0.00) | Grad Norm 24.2101(7.1871) | Total Time 0.00(0.00)\n",
      "Iter 0837 | Time 63.7564(66.7909) | Bit/dim 3.9098(3.7795) | Xent 1.9051(1.3467) | Loss 11.6718(11.7281) | Error 0.6658(0.4812) Steps 0(0.00) | Grad Norm 16.5324(7.4674) | Total Time 0.00(0.00)\n",
      "Iter 0838 | Time 71.8097(66.9415) | Bit/dim 3.9887(3.7858) | Xent 1.6756(1.3566) | Loss 11.6522(11.7258) | Error 0.5865(0.4843) Steps 0(0.00) | Grad Norm 12.9094(7.6307) | Total Time 0.00(0.00)\n",
      "Iter 0839 | Time 67.2970(66.9521) | Bit/dim 3.9782(3.7916) | Xent 1.7103(1.3672) | Loss 11.4032(11.7162) | Error 0.6081(0.4880) Steps 0(0.00) | Grad Norm 12.4295(7.7746) | Total Time 0.00(0.00)\n",
      "Iter 0840 | Time 71.3035(67.0827) | Bit/dim 4.0761(3.8001) | Xent 2.1446(1.3905) | Loss 12.1734(11.7299) | Error 0.6560(0.4931) Steps 0(0.00) | Grad Norm 26.7464(8.3438) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0140 | Time 28.5765, Epoch Time 456.7923(440.8060), Bit/dim 4.1025(best: 3.7489), Xent 1.6446, Loss 4.9247, Error 0.5745(best: 0.4402)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0841 | Time 69.9135(67.1676) | Bit/dim 4.0961(3.8090) | Xent 1.7378(1.4009) | Loss 17.9923(11.9178) | Error 0.6044(0.4964) Steps 0(0.00) | Grad Norm 22.1876(8.7591) | Total Time 0.00(0.00)\n",
      "Iter 0842 | Time 64.5636(67.0895) | Bit/dim 4.1593(3.8195) | Xent 1.6058(1.4071) | Loss 11.3965(11.9021) | Error 0.5714(0.4987) Steps 0(0.00) | Grad Norm 6.0816(8.6788) | Total Time 0.00(0.00)\n",
      "Iter 0843 | Time 65.8817(67.0533) | Bit/dim 4.1110(3.8282) | Xent 1.5948(1.4127) | Loss 11.6069(11.8933) | Error 0.5717(0.5009) Steps 0(0.00) | Grad Norm 6.9456(8.6268) | Total Time 0.00(0.00)\n",
      "Iter 0844 | Time 80.8061(67.4658) | Bit/dim 4.0672(3.8354) | Xent 1.5969(1.4182) | Loss 11.6898(11.8872) | Error 0.5787(0.5032) Steps 0(0.00) | Grad Norm 6.1584(8.5527) | Total Time 0.00(0.00)\n",
      "Iter 0845 | Time 69.5175(67.5274) | Bit/dim 4.0491(3.8418) | Xent 1.5673(1.4227) | Loss 11.5003(11.8756) | Error 0.5617(0.5050) Steps 0(0.00) | Grad Norm 5.3307(8.4561) | Total Time 0.00(0.00)\n",
      "Iter 0846 | Time 72.1774(67.6669) | Bit/dim 4.0627(3.8484) | Xent 1.5375(1.4261) | Loss 11.4226(11.8620) | Error 0.5556(0.5065) Steps 0(0.00) | Grad Norm 5.7478(8.3748) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0141 | Time 28.1305, Epoch Time 466.5612(441.5786), Bit/dim 3.9988(best: 3.7489), Xent 1.4492, Loss 4.7234, Error 0.5191(best: 0.4402)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0847 | Time 66.5095(67.6322) | Bit/dim 3.9970(3.8529) | Xent 1.5159(1.4288) | Loss 16.1203(11.9897) | Error 0.5381(0.5074) Steps 0(0.00) | Grad Norm 4.3018(8.2526) | Total Time 0.00(0.00)\n",
      "Iter 0848 | Time 72.7887(67.7869) | Bit/dim 3.9720(3.8565) | Xent 1.4844(1.4305) | Loss 11.4723(11.9742) | Error 0.5320(0.5082) Steps 0(0.00) | Grad Norm 4.4785(8.1394) | Total Time 0.00(0.00)\n",
      "Iter 0849 | Time 69.6033(67.8414) | Bit/dim 3.9486(3.8592) | Xent 1.5208(1.4332) | Loss 11.1826(11.9504) | Error 0.5393(0.5091) Steps 0(0.00) | Grad Norm 5.5455(8.0616) | Total Time 0.00(0.00)\n",
      "Iter 0850 | Time 72.8643(67.9920) | Bit/dim 3.9496(3.8619) | Xent 1.4528(1.4338) | Loss 11.0966(11.9248) | Error 0.5188(0.5094) Steps 0(0.00) | Grad Norm 4.2821(7.9482) | Total Time 0.00(0.00)\n",
      "Iter 0851 | Time 73.2398(68.1495) | Bit/dim 3.9085(3.8633) | Xent 1.4688(1.4348) | Loss 11.3253(11.9068) | Error 0.5270(0.5099) Steps 0(0.00) | Grad Norm 4.8929(7.8566) | Total Time 0.00(0.00)\n",
      "Iter 0852 | Time 73.9290(68.3229) | Bit/dim 3.9074(3.8647) | Xent 1.4308(1.4347) | Loss 11.2505(11.8872) | Error 0.5191(0.5102) Steps 0(0.00) | Grad Norm 4.3709(7.7520) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0142 | Time 29.5162, Epoch Time 474.4063(442.5634), Bit/dim 3.8995(best: 3.7489), Xent 1.3673, Loss 4.5832, Error 0.4879(best: 0.4402)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0853 | Time 73.5918(68.4809) | Bit/dim 3.8937(3.8655) | Xent 1.3967(1.4336) | Loss 15.4614(11.9944) | Error 0.4991(0.5099) Steps 0(0.00) | Grad Norm 3.1341(7.6134) | Total Time 0.00(0.00)\n",
      "Iter 0854 | Time 82.2931(68.8953) | Bit/dim 3.8749(3.8658) | Xent 1.4103(1.4329) | Loss 11.0829(11.9670) | Error 0.5015(0.5096) Steps 0(0.00) | Grad Norm 3.0700(7.4771) | Total Time 0.00(0.00)\n",
      "Iter 0855 | Time 69.9840(68.9280) | Bit/dim 3.8699(3.8659) | Xent 1.3962(1.4318) | Loss 10.9121(11.9354) | Error 0.5004(0.5093) Steps 0(0.00) | Grad Norm 3.2662(7.3508) | Total Time 0.00(0.00)\n",
      "Iter 0856 | Time 73.6026(69.0682) | Bit/dim 3.8595(3.8657) | Xent 1.3915(1.4306) | Loss 11.1526(11.9119) | Error 0.4910(0.5088) Steps 0(0.00) | Grad Norm 3.6606(7.2401) | Total Time 0.00(0.00)\n",
      "Iter 0857 | Time 75.9596(69.2749) | Bit/dim 3.8608(3.8656) | Xent 1.3849(1.4292) | Loss 11.1939(11.8904) | Error 0.4910(0.5082) Steps 0(0.00) | Grad Norm 5.3713(7.1840) | Total Time 0.00(0.00)\n",
      "Iter 0858 | Time 77.5345(69.5227) | Bit/dim 3.8768(3.8659) | Xent 1.4368(1.4294) | Loss 10.4253(11.8464) | Error 0.5082(0.5082) Steps 0(0.00) | Grad Norm 8.6948(7.2294) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0143 | Time 27.6055, Epoch Time 496.3075(444.1758), Bit/dim 3.8883(best: 3.7489), Xent 1.4665, Loss 4.6216, Error 0.5255(best: 0.4402)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0859 | Time 68.2533(69.4846) | Bit/dim 3.8800(3.8663) | Xent 1.4879(1.4312) | Loss 16.6170(11.9895) | Error 0.5335(0.5090) Steps 0(0.00) | Grad Norm 28.8614(7.8783) | Total Time 0.00(0.00)\n",
      "Iter 0860 | Time 65.4348(69.3631) | Bit/dim 3.8512(3.8659) | Xent 1.3778(1.4296) | Loss 10.8917(11.9566) | Error 0.4920(0.5085) Steps 0(0.00) | Grad Norm 5.7024(7.8131) | Total Time 0.00(0.00)\n",
      "Iter 0861 | Time 72.0524(69.4438) | Bit/dim 3.8898(3.8666) | Xent 1.5369(1.4328) | Loss 11.2522(11.9355) | Error 0.5445(0.5096) Steps 0(0.00) | Grad Norm 12.6377(7.9578) | Total Time 0.00(0.00)\n",
      "Iter 0862 | Time 66.1505(69.3450) | Bit/dim 3.8923(3.8674) | Xent 1.4954(1.4347) | Loss 11.1971(11.9133) | Error 0.5195(0.5099) Steps 0(0.00) | Grad Norm 10.9989(8.0490) | Total Time 0.00(0.00)\n",
      "Iter 0863 | Time 71.2409(69.4019) | Bit/dim 3.9528(3.8699) | Xent 1.5092(1.4369) | Loss 11.2208(11.8925) | Error 0.5429(0.5109) Steps 0(0.00) | Grad Norm 11.9078(8.1648) | Total Time 0.00(0.00)\n",
      "Iter 0864 | Time 77.7859(69.6534) | Bit/dim 3.8629(3.8697) | Xent 1.4564(1.4375) | Loss 11.1779(11.8711) | Error 0.5197(0.5111) Steps 0(0.00) | Grad Norm 3.8435(8.0351) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0144 | Time 27.5303, Epoch Time 464.5566(444.7872), Bit/dim 3.8805(best: 3.7489), Xent 1.4237, Loss 4.5923, Error 0.5030(best: 0.4402)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_rl.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 8000 --test_batch_size 5000 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_rl_stdscale_3_run1 --resume ../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_rl_stdscale_3_run1/epoch_86_checkpt.pth --seed 1 --conditional True --controlled_tol True --train_mode semisup --lr 0.01 --warmup_iters 1000 --atol 1e-4  --rtol 1e-4 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5 --scale_fac 1.0 --scale_std 3.0 --max_grad_norm 20.0\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
