{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='3,4,5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "if args.controlled_tol:\n",
      "    import lib.odenvp_conditional_tol as odenvp\n",
      "else:\n",
      "    import lib.odenvp_conditional as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            loss.backward()\n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, condition_ratio=0.5, conditional=True, controlled_tol=False, conv=True, data='mnist', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume=None, rtol=1e-05, save='../experiments_published/cnf_conditional_disentangle_bs900_sratio_0_5_drop_0_5_run2', seed=2, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5)\n",
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=784, bias=True)\n",
      "  (project_class): LinearZeros(in_features=392, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 814778\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc.py:286: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "Iter 0010 | Time 7.7211(17.0836) | Bit/dim 22.4858(24.3686) | Xent 2.2890(2.3010) | Loss 23.6302(25.5190) | Error 0.7911(0.8528) Steps 410(410.00) | Grad Norm 174.2675(190.7266) | Total Time 10.00(10.00)\n",
      "Iter 0020 | Time 7.6153(14.6328) | Bit/dim 17.8728(23.1936) | Xent 2.2493(2.2917) | Loss 18.9974(24.3395) | Error 0.7722(0.8376) Steps 410(410.00) | Grad Norm 128.8969(180.0306) | Total Time 10.00(10.00)\n",
      "Iter 0030 | Time 7.5336(12.7952) | Bit/dim 13.5993(21.0971) | Xent 2.1807(2.2705) | Loss 14.6896(22.2324) | Error 0.5311(0.7900) Steps 410(410.00) | Grad Norm 70.5703(158.0265) | Total Time 10.00(10.00)\n",
      "Iter 0040 | Time 7.9617(11.4436) | Bit/dim 11.3115(18.7382) | Xent 2.1015(2.2352) | Loss 12.3622(19.8558) | Error 0.3211(0.6798) Steps 410(410.00) | Grad Norm 28.6296(128.1260) | Total Time 10.00(10.00)\n",
      "Iter 0050 | Time 7.6395(10.4720) | Bit/dim 9.9285(16.5587) | Xent 2.0183(2.1884) | Loss 10.9377(17.6529) | Error 0.3222(0.5860) Steps 410(410.00) | Grad Norm 17.8996(100.0351) | Total Time 10.00(10.00)\n",
      "Iter 0060 | Time 7.6356(9.7321) | Bit/dim 8.3248(14.5562) | Xent 1.9424(2.1330) | Loss 9.2959(15.6227) | Error 0.3178(0.5141) Steps 410(410.00) | Grad Norm 13.5606(77.5367) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0001 | Time 38.7992, Epoch Time 574.0701(574.0701), Bit/dim 7.5405(best: inf), Xent 1.8926, Loss 8.4868, Error 0.2485(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0070 | Time 8.0065(9.1924) | Bit/dim 7.0877(12.7358) | Xent 1.8899(2.0749) | Loss 8.0327(13.7733) | Error 0.2889(0.4589) Steps 410(410.00) | Grad Norm 15.5977(61.1395) | Total Time 10.00(10.00)\n",
      "Iter 0080 | Time 7.5821(8.7904) | Bit/dim 5.8358(11.0567) | Xent 1.8562(2.0218) | Loss 6.7639(12.0676) | Error 0.2700(0.4163) Steps 410(410.00) | Grad Norm 12.0305(48.6839) | Total Time 10.00(10.00)\n",
      "Iter 0090 | Time 7.5765(8.5345) | Bit/dim 4.6845(9.4935) | Xent 1.8704(1.9795) | Loss 5.6197(10.4832) | Error 0.3033(0.3856) Steps 410(410.00) | Grad Norm 9.7951(38.6709) | Total Time 10.00(10.00)\n",
      "Iter 0100 | Time 7.7674(8.3337) | Bit/dim 3.7153(8.0744) | Xent 1.8889(1.9531) | Loss 4.6597(9.0510) | Error 0.3167(0.3686) Steps 410(410.00) | Grad Norm 8.4596(30.8762) | Total Time 10.00(10.00)\n",
      "Iter 0110 | Time 8.2174(8.1788) | Bit/dim 3.0426(6.8171) | Xent 1.9466(1.9464) | Loss 4.0159(7.7903) | Error 0.3544(0.3619) Steps 422(410.36) | Grad Norm 5.8809(24.6205) | Total Time 10.00(10.00)\n",
      "Iter 0120 | Time 8.5333(8.2573) | Bit/dim 2.6615(5.7653) | Xent 2.0030(1.9554) | Loss 3.6631(6.7431) | Error 0.3822(0.3635) Steps 434(415.99) | Grad Norm 3.5416(19.3167) | Total Time 10.00(10.00)\n",
      "Iter 0130 | Time 8.3750(8.3463) | Bit/dim 2.4922(4.9223) | Xent 2.0343(1.9742) | Loss 3.5094(5.9094) | Error 0.3856(0.3719) Steps 434(420.72) | Grad Norm 2.0606(14.9475) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0002 | Time 38.5356, Epoch Time 582.7477(574.3304), Bit/dim 2.4376(best: 7.5405), Xent 2.0374, Loss 3.4563, Error 0.3167(best: 0.2485)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0140 | Time 8.5917(8.4110) | Bit/dim 2.3741(4.2654) | Xent 2.0576(1.9944) | Loss 3.4030(5.2626) | Error 0.4356(0.3809) Steps 434(424.21) | Grad Norm 1.7281(11.5034) | Total Time 10.00(10.00)\n",
      "Iter 0150 | Time 8.5650(8.4618) | Bit/dim 2.3095(3.7582) | Xent 2.0489(2.0105) | Loss 3.3339(4.7635) | Error 0.4078(0.3891) Steps 434(426.78) | Grad Norm 1.3970(8.8793) | Total Time 10.00(10.00)\n",
      "Iter 0160 | Time 8.5269(8.5110) | Bit/dim 2.2725(3.3710) | Xent 2.0384(2.0212) | Loss 3.2917(4.3816) | Error 0.4111(0.3972) Steps 434(428.68) | Grad Norm 1.2382(6.8862) | Total Time 10.00(10.00)\n",
      "Iter 0170 | Time 8.4105(8.5459) | Bit/dim 2.2409(3.0774) | Xent 2.0216(2.0246) | Loss 3.2517(4.0897) | Error 0.4256(0.3996) Steps 434(430.07) | Grad Norm 1.0820(5.3736) | Total Time 10.00(10.00)\n",
      "Iter 0180 | Time 8.5585(8.5709) | Bit/dim 2.2260(2.8538) | Xent 1.9938(2.0216) | Loss 3.2229(3.8646) | Error 0.3722(0.3985) Steps 434(431.10) | Grad Norm 0.9959(4.2322) | Total Time 10.00(10.00)\n",
      "Iter 0190 | Time 8.6520(8.5998) | Bit/dim 2.2171(2.6860) | Xent 1.9778(2.0119) | Loss 3.2059(3.6920) | Error 0.3667(0.3943) Steps 434(431.86) | Grad Norm 0.9775(3.3813) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0003 | Time 38.2272, Epoch Time 624.2058(575.8267), Bit/dim 2.1905(best: 2.4376), Xent 1.9346, Loss 3.1577, Error 0.2848(best: 0.2485)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0200 | Time 8.4997(8.6108) | Bit/dim 2.2105(2.5585) | Xent 1.9490(1.9989) | Loss 3.1850(3.5579) | Error 0.3722(0.3900) Steps 434(432.43) | Grad Norm 1.0050(2.7544) | Total Time 10.00(10.00)\n",
      "Iter 0210 | Time 8.6462(8.5962) | Bit/dim 2.1831(2.4618) | Xent 1.9034(1.9797) | Loss 3.1348(3.4517) | Error 0.3333(0.3827) Steps 434(432.84) | Grad Norm 1.0045(2.2908) | Total Time 10.00(10.00)\n",
      "Iter 0220 | Time 8.6478(8.6284) | Bit/dim 2.1784(2.3873) | Xent 1.8509(1.9548) | Loss 3.1038(3.3647) | Error 0.3400(0.3726) Steps 434(433.14) | Grad Norm 1.0202(1.9531) | Total Time 10.00(10.00)\n",
      "Iter 0230 | Time 8.5544(8.6250) | Bit/dim 2.1697(2.3301) | Xent 1.7992(1.9241) | Loss 3.0692(3.2922) | Error 0.3322(0.3647) Steps 434(433.37) | Grad Norm 1.0395(1.7077) | Total Time 10.00(10.00)\n",
      "Iter 0240 | Time 8.8970(8.6521) | Bit/dim 2.1597(2.2863) | Xent 1.7537(1.8849) | Loss 3.0365(3.2287) | Error 0.3044(0.3531) Steps 434(433.53) | Grad Norm 1.0664(1.5352) | Total Time 10.00(10.00)\n",
      "Iter 0250 | Time 8.6013(8.6487) | Bit/dim 2.1768(2.2537) | Xent 1.6863(1.8426) | Loss 3.0199(3.1750) | Error 0.3156(0.3439) Steps 434(433.66) | Grad Norm 1.0661(1.4075) | Total Time 10.00(10.00)\n",
      "Iter 0260 | Time 8.5377(8.6954) | Bit/dim 2.1538(2.2281) | Xent 1.5909(1.7886) | Loss 2.9493(3.1224) | Error 0.3089(0.3314) Steps 440(434.75) | Grad Norm 1.0534(1.3195) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0004 | Time 39.5271, Epoch Time 627.9745(577.3911), Bit/dim 2.1573(best: 2.1905), Xent 1.5225, Loss 2.9185, Error 0.2272(best: 0.2485)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0270 | Time 8.3770(8.6774) | Bit/dim 2.1533(2.2108) | Xent 1.5058(1.7241) | Loss 2.9062(3.0728) | Error 0.2611(0.3178) Steps 434(434.98) | Grad Norm 1.0556(1.2582) | Total Time 10.00(10.00)\n",
      "Iter 0280 | Time 8.4996(8.6340) | Bit/dim 2.1747(2.1975) | Xent 1.4336(1.6548) | Loss 2.8916(3.0249) | Error 0.2878(0.3069) Steps 434(434.72) | Grad Norm 1.0696(1.2160) | Total Time 10.00(10.00)\n",
      "Iter 0290 | Time 8.4999(8.5872) | Bit/dim 2.1508(2.1869) | Xent 1.3250(1.5783) | Loss 2.8133(2.9760) | Error 0.2711(0.2949) Steps 434(434.53) | Grad Norm 0.9961(1.1772) | Total Time 10.00(10.00)\n",
      "Iter 0300 | Time 8.1118(8.5568) | Bit/dim 2.1568(2.1801) | Xent 1.2610(1.4994) | Loss 2.7873(2.9298) | Error 0.2522(0.2836) Steps 434(434.39) | Grad Norm 1.0509(1.1667) | Total Time 10.00(10.00)\n",
      "Iter 0310 | Time 8.3258(8.5058) | Bit/dim 2.1448(2.1718) | Xent 1.1434(1.4204) | Loss 2.7165(2.8820) | Error 0.2244(0.2737) Steps 434(434.29) | Grad Norm 1.0034(1.1315) | Total Time 10.00(10.00)\n",
      "Iter 0320 | Time 8.3519(8.4897) | Bit/dim 2.1466(2.1636) | Xent 1.0765(1.3410) | Loss 2.6849(2.8342) | Error 0.2211(0.2614) Steps 434(434.21) | Grad Norm 1.0262(1.1285) | Total Time 10.00(10.00)\n",
      "Iter 0330 | Time 8.8155(8.4641) | Bit/dim 2.1171(2.1553) | Xent 1.0366(1.2670) | Loss 2.6355(2.7888) | Error 0.2278(0.2522) Steps 434(434.16) | Grad Norm 1.1204(1.1263) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0005 | Time 39.1515, Epoch Time 612.2338(578.4364), Bit/dim 2.1175(best: 2.1573), Xent 0.9626, Loss 2.5988, Error 0.1762(best: 0.2272)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0340 | Time 8.3549(8.4678) | Bit/dim 2.1252(2.1454) | Xent 0.9935(1.1955) | Loss 2.6219(2.7432) | Error 0.2211(0.2429) Steps 434(434.12) | Grad Norm 1.4458(1.1665) | Total Time 10.00(10.00)\n",
      "Iter 0350 | Time 8.1967(8.4624) | Bit/dim 2.1045(2.1332) | Xent 0.9049(1.1333) | Loss 2.5570(2.6998) | Error 0.1889(0.2363) Steps 434(434.09) | Grad Norm 1.5749(1.2009) | Total Time 10.00(10.00)\n",
      "Iter 0360 | Time 8.4331(8.4801) | Bit/dim 2.0844(2.1202) | Xent 0.9161(1.0755) | Loss 2.5425(2.6580) | Error 0.2144(0.2288) Steps 434(434.06) | Grad Norm 1.5021(1.1826) | Total Time 10.00(10.00)\n",
      "Iter 0370 | Time 8.3216(8.4673) | Bit/dim 2.0613(2.1060) | Xent 0.8787(1.0237) | Loss 2.5006(2.6179) | Error 0.2111(0.2232) Steps 434(434.05) | Grad Norm 0.9511(1.1461) | Total Time 10.00(10.00)\n",
      "Iter 0380 | Time 8.5772(8.4678) | Bit/dim 2.0554(2.0925) | Xent 0.8568(0.9797) | Loss 2.4838(2.5824) | Error 0.2267(0.2184) Steps 440(435.47) | Grad Norm 0.7552(1.0962) | Total Time 10.00(10.00)\n",
      "Iter 0390 | Time 9.4163(8.5826) | Bit/dim 2.0413(2.0779) | Xent 0.7939(0.9377) | Loss 2.4383(2.5467) | Error 0.1956(0.2149) Steps 458(438.53) | Grad Norm 1.1634(1.1113) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0006 | Time 41.7532, Epoch Time 625.5884(579.8510), Bit/dim 2.0199(best: 2.1175), Xent 0.6937, Loss 2.3668, Error 0.1481(best: 0.1762)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0400 | Time 9.1293(8.7873) | Bit/dim 2.0080(2.0628) | Xent 0.7550(0.8989) | Loss 2.3855(2.5122) | Error 0.1711(0.2101) Steps 458(443.64) | Grad Norm 1.5586(1.2248) | Total Time 10.00(10.00)\n",
      "Iter 0410 | Time 9.5869(8.9575) | Bit/dim 2.0130(2.0493) | Xent 0.7002(0.8629) | Loss 2.3631(2.4808) | Error 0.1633(0.2054) Steps 464(448.85) | Grad Norm 2.0391(1.3876) | Total Time 10.00(10.00)\n",
      "Iter 0420 | Time 9.2417(9.1101) | Bit/dim 2.0073(2.0377) | Xent 0.7172(0.8339) | Loss 2.3659(2.4546) | Error 0.1867(0.2032) Steps 470(453.97) | Grad Norm 2.0020(1.4459) | Total Time 10.00(10.00)\n",
      "Iter 0430 | Time 9.3447(9.2166) | Bit/dim 1.9875(2.0264) | Xent 0.6959(0.8040) | Loss 2.3355(2.4284) | Error 0.1833(0.1988) Steps 470(458.18) | Grad Norm 1.2139(1.5558) | Total Time 10.00(10.00)\n",
      "Iter 0440 | Time 9.8586(9.2839) | Bit/dim 1.9631(2.0135) | Xent 0.6935(0.7800) | Loss 2.3099(2.4035) | Error 0.1889(0.1968) Steps 476(461.63) | Grad Norm 1.2073(1.6096) | Total Time 10.00(10.00)\n",
      "Iter 0450 | Time 9.6649(9.4035) | Bit/dim 1.9662(2.0004) | Xent 0.7051(0.7605) | Loss 2.3187(2.3806) | Error 0.2022(0.1961) Steps 476(465.41) | Grad Norm 1.2399(1.5904) | Total Time 10.00(10.00)\n",
      "Iter 0460 | Time 9.5035(9.4648) | Bit/dim 1.9489(1.9884) | Xent 0.6274(0.7394) | Loss 2.2626(2.3581) | Error 0.1689(0.1930) Steps 476(468.19) | Grad Norm 2.0231(1.6850) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0007 | Time 42.8592, Epoch Time 688.7373(583.1176), Bit/dim 1.9431(best: 2.0199), Xent 0.5599, Loss 2.2230, Error 0.1286(best: 0.1481)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0470 | Time 9.6956(9.5432) | Bit/dim 1.9442(1.9768) | Xent 0.6189(0.7213) | Loss 2.2536(2.3374) | Error 0.1644(0.1907) Steps 476(470.24) | Grad Norm 4.1871(1.9542) | Total Time 10.00(10.00)\n",
      "Iter 0480 | Time 9.4883(9.5540) | Bit/dim 1.9222(1.9666) | Xent 0.6188(0.7044) | Loss 2.2316(2.3188) | Error 0.1689(0.1873) Steps 476(471.75) | Grad Norm 3.3123(2.2522) | Total Time 10.00(10.00)\n",
      "Iter 0490 | Time 9.8497(9.5639) | Bit/dim 1.9187(1.9552) | Xent 0.6410(0.6866) | Loss 2.2392(2.2985) | Error 0.1744(0.1844) Steps 476(472.87) | Grad Norm 2.0837(2.2236) | Total Time 10.00(10.00)\n",
      "Iter 0500 | Time 9.8151(9.5934) | Bit/dim 1.9192(1.9452) | Xent 0.6817(0.6756) | Loss 2.2600(2.2830) | Error 0.2022(0.1834) Steps 476(473.69) | Grad Norm 2.2098(2.1929) | Total Time 10.00(10.00)\n",
      "Iter 0510 | Time 9.5201(9.5874) | Bit/dim 1.8976(1.9362) | Xent 0.6044(0.6567) | Loss 2.1998(2.2646) | Error 0.1689(0.1804) Steps 476(474.30) | Grad Norm 1.4110(2.5959) | Total Time 10.00(10.00)\n",
      "Iter 0520 | Time 9.5459(9.6099) | Bit/dim 1.8777(1.9252) | Xent 0.6015(0.6420) | Loss 2.1785(2.2462) | Error 0.1789(0.1774) Steps 482(474.92) | Grad Norm 3.8772(2.9544) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0008 | Time 43.0662, Epoch Time 695.0335(586.4751), Bit/dim 1.8759(best: 1.9431), Xent 0.4647, Loss 2.1083, Error 0.1147(best: 0.1286)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0530 | Time 9.5755(9.6376) | Bit/dim 1.8820(1.9136) | Xent 0.5545(0.6298) | Loss 2.1593(2.2285) | Error 0.1622(0.1741) Steps 476(475.78) | Grad Norm 1.6429(3.0357) | Total Time 10.00(10.00)\n",
      "Iter 0540 | Time 9.5907(9.6276) | Bit/dim 1.8665(1.9033) | Xent 0.5813(0.6209) | Loss 2.1571(2.2137) | Error 0.1733(0.1738) Steps 476(475.84) | Grad Norm 3.3505(3.0078) | Total Time 10.00(10.00)\n",
      "Iter 0550 | Time 9.4431(9.6162) | Bit/dim 1.8545(1.8910) | Xent 0.5325(0.6066) | Loss 2.1208(2.1943) | Error 0.1522(0.1722) Steps 476(475.88) | Grad Norm 2.1544(2.8630) | Total Time 10.00(10.00)\n",
      "Iter 0560 | Time 9.9207(9.6714) | Bit/dim 1.8238(1.8789) | Xent 0.5438(0.5930) | Loss 2.0957(2.1754) | Error 0.1556(0.1691) Steps 476(476.09) | Grad Norm 2.5260(2.7005) | Total Time 10.00(10.00)\n",
      "Iter 0570 | Time 9.8918(9.7816) | Bit/dim 1.8128(1.8673) | Xent 0.6196(0.5857) | Loss 2.1226(2.1602) | Error 0.1856(0.1698) Steps 482(477.37) | Grad Norm 2.2308(2.7455) | Total Time 10.00(10.00)\n",
      "Iter 0580 | Time 10.2424(9.8598) | Bit/dim 1.7965(1.8561) | Xent 0.5271(0.5809) | Loss 2.0601(2.1466) | Error 0.1567(0.1697) Steps 488(479.39) | Grad Norm 3.1101(2.7386) | Total Time 10.00(10.00)\n",
      "Iter 0590 | Time 10.1871(9.9103) | Bit/dim 1.7834(1.8425) | Xent 0.5789(0.5738) | Loss 2.0729(2.1294) | Error 0.1744(0.1696) Steps 488(481.80) | Grad Norm 2.4090(2.6516) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0009 | Time 42.9401, Epoch Time 710.5151(590.1963), Bit/dim 1.7900(best: 1.8759), Xent 0.4010, Loss 1.9904, Error 0.1054(best: 0.1147)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0600 | Time 10.1226(9.9704) | Bit/dim 1.7686(1.8261) | Xent 0.5509(0.5674) | Loss 2.0441(2.1098) | Error 0.1789(0.1690) Steps 494(483.75) | Grad Norm 2.4572(2.8018) | Total Time 10.00(10.00)\n",
      "Iter 0610 | Time 10.2606(10.0080) | Bit/dim 1.7564(1.8102) | Xent 0.5510(0.5588) | Loss 2.0319(2.0896) | Error 0.1467(0.1666) Steps 494(486.31) | Grad Norm 4.6659(3.0361) | Total Time 10.00(10.00)\n",
      "Iter 0620 | Time 10.3558(10.0581) | Bit/dim 1.7308(1.7910) | Xent 0.5585(0.5503) | Loss 2.0101(2.0662) | Error 0.1656(0.1654) Steps 494(488.33) | Grad Norm 4.2428(3.3265) | Total Time 10.00(10.00)\n",
      "Iter 0630 | Time 9.8814(10.0789) | Bit/dim 1.6906(1.7660) | Xent 0.5193(0.5486) | Loss 1.9502(2.0404) | Error 0.1433(0.1651) Steps 482(489.13) | Grad Norm 3.8954(3.4545) | Total Time 10.00(10.00)\n",
      "Iter 0640 | Time 10.2638(10.0453) | Bit/dim 1.6697(1.7397) | Xent 0.4935(0.5406) | Loss 1.9164(2.0100) | Error 0.1533(0.1619) Steps 488(487.61) | Grad Norm 5.1229(4.0929) | Total Time 10.00(10.00)\n",
      "Iter 0650 | Time 9.6888(10.0327) | Bit/dim 1.6234(1.7122) | Xent 0.5013(0.5391) | Loss 1.8740(1.9817) | Error 0.1600(0.1624) Steps 482(487.67) | Grad Norm 8.4173(5.1977) | Total Time 10.00(10.00)\n",
      "Iter 0660 | Time 9.4916(10.0279) | Bit/dim 1.5959(1.6813) | Xent 0.5482(0.5327) | Loss 1.8700(1.9477) | Error 0.1644(0.1608) Steps 482(487.92) | Grad Norm 9.1588(5.2907) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0010 | Time 45.8945, Epoch Time 726.7165(594.2919), Bit/dim 1.5704(best: 1.7900), Xent 0.3945, Loss 1.7676, Error 0.1053(best: 0.1054)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0670 | Time 10.1954(10.0299) | Bit/dim 1.5532(1.6543) | Xent 0.4540(0.5237) | Loss 1.7802(1.9162) | Error 0.1344(0.1585) Steps 488(489.14) | Grad Norm 2.7630(5.6756) | Total Time 10.00(10.00)\n",
      "Iter 0680 | Time 10.5138(10.0710) | Bit/dim 1.5587(1.6294) | Xent 0.5631(0.5231) | Loss 1.8403(1.8910) | Error 0.1667(0.1597) Steps 506(490.59) | Grad Norm 7.4512(5.9618) | Total Time 10.00(10.00)\n",
      "Iter 0690 | Time 10.4350(10.1203) | Bit/dim 1.5169(1.6063) | Xent 0.4810(0.5150) | Loss 1.7574(1.8638) | Error 0.1622(0.1576) Steps 512(492.35) | Grad Norm 6.9982(5.7944) | Total Time 10.00(10.00)\n",
      "Iter 0700 | Time 10.6379(10.2185) | Bit/dim 1.4948(1.5849) | Xent 0.5237(0.5086) | Loss 1.7566(1.8392) | Error 0.1467(0.1553) Steps 512(495.46) | Grad Norm 5.3033(5.7000) | Total Time 10.00(10.00)\n",
      "Iter 0710 | Time 10.6074(10.2728) | Bit/dim 1.5235(1.5710) | Xent 0.5940(0.5052) | Loss 1.8205(1.8236) | Error 0.1822(0.1550) Steps 518(497.76) | Grad Norm 19.3162(7.0800) | Total Time 10.00(10.00)\n",
      "Iter 0720 | Time 10.7486(10.3192) | Bit/dim 1.4914(1.5605) | Xent 0.5472(0.5013) | Loss 1.7650(1.8112) | Error 0.1811(0.1547) Steps 518(499.22) | Grad Norm 10.0754(8.3947) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0011 | Time 44.2447, Epoch Time 739.4647(598.6470), Bit/dim 1.4886(best: 1.5704), Xent 0.3365, Loss 1.6569, Error 0.0915(best: 0.1053)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0730 | Time 10.5786(10.3196) | Bit/dim 1.5080(1.5477) | Xent 0.4770(0.4931) | Loss 1.7466(1.7943) | Error 0.1633(0.1533) Steps 500(500.38) | Grad Norm 8.0285(8.2189) | Total Time 10.00(10.00)\n",
      "Iter 0740 | Time 9.8101(10.3061) | Bit/dim 1.5109(1.5344) | Xent 0.4703(0.4861) | Loss 1.7461(1.7775) | Error 0.1522(0.1513) Steps 494(501.77) | Grad Norm 10.7045(8.0763) | Total Time 10.00(10.00)\n",
      "Iter 0750 | Time 10.5430(10.3205) | Bit/dim 1.5108(1.5237) | Xent 0.5077(0.4825) | Loss 1.7647(1.7649) | Error 0.1600(0.1499) Steps 500(502.51) | Grad Norm 6.4410(7.7658) | Total Time 10.00(10.00)\n",
      "Iter 0760 | Time 10.4946(10.3380) | Bit/dim 1.4616(1.5120) | Xent 0.4870(0.4797) | Loss 1.7051(1.7519) | Error 0.1478(0.1492) Steps 518(504.10) | Grad Norm 5.2917(7.3763) | Total Time 10.00(10.00)\n",
      "Iter 0770 | Time 10.5451(10.3693) | Bit/dim 1.4653(1.5006) | Xent 0.4017(0.4725) | Loss 1.6662(1.7369) | Error 0.1178(0.1476) Steps 512(505.44) | Grad Norm 2.7322(6.8489) | Total Time 10.00(10.00)\n",
      "Iter 0780 | Time 10.3396(10.3798) | Bit/dim 1.4591(1.4910) | Xent 0.5257(0.4706) | Loss 1.7220(1.7263) | Error 0.1622(0.1460) Steps 512(506.36) | Grad Norm 10.2589(6.9064) | Total Time 10.00(10.00)\n",
      "Iter 0790 | Time 10.8816(10.4311) | Bit/dim 1.4521(1.4844) | Xent 0.4859(0.4741) | Loss 1.6951(1.7215) | Error 0.1467(0.1473) Steps 524(507.14) | Grad Norm 11.8540(8.0602) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0012 | Time 45.8520, Epoch Time 748.1990(603.1336), Bit/dim 1.4444(best: 1.4886), Xent 0.3048, Loss 1.5968, Error 0.0857(best: 0.0915)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0800 | Time 10.6009(10.4879) | Bit/dim 1.4464(1.4797) | Xent 0.4873(0.4720) | Loss 1.6901(1.7158) | Error 0.1533(0.1474) Steps 506(507.80) | Grad Norm 4.6752(8.7060) | Total Time 10.00(10.00)\n",
      "Iter 0810 | Time 10.6419(10.5211) | Bit/dim 1.4681(1.4738) | Xent 0.4871(0.4707) | Loss 1.7116(1.7091) | Error 0.1522(0.1466) Steps 512(508.35) | Grad Norm 5.3036(8.5422) | Total Time 10.00(10.00)\n",
      "Iter 0820 | Time 10.8077(10.5648) | Bit/dim 1.4645(1.4661) | Xent 0.4243(0.4651) | Loss 1.6767(1.6987) | Error 0.1389(0.1453) Steps 524(510.05) | Grad Norm 10.4595(8.3542) | Total Time 10.00(10.00)\n",
      "Iter 0830 | Time 10.9861(10.5507) | Bit/dim 1.4532(1.4657) | Xent 0.4974(0.4628) | Loss 1.7019(1.6971) | Error 0.1578(0.1445) Steps 530(509.96) | Grad Norm 17.8537(9.7269) | Total Time 10.00(10.00)\n",
      "Iter 0840 | Time 9.8479(10.6026) | Bit/dim 1.5052(1.4627) | Xent 0.4663(0.4559) | Loss 1.7383(1.6906) | Error 0.1389(0.1424) Steps 494(510.68) | Grad Norm 21.6911(10.6674) | Total Time 10.00(10.00)\n",
      "Iter 0850 | Time 10.0792(10.6278) | Bit/dim 1.4667(1.4637) | Xent 0.3962(0.4515) | Loss 1.6648(1.6894) | Error 0.1311(0.1403) Steps 500(511.08) | Grad Norm 13.9009(11.6647) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0013 | Time 46.8720, Epoch Time 765.8736(608.0158), Bit/dim 1.4341(best: 1.4444), Xent 0.2913, Loss 1.5798, Error 0.0806(best: 0.0857)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0860 | Time 10.0886(10.6373) | Bit/dim 1.4363(1.4572) | Xent 0.4268(0.4458) | Loss 1.6497(1.6801) | Error 0.1378(0.1387) Steps 494(511.29) | Grad Norm 7.9020(11.0291) | Total Time 10.00(10.00)\n",
      "Iter 0870 | Time 10.9034(10.6979) | Bit/dim 1.4241(1.4502) | Xent 0.4143(0.4410) | Loss 1.6312(1.6707) | Error 0.1344(0.1384) Steps 524(513.00) | Grad Norm 6.4993(10.2091) | Total Time 10.00(10.00)\n",
      "Iter 0880 | Time 10.8780(10.7774) | Bit/dim 1.4432(1.4472) | Xent 0.4117(0.4378) | Loss 1.6491(1.6661) | Error 0.1356(0.1379) Steps 524(515.60) | Grad Norm 4.8178(9.5348) | Total Time 10.00(10.00)\n",
      "Iter 0890 | Time 11.0298(10.8476) | Bit/dim 1.4314(1.4454) | Xent 0.4337(0.4328) | Loss 1.6483(1.6618) | Error 0.1300(0.1361) Steps 536(516.56) | Grad Norm 13.0501(10.3512) | Total Time 10.00(10.00)\n",
      "Iter 0900 | Time 10.9298(10.9236) | Bit/dim 1.4091(1.4376) | Xent 0.4712(0.4327) | Loss 1.6447(1.6540) | Error 0.1378(0.1357) Steps 524(519.33) | Grad Norm 2.8588(9.2745) | Total Time 10.00(10.00)\n",
      "Iter 0910 | Time 11.4742(10.9644) | Bit/dim 1.4143(1.4299) | Xent 0.4363(0.4302) | Loss 1.6325(1.6450) | Error 0.1356(0.1351) Steps 536(520.58) | Grad Norm 8.1269(8.3178) | Total Time 10.00(10.00)\n",
      "Iter 0920 | Time 11.1402(10.9913) | Bit/dim 1.4048(1.4245) | Xent 0.4258(0.4305) | Loss 1.6177(1.6397) | Error 0.1322(0.1351) Steps 524(521.68) | Grad Norm 5.2194(8.3419) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0014 | Time 46.8515, Epoch Time 790.3850(613.4869), Bit/dim 1.3987(best: 1.4341), Xent 0.2590, Loss 1.5282, Error 0.0750(best: 0.0806)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0930 | Time 11.0855(11.0506) | Bit/dim 1.3976(1.4194) | Xent 0.4818(0.4308) | Loss 1.6385(1.6348) | Error 0.1489(0.1356) Steps 524(523.54) | Grad Norm 7.4844(8.4190) | Total Time 10.00(10.00)\n",
      "Iter 0940 | Time 10.5761(11.0409) | Bit/dim 1.4309(1.4169) | Xent 0.4025(0.4277) | Loss 1.6322(1.6308) | Error 0.1189(0.1341) Steps 512(524.89) | Grad Norm 20.8237(9.2625) | Total Time 10.00(10.00)\n",
      "Iter 0950 | Time 10.6661(11.0123) | Bit/dim 1.4324(1.4304) | Xent 0.4049(0.4288) | Loss 1.6348(1.6448) | Error 0.1478(0.1348) Steps 518(524.48) | Grad Norm 11.9897(11.7177) | Total Time 10.00(10.00)\n",
      "Iter 0960 | Time 11.0859(11.0360) | Bit/dim 1.3816(1.4285) | Xent 0.4109(0.4222) | Loss 1.5871(1.6396) | Error 0.1133(0.1330) Steps 524(525.89) | Grad Norm 9.0762(11.6404) | Total Time 10.00(10.00)\n",
      "Iter 0970 | Time 10.9067(11.0373) | Bit/dim 1.3966(1.4230) | Xent 0.3572(0.4144) | Loss 1.5752(1.6302) | Error 0.1056(0.1308) Steps 530(525.05) | Grad Norm 7.6705(10.4731) | Total Time 10.00(10.00)\n",
      "Iter 0980 | Time 10.9374(11.0032) | Bit/dim 1.3922(1.4161) | Xent 0.3872(0.4056) | Loss 1.5858(1.6189) | Error 0.1200(0.1276) Steps 518(523.89) | Grad Norm 3.9167(9.0765) | Total Time 10.00(10.00)\n",
      "Iter 0990 | Time 11.5779(11.0075) | Bit/dim 1.3913(1.4083) | Xent 0.4394(0.4042) | Loss 1.6110(1.6104) | Error 0.1478(0.1285) Steps 536(524.41) | Grad Norm 16.2451(8.6472) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0015 | Time 46.0219, Epoch Time 789.2516(618.7598), Bit/dim 1.4044(best: 1.3987), Xent 0.2432, Loss 1.5260, Error 0.0722(best: 0.0750)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1000 | Time 11.0124(11.0588) | Bit/dim 1.3769(1.4051) | Xent 0.4655(0.4061) | Loss 1.6096(1.6082) | Error 0.1433(0.1283) Steps 536(525.40) | Grad Norm 9.2984(8.9709) | Total Time 10.00(10.00)\n",
      "Iter 1010 | Time 11.3587(11.0804) | Bit/dim 1.3914(1.4031) | Xent 0.4389(0.3997) | Loss 1.6108(1.6029) | Error 0.1500(0.1267) Steps 536(525.53) | Grad Norm 17.1311(10.5625) | Total Time 10.00(10.00)\n",
      "Iter 1020 | Time 11.5209(11.1140) | Bit/dim 1.4497(1.4066) | Xent 0.3404(0.4025) | Loss 1.6199(1.6078) | Error 0.1111(0.1283) Steps 530(527.04) | Grad Norm 20.5661(12.1956) | Total Time 10.00(10.00)\n",
      "Iter 1030 | Time 11.1752(11.1624) | Bit/dim 1.3786(1.4081) | Xent 0.3672(0.4012) | Loss 1.5622(1.6087) | Error 0.1289(0.1293) Steps 542(528.62) | Grad Norm 3.5121(12.7087) | Total Time 10.00(10.00)\n",
      "Iter 1040 | Time 11.2916(11.1386) | Bit/dim 1.3672(1.4014) | Xent 0.3918(0.3982) | Loss 1.5631(1.6005) | Error 0.1322(0.1283) Steps 536(528.75) | Grad Norm 8.7991(11.6876) | Total Time 10.00(10.00)\n",
      "Iter 1050 | Time 11.0996(11.1188) | Bit/dim 1.3670(1.3933) | Xent 0.3517(0.3951) | Loss 1.5428(1.5908) | Error 0.1078(0.1261) Steps 536(528.46) | Grad Norm 4.7365(10.3792) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0016 | Time 46.6933, Epoch Time 798.9223(624.1647), Bit/dim 1.3741(best: 1.3987), Xent 0.2231, Loss 1.4857, Error 0.0686(best: 0.0722)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1060 | Time 11.5164(11.1404) | Bit/dim 1.3859(1.3871) | Xent 0.3037(0.3833) | Loss 1.5378(1.5787) | Error 0.1111(0.1228) Steps 524(529.20) | Grad Norm 8.0838(9.2481) | Total Time 10.00(10.00)\n",
      "Iter 1070 | Time 11.2232(11.1104) | Bit/dim 1.3546(1.3810) | Xent 0.4124(0.3826) | Loss 1.5608(1.5723) | Error 0.1278(0.1222) Steps 536(527.82) | Grad Norm 13.2549(9.6786) | Total Time 10.00(10.00)\n",
      "Iter 1080 | Time 11.0493(11.1065) | Bit/dim 1.3589(1.3776) | Xent 0.2831(0.3785) | Loss 1.5004(1.5669) | Error 0.0889(0.1202) Steps 524(528.07) | Grad Norm 7.3987(10.0643) | Total Time 10.00(10.00)\n",
      "Iter 1090 | Time 11.3310(11.0854) | Bit/dim 1.3726(1.3744) | Xent 0.3571(0.3781) | Loss 1.5511(1.5634) | Error 0.1178(0.1196) Steps 512(528.39) | Grad Norm 13.2351(10.0547) | Total Time 10.00(10.00)\n",
      "Iter 1100 | Time 11.1762(11.1331) | Bit/dim 1.3617(1.3691) | Xent 0.3837(0.3739) | Loss 1.5535(1.5561) | Error 0.1133(0.1183) Steps 542(528.42) | Grad Norm 5.8629(9.0180) | Total Time 10.00(10.00)\n",
      "Iter 1110 | Time 11.7340(11.2137) | Bit/dim 1.3750(1.3661) | Xent 0.3423(0.3737) | Loss 1.5461(1.5530) | Error 0.1067(0.1185) Steps 548(531.10) | Grad Norm 14.7407(9.1697) | Total Time 10.00(10.00)\n",
      "Iter 1120 | Time 11.8512(11.2623) | Bit/dim 1.3788(1.3719) | Xent 0.3639(0.3736) | Loss 1.5607(1.5587) | Error 0.1233(0.1188) Steps 524(532.01) | Grad Norm 7.8888(11.0155) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0017 | Time 46.3393, Epoch Time 801.5711(629.4869), Bit/dim 1.5499(best: 1.3741), Xent 0.2232, Loss 1.6615, Error 0.0645(best: 0.0686)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1130 | Time 10.5247(11.2215) | Bit/dim 1.3950(1.3989) | Xent 0.4693(0.3821) | Loss 1.6296(1.5900) | Error 0.1522(0.1219) Steps 524(532.23) | Grad Norm 9.1816(12.9263) | Total Time 10.00(10.00)\n",
      "Iter 1140 | Time 10.6527(11.1313) | Bit/dim 1.3761(1.3943) | Xent 0.3928(0.3791) | Loss 1.5725(1.5839) | Error 0.1089(0.1208) Steps 512(530.37) | Grad Norm 6.7471(11.5520) | Total Time 10.00(10.00)\n",
      "Iter 1150 | Time 10.6385(10.9704) | Bit/dim 1.3844(1.3855) | Xent 0.3138(0.3743) | Loss 1.5413(1.5726) | Error 0.1056(0.1188) Steps 518(526.75) | Grad Norm 6.3130(9.9562) | Total Time 10.00(10.00)\n",
      "Iter 1160 | Time 10.7299(10.8990) | Bit/dim 1.3417(1.3751) | Xent 0.3344(0.3653) | Loss 1.5089(1.5577) | Error 0.1078(0.1166) Steps 518(524.15) | Grad Norm 2.9037(8.4466) | Total Time 10.00(10.00)\n",
      "Iter 1170 | Time 10.5985(10.8538) | Bit/dim 1.3333(1.3640) | Xent 0.3294(0.3629) | Loss 1.4979(1.5454) | Error 0.1022(0.1147) Steps 518(523.07) | Grad Norm 1.9462(7.0454) | Total Time 10.00(10.00)\n",
      "Iter 1180 | Time 10.6662(10.8946) | Bit/dim 1.3547(1.3584) | Xent 0.3591(0.3587) | Loss 1.5342(1.5378) | Error 0.1200(0.1133) Steps 524(525.31) | Grad Norm 11.4257(7.3475) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0018 | Time 49.0617, Epoch Time 779.7235(633.9940), Bit/dim 1.3212(best: 1.3741), Xent 0.2216, Loss 1.4321, Error 0.0623(best: 0.0645)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1190 | Time 10.3170(10.9100) | Bit/dim 1.3695(1.3558) | Xent 0.3700(0.3586) | Loss 1.5545(1.5351) | Error 0.1122(0.1139) Steps 524(527.29) | Grad Norm 12.8565(8.3836) | Total Time 10.00(10.00)\n",
      "Iter 1200 | Time 11.7776(10.9755) | Bit/dim 1.3873(1.3545) | Xent 0.4142(0.3580) | Loss 1.5944(1.5335) | Error 0.1422(0.1135) Steps 554(530.27) | Grad Norm 35.0624(10.1900) | Total Time 10.00(10.00)\n",
      "Iter 1210 | Time 10.9294(11.0214) | Bit/dim 1.4019(1.3687) | Xent 0.2847(0.3511) | Loss 1.5443(1.5443) | Error 0.0989(0.1118) Steps 530(532.06) | Grad Norm 11.7505(11.9793) | Total Time 10.00(10.00)\n",
      "Iter 1220 | Time 10.5284(11.0506) | Bit/dim 1.3451(1.3651) | Xent 0.3001(0.3471) | Loss 1.4951(1.5387) | Error 0.0944(0.1111) Steps 530(532.59) | Grad Norm 6.2619(11.3910) | Total Time 10.00(10.00)\n",
      "Iter 1230 | Time 10.9563(11.0623) | Bit/dim 1.3348(1.3576) | Xent 0.3257(0.3474) | Loss 1.4977(1.5313) | Error 0.1000(0.1116) Steps 542(534.53) | Grad Norm 5.5920(10.0503) | Total Time 10.00(10.00)\n",
      "Iter 1240 | Time 11.2474(11.1166) | Bit/dim 1.3306(1.3499) | Xent 0.3160(0.3434) | Loss 1.4886(1.5216) | Error 0.1000(0.1103) Steps 548(536.24) | Grad Norm 3.7688(8.6643) | Total Time 10.00(10.00)\n",
      "Iter 1250 | Time 11.2856(11.2163) | Bit/dim 1.3192(1.3408) | Xent 0.3557(0.3448) | Loss 1.4971(1.5132) | Error 0.1144(0.1105) Steps 524(538.88) | Grad Norm 4.9999(7.7584) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0019 | Time 47.8685, Epoch Time 804.2310(639.1011), Bit/dim 1.3094(best: 1.3212), Xent 0.1893, Loss 1.4041, Error 0.0601(best: 0.0623)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1260 | Time 12.2292(11.2707) | Bit/dim 1.3242(1.3345) | Xent 0.2968(0.3362) | Loss 1.4726(1.5026) | Error 0.0967(0.1071) Steps 566(540.37) | Grad Norm 10.5857(7.3579) | Total Time 10.00(10.00)\n",
      "Iter 1270 | Time 11.3319(11.2863) | Bit/dim 1.3473(1.3367) | Xent 0.3292(0.3325) | Loss 1.5119(1.5030) | Error 0.1067(0.1063) Steps 530(540.46) | Grad Norm 10.6855(9.0512) | Total Time 10.00(10.00)\n",
      "Iter 1280 | Time 11.6270(11.3202) | Bit/dim 1.3531(1.3549) | Xent 0.3676(0.3454) | Loss 1.5369(1.5276) | Error 0.1133(0.1096) Steps 548(541.21) | Grad Norm 8.5164(12.2831) | Total Time 10.00(10.00)\n",
      "Iter 1290 | Time 10.6402(11.2795) | Bit/dim 1.3517(1.3576) | Xent 0.3254(0.3417) | Loss 1.5144(1.5284) | Error 0.0933(0.1083) Steps 530(541.22) | Grad Norm 11.3610(12.2586) | Total Time 10.00(10.00)\n",
      "Iter 1300 | Time 11.1819(11.2707) | Bit/dim 1.3295(1.3521) | Xent 0.3207(0.3381) | Loss 1.4899(1.5211) | Error 0.0989(0.1073) Steps 530(541.46) | Grad Norm 6.4909(11.6707) | Total Time 10.00(10.00)\n",
      "Iter 1310 | Time 11.0015(11.2247) | Bit/dim 1.2931(1.3415) | Xent 0.2849(0.3283) | Loss 1.4356(1.5057) | Error 0.0889(0.1039) Steps 548(541.48) | Grad Norm 3.6785(10.1958) | Total Time 10.00(10.00)\n",
      "Iter 1320 | Time 10.8704(11.2428) | Bit/dim 1.3030(1.3342) | Xent 0.3322(0.3210) | Loss 1.4691(1.4947) | Error 0.0978(0.1015) Steps 530(541.49) | Grad Norm 5.9817(8.8429) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0020 | Time 47.7799, Epoch Time 806.8709(644.1342), Bit/dim 1.3003(best: 1.3094), Xent 0.1793, Loss 1.3899, Error 0.0549(best: 0.0601)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1330 | Time 11.7775(11.2591) | Bit/dim 1.2957(1.3286) | Xent 0.2378(0.3075) | Loss 1.4146(1.4823) | Error 0.0878(0.0974) Steps 560(543.41) | Grad Norm 2.9268(8.4767) | Total Time 10.00(10.00)\n",
      "Iter 1340 | Time 11.7782(11.2848) | Bit/dim 1.3521(1.3309) | Xent 0.3266(0.3060) | Loss 1.5154(1.4839) | Error 0.1067(0.0979) Steps 566(544.07) | Grad Norm 25.0280(10.7225) | Total Time 10.00(10.00)\n",
      "Iter 1350 | Time 11.8964(11.2560) | Bit/dim 1.2945(1.3281) | Xent 0.3360(0.3039) | Loss 1.4625(1.4800) | Error 0.1100(0.0963) Steps 566(543.70) | Grad Norm 11.2826(11.0592) | Total Time 10.00(10.00)\n",
      "Iter 1360 | Time 11.8436(11.2814) | Bit/dim 1.3019(1.3223) | Xent 0.2856(0.3025) | Loss 1.4447(1.4736) | Error 0.0933(0.0957) Steps 542(543.63) | Grad Norm 4.6061(9.6103) | Total Time 10.00(10.00)\n",
      "Iter 1370 | Time 11.6604(11.3813) | Bit/dim 1.3059(1.3181) | Xent 0.3481(0.3016) | Loss 1.4799(1.4689) | Error 0.0978(0.0950) Steps 560(548.11) | Grad Norm 10.1228(8.3634) | Total Time 10.00(10.00)\n",
      "Iter 1380 | Time 11.5910(11.4260) | Bit/dim 1.3083(1.3130) | Xent 0.2908(0.2982) | Loss 1.4537(1.4621) | Error 0.0956(0.0940) Steps 560(549.12) | Grad Norm 3.3248(8.0818) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0021 | Time 47.5167, Epoch Time 816.0587(649.2919), Bit/dim 1.2984(best: 1.3003), Xent 0.1561, Loss 1.3765, Error 0.0493(best: 0.0549)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1390 | Time 11.4164(11.4327) | Bit/dim 1.2856(1.3068) | Xent 0.2650(0.2949) | Loss 1.4181(1.4543) | Error 0.0878(0.0944) Steps 542(549.43) | Grad Norm 2.9328(7.2758) | Total Time 10.00(10.00)\n",
      "Iter 1400 | Time 11.3627(11.3742) | Bit/dim 1.2988(1.3068) | Xent 0.2838(0.2964) | Loss 1.4408(1.4550) | Error 0.0878(0.0943) Steps 542(546.53) | Grad Norm 17.3051(8.3797) | Total Time 10.00(10.00)\n",
      "Iter 1410 | Time 10.5863(11.3315) | Bit/dim 1.4366(1.3210) | Xent 0.2442(0.2959) | Loss 1.5587(1.4689) | Error 0.0756(0.0948) Steps 518(544.60) | Grad Norm 17.9637(11.9825) | Total Time 10.00(10.00)\n",
      "Iter 1420 | Time 11.3016(11.3263) | Bit/dim 1.3549(1.3367) | Xent 0.3109(0.2960) | Loss 1.5104(1.4847) | Error 0.0944(0.0945) Steps 542(544.17) | Grad Norm 9.5948(12.8558) | Total Time 10.00(10.00)\n",
      "Iter 1430 | Time 10.5917(11.2474) | Bit/dim 1.3175(1.3340) | Xent 0.2319(0.2936) | Loss 1.4334(1.4808) | Error 0.0756(0.0925) Steps 524(542.48) | Grad Norm 9.6768(12.2325) | Total Time 10.00(10.00)\n",
      "Iter 1440 | Time 11.4771(11.2361) | Bit/dim 1.2899(1.3260) | Xent 0.2728(0.2874) | Loss 1.4263(1.4697) | Error 0.0867(0.0901) Steps 542(541.77) | Grad Norm 4.5635(10.6950) | Total Time 10.00(10.00)\n",
      "Iter 1450 | Time 11.1997(11.2863) | Bit/dim 1.3043(1.3153) | Xent 0.2318(0.2809) | Loss 1.4202(1.4557) | Error 0.0644(0.0877) Steps 548(543.40) | Grad Norm 5.4542(8.9607) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0022 | Time 48.3944, Epoch Time 805.6611(653.9830), Bit/dim 1.2823(best: 1.2984), Xent 0.1537, Loss 1.3591, Error 0.0467(best: 0.0493)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1460 | Time 11.7983(11.3080) | Bit/dim 1.2620(1.3075) | Xent 0.2930(0.2764) | Loss 1.4086(1.4457) | Error 0.0900(0.0873) Steps 548(543.79) | Grad Norm 3.8735(8.1113) | Total Time 10.00(10.00)\n",
      "Iter 1470 | Time 11.4283(11.3217) | Bit/dim 1.2819(1.3025) | Xent 0.2294(0.2669) | Loss 1.3966(1.4360) | Error 0.0689(0.0835) Steps 548(544.74) | Grad Norm 8.7648(7.7108) | Total Time 10.00(10.00)\n",
      "Iter 1480 | Time 11.3817(11.2911) | Bit/dim 1.2790(1.2968) | Xent 0.2277(0.2618) | Loss 1.3929(1.4277) | Error 0.0767(0.0823) Steps 542(544.05) | Grad Norm 2.7757(7.3896) | Total Time 10.00(10.00)\n",
      "Iter 1490 | Time 12.0151(11.3300) | Bit/dim 1.3063(1.2939) | Xent 0.1964(0.2586) | Loss 1.4045(1.4232) | Error 0.0656(0.0825) Steps 542(543.37) | Grad Norm 8.8787(7.2692) | Total Time 10.00(10.00)\n",
      "Iter 1500 | Time 11.1880(11.3226) | Bit/dim 1.2700(1.2896) | Xent 0.2302(0.2559) | Loss 1.3851(1.4176) | Error 0.0644(0.0821) Steps 548(543.85) | Grad Norm 9.1024(6.9910) | Total Time 10.00(10.00)\n",
      "Iter 1510 | Time 11.6098(11.3554) | Bit/dim 1.2771(1.2890) | Xent 0.2704(0.2572) | Loss 1.4123(1.4176) | Error 0.0867(0.0814) Steps 542(544.50) | Grad Norm 15.7126(7.2831) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0023 | Time 48.9940, Epoch Time 814.6314(658.8025), Bit/dim 1.3470(best: 1.2823), Xent 0.1313, Loss 1.4126, Error 0.0392(best: 0.0467)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1520 | Time 10.7462(11.3767) | Bit/dim 1.4175(1.3050) | Xent 0.2215(0.2586) | Loss 1.5282(1.4344) | Error 0.0600(0.0812) Steps 524(545.21) | Grad Norm 18.5680(10.6861) | Total Time 10.00(10.00)\n",
      "Iter 1530 | Time 10.6874(11.3081) | Bit/dim 1.3419(1.3207) | Xent 0.2570(0.2564) | Loss 1.4704(1.4489) | Error 0.0767(0.0806) Steps 524(543.67) | Grad Norm 11.6090(11.8102) | Total Time 10.00(10.00)\n",
      "Iter 1540 | Time 11.5345(11.3215) | Bit/dim 1.2920(1.3157) | Xent 0.2295(0.2532) | Loss 1.4068(1.4422) | Error 0.0722(0.0792) Steps 542(542.94) | Grad Norm 4.3379(10.9175) | Total Time 10.00(10.00)\n",
      "Iter 1550 | Time 11.5365(11.3813) | Bit/dim 1.2697(1.3079) | Xent 0.2074(0.2524) | Loss 1.3734(1.4341) | Error 0.0656(0.0792) Steps 548(543.79) | Grad Norm 3.6149(10.0406) | Total Time 10.00(10.00)\n",
      "Iter 1560 | Time 11.1692(11.3946) | Bit/dim 1.2684(1.3013) | Xent 0.2542(0.2481) | Loss 1.3955(1.4254) | Error 0.0833(0.0781) Steps 542(543.97) | Grad Norm 5.7181(9.9259) | Total Time 10.00(10.00)\n",
      "Iter 1570 | Time 11.6421(11.4088) | Bit/dim 1.2634(1.2985) | Xent 0.2730(0.2490) | Loss 1.3999(1.4230) | Error 0.0900(0.0780) Steps 548(544.44) | Grad Norm 9.9190(10.3722) | Total Time 10.00(10.00)\n",
      "Iter 1580 | Time 11.1878(11.3957) | Bit/dim 1.2714(1.2935) | Xent 0.2455(0.2443) | Loss 1.3941(1.4157) | Error 0.0789(0.0762) Steps 548(545.04) | Grad Norm 10.3393(9.9796) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0024 | Time 49.1120, Epoch Time 814.8598(663.4842), Bit/dim 1.2606(best: 1.2823), Xent 0.1251, Loss 1.3231, Error 0.0376(best: 0.0392)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1590 | Time 10.7400(11.3671) | Bit/dim 1.3149(1.2905) | Xent 0.2210(0.2406) | Loss 1.4254(1.4108) | Error 0.0644(0.0758) Steps 524(544.16) | Grad Norm 20.4930(10.3799) | Total Time 10.00(10.00)\n",
      "Iter 1600 | Time 11.1967(11.2979) | Bit/dim 1.2929(1.2911) | Xent 0.2609(0.2408) | Loss 1.4233(1.4116) | Error 0.0767(0.0757) Steps 518(542.10) | Grad Norm 11.4093(11.1061) | Total Time 10.00(10.00)\n",
      "Iter 1610 | Time 11.7523(11.3098) | Bit/dim 1.2712(1.2889) | Xent 0.1862(0.2345) | Loss 1.3642(1.4061) | Error 0.0633(0.0741) Steps 548(542.09) | Grad Norm 8.6249(11.1825) | Total Time 10.00(10.00)\n",
      "Iter 1620 | Time 11.1620(11.3190) | Bit/dim 1.2538(1.2854) | Xent 0.2451(0.2294) | Loss 1.3763(1.4000) | Error 0.0711(0.0716) Steps 542(541.46) | Grad Norm 9.0090(11.3956) | Total Time 10.00(10.00)\n",
      "Iter 1630 | Time 11.3862(11.3255) | Bit/dim 1.2845(1.2798) | Xent 0.1902(0.2279) | Loss 1.3796(1.3937) | Error 0.0622(0.0711) Steps 542(541.92) | Grad Norm 6.7929(9.8955) | Total Time 10.00(10.00)\n",
      "Iter 1640 | Time 11.1206(11.3301) | Bit/dim 1.2668(1.2764) | Xent 0.2149(0.2303) | Loss 1.3743(1.3915) | Error 0.0678(0.0716) Steps 548(542.60) | Grad Norm 5.1766(9.4009) | Total Time 10.00(10.00)\n",
      "Iter 1650 | Time 11.3668(11.3385) | Bit/dim 1.2524(1.2740) | Xent 0.2417(0.2251) | Loss 1.3733(1.3866) | Error 0.0789(0.0698) Steps 542(542.71) | Grad Norm 8.6380(9.0468) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0025 | Time 48.6297, Epoch Time 810.1059(667.8828), Bit/dim 1.2694(best: 1.2606), Xent 0.1090, Loss 1.3239, Error 0.0345(best: 0.0376)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1660 | Time 11.7104(11.3444) | Bit/dim 1.2758(1.2720) | Xent 0.2171(0.2193) | Loss 1.3843(1.3817) | Error 0.0667(0.0672) Steps 542(542.21) | Grad Norm 5.3828(8.1551) | Total Time 10.00(10.00)\n",
      "Iter 1670 | Time 11.2134(11.3483) | Bit/dim 1.2541(1.2677) | Xent 0.1781(0.2152) | Loss 1.3431(1.3752) | Error 0.0633(0.0670) Steps 542(543.10) | Grad Norm 4.4252(8.1546) | Total Time 10.00(10.00)\n",
      "Iter 1680 | Time 11.5790(11.3807) | Bit/dim 1.2605(1.2656) | Xent 0.2100(0.2091) | Loss 1.3655(1.3701) | Error 0.0556(0.0655) Steps 554(543.78) | Grad Norm 9.5826(8.5889) | Total Time 10.00(10.00)\n",
      "Iter 1690 | Time 11.3888(11.3672) | Bit/dim 1.2504(1.2672) | Xent 0.2013(0.2074) | Loss 1.3511(1.3708) | Error 0.0656(0.0650) Steps 542(542.71) | Grad Norm 2.7351(8.9913) | Total Time 10.00(10.00)\n",
      "Iter 1700 | Time 11.3443(11.3747) | Bit/dim 1.2643(1.2649) | Xent 0.2196(0.2053) | Loss 1.3741(1.3675) | Error 0.0767(0.0645) Steps 542(544.10) | Grad Norm 15.4453(9.0369) | Total Time 10.00(10.00)\n",
      "Iter 1710 | Time 12.3826(11.3947) | Bit/dim 1.4041(1.2832) | Xent 0.3646(0.2134) | Loss 1.5864(1.3899) | Error 0.1156(0.0672) Steps 578(545.67) | Grad Norm 38.8393(12.0778) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0026 | Time 47.0179, Epoch Time 813.0537(672.2380), Bit/dim 1.3423(best: 1.2606), Xent 0.1014, Loss 1.3929, Error 0.0328(best: 0.0345)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1720 | Time 11.2792(11.3164) | Bit/dim 1.2662(1.2919) | Xent 0.1887(0.2112) | Loss 1.3605(1.3975) | Error 0.0656(0.0672) Steps 542(544.49) | Grad Norm 8.8964(12.1655) | Total Time 10.00(10.00)\n",
      "Iter 1730 | Time 11.2788(11.2931) | Bit/dim 1.2408(1.2875) | Xent 0.2007(0.2073) | Loss 1.3412(1.3912) | Error 0.0722(0.0665) Steps 548(544.32) | Grad Norm 7.9831(11.7938) | Total Time 10.00(10.00)\n",
      "Iter 1740 | Time 11.6637(11.3560) | Bit/dim 1.2644(1.2812) | Xent 0.2258(0.2063) | Loss 1.3773(1.3844) | Error 0.0756(0.0657) Steps 548(545.60) | Grad Norm 7.5050(10.6326) | Total Time 10.00(10.00)\n",
      "Iter 1750 | Time 11.5872(11.3546) | Bit/dim 1.2601(1.2745) | Xent 0.1884(0.2009) | Loss 1.3543(1.3750) | Error 0.0533(0.0639) Steps 554(546.34) | Grad Norm 4.0054(9.2552) | Total Time 10.00(10.00)\n",
      "Iter 1760 | Time 11.3700(11.3572) | Bit/dim 1.2436(1.2665) | Xent 0.1999(0.1949) | Loss 1.3435(1.3640) | Error 0.0611(0.0618) Steps 554(547.29) | Grad Norm 3.8681(8.0223) | Total Time 10.00(10.00)\n",
      "Iter 1770 | Time 11.4629(11.4016) | Bit/dim 1.2388(1.2621) | Xent 0.1942(0.1947) | Loss 1.3359(1.3595) | Error 0.0522(0.0609) Steps 554(549.37) | Grad Norm 4.3902(6.8792) | Total Time 10.00(10.00)\n",
      "Iter 1780 | Time 11.3561(11.4442) | Bit/dim 1.2698(1.2601) | Xent 0.1876(0.1924) | Loss 1.3636(1.3563) | Error 0.0489(0.0600) Steps 554(550.43) | Grad Norm 8.6569(7.0065) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0027 | Time 50.2214, Epoch Time 818.1523(676.6154), Bit/dim 1.2391(best: 1.2606), Xent 0.1064, Loss 1.2924, Error 0.0321(best: 0.0328)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1790 | Time 11.7121(11.4860) | Bit/dim 1.2835(1.2591) | Xent 0.1756(0.1890) | Loss 1.3713(1.3536) | Error 0.0522(0.0589) Steps 560(550.91) | Grad Norm 13.2332(7.4926) | Total Time 10.00(10.00)\n",
      "Iter 1800 | Time 11.7843(11.4871) | Bit/dim 1.2495(1.2556) | Xent 0.1849(0.1883) | Loss 1.3420(1.3498) | Error 0.0611(0.0583) Steps 560(551.90) | Grad Norm 12.4178(7.2233) | Total Time 10.00(10.00)\n",
      "Iter 1810 | Time 10.6743(11.4290) | Bit/dim 1.3975(1.2606) | Xent 0.1937(0.1853) | Loss 1.4943(1.3532) | Error 0.0633(0.0578) Steps 530(551.03) | Grad Norm 29.3373(9.3447) | Total Time 10.00(10.00)\n",
      "Iter 1820 | Time 11.9568(11.4170) | Bit/dim 1.3299(1.2766) | Xent 0.2262(0.1874) | Loss 1.4430(1.3704) | Error 0.0678(0.0586) Steps 560(549.67) | Grad Norm 21.2741(11.8336) | Total Time 10.00(10.00)\n",
      "Iter 1830 | Time 11.4911(11.3935) | Bit/dim 1.2685(1.2792) | Xent 0.2085(0.1882) | Loss 1.3728(1.3733) | Error 0.0644(0.0592) Steps 554(547.86) | Grad Norm 7.4287(11.8922) | Total Time 10.00(10.00)\n",
      "Iter 1840 | Time 11.5059(11.4047) | Bit/dim 1.2724(1.2770) | Xent 0.1975(0.1856) | Loss 1.3712(1.3697) | Error 0.0589(0.0589) Steps 548(548.25) | Grad Norm 20.1343(12.0940) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0028 | Time 49.1572, Epoch Time 818.7694(680.8800), Bit/dim 1.2575(best: 1.2391), Xent 0.0872, Loss 1.3010, Error 0.0274(best: 0.0321)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1850 | Time 11.4759(11.4211) | Bit/dim 1.2557(1.2727) | Xent 0.1794(0.1835) | Loss 1.3454(1.3645) | Error 0.0533(0.0580) Steps 554(549.50) | Grad Norm 2.8016(10.9815) | Total Time 10.00(10.00)\n",
      "Iter 1860 | Time 11.4728(11.4765) | Bit/dim 1.2235(1.2655) | Xent 0.2113(0.1798) | Loss 1.3292(1.3554) | Error 0.0667(0.0573) Steps 554(550.85) | Grad Norm 6.6103(10.2674) | Total Time 10.00(10.00)\n",
      "Iter 1870 | Time 11.5776(11.5011) | Bit/dim 1.2304(1.2592) | Xent 0.1498(0.1788) | Loss 1.3053(1.3486) | Error 0.0456(0.0561) Steps 554(551.68) | Grad Norm 3.7876(9.3146) | Total Time 10.00(10.00)\n",
      "Iter 1880 | Time 11.2704(11.5049) | Bit/dim 1.2416(1.2538) | Xent 0.1565(0.1727) | Loss 1.3198(1.3402) | Error 0.0489(0.0542) Steps 554(552.29) | Grad Norm 2.2454(7.6590) | Total Time 10.00(10.00)\n",
      "Iter 1890 | Time 11.9919(11.5327) | Bit/dim 1.2271(1.2495) | Xent 0.1672(0.1726) | Loss 1.3107(1.3358) | Error 0.0578(0.0537) Steps 560(552.92) | Grad Norm 11.4029(6.9572) | Total Time 10.00(10.00)\n",
      "Iter 1900 | Time 11.4843(11.5777) | Bit/dim 1.2376(1.2473) | Xent 0.1292(0.1670) | Loss 1.3022(1.3308) | Error 0.0367(0.0517) Steps 554(553.35) | Grad Norm 9.3065(7.1849) | Total Time 10.00(10.00)\n",
      "Iter 1910 | Time 11.8146(11.5880) | Bit/dim 1.2423(1.2473) | Xent 0.1674(0.1681) | Loss 1.3260(1.3314) | Error 0.0489(0.0521) Steps 560(553.85) | Grad Norm 7.3353(7.8008) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0029 | Time 49.8464, Epoch Time 830.9007(685.3806), Bit/dim 1.2267(best: 1.2391), Xent 0.0857, Loss 1.2695, Error 0.0269(best: 0.0274)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1920 | Time 11.4149(11.5499) | Bit/dim 1.2359(1.2442) | Xent 0.1714(0.1674) | Loss 1.3216(1.3279) | Error 0.0522(0.0518) Steps 554(553.89) | Grad Norm 5.1295(7.1803) | Total Time 10.00(10.00)\n",
      "Iter 1930 | Time 11.4086(11.5556) | Bit/dim 1.2139(1.2422) | Xent 0.1733(0.1626) | Loss 1.3006(1.3235) | Error 0.0556(0.0501) Steps 554(553.92) | Grad Norm 8.0704(6.5652) | Total Time 10.00(10.00)\n",
      "Iter 1940 | Time 12.3399(11.5678) | Bit/dim 1.2654(1.2421) | Xent 0.2362(0.1664) | Loss 1.3835(1.3253) | Error 0.0767(0.0515) Steps 572(554.13) | Grad Norm 34.0003(8.3318) | Total Time 10.00(10.00)\n",
      "Iter 1950 | Time 10.8469(11.5255) | Bit/dim 1.3791(1.2681) | Xent 0.1903(0.1718) | Loss 1.4742(1.3540) | Error 0.0556(0.0538) Steps 530(551.82) | Grad Norm 15.4023(11.2491) | Total Time 10.00(10.00)\n",
      "Iter 1960 | Time 11.4993(11.4753) | Bit/dim 1.2666(1.2730) | Xent 0.1782(0.1713) | Loss 1.3557(1.3587) | Error 0.0578(0.0542) Steps 548(549.74) | Grad Norm 10.4533(11.6309) | Total Time 10.00(10.00)\n",
      "Iter 1970 | Time 11.8772(11.4976) | Bit/dim 1.2631(1.2697) | Xent 0.1586(0.1694) | Loss 1.3424(1.3544) | Error 0.0478(0.0528) Steps 566(550.95) | Grad Norm 6.9034(11.2261) | Total Time 10.00(10.00)\n",
      "Iter 1980 | Time 11.4127(11.5201) | Bit/dim 1.2384(1.2628) | Xent 0.1503(0.1664) | Loss 1.3136(1.3460) | Error 0.0411(0.0523) Steps 560(553.33) | Grad Norm 5.5881(9.5640) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0030 | Time 50.3234, Epoch Time 824.6588(689.5590), Bit/dim 1.2284(best: 1.2267), Xent 0.0740, Loss 1.2654, Error 0.0242(best: 0.0269)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1990 | Time 11.5959(11.5419) | Bit/dim 1.2293(1.2532) | Xent 0.1269(0.1643) | Loss 1.2927(1.3354) | Error 0.0322(0.0507) Steps 560(555.08) | Grad Norm 3.0470(8.2213) | Total Time 10.00(10.00)\n",
      "Iter 2000 | Time 12.0929(11.5700) | Bit/dim 1.2132(1.2467) | Xent 0.1349(0.1596) | Loss 1.2806(1.3266) | Error 0.0422(0.0493) Steps 560(556.37) | Grad Norm 3.7285(7.2972) | Total Time 10.00(10.00)\n",
      "Iter 2010 | Time 11.6298(11.5850) | Bit/dim 1.2319(1.2424) | Xent 0.1314(0.1554) | Loss 1.2976(1.3201) | Error 0.0500(0.0491) Steps 554(556.48) | Grad Norm 3.9064(6.4509) | Total Time 10.00(10.00)\n",
      "Iter 2020 | Time 11.8042(11.5662) | Bit/dim 1.2317(1.2383) | Xent 0.1129(0.1534) | Loss 1.2881(1.3150) | Error 0.0422(0.0483) Steps 560(556.17) | Grad Norm 11.8645(6.4593) | Total Time 10.00(10.00)\n",
      "Iter 2030 | Time 12.8583(11.5588) | Bit/dim 1.2813(1.2436) | Xent 0.1781(0.1519) | Loss 1.3703(1.3196) | Error 0.0533(0.0476) Steps 578(555.27) | Grad Norm 28.6787(8.9762) | Total Time 10.00(10.00)\n",
      "Iter 2040 | Time 11.9094(11.5582) | Bit/dim 1.2284(1.2441) | Xent 0.1513(0.1504) | Loss 1.3040(1.3193) | Error 0.0589(0.0477) Steps 554(554.33) | Grad Norm 5.7529(9.4885) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0031 | Time 50.5346, Epoch Time 830.2887(693.7809), Bit/dim 1.2619(best: 1.2267), Xent 0.0887, Loss 1.3063, Error 0.0281(best: 0.0242)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2050 | Time 12.0929(11.5718) | Bit/dim 1.2606(1.2469) | Xent 0.1717(0.1527) | Loss 1.3465(1.3232) | Error 0.0556(0.0477) Steps 554(553.86) | Grad Norm 26.1636(10.9433) | Total Time 10.00(10.00)\n",
      "Iter 2060 | Time 11.3911(11.5922) | Bit/dim 1.2392(1.2458) | Xent 0.1524(0.1494) | Loss 1.3154(1.3205) | Error 0.0500(0.0469) Steps 554(554.24) | Grad Norm 17.1657(10.8740) | Total Time 10.00(10.00)\n",
      "Iter 2070 | Time 11.6563(11.6035) | Bit/dim 1.2406(1.2443) | Xent 0.1853(0.1486) | Loss 1.3332(1.3186) | Error 0.0522(0.0465) Steps 560(555.29) | Grad Norm 5.4196(10.4263) | Total Time 10.00(10.00)\n",
      "Iter 2080 | Time 11.7514(11.6072) | Bit/dim 1.2330(1.2415) | Xent 0.1520(0.1477) | Loss 1.3090(1.3153) | Error 0.0544(0.0465) Steps 566(556.79) | Grad Norm 9.1532(10.7260) | Total Time 10.00(10.00)\n",
      "Iter 2090 | Time 10.9022(11.5910) | Bit/dim 1.3018(1.2531) | Xent 0.1136(0.1485) | Loss 1.3585(1.3274) | Error 0.0422(0.0459) Steps 542(555.55) | Grad Norm 12.5113(12.7380) | Total Time 10.00(10.00)\n",
      "Iter 2100 | Time 10.7289(11.5170) | Bit/dim 1.3110(1.2600) | Xent 0.1773(0.1519) | Loss 1.3996(1.3359) | Error 0.0478(0.0468) Steps 542(554.42) | Grad Norm 13.0834(12.9774) | Total Time 10.00(10.00)\n",
      "Iter 2110 | Time 11.4884(11.5594) | Bit/dim 1.2099(1.2551) | Xent 0.1528(0.1514) | Loss 1.2863(1.3308) | Error 0.0478(0.0466) Steps 560(555.18) | Grad Norm 9.2745(12.2374) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0032 | Time 50.3443, Epoch Time 829.7380(697.8596), Bit/dim 1.2180(best: 1.2267), Xent 0.0710, Loss 1.2535, Error 0.0204(best: 0.0242)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2120 | Time 11.5178(11.5251) | Bit/dim 1.2274(1.2484) | Xent 0.1260(0.1504) | Loss 1.2904(1.3237) | Error 0.0378(0.0467) Steps 554(555.46) | Grad Norm 3.2409(10.4428) | Total Time 10.00(10.00)\n",
      "Iter 2130 | Time 11.3760(11.5292) | Bit/dim 1.2217(1.2424) | Xent 0.1024(0.1485) | Loss 1.2729(1.3167) | Error 0.0356(0.0457) Steps 554(555.22) | Grad Norm 3.2769(8.6448) | Total Time 10.00(10.00)\n",
      "Iter 2140 | Time 11.2697(11.5166) | Bit/dim 1.2173(1.2368) | Xent 0.1368(0.1445) | Loss 1.2856(1.3090) | Error 0.0433(0.0448) Steps 554(554.90) | Grad Norm 8.2467(7.7249) | Total Time 10.00(10.00)\n",
      "Iter 2150 | Time 11.6690(11.5408) | Bit/dim 1.2478(1.2354) | Xent 0.0984(0.1424) | Loss 1.2970(1.3066) | Error 0.0289(0.0438) Steps 548(554.78) | Grad Norm 13.6705(9.0692) | Total Time 10.00(10.00)\n",
      "Iter 2160 | Time 11.4419(11.5616) | Bit/dim 1.2221(1.2332) | Xent 0.1389(0.1404) | Loss 1.2916(1.3034) | Error 0.0389(0.0436) Steps 554(554.62) | Grad Norm 4.4298(8.8859) | Total Time 10.00(10.00)\n",
      "Iter 2170 | Time 11.8960(11.5683) | Bit/dim 1.2286(1.2302) | Xent 0.0969(0.1368) | Loss 1.2771(1.2986) | Error 0.0322(0.0418) Steps 554(554.62) | Grad Norm 10.0728(8.3089) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0033 | Time 50.2344, Epoch Time 826.6374(701.7229), Bit/dim 1.3077(best: 1.2180), Xent 0.0921, Loss 1.3537, Error 0.0273(best: 0.0204)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2180 | Time 11.3037(11.5761) | Bit/dim 1.2507(1.2423) | Xent 0.1197(0.1401) | Loss 1.3106(1.3124) | Error 0.0422(0.0435) Steps 536(554.29) | Grad Norm 10.4478(10.8490) | Total Time 10.00(10.00)\n",
      "Iter 2190 | Time 10.8386(11.4822) | Bit/dim 1.2558(1.2485) | Xent 0.1053(0.1369) | Loss 1.3085(1.3170) | Error 0.0367(0.0426) Steps 536(552.39) | Grad Norm 11.3037(11.2311) | Total Time 10.00(10.00)\n",
      "Iter 2200 | Time 11.4609(11.4974) | Bit/dim 1.2131(1.2432) | Xent 0.1206(0.1357) | Loss 1.2734(1.3110) | Error 0.0356(0.0420) Steps 554(553.10) | Grad Norm 1.9290(9.9955) | Total Time 10.00(10.00)\n",
      "Iter 2210 | Time 11.1963(11.4843) | Bit/dim 1.2064(1.2356) | Xent 0.1157(0.1348) | Loss 1.2642(1.3030) | Error 0.0367(0.0416) Steps 554(553.34) | Grad Norm 4.1298(8.6930) | Total Time 10.00(10.00)\n",
      "Iter 2220 | Time 11.7883(11.4959) | Bit/dim 1.2134(1.2301) | Xent 0.1242(0.1325) | Loss 1.2755(1.2963) | Error 0.0444(0.0417) Steps 554(553.51) | Grad Norm 1.9733(7.3529) | Total Time 10.00(10.00)\n",
      "Iter 2230 | Time 11.3674(11.5012) | Bit/dim 1.2264(1.2259) | Xent 0.1254(0.1323) | Loss 1.2891(1.2920) | Error 0.0444(0.0418) Steps 554(553.64) | Grad Norm 3.2863(6.2981) | Total Time 10.00(10.00)\n",
      "Iter 2240 | Time 11.4354(11.5097) | Bit/dim 1.2189(1.2216) | Xent 0.1639(0.1349) | Loss 1.3009(1.2891) | Error 0.0411(0.0422) Steps 554(553.73) | Grad Norm 5.6153(5.8936) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0034 | Time 50.8230, Epoch Time 823.9323(705.3892), Bit/dim 1.2036(best: 1.2180), Xent 0.0616, Loss 1.2344, Error 0.0205(best: 0.0204)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2250 | Time 12.1286(11.5445) | Bit/dim 1.2063(1.2194) | Xent 0.1355(0.1321) | Loss 1.2741(1.2855) | Error 0.0367(0.0411) Steps 554(553.80) | Grad Norm 10.2919(5.7001) | Total Time 10.00(10.00)\n",
      "Iter 2260 | Time 11.4479(11.5295) | Bit/dim 1.2618(1.2239) | Xent 0.1400(0.1335) | Loss 1.3318(1.2907) | Error 0.0411(0.0414) Steps 548(553.52) | Grad Norm 17.5616(7.9751) | Total Time 10.00(10.00)\n",
      "Iter 2270 | Time 11.6094(11.4844) | Bit/dim 1.2000(1.2279) | Xent 0.1448(0.1326) | Loss 1.2724(1.2941) | Error 0.0511(0.0412) Steps 554(552.37) | Grad Norm 6.2821(9.6168) | Total Time 10.00(10.00)\n",
      "Iter 2280 | Time 12.2558(11.5379) | Bit/dim 1.2576(1.2299) | Xent 0.1501(0.1315) | Loss 1.3326(1.2957) | Error 0.0544(0.0408) Steps 572(553.21) | Grad Norm 24.6564(10.6343) | Total Time 10.00(10.00)\n",
      "Iter 2290 | Time 11.6821(11.5369) | Bit/dim 1.2304(1.2308) | Xent 0.1059(0.1274) | Loss 1.2834(1.2945) | Error 0.0378(0.0395) Steps 560(552.79) | Grad Norm 8.9159(11.2016) | Total Time 10.00(10.00)\n",
      "Iter 2300 | Time 11.5366(11.5505) | Bit/dim 1.2249(1.2297) | Xent 0.0876(0.1286) | Loss 1.2687(1.2940) | Error 0.0267(0.0401) Steps 560(553.29) | Grad Norm 9.3703(11.0357) | Total Time 10.00(10.00)\n",
      "Iter 2310 | Time 11.7203(11.5791) | Bit/dim 1.2066(1.2251) | Xent 0.1221(0.1274) | Loss 1.2677(1.2888) | Error 0.0344(0.0397) Steps 560(553.97) | Grad Norm 7.5392(10.2737) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0035 | Time 49.9165, Epoch Time 828.8081(709.0918), Bit/dim 1.1989(best: 1.2036), Xent 0.0565, Loss 1.2272, Error 0.0182(best: 0.0204)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2320 | Time 11.4195(11.5647) | Bit/dim 1.2066(1.2197) | Xent 0.1431(0.1239) | Loss 1.2781(1.2816) | Error 0.0378(0.0385) Steps 554(553.98) | Grad Norm 2.9857(8.6509) | Total Time 10.00(10.00)\n",
      "Iter 2330 | Time 11.5369(11.5663) | Bit/dim 1.2133(1.2164) | Xent 0.1325(0.1228) | Loss 1.2796(1.2778) | Error 0.0378(0.0381) Steps 554(553.98) | Grad Norm 14.5940(8.0386) | Total Time 10.00(10.00)\n",
      "Iter 2340 | Time 11.3038(11.5091) | Bit/dim 1.2146(1.2250) | Xent 0.1344(0.1224) | Loss 1.2818(1.2862) | Error 0.0344(0.0374) Steps 554(552.34) | Grad Norm 13.6927(10.3690) | Total Time 10.00(10.00)\n",
      "Iter 2350 | Time 11.8065(11.4743) | Bit/dim 1.2158(1.2321) | Xent 0.1286(0.1233) | Loss 1.2801(1.2938) | Error 0.0444(0.0383) Steps 554(551.24) | Grad Norm 7.8954(11.3818) | Total Time 10.00(10.00)\n",
      "Iter 2360 | Time 11.3000(11.4997) | Bit/dim 1.1844(1.2272) | Xent 0.1152(0.1225) | Loss 1.2420(1.2884) | Error 0.0356(0.0383) Steps 554(552.17) | Grad Norm 2.5552(10.3232) | Total Time 10.00(10.00)\n",
      "Iter 2370 | Time 11.2179(11.4732) | Bit/dim 1.2016(1.2222) | Xent 0.1051(0.1215) | Loss 1.2542(1.2830) | Error 0.0333(0.0384) Steps 554(552.65) | Grad Norm 3.1594(8.9012) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0036 | Time 50.6834, Epoch Time 823.3690(712.5201), Bit/dim 1.1997(best: 1.1989), Xent 0.0570, Loss 1.2282, Error 0.0180(best: 0.0182)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2380 | Time 11.2527(11.4858) | Bit/dim 1.2028(1.2178) | Xent 0.1286(0.1216) | Loss 1.2671(1.2786) | Error 0.0433(0.0378) Steps 554(553.16) | Grad Norm 10.7526(8.3767) | Total Time 10.00(10.00)\n",
      "Iter 2390 | Time 11.5052(11.4954) | Bit/dim 1.2198(1.2136) | Xent 0.0664(0.1168) | Loss 1.2530(1.2720) | Error 0.0244(0.0370) Steps 554(553.53) | Grad Norm 5.3696(7.8838) | Total Time 10.00(10.00)\n",
      "Iter 2400 | Time 11.4478(11.5036) | Bit/dim 1.1861(1.2113) | Xent 0.1214(0.1159) | Loss 1.2468(1.2692) | Error 0.0311(0.0363) Steps 554(553.96) | Grad Norm 3.3640(7.1549) | Total Time 10.00(10.00)\n",
      "Iter 2410 | Time 11.9384(11.5069) | Bit/dim 1.2178(1.2107) | Xent 0.1146(0.1170) | Loss 1.2751(1.2692) | Error 0.0311(0.0363) Steps 560(554.15) | Grad Norm 13.3899(6.9611) | Total Time 10.00(10.00)\n",
      "Iter 2420 | Time 11.0423(11.4868) | Bit/dim 1.3366(1.2194) | Xent 0.1197(0.1179) | Loss 1.3965(1.2784) | Error 0.0411(0.0367) Steps 530(552.72) | Grad Norm 15.5213(9.0476) | Total Time 10.00(10.00)\n",
      "Iter 2430 | Time 11.4291(11.4468) | Bit/dim 1.2658(1.2321) | Xent 0.2134(0.1265) | Loss 1.3725(1.2954) | Error 0.0578(0.0386) Steps 554(551.69) | Grad Norm 17.0112(10.7118) | Total Time 10.00(10.00)\n",
      "Iter 2440 | Time 11.1955(11.4404) | Bit/dim 1.2173(1.2300) | Xent 0.1407(0.1273) | Loss 1.2877(1.2937) | Error 0.0433(0.0392) Steps 536(551.13) | Grad Norm 7.0650(10.1550) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0037 | Time 50.0836, Epoch Time 822.6224(715.8231), Bit/dim 1.2074(best: 1.1989), Xent 0.0538, Loss 1.2343, Error 0.0178(best: 0.0180)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2450 | Time 11.4273(11.4776) | Bit/dim 1.2086(1.2245) | Xent 0.1081(0.1250) | Loss 1.2627(1.2870) | Error 0.0356(0.0386) Steps 554(551.88) | Grad Norm 2.2973(8.7323) | Total Time 10.00(10.00)\n",
      "Iter 2460 | Time 11.2521(11.4674) | Bit/dim 1.2034(1.2180) | Xent 0.1054(0.1223) | Loss 1.2561(1.2792) | Error 0.0333(0.0377) Steps 554(552.44) | Grad Norm 10.1410(8.2376) | Total Time 10.00(10.00)\n",
      "Iter 2470 | Time 11.7714(11.5182) | Bit/dim 1.2301(1.2143) | Xent 0.1400(0.1230) | Loss 1.3001(1.2758) | Error 0.0411(0.0375) Steps 560(553.19) | Grad Norm 12.3158(8.2547) | Total Time 10.00(10.00)\n",
      "Iter 2480 | Time 11.4637(11.5579) | Bit/dim 1.2027(1.2145) | Xent 0.0886(0.1178) | Loss 1.2470(1.2733) | Error 0.0311(0.0366) Steps 560(553.59) | Grad Norm 7.7283(9.3783) | Total Time 10.00(10.00)\n",
      "Iter 2490 | Time 11.6393(11.5453) | Bit/dim 1.1980(1.2123) | Xent 0.0705(0.1128) | Loss 1.2333(1.2687) | Error 0.0189(0.0351) Steps 554(554.17) | Grad Norm 8.0039(9.0308) | Total Time 10.00(10.00)\n",
      "Iter 2500 | Time 11.7304(11.5589) | Bit/dim 1.2106(1.2093) | Xent 0.1206(0.1151) | Loss 1.2709(1.2668) | Error 0.0333(0.0352) Steps 554(554.29) | Grad Norm 11.0324(8.8587) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0038 | Time 50.6717, Epoch Time 830.8340(719.2735), Bit/dim 1.2051(best: 1.1989), Xent 0.0562, Loss 1.2332, Error 0.0173(best: 0.0178)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2510 | Time 11.3231(11.5783) | Bit/dim 1.2231(1.2143) | Xent 0.1466(0.1172) | Loss 1.2964(1.2729) | Error 0.0467(0.0363) Steps 554(553.54) | Grad Norm 20.7599(10.4487) | Total Time 10.00(10.00)\n",
      "Iter 2520 | Time 11.7834(11.5064) | Bit/dim 1.1982(1.2169) | Xent 0.0999(0.1180) | Loss 1.2482(1.2759) | Error 0.0289(0.0365) Steps 560(551.74) | Grad Norm 6.0992(10.8476) | Total Time 10.00(10.00)\n",
      "Iter 2530 | Time 11.8062(11.5730) | Bit/dim 1.1956(1.2144) | Xent 0.1137(0.1171) | Loss 1.2524(1.2729) | Error 0.0333(0.0356) Steps 554(552.81) | Grad Norm 7.4219(10.1637) | Total Time 10.00(10.00)\n",
      "Iter 2540 | Time 11.4085(11.5877) | Bit/dim 1.2091(1.2140) | Xent 0.1096(0.1147) | Loss 1.2639(1.2714) | Error 0.0267(0.0348) Steps 554(553.75) | Grad Norm 15.3971(10.7103) | Total Time 10.00(10.00)\n",
      "Iter 2550 | Time 11.3249(11.5990) | Bit/dim 1.2047(1.2087) | Xent 0.1185(0.1128) | Loss 1.2639(1.2651) | Error 0.0311(0.0343) Steps 554(554.26) | Grad Norm 8.8897(9.3537) | Total Time 10.00(10.00)\n",
      "Iter 2560 | Time 11.4223(11.6421) | Bit/dim 1.2354(1.2111) | Xent 0.1057(0.1138) | Loss 1.2882(1.2680) | Error 0.0311(0.0348) Steps 548(554.99) | Grad Norm 14.1391(10.3537) | Total Time 10.00(10.00)\n",
      "Iter 2570 | Time 12.0355(11.6556) | Bit/dim 1.2630(1.2151) | Xent 0.1316(0.1101) | Loss 1.3288(1.2702) | Error 0.0456(0.0341) Steps 566(554.97) | Grad Norm 34.7847(11.5728) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0039 | Time 50.5373, Epoch Time 831.1329(722.6293), Bit/dim 1.2228(best: 1.1989), Xent 0.0632, Loss 1.2544, Error 0.0196(best: 0.0173)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2580 | Time 11.2708(11.5427) | Bit/dim 1.2287(1.2238) | Xent 0.1388(0.1126) | Loss 1.2981(1.2801) | Error 0.0422(0.0350) Steps 548(552.46) | Grad Norm 7.4512(12.2202) | Total Time 10.00(10.00)\n",
      "Iter 2590 | Time 11.5613(11.5278) | Bit/dim 1.1965(1.2214) | Xent 0.1099(0.1126) | Loss 1.2515(1.2777) | Error 0.0289(0.0346) Steps 554(552.32) | Grad Norm 3.2070(11.3409) | Total Time 10.00(10.00)\n",
      "Iter 2600 | Time 11.5048(11.5320) | Bit/dim 1.1915(1.2159) | Xent 0.1131(0.1085) | Loss 1.2481(1.2702) | Error 0.0400(0.0337) Steps 554(552.76) | Grad Norm 4.5316(9.8546) | Total Time 10.00(10.00)\n",
      "Iter 2610 | Time 11.8917(11.5595) | Bit/dim 1.1851(1.2092) | Xent 0.0971(0.1074) | Loss 1.2337(1.2629) | Error 0.0256(0.0331) Steps 554(553.08) | Grad Norm 2.3334(8.2413) | Total Time 10.00(10.00)\n",
      "Iter 2620 | Time 11.4517(11.5649) | Bit/dim 1.1914(1.2042) | Xent 0.1635(0.1059) | Loss 1.2732(1.2571) | Error 0.0444(0.0333) Steps 554(553.46) | Grad Norm 3.6370(6.9420) | Total Time 10.00(10.00)\n",
      "Iter 2630 | Time 11.8662(11.5665) | Bit/dim 1.2025(1.2078) | Xent 0.0867(0.1053) | Loss 1.2458(1.2604) | Error 0.0278(0.0333) Steps 554(553.40) | Grad Norm 3.1065(8.4207) | Total Time 10.00(10.00)\n",
      "Iter 2640 | Time 11.3706(11.6241) | Bit/dim 1.1982(1.2075) | Xent 0.0926(0.1044) | Loss 1.2445(1.2598) | Error 0.0333(0.0331) Steps 554(553.88) | Grad Norm 4.7342(8.6329) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0040 | Time 50.8242, Epoch Time 829.7207(725.8420), Bit/dim 1.1981(best: 1.1989), Xent 0.0524, Loss 1.2243, Error 0.0172(best: 0.0173)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2650 | Time 11.4816(11.6663) | Bit/dim 1.1930(1.2050) | Xent 0.0998(0.1018) | Loss 1.2428(1.2559) | Error 0.0378(0.0326) Steps 554(555.13) | Grad Norm 12.1660(9.3326) | Total Time 10.00(10.00)\n",
      "Iter 2660 | Time 11.5196(11.6159) | Bit/dim 1.1917(1.2007) | Xent 0.0921(0.1021) | Loss 1.2378(1.2518) | Error 0.0289(0.0324) Steps 554(554.97) | Grad Norm 5.4100(8.3832) | Total Time 10.00(10.00)\n",
      "Iter 2670 | Time 11.6882(11.6064) | Bit/dim 1.1887(1.1996) | Xent 0.1066(0.0993) | Loss 1.2420(1.2492) | Error 0.0389(0.0317) Steps 560(555.36) | Grad Norm 8.2813(8.3497) | Total Time 10.00(10.00)\n",
      "Iter 2680 | Time 11.4488(11.5593) | Bit/dim 1.2330(1.2161) | Xent 0.1258(0.1013) | Loss 1.2960(1.2667) | Error 0.0367(0.0326) Steps 554(553.58) | Grad Norm 6.8600(10.8340) | Total Time 10.00(10.00)\n",
      "Iter 2690 | Time 11.9245(11.5602) | Bit/dim 1.2108(1.2170) | Xent 0.0892(0.1030) | Loss 1.2554(1.2685) | Error 0.0322(0.0328) Steps 554(553.33) | Grad Norm 10.4271(10.7053) | Total Time 10.00(10.00)\n",
      "Iter 2700 | Time 11.6724(11.6186) | Bit/dim 1.1979(1.2126) | Xent 0.1133(0.1034) | Loss 1.2545(1.2643) | Error 0.0333(0.0327) Steps 560(554.76) | Grad Norm 9.8232(10.4604) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0041 | Time 50.1357, Epoch Time 832.3554(729.0374), Bit/dim 1.1867(best: 1.1981), Xent 0.0459, Loss 1.2097, Error 0.0146(best: 0.0172)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2710 | Time 11.2870(11.6088) | Bit/dim 1.1745(1.2064) | Xent 0.0955(0.1034) | Loss 1.2222(1.2581) | Error 0.0344(0.0324) Steps 554(554.56) | Grad Norm 5.5008(9.4055) | Total Time 10.00(10.00)\n",
      "Iter 2720 | Time 11.7381(11.5985) | Bit/dim 1.1966(1.2019) | Xent 0.0842(0.1015) | Loss 1.2387(1.2527) | Error 0.0178(0.0316) Steps 554(554.56) | Grad Norm 6.8209(8.3208) | Total Time 10.00(10.00)\n",
      "Iter 2730 | Time 11.5708(11.6059) | Bit/dim 1.1963(1.2015) | Xent 0.1058(0.1029) | Loss 1.2492(1.2529) | Error 0.0344(0.0316) Steps 560(554.92) | Grad Norm 9.0510(8.6567) | Total Time 10.00(10.00)\n",
      "Iter 2740 | Time 11.2834(11.6049) | Bit/dim 1.2144(1.2039) | Xent 0.0874(0.1058) | Loss 1.2581(1.2568) | Error 0.0300(0.0325) Steps 548(554.36) | Grad Norm 11.6209(9.5803) | Total Time 10.00(10.00)\n",
      "Iter 2750 | Time 11.3018(11.6313) | Bit/dim 1.2076(1.2017) | Xent 0.1010(0.1046) | Loss 1.2581(1.2540) | Error 0.0289(0.0319) Steps 554(555.23) | Grad Norm 11.3798(10.0112) | Total Time 10.00(10.00)\n",
      "Iter 2760 | Time 11.5829(11.6031) | Bit/dim 1.1797(1.1992) | Xent 0.0902(0.1015) | Loss 1.2248(1.2499) | Error 0.0178(0.0310) Steps 554(555.21) | Grad Norm 1.7616(9.3232) | Total Time 10.00(10.00)\n",
      "Iter 2770 | Time 12.0592(11.6613) | Bit/dim 1.2076(1.1983) | Xent 0.0741(0.0988) | Loss 1.2447(1.2477) | Error 0.0267(0.0300) Steps 566(555.92) | Grad Norm 28.9454(10.1725) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0042 | Time 50.3689, Epoch Time 833.2964(732.1652), Bit/dim 1.1890(best: 1.1867), Xent 0.0473, Loss 1.2127, Error 0.0155(best: 0.0146)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2780 | Time 11.0384(11.5937) | Bit/dim 1.2368(1.2045) | Xent 0.0965(0.0963) | Loss 1.2850(1.2527) | Error 0.0311(0.0291) Steps 530(553.29) | Grad Norm 13.7785(11.3255) | Total Time 10.00(10.00)\n",
      "Iter 2790 | Time 11.4948(11.5643) | Bit/dim 1.2278(1.2096) | Xent 0.1416(0.1029) | Loss 1.2987(1.2611) | Error 0.0511(0.0315) Steps 554(552.38) | Grad Norm 19.3317(11.7949) | Total Time 10.00(10.00)\n",
      "Iter 2800 | Time 11.3894(11.5825) | Bit/dim 1.1956(1.2092) | Xent 0.0915(0.1032) | Loss 1.2413(1.2608) | Error 0.0267(0.0320) Steps 554(552.23) | Grad Norm 6.1869(11.3558) | Total Time 10.00(10.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle.py --data mnist --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/cnf_conditional_disentangle_bs900_sratio_0_5_drop_0_5_run2 --seed 2 --lr 0.001 --conditional True --controlled_tol False --train_mode semisup --log_freq 10 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
