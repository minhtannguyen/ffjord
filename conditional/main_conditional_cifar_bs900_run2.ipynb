{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='4,5,6,7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_cifar.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "if args.controlled_tol:\n",
      "    import lib.odenvp_conditional_tol as odenvp\n",
      "else:\n",
      "    import lib.odenvp_conditional as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz = modules.GaussianDiag.logp(mean, logs, z).view(-1,1)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(z)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol},)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z = modules.GaussianDiag.sample(mean, logs)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            loss.backward()\n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, conditional=True, controlled_tol=False, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.0001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume='../experiments_published/cnf_conditional_cifar10_bs900_run2/epoch_250_checkpt.pth', rtol=1e-05, save='../experiments_published/cnf_conditional_cifar10_bs900_run2', seed=2, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=6144, bias=True)\n",
      "  (project_class): LinearZeros(in_features=3072, out_features=10, bias=True)\n",
      ")\n",
      "Number of trainable parameters: 1469494\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc.py:286: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "Iter 13760 | Time 22.6133(23.6511) | Bit/dim 3.5050(3.5044) | Xent 0.0124(0.0239) | Loss 3.5112(3.5163) | Error 0.0056(0.0081) Steps 988(999.61) | Grad Norm 0.9087(1.4006) | Total Time 14.00(14.00)\n",
      "Iter 13770 | Time 23.1949(23.5512) | Bit/dim 3.5012(3.5001) | Xent 0.0146(0.0209) | Loss 3.5085(3.5105) | Error 0.0056(0.0070) Steps 988(999.08) | Grad Norm 0.8908(1.2406) | Total Time 14.00(14.00)\n",
      "Iter 13780 | Time 23.6974(23.5362) | Bit/dim 3.4997(3.4975) | Xent 0.0128(0.0178) | Loss 3.5061(3.5064) | Error 0.0033(0.0059) Steps 994(999.46) | Grad Norm 0.6309(1.0791) | Total Time 14.00(14.00)\n",
      "Iter 13790 | Time 22.8163(23.4959) | Bit/dim 3.5022(3.4985) | Xent 0.0088(0.0159) | Loss 3.5066(3.5065) | Error 0.0022(0.0051) Steps 1006(999.35) | Grad Norm 0.5806(0.9491) | Total Time 14.00(14.00)\n",
      "Iter 13800 | Time 22.9804(23.4434) | Bit/dim 3.4472(3.4938) | Xent 0.0079(0.0140) | Loss 3.4511(3.5008) | Error 0.0033(0.0044) Steps 1006(999.42) | Grad Norm 0.3944(0.8320) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0251 | Time 118.3402, Epoch Time 1438.2146(1378.6075), Bit/dim 3.4936(best: inf), Xent 2.9781, Loss 4.9827, Error 0.3814(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13810 | Time 23.2557(23.4272) | Bit/dim 3.4924(3.4945) | Xent 0.0079(0.0123) | Loss 3.4963(3.5007) | Error 0.0022(0.0038) Steps 1000(999.43) | Grad Norm 0.4510(0.7378) | Total Time 14.00(14.00)\n",
      "Iter 13820 | Time 23.1927(23.4371) | Bit/dim 3.4819(3.4913) | Xent 0.0120(0.0116) | Loss 3.4879(3.4971) | Error 0.0022(0.0037) Steps 1006(999.53) | Grad Norm 0.5561(0.6913) | Total Time 14.00(14.00)\n",
      "Iter 13830 | Time 23.3094(23.4097) | Bit/dim 3.4411(3.4912) | Xent 0.0074(0.0105) | Loss 3.4448(3.4965) | Error 0.0011(0.0032) Steps 1000(998.99) | Grad Norm 0.4257(0.6276) | Total Time 14.00(14.00)\n",
      "Iter 13840 | Time 22.9445(23.4115) | Bit/dim 3.5055(3.4891) | Xent 0.0099(0.0099) | Loss 3.5105(3.4940) | Error 0.0022(0.0030) Steps 1006(999.06) | Grad Norm 0.4248(0.5819) | Total Time 14.00(14.00)\n",
      "Iter 13850 | Time 23.4474(23.4009) | Bit/dim 3.4907(3.4897) | Xent 0.0071(0.0091) | Loss 3.4943(3.4942) | Error 0.0022(0.0027) Steps 1000(998.08) | Grad Norm 0.3880(0.5422) | Total Time 14.00(14.00)\n",
      "Iter 13860 | Time 23.2276(23.4039) | Bit/dim 3.5051(3.4891) | Xent 0.0111(0.0086) | Loss 3.5107(3.4934) | Error 0.0011(0.0024) Steps 982(998.27) | Grad Norm 0.4956(0.5128) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0252 | Time 112.4762, Epoch Time 1415.3925(1379.7111), Bit/dim 3.4900(best: 3.4936), Xent 2.9600, Loss 4.9700, Error 0.3770(best: 0.3814)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13870 | Time 23.8676(23.3994) | Bit/dim 3.5051(3.4864) | Xent 0.0134(0.0083) | Loss 3.5118(3.4906) | Error 0.0056(0.0024) Steps 1012(999.39) | Grad Norm 0.5842(0.4882) | Total Time 14.00(14.00)\n",
      "Iter 13880 | Time 23.5244(23.3503) | Bit/dim 3.5194(3.4869) | Xent 0.0052(0.0075) | Loss 3.5220(3.4907) | Error 0.0011(0.0021) Steps 994(997.87) | Grad Norm 0.3653(0.4632) | Total Time 14.00(14.00)\n",
      "Iter 13890 | Time 23.7516(23.3793) | Bit/dim 3.4765(3.4869) | Xent 0.0083(0.0074) | Loss 3.4806(3.4906) | Error 0.0011(0.0020) Steps 1000(996.79) | Grad Norm 0.5534(0.4619) | Total Time 14.00(14.00)\n",
      "Iter 13900 | Time 22.9111(23.3441) | Bit/dim 3.5037(3.4867) | Xent 0.0055(0.0078) | Loss 3.5065(3.4906) | Error 0.0011(0.0020) Steps 988(996.18) | Grad Norm 0.4650(0.4700) | Total Time 14.00(14.00)\n",
      "Iter 13910 | Time 22.9561(23.3747) | Bit/dim 3.5180(3.4885) | Xent 0.0048(0.0076) | Loss 3.5204(3.4923) | Error 0.0011(0.0020) Steps 1000(997.34) | Grad Norm 0.3292(0.4611) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0253 | Time 112.0070, Epoch Time 1411.8909(1380.6765), Bit/dim 3.4883(best: 3.4900), Xent 2.9443, Loss 4.9604, Error 0.3833(best: 0.3770)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13920 | Time 22.9767(23.3776) | Bit/dim 3.4822(3.4869) | Xent 0.0025(0.0071) | Loss 3.4835(3.4904) | Error 0.0000(0.0018) Steps 994(996.77) | Grad Norm 0.2207(0.4401) | Total Time 14.00(14.00)\n",
      "Iter 13930 | Time 23.1216(23.4099) | Bit/dim 3.4937(3.4888) | Xent 0.0119(0.0069) | Loss 3.4996(3.4922) | Error 0.0022(0.0018) Steps 988(996.75) | Grad Norm 0.5464(0.4275) | Total Time 14.00(14.00)\n",
      "Iter 13940 | Time 22.9962(23.4354) | Bit/dim 3.4805(3.4887) | Xent 0.0021(0.0066) | Loss 3.4815(3.4920) | Error 0.0000(0.0017) Steps 1006(997.03) | Grad Norm 0.2796(0.4227) | Total Time 14.00(14.00)\n",
      "Iter 13950 | Time 23.7567(23.4246) | Bit/dim 3.4716(3.4864) | Xent 0.0049(0.0069) | Loss 3.4741(3.4898) | Error 0.0011(0.0017) Steps 994(996.73) | Grad Norm 0.3356(0.4198) | Total Time 14.00(14.00)\n",
      "Iter 13960 | Time 23.7545(23.4234) | Bit/dim 3.4627(3.4874) | Xent 0.0043(0.0068) | Loss 3.4648(3.4908) | Error 0.0000(0.0017) Steps 988(996.08) | Grad Norm 0.3581(0.4231) | Total Time 14.00(14.00)\n",
      "Iter 13970 | Time 23.8519(23.4501) | Bit/dim 3.4541(3.4838) | Xent 0.0034(0.0071) | Loss 3.4558(3.4874) | Error 0.0000(0.0019) Steps 1006(996.18) | Grad Norm 0.2601(0.4277) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0254 | Time 111.5136, Epoch Time 1418.7461(1381.8186), Bit/dim 3.4884(best: 3.4883), Xent 2.9538, Loss 4.9653, Error 0.3808(best: 0.3770)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13980 | Time 23.1421(23.4102) | Bit/dim 3.4868(3.4856) | Xent 0.0101(0.0068) | Loss 3.4919(3.4890) | Error 0.0011(0.0016) Steps 988(995.49) | Grad Norm 0.5100(0.4286) | Total Time 14.00(14.00)\n",
      "Iter 13990 | Time 22.4946(23.3909) | Bit/dim 3.4900(3.4852) | Xent 0.0061(0.0065) | Loss 3.4931(3.4884) | Error 0.0022(0.0015) Steps 988(995.70) | Grad Norm 0.3152(0.4130) | Total Time 14.00(14.00)\n",
      "Iter 14000 | Time 23.6395(23.4096) | Bit/dim 3.4898(3.4845) | Xent 0.0041(0.0063) | Loss 3.4918(3.4876) | Error 0.0000(0.0013) Steps 976(994.44) | Grad Norm 0.2980(0.3980) | Total Time 14.00(14.00)\n",
      "Iter 14010 | Time 23.3552(23.4359) | Bit/dim 3.4947(3.4848) | Xent 0.0055(0.0061) | Loss 3.4974(3.4879) | Error 0.0022(0.0014) Steps 994(995.47) | Grad Norm 0.4439(0.3957) | Total Time 14.00(14.00)\n",
      "Iter 14020 | Time 23.8292(23.4395) | Bit/dim 3.4743(3.4849) | Xent 0.0029(0.0060) | Loss 3.4758(3.4879) | Error 0.0000(0.0013) Steps 1006(995.10) | Grad Norm 0.2568(0.4058) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0255 | Time 111.6479, Epoch Time 1416.6414(1382.8632), Bit/dim 3.4870(best: 3.4883), Xent 2.9339, Loss 4.9539, Error 0.3858(best: 0.3770)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14030 | Time 23.6170(23.4669) | Bit/dim 3.5047(3.4827) | Xent 0.0035(0.0058) | Loss 3.5065(3.4856) | Error 0.0011(0.0013) Steps 988(995.44) | Grad Norm 0.3832(0.4021) | Total Time 14.00(14.00)\n",
      "Iter 14040 | Time 23.3174(23.4511) | Bit/dim 3.4638(3.4801) | Xent 0.0053(0.0055) | Loss 3.4665(3.4828) | Error 0.0000(0.0011) Steps 1006(995.49) | Grad Norm 0.3893(0.3758) | Total Time 14.00(14.00)\n",
      "Iter 14050 | Time 24.1734(23.4869) | Bit/dim 3.5114(3.4821) | Xent 0.0030(0.0055) | Loss 3.5129(3.4848) | Error 0.0000(0.0012) Steps 958(993.61) | Grad Norm 0.2137(0.3853) | Total Time 14.00(14.00)\n",
      "Iter 14060 | Time 22.9851(23.5056) | Bit/dim 3.4695(3.4804) | Xent 0.0078(0.0054) | Loss 3.4734(3.4831) | Error 0.0022(0.0012) Steps 1000(993.74) | Grad Norm 0.5341(0.3826) | Total Time 14.00(14.00)\n",
      "Iter 14070 | Time 22.9939(23.4268) | Bit/dim 3.4880(3.4832) | Xent 0.0035(0.0053) | Loss 3.4897(3.4858) | Error 0.0000(0.0011) Steps 988(993.62) | Grad Norm 0.2663(0.3725) | Total Time 14.00(14.00)\n",
      "Iter 14080 | Time 23.2836(23.3685) | Bit/dim 3.4749(3.4861) | Xent 0.0052(0.0055) | Loss 3.4775(3.4889) | Error 0.0011(0.0011) Steps 988(992.34) | Grad Norm 0.4169(0.3882) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0256 | Time 112.0338, Epoch Time 1415.5318(1383.8433), Bit/dim 3.4867(best: 3.4870), Xent 2.9406, Loss 4.9570, Error 0.3866(best: 0.3770)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14090 | Time 23.6497(23.3903) | Bit/dim 3.4602(3.4845) | Xent 0.0062(0.0053) | Loss 3.4633(3.4872) | Error 0.0033(0.0011) Steps 1000(991.45) | Grad Norm 0.5522(0.3802) | Total Time 14.00(14.00)\n",
      "Iter 14100 | Time 22.9281(23.2853) | Bit/dim 3.5006(3.4847) | Xent 0.0045(0.0051) | Loss 3.5028(3.4873) | Error 0.0011(0.0011) Steps 1000(992.09) | Grad Norm 0.3093(0.3678) | Total Time 14.00(14.00)\n",
      "Iter 14110 | Time 22.7380(23.3143) | Bit/dim 3.4718(3.4842) | Xent 0.0036(0.0049) | Loss 3.4736(3.4866) | Error 0.0000(0.0009) Steps 988(991.48) | Grad Norm 0.2840(0.3566) | Total Time 14.00(14.00)\n",
      "Iter 14120 | Time 22.8375(23.2705) | Bit/dim 3.4665(3.4842) | Xent 0.0039(0.0048) | Loss 3.4685(3.4866) | Error 0.0011(0.0009) Steps 976(991.01) | Grad Norm 0.3386(0.3528) | Total Time 14.00(14.00)\n",
      "Iter 14130 | Time 23.0382(23.2746) | Bit/dim 3.4832(3.4839) | Xent 0.0078(0.0050) | Loss 3.4871(3.4864) | Error 0.0022(0.0010) Steps 994(989.64) | Grad Norm 0.4847(0.3716) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0257 | Time 112.8801, Epoch Time 1409.1489(1384.6025), Bit/dim 3.4861(best: 3.4867), Xent 2.9493, Loss 4.9607, Error 0.3813(best: 0.3770)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14140 | Time 23.0896(23.2598) | Bit/dim 3.4723(3.4797) | Xent 0.0044(0.0049) | Loss 3.4745(3.4821) | Error 0.0011(0.0010) Steps 994(990.71) | Grad Norm 0.4746(0.3997) | Total Time 14.00(14.00)\n",
      "Iter 14150 | Time 23.4523(23.2486) | Bit/dim 3.4825(3.4820) | Xent 0.0025(0.0048) | Loss 3.4838(3.4843) | Error 0.0000(0.0009) Steps 994(991.05) | Grad Norm 0.2353(0.3853) | Total Time 14.00(14.00)\n",
      "Iter 14160 | Time 22.8926(23.2896) | Bit/dim 3.4625(3.4814) | Xent 0.0089(0.0052) | Loss 3.4669(3.4840) | Error 0.0011(0.0010) Steps 988(990.41) | Grad Norm 0.4120(0.3824) | Total Time 14.00(14.00)\n",
      "Iter 14170 | Time 23.0440(23.3299) | Bit/dim 3.5071(3.4838) | Xent 0.0081(0.0050) | Loss 3.5112(3.4863) | Error 0.0011(0.0008) Steps 994(992.68) | Grad Norm 0.4101(0.3609) | Total Time 14.00(14.00)\n",
      "Iter 14180 | Time 22.3591(23.2776) | Bit/dim 3.4875(3.4832) | Xent 0.0048(0.0050) | Loss 3.4899(3.4857) | Error 0.0000(0.0008) Steps 988(991.11) | Grad Norm 0.3256(0.3666) | Total Time 14.00(14.00)\n",
      "Iter 14190 | Time 23.1574(23.2938) | Bit/dim 3.5163(3.4823) | Xent 0.0041(0.0051) | Loss 3.5184(3.4849) | Error 0.0000(0.0009) Steps 982(989.64) | Grad Norm 0.2653(0.3810) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0258 | Time 111.8284, Epoch Time 1408.7391(1385.3266), Bit/dim 3.4854(best: 3.4861), Xent 2.9379, Loss 4.9543, Error 0.3821(best: 0.3770)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14200 | Time 22.5490(23.2873) | Bit/dim 3.4863(3.4826) | Xent 0.0037(0.0050) | Loss 3.4882(3.4851) | Error 0.0000(0.0008) Steps 994(989.16) | Grad Norm 0.2701(0.3736) | Total Time 14.00(14.00)\n",
      "Iter 14210 | Time 23.5606(23.2979) | Bit/dim 3.4767(3.4822) | Xent 0.0058(0.0051) | Loss 3.4796(3.4848) | Error 0.0011(0.0009) Steps 994(989.42) | Grad Norm 0.3634(0.3746) | Total Time 14.00(14.00)\n",
      "Iter 14220 | Time 23.5153(23.2195) | Bit/dim 3.4617(3.4803) | Xent 0.0028(0.0050) | Loss 3.4631(3.4828) | Error 0.0000(0.0009) Steps 988(989.38) | Grad Norm 0.2555(0.3735) | Total Time 14.00(14.00)\n",
      "Iter 14230 | Time 23.0232(23.2030) | Bit/dim 3.4693(3.4793) | Xent 0.0031(0.0050) | Loss 3.4708(3.4818) | Error 0.0000(0.0009) Steps 988(989.13) | Grad Norm 0.2507(0.3701) | Total Time 14.00(14.00)\n",
      "Iter 14240 | Time 23.2562(23.1573) | Bit/dim 3.4902(3.4817) | Xent 0.0068(0.0046) | Loss 3.4936(3.4840) | Error 0.0011(0.0007) Steps 988(988.84) | Grad Norm 0.7209(0.3597) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0259 | Time 112.2890, Epoch Time 1400.7244(1385.7885), Bit/dim 3.4842(best: 3.4854), Xent 2.9338, Loss 4.9511, Error 0.3848(best: 0.3770)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14250 | Time 22.9647(23.1461) | Bit/dim 3.4771(3.4816) | Xent 0.0039(0.0045) | Loss 3.4791(3.4839) | Error 0.0000(0.0007) Steps 1000(988.97) | Grad Norm 0.2868(0.3580) | Total Time 14.00(14.00)\n",
      "Iter 14260 | Time 22.8380(23.1561) | Bit/dim 3.5044(3.4818) | Xent 0.0034(0.0046) | Loss 3.5061(3.4841) | Error 0.0022(0.0008) Steps 988(990.41) | Grad Norm 0.3833(0.3641) | Total Time 14.00(14.00)\n",
      "Iter 14270 | Time 22.9144(23.1759) | Bit/dim 3.4829(3.4814) | Xent 0.0056(0.0049) | Loss 3.4857(3.4839) | Error 0.0011(0.0008) Steps 994(989.99) | Grad Norm 0.6247(0.3872) | Total Time 14.00(14.00)\n",
      "Iter 14280 | Time 23.1420(23.2179) | Bit/dim 3.4592(3.4812) | Xent 0.0060(0.0047) | Loss 3.4622(3.4835) | Error 0.0011(0.0007) Steps 994(988.60) | Grad Norm 0.4677(0.3765) | Total Time 14.00(14.00)\n",
      "Iter 14290 | Time 22.6605(23.2279) | Bit/dim 3.4726(3.4816) | Xent 0.0079(0.0048) | Loss 3.4766(3.4840) | Error 0.0033(0.0008) Steps 982(989.67) | Grad Norm 0.6491(0.3769) | Total Time 14.00(14.00)\n",
      "Iter 14300 | Time 23.3454(23.2804) | Bit/dim 3.4687(3.4814) | Xent 0.0021(0.0046) | Loss 3.4697(3.4837) | Error 0.0000(0.0008) Steps 982(988.49) | Grad Norm 0.2360(0.3759) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0260 | Time 112.4801, Epoch Time 1409.7393(1386.5070), Bit/dim 3.4839(best: 3.4842), Xent 2.9465, Loss 4.9572, Error 0.3863(best: 0.3770)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14310 | Time 23.8967(23.2792) | Bit/dim 3.4799(3.4819) | Xent 0.0037(0.0046) | Loss 3.4817(3.4842) | Error 0.0011(0.0009) Steps 988(989.80) | Grad Norm 0.4520(0.3722) | Total Time 14.00(14.00)\n",
      "Iter 14320 | Time 23.0193(23.2575) | Bit/dim 3.4640(3.4796) | Xent 0.0055(0.0050) | Loss 3.4667(3.4822) | Error 0.0000(0.0009) Steps 988(989.57) | Grad Norm 0.4568(0.3830) | Total Time 14.00(14.00)\n",
      "Iter 14330 | Time 22.8161(23.2222) | Bit/dim 3.4786(3.4782) | Xent 0.0148(0.0054) | Loss 3.4860(3.4809) | Error 0.0044(0.0011) Steps 1000(989.24) | Grad Norm 0.8713(0.3981) | Total Time 14.00(14.00)\n",
      "Iter 14340 | Time 23.2918(23.2404) | Bit/dim 3.5019(3.4791) | Xent 0.0032(0.0050) | Loss 3.5035(3.4816) | Error 0.0000(0.0009) Steps 994(989.86) | Grad Norm 0.2693(0.3869) | Total Time 14.00(14.00)\n",
      "Iter 14350 | Time 23.2273(23.2669) | Bit/dim 3.4810(3.4812) | Xent 0.0029(0.0049) | Loss 3.4824(3.4836) | Error 0.0011(0.0009) Steps 994(988.55) | Grad Norm 0.2345(0.3859) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0261 | Time 113.6387, Epoch Time 1408.5402(1387.1680), Bit/dim 3.4835(best: 3.4839), Xent 2.9509, Loss 4.9590, Error 0.3824(best: 0.3770)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14360 | Time 22.9038(23.2926) | Bit/dim 3.4613(3.4817) | Xent 0.0093(0.0051) | Loss 3.4659(3.4842) | Error 0.0022(0.0010) Steps 988(987.18) | Grad Norm 0.5746(0.3989) | Total Time 14.00(14.00)\n",
      "Iter 14370 | Time 23.5173(23.3236) | Bit/dim 3.4960(3.4798) | Xent 0.0099(0.0052) | Loss 3.5009(3.4824) | Error 0.0011(0.0010) Steps 1006(987.57) | Grad Norm 0.5160(0.3973) | Total Time 14.00(14.00)\n",
      "Iter 14380 | Time 23.6039(23.3536) | Bit/dim 3.5028(3.4797) | Xent 0.0043(0.0050) | Loss 3.5049(3.4822) | Error 0.0011(0.0009) Steps 988(988.65) | Grad Norm 0.4598(0.4112) | Total Time 14.00(14.00)\n",
      "Iter 14390 | Time 23.0633(23.3271) | Bit/dim 3.4717(3.4808) | Xent 0.0043(0.0048) | Loss 3.4739(3.4832) | Error 0.0011(0.0010) Steps 988(988.19) | Grad Norm 0.4119(0.4117) | Total Time 14.00(14.00)\n",
      "Iter 14400 | Time 23.2628(23.3569) | Bit/dim 3.4916(3.4827) | Xent 0.0035(0.0044) | Loss 3.4933(3.4849) | Error 0.0000(0.0008) Steps 994(988.69) | Grad Norm 0.3218(0.3865) | Total Time 14.00(14.00)\n",
      "Iter 14410 | Time 23.7634(23.3812) | Bit/dim 3.5068(3.4821) | Xent 0.0045(0.0042) | Loss 3.5091(3.4842) | Error 0.0011(0.0008) Steps 1006(988.24) | Grad Norm 0.3440(0.3659) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0262 | Time 112.7493, Epoch Time 1416.0274(1388.0338), Bit/dim 3.4838(best: 3.4835), Xent 2.9430, Loss 4.9553, Error 0.3839(best: 0.3770)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14420 | Time 22.9285(23.3466) | Bit/dim 3.4649(3.4814) | Xent 0.0038(0.0043) | Loss 3.4668(3.4835) | Error 0.0000(0.0008) Steps 970(987.57) | Grad Norm 0.2238(0.3637) | Total Time 14.00(14.00)\n",
      "Iter 14430 | Time 23.8492(23.3887) | Bit/dim 3.4838(3.4811) | Xent 0.0073(0.0043) | Loss 3.4874(3.4832) | Error 0.0033(0.0008) Steps 1000(988.91) | Grad Norm 0.6318(0.3665) | Total Time 14.00(14.00)\n",
      "Iter 14440 | Time 23.1980(23.3856) | Bit/dim 3.4709(3.4795) | Xent 0.0057(0.0044) | Loss 3.4737(3.4818) | Error 0.0022(0.0009) Steps 988(988.96) | Grad Norm 0.3464(0.3802) | Total Time 14.00(14.00)\n",
      "Iter 14450 | Time 23.7087(23.3240) | Bit/dim 3.4526(3.4796) | Xent 0.0023(0.0042) | Loss 3.4537(3.4817) | Error 0.0000(0.0008) Steps 982(988.79) | Grad Norm 0.2658(0.3556) | Total Time 14.00(14.00)\n",
      "Iter 14460 | Time 23.9108(23.3061) | Bit/dim 3.4493(3.4811) | Xent 0.0034(0.0043) | Loss 3.4510(3.4832) | Error 0.0000(0.0008) Steps 1006(989.67) | Grad Norm 0.3281(0.3481) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0263 | Time 112.3727, Epoch Time 1409.1803(1388.6682), Bit/dim 3.4829(best: 3.4835), Xent 2.9569, Loss 4.9613, Error 0.3850(best: 0.3770)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14470 | Time 22.7993(23.2272) | Bit/dim 3.4728(3.4799) | Xent 0.0023(0.0041) | Loss 3.4739(3.4819) | Error 0.0000(0.0008) Steps 976(987.77) | Grad Norm 0.1916(0.3455) | Total Time 14.00(14.00)\n",
      "Iter 14480 | Time 23.3687(23.1742) | Bit/dim 3.4615(3.4802) | Xent 0.0041(0.0041) | Loss 3.4636(3.4822) | Error 0.0011(0.0008) Steps 982(986.49) | Grad Norm 0.3934(0.3379) | Total Time 14.00(14.00)\n",
      "Iter 14490 | Time 23.5309(23.2279) | Bit/dim 3.4625(3.4800) | Xent 0.0114(0.0043) | Loss 3.4682(3.4821) | Error 0.0022(0.0008) Steps 976(985.91) | Grad Norm 0.5949(0.3373) | Total Time 14.00(14.00)\n",
      "Iter 14500 | Time 22.8553(23.2661) | Bit/dim 3.4944(3.4783) | Xent 0.0022(0.0042) | Loss 3.4955(3.4804) | Error 0.0000(0.0007) Steps 976(986.31) | Grad Norm 0.2037(0.3346) | Total Time 14.00(14.00)\n",
      "Iter 14510 | Time 23.0358(23.2204) | Bit/dim 3.4946(3.4790) | Xent 0.0041(0.0043) | Loss 3.4967(3.4811) | Error 0.0000(0.0007) Steps 982(987.83) | Grad Norm 0.2784(0.3518) | Total Time 14.00(14.00)\n",
      "Iter 14520 | Time 22.6429(23.1593) | Bit/dim 3.4751(3.4791) | Xent 0.0024(0.0043) | Loss 3.4763(3.4812) | Error 0.0000(0.0007) Steps 982(987.75) | Grad Norm 0.2334(0.3497) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0264 | Time 111.6332, Epoch Time 1400.8584(1389.0339), Bit/dim 3.4821(best: 3.4829), Xent 2.9723, Loss 4.9683, Error 0.3856(best: 0.3770)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14530 | Time 23.1900(23.1497) | Bit/dim 3.4664(3.4777) | Xent 0.0021(0.0042) | Loss 3.4674(3.4798) | Error 0.0000(0.0006) Steps 988(987.95) | Grad Norm 0.1917(0.3614) | Total Time 14.00(14.00)\n",
      "Iter 14540 | Time 22.8684(23.2024) | Bit/dim 3.4845(3.4778) | Xent 0.0037(0.0040) | Loss 3.4864(3.4798) | Error 0.0000(0.0005) Steps 976(987.32) | Grad Norm 0.2404(0.3414) | Total Time 14.00(14.00)\n",
      "Iter 14550 | Time 23.3059(23.1543) | Bit/dim 3.4948(3.4784) | Xent 0.0033(0.0041) | Loss 3.4964(3.4805) | Error 0.0011(0.0005) Steps 976(987.77) | Grad Norm 0.2122(0.3372) | Total Time 14.00(14.00)\n",
      "Iter 14560 | Time 23.5857(23.1176) | Bit/dim 3.5158(3.4767) | Xent 0.0040(0.0045) | Loss 3.5178(3.4789) | Error 0.0000(0.0006) Steps 994(986.15) | Grad Norm 0.4239(0.3569) | Total Time 14.00(14.00)\n",
      "Iter 14570 | Time 23.4627(23.1282) | Bit/dim 3.4711(3.4779) | Xent 0.0031(0.0043) | Loss 3.4727(3.4800) | Error 0.0000(0.0006) Steps 994(987.32) | Grad Norm 0.2873(0.3478) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0265 | Time 111.0616, Epoch Time 1399.4636(1389.3468), Bit/dim 3.4821(best: 3.4821), Xent 2.9128, Loss 4.9385, Error 0.3867(best: 0.3770)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14580 | Time 23.2067(23.1805) | Bit/dim 3.4887(3.4789) | Xent 0.0074(0.0043) | Loss 3.4924(3.4810) | Error 0.0011(0.0006) Steps 1000(988.33) | Grad Norm 0.5806(0.3437) | Total Time 14.00(14.00)\n",
      "Iter 14590 | Time 23.3973(23.2344) | Bit/dim 3.4669(3.4794) | Xent 0.0033(0.0041) | Loss 3.4685(3.4815) | Error 0.0000(0.0006) Steps 976(986.74) | Grad Norm 0.2648(0.3391) | Total Time 14.00(14.00)\n",
      "Iter 14600 | Time 22.8689(23.1665) | Bit/dim 3.4807(3.4799) | Xent 0.0023(0.0042) | Loss 3.4818(3.4820) | Error 0.0000(0.0006) Steps 970(985.15) | Grad Norm 0.2163(0.3401) | Total Time 14.00(14.00)\n",
      "Iter 14610 | Time 23.3910(23.1892) | Bit/dim 3.4852(3.4789) | Xent 0.0031(0.0041) | Loss 3.4867(3.4809) | Error 0.0000(0.0005) Steps 994(987.09) | Grad Norm 0.1988(0.3377) | Total Time 14.00(14.00)\n",
      "Iter 14620 | Time 23.2302(23.2328) | Bit/dim 3.4432(3.4774) | Xent 0.0052(0.0041) | Loss 3.4458(3.4795) | Error 0.0022(0.0005) Steps 988(985.88) | Grad Norm 0.4092(0.3425) | Total Time 14.00(14.00)\n",
      "Iter 14630 | Time 23.4527(23.2370) | Bit/dim 3.4771(3.4782) | Xent 0.0063(0.0041) | Loss 3.4803(3.4802) | Error 0.0011(0.0005) Steps 964(984.62) | Grad Norm 0.3665(0.3419) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0266 | Time 112.5734, Epoch Time 1407.9244(1389.9041), Bit/dim 3.4823(best: 3.4821), Xent 2.9620, Loss 4.9634, Error 0.3873(best: 0.3770)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14640 | Time 23.3612(23.2605) | Bit/dim 3.4733(3.4793) | Xent 0.0030(0.0041) | Loss 3.4748(3.4814) | Error 0.0000(0.0005) Steps 1000(986.21) | Grad Norm 0.4214(0.3704) | Total Time 14.00(14.00)\n",
      "Iter 14650 | Time 23.5731(23.3008) | Bit/dim 3.4861(3.4773) | Xent 0.0046(0.0041) | Loss 3.4885(3.4794) | Error 0.0011(0.0006) Steps 976(987.03) | Grad Norm 0.6346(0.3858) | Total Time 14.00(14.00)\n",
      "Iter 14660 | Time 23.0790(23.2395) | Bit/dim 3.4793(3.4789) | Xent 0.0040(0.0044) | Loss 3.4812(3.4810) | Error 0.0011(0.0007) Steps 982(988.01) | Grad Norm 0.4610(0.4254) | Total Time 14.00(14.00)\n",
      "Iter 14670 | Time 23.4016(23.2599) | Bit/dim 3.5156(3.4771) | Xent 0.0062(0.0043) | Loss 3.5187(3.4792) | Error 0.0011(0.0006) Steps 988(986.61) | Grad Norm 0.5103(0.4229) | Total Time 14.00(14.00)\n",
      "Iter 14680 | Time 23.9207(23.2410) | Bit/dim 3.4895(3.4792) | Xent 0.0045(0.0042) | Loss 3.4917(3.4813) | Error 0.0011(0.0006) Steps 988(986.63) | Grad Norm 0.3337(0.3981) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0267 | Time 112.5341, Epoch Time 1405.0698(1390.3591), Bit/dim 3.4812(best: 3.4821), Xent 2.9620, Loss 4.9621, Error 0.3853(best: 0.3770)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14690 | Time 23.0578(23.1681) | Bit/dim 3.4697(3.4782) | Xent 0.0038(0.0042) | Loss 3.4716(3.4804) | Error 0.0000(0.0006) Steps 970(984.95) | Grad Norm 0.3409(0.3878) | Total Time 14.00(14.00)\n",
      "Iter 14700 | Time 23.5290(23.1139) | Bit/dim 3.4698(3.4770) | Xent 0.0055(0.0042) | Loss 3.4725(3.4791) | Error 0.0011(0.0007) Steps 988(984.24) | Grad Norm 0.5724(0.3853) | Total Time 14.00(14.00)\n",
      "Iter 14710 | Time 23.3810(23.1526) | Bit/dim 3.5159(3.4784) | Xent 0.0062(0.0041) | Loss 3.5189(3.4805) | Error 0.0022(0.0007) Steps 988(985.22) | Grad Norm 0.4495(0.3609) | Total Time 14.00(14.00)\n",
      "Iter 14720 | Time 23.5021(23.1906) | Bit/dim 3.4525(3.4796) | Xent 0.0031(0.0044) | Loss 3.4540(3.4818) | Error 0.0000(0.0008) Steps 1000(984.52) | Grad Norm 0.3164(0.3688) | Total Time 14.00(14.00)\n",
      "Iter 14730 | Time 23.2938(23.1919) | Bit/dim 3.4867(3.4796) | Xent 0.0039(0.0046) | Loss 3.4886(3.4819) | Error 0.0011(0.0008) Steps 1000(985.18) | Grad Norm 0.5265(0.3926) | Total Time 14.00(14.00)\n",
      "Iter 14740 | Time 22.9456(23.1706) | Bit/dim 3.4829(3.4777) | Xent 0.0057(0.0044) | Loss 3.4857(3.4799) | Error 0.0011(0.0008) Steps 988(984.88) | Grad Norm 0.4126(0.3942) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0268 | Time 111.3073, Epoch Time 1401.0368(1390.6794), Bit/dim 3.4811(best: 3.4812), Xent 2.9703, Loss 4.9663, Error 0.3891(best: 0.3770)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14750 | Time 23.3933(23.1483) | Bit/dim 3.4606(3.4774) | Xent 0.0030(0.0041) | Loss 3.4621(3.4794) | Error 0.0000(0.0006) Steps 982(984.36) | Grad Norm 0.2474(0.3703) | Total Time 14.00(14.00)\n",
      "Iter 14760 | Time 22.9741(23.1272) | Bit/dim 3.4849(3.4798) | Xent 0.0036(0.0041) | Loss 3.4867(3.4819) | Error 0.0000(0.0007) Steps 988(983.76) | Grad Norm 0.4475(0.3786) | Total Time 14.00(14.00)\n",
      "Iter 14770 | Time 23.1508(23.1297) | Bit/dim 3.4894(3.4797) | Xent 0.0027(0.0041) | Loss 3.4908(3.4817) | Error 0.0000(0.0007) Steps 988(984.48) | Grad Norm 0.2866(0.3628) | Total Time 14.00(14.00)\n",
      "Iter 14780 | Time 23.0684(23.0920) | Bit/dim 3.4920(3.4762) | Xent 0.0041(0.0042) | Loss 3.4940(3.4783) | Error 0.0011(0.0008) Steps 982(984.22) | Grad Norm 0.3372(0.3614) | Total Time 14.00(14.00)\n",
      "Iter 14790 | Time 23.2851(23.0835) | Bit/dim 3.4511(3.4750) | Xent 0.0049(0.0042) | Loss 3.4535(3.4771) | Error 0.0000(0.0007) Steps 988(982.67) | Grad Norm 0.3254(0.3587) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0269 | Time 111.6117, Epoch Time 1395.9883(1390.8387), Bit/dim 3.4816(best: 3.4811), Xent 2.9784, Loss 4.9708, Error 0.3876(best: 0.3770)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14800 | Time 23.4085(23.0841) | Bit/dim 3.4755(3.4779) | Xent 0.0024(0.0040) | Loss 3.4767(3.4799) | Error 0.0000(0.0007) Steps 994(981.64) | Grad Norm 0.3670(0.3434) | Total Time 14.00(14.00)\n",
      "Iter 14810 | Time 22.7752(23.0667) | Bit/dim 3.4934(3.4768) | Xent 0.0059(0.0041) | Loss 3.4963(3.4788) | Error 0.0000(0.0006) Steps 982(983.71) | Grad Norm 0.3574(0.3455) | Total Time 14.00(14.00)\n",
      "Iter 14820 | Time 22.3153(22.9863) | Bit/dim 3.4768(3.4768) | Xent 0.0031(0.0043) | Loss 3.4783(3.4790) | Error 0.0000(0.0006) Steps 988(984.52) | Grad Norm 0.2893(0.3651) | Total Time 14.00(14.00)\n",
      "Iter 14830 | Time 22.9185(23.0148) | Bit/dim 3.5094(3.4757) | Xent 0.0032(0.0041) | Loss 3.5110(3.4778) | Error 0.0011(0.0006) Steps 970(984.13) | Grad Norm 0.2742(0.3565) | Total Time 14.00(14.00)\n",
      "Iter 14840 | Time 23.3089(23.0361) | Bit/dim 3.4758(3.4774) | Xent 0.0027(0.0041) | Loss 3.4772(3.4795) | Error 0.0000(0.0007) Steps 976(983.26) | Grad Norm 0.3269(0.3727) | Total Time 14.00(14.00)\n",
      "Iter 14850 | Time 22.9929(23.0980) | Bit/dim 3.4918(3.4769) | Xent 0.0061(0.0041) | Loss 3.4948(3.4789) | Error 0.0022(0.0007) Steps 988(984.64) | Grad Norm 0.4269(0.3683) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0270 | Time 112.0350, Epoch Time 1396.9489(1391.0220), Bit/dim 3.4794(best: 3.4811), Xent 2.9771, Loss 4.9680, Error 0.3834(best: 0.3770)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14860 | Time 23.1703(23.1085) | Bit/dim 3.4579(3.4763) | Xent 0.0027(0.0041) | Loss 3.4593(3.4783) | Error 0.0000(0.0007) Steps 982(985.08) | Grad Norm 0.3721(0.3706) | Total Time 14.00(14.00)\n",
      "Iter 14870 | Time 22.7936(23.1271) | Bit/dim 3.4924(3.4796) | Xent 0.0033(0.0039) | Loss 3.4941(3.4815) | Error 0.0011(0.0006) Steps 982(985.88) | Grad Norm 0.3344(0.3536) | Total Time 14.00(14.00)\n",
      "Iter 14880 | Time 23.2259(23.1444) | Bit/dim 3.4406(3.4775) | Xent 0.0046(0.0041) | Loss 3.4429(3.4795) | Error 0.0011(0.0008) Steps 970(985.76) | Grad Norm 0.3563(0.3577) | Total Time 14.00(14.00)\n",
      "Iter 14890 | Time 23.1267(23.1697) | Bit/dim 3.5068(3.4797) | Xent 0.0108(0.0041) | Loss 3.5122(3.4817) | Error 0.0033(0.0007) Steps 988(984.82) | Grad Norm 0.4591(0.3417) | Total Time 14.00(14.00)\n",
      "Iter 14900 | Time 23.2548(23.1259) | Bit/dim 3.4724(3.4772) | Xent 0.0044(0.0040) | Loss 3.4747(3.4792) | Error 0.0022(0.0007) Steps 976(981.85) | Grad Norm 0.4388(0.3483) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0271 | Time 111.2898, Epoch Time 1398.5565(1391.2480), Bit/dim 3.4795(best: 3.4794), Xent 2.9773, Loss 4.9682, Error 0.3864(best: 0.3770)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14910 | Time 23.0669(23.0353) | Bit/dim 3.4889(3.4764) | Xent 0.0044(0.0040) | Loss 3.4911(3.4784) | Error 0.0011(0.0006) Steps 958(980.54) | Grad Norm 0.3110(0.3573) | Total Time 14.00(14.00)\n",
      "Iter 14920 | Time 23.6052(23.0044) | Bit/dim 3.4930(3.4767) | Xent 0.0035(0.0040) | Loss 3.4948(3.4787) | Error 0.0000(0.0006) Steps 988(982.29) | Grad Norm 0.2867(0.3503) | Total Time 14.00(14.00)\n",
      "Iter 14930 | Time 23.3157(23.0680) | Bit/dim 3.4633(3.4746) | Xent 0.0056(0.0043) | Loss 3.4661(3.4767) | Error 0.0011(0.0007) Steps 982(981.42) | Grad Norm 0.5118(0.3772) | Total Time 14.00(14.00)\n",
      "Iter 14940 | Time 22.9279(23.1238) | Bit/dim 3.5005(3.4756) | Xent 0.0045(0.0041) | Loss 3.5027(3.4777) | Error 0.0011(0.0006) Steps 970(981.87) | Grad Norm 0.4715(0.3712) | Total Time 14.00(14.00)\n",
      "Iter 14950 | Time 23.7106(23.1730) | Bit/dim 3.4543(3.4748) | Xent 0.0035(0.0040) | Loss 3.4560(3.4768) | Error 0.0000(0.0006) Steps 988(982.32) | Grad Norm 0.4062(0.3655) | Total Time 14.00(14.00)\n",
      "Iter 14960 | Time 23.3305(23.2094) | Bit/dim 3.4703(3.4756) | Xent 0.0023(0.0039) | Loss 3.4715(3.4775) | Error 0.0000(0.0005) Steps 976(980.90) | Grad Norm 0.2627(0.3560) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0272 | Time 111.8695, Epoch Time 1403.0392(1391.6018), Bit/dim 3.4805(best: 3.4794), Xent 2.9665, Loss 4.9637, Error 0.3895(best: 0.3770)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14970 | Time 23.6191(23.2734) | Bit/dim 3.4599(3.4745) | Xent 0.0028(0.0039) | Loss 3.4613(3.4765) | Error 0.0000(0.0005) Steps 976(981.04) | Grad Norm 0.3394(0.3630) | Total Time 14.00(14.00)\n",
      "Iter 14980 | Time 23.1328(23.2427) | Bit/dim 3.5023(3.4761) | Xent 0.0073(0.0040) | Loss 3.5060(3.4781) | Error 0.0022(0.0006) Steps 988(981.52) | Grad Norm 0.4543(0.3630) | Total Time 14.00(14.00)\n",
      "Iter 14990 | Time 22.6320(23.2286) | Bit/dim 3.4920(3.4759) | Xent 0.0040(0.0041) | Loss 3.4940(3.4779) | Error 0.0011(0.0007) Steps 988(982.23) | Grad Norm 0.3075(0.3669) | Total Time 14.00(14.00)\n",
      "Iter 15000 | Time 23.2858(23.1975) | Bit/dim 3.4819(3.4744) | Xent 0.0020(0.0039) | Loss 3.4829(3.4764) | Error 0.0000(0.0006) Steps 982(981.63) | Grad Norm 0.2389(0.3458) | Total Time 14.00(14.00)\n",
      "Iter 15010 | Time 24.0362(23.1938) | Bit/dim 3.4902(3.4758) | Xent 0.0032(0.0037) | Loss 3.4917(3.4776) | Error 0.0000(0.0005) Steps 982(980.85) | Grad Norm 0.2407(0.3283) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0273 | Time 111.0110, Epoch Time 1403.6249(1391.9625), Bit/dim 3.4799(best: 3.4794), Xent 2.9719, Loss 4.9659, Error 0.3846(best: 0.3770)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15020 | Time 22.6563(23.1653) | Bit/dim 3.4663(3.4751) | Xent 0.0036(0.0036) | Loss 3.4682(3.4769) | Error 0.0000(0.0005) Steps 988(981.62) | Grad Norm 0.3338(0.3190) | Total Time 14.00(14.00)\n",
      "Iter 15030 | Time 23.2049(23.1054) | Bit/dim 3.4568(3.4755) | Xent 0.0030(0.0037) | Loss 3.4583(3.4773) | Error 0.0011(0.0005) Steps 982(982.88) | Grad Norm 0.3679(0.3211) | Total Time 14.00(14.00)\n",
      "Iter 15040 | Time 22.9769(23.1322) | Bit/dim 3.4918(3.4751) | Xent 0.0026(0.0035) | Loss 3.4931(3.4768) | Error 0.0000(0.0004) Steps 976(982.97) | Grad Norm 0.2117(0.3152) | Total Time 14.00(14.00)\n",
      "Iter 15050 | Time 23.3366(23.1396) | Bit/dim 3.5245(3.4770) | Xent 0.0061(0.0038) | Loss 3.5275(3.4788) | Error 0.0011(0.0005) Steps 982(983.25) | Grad Norm 1.0587(0.3607) | Total Time 14.00(14.00)\n",
      "Iter 15060 | Time 22.2144(23.1609) | Bit/dim 3.4970(3.4755) | Xent 0.0044(0.0039) | Loss 3.4992(3.4774) | Error 0.0000(0.0005) Steps 982(983.89) | Grad Norm 0.5045(0.4194) | Total Time 14.00(14.00)\n",
      "Iter 15070 | Time 23.7636(23.1702) | Bit/dim 3.4589(3.4747) | Xent 0.0027(0.0038) | Loss 3.4603(3.4766) | Error 0.0000(0.0005) Steps 976(982.59) | Grad Norm 0.2799(0.4186) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0274 | Time 111.2895, Epoch Time 1399.3187(1392.1831), Bit/dim 3.4789(best: 3.4794), Xent 3.0034, Loss 4.9806, Error 0.3932(best: 0.3770)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15080 | Time 23.5376(23.1515) | Bit/dim 3.4695(3.4722) | Xent 0.0085(0.0039) | Loss 3.4737(3.4741) | Error 0.0022(0.0005) Steps 970(981.00) | Grad Norm 0.4871(0.3966) | Total Time 14.00(14.00)\n",
      "Iter 15090 | Time 23.3765(23.1532) | Bit/dim 3.4735(3.4737) | Xent 0.0019(0.0040) | Loss 3.4744(3.4757) | Error 0.0000(0.0006) Steps 982(980.80) | Grad Norm 0.2712(0.4210) | Total Time 14.00(14.00)\n",
      "Iter 15100 | Time 23.1524(23.1126) | Bit/dim 3.4631(3.4741) | Xent 0.0060(0.0041) | Loss 3.4661(3.4761) | Error 0.0011(0.0006) Steps 988(981.77) | Grad Norm 0.4426(0.4061) | Total Time 14.00(14.00)\n",
      "Iter 15110 | Time 23.2011(23.1101) | Bit/dim 3.4615(3.4771) | Xent 0.0023(0.0040) | Loss 3.4626(3.4791) | Error 0.0000(0.0006) Steps 970(979.67) | Grad Norm 0.4103(0.4076) | Total Time 14.00(14.00)\n",
      "Iter 15120 | Time 22.9231(23.0967) | Bit/dim 3.4728(3.4777) | Xent 0.0035(0.0039) | Loss 3.4745(3.4796) | Error 0.0011(0.0005) Steps 982(978.11) | Grad Norm 0.3370(0.3828) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0275 | Time 111.0909, Epoch Time 1395.5516(1392.2842), Bit/dim 3.4787(best: 3.4789), Xent 2.9921, Loss 4.9748, Error 0.3882(best: 0.3770)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15130 | Time 22.4424(23.0489) | Bit/dim 3.4555(3.4757) | Xent 0.0046(0.0040) | Loss 3.4578(3.4777) | Error 0.0022(0.0006) Steps 970(978.53) | Grad Norm 0.4254(0.3790) | Total Time 14.00(14.00)\n",
      "Iter 15140 | Time 23.1733(22.9942) | Bit/dim 3.4646(3.4741) | Xent 0.0029(0.0039) | Loss 3.4660(3.4760) | Error 0.0000(0.0006) Steps 958(977.96) | Grad Norm 0.2358(0.3667) | Total Time 14.00(14.00)\n",
      "Iter 15150 | Time 22.3605(22.9979) | Bit/dim 3.4751(3.4724) | Xent 0.0062(0.0040) | Loss 3.4782(3.4744) | Error 0.0011(0.0006) Steps 988(978.44) | Grad Norm 0.5516(0.3637) | Total Time 14.00(14.00)\n",
      "Iter 15160 | Time 22.1968(22.9422) | Bit/dim 3.5015(3.4756) | Xent 0.0033(0.0038) | Loss 3.5032(3.4775) | Error 0.0000(0.0006) Steps 970(976.50) | Grad Norm 0.2679(0.3483) | Total Time 14.00(14.00)\n",
      "Iter 15170 | Time 22.0078(22.9054) | Bit/dim 3.4551(3.4751) | Xent 0.0055(0.0037) | Loss 3.4578(3.4769) | Error 0.0011(0.0005) Steps 970(977.71) | Grad Norm 0.5550(0.3588) | Total Time 14.00(14.00)\n",
      "Iter 15180 | Time 22.4864(22.9443) | Bit/dim 3.4733(3.4756) | Xent 0.0073(0.0037) | Loss 3.4769(3.4775) | Error 0.0011(0.0005) Steps 982(978.63) | Grad Norm 0.6483(0.3745) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0276 | Time 111.0379, Epoch Time 1387.4855(1392.1402), Bit/dim 3.4778(best: 3.4787), Xent 2.9983, Loss 4.9770, Error 0.3899(best: 0.3770)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15190 | Time 22.5729(22.9656) | Bit/dim 3.4484(3.4721) | Xent 0.0033(0.0036) | Loss 3.4500(3.4739) | Error 0.0000(0.0004) Steps 988(979.17) | Grad Norm 0.3797(0.3832) | Total Time 14.00(14.00)\n",
      "Iter 15200 | Time 22.5639(22.9873) | Bit/dim 3.4929(3.4745) | Xent 0.0065(0.0036) | Loss 3.4962(3.4763) | Error 0.0011(0.0005) Steps 994(979.06) | Grad Norm 0.4990(0.3917) | Total Time 14.00(14.00)\n",
      "Iter 15210 | Time 22.6665(22.9932) | Bit/dim 3.4619(3.4728) | Xent 0.0071(0.0039) | Loss 3.4654(3.4747) | Error 0.0022(0.0006) Steps 964(977.76) | Grad Norm 0.5875(0.4014) | Total Time 14.00(14.00)\n",
      "Iter 15220 | Time 22.5847(23.0099) | Bit/dim 3.4675(3.4733) | Xent 0.0041(0.0040) | Loss 3.4695(3.4753) | Error 0.0011(0.0006) Steps 988(977.86) | Grad Norm 0.3997(0.4072) | Total Time 14.00(14.00)\n",
      "Iter 15230 | Time 23.1823(22.9740) | Bit/dim 3.4591(3.4757) | Xent 0.0073(0.0040) | Loss 3.4628(3.4777) | Error 0.0033(0.0007) Steps 976(979.58) | Grad Norm 0.6822(0.4029) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0277 | Time 110.3523, Epoch Time 1390.6049(1392.0942), Bit/dim 3.4778(best: 3.4778), Xent 2.9563, Loss 4.9560, Error 0.3875(best: 0.3770)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15240 | Time 23.4775(22.9212) | Bit/dim 3.4485(3.4754) | Xent 0.0029(0.0038) | Loss 3.4499(3.4772) | Error 0.0000(0.0006) Steps 964(977.81) | Grad Norm 0.2850(0.3846) | Total Time 14.00(14.00)\n",
      "Iter 15250 | Time 22.3768(22.9063) | Bit/dim 3.4330(3.4747) | Xent 0.0021(0.0035) | Loss 3.4340(3.4765) | Error 0.0000(0.0005) Steps 970(978.18) | Grad Norm 0.2313(0.3546) | Total Time 14.00(14.00)\n",
      "Iter 15260 | Time 22.7557(22.9422) | Bit/dim 3.4763(3.4759) | Xent 0.0042(0.0035) | Loss 3.4784(3.4777) | Error 0.0000(0.0004) Steps 976(978.69) | Grad Norm 0.2899(0.3339) | Total Time 14.00(14.00)\n",
      "Iter 15270 | Time 22.8516(22.9926) | Bit/dim 3.5141(3.4790) | Xent 0.0034(0.0038) | Loss 3.5158(3.4809) | Error 0.0000(0.0005) Steps 982(978.05) | Grad Norm 0.2795(0.3571) | Total Time 14.00(14.00)\n",
      "Iter 15280 | Time 22.3328(22.9831) | Bit/dim 3.4404(3.4762) | Xent 0.0035(0.0037) | Loss 3.4421(3.4781) | Error 0.0000(0.0005) Steps 994(979.36) | Grad Norm 0.3130(0.3571) | Total Time 14.00(14.00)\n",
      "Iter 15290 | Time 22.6031(22.9302) | Bit/dim 3.5044(3.4745) | Xent 0.0018(0.0037) | Loss 3.5053(3.4764) | Error 0.0000(0.0005) Steps 982(979.04) | Grad Norm 0.2924(0.3683) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0278 | Time 112.0877, Epoch Time 1390.3925(1392.0431), Bit/dim 3.4777(best: 3.4778), Xent 2.9880, Loss 4.9716, Error 0.3886(best: 0.3770)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15300 | Time 22.5859(23.0376) | Bit/dim 3.4329(3.4725) | Xent 0.0053(0.0037) | Loss 3.4356(3.4744) | Error 0.0011(0.0006) Steps 982(978.47) | Grad Norm 0.5602(0.3757) | Total Time 14.00(14.00)\n",
      "Iter 15310 | Time 23.5499(23.0690) | Bit/dim 3.4278(3.4730) | Xent 0.0047(0.0039) | Loss 3.4302(3.4749) | Error 0.0000(0.0007) Steps 1006(980.64) | Grad Norm 0.4350(0.3939) | Total Time 14.00(14.00)\n",
      "Iter 15320 | Time 22.7767(22.9414) | Bit/dim 3.4773(3.4730) | Xent 0.0081(0.0039) | Loss 3.4813(3.4750) | Error 0.0011(0.0007) Steps 982(980.38) | Grad Norm 0.3223(0.3850) | Total Time 14.00(14.00)\n",
      "Iter 15330 | Time 23.4538(22.9572) | Bit/dim 3.4490(3.4733) | Xent 0.0031(0.0038) | Loss 3.4505(3.4753) | Error 0.0000(0.0006) Steps 994(979.11) | Grad Norm 0.2100(0.3728) | Total Time 14.00(14.00)\n",
      "Iter 15340 | Time 22.4855(22.9226) | Bit/dim 3.5011(3.4744) | Xent 0.0021(0.0040) | Loss 3.5022(3.4763) | Error 0.0000(0.0006) Steps 988(978.78) | Grad Norm 0.4245(0.3913) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0279 | Time 110.6061, Epoch Time 1390.3269(1391.9916), Bit/dim 3.4774(best: 3.4777), Xent 3.0045, Loss 4.9797, Error 0.3915(best: 0.3770)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15350 | Time 22.7512(22.9079) | Bit/dim 3.4853(3.4765) | Xent 0.0033(0.0037) | Loss 3.4870(3.4784) | Error 0.0000(0.0005) Steps 988(978.24) | Grad Norm 0.3946(0.3704) | Total Time 14.00(14.00)\n",
      "Iter 15360 | Time 22.6475(22.9537) | Bit/dim 3.5044(3.4758) | Xent 0.0044(0.0038) | Loss 3.5066(3.4777) | Error 0.0011(0.0006) Steps 982(979.48) | Grad Norm 0.4876(0.3666) | Total Time 14.00(14.00)\n",
      "Iter 15370 | Time 23.2441(22.9908) | Bit/dim 3.4939(3.4747) | Xent 0.0032(0.0037) | Loss 3.4955(3.4765) | Error 0.0000(0.0006) Steps 982(980.46) | Grad Norm 0.3314(0.3737) | Total Time 14.00(14.00)\n",
      "Iter 15380 | Time 22.5372(23.0039) | Bit/dim 3.4559(3.4750) | Xent 0.0019(0.0035) | Loss 3.4569(3.4767) | Error 0.0000(0.0004) Steps 1000(979.31) | Grad Norm 0.2261(0.3497) | Total Time 14.00(14.00)\n",
      "Iter 15390 | Time 22.8565(23.0460) | Bit/dim 3.4610(3.4732) | Xent 0.0038(0.0038) | Loss 3.4628(3.4751) | Error 0.0000(0.0006) Steps 988(979.65) | Grad Norm 0.4450(0.3736) | Total Time 14.00(14.00)\n",
      "Iter 15400 | Time 23.4180(23.0760) | Bit/dim 3.4890(3.4737) | Xent 0.0048(0.0040) | Loss 3.4914(3.4757) | Error 0.0011(0.0007) Steps 970(979.07) | Grad Norm 0.5884(0.4053) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0280 | Time 110.2209, Epoch Time 1396.6396(1392.1311), Bit/dim 3.4778(best: 3.4774), Xent 2.9939, Loss 4.9747, Error 0.3894(best: 0.3770)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15410 | Time 23.4936(23.0780) | Bit/dim 3.4808(3.4744) | Xent 0.0037(0.0039) | Loss 3.4827(3.4763) | Error 0.0011(0.0006) Steps 976(978.38) | Grad Norm 0.4793(0.4191) | Total Time 14.00(14.00)\n",
      "Iter 15420 | Time 22.9896(23.1027) | Bit/dim 3.4712(3.4745) | Xent 0.0016(0.0036) | Loss 3.4720(3.4763) | Error 0.0000(0.0006) Steps 988(977.94) | Grad Norm 0.1916(0.4006) | Total Time 14.00(14.00)\n",
      "Iter 15430 | Time 23.0818(23.1053) | Bit/dim 3.4797(3.4732) | Xent 0.0028(0.0037) | Loss 3.4811(3.4750) | Error 0.0000(0.0005) Steps 976(977.47) | Grad Norm 0.3814(0.3884) | Total Time 14.00(14.00)\n",
      "Iter 15440 | Time 23.3585(23.1421) | Bit/dim 3.4499(3.4705) | Xent 0.0029(0.0036) | Loss 3.4514(3.4723) | Error 0.0000(0.0004) Steps 982(978.35) | Grad Norm 0.2577(0.3659) | Total Time 14.00(14.00)\n",
      "Iter 15450 | Time 23.0143(23.0888) | Bit/dim 3.4698(3.4732) | Xent 0.0052(0.0037) | Loss 3.4724(3.4750) | Error 0.0022(0.0006) Steps 994(979.20) | Grad Norm 0.5980(0.3869) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0281 | Time 111.4888, Epoch Time 1398.4878(1392.3218), Bit/dim 3.4773(best: 3.4774), Xent 3.0009, Loss 4.9777, Error 0.3924(best: 0.3770)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15460 | Time 22.9075(23.0513) | Bit/dim 3.4721(3.4726) | Xent 0.0030(0.0038) | Loss 3.4736(3.4745) | Error 0.0000(0.0007) Steps 988(978.54) | Grad Norm 0.5042(0.4064) | Total Time 14.00(14.00)\n",
      "Iter 15470 | Time 23.1830(23.0988) | Bit/dim 3.4651(3.4716) | Xent 0.0030(0.0037) | Loss 3.4666(3.4735) | Error 0.0000(0.0006) Steps 982(978.32) | Grad Norm 0.3740(0.3995) | Total Time 14.00(14.00)\n",
      "Iter 15480 | Time 24.2485(23.1298) | Bit/dim 3.4842(3.4743) | Xent 0.0029(0.0038) | Loss 3.4856(3.4762) | Error 0.0000(0.0006) Steps 964(977.76) | Grad Norm 0.2797(0.3886) | Total Time 14.00(14.00)\n",
      "Iter 15490 | Time 23.5707(23.1462) | Bit/dim 3.4776(3.4729) | Xent 0.0025(0.0037) | Loss 3.4788(3.4748) | Error 0.0000(0.0005) Steps 952(977.29) | Grad Norm 0.2855(0.3685) | Total Time 14.00(14.00)\n",
      "Iter 15500 | Time 22.3443(23.0494) | Bit/dim 3.4708(3.4711) | Xent 0.0028(0.0039) | Loss 3.4722(3.4731) | Error 0.0000(0.0005) Steps 970(976.28) | Grad Norm 0.2732(0.3855) | Total Time 14.00(14.00)\n",
      "Iter 15510 | Time 22.2735(23.0291) | Bit/dim 3.4598(3.4733) | Xent 0.0052(0.0038) | Loss 3.4624(3.4752) | Error 0.0011(0.0005) Steps 988(976.79) | Grad Norm 0.4983(0.3865) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0282 | Time 111.7335, Epoch Time 1395.2709(1392.4103), Bit/dim 3.4771(best: 3.4773), Xent 3.0156, Loss 4.9849, Error 0.3899(best: 0.3770)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15520 | Time 22.9287(22.9810) | Bit/dim 3.4720(3.4733) | Xent 0.0067(0.0037) | Loss 3.4753(3.4751) | Error 0.0033(0.0006) Steps 970(976.38) | Grad Norm 0.6524(0.4046) | Total Time 14.00(14.00)\n",
      "Iter 15530 | Time 22.7499(22.9244) | Bit/dim 3.4774(3.4729) | Xent 0.0028(0.0037) | Loss 3.4789(3.4747) | Error 0.0011(0.0006) Steps 970(975.56) | Grad Norm 0.5867(0.4291) | Total Time 14.00(14.00)\n",
      "Iter 15540 | Time 23.1917(22.9410) | Bit/dim 3.4742(3.4727) | Xent 0.0060(0.0037) | Loss 3.4772(3.4746) | Error 0.0011(0.0006) Steps 970(974.49) | Grad Norm 0.3847(0.4219) | Total Time 14.00(14.00)\n",
      "Iter 15550 | Time 23.5060(22.9467) | Bit/dim 3.4747(3.4734) | Xent 0.0030(0.0037) | Loss 3.4762(3.4752) | Error 0.0011(0.0007) Steps 988(975.33) | Grad Norm 0.3288(0.3993) | Total Time 14.00(14.00)\n",
      "Iter 15560 | Time 22.0439(22.9079) | Bit/dim 3.4789(3.4745) | Xent 0.0032(0.0037) | Loss 3.4805(3.4763) | Error 0.0011(0.0006) Steps 970(974.36) | Grad Norm 0.4965(0.3952) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0283 | Time 111.7978, Epoch Time 1387.9927(1392.2777), Bit/dim 3.4761(best: 3.4771), Xent 2.9938, Loss 4.9731, Error 0.3912(best: 0.3770)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15570 | Time 23.2523(22.9623) | Bit/dim 3.4375(3.4721) | Xent 0.0039(0.0035) | Loss 3.4394(3.4739) | Error 0.0011(0.0006) Steps 958(973.58) | Grad Norm 0.3465(0.3880) | Total Time 14.00(14.00)\n",
      "Iter 15580 | Time 22.6892(22.9627) | Bit/dim 3.4379(3.4716) | Xent 0.0056(0.0035) | Loss 3.4407(3.4733) | Error 0.0011(0.0006) Steps 976(974.10) | Grad Norm 0.5410(0.3755) | Total Time 14.00(14.00)\n",
      "Iter 15590 | Time 22.7168(23.0064) | Bit/dim 3.4901(3.4712) | Xent 0.0026(0.0037) | Loss 3.4914(3.4731) | Error 0.0000(0.0006) Steps 982(975.74) | Grad Norm 0.2532(0.3748) | Total Time 14.00(14.00)\n",
      "Iter 15600 | Time 22.6879(23.0336) | Bit/dim 3.4758(3.4714) | Xent 0.0043(0.0037) | Loss 3.4780(3.4732) | Error 0.0011(0.0007) Steps 964(977.34) | Grad Norm 0.4521(0.3759) | Total Time 14.00(14.00)\n",
      "Iter 15610 | Time 23.7347(23.0700) | Bit/dim 3.4660(3.4730) | Xent 0.0029(0.0035) | Loss 3.4674(3.4747) | Error 0.0000(0.0007) Steps 982(977.62) | Grad Norm 0.3702(0.3804) | Total Time 14.00(14.00)\n",
      "Iter 15620 | Time 23.1636(23.1212) | Bit/dim 3.4490(3.4742) | Xent 0.0049(0.0034) | Loss 3.4515(3.4759) | Error 0.0011(0.0006) Steps 970(976.56) | Grad Norm 0.5330(0.3677) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0284 | Time 112.1698, Epoch Time 1400.0374(1392.5105), Bit/dim 3.4766(best: 3.4761), Xent 3.0314, Loss 4.9923, Error 0.3930(best: 0.3770)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15630 | Time 23.6319(23.1994) | Bit/dim 3.4606(3.4728) | Xent 0.0035(0.0035) | Loss 3.4623(3.4746) | Error 0.0000(0.0005) Steps 976(977.45) | Grad Norm 0.4128(0.3765) | Total Time 14.00(14.00)\n",
      "Iter 15640 | Time 23.3419(23.1851) | Bit/dim 3.4944(3.4758) | Xent 0.0023(0.0034) | Loss 3.4956(3.4775) | Error 0.0000(0.0005) Steps 970(977.52) | Grad Norm 0.2169(0.3675) | Total Time 14.00(14.00)\n",
      "Iter 15650 | Time 23.6398(23.1461) | Bit/dim 3.4533(3.4743) | Xent 0.0028(0.0036) | Loss 3.4547(3.4761) | Error 0.0000(0.0006) Steps 970(977.57) | Grad Norm 0.4033(0.3873) | Total Time 14.00(14.00)\n",
      "Iter 15660 | Time 22.9589(23.1259) | Bit/dim 3.4739(3.4728) | Xent 0.0033(0.0036) | Loss 3.4755(3.4746) | Error 0.0000(0.0005) Steps 970(976.72) | Grad Norm 0.2737(0.3836) | Total Time 14.00(14.00)\n",
      "Iter 15670 | Time 23.6106(23.1852) | Bit/dim 3.4563(3.4723) | Xent 0.0028(0.0035) | Loss 3.4577(3.4741) | Error 0.0000(0.0005) Steps 976(977.91) | Grad Norm 0.3299(0.3635) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0285 | Time 111.8941, Epoch Time 1402.3954(1392.8071), Bit/dim 3.4754(best: 3.4761), Xent 3.0046, Loss 4.9777, Error 0.3912(best: 0.3770)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15680 | Time 23.4515(23.1734) | Bit/dim 3.4918(3.4738) | Xent 0.0018(0.0036) | Loss 3.4927(3.4756) | Error 0.0000(0.0006) Steps 988(979.12) | Grad Norm 0.4023(0.3701) | Total Time 14.00(14.00)\n",
      "Iter 15690 | Time 23.2261(23.1504) | Bit/dim 3.4824(3.4767) | Xent 0.0025(0.0037) | Loss 3.4837(3.4785) | Error 0.0000(0.0007) Steps 970(978.24) | Grad Norm 0.2549(0.3925) | Total Time 14.00(14.00)\n",
      "Iter 15700 | Time 23.0933(23.1197) | Bit/dim 3.4311(3.4737) | Xent 0.0068(0.0038) | Loss 3.4345(3.4756) | Error 0.0022(0.0006) Steps 976(977.83) | Grad Norm 0.6922(0.3925) | Total Time 14.00(14.00)\n",
      "Iter 15710 | Time 22.1868(23.1139) | Bit/dim 3.4714(3.4739) | Xent 0.0036(0.0038) | Loss 3.4732(3.4758) | Error 0.0011(0.0006) Steps 982(979.42) | Grad Norm 0.3066(0.3751) | Total Time 14.00(14.00)\n",
      "Iter 15720 | Time 22.2489(23.0596) | Bit/dim 3.4985(3.4708) | Xent 0.0033(0.0037) | Loss 3.5002(3.4726) | Error 0.0000(0.0005) Steps 970(976.52) | Grad Norm 0.3896(0.3826) | Total Time 14.00(14.00)\n",
      "Iter 15730 | Time 22.6325(23.0526) | Bit/dim 3.4557(3.4707) | Xent 0.0019(0.0035) | Loss 3.4566(3.4725) | Error 0.0000(0.0006) Steps 970(976.14) | Grad Norm 0.2422(0.3911) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0286 | Time 112.6434, Epoch Time 1397.9948(1392.9627), Bit/dim 3.4759(best: 3.4754), Xent 3.0148, Loss 4.9833, Error 0.3855(best: 0.3770)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15740 | Time 22.7447(23.0504) | Bit/dim 3.5056(3.4723) | Xent 0.0023(0.0033) | Loss 3.5067(3.4740) | Error 0.0000(0.0005) Steps 970(975.98) | Grad Norm 0.3717(0.3857) | Total Time 14.00(14.00)\n",
      "Iter 15750 | Time 22.6050(23.0175) | Bit/dim 3.4884(3.4708) | Xent 0.0019(0.0032) | Loss 3.4894(3.4724) | Error 0.0000(0.0005) Steps 958(974.28) | Grad Norm 0.2526(0.3712) | Total Time 14.00(14.00)\n",
      "Iter 15760 | Time 23.6550(23.0297) | Bit/dim 3.4684(3.4729) | Xent 0.0037(0.0034) | Loss 3.4702(3.4746) | Error 0.0011(0.0005) Steps 970(974.51) | Grad Norm 0.4034(0.3618) | Total Time 14.00(14.00)\n",
      "Iter 15770 | Time 22.5837(22.9820) | Bit/dim 3.4757(3.4716) | Xent 0.0028(0.0032) | Loss 3.4771(3.4733) | Error 0.0000(0.0003) Steps 976(974.95) | Grad Norm 0.2253(0.3433) | Total Time 14.00(14.00)\n",
      "Iter 15780 | Time 22.9509(22.9739) | Bit/dim 3.4679(3.4721) | Xent 0.0062(0.0034) | Loss 3.4710(3.4738) | Error 0.0022(0.0005) Steps 970(975.03) | Grad Norm 0.8543(0.3590) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0287 | Time 110.3925, Epoch Time 1388.3300(1392.8237), Bit/dim 3.4766(best: 3.4754), Xent 3.0126, Loss 4.9828, Error 0.3878(best: 0.3770)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15790 | Time 22.7489(22.9431) | Bit/dim 3.4596(3.4709) | Xent 0.0026(0.0034) | Loss 3.4609(3.4726) | Error 0.0000(0.0005) Steps 988(973.53) | Grad Norm 0.3035(0.3730) | Total Time 14.00(14.00)\n",
      "Iter 15800 | Time 22.6426(22.9565) | Bit/dim 3.4943(3.4718) | Xent 0.0027(0.0035) | Loss 3.4956(3.4736) | Error 0.0000(0.0006) Steps 964(973.50) | Grad Norm 0.2838(0.3936) | Total Time 14.00(14.00)\n",
      "Iter 15810 | Time 22.6380(22.9359) | Bit/dim 3.4676(3.4726) | Xent 0.0033(0.0035) | Loss 3.4692(3.4744) | Error 0.0000(0.0006) Steps 970(974.45) | Grad Norm 0.3953(0.3991) | Total Time 14.00(14.00)\n",
      "Iter 15820 | Time 23.4364(22.9287) | Bit/dim 3.4667(3.4707) | Xent 0.0046(0.0036) | Loss 3.4690(3.4725) | Error 0.0011(0.0005) Steps 976(974.74) | Grad Norm 0.4819(0.3830) | Total Time 14.00(14.00)\n",
      "Iter 15830 | Time 21.9257(22.9509) | Bit/dim 3.5070(3.4709) | Xent 0.0024(0.0036) | Loss 3.5082(3.4727) | Error 0.0000(0.0005) Steps 982(975.95) | Grad Norm 0.3100(0.3771) | Total Time 14.00(14.00)\n",
      "Iter 15840 | Time 22.4662(22.9526) | Bit/dim 3.4744(3.4721) | Xent 0.0032(0.0036) | Loss 3.4760(3.4739) | Error 0.0011(0.0006) Steps 958(975.42) | Grad Norm 0.2876(0.3776) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0288 | Time 111.2945, Epoch Time 1389.7071(1392.7302), Bit/dim 3.4757(best: 3.4754), Xent 3.0706, Loss 5.0109, Error 0.3933(best: 0.3770)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15850 | Time 22.0423(22.8996) | Bit/dim 3.4533(3.4703) | Xent 0.0023(0.0033) | Loss 3.4545(3.4720) | Error 0.0000(0.0005) Steps 964(975.90) | Grad Norm 0.2350(0.3604) | Total Time 14.00(14.00)\n",
      "Iter 15860 | Time 23.1975(22.9105) | Bit/dim 3.4671(3.4710) | Xent 0.0083(0.0035) | Loss 3.4712(3.4728) | Error 0.0011(0.0005) Steps 988(975.97) | Grad Norm 0.5650(0.3659) | Total Time 14.00(14.00)\n",
      "Iter 15870 | Time 22.6888(22.8753) | Bit/dim 3.5084(3.4728) | Xent 0.0035(0.0033) | Loss 3.5102(3.4744) | Error 0.0011(0.0005) Steps 958(973.87) | Grad Norm 0.5226(0.3624) | Total Time 14.00(14.00)\n",
      "Iter 15880 | Time 22.9661(22.8784) | Bit/dim 3.4735(3.4718) | Xent 0.0050(0.0033) | Loss 3.4760(3.4735) | Error 0.0011(0.0004) Steps 970(975.46) | Grad Norm 0.5058(0.3485) | Total Time 14.00(14.00)\n",
      "Iter 15890 | Time 23.4672(22.9172) | Bit/dim 3.5016(3.4706) | Xent 0.0031(0.0034) | Loss 3.5031(3.4723) | Error 0.0000(0.0004) Steps 988(976.09) | Grad Norm 0.4927(0.3654) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0289 | Time 112.4123, Epoch Time 1387.2454(1392.5657), Bit/dim 3.4748(best: 3.4754), Xent 3.0640, Loss 5.0068, Error 0.3930(best: 0.3770)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15900 | Time 23.6505(22.9602) | Bit/dim 3.4768(3.4707) | Xent 0.0027(0.0034) | Loss 3.4781(3.4724) | Error 0.0000(0.0004) Steps 988(976.78) | Grad Norm 0.2213(0.3524) | Total Time 14.00(14.00)\n",
      "Iter 15910 | Time 22.6928(22.9431) | Bit/dim 3.4849(3.4689) | Xent 0.0032(0.0033) | Loss 3.4865(3.4706) | Error 0.0011(0.0004) Steps 976(976.14) | Grad Norm 0.6256(0.3612) | Total Time 14.00(14.00)\n",
      "Iter 15920 | Time 22.4572(22.9127) | Bit/dim 3.4748(3.4701) | Xent 0.0040(0.0033) | Loss 3.4767(3.4717) | Error 0.0011(0.0005) Steps 952(976.57) | Grad Norm 0.3519(0.3765) | Total Time 14.00(14.00)\n",
      "Iter 15930 | Time 23.5957(22.9178) | Bit/dim 3.4894(3.4706) | Xent 0.0045(0.0034) | Loss 3.4916(3.4724) | Error 0.0011(0.0005) Steps 952(974.40) | Grad Norm 0.4335(0.3867) | Total Time 14.00(14.00)\n",
      "Iter 15940 | Time 22.4662(22.8974) | Bit/dim 3.4996(3.4732) | Xent 0.0044(0.0034) | Loss 3.5019(3.4748) | Error 0.0011(0.0005) Steps 982(974.36) | Grad Norm 0.4735(0.3834) | Total Time 14.00(14.00)\n",
      "Iter 15950 | Time 23.7628(22.9598) | Bit/dim 3.4476(3.4726) | Xent 0.0027(0.0033) | Loss 3.4490(3.4742) | Error 0.0011(0.0005) Steps 1000(975.22) | Grad Norm 0.3839(0.3985) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0290 | Time 112.4707, Epoch Time 1390.8036(1392.5128), Bit/dim 3.4751(best: 3.4748), Xent 3.0673, Loss 5.0088, Error 0.3925(best: 0.3770)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15960 | Time 22.8516(22.9076) | Bit/dim 3.4662(3.4709) | Xent 0.0033(0.0035) | Loss 3.4678(3.4726) | Error 0.0000(0.0005) Steps 976(975.63) | Grad Norm 0.7146(0.4667) | Total Time 14.00(14.00)\n",
      "Iter 15970 | Time 23.9530(22.9701) | Bit/dim 3.4767(3.4699) | Xent 0.0026(0.0036) | Loss 3.4780(3.4718) | Error 0.0000(0.0005) Steps 976(974.89) | Grad Norm 0.3144(0.4786) | Total Time 14.00(14.00)\n",
      "Iter 15980 | Time 23.3463(22.9564) | Bit/dim 3.4724(3.4711) | Xent 0.0037(0.0037) | Loss 3.4742(3.4730) | Error 0.0000(0.0006) Steps 970(974.24) | Grad Norm 0.3932(0.4873) | Total Time 14.00(14.00)\n",
      "Iter 15990 | Time 22.9295(23.0212) | Bit/dim 3.4756(3.4722) | Xent 0.0021(0.0037) | Loss 3.4767(3.4741) | Error 0.0000(0.0005) Steps 982(973.82) | Grad Norm 0.2402(0.4849) | Total Time 14.00(14.00)\n",
      "Iter 16000 | Time 22.4228(22.9776) | Bit/dim 3.4669(3.4704) | Xent 0.0017(0.0034) | Loss 3.4677(3.4721) | Error 0.0000(0.0004) Steps 982(975.55) | Grad Norm 0.2375(0.4731) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0291 | Time 111.1469, Epoch Time 1391.4128(1392.4798), Bit/dim 3.4732(best: 3.4748), Xent 3.0429, Loss 4.9947, Error 0.3902(best: 0.3770)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16010 | Time 22.7877(22.9533) | Bit/dim 3.4336(3.4694) | Xent 0.0049(0.0034) | Loss 3.4360(3.4711) | Error 0.0022(0.0004) Steps 970(974.42) | Grad Norm 0.4947(0.4531) | Total Time 14.00(14.00)\n",
      "Iter 16020 | Time 23.2403(22.9968) | Bit/dim 3.4386(3.4707) | Xent 0.0040(0.0033) | Loss 3.4405(3.4724) | Error 0.0000(0.0004) Steps 988(975.77) | Grad Norm 0.3765(0.4183) | Total Time 14.00(14.00)\n",
      "Iter 16030 | Time 23.0171(22.9876) | Bit/dim 3.4770(3.4717) | Xent 0.0026(0.0032) | Loss 3.4783(3.4733) | Error 0.0000(0.0003) Steps 994(975.48) | Grad Norm 0.2143(0.3966) | Total Time 14.00(14.00)\n",
      "Iter 16040 | Time 23.7870(23.0482) | Bit/dim 3.5096(3.4737) | Xent 0.0032(0.0033) | Loss 3.5112(3.4754) | Error 0.0000(0.0004) Steps 994(974.93) | Grad Norm 0.4856(0.3936) | Total Time 14.00(14.00)\n",
      "Iter 16050 | Time 22.5512(23.0195) | Bit/dim 3.4811(3.4729) | Xent 0.0079(0.0034) | Loss 3.4850(3.4746) | Error 0.0022(0.0005) Steps 982(973.03) | Grad Norm 0.5790(0.4104) | Total Time 14.00(14.00)\n",
      "Iter 16060 | Time 23.3919(22.9692) | Bit/dim 3.4523(3.4690) | Xent 0.0018(0.0033) | Loss 3.4532(3.4707) | Error 0.0000(0.0005) Steps 988(972.76) | Grad Norm 0.3212(0.4183) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0292 | Time 111.9317, Epoch Time 1391.8551(1392.4611), Bit/dim 3.4743(best: 3.4732), Xent 3.0438, Loss 4.9962, Error 0.3919(best: 0.3770)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16070 | Time 22.8055(22.9083) | Bit/dim 3.4498(3.4700) | Xent 0.0033(0.0036) | Loss 3.4515(3.4718) | Error 0.0011(0.0006) Steps 964(972.73) | Grad Norm 0.4565(0.4386) | Total Time 14.00(14.00)\n",
      "Iter 16080 | Time 22.9134(22.9577) | Bit/dim 3.4639(3.4715) | Xent 0.0023(0.0037) | Loss 3.4650(3.4734) | Error 0.0000(0.0006) Steps 982(974.07) | Grad Norm 0.4760(0.4561) | Total Time 14.00(14.00)\n",
      "Iter 16090 | Time 22.6478(22.9327) | Bit/dim 3.4642(3.4705) | Xent 0.0046(0.0036) | Loss 3.4664(3.4723) | Error 0.0011(0.0006) Steps 952(974.00) | Grad Norm 0.4751(0.4642) | Total Time 14.00(14.00)\n",
      "Iter 16100 | Time 22.6703(22.9073) | Bit/dim 3.4111(3.4670) | Xent 0.0025(0.0034) | Loss 3.4124(3.4687) | Error 0.0000(0.0005) Steps 982(974.84) | Grad Norm 0.2259(0.4393) | Total Time 14.00(14.00)\n",
      "Iter 16110 | Time 22.4828(22.9289) | Bit/dim 3.4860(3.4675) | Xent 0.0044(0.0035) | Loss 3.4882(3.4693) | Error 0.0011(0.0005) Steps 958(973.32) | Grad Norm 0.3977(0.4268) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0293 | Time 111.4872, Epoch Time 1387.3259(1392.3070), Bit/dim 3.4750(best: 3.4732), Xent 3.0812, Loss 5.0156, Error 0.3913(best: 0.3770)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16120 | Time 23.0024(22.8735) | Bit/dim 3.4904(3.4716) | Xent 0.0032(0.0034) | Loss 3.4920(3.4733) | Error 0.0011(0.0004) Steps 976(973.55) | Grad Norm 0.3763(0.4190) | Total Time 14.00(14.00)\n",
      "Iter 16130 | Time 23.0746(22.8435) | Bit/dim 3.4580(3.4698) | Xent 0.0033(0.0034) | Loss 3.4597(3.4715) | Error 0.0000(0.0005) Steps 964(972.25) | Grad Norm 0.2729(0.4123) | Total Time 14.00(14.00)\n",
      "Iter 16140 | Time 22.9770(22.8216) | Bit/dim 3.4728(3.4734) | Xent 0.0026(0.0034) | Loss 3.4742(3.4751) | Error 0.0000(0.0004) Steps 970(970.81) | Grad Norm 0.3286(0.4108) | Total Time 14.00(14.00)\n",
      "Iter 16150 | Time 22.2850(22.7785) | Bit/dim 3.4548(3.4709) | Xent 0.0032(0.0033) | Loss 3.4564(3.4726) | Error 0.0011(0.0005) Steps 964(970.68) | Grad Norm 0.4250(0.3988) | Total Time 14.00(14.00)\n",
      "Iter 16160 | Time 22.6712(22.7338) | Bit/dim 3.4444(3.4698) | Xent 0.0065(0.0032) | Loss 3.4477(3.4714) | Error 0.0033(0.0005) Steps 976(969.82) | Grad Norm 0.7341(0.4004) | Total Time 14.00(14.00)\n",
      "Iter 16170 | Time 23.1711(22.8057) | Bit/dim 3.4490(3.4680) | Xent 0.0045(0.0032) | Loss 3.4512(3.4696) | Error 0.0011(0.0006) Steps 976(971.27) | Grad Norm 0.4671(0.4452) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0294 | Time 110.7908, Epoch Time 1377.9827(1391.8773), Bit/dim 3.4735(best: 3.4732), Xent 3.0792, Loss 5.0131, Error 0.3895(best: 0.3770)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16180 | Time 22.8784(22.8304) | Bit/dim 3.4740(3.4660) | Xent 0.0036(0.0032) | Loss 3.4758(3.4676) | Error 0.0011(0.0005) Steps 976(971.74) | Grad Norm 0.4002(0.4506) | Total Time 14.00(14.00)\n",
      "Iter 16190 | Time 21.6632(22.8132) | Bit/dim 3.4597(3.4664) | Xent 0.0035(0.0033) | Loss 3.4615(3.4680) | Error 0.0000(0.0005) Steps 964(972.63) | Grad Norm 0.3854(0.4365) | Total Time 14.00(14.00)\n",
      "Iter 16200 | Time 22.6606(22.8507) | Bit/dim 3.4886(3.4671) | Xent 0.0017(0.0032) | Loss 3.4895(3.4687) | Error 0.0000(0.0005) Steps 970(972.92) | Grad Norm 0.1882(0.4076) | Total Time 14.00(14.00)\n",
      "Iter 16210 | Time 22.6725(22.8242) | Bit/dim 3.5236(3.4710) | Xent 0.0043(0.0035) | Loss 3.5257(3.4727) | Error 0.0011(0.0006) Steps 982(973.01) | Grad Norm 0.3718(0.4139) | Total Time 14.00(14.00)\n",
      "Iter 16220 | Time 23.5703(22.8366) | Bit/dim 3.4615(3.4719) | Xent 0.0019(0.0034) | Loss 3.4624(3.4736) | Error 0.0000(0.0005) Steps 988(972.68) | Grad Norm 0.3360(0.4023) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0295 | Time 110.7367, Epoch Time 1384.5868(1391.6586), Bit/dim 3.4733(best: 3.4732), Xent 3.0903, Loss 5.0184, Error 0.3939(best: 0.3770)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16230 | Time 22.5979(22.8331) | Bit/dim 3.4879(3.4709) | Xent 0.0020(0.0033) | Loss 3.4889(3.4725) | Error 0.0000(0.0005) Steps 1000(974.05) | Grad Norm 0.3564(0.3978) | Total Time 14.00(14.00)\n",
      "Iter 16240 | Time 23.2067(22.8695) | Bit/dim 3.4870(3.4701) | Xent 0.0019(0.0030) | Loss 3.4879(3.4717) | Error 0.0000(0.0004) Steps 976(974.15) | Grad Norm 0.3330(0.3817) | Total Time 14.00(14.00)\n",
      "Iter 16250 | Time 22.8904(22.8526) | Bit/dim 3.4431(3.4679) | Xent 0.0064(0.0030) | Loss 3.4463(3.4694) | Error 0.0033(0.0004) Steps 958(972.83) | Grad Norm 0.5795(0.3626) | Total Time 14.00(14.00)\n",
      "Iter 16260 | Time 23.1834(22.9048) | Bit/dim 3.5008(3.4682) | Xent 0.0026(0.0030) | Loss 3.5021(3.4697) | Error 0.0000(0.0004) Steps 976(972.36) | Grad Norm 0.2879(0.3657) | Total Time 14.00(14.00)\n",
      "Iter 16270 | Time 22.7930(22.8939) | Bit/dim 3.5041(3.4708) | Xent 0.0025(0.0031) | Loss 3.5053(3.4723) | Error 0.0000(0.0003) Steps 964(971.65) | Grad Norm 0.2841(0.3622) | Total Time 14.00(14.00)\n",
      "Iter 16280 | Time 22.9772(22.8759) | Bit/dim 3.4687(3.4700) | Xent 0.0044(0.0033) | Loss 3.4709(3.4716) | Error 0.0000(0.0004) Steps 970(970.69) | Grad Norm 0.3665(0.3556) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0296 | Time 110.9533, Epoch Time 1385.6678(1391.4788), Bit/dim 3.4724(best: 3.4732), Xent 3.0783, Loss 5.0115, Error 0.3933(best: 0.3770)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16290 | Time 23.2069(22.8926) | Bit/dim 3.4555(3.4711) | Xent 0.0017(0.0031) | Loss 3.4563(3.4727) | Error 0.0000(0.0005) Steps 964(970.69) | Grad Norm 0.2979(0.3741) | Total Time 14.00(14.00)\n",
      "Iter 16300 | Time 22.6303(22.8087) | Bit/dim 3.4456(3.4692) | Xent 0.0053(0.0033) | Loss 3.4483(3.4708) | Error 0.0011(0.0005) Steps 964(971.48) | Grad Norm 0.6343(0.3945) | Total Time 14.00(14.00)\n",
      "Iter 16310 | Time 22.6486(22.8107) | Bit/dim 3.4722(3.4705) | Xent 0.0028(0.0032) | Loss 3.4736(3.4721) | Error 0.0000(0.0004) Steps 976(969.40) | Grad Norm 0.2766(0.3806) | Total Time 14.00(14.00)\n",
      "Iter 16320 | Time 23.2425(22.8631) | Bit/dim 3.4837(3.4689) | Xent 0.0019(0.0032) | Loss 3.4846(3.4705) | Error 0.0000(0.0004) Steps 970(968.61) | Grad Norm 0.4417(0.3885) | Total Time 14.00(14.00)\n",
      "Iter 16330 | Time 22.6194(22.8779) | Bit/dim 3.4697(3.4701) | Xent 0.0042(0.0033) | Loss 3.4718(3.4718) | Error 0.0011(0.0005) Steps 946(968.48) | Grad Norm 0.4005(0.3910) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0297 | Time 112.1845, Epoch Time 1384.3863(1391.2661), Bit/dim 3.4731(best: 3.4724), Xent 3.0838, Loss 5.0150, Error 0.3948(best: 0.3770)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16340 | Time 23.1790(22.8400) | Bit/dim 3.4714(3.4691) | Xent 0.0080(0.0034) | Loss 3.4754(3.4708) | Error 0.0011(0.0004) Steps 964(967.52) | Grad Norm 0.4482(0.4057) | Total Time 14.00(14.00)\n",
      "Iter 16350 | Time 23.7973(22.8987) | Bit/dim 3.4674(3.4680) | Xent 0.0028(0.0032) | Loss 3.4688(3.4696) | Error 0.0011(0.0004) Steps 982(967.76) | Grad Norm 0.3161(0.3856) | Total Time 14.00(14.00)\n",
      "Iter 16360 | Time 23.2390(22.8533) | Bit/dim 3.4621(3.4684) | Xent 0.0023(0.0032) | Loss 3.4632(3.4700) | Error 0.0000(0.0004) Steps 964(968.53) | Grad Norm 0.4479(0.3767) | Total Time 14.00(14.00)\n",
      "Iter 16370 | Time 23.5770(22.9021) | Bit/dim 3.4503(3.4696) | Xent 0.0022(0.0031) | Loss 3.4514(3.4712) | Error 0.0000(0.0003) Steps 958(968.60) | Grad Norm 0.2943(0.3618) | Total Time 14.00(14.00)\n",
      "Iter 16380 | Time 22.7259(22.8711) | Bit/dim 3.4259(3.4672) | Xent 0.0045(0.0030) | Loss 3.4282(3.4687) | Error 0.0000(0.0002) Steps 964(966.92) | Grad Norm 0.3838(0.3539) | Total Time 14.00(14.00)\n",
      "Iter 16390 | Time 22.8464(22.9133) | Bit/dim 3.4547(3.4681) | Xent 0.0040(0.0031) | Loss 3.4568(3.4697) | Error 0.0011(0.0003) Steps 964(965.61) | Grad Norm 0.3816(0.3681) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0298 | Time 110.4898, Epoch Time 1386.9379(1391.1362), Bit/dim 3.4731(best: 3.4724), Xent 3.0821, Loss 5.0141, Error 0.3952(best: 0.3770)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_cifar.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/cnf_conditional_cifar10_bs900_run2 --resume ../experiments_published/cnf_conditional_cifar10_bs900_run2/epoch_250_checkpt.pth --seed 2 --lr 0.0001 --conditional True --controlled_tol False --train_mode semisup --log_freq 10 --weight_y 0.5\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
