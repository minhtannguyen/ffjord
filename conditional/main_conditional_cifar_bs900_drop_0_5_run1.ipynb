{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0,1,2,3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_drop_cifar.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "if args.controlled_tol:\n",
      "    import lib.odenvp_conditional_tol as odenvp\n",
      "else:\n",
      "    import lib.odenvp_conditional as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz = modules.GaussianDiag.logp(mean, logs, z).view(-1,1)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        z = model.module.dropout(z)\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(z)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol},\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z = modules.GaussianDiag.sample(mean, logs)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            loss.backward()\n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, conditional=True, controlled_tol=False, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.0001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume='../experiments_published/cnf_conditional_cifar10_bs900_drop_0_5_run1/current_checkpt.pth', rtol=1e-05, save='../experiments_published/cnf_conditional_cifar10_bs900_drop_0_5_run1', seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=6144, bias=True)\n",
      "  (project_class): LinearZeros(in_features=3072, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1469494\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc.py:286: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "Iter 15240 | Time 23.4931(24.2871) | Bit/dim 3.4986(3.5108) | Xent 0.0017(0.0038) | Loss 3.4994(3.5127) | Error 0.0000(0.0009) Steps 940(954.61) | Grad Norm 0.3578(0.5270) | Total Time 14.00(14.00)\n",
      "Iter 15250 | Time 22.6794(24.0220) | Bit/dim 3.5280(3.5122) | Xent 0.0033(0.0040) | Loss 3.5296(3.5142) | Error 0.0011(0.0009) Steps 952(955.02) | Grad Norm 0.7205(0.5612) | Total Time 14.00(14.00)\n",
      "Iter 15260 | Time 23.3423(23.8609) | Bit/dim 3.5196(3.5092) | Xent 0.0014(0.0041) | Loss 3.5203(3.5113) | Error 0.0000(0.0009) Steps 958(954.76) | Grad Norm 0.3177(0.5756) | Total Time 14.00(14.00)\n",
      "Iter 15270 | Time 23.8654(23.7480) | Bit/dim 3.5126(3.5095) | Xent 0.0028(0.0040) | Loss 3.5140(3.5115) | Error 0.0011(0.0010) Steps 952(954.47) | Grad Norm 0.5774(0.6052) | Total Time 14.00(14.00)\n",
      "Iter 15280 | Time 23.2104(23.6846) | Bit/dim 3.5057(3.5082) | Xent 0.0074(0.0039) | Loss 3.5094(3.5102) | Error 0.0011(0.0009) Steps 946(954.80) | Grad Norm 0.6782(0.5826) | Total Time 14.00(14.00)\n",
      "Iter 15290 | Time 23.0297(23.5816) | Bit/dim 3.4772(3.5091) | Xent 0.0025(0.0036) | Loss 3.4784(3.5109) | Error 0.0011(0.0009) Steps 946(955.07) | Grad Norm 0.4171(0.5532) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0278 | Time 122.0022, Epoch Time 1453.3819(1432.3716), Bit/dim 3.5410(best: inf), Xent 2.4639, Loss 4.7730, Error 0.3438(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15300 | Time 22.7434(23.5818) | Bit/dim 3.5154(3.5108) | Xent 0.0075(0.0039) | Loss 3.5192(3.5128) | Error 0.0022(0.0010) Steps 952(959.75) | Grad Norm 0.8950(0.5902) | Total Time 14.00(14.00)\n",
      "Iter 15310 | Time 23.1344(23.5614) | Bit/dim 3.5193(3.5109) | Xent 0.0023(0.0036) | Loss 3.5204(3.5127) | Error 0.0011(0.0010) Steps 952(955.57) | Grad Norm 0.4516(0.5744) | Total Time 14.00(14.00)\n",
      "Iter 15320 | Time 23.1801(23.4838) | Bit/dim 3.4713(3.5069) | Xent 0.0024(0.0033) | Loss 3.4725(3.5085) | Error 0.0011(0.0009) Steps 976(955.90) | Grad Norm 0.5161(0.5445) | Total Time 14.00(14.00)\n",
      "Iter 15330 | Time 23.1970(23.4519) | Bit/dim 3.4925(3.5057) | Xent 0.0010(0.0032) | Loss 3.4930(3.5073) | Error 0.0000(0.0008) Steps 946(954.27) | Grad Norm 0.2576(0.5145) | Total Time 14.00(14.00)\n",
      "Iter 15340 | Time 23.4612(23.4145) | Bit/dim 3.5105(3.5081) | Xent 0.0008(0.0032) | Loss 3.5110(3.5097) | Error 0.0000(0.0009) Steps 946(953.47) | Grad Norm 0.2148(0.5081) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0279 | Time 112.5558, Epoch Time 1418.0225(1431.9412), Bit/dim 3.5406(best: 3.5410), Xent 2.5042, Loss 4.7926, Error 0.3456(best: 0.3438)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15350 | Time 23.8419(23.4291) | Bit/dim 3.5335(3.5091) | Xent 0.0019(0.0029) | Loss 3.5345(3.5106) | Error 0.0000(0.0007) Steps 976(955.12) | Grad Norm 0.3982(0.4835) | Total Time 14.00(14.00)\n",
      "Iter 15360 | Time 23.6166(23.3870) | Bit/dim 3.4978(3.5088) | Xent 0.0155(0.0033) | Loss 3.5055(3.5104) | Error 0.0022(0.0007) Steps 970(957.28) | Grad Norm 0.7608(0.5097) | Total Time 14.00(14.00)\n",
      "Iter 15370 | Time 23.6837(23.3634) | Bit/dim 3.4906(3.5094) | Xent 0.0047(0.0037) | Loss 3.4930(3.5112) | Error 0.0011(0.0008) Steps 976(954.80) | Grad Norm 0.7340(0.5088) | Total Time 14.00(14.00)\n",
      "Iter 15380 | Time 24.6656(23.3797) | Bit/dim 3.4989(3.5092) | Xent 0.0012(0.0033) | Loss 3.4995(3.5108) | Error 0.0000(0.0007) Steps 988(956.52) | Grad Norm 0.1696(0.4552) | Total Time 14.00(14.00)\n",
      "Iter 15390 | Time 23.5149(23.3564) | Bit/dim 3.5318(3.5108) | Xent 0.0020(0.0033) | Loss 3.5328(3.5125) | Error 0.0000(0.0007) Steps 964(954.76) | Grad Norm 0.2949(0.4336) | Total Time 14.00(14.00)\n",
      "Iter 15400 | Time 22.8591(23.4094) | Bit/dim 3.5203(3.5095) | Xent 0.0073(0.0035) | Loss 3.5240(3.5112) | Error 0.0011(0.0008) Steps 964(954.40) | Grad Norm 1.2806(0.4604) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0280 | Time 112.4290, Epoch Time 1416.2605(1431.4707), Bit/dim 3.5399(best: 3.5406), Xent 2.5176, Loss 4.7987, Error 0.3474(best: 0.3438)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15410 | Time 23.3715(23.4311) | Bit/dim 3.5027(3.5074) | Xent 0.0024(0.0035) | Loss 3.5039(3.5092) | Error 0.0011(0.0009) Steps 940(954.97) | Grad Norm 0.4590(0.4908) | Total Time 14.00(14.00)\n",
      "Iter 15420 | Time 22.9507(23.3949) | Bit/dim 3.4728(3.5040) | Xent 0.0009(0.0033) | Loss 3.4733(3.5056) | Error 0.0000(0.0008) Steps 940(952.62) | Grad Norm 0.2942(0.4688) | Total Time 14.00(14.00)\n",
      "Iter 15430 | Time 23.8502(23.4203) | Bit/dim 3.5220(3.5071) | Xent 0.0136(0.0038) | Loss 3.5288(3.5090) | Error 0.0011(0.0009) Steps 952(952.82) | Grad Norm 0.6792(0.5433) | Total Time 14.00(14.00)\n",
      "Iter 15440 | Time 23.9586(23.4818) | Bit/dim 3.5274(3.5095) | Xent 0.0025(0.0036) | Loss 3.5287(3.5113) | Error 0.0011(0.0009) Steps 958(954.86) | Grad Norm 0.7037(0.5984) | Total Time 14.00(14.00)\n",
      "Iter 15450 | Time 24.3209(23.4888) | Bit/dim 3.5151(3.5094) | Xent 0.0019(0.0035) | Loss 3.5161(3.5112) | Error 0.0000(0.0009) Steps 934(952.88) | Grad Norm 0.3737(0.5928) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0281 | Time 112.7938, Epoch Time 1423.1008(1431.2196), Bit/dim 3.5403(best: 3.5399), Xent 2.4960, Loss 4.7883, Error 0.3426(best: 0.3438)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15460 | Time 22.9080(23.4859) | Bit/dim 3.4905(3.5100) | Xent 0.0088(0.0039) | Loss 3.4949(3.5120) | Error 0.0033(0.0011) Steps 976(954.15) | Grad Norm 0.9241(0.6156) | Total Time 14.00(14.00)\n",
      "Iter 15470 | Time 23.1042(23.3983) | Bit/dim 3.5063(3.5103) | Xent 0.0046(0.0038) | Loss 3.5086(3.5122) | Error 0.0011(0.0010) Steps 928(952.12) | Grad Norm 0.8239(0.5948) | Total Time 14.00(14.00)\n",
      "Iter 15480 | Time 23.4740(23.4278) | Bit/dim 3.4927(3.5089) | Xent 0.0022(0.0036) | Loss 3.4938(3.5107) | Error 0.0000(0.0009) Steps 946(950.39) | Grad Norm 0.2355(0.5658) | Total Time 14.00(14.00)\n",
      "Iter 15490 | Time 23.4197(23.3900) | Bit/dim 3.4910(3.5078) | Xent 0.0051(0.0038) | Loss 3.4936(3.5097) | Error 0.0011(0.0011) Steps 982(953.08) | Grad Norm 0.4431(0.5678) | Total Time 14.00(14.00)\n",
      "Iter 15500 | Time 23.5500(23.3994) | Bit/dim 3.5051(3.5099) | Xent 0.0017(0.0037) | Loss 3.5060(3.5117) | Error 0.0000(0.0011) Steps 940(952.43) | Grad Norm 0.2386(0.5787) | Total Time 14.00(14.00)\n",
      "Iter 15510 | Time 24.1876(23.3797) | Bit/dim 3.5457(3.5096) | Xent 0.0028(0.0041) | Loss 3.5472(3.5117) | Error 0.0011(0.0012) Steps 928(951.88) | Grad Norm 0.8446(0.5978) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0282 | Time 112.6178, Epoch Time 1413.5885(1430.6907), Bit/dim 3.5403(best: 3.5399), Xent 2.4870, Loss 4.7838, Error 0.3435(best: 0.3426)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15520 | Time 23.7323(23.4040) | Bit/dim 3.4955(3.5095) | Xent 0.0017(0.0037) | Loss 3.4963(3.5113) | Error 0.0000(0.0010) Steps 934(950.66) | Grad Norm 0.3261(0.5750) | Total Time 14.00(14.00)\n",
      "Iter 15530 | Time 23.6513(23.4237) | Bit/dim 3.4891(3.5068) | Xent 0.0086(0.0037) | Loss 3.4934(3.5087) | Error 0.0022(0.0011) Steps 952(949.94) | Grad Norm 1.1497(0.5784) | Total Time 14.00(14.00)\n",
      "Iter 15540 | Time 23.6068(23.4490) | Bit/dim 3.5005(3.5084) | Xent 0.0043(0.0039) | Loss 3.5026(3.5104) | Error 0.0011(0.0012) Steps 958(949.54) | Grad Norm 0.9729(0.6082) | Total Time 14.00(14.00)\n",
      "Iter 15550 | Time 23.7593(23.4636) | Bit/dim 3.5011(3.5094) | Xent 0.0034(0.0040) | Loss 3.5028(3.5115) | Error 0.0011(0.0012) Steps 952(950.98) | Grad Norm 0.4345(0.6156) | Total Time 14.00(14.00)\n",
      "Iter 15560 | Time 23.0978(23.4372) | Bit/dim 3.5051(3.5087) | Xent 0.0014(0.0038) | Loss 3.5058(3.5106) | Error 0.0000(0.0011) Steps 940(948.95) | Grad Norm 0.3211(0.5953) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0283 | Time 112.7853, Epoch Time 1419.9750(1430.3692), Bit/dim 3.5394(best: 3.5399), Xent 2.5281, Loss 4.8035, Error 0.3433(best: 0.3426)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15570 | Time 23.1105(23.4197) | Bit/dim 3.4807(3.5078) | Xent 0.0031(0.0036) | Loss 3.4822(3.5096) | Error 0.0011(0.0010) Steps 946(949.25) | Grad Norm 0.4876(0.5792) | Total Time 14.00(14.00)\n",
      "Iter 15580 | Time 23.1520(23.3937) | Bit/dim 3.5271(3.5080) | Xent 0.0028(0.0037) | Loss 3.5285(3.5099) | Error 0.0011(0.0010) Steps 958(951.01) | Grad Norm 0.5875(0.5858) | Total Time 14.00(14.00)\n",
      "Iter 15590 | Time 24.0897(23.4118) | Bit/dim 3.5032(3.5074) | Xent 0.0010(0.0037) | Loss 3.5038(3.5093) | Error 0.0000(0.0010) Steps 964(951.58) | Grad Norm 0.3317(0.6125) | Total Time 14.00(14.00)\n",
      "Iter 15600 | Time 24.1232(23.4526) | Bit/dim 3.5118(3.5083) | Xent 0.0071(0.0038) | Loss 3.5153(3.5102) | Error 0.0011(0.0010) Steps 964(952.79) | Grad Norm 0.8093(0.6200) | Total Time 14.00(14.00)\n",
      "Iter 15610 | Time 23.8735(23.4651) | Bit/dim 3.4941(3.5082) | Xent 0.0020(0.0042) | Loss 3.4951(3.5103) | Error 0.0000(0.0010) Steps 982(954.25) | Grad Norm 0.4220(0.6731) | Total Time 14.00(14.00)\n",
      "Iter 15620 | Time 24.2723(23.5513) | Bit/dim 3.5371(3.5097) | Xent 0.0026(0.0038) | Loss 3.5384(3.5116) | Error 0.0011(0.0009) Steps 970(954.95) | Grad Norm 0.3820(0.6220) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0284 | Time 112.5856, Epoch Time 1424.2740(1430.1864), Bit/dim 3.5390(best: 3.5394), Xent 2.4923, Loss 4.7852, Error 0.3433(best: 0.3426)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15630 | Time 23.2646(23.5002) | Bit/dim 3.5102(3.5097) | Xent 0.0027(0.0036) | Loss 3.5116(3.5115) | Error 0.0011(0.0009) Steps 916(954.47) | Grad Norm 0.3659(0.6057) | Total Time 14.00(14.00)\n",
      "Iter 15640 | Time 23.6739(23.4786) | Bit/dim 3.5608(3.5101) | Xent 0.0011(0.0034) | Loss 3.5614(3.5118) | Error 0.0000(0.0008) Steps 952(951.59) | Grad Norm 0.2444(0.5786) | Total Time 14.00(14.00)\n",
      "Iter 15650 | Time 23.4783(23.5044) | Bit/dim 3.5104(3.5088) | Xent 0.0053(0.0033) | Loss 3.5130(3.5104) | Error 0.0011(0.0008) Steps 940(953.40) | Grad Norm 0.7420(0.5726) | Total Time 14.00(14.00)\n",
      "Iter 15660 | Time 23.8127(23.4717) | Bit/dim 3.5171(3.5073) | Xent 0.0011(0.0032) | Loss 3.5176(3.5089) | Error 0.0000(0.0008) Steps 964(953.72) | Grad Norm 0.2478(0.5414) | Total Time 14.00(14.00)\n",
      "Iter 15670 | Time 23.2568(23.4859) | Bit/dim 3.4924(3.5070) | Xent 0.0018(0.0034) | Loss 3.4933(3.5086) | Error 0.0000(0.0008) Steps 976(954.65) | Grad Norm 0.3957(0.5405) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0285 | Time 111.1850, Epoch Time 1417.7659(1429.8138), Bit/dim 3.5397(best: 3.5390), Xent 2.5882, Loss 4.8337, Error 0.3512(best: 0.3426)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15680 | Time 24.0448(23.4540) | Bit/dim 3.5133(3.5088) | Xent 0.0075(0.0039) | Loss 3.5171(3.5108) | Error 0.0022(0.0010) Steps 970(954.22) | Grad Norm 1.3263(0.6227) | Total Time 14.00(14.00)\n",
      "Iter 15690 | Time 23.5305(23.4836) | Bit/dim 3.5226(3.5084) | Xent 0.0016(0.0038) | Loss 3.5234(3.5102) | Error 0.0000(0.0009) Steps 976(953.94) | Grad Norm 0.3563(0.6320) | Total Time 14.00(14.00)\n",
      "Iter 15700 | Time 23.1043(23.4693) | Bit/dim 3.4855(3.5067) | Xent 0.0039(0.0034) | Loss 3.4874(3.5084) | Error 0.0022(0.0009) Steps 946(955.10) | Grad Norm 0.6606(0.5870) | Total Time 14.00(14.00)\n",
      "Iter 15710 | Time 22.6434(23.4331) | Bit/dim 3.5140(3.5095) | Xent 0.0107(0.0041) | Loss 3.5194(3.5116) | Error 0.0022(0.0010) Steps 964(955.44) | Grad Norm 1.3215(0.6427) | Total Time 14.00(14.00)\n",
      "Iter 15720 | Time 22.7531(23.3730) | Bit/dim 3.4956(3.5090) | Xent 0.0022(0.0038) | Loss 3.4967(3.5109) | Error 0.0011(0.0009) Steps 964(955.28) | Grad Norm 0.3533(0.5966) | Total Time 14.00(14.00)\n",
      "Iter 15730 | Time 23.3759(23.3760) | Bit/dim 3.5229(3.5080) | Xent 0.0011(0.0034) | Loss 3.5235(3.5097) | Error 0.0000(0.0008) Steps 964(955.00) | Grad Norm 0.2065(0.5700) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0286 | Time 111.9472, Epoch Time 1416.9810(1429.4288), Bit/dim 3.5393(best: 3.5390), Xent 2.5563, Loss 4.8174, Error 0.3483(best: 0.3426)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15740 | Time 23.7961(23.3922) | Bit/dim 3.5056(3.5080) | Xent 0.0048(0.0036) | Loss 3.5080(3.5098) | Error 0.0011(0.0009) Steps 952(954.51) | Grad Norm 0.8733(0.5837) | Total Time 14.00(14.00)\n",
      "Iter 15750 | Time 24.1032(23.3737) | Bit/dim 3.5424(3.5089) | Xent 0.0024(0.0034) | Loss 3.5436(3.5106) | Error 0.0011(0.0009) Steps 940(953.19) | Grad Norm 0.4891(0.5760) | Total Time 14.00(14.00)\n",
      "Iter 15760 | Time 23.7440(23.4603) | Bit/dim 3.4713(3.5092) | Xent 0.0053(0.0035) | Loss 3.4740(3.5109) | Error 0.0022(0.0009) Steps 934(953.44) | Grad Norm 1.3648(0.6102) | Total Time 14.00(14.00)\n",
      "Iter 15770 | Time 23.5812(23.4886) | Bit/dim 3.4984(3.5073) | Xent 0.0052(0.0035) | Loss 3.5010(3.5090) | Error 0.0011(0.0009) Steps 946(953.70) | Grad Norm 0.8607(0.6069) | Total Time 14.00(14.00)\n",
      "Iter 15780 | Time 24.4020(23.5409) | Bit/dim 3.5244(3.5059) | Xent 0.0007(0.0034) | Loss 3.5248(3.5076) | Error 0.0000(0.0008) Steps 952(954.04) | Grad Norm 0.2238(0.5828) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0287 | Time 112.7463, Epoch Time 1426.4430(1429.3392), Bit/dim 3.5385(best: 3.5390), Xent 2.5336, Loss 4.8053, Error 0.3464(best: 0.3426)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15790 | Time 23.1235(23.5221) | Bit/dim 3.5216(3.5072) | Xent 0.0044(0.0034) | Loss 3.5237(3.5088) | Error 0.0011(0.0008) Steps 946(953.71) | Grad Norm 0.7739(0.5843) | Total Time 14.00(14.00)\n",
      "Iter 15800 | Time 23.5183(23.4913) | Bit/dim 3.5126(3.5069) | Xent 0.0012(0.0033) | Loss 3.5132(3.5086) | Error 0.0000(0.0008) Steps 946(953.78) | Grad Norm 0.2864(0.5705) | Total Time 14.00(14.00)\n",
      "Iter 15810 | Time 22.9937(23.4347) | Bit/dim 3.4901(3.5075) | Xent 0.0025(0.0033) | Loss 3.4913(3.5091) | Error 0.0011(0.0008) Steps 964(953.74) | Grad Norm 0.4459(0.5789) | Total Time 14.00(14.00)\n",
      "Iter 15820 | Time 23.5407(23.4467) | Bit/dim 3.4680(3.5040) | Xent 0.0102(0.0036) | Loss 3.4731(3.5058) | Error 0.0033(0.0009) Steps 964(955.37) | Grad Norm 1.1953(0.6326) | Total Time 14.00(14.00)\n",
      "Iter 15830 | Time 22.5764(23.4195) | Bit/dim 3.5284(3.5058) | Xent 0.0096(0.0043) | Loss 3.5332(3.5079) | Error 0.0011(0.0010) Steps 958(953.67) | Grad Norm 0.7825(0.6512) | Total Time 14.00(14.00)\n",
      "Iter 15840 | Time 23.5063(23.4343) | Bit/dim 3.5195(3.5082) | Xent 0.0009(0.0044) | Loss 3.5200(3.5104) | Error 0.0000(0.0009) Steps 958(955.51) | Grad Norm 0.3402(0.6213) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0288 | Time 112.3242, Epoch Time 1416.5883(1428.9567), Bit/dim 3.5385(best: 3.5385), Xent 2.5119, Loss 4.7945, Error 0.3425(best: 0.3426)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15850 | Time 24.1616(23.3653) | Bit/dim 3.5156(3.5090) | Xent 0.0017(0.0038) | Loss 3.5165(3.5109) | Error 0.0011(0.0008) Steps 988(955.89) | Grad Norm 0.3143(0.5793) | Total Time 14.00(14.00)\n",
      "Iter 15860 | Time 24.0260(23.3139) | Bit/dim 3.4728(3.5075) | Xent 0.0009(0.0038) | Loss 3.4732(3.5093) | Error 0.0000(0.0008) Steps 964(955.09) | Grad Norm 0.2961(0.5921) | Total Time 14.00(14.00)\n",
      "Iter 15870 | Time 22.8793(23.3182) | Bit/dim 3.4838(3.5065) | Xent 0.0036(0.0035) | Loss 3.4856(3.5082) | Error 0.0011(0.0008) Steps 946(954.33) | Grad Norm 0.5558(0.5744) | Total Time 14.00(14.00)\n",
      "Iter 15880 | Time 23.8041(23.3012) | Bit/dim 3.4882(3.5062) | Xent 0.0031(0.0036) | Loss 3.4898(3.5080) | Error 0.0000(0.0009) Steps 940(951.10) | Grad Norm 0.4731(0.5978) | Total Time 14.00(14.00)\n",
      "Iter 15890 | Time 22.5991(23.2849) | Bit/dim 3.5104(3.5060) | Xent 0.0012(0.0033) | Loss 3.5110(3.5077) | Error 0.0000(0.0008) Steps 952(950.52) | Grad Norm 0.2717(0.5655) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0289 | Time 113.1570, Epoch Time 1407.9054(1428.3251), Bit/dim 3.5381(best: 3.5385), Xent 2.5401, Loss 4.8082, Error 0.3459(best: 0.3425)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15900 | Time 24.2258(23.3336) | Bit/dim 3.5058(3.5083) | Xent 0.0033(0.0032) | Loss 3.5075(3.5099) | Error 0.0011(0.0008) Steps 964(951.34) | Grad Norm 1.1489(0.5780) | Total Time 14.00(14.00)\n",
      "Iter 15910 | Time 24.2594(23.3489) | Bit/dim 3.5144(3.5066) | Xent 0.0111(0.0035) | Loss 3.5199(3.5084) | Error 0.0022(0.0009) Steps 958(950.99) | Grad Norm 0.7234(0.5739) | Total Time 14.00(14.00)\n",
      "Iter 15920 | Time 23.2474(23.3518) | Bit/dim 3.4710(3.5081) | Xent 0.0054(0.0032) | Loss 3.4737(3.5097) | Error 0.0011(0.0008) Steps 964(952.17) | Grad Norm 1.4895(0.5731) | Total Time 14.00(14.00)\n",
      "Iter 15930 | Time 23.3174(23.4361) | Bit/dim 3.4948(3.5074) | Xent 0.0070(0.0031) | Loss 3.4983(3.5090) | Error 0.0033(0.0009) Steps 964(953.79) | Grad Norm 1.0957(0.5838) | Total Time 14.00(14.00)\n",
      "Iter 15940 | Time 23.2687(23.4046) | Bit/dim 3.5338(3.5043) | Xent 0.0038(0.0032) | Loss 3.5357(3.5059) | Error 0.0022(0.0009) Steps 952(952.13) | Grad Norm 0.5912(0.5707) | Total Time 14.00(14.00)\n",
      "Iter 15950 | Time 23.1191(23.3313) | Bit/dim 3.5140(3.5073) | Xent 0.0013(0.0034) | Loss 3.5147(3.5090) | Error 0.0000(0.0010) Steps 940(950.71) | Grad Norm 0.4652(0.6205) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0290 | Time 113.3381, Epoch Time 1419.1732(1428.0506), Bit/dim 3.5404(best: 3.5381), Xent 2.5609, Loss 4.8209, Error 0.3436(best: 0.3425)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15960 | Time 23.3611(23.2918) | Bit/dim 3.5020(3.5084) | Xent 0.0030(0.0034) | Loss 3.5035(3.5101) | Error 0.0011(0.0010) Steps 940(951.62) | Grad Norm 0.9979(0.7005) | Total Time 14.00(14.00)\n",
      "Iter 15970 | Time 23.1383(23.2741) | Bit/dim 3.5057(3.5087) | Xent 0.0042(0.0032) | Loss 3.5078(3.5104) | Error 0.0011(0.0009) Steps 964(952.77) | Grad Norm 0.9473(0.6989) | Total Time 14.00(14.00)\n",
      "Iter 15980 | Time 22.7985(23.3250) | Bit/dim 3.5009(3.5046) | Xent 0.0046(0.0032) | Loss 3.5033(3.5062) | Error 0.0022(0.0009) Steps 940(953.36) | Grad Norm 1.1130(0.7009) | Total Time 14.00(14.00)\n",
      "Iter 15990 | Time 23.6324(23.3410) | Bit/dim 3.5008(3.5067) | Xent 0.0011(0.0034) | Loss 3.5014(3.5084) | Error 0.0000(0.0010) Steps 934(953.03) | Grad Norm 0.3837(0.7068) | Total Time 14.00(14.00)\n",
      "Iter 16000 | Time 23.6171(23.4302) | Bit/dim 3.5529(3.5080) | Xent 0.0026(0.0032) | Loss 3.5542(3.5096) | Error 0.0011(0.0009) Steps 964(952.18) | Grad Norm 0.4035(0.6457) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0291 | Time 111.1210, Epoch Time 1414.7456(1427.6514), Bit/dim 3.5397(best: 3.5381), Xent 2.5952, Loss 4.8373, Error 0.3485(best: 0.3425)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16010 | Time 22.7280(23.4574) | Bit/dim 3.5073(3.5081) | Xent 0.0033(0.0035) | Loss 3.5089(3.5099) | Error 0.0011(0.0010) Steps 970(952.50) | Grad Norm 1.3708(0.6994) | Total Time 14.00(14.00)\n",
      "Iter 16020 | Time 23.3052(23.4464) | Bit/dim 3.5231(3.5070) | Xent 0.0018(0.0036) | Loss 3.5241(3.5088) | Error 0.0000(0.0009) Steps 922(952.80) | Grad Norm 0.3181(0.6704) | Total Time 14.00(14.00)\n",
      "Iter 16030 | Time 23.5867(23.4849) | Bit/dim 3.5203(3.5038) | Xent 0.0055(0.0033) | Loss 3.5230(3.5054) | Error 0.0022(0.0008) Steps 964(953.88) | Grad Norm 0.8390(0.6086) | Total Time 14.00(14.00)\n",
      "Iter 16040 | Time 23.2402(23.4797) | Bit/dim 3.5217(3.5049) | Xent 0.0078(0.0035) | Loss 3.5256(3.5066) | Error 0.0011(0.0008) Steps 940(954.60) | Grad Norm 1.5881(0.6162) | Total Time 14.00(14.00)\n",
      "Iter 16050 | Time 22.9102(23.4548) | Bit/dim 3.5398(3.5093) | Xent 0.0067(0.0038) | Loss 3.5431(3.5111) | Error 0.0022(0.0009) Steps 964(955.65) | Grad Norm 1.4855(0.6406) | Total Time 14.00(14.00)\n",
      "Iter 16060 | Time 23.1409(23.5071) | Bit/dim 3.5329(3.5078) | Xent 0.0010(0.0033) | Loss 3.5334(3.5095) | Error 0.0000(0.0008) Steps 976(957.65) | Grad Norm 0.4660(0.6284) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0292 | Time 113.6705, Epoch Time 1425.4041(1427.5840), Bit/dim 3.5380(best: 3.5381), Xent 2.5777, Loss 4.8269, Error 0.3447(best: 0.3425)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16070 | Time 23.7574(23.4900) | Bit/dim 3.5348(3.5078) | Xent 0.0015(0.0029) | Loss 3.5356(3.5093) | Error 0.0000(0.0007) Steps 964(957.72) | Grad Norm 0.3252(0.5754) | Total Time 14.00(14.00)\n",
      "Iter 16080 | Time 23.4687(23.4346) | Bit/dim 3.4853(3.5077) | Xent 0.0027(0.0028) | Loss 3.4867(3.5090) | Error 0.0011(0.0007) Steps 946(955.32) | Grad Norm 0.5237(0.5658) | Total Time 14.00(14.00)\n",
      "Iter 16090 | Time 24.0801(23.5240) | Bit/dim 3.5229(3.5095) | Xent 0.0068(0.0030) | Loss 3.5263(3.5110) | Error 0.0011(0.0007) Steps 970(955.87) | Grad Norm 0.8623(0.5691) | Total Time 14.00(14.00)\n",
      "Iter 16100 | Time 23.2515(23.5315) | Bit/dim 3.5125(3.5070) | Xent 0.0023(0.0031) | Loss 3.5136(3.5085) | Error 0.0000(0.0007) Steps 946(954.62) | Grad Norm 0.6323(0.5535) | Total Time 14.00(14.00)\n",
      "Iter 16110 | Time 23.7801(23.5067) | Bit/dim 3.4988(3.5062) | Xent 0.0012(0.0029) | Loss 3.4994(3.5076) | Error 0.0000(0.0007) Steps 970(953.68) | Grad Norm 0.2362(0.5028) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0293 | Time 111.6248, Epoch Time 1420.3768(1427.3678), Bit/dim 3.5378(best: 3.5380), Xent 2.5998, Loss 4.8377, Error 0.3448(best: 0.3425)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16120 | Time 24.0548(23.4733) | Bit/dim 3.4948(3.5033) | Xent 0.0032(0.0028) | Loss 3.4963(3.5046) | Error 0.0011(0.0007) Steps 982(956.65) | Grad Norm 0.4168(0.4970) | Total Time 14.00(14.00)\n",
      "Iter 16130 | Time 23.2866(23.4262) | Bit/dim 3.5111(3.5041) | Xent 0.0036(0.0029) | Loss 3.5129(3.5055) | Error 0.0011(0.0008) Steps 940(952.86) | Grad Norm 0.5100(0.5098) | Total Time 14.00(14.00)\n",
      "Iter 16140 | Time 23.0625(23.3732) | Bit/dim 3.4989(3.5051) | Xent 0.0017(0.0027) | Loss 3.4997(3.5065) | Error 0.0000(0.0006) Steps 976(951.70) | Grad Norm 0.5719(0.4837) | Total Time 14.00(14.00)\n",
      "Iter 16150 | Time 22.5273(23.3502) | Bit/dim 3.4965(3.5059) | Xent 0.0089(0.0029) | Loss 3.5009(3.5074) | Error 0.0022(0.0007) Steps 958(950.77) | Grad Norm 0.7303(0.4954) | Total Time 14.00(14.00)\n",
      "Iter 16160 | Time 22.4792(23.3109) | Bit/dim 3.4765(3.5021) | Xent 0.0019(0.0028) | Loss 3.4775(3.5035) | Error 0.0000(0.0007) Steps 928(950.21) | Grad Norm 0.5557(0.5363) | Total Time 14.00(14.00)\n",
      "Iter 16170 | Time 23.7528(23.3108) | Bit/dim 3.5273(3.5053) | Xent 0.0016(0.0029) | Loss 3.5282(3.5068) | Error 0.0000(0.0006) Steps 988(950.79) | Grad Norm 0.4287(0.5193) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0294 | Time 110.9638, Epoch Time 1409.9305(1426.8447), Bit/dim 3.5378(best: 3.5378), Xent 2.6223, Loss 4.8490, Error 0.3525(best: 0.3425)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16180 | Time 22.9950(23.2894) | Bit/dim 3.5231(3.5065) | Xent 0.0018(0.0030) | Loss 3.5240(3.5080) | Error 0.0000(0.0007) Steps 940(950.42) | Grad Norm 0.2620(0.5130) | Total Time 14.00(14.00)\n",
      "Iter 16190 | Time 23.0889(23.2066) | Bit/dim 3.5219(3.5089) | Xent 0.0025(0.0029) | Loss 3.5231(3.5103) | Error 0.0000(0.0006) Steps 946(947.78) | Grad Norm 0.4727(0.4878) | Total Time 14.00(14.00)\n",
      "Iter 16200 | Time 23.3296(23.2673) | Bit/dim 3.4829(3.5066) | Xent 0.0059(0.0028) | Loss 3.4858(3.5079) | Error 0.0011(0.0006) Steps 940(948.30) | Grad Norm 0.5261(0.4984) | Total Time 14.00(14.00)\n",
      "Iter 16210 | Time 23.8020(23.2759) | Bit/dim 3.5060(3.5047) | Xent 0.0029(0.0028) | Loss 3.5074(3.5061) | Error 0.0011(0.0007) Steps 970(947.45) | Grad Norm 0.5786(0.5415) | Total Time 14.00(14.00)\n",
      "Iter 16220 | Time 24.0933(23.2782) | Bit/dim 3.4936(3.5021) | Xent 0.0074(0.0032) | Loss 3.4973(3.5037) | Error 0.0011(0.0008) Steps 952(948.61) | Grad Norm 0.8847(0.6174) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0295 | Time 110.3219, Epoch Time 1408.2894(1426.2880), Bit/dim 3.5373(best: 3.5378), Xent 2.6273, Loss 4.8510, Error 0.3486(best: 0.3425)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16230 | Time 22.8849(23.2701) | Bit/dim 3.5132(3.5057) | Xent 0.0076(0.0033) | Loss 3.5170(3.5074) | Error 0.0022(0.0008) Steps 934(947.55) | Grad Norm 1.4312(0.6908) | Total Time 14.00(14.00)\n",
      "Iter 16240 | Time 23.5948(23.2845) | Bit/dim 3.5172(3.5062) | Xent 0.0017(0.0032) | Loss 3.5180(3.5079) | Error 0.0000(0.0008) Steps 946(946.41) | Grad Norm 0.6160(0.6816) | Total Time 14.00(14.00)\n",
      "Iter 16250 | Time 23.4695(23.2698) | Bit/dim 3.5143(3.5068) | Xent 0.0019(0.0034) | Loss 3.5152(3.5085) | Error 0.0000(0.0009) Steps 964(947.62) | Grad Norm 0.3538(0.6884) | Total Time 14.00(14.00)\n",
      "Iter 16260 | Time 23.0986(23.2615) | Bit/dim 3.5148(3.5060) | Xent 0.0042(0.0036) | Loss 3.5169(3.5078) | Error 0.0011(0.0009) Steps 952(947.99) | Grad Norm 0.5981(0.7046) | Total Time 14.00(14.00)\n",
      "Iter 16270 | Time 23.8323(23.3180) | Bit/dim 3.5388(3.5050) | Xent 0.0034(0.0036) | Loss 3.5404(3.5068) | Error 0.0011(0.0009) Steps 964(949.77) | Grad Norm 0.5543(0.6882) | Total Time 14.00(14.00)\n",
      "Iter 16280 | Time 23.1448(23.3007) | Bit/dim 3.5179(3.5045) | Xent 0.0032(0.0041) | Loss 3.5195(3.5065) | Error 0.0011(0.0011) Steps 940(950.71) | Grad Norm 0.7491(0.7604) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0296 | Time 112.4286, Epoch Time 1411.1913(1425.8351), Bit/dim 3.5396(best: 3.5373), Xent 2.6611, Loss 4.8702, Error 0.3522(best: 0.3425)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16290 | Time 22.6544(23.3693) | Bit/dim 3.5237(3.5023) | Xent 0.0016(0.0042) | Loss 3.5245(3.5044) | Error 0.0000(0.0011) Steps 940(948.75) | Grad Norm 0.4018(0.7743) | Total Time 14.00(14.00)\n",
      "Iter 16300 | Time 22.8369(23.3311) | Bit/dim 3.4847(3.5049) | Xent 0.0006(0.0038) | Loss 3.4850(3.5068) | Error 0.0000(0.0010) Steps 958(949.64) | Grad Norm 0.3199(0.7263) | Total Time 14.00(14.00)\n",
      "Iter 16310 | Time 23.6804(23.3253) | Bit/dim 3.5155(3.5044) | Xent 0.0008(0.0037) | Loss 3.5160(3.5062) | Error 0.0000(0.0010) Steps 940(948.07) | Grad Norm 0.4072(0.7350) | Total Time 14.00(14.00)\n",
      "Iter 16320 | Time 22.7748(23.2716) | Bit/dim 3.5311(3.5069) | Xent 0.0115(0.0042) | Loss 3.5369(3.5090) | Error 0.0022(0.0012) Steps 958(947.93) | Grad Norm 1.5464(0.8016) | Total Time 14.00(14.00)\n",
      "Iter 16330 | Time 23.7103(23.3070) | Bit/dim 3.4679(3.5061) | Xent 0.0035(0.0043) | Loss 3.4696(3.5083) | Error 0.0011(0.0013) Steps 970(950.23) | Grad Norm 1.3075(0.8596) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0297 | Time 111.5699, Epoch Time 1413.4908(1425.4648), Bit/dim 3.5397(best: 3.5373), Xent 2.6177, Loss 4.8486, Error 0.3457(best: 0.3425)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16340 | Time 23.1981(23.2941) | Bit/dim 3.5010(3.5071) | Xent 0.0047(0.0044) | Loss 3.5033(3.5093) | Error 0.0011(0.0013) Steps 928(950.97) | Grad Norm 1.3332(0.8426) | Total Time 14.00(14.00)\n",
      "Iter 16350 | Time 23.7457(23.3942) | Bit/dim 3.5275(3.5067) | Xent 0.0009(0.0043) | Loss 3.5280(3.5089) | Error 0.0000(0.0012) Steps 964(955.17) | Grad Norm 0.3913(0.8213) | Total Time 14.00(14.00)\n",
      "Iter 16360 | Time 23.3410(23.4290) | Bit/dim 3.5254(3.5060) | Xent 0.0011(0.0040) | Loss 3.5259(3.5081) | Error 0.0000(0.0011) Steps 958(955.46) | Grad Norm 0.3601(0.7505) | Total Time 14.00(14.00)\n",
      "Iter 16370 | Time 23.6775(23.4118) | Bit/dim 3.5263(3.5046) | Xent 0.0014(0.0040) | Loss 3.5270(3.5066) | Error 0.0000(0.0010) Steps 946(954.88) | Grad Norm 0.3921(0.7257) | Total Time 14.00(14.00)\n",
      "Iter 16380 | Time 22.8836(23.3815) | Bit/dim 3.4752(3.5054) | Xent 0.0128(0.0042) | Loss 3.4816(3.5075) | Error 0.0033(0.0010) Steps 946(952.73) | Grad Norm 1.0864(0.6955) | Total Time 14.00(14.00)\n",
      "Iter 16390 | Time 24.0674(23.4477) | Bit/dim 3.4614(3.5057) | Xent 0.0015(0.0041) | Loss 3.4622(3.5077) | Error 0.0000(0.0010) Steps 958(952.88) | Grad Norm 0.4375(0.6859) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0298 | Time 110.5169, Epoch Time 1418.7315(1425.2628), Bit/dim 3.5369(best: 3.5373), Xent 2.6161, Loss 4.8449, Error 0.3452(best: 0.3425)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16400 | Time 22.7874(23.3962) | Bit/dim 3.5205(3.5078) | Xent 0.0020(0.0038) | Loss 3.5215(3.5097) | Error 0.0011(0.0009) Steps 928(953.33) | Grad Norm 0.8742(0.6311) | Total Time 14.00(14.00)\n",
      "Iter 16410 | Time 23.2556(23.3720) | Bit/dim 3.5117(3.5063) | Xent 0.0020(0.0033) | Loss 3.5128(3.5079) | Error 0.0011(0.0007) Steps 934(949.88) | Grad Norm 0.5193(0.5647) | Total Time 14.00(14.00)\n",
      "Iter 16420 | Time 23.3007(23.3227) | Bit/dim 3.5048(3.5052) | Xent 0.0046(0.0032) | Loss 3.5071(3.5068) | Error 0.0022(0.0007) Steps 946(950.11) | Grad Norm 0.8006(0.5093) | Total Time 14.00(14.00)\n",
      "Iter 16430 | Time 23.1569(23.3103) | Bit/dim 3.5108(3.5039) | Xent 0.0063(0.0037) | Loss 3.5140(3.5058) | Error 0.0022(0.0008) Steps 940(946.91) | Grad Norm 0.8471(0.5479) | Total Time 14.00(14.00)\n",
      "Iter 16440 | Time 23.3491(23.3752) | Bit/dim 3.5129(3.5035) | Xent 0.0063(0.0033) | Loss 3.5161(3.5051) | Error 0.0011(0.0007) Steps 946(946.32) | Grad Norm 0.6997(0.5079) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0299 | Time 110.8263, Epoch Time 1410.7141(1424.8263), Bit/dim 3.5357(best: 3.5369), Xent 2.6323, Loss 4.8518, Error 0.3492(best: 0.3425)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16450 | Time 23.2238(23.2845) | Bit/dim 3.5087(3.5054) | Xent 0.0057(0.0033) | Loss 3.5115(3.5070) | Error 0.0022(0.0007) Steps 946(947.43) | Grad Norm 0.7863(0.5042) | Total Time 14.00(14.00)\n",
      "Iter 16460 | Time 23.7804(23.2911) | Bit/dim 3.5139(3.5048) | Xent 0.0007(0.0031) | Loss 3.5143(3.5063) | Error 0.0000(0.0006) Steps 940(947.47) | Grad Norm 0.2738(0.5023) | Total Time 14.00(14.00)\n",
      "Iter 16470 | Time 23.1597(23.3414) | Bit/dim 3.5407(3.5063) | Xent 0.0051(0.0035) | Loss 3.5432(3.5081) | Error 0.0011(0.0008) Steps 940(949.76) | Grad Norm 0.5290(0.5799) | Total Time 14.00(14.00)\n",
      "Iter 16480 | Time 22.9374(23.3521) | Bit/dim 3.4595(3.5055) | Xent 0.0100(0.0034) | Loss 3.4645(3.5072) | Error 0.0033(0.0008) Steps 964(951.17) | Grad Norm 1.9569(0.5959) | Total Time 14.00(14.00)\n",
      "Iter 16490 | Time 23.4895(23.3790) | Bit/dim 3.5289(3.5046) | Xent 0.0022(0.0036) | Loss 3.5300(3.5064) | Error 0.0000(0.0008) Steps 928(949.19) | Grad Norm 0.5756(0.6278) | Total Time 14.00(14.00)\n",
      "Iter 16500 | Time 23.5182(23.3805) | Bit/dim 3.5007(3.5027) | Xent 0.0020(0.0038) | Loss 3.5017(3.5046) | Error 0.0011(0.0010) Steps 934(951.74) | Grad Norm 0.5804(0.6801) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0300 | Time 110.8896, Epoch Time 1414.1877(1424.5072), Bit/dim 3.5373(best: 3.5357), Xent 2.6191, Loss 4.8468, Error 0.3476(best: 0.3425)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16510 | Time 23.4044(23.3656) | Bit/dim 3.4861(3.5028) | Xent 0.0022(0.0035) | Loss 3.4872(3.5046) | Error 0.0000(0.0009) Steps 940(951.14) | Grad Norm 0.6207(0.6699) | Total Time 14.00(14.00)\n",
      "Iter 16520 | Time 23.6831(23.3659) | Bit/dim 3.4943(3.5018) | Xent 0.0094(0.0034) | Loss 3.4990(3.5035) | Error 0.0011(0.0007) Steps 952(952.57) | Grad Norm 0.4491(0.6111) | Total Time 14.00(14.00)\n",
      "Iter 16530 | Time 22.9669(23.3320) | Bit/dim 3.4965(3.5018) | Xent 0.0017(0.0036) | Loss 3.4973(3.5036) | Error 0.0000(0.0007) Steps 970(954.38) | Grad Norm 0.6446(0.6223) | Total Time 14.00(14.00)\n",
      "Iter 16540 | Time 23.4048(23.3102) | Bit/dim 3.4853(3.5012) | Xent 0.0076(0.0034) | Loss 3.4891(3.5029) | Error 0.0011(0.0007) Steps 970(954.38) | Grad Norm 0.9061(0.6013) | Total Time 14.00(14.00)\n",
      "Iter 16550 | Time 23.7288(23.2248) | Bit/dim 3.5075(3.5019) | Xent 0.0015(0.0037) | Loss 3.5083(3.5037) | Error 0.0000(0.0008) Steps 982(953.97) | Grad Norm 0.2989(0.6063) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0301 | Time 111.2277, Epoch Time 1407.8628(1424.0078), Bit/dim 3.5375(best: 3.5357), Xent 2.7000, Loss 4.8875, Error 0.3555(best: 0.3425)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16560 | Time 23.5047(23.3122) | Bit/dim 3.5272(3.5062) | Xent 0.0009(0.0033) | Loss 3.5276(3.5079) | Error 0.0000(0.0007) Steps 958(952.26) | Grad Norm 0.3348(0.5501) | Total Time 14.00(14.00)\n",
      "Iter 16570 | Time 23.1013(23.3236) | Bit/dim 3.4989(3.5037) | Xent 0.0013(0.0035) | Loss 3.4996(3.5054) | Error 0.0000(0.0008) Steps 940(952.06) | Grad Norm 0.3891(0.5572) | Total Time 14.00(14.00)\n",
      "Iter 16580 | Time 23.1822(23.4534) | Bit/dim 3.5068(3.5036) | Xent 0.0088(0.0037) | Loss 3.5112(3.5054) | Error 0.0022(0.0008) Steps 940(953.00) | Grad Norm 2.2287(0.6353) | Total Time 14.00(14.00)\n",
      "Iter 16590 | Time 23.5923(23.4736) | Bit/dim 3.4828(3.5030) | Xent 0.0089(0.0044) | Loss 3.4873(3.5052) | Error 0.0044(0.0011) Steps 940(952.93) | Grad Norm 1.2994(0.7419) | Total Time 14.00(14.00)\n",
      "Iter 16600 | Time 24.0180(23.4299) | Bit/dim 3.4902(3.5037) | Xent 0.0034(0.0041) | Loss 3.4919(3.5057) | Error 0.0011(0.0010) Steps 952(953.82) | Grad Norm 0.4998(0.7414) | Total Time 14.00(14.00)\n",
      "Iter 16610 | Time 22.9838(23.4121) | Bit/dim 3.5328(3.5048) | Xent 0.0008(0.0037) | Loss 3.5332(3.5066) | Error 0.0000(0.0009) Steps 952(954.33) | Grad Norm 0.2098(0.6870) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0302 | Time 110.1023, Epoch Time 1419.7764(1423.8809), Bit/dim 3.5376(best: 3.5357), Xent 2.6352, Loss 4.8551, Error 0.3426(best: 0.3425)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16620 | Time 23.5193(23.3676) | Bit/dim 3.5154(3.5059) | Xent 0.0006(0.0039) | Loss 3.5157(3.5078) | Error 0.0000(0.0010) Steps 952(953.91) | Grad Norm 0.2543(0.6978) | Total Time 14.00(14.00)\n",
      "Iter 16630 | Time 23.3633(23.3327) | Bit/dim 3.4905(3.5041) | Xent 0.0015(0.0037) | Loss 3.4913(3.5060) | Error 0.0000(0.0009) Steps 946(952.66) | Grad Norm 0.5555(0.6966) | Total Time 14.00(14.00)\n",
      "Iter 16640 | Time 22.9546(23.3402) | Bit/dim 3.5314(3.5048) | Xent 0.0012(0.0036) | Loss 3.5320(3.5066) | Error 0.0000(0.0009) Steps 970(957.24) | Grad Norm 0.4455(0.7061) | Total Time 14.00(14.00)\n",
      "Iter 16650 | Time 23.6289(23.3480) | Bit/dim 3.4674(3.5052) | Xent 0.0056(0.0035) | Loss 3.4702(3.5069) | Error 0.0022(0.0009) Steps 976(959.15) | Grad Norm 0.8292(0.7264) | Total Time 14.00(14.00)\n",
      "Iter 16660 | Time 22.9104(23.3419) | Bit/dim 3.5021(3.5034) | Xent 0.0015(0.0033) | Loss 3.5029(3.5051) | Error 0.0011(0.0008) Steps 946(959.69) | Grad Norm 0.3792(0.6779) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0303 | Time 111.1496, Epoch Time 1412.9277(1423.5523), Bit/dim 3.5368(best: 3.5357), Xent 2.6766, Loss 4.8751, Error 0.3504(best: 0.3425)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16670 | Time 23.6872(23.4031) | Bit/dim 3.4925(3.5049) | Xent 0.0026(0.0032) | Loss 3.4938(3.5065) | Error 0.0011(0.0008) Steps 964(961.49) | Grad Norm 0.4632(0.6781) | Total Time 14.00(14.00)\n",
      "Iter 16680 | Time 23.9819(23.4467) | Bit/dim 3.5032(3.5065) | Xent 0.0094(0.0030) | Loss 3.5079(3.5080) | Error 0.0011(0.0007) Steps 928(957.46) | Grad Norm 1.5201(0.6333) | Total Time 14.00(14.00)\n",
      "Iter 16690 | Time 23.2102(23.4123) | Bit/dim 3.4946(3.5045) | Xent 0.0006(0.0034) | Loss 3.4949(3.5062) | Error 0.0000(0.0008) Steps 934(955.80) | Grad Norm 0.3938(0.6673) | Total Time 14.00(14.00)\n",
      "Iter 16700 | Time 24.1249(23.4259) | Bit/dim 3.5087(3.5029) | Xent 0.0009(0.0033) | Loss 3.5092(3.5045) | Error 0.0000(0.0008) Steps 952(956.22) | Grad Norm 0.5013(0.6894) | Total Time 14.00(14.00)\n",
      "Iter 16710 | Time 23.0633(23.4460) | Bit/dim 3.5446(3.5009) | Xent 0.0014(0.0039) | Loss 3.5454(3.5029) | Error 0.0000(0.0010) Steps 958(958.32) | Grad Norm 0.5678(0.7118) | Total Time 14.00(14.00)\n",
      "Iter 16720 | Time 23.2016(23.4319) | Bit/dim 3.5198(3.5037) | Xent 0.0035(0.0035) | Loss 3.5216(3.5054) | Error 0.0011(0.0009) Steps 928(953.03) | Grad Norm 0.6005(0.6483) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0304 | Time 111.7976, Epoch Time 1419.7269(1423.4375), Bit/dim 3.5351(best: 3.5357), Xent 2.6583, Loss 4.8642, Error 0.3506(best: 0.3425)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16730 | Time 23.6185(23.3846) | Bit/dim 3.5172(3.5035) | Xent 0.0006(0.0031) | Loss 3.5175(3.5051) | Error 0.0000(0.0008) Steps 946(954.19) | Grad Norm 0.2886(0.6172) | Total Time 14.00(14.00)\n",
      "Iter 16740 | Time 22.8812(23.3503) | Bit/dim 3.4735(3.5028) | Xent 0.0009(0.0032) | Loss 3.4739(3.5044) | Error 0.0000(0.0008) Steps 952(954.10) | Grad Norm 0.4411(0.6278) | Total Time 14.00(14.00)\n",
      "Iter 16750 | Time 24.6318(23.4695) | Bit/dim 3.4972(3.5021) | Xent 0.0017(0.0031) | Loss 3.4980(3.5037) | Error 0.0000(0.0009) Steps 934(952.64) | Grad Norm 0.4091(0.6278) | Total Time 14.00(14.00)\n",
      "Iter 16760 | Time 23.8809(23.4397) | Bit/dim 3.4609(3.5008) | Xent 0.0082(0.0032) | Loss 3.4650(3.5024) | Error 0.0022(0.0008) Steps 958(954.83) | Grad Norm 1.0925(0.6123) | Total Time 14.00(14.00)\n",
      "Iter 16770 | Time 22.8788(23.4219) | Bit/dim 3.5133(3.5028) | Xent 0.0145(0.0054) | Loss 3.5206(3.5055) | Error 0.0033(0.0014) Steps 922(953.52) | Grad Norm 2.4975(0.8471) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0305 | Time 112.6059, Epoch Time 1417.0659(1423.2464), Bit/dim 3.5403(best: 3.5351), Xent 2.6353, Loss 4.8579, Error 0.3470(best: 0.3425)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16780 | Time 23.9209(23.4538) | Bit/dim 3.5205(3.5048) | Xent 0.0016(0.0046) | Loss 3.5213(3.5071) | Error 0.0000(0.0013) Steps 928(954.90) | Grad Norm 0.6476(0.8594) | Total Time 14.00(14.00)\n",
      "Iter 16790 | Time 23.7992(23.4239) | Bit/dim 3.4791(3.5062) | Xent 0.0073(0.0044) | Loss 3.4827(3.5084) | Error 0.0033(0.0013) Steps 976(956.27) | Grad Norm 1.0032(0.8562) | Total Time 14.00(14.00)\n",
      "Iter 16800 | Time 22.6982(23.4311) | Bit/dim 3.5342(3.5077) | Xent 0.0012(0.0042) | Loss 3.5348(3.5098) | Error 0.0000(0.0012) Steps 952(955.43) | Grad Norm 0.3919(0.7874) | Total Time 14.00(14.00)\n",
      "Iter 16810 | Time 24.8857(23.4480) | Bit/dim 3.5023(3.5050) | Xent 0.0031(0.0040) | Loss 3.5038(3.5070) | Error 0.0011(0.0011) Steps 970(953.81) | Grad Norm 0.5985(0.7606) | Total Time 14.00(14.00)\n",
      "Iter 16820 | Time 22.9016(23.3864) | Bit/dim 3.5154(3.5049) | Xent 0.0006(0.0036) | Loss 3.5157(3.5067) | Error 0.0000(0.0010) Steps 958(953.17) | Grad Norm 0.3496(0.7044) | Total Time 14.00(14.00)\n",
      "Iter 16830 | Time 23.3502(23.3549) | Bit/dim 3.5080(3.5032) | Xent 0.0038(0.0034) | Loss 3.5099(3.5049) | Error 0.0011(0.0010) Steps 946(952.93) | Grad Norm 0.7478(0.7192) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0306 | Time 111.3548, Epoch Time 1415.3914(1423.0107), Bit/dim 3.5360(best: 3.5351), Xent 2.6634, Loss 4.8677, Error 0.3473(best: 0.3425)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16840 | Time 22.6699(23.2653) | Bit/dim 3.4836(3.5032) | Xent 0.0079(0.0038) | Loss 3.4876(3.5051) | Error 0.0011(0.0011) Steps 964(952.13) | Grad Norm 0.7514(0.7193) | Total Time 14.00(14.00)\n",
      "Iter 16850 | Time 23.0623(23.2624) | Bit/dim 3.5500(3.5041) | Xent 0.0011(0.0036) | Loss 3.5506(3.5059) | Error 0.0000(0.0010) Steps 958(953.20) | Grad Norm 0.3259(0.7048) | Total Time 14.00(14.00)\n",
      "Iter 16860 | Time 23.8110(23.2649) | Bit/dim 3.4934(3.5041) | Xent 0.0071(0.0042) | Loss 3.4970(3.5062) | Error 0.0011(0.0011) Steps 958(953.59) | Grad Norm 0.9043(0.7299) | Total Time 14.00(14.00)\n",
      "Iter 16870 | Time 22.8156(23.3424) | Bit/dim 3.5091(3.5040) | Xent 0.0007(0.0038) | Loss 3.5094(3.5059) | Error 0.0000(0.0011) Steps 934(952.97) | Grad Norm 0.2880(0.7274) | Total Time 14.00(14.00)\n",
      "Iter 16880 | Time 23.6674(23.4469) | Bit/dim 3.5377(3.5048) | Xent 0.0043(0.0034) | Loss 3.5398(3.5065) | Error 0.0011(0.0009) Steps 934(954.05) | Grad Norm 0.9640(0.6744) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0307 | Time 112.0539, Epoch Time 1417.2870(1422.8390), Bit/dim 3.5354(best: 3.5351), Xent 2.6621, Loss 4.8665, Error 0.3474(best: 0.3425)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16890 | Time 24.7233(23.5810) | Bit/dim 3.5135(3.5015) | Xent 0.0034(0.0036) | Loss 3.5152(3.5034) | Error 0.0011(0.0010) Steps 1012(956.70) | Grad Norm 0.8778(0.7466) | Total Time 14.00(14.00)\n",
      "Iter 16900 | Time 23.0413(23.4904) | Bit/dim 3.5031(3.5012) | Xent 0.0023(0.0033) | Loss 3.5043(3.5029) | Error 0.0011(0.0009) Steps 946(956.62) | Grad Norm 0.6940(0.6897) | Total Time 14.00(14.00)\n",
      "Iter 16910 | Time 23.3589(23.5135) | Bit/dim 3.4958(3.5010) | Xent 0.0060(0.0041) | Loss 3.4988(3.5031) | Error 0.0011(0.0010) Steps 934(957.37) | Grad Norm 0.7868(0.7308) | Total Time 14.00(14.00)\n",
      "Iter 16920 | Time 23.9485(23.5174) | Bit/dim 3.4933(3.5007) | Xent 0.0014(0.0039) | Loss 3.4940(3.5026) | Error 0.0000(0.0011) Steps 976(957.73) | Grad Norm 0.3637(0.7547) | Total Time 14.00(14.00)\n",
      "Iter 16930 | Time 23.5692(23.5054) | Bit/dim 3.4987(3.5030) | Xent 0.0019(0.0044) | Loss 3.4996(3.5052) | Error 0.0011(0.0012) Steps 946(956.89) | Grad Norm 0.7042(0.7966) | Total Time 14.00(14.00)\n",
      "Iter 16940 | Time 23.3338(23.5452) | Bit/dim 3.5422(3.5042) | Xent 0.0047(0.0046) | Loss 3.5446(3.5065) | Error 0.0022(0.0012) Steps 928(956.46) | Grad Norm 1.0446(0.7781) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0308 | Time 111.5230, Epoch Time 1425.4279(1422.9167), Bit/dim 3.5354(best: 3.5351), Xent 2.6599, Loss 4.8653, Error 0.3458(best: 0.3425)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16950 | Time 23.1333(23.5410) | Bit/dim 3.5242(3.5035) | Xent 0.0013(0.0049) | Loss 3.5248(3.5059) | Error 0.0000(0.0012) Steps 946(958.42) | Grad Norm 0.4280(0.7711) | Total Time 14.00(14.00)\n",
      "Iter 16960 | Time 24.1399(23.5514) | Bit/dim 3.4629(3.5023) | Xent 0.0030(0.0050) | Loss 3.4644(3.5048) | Error 0.0000(0.0012) Steps 928(957.78) | Grad Norm 0.7716(0.8185) | Total Time 14.00(14.00)\n",
      "Iter 16970 | Time 23.1123(23.5029) | Bit/dim 3.5168(3.5016) | Xent 0.0014(0.0048) | Loss 3.5175(3.5040) | Error 0.0000(0.0012) Steps 952(957.81) | Grad Norm 0.5269(0.8066) | Total Time 14.00(14.00)\n",
      "Iter 16980 | Time 23.2671(23.4378) | Bit/dim 3.4958(3.5024) | Xent 0.0041(0.0048) | Loss 3.4979(3.5048) | Error 0.0022(0.0013) Steps 976(956.33) | Grad Norm 0.6770(0.8211) | Total Time 14.00(14.00)\n",
      "Iter 16990 | Time 23.3253(23.5003) | Bit/dim 3.4878(3.5037) | Xent 0.0035(0.0045) | Loss 3.4896(3.5060) | Error 0.0011(0.0013) Steps 940(955.56) | Grad Norm 0.5428(0.8046) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0309 | Time 111.2308, Epoch Time 1420.0072(1422.8294), Bit/dim 3.5361(best: 3.5351), Xent 2.6519, Loss 4.8621, Error 0.3480(best: 0.3425)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17000 | Time 23.0837(23.3975) | Bit/dim 3.5097(3.5051) | Xent 0.0023(0.0043) | Loss 3.5108(3.5073) | Error 0.0011(0.0012) Steps 934(953.74) | Grad Norm 0.5475(0.7308) | Total Time 14.00(14.00)\n",
      "Iter 17010 | Time 24.4038(23.4149) | Bit/dim 3.4960(3.5077) | Xent 0.0015(0.0043) | Loss 3.4968(3.5098) | Error 0.0000(0.0011) Steps 964(953.30) | Grad Norm 0.6408(0.7413) | Total Time 14.00(14.00)\n",
      "Iter 17020 | Time 23.5233(23.4691) | Bit/dim 3.5032(3.5050) | Xent 0.0015(0.0039) | Loss 3.5040(3.5069) | Error 0.0000(0.0009) Steps 964(954.05) | Grad Norm 0.2768(0.6492) | Total Time 14.00(14.00)\n",
      "Iter 17030 | Time 23.8263(23.5177) | Bit/dim 3.5232(3.5053) | Xent 0.0055(0.0035) | Loss 3.5260(3.5071) | Error 0.0022(0.0008) Steps 946(953.55) | Grad Norm 1.0857(0.6168) | Total Time 14.00(14.00)\n",
      "Iter 17040 | Time 23.4020(23.4904) | Bit/dim 3.5168(3.5044) | Xent 0.0036(0.0035) | Loss 3.5186(3.5061) | Error 0.0011(0.0009) Steps 952(954.57) | Grad Norm 0.9658(0.6548) | Total Time 14.00(14.00)\n",
      "Iter 17050 | Time 22.8444(23.5517) | Bit/dim 3.5047(3.5017) | Xent 0.0018(0.0032) | Loss 3.5056(3.5033) | Error 0.0011(0.0009) Steps 928(953.63) | Grad Norm 0.4584(0.6171) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0310 | Time 112.1869, Epoch Time 1423.4571(1422.8482), Bit/dim 3.5335(best: 3.5351), Xent 2.6842, Loss 4.8756, Error 0.3495(best: 0.3425)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17060 | Time 22.4171(23.4118) | Bit/dim 3.5043(3.5028) | Xent 0.0045(0.0040) | Loss 3.5065(3.5048) | Error 0.0033(0.0011) Steps 940(951.21) | Grad Norm 1.2804(0.7607) | Total Time 14.00(14.00)\n",
      "Iter 17070 | Time 23.3827(23.4292) | Bit/dim 3.5082(3.5054) | Xent 0.0016(0.0040) | Loss 3.5089(3.5074) | Error 0.0000(0.0012) Steps 958(950.80) | Grad Norm 0.5340(0.7983) | Total Time 14.00(14.00)\n",
      "Iter 17080 | Time 23.7644(23.4837) | Bit/dim 3.5015(3.5030) | Xent 0.0047(0.0038) | Loss 3.5039(3.5049) | Error 0.0022(0.0011) Steps 940(950.38) | Grad Norm 0.8142(0.7601) | Total Time 14.00(14.00)\n",
      "Iter 17090 | Time 24.1106(23.4350) | Bit/dim 3.4920(3.5047) | Xent 0.0250(0.0047) | Loss 3.5045(3.5071) | Error 0.0033(0.0012) Steps 946(948.87) | Grad Norm 0.9599(0.7920) | Total Time 14.00(14.00)\n",
      "Iter 17100 | Time 23.3194(23.3444) | Bit/dim 3.5207(3.5011) | Xent 0.0063(0.0053) | Loss 3.5239(3.5037) | Error 0.0022(0.0014) Steps 964(949.20) | Grad Norm 0.8456(0.8483) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0311 | Time 113.6010, Epoch Time 1414.6525(1422.6024), Bit/dim 3.5364(best: 3.5335), Xent 2.6483, Loss 4.8606, Error 0.3484(best: 0.3425)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17110 | Time 22.9572(23.3826) | Bit/dim 3.5080(3.5049) | Xent 0.0016(0.0049) | Loss 3.5088(3.5074) | Error 0.0000(0.0013) Steps 970(953.16) | Grad Norm 0.4676(0.8145) | Total Time 14.00(14.00)\n",
      "Iter 17120 | Time 23.7562(23.4377) | Bit/dim 3.5215(3.5068) | Xent 0.0006(0.0045) | Loss 3.5218(3.5090) | Error 0.0000(0.0012) Steps 952(954.85) | Grad Norm 0.3102(0.7586) | Total Time 14.00(14.00)\n",
      "Iter 17130 | Time 23.3862(23.4714) | Bit/dim 3.4754(3.5050) | Xent 0.0054(0.0041) | Loss 3.4781(3.5070) | Error 0.0022(0.0011) Steps 946(955.34) | Grad Norm 0.8566(0.7229) | Total Time 14.00(14.00)\n",
      "Iter 17140 | Time 23.6762(23.5780) | Bit/dim 3.4992(3.5019) | Xent 0.0009(0.0036) | Loss 3.4996(3.5038) | Error 0.0000(0.0010) Steps 946(958.96) | Grad Norm 0.3110(0.6754) | Total Time 14.00(14.00)\n",
      "Iter 17150 | Time 24.4495(23.6267) | Bit/dim 3.5139(3.5037) | Xent 0.0006(0.0034) | Loss 3.5142(3.5053) | Error 0.0000(0.0009) Steps 964(959.17) | Grad Norm 0.2679(0.6460) | Total Time 14.00(14.00)\n",
      "Iter 17160 | Time 23.0760(23.6635) | Bit/dim 3.5202(3.5006) | Xent 0.0163(0.0037) | Loss 3.5284(3.5025) | Error 0.0022(0.0009) Steps 934(960.70) | Grad Norm 0.4702(0.6674) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0312 | Time 113.0390, Epoch Time 1433.4487(1422.9278), Bit/dim 3.5351(best: 3.5335), Xent 2.6826, Loss 4.8764, Error 0.3482(best: 0.3425)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17170 | Time 23.3005(23.5446) | Bit/dim 3.5249(3.5003) | Xent 0.0030(0.0045) | Loss 3.5264(3.5026) | Error 0.0011(0.0010) Steps 964(959.60) | Grad Norm 0.9075(0.7169) | Total Time 14.00(14.00)\n",
      "Iter 17180 | Time 24.0911(23.5041) | Bit/dim 3.4888(3.5019) | Xent 0.0014(0.0050) | Loss 3.4895(3.5044) | Error 0.0000(0.0011) Steps 982(957.91) | Grad Norm 0.5522(0.7563) | Total Time 14.00(14.00)\n",
      "Iter 17190 | Time 23.3017(23.4855) | Bit/dim 3.5263(3.5028) | Xent 0.0017(0.0053) | Loss 3.5271(3.5055) | Error 0.0000(0.0013) Steps 946(957.17) | Grad Norm 0.4445(0.7996) | Total Time 14.00(14.00)\n",
      "Iter 17200 | Time 23.6394(23.5034) | Bit/dim 3.5293(3.5026) | Xent 0.0082(0.0055) | Loss 3.5334(3.5054) | Error 0.0022(0.0013) Steps 916(953.54) | Grad Norm 1.0524(0.8197) | Total Time 14.00(14.00)\n",
      "Iter 17210 | Time 23.5290(23.4922) | Bit/dim 3.4881(3.5002) | Xent 0.0024(0.0050) | Loss 3.4893(3.5027) | Error 0.0011(0.0012) Steps 958(954.08) | Grad Norm 0.5003(0.7786) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0313 | Time 111.9915, Epoch Time 1419.3891(1422.8216), Bit/dim 3.5350(best: 3.5335), Xent 2.6312, Loss 4.8505, Error 0.3474(best: 0.3425)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17220 | Time 22.7144(23.5021) | Bit/dim 3.4837(3.5022) | Xent 0.0060(0.0048) | Loss 3.4867(3.5046) | Error 0.0022(0.0013) Steps 988(955.52) | Grad Norm 1.1111(0.8015) | Total Time 14.00(14.00)\n",
      "Iter 17230 | Time 23.2887(23.4821) | Bit/dim 3.4759(3.5026) | Xent 0.0014(0.0044) | Loss 3.4766(3.5048) | Error 0.0000(0.0012) Steps 982(955.65) | Grad Norm 0.5597(0.8317) | Total Time 14.00(14.00)\n",
      "Iter 17240 | Time 23.3603(23.5036) | Bit/dim 3.5132(3.5021) | Xent 0.0064(0.0046) | Loss 3.5164(3.5044) | Error 0.0011(0.0011) Steps 958(954.97) | Grad Norm 1.2476(0.8031) | Total Time 14.00(14.00)\n",
      "Iter 17250 | Time 23.0643(23.4122) | Bit/dim 3.5305(3.5037) | Xent 0.0086(0.0044) | Loss 3.5349(3.5059) | Error 0.0022(0.0012) Steps 928(953.56) | Grad Norm 0.9773(0.8685) | Total Time 14.00(14.00)\n",
      "Iter 17260 | Time 23.1951(23.3997) | Bit/dim 3.5156(3.5035) | Xent 0.0076(0.0045) | Loss 3.5194(3.5057) | Error 0.0033(0.0013) Steps 970(954.58) | Grad Norm 1.2623(0.9011) | Total Time 14.00(14.00)\n",
      "Iter 17270 | Time 23.8698(23.3086) | Bit/dim 3.4996(3.5035) | Xent 0.0049(0.0044) | Loss 3.5020(3.5058) | Error 0.0022(0.0013) Steps 934(953.65) | Grad Norm 1.2245(0.8843) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0314 | Time 110.8944, Epoch Time 1411.2222(1422.4736), Bit/dim 3.5350(best: 3.5335), Xent 2.6218, Loss 4.8459, Error 0.3417(best: 0.3425)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17280 | Time 23.9669(23.3943) | Bit/dim 3.5268(3.5040) | Xent 0.0009(0.0047) | Loss 3.5273(3.5063) | Error 0.0000(0.0012) Steps 988(954.96) | Grad Norm 0.3648(0.8775) | Total Time 14.00(14.00)\n",
      "Iter 17290 | Time 23.4464(23.4221) | Bit/dim 3.5081(3.5041) | Xent 0.0096(0.0043) | Loss 3.5129(3.5063) | Error 0.0022(0.0011) Steps 940(955.02) | Grad Norm 1.0406(0.8264) | Total Time 14.00(14.00)\n",
      "Iter 17300 | Time 24.0721(23.3707) | Bit/dim 3.4848(3.5031) | Xent 0.0014(0.0041) | Loss 3.4855(3.5051) | Error 0.0000(0.0011) Steps 964(954.27) | Grad Norm 0.4872(0.8482) | Total Time 14.00(14.00)\n",
      "Iter 17310 | Time 24.0849(23.4102) | Bit/dim 3.4975(3.5038) | Xent 0.0021(0.0039) | Loss 3.4985(3.5057) | Error 0.0011(0.0010) Steps 982(955.98) | Grad Norm 0.6600(0.8347) | Total Time 14.00(14.00)\n",
      "Iter 17320 | Time 23.1975(23.3667) | Bit/dim 3.4626(3.5031) | Xent 0.0012(0.0040) | Loss 3.4632(3.5051) | Error 0.0000(0.0011) Steps 958(953.85) | Grad Norm 0.4663(0.8711) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0315 | Time 112.0753, Epoch Time 1419.8559(1422.3951), Bit/dim 3.5349(best: 3.5335), Xent 2.6298, Loss 4.8498, Error 0.3442(best: 0.3417)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17330 | Time 22.9614(23.4351) | Bit/dim 3.5059(3.5017) | Xent 0.0007(0.0039) | Loss 3.5062(3.5036) | Error 0.0000(0.0010) Steps 970(952.49) | Grad Norm 0.3386(0.8392) | Total Time 14.00(14.00)\n",
      "Iter 17340 | Time 23.5777(23.4004) | Bit/dim 3.4353(3.5006) | Xent 0.0083(0.0042) | Loss 3.4394(3.5027) | Error 0.0011(0.0011) Steps 928(949.19) | Grad Norm 0.3529(0.8065) | Total Time 14.00(14.00)\n",
      "Iter 17350 | Time 22.9088(23.3884) | Bit/dim 3.5297(3.5036) | Xent 0.0025(0.0038) | Loss 3.5309(3.5055) | Error 0.0000(0.0010) Steps 940(948.33) | Grad Norm 0.7555(0.7601) | Total Time 14.00(14.00)\n",
      "Iter 17360 | Time 23.5564(23.3784) | Bit/dim 3.4846(3.4995) | Xent 0.0117(0.0042) | Loss 3.4905(3.5016) | Error 0.0022(0.0011) Steps 952(947.70) | Grad Norm 1.2415(0.8351) | Total Time 14.00(14.00)\n",
      "Iter 17370 | Time 22.8653(23.2890) | Bit/dim 3.5192(3.5035) | Xent 0.0007(0.0047) | Loss 3.5195(3.5059) | Error 0.0000(0.0013) Steps 952(948.42) | Grad Norm 0.4643(0.8485) | Total Time 14.00(14.00)\n",
      "Iter 17380 | Time 22.5209(23.2389) | Bit/dim 3.4774(3.5029) | Xent 0.0040(0.0047) | Loss 3.4794(3.5053) | Error 0.0022(0.0013) Steps 940(945.26) | Grad Norm 0.4610(0.8021) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0316 | Time 113.2214, Epoch Time 1410.9606(1422.0520), Bit/dim 3.5335(best: 3.5335), Xent 2.6570, Loss 4.8620, Error 0.3484(best: 0.3417)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17390 | Time 23.7569(23.3378) | Bit/dim 3.5078(3.5045) | Xent 0.0022(0.0044) | Loss 3.5089(3.5066) | Error 0.0011(0.0012) Steps 946(945.18) | Grad Norm 0.9555(0.7363) | Total Time 14.00(14.00)\n",
      "Iter 17400 | Time 23.1515(23.3085) | Bit/dim 3.4989(3.5029) | Xent 0.0074(0.0046) | Loss 3.5026(3.5052) | Error 0.0011(0.0012) Steps 922(945.87) | Grad Norm 0.7514(0.8411) | Total Time 14.00(14.00)\n",
      "Iter 17410 | Time 23.0295(23.3197) | Bit/dim 3.5172(3.5022) | Xent 0.0011(0.0043) | Loss 3.5178(3.5043) | Error 0.0000(0.0012) Steps 952(944.12) | Grad Norm 0.3562(0.8279) | Total Time 14.00(14.00)\n",
      "Iter 17420 | Time 23.5936(23.3327) | Bit/dim 3.5260(3.5043) | Xent 0.0020(0.0044) | Loss 3.5270(3.5065) | Error 0.0011(0.0012) Steps 976(946.77) | Grad Norm 0.4457(0.7808) | Total Time 14.00(14.00)\n",
      "Iter 17430 | Time 23.7544(23.3239) | Bit/dim 3.4933(3.5055) | Xent 0.0010(0.0041) | Loss 3.4938(3.5075) | Error 0.0000(0.0012) Steps 952(946.83) | Grad Norm 0.2863(0.7122) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0317 | Time 113.5549, Epoch Time 1415.9598(1421.8693), Bit/dim 3.5328(best: 3.5335), Xent 2.6497, Loss 4.8576, Error 0.3451(best: 0.3417)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17440 | Time 22.9589(23.3339) | Bit/dim 3.5065(3.5034) | Xent 0.0016(0.0038) | Loss 3.5073(3.5053) | Error 0.0000(0.0011) Steps 946(949.88) | Grad Norm 0.5380(0.6445) | Total Time 14.00(14.00)\n",
      "Iter 17450 | Time 23.0303(23.3098) | Bit/dim 3.5035(3.5026) | Xent 0.0061(0.0038) | Loss 3.5066(3.5045) | Error 0.0022(0.0010) Steps 946(951.12) | Grad Norm 1.4695(0.6547) | Total Time 14.00(14.00)\n",
      "Iter 17460 | Time 22.9652(23.2466) | Bit/dim 3.5149(3.5031) | Xent 0.0005(0.0045) | Loss 3.5151(3.5054) | Error 0.0000(0.0011) Steps 952(949.39) | Grad Norm 0.4188(0.7035) | Total Time 14.00(14.00)\n",
      "Iter 17470 | Time 23.3794(23.2998) | Bit/dim 3.5181(3.5044) | Xent 0.0011(0.0048) | Loss 3.5186(3.5068) | Error 0.0000(0.0012) Steps 976(950.86) | Grad Norm 0.3250(0.6970) | Total Time 14.00(14.00)\n",
      "Iter 17480 | Time 23.3505(23.3118) | Bit/dim 3.4696(3.5042) | Xent 0.0072(0.0044) | Loss 3.4731(3.5064) | Error 0.0022(0.0012) Steps 946(953.88) | Grad Norm 1.3092(0.7173) | Total Time 14.00(14.00)\n",
      "Iter 17490 | Time 23.3941(23.3286) | Bit/dim 3.4977(3.5004) | Xent 0.0034(0.0043) | Loss 3.4994(3.5025) | Error 0.0011(0.0012) Steps 946(950.09) | Grad Norm 1.0595(0.7183) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0318 | Time 112.5564, Epoch Time 1412.5449(1421.5895), Bit/dim 3.5327(best: 3.5328), Xent 2.6589, Loss 4.8622, Error 0.3481(best: 0.3417)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17500 | Time 23.2646(23.3308) | Bit/dim 3.4892(3.4999) | Xent 0.0042(0.0042) | Loss 3.4913(3.5020) | Error 0.0011(0.0013) Steps 952(950.30) | Grad Norm 0.6167(0.7096) | Total Time 14.00(14.00)\n",
      "Iter 17510 | Time 23.0067(23.2680) | Bit/dim 3.5016(3.5013) | Xent 0.0042(0.0039) | Loss 3.5037(3.5033) | Error 0.0011(0.0011) Steps 952(948.79) | Grad Norm 0.6035(0.6402) | Total Time 14.00(14.00)\n",
      "Iter 17520 | Time 24.3557(23.3051) | Bit/dim 3.5006(3.5009) | Xent 0.0007(0.0040) | Loss 3.5010(3.5029) | Error 0.0000(0.0011) Steps 940(947.49) | Grad Norm 0.3660(0.6639) | Total Time 14.00(14.00)\n",
      "Iter 17530 | Time 24.3710(23.3926) | Bit/dim 3.4960(3.5023) | Xent 0.0010(0.0033) | Loss 3.4965(3.5040) | Error 0.0000(0.0009) Steps 952(948.66) | Grad Norm 0.3615(0.5954) | Total Time 14.00(14.00)\n",
      "Iter 17540 | Time 23.4794(23.4136) | Bit/dim 3.4946(3.5018) | Xent 0.0075(0.0035) | Loss 3.4983(3.5035) | Error 0.0033(0.0010) Steps 946(947.55) | Grad Norm 1.3225(0.6569) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0319 | Time 110.4553, Epoch Time 1413.5608(1421.3487), Bit/dim 3.5333(best: 3.5327), Xent 2.6742, Loss 4.8704, Error 0.3440(best: 0.3417)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17550 | Time 23.7731(23.3985) | Bit/dim 3.4812(3.4977) | Xent 0.0115(0.0036) | Loss 3.4869(3.4995) | Error 0.0022(0.0010) Steps 964(949.73) | Grad Norm 1.9389(0.6828) | Total Time 14.00(14.00)\n",
      "Iter 17560 | Time 23.3962(23.3602) | Bit/dim 3.5339(3.5010) | Xent 0.0011(0.0035) | Loss 3.5344(3.5027) | Error 0.0000(0.0009) Steps 928(947.98) | Grad Norm 0.4069(0.6844) | Total Time 14.00(14.00)\n",
      "Iter 17570 | Time 22.7963(23.2807) | Bit/dim 3.4960(3.5016) | Xent 0.0006(0.0039) | Loss 3.4963(3.5035) | Error 0.0000(0.0011) Steps 928(946.47) | Grad Norm 0.4396(0.7242) | Total Time 14.00(14.00)\n",
      "Iter 17580 | Time 23.1127(23.2636) | Bit/dim 3.5286(3.5016) | Xent 0.0058(0.0037) | Loss 3.5315(3.5034) | Error 0.0011(0.0011) Steps 934(945.64) | Grad Norm 0.9244(0.7065) | Total Time 14.00(14.00)\n",
      "Iter 17590 | Time 22.6193(23.2242) | Bit/dim 3.5134(3.5017) | Xent 0.0057(0.0044) | Loss 3.5163(3.5039) | Error 0.0022(0.0013) Steps 946(947.70) | Grad Norm 1.2047(0.8278) | Total Time 14.00(14.00)\n",
      "Iter 17600 | Time 23.2974(23.2221) | Bit/dim 3.5080(3.5006) | Xent 0.0010(0.0038) | Loss 3.5085(3.5025) | Error 0.0000(0.0011) Steps 964(949.62) | Grad Norm 0.4166(0.7730) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0320 | Time 110.6083, Epoch Time 1405.2750(1420.8665), Bit/dim 3.5328(best: 3.5327), Xent 2.7051, Loss 4.8853, Error 0.3454(best: 0.3417)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17610 | Time 23.1267(23.2008) | Bit/dim 3.5356(3.5028) | Xent 0.0019(0.0036) | Loss 3.5366(3.5046) | Error 0.0011(0.0010) Steps 952(948.73) | Grad Norm 0.4962(0.7364) | Total Time 14.00(14.00)\n",
      "Iter 17620 | Time 23.5845(23.1874) | Bit/dim 3.4556(3.5004) | Xent 0.0028(0.0034) | Loss 3.4570(3.5021) | Error 0.0022(0.0011) Steps 946(947.49) | Grad Norm 1.4486(0.7161) | Total Time 14.00(14.00)\n",
      "Iter 17630 | Time 23.5136(23.2057) | Bit/dim 3.5228(3.5010) | Xent 0.0010(0.0040) | Loss 3.5233(3.5030) | Error 0.0000(0.0011) Steps 940(946.45) | Grad Norm 0.5599(0.7920) | Total Time 14.00(14.00)\n",
      "Iter 17640 | Time 23.4213(23.2495) | Bit/dim 3.5260(3.5022) | Xent 0.0014(0.0041) | Loss 3.5267(3.5042) | Error 0.0000(0.0012) Steps 952(947.17) | Grad Norm 0.3559(0.8109) | Total Time 14.00(14.00)\n",
      "Iter 17650 | Time 23.3023(23.3103) | Bit/dim 3.4962(3.5011) | Xent 0.0011(0.0039) | Loss 3.4968(3.5030) | Error 0.0000(0.0011) Steps 958(946.46) | Grad Norm 0.4036(0.7685) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0321 | Time 112.5892, Epoch Time 1410.5006(1420.5555), Bit/dim 3.5331(best: 3.5327), Xent 2.7138, Loss 4.8900, Error 0.3544(best: 0.3417)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17660 | Time 23.0116(23.2650) | Bit/dim 3.4978(3.5010) | Xent 0.0013(0.0035) | Loss 3.4984(3.5028) | Error 0.0000(0.0010) Steps 934(946.10) | Grad Norm 0.3143(0.6967) | Total Time 14.00(14.00)\n",
      "Iter 17670 | Time 22.8532(23.2980) | Bit/dim 3.4932(3.4991) | Xent 0.0010(0.0038) | Loss 3.4937(3.5010) | Error 0.0000(0.0011) Steps 922(947.06) | Grad Norm 0.5177(0.7388) | Total Time 14.00(14.00)\n",
      "Iter 17680 | Time 23.2515(23.2851) | Bit/dim 3.5073(3.5020) | Xent 0.0037(0.0046) | Loss 3.5092(3.5044) | Error 0.0011(0.0012) Steps 952(947.78) | Grad Norm 0.5181(0.8039) | Total Time 14.00(14.00)\n",
      "Iter 17690 | Time 23.8598(23.3537) | Bit/dim 3.5031(3.5021) | Xent 0.0111(0.0052) | Loss 3.5087(3.5047) | Error 0.0022(0.0012) Steps 964(949.67) | Grad Norm 0.7712(0.7865) | Total Time 14.00(14.00)\n",
      "Iter 17700 | Time 23.6279(23.4232) | Bit/dim 3.5240(3.5022) | Xent 0.0007(0.0046) | Loss 3.5243(3.5044) | Error 0.0000(0.0012) Steps 940(947.88) | Grad Norm 0.3244(0.7531) | Total Time 14.00(14.00)\n",
      "Iter 17710 | Time 23.9126(23.4197) | Bit/dim 3.4943(3.5006) | Xent 0.0040(0.0044) | Loss 3.4963(3.5028) | Error 0.0011(0.0012) Steps 988(947.03) | Grad Norm 0.7082(0.7487) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0322 | Time 112.4883, Epoch Time 1417.7571(1420.4715), Bit/dim 3.5322(best: 3.5327), Xent 2.6810, Loss 4.8727, Error 0.3486(best: 0.3417)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17720 | Time 22.0132(23.4174) | Bit/dim 3.4989(3.4995) | Xent 0.0034(0.0048) | Loss 3.5006(3.5019) | Error 0.0022(0.0013) Steps 928(947.11) | Grad Norm 0.4608(0.7848) | Total Time 14.00(14.00)\n",
      "Iter 17730 | Time 23.0229(23.3953) | Bit/dim 3.5261(3.5002) | Xent 0.0077(0.0047) | Loss 3.5299(3.5026) | Error 0.0022(0.0012) Steps 952(946.21) | Grad Norm 0.6172(0.7239) | Total Time 14.00(14.00)\n",
      "Iter 17740 | Time 22.7734(23.3797) | Bit/dim 3.5001(3.5010) | Xent 0.0070(0.0047) | Loss 3.5036(3.5034) | Error 0.0022(0.0012) Steps 952(947.87) | Grad Norm 0.8568(0.7196) | Total Time 14.00(14.00)\n",
      "Iter 17750 | Time 24.1786(23.3449) | Bit/dim 3.5059(3.4995) | Xent 0.0031(0.0040) | Loss 3.5074(3.5015) | Error 0.0011(0.0011) Steps 952(948.43) | Grad Norm 0.5423(0.6659) | Total Time 14.00(14.00)\n",
      "Iter 17760 | Time 23.2737(23.3095) | Bit/dim 3.5244(3.5010) | Xent 0.0019(0.0039) | Loss 3.5253(3.5029) | Error 0.0011(0.0010) Steps 940(949.46) | Grad Norm 0.8502(0.6859) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0323 | Time 112.6063, Epoch Time 1412.8710(1420.2435), Bit/dim 3.5327(best: 3.5322), Xent 2.6665, Loss 4.8660, Error 0.3456(best: 0.3417)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17770 | Time 22.9826(23.3625) | Bit/dim 3.4745(3.5005) | Xent 0.0004(0.0037) | Loss 3.4747(3.5024) | Error 0.0000(0.0010) Steps 970(950.40) | Grad Norm 0.5251(0.7525) | Total Time 14.00(14.00)\n",
      "Iter 17780 | Time 23.8233(23.4734) | Bit/dim 3.5032(3.5007) | Xent 0.0016(0.0033) | Loss 3.5041(3.5023) | Error 0.0000(0.0009) Steps 910(949.87) | Grad Norm 0.8682(0.7393) | Total Time 14.00(14.00)\n",
      "Iter 17790 | Time 24.1228(23.4883) | Bit/dim 3.5306(3.5011) | Xent 0.0013(0.0036) | Loss 3.5312(3.5029) | Error 0.0000(0.0009) Steps 940(949.20) | Grad Norm 0.3739(0.7539) | Total Time 14.00(14.00)\n",
      "Iter 17800 | Time 23.4234(23.4951) | Bit/dim 3.5231(3.5017) | Xent 0.0007(0.0034) | Loss 3.5234(3.5034) | Error 0.0000(0.0008) Steps 940(952.04) | Grad Norm 0.3562(0.7193) | Total Time 14.00(14.00)\n",
      "Iter 17810 | Time 23.7571(23.4885) | Bit/dim 3.5132(3.5000) | Xent 0.0031(0.0036) | Loss 3.5148(3.5018) | Error 0.0011(0.0009) Steps 964(953.15) | Grad Norm 1.1313(0.7448) | Total Time 14.00(14.00)\n",
      "Iter 17820 | Time 23.4957(23.5213) | Bit/dim 3.5137(3.5013) | Xent 0.0110(0.0040) | Loss 3.5192(3.5033) | Error 0.0011(0.0009) Steps 958(952.11) | Grad Norm 0.5239(0.7207) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0324 | Time 111.1770, Epoch Time 1426.1754(1420.4215), Bit/dim 3.5332(best: 3.5322), Xent 2.7155, Loss 4.8910, Error 0.3499(best: 0.3417)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17830 | Time 23.8621(23.5560) | Bit/dim 3.4926(3.4977) | Xent 0.0011(0.0038) | Loss 3.4932(3.4996) | Error 0.0000(0.0008) Steps 964(953.66) | Grad Norm 0.5158(0.6799) | Total Time 14.00(14.00)\n",
      "Iter 17840 | Time 23.2941(23.4975) | Bit/dim 3.5378(3.4984) | Xent 0.0090(0.0040) | Loss 3.5423(3.5004) | Error 0.0033(0.0010) Steps 946(954.95) | Grad Norm 1.2796(0.7797) | Total Time 14.00(14.00)\n",
      "Iter 17850 | Time 23.0002(23.4962) | Bit/dim 3.4720(3.4999) | Xent 0.0069(0.0039) | Loss 3.4754(3.5019) | Error 0.0011(0.0009) Steps 958(954.47) | Grad Norm 0.9666(0.7736) | Total Time 14.00(14.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_drop_cifar.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/cnf_conditional_cifar10_bs900_drop_0_5_run1 --resume ../experiments_published/cnf_conditional_cifar10_bs900_drop_0_5_run1/current_checkpt.pth --seed 1 --lr 0.0001 --conditional True --controlled_tol False --train_mode semisup --log_freq 10 --weight_y 0.5 --dropout_rate 0.5\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
