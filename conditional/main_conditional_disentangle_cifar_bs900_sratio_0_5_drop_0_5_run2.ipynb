{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0,1,2,3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_cifar.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "if args.controlled_tol:\n",
      "    import lib.odenvp_conditional_tol as odenvp\n",
      "else:\n",
      "    import lib.odenvp_conditional as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            loss.backward()\n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, condition_ratio=0.5, conditional=True, controlled_tol=False, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.0001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume='../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_run2/epoch_250_checkpt.pth', rtol=1e-05, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_run2', seed=2, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1414198\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc.py:286: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "Iter 13760 | Time 27.9716(28.3870) | Bit/dim 3.5944(3.5735) | Xent 0.1461(0.1789) | Loss 3.6674(3.6630) | Error 0.0544(0.0633) Steps 1114(1128.54) | Grad Norm 2.0259(3.1375) | Total Time 14.00(14.00)\n",
      "Iter 13770 | Time 26.9264(28.1307) | Bit/dim 3.5903(3.5708) | Xent 0.1146(0.1673) | Loss 3.6476(3.6545) | Error 0.0356(0.0586) Steps 1132(1127.04) | Grad Norm 1.6696(2.7049) | Total Time 14.00(14.00)\n",
      "Iter 13780 | Time 27.9454(27.9928) | Bit/dim 3.5507(3.5663) | Xent 0.1303(0.1560) | Loss 3.6158(3.6443) | Error 0.0489(0.0548) Steps 1120(1127.14) | Grad Norm 1.4781(2.3829) | Total Time 14.00(14.00)\n",
      "Iter 13790 | Time 27.5451(27.8572) | Bit/dim 3.5333(3.5644) | Xent 0.1096(0.1465) | Loss 3.5881(3.6377) | Error 0.0389(0.0517) Steps 1120(1128.64) | Grad Norm 1.6749(2.1209) | Total Time 14.00(14.00)\n",
      "Iter 13800 | Time 26.5540(27.7789) | Bit/dim 3.5587(3.5616) | Xent 0.1291(0.1402) | Loss 3.6232(3.6317) | Error 0.0467(0.0492) Steps 1120(1128.39) | Grad Norm 1.2972(1.8953) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0251 | Time 141.9263, Epoch Time 1693.1604(1600.4496), Bit/dim 3.5654(best: inf), Xent 0.9228, Loss 4.0268, Error 0.2211(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13810 | Time 26.6732(27.7605) | Bit/dim 3.5586(3.5614) | Xent 0.1308(0.1338) | Loss 3.6240(3.6283) | Error 0.0478(0.0467) Steps 1120(1129.01) | Grad Norm 1.2192(1.7201) | Total Time 14.00(14.00)\n",
      "Iter 13820 | Time 27.2355(27.7275) | Bit/dim 3.5899(3.5589) | Xent 0.1214(0.1317) | Loss 3.6506(3.6247) | Error 0.0500(0.0463) Steps 1144(1129.25) | Grad Norm 2.0225(1.6841) | Total Time 14.00(14.00)\n",
      "Iter 13830 | Time 28.0221(27.6854) | Bit/dim 3.5641(3.5570) | Xent 0.1158(0.1280) | Loss 3.6220(3.6210) | Error 0.0456(0.0449) Steps 1114(1126.58) | Grad Norm 1.1657(1.6136) | Total Time 14.00(14.00)\n",
      "Iter 13840 | Time 26.9907(27.6388) | Bit/dim 3.5806(3.5571) | Xent 0.0988(0.1225) | Loss 3.6300(3.6183) | Error 0.0300(0.0427) Steps 1126(1128.45) | Grad Norm 1.6688(1.5422) | Total Time 14.00(14.00)\n",
      "Iter 13850 | Time 26.7221(27.5880) | Bit/dim 3.5304(3.5559) | Xent 0.1198(0.1221) | Loss 3.5904(3.6169) | Error 0.0456(0.0422) Steps 1102(1125.68) | Grad Norm 1.3330(1.4863) | Total Time 14.00(14.00)\n",
      "Iter 13860 | Time 27.6960(27.5847) | Bit/dim 3.5380(3.5550) | Xent 0.1367(0.1216) | Loss 3.6064(3.6158) | Error 0.0467(0.0417) Steps 1132(1126.07) | Grad Norm 1.7753(1.4603) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0252 | Time 129.0670, Epoch Time 1664.0233(1602.3568), Bit/dim 3.5618(best: 3.5654), Xent 0.9339, Loss 4.0288, Error 0.2216(best: 0.2211)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13870 | Time 26.5832(27.5073) | Bit/dim 3.5678(3.5549) | Xent 0.1299(0.1194) | Loss 3.6328(3.6146) | Error 0.0467(0.0410) Steps 1132(1127.25) | Grad Norm 1.2273(1.4404) | Total Time 14.00(14.00)\n",
      "Iter 13880 | Time 27.2408(27.5227) | Bit/dim 3.5617(3.5517) | Xent 0.1081(0.1174) | Loss 3.6157(3.6104) | Error 0.0311(0.0397) Steps 1120(1126.60) | Grad Norm 1.1212(1.3883) | Total Time 14.00(14.00)\n",
      "Iter 13890 | Time 27.7725(27.4993) | Bit/dim 3.5565(3.5523) | Xent 0.1024(0.1160) | Loss 3.6077(3.6102) | Error 0.0300(0.0397) Steps 1144(1126.11) | Grad Norm 1.0889(1.3385) | Total Time 14.00(14.00)\n",
      "Iter 13900 | Time 27.4007(27.4507) | Bit/dim 3.5686(3.5531) | Xent 0.0975(0.1152) | Loss 3.6173(3.6107) | Error 0.0344(0.0392) Steps 1138(1124.54) | Grad Norm 1.1460(1.3265) | Total Time 14.00(14.00)\n",
      "Iter 13910 | Time 28.0398(27.5267) | Bit/dim 3.5479(3.5538) | Xent 0.1142(0.1137) | Loss 3.6050(3.6107) | Error 0.0322(0.0390) Steps 1126(1124.41) | Grad Norm 1.4189(1.3226) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0253 | Time 128.3833, Epoch Time 1658.1065(1604.0293), Bit/dim 3.5596(best: 3.5618), Xent 0.9503, Loss 4.0348, Error 0.2226(best: 0.2211)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13920 | Time 27.4815(27.5325) | Bit/dim 3.5474(3.5533) | Xent 0.1393(0.1151) | Loss 3.6171(3.6109) | Error 0.0544(0.0395) Steps 1108(1122.74) | Grad Norm 1.2801(1.3477) | Total Time 14.00(14.00)\n",
      "Iter 13930 | Time 27.9986(27.5795) | Bit/dim 3.5488(3.5528) | Xent 0.0803(0.1139) | Loss 3.5890(3.6097) | Error 0.0289(0.0393) Steps 1108(1122.18) | Grad Norm 1.0801(1.3735) | Total Time 14.00(14.00)\n",
      "Iter 13940 | Time 27.8071(27.5750) | Bit/dim 3.5539(3.5532) | Xent 0.1021(0.1131) | Loss 3.6050(3.6098) | Error 0.0378(0.0387) Steps 1126(1122.81) | Grad Norm 1.3985(1.3727) | Total Time 14.00(14.00)\n",
      "Iter 13950 | Time 27.6728(27.6041) | Bit/dim 3.5574(3.5547) | Xent 0.1052(0.1116) | Loss 3.6100(3.6105) | Error 0.0400(0.0382) Steps 1102(1122.41) | Grad Norm 1.0986(1.3339) | Total Time 14.00(14.00)\n",
      "Iter 13960 | Time 27.2075(27.5250) | Bit/dim 3.5728(3.5541) | Xent 0.1220(0.1113) | Loss 3.6338(3.6097) | Error 0.0489(0.0383) Steps 1120(1123.26) | Grad Norm 1.3550(1.3328) | Total Time 14.00(14.00)\n",
      "Iter 13970 | Time 28.5094(27.5414) | Bit/dim 3.5399(3.5502) | Xent 0.1234(0.1117) | Loss 3.6016(3.6060) | Error 0.0467(0.0383) Steps 1144(1123.95) | Grad Norm 1.5727(1.3363) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0254 | Time 129.2075, Epoch Time 1663.6669(1605.8184), Bit/dim 3.5606(best: 3.5596), Xent 0.9536, Loss 4.0374, Error 0.2278(best: 0.2211)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13980 | Time 27.9605(27.6137) | Bit/dim 3.5465(3.5496) | Xent 0.1011(0.1089) | Loss 3.5971(3.6041) | Error 0.0322(0.0372) Steps 1120(1123.09) | Grad Norm 1.1525(1.3685) | Total Time 14.00(14.00)\n",
      "Iter 13990 | Time 28.0782(27.6697) | Bit/dim 3.5506(3.5496) | Xent 0.1202(0.1076) | Loss 3.6108(3.6034) | Error 0.0489(0.0367) Steps 1144(1125.27) | Grad Norm 1.8743(1.3644) | Total Time 14.00(14.00)\n",
      "Iter 14000 | Time 27.0291(27.6228) | Bit/dim 3.5705(3.5542) | Xent 0.1041(0.1088) | Loss 3.6225(3.6085) | Error 0.0322(0.0372) Steps 1120(1127.53) | Grad Norm 1.2150(1.3614) | Total Time 14.00(14.00)\n",
      "Iter 14010 | Time 27.5461(27.5922) | Bit/dim 3.5649(3.5555) | Xent 0.0979(0.1071) | Loss 3.6139(3.6090) | Error 0.0278(0.0364) Steps 1138(1128.19) | Grad Norm 1.2293(1.3438) | Total Time 14.00(14.00)\n",
      "Iter 14020 | Time 28.4654(27.7063) | Bit/dim 3.5180(3.5496) | Xent 0.1060(0.1082) | Loss 3.5709(3.6037) | Error 0.0344(0.0369) Steps 1156(1128.81) | Grad Norm 1.3999(1.3558) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0255 | Time 129.3995, Epoch Time 1672.5312(1607.8198), Bit/dim 3.5611(best: 3.5596), Xent 0.9800, Loss 4.0511, Error 0.2242(best: 0.2211)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14030 | Time 28.7036(27.7447) | Bit/dim 3.5643(3.5508) | Xent 0.1213(0.1081) | Loss 3.6249(3.6049) | Error 0.0489(0.0372) Steps 1120(1128.24) | Grad Norm 1.3930(1.3952) | Total Time 14.00(14.00)\n",
      "Iter 14040 | Time 27.5577(27.7127) | Bit/dim 3.5448(3.5497) | Xent 0.1209(0.1078) | Loss 3.6052(3.6036) | Error 0.0411(0.0368) Steps 1150(1128.32) | Grad Norm 1.4458(1.3833) | Total Time 14.00(14.00)\n",
      "Iter 14050 | Time 27.2323(27.7384) | Bit/dim 3.5443(3.5509) | Xent 0.1085(0.1075) | Loss 3.5986(3.6047) | Error 0.0411(0.0364) Steps 1114(1127.84) | Grad Norm 1.4410(1.3864) | Total Time 14.00(14.00)\n",
      "Iter 14060 | Time 28.0430(27.6956) | Bit/dim 3.5657(3.5510) | Xent 0.0983(0.1075) | Loss 3.6148(3.6048) | Error 0.0311(0.0366) Steps 1126(1126.11) | Grad Norm 1.1315(1.3606) | Total Time 14.00(14.00)\n",
      "Iter 14070 | Time 28.6408(27.6982) | Bit/dim 3.5742(3.5516) | Xent 0.1185(0.1084) | Loss 3.6335(3.6058) | Error 0.0422(0.0375) Steps 1132(1124.75) | Grad Norm 1.6204(1.3629) | Total Time 14.00(14.00)\n",
      "Iter 14080 | Time 27.5594(27.6910) | Bit/dim 3.5754(3.5506) | Xent 0.1096(0.1079) | Loss 3.6302(3.6046) | Error 0.0389(0.0374) Steps 1144(1126.21) | Grad Norm 1.4042(1.3614) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0256 | Time 130.6667, Epoch Time 1672.9288(1609.7731), Bit/dim 3.5597(best: 3.5596), Xent 0.9666, Loss 4.0430, Error 0.2249(best: 0.2211)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14090 | Time 28.1378(27.6736) | Bit/dim 3.5623(3.5511) | Xent 0.1210(0.1085) | Loss 3.6228(3.6054) | Error 0.0433(0.0373) Steps 1132(1126.99) | Grad Norm 1.7850(1.4025) | Total Time 14.00(14.00)\n",
      "Iter 14100 | Time 27.6214(27.5971) | Bit/dim 3.5753(3.5502) | Xent 0.1056(0.1096) | Loss 3.6281(3.6050) | Error 0.0300(0.0377) Steps 1126(1128.19) | Grad Norm 1.2037(1.4386) | Total Time 14.00(14.00)\n",
      "Iter 14110 | Time 27.6844(27.5894) | Bit/dim 3.5710(3.5524) | Xent 0.1058(0.1081) | Loss 3.6239(3.6065) | Error 0.0356(0.0373) Steps 1138(1127.40) | Grad Norm 1.5231(1.4590) | Total Time 14.00(14.00)\n",
      "Iter 14120 | Time 27.4371(27.6251) | Bit/dim 3.5315(3.5504) | Xent 0.0895(0.1058) | Loss 3.5762(3.6033) | Error 0.0289(0.0366) Steps 1108(1126.40) | Grad Norm 1.0653(1.4617) | Total Time 14.00(14.00)\n",
      "Iter 14130 | Time 27.6381(27.6813) | Bit/dim 3.5365(3.5506) | Xent 0.1570(0.1081) | Loss 3.6151(3.6047) | Error 0.0567(0.0375) Steps 1126(1125.73) | Grad Norm 1.8015(1.4966) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0257 | Time 131.4492, Epoch Time 1676.2681(1611.7679), Bit/dim 3.5584(best: 3.5596), Xent 0.9696, Loss 4.0432, Error 0.2239(best: 0.2211)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14140 | Time 26.9908(27.6330) | Bit/dim 3.5389(3.5488) | Xent 0.1077(0.1064) | Loss 3.5928(3.6020) | Error 0.0378(0.0366) Steps 1132(1125.72) | Grad Norm 1.1214(1.4748) | Total Time 14.00(14.00)\n",
      "Iter 14150 | Time 27.0759(27.5897) | Bit/dim 3.5606(3.5513) | Xent 0.0899(0.1051) | Loss 3.6055(3.6038) | Error 0.0344(0.0360) Steps 1126(1126.10) | Grad Norm 1.1727(1.4537) | Total Time 14.00(14.00)\n",
      "Iter 14160 | Time 28.0409(27.6013) | Bit/dim 3.5675(3.5507) | Xent 0.0889(0.1047) | Loss 3.6119(3.6030) | Error 0.0333(0.0356) Steps 1150(1127.91) | Grad Norm 1.4228(1.4795) | Total Time 14.00(14.00)\n",
      "Iter 14170 | Time 27.6972(27.6437) | Bit/dim 3.5459(3.5498) | Xent 0.1259(0.1038) | Loss 3.6088(3.6017) | Error 0.0456(0.0359) Steps 1126(1126.79) | Grad Norm 1.9002(1.4668) | Total Time 14.00(14.00)\n",
      "Iter 14180 | Time 27.8552(27.5937) | Bit/dim 3.5058(3.5486) | Xent 0.1140(0.1024) | Loss 3.5628(3.5998) | Error 0.0367(0.0354) Steps 1120(1126.60) | Grad Norm 1.8528(1.4456) | Total Time 14.00(14.00)\n",
      "Iter 14190 | Time 27.9899(27.6794) | Bit/dim 3.5431(3.5483) | Xent 0.1076(0.1053) | Loss 3.5969(3.6010) | Error 0.0322(0.0364) Steps 1144(1127.44) | Grad Norm 1.1857(1.4178) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0258 | Time 129.4341, Epoch Time 1666.9862(1613.4245), Bit/dim 3.5588(best: 3.5584), Xent 0.9853, Loss 4.0515, Error 0.2238(best: 0.2211)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14200 | Time 28.0982(27.7223) | Bit/dim 3.5385(3.5487) | Xent 0.1064(0.1050) | Loss 3.5917(3.6012) | Error 0.0378(0.0366) Steps 1114(1127.37) | Grad Norm 1.4474(1.4465) | Total Time 14.00(14.00)\n",
      "Iter 14210 | Time 27.9089(27.6637) | Bit/dim 3.5319(3.5468) | Xent 0.1023(0.1033) | Loss 3.5830(3.5985) | Error 0.0322(0.0354) Steps 1132(1127.77) | Grad Norm 1.7138(1.4392) | Total Time 14.00(14.00)\n",
      "Iter 14220 | Time 27.6431(27.6572) | Bit/dim 3.5664(3.5479) | Xent 0.1045(0.1035) | Loss 3.6187(3.5997) | Error 0.0356(0.0358) Steps 1138(1128.62) | Grad Norm 1.9566(1.4815) | Total Time 14.00(14.00)\n",
      "Iter 14230 | Time 26.9738(27.5917) | Bit/dim 3.5582(3.5503) | Xent 0.1095(0.1033) | Loss 3.6130(3.6019) | Error 0.0378(0.0357) Steps 1126(1128.36) | Grad Norm 1.4200(1.4674) | Total Time 14.00(14.00)\n",
      "Iter 14240 | Time 27.6483(27.6083) | Bit/dim 3.5570(3.5488) | Xent 0.1164(0.1037) | Loss 3.6152(3.6006) | Error 0.0311(0.0357) Steps 1126(1129.16) | Grad Norm 1.2889(1.4377) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0259 | Time 131.0645, Epoch Time 1667.1573(1615.0365), Bit/dim 3.5599(best: 3.5584), Xent 0.9988, Loss 4.0594, Error 0.2230(best: 0.2211)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14250 | Time 27.9815(27.5676) | Bit/dim 3.5497(3.5493) | Xent 0.1049(0.1039) | Loss 3.6022(3.6013) | Error 0.0356(0.0357) Steps 1150(1128.93) | Grad Norm 1.1496(1.4203) | Total Time 14.00(14.00)\n",
      "Iter 14260 | Time 27.7237(27.6607) | Bit/dim 3.5312(3.5494) | Xent 0.0904(0.1037) | Loss 3.5764(3.6013) | Error 0.0311(0.0355) Steps 1132(1130.69) | Grad Norm 1.1526(1.3951) | Total Time 14.00(14.00)\n",
      "Iter 14270 | Time 27.5151(27.7107) | Bit/dim 3.5411(3.5484) | Xent 0.0887(0.1007) | Loss 3.5854(3.5987) | Error 0.0278(0.0339) Steps 1138(1133.24) | Grad Norm 1.2893(1.3712) | Total Time 14.00(14.00)\n",
      "Iter 14280 | Time 27.9602(27.7461) | Bit/dim 3.5274(3.5476) | Xent 0.1081(0.1035) | Loss 3.5815(3.5993) | Error 0.0389(0.0351) Steps 1108(1132.61) | Grad Norm 1.3899(1.3688) | Total Time 14.00(14.00)\n",
      "Iter 14290 | Time 26.9898(27.6788) | Bit/dim 3.5687(3.5492) | Xent 0.1290(0.1044) | Loss 3.6332(3.6014) | Error 0.0500(0.0354) Steps 1132(1132.79) | Grad Norm 1.6044(1.3581) | Total Time 14.00(14.00)\n",
      "Iter 14300 | Time 27.9150(27.6914) | Bit/dim 3.5498(3.5487) | Xent 0.1226(0.1053) | Loss 3.6111(3.6013) | Error 0.0400(0.0359) Steps 1126(1132.18) | Grad Norm 1.6326(1.4007) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0260 | Time 131.7418, Epoch Time 1675.0624(1616.8373), Bit/dim 3.5589(best: 3.5584), Xent 1.0032, Loss 4.0605, Error 0.2263(best: 0.2211)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14310 | Time 28.0609(27.7369) | Bit/dim 3.5453(3.5494) | Xent 0.1050(0.1042) | Loss 3.5978(3.6015) | Error 0.0400(0.0355) Steps 1144(1130.91) | Grad Norm 1.6078(1.4194) | Total Time 14.00(14.00)\n",
      "Iter 14320 | Time 28.0153(27.7180) | Bit/dim 3.5388(3.5480) | Xent 0.1240(0.1040) | Loss 3.6008(3.6000) | Error 0.0422(0.0350) Steps 1144(1131.93) | Grad Norm 1.7346(1.4549) | Total Time 14.00(14.00)\n",
      "Iter 14330 | Time 27.2679(27.6975) | Bit/dim 3.5376(3.5471) | Xent 0.0980(0.1030) | Loss 3.5866(3.5986) | Error 0.0289(0.0346) Steps 1132(1131.10) | Grad Norm 1.6354(1.4485) | Total Time 14.00(14.00)\n",
      "Iter 14340 | Time 27.9561(27.7083) | Bit/dim 3.5734(3.5498) | Xent 0.1152(0.1034) | Loss 3.6310(3.6015) | Error 0.0356(0.0344) Steps 1108(1130.47) | Grad Norm 1.4698(1.4735) | Total Time 14.00(14.00)\n",
      "Iter 14350 | Time 27.6603(27.7064) | Bit/dim 3.5672(3.5488) | Xent 0.1079(0.1025) | Loss 3.6211(3.6000) | Error 0.0433(0.0349) Steps 1132(1129.90) | Grad Norm 1.9195(1.4619) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0261 | Time 130.5111, Epoch Time 1672.4818(1618.5066), Bit/dim 3.5585(best: 3.5584), Xent 1.0089, Loss 4.0630, Error 0.2268(best: 0.2211)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14360 | Time 27.8158(27.7196) | Bit/dim 3.5457(3.5487) | Xent 0.0939(0.1020) | Loss 3.5926(3.5997) | Error 0.0333(0.0346) Steps 1108(1130.37) | Grad Norm 1.1345(1.4466) | Total Time 14.00(14.00)\n",
      "Iter 14370 | Time 27.4141(27.7165) | Bit/dim 3.5545(3.5496) | Xent 0.1070(0.1001) | Loss 3.6080(3.5996) | Error 0.0300(0.0335) Steps 1132(1131.96) | Grad Norm 1.4932(1.3920) | Total Time 14.00(14.00)\n",
      "Iter 14380 | Time 27.3987(27.6820) | Bit/dim 3.5809(3.5492) | Xent 0.0792(0.0985) | Loss 3.6205(3.5984) | Error 0.0244(0.0332) Steps 1138(1131.83) | Grad Norm 1.0965(1.3771) | Total Time 14.00(14.00)\n",
      "Iter 14390 | Time 27.6505(27.7568) | Bit/dim 3.5375(3.5486) | Xent 0.0878(0.0993) | Loss 3.5814(3.5982) | Error 0.0322(0.0340) Steps 1132(1131.37) | Grad Norm 1.5925(1.3847) | Total Time 14.00(14.00)\n",
      "Iter 14400 | Time 27.4544(27.7245) | Bit/dim 3.5744(3.5500) | Xent 0.1177(0.0993) | Loss 3.6332(3.5996) | Error 0.0378(0.0339) Steps 1132(1131.45) | Grad Norm 1.6607(1.3890) | Total Time 14.00(14.00)\n",
      "Iter 14410 | Time 27.2843(27.6698) | Bit/dim 3.5219(3.5478) | Xent 0.1338(0.1019) | Loss 3.5888(3.5988) | Error 0.0444(0.0353) Steps 1120(1129.91) | Grad Norm 1.7190(1.4373) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0262 | Time 129.5277, Epoch Time 1672.3535(1620.1220), Bit/dim 3.5600(best: 3.5584), Xent 1.0082, Loss 4.0641, Error 0.2266(best: 0.2211)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14420 | Time 27.6952(27.6204) | Bit/dim 3.5581(3.5487) | Xent 0.1098(0.1018) | Loss 3.6130(3.5996) | Error 0.0378(0.0349) Steps 1132(1131.97) | Grad Norm 1.2984(1.4198) | Total Time 14.00(14.00)\n",
      "Iter 14430 | Time 27.4158(27.6283) | Bit/dim 3.5337(3.5492) | Xent 0.1076(0.1016) | Loss 3.5875(3.6000) | Error 0.0256(0.0348) Steps 1120(1128.69) | Grad Norm 1.5031(1.4344) | Total Time 14.00(14.00)\n",
      "Iter 14440 | Time 27.4403(27.5618) | Bit/dim 3.5372(3.5507) | Xent 0.1155(0.1009) | Loss 3.5950(3.6012) | Error 0.0411(0.0345) Steps 1126(1127.72) | Grad Norm 1.7603(1.4523) | Total Time 14.00(14.00)\n",
      "Iter 14450 | Time 28.1082(27.6344) | Bit/dim 3.5433(3.5505) | Xent 0.1234(0.1033) | Loss 3.6050(3.6021) | Error 0.0378(0.0356) Steps 1138(1129.49) | Grad Norm 1.7400(1.4643) | Total Time 14.00(14.00)\n",
      "Iter 14460 | Time 26.8579(27.5949) | Bit/dim 3.5176(3.5453) | Xent 0.1076(0.1044) | Loss 3.5714(3.5975) | Error 0.0367(0.0359) Steps 1114(1129.46) | Grad Norm 1.9639(1.5168) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0263 | Time 130.4214, Epoch Time 1663.9208(1621.4360), Bit/dim 3.5608(best: 3.5584), Xent 1.0223, Loss 4.0719, Error 0.2272(best: 0.2211)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14470 | Time 28.0077(27.5727) | Bit/dim 3.5630(3.5485) | Xent 0.0935(0.1038) | Loss 3.6098(3.6004) | Error 0.0356(0.0356) Steps 1144(1128.41) | Grad Norm 1.5312(1.5170) | Total Time 14.00(14.00)\n",
      "Iter 14480 | Time 27.9704(27.5729) | Bit/dim 3.5337(3.5481) | Xent 0.0919(0.1016) | Loss 3.5797(3.5989) | Error 0.0367(0.0352) Steps 1156(1130.92) | Grad Norm 2.0996(1.5416) | Total Time 14.00(14.00)\n",
      "Iter 14490 | Time 27.8311(27.6121) | Bit/dim 3.5538(3.5499) | Xent 0.0792(0.1004) | Loss 3.5934(3.6001) | Error 0.0267(0.0352) Steps 1138(1132.52) | Grad Norm 1.0345(1.5631) | Total Time 14.00(14.00)\n",
      "Iter 14500 | Time 27.6229(27.6369) | Bit/dim 3.5879(3.5499) | Xent 0.0907(0.1007) | Loss 3.6332(3.6003) | Error 0.0289(0.0348) Steps 1150(1134.92) | Grad Norm 1.6078(1.5620) | Total Time 14.00(14.00)\n",
      "Iter 14510 | Time 27.6918(27.5423) | Bit/dim 3.5569(3.5475) | Xent 0.0959(0.0992) | Loss 3.6048(3.5971) | Error 0.0300(0.0337) Steps 1132(1133.71) | Grad Norm 1.9487(1.5542) | Total Time 14.00(14.00)\n",
      "Iter 14520 | Time 26.7633(27.4360) | Bit/dim 3.5695(3.5468) | Xent 0.1034(0.0997) | Loss 3.6211(3.5966) | Error 0.0300(0.0339) Steps 1114(1131.42) | Grad Norm 1.5708(1.5711) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0264 | Time 130.7433, Epoch Time 1661.6249(1622.6416), Bit/dim 3.5564(best: 3.5584), Xent 1.0005, Loss 4.0566, Error 0.2266(best: 0.2211)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14530 | Time 28.1926(27.5270) | Bit/dim 3.5240(3.5452) | Xent 0.1036(0.1006) | Loss 3.5758(3.5955) | Error 0.0344(0.0347) Steps 1120(1131.61) | Grad Norm 1.6202(1.5767) | Total Time 14.00(14.00)\n",
      "Iter 14540 | Time 27.6461(27.5600) | Bit/dim 3.5146(3.5447) | Xent 0.0949(0.0981) | Loss 3.5621(3.5937) | Error 0.0356(0.0340) Steps 1114(1130.46) | Grad Norm 2.2650(1.5359) | Total Time 14.00(14.00)\n",
      "Iter 14550 | Time 27.7331(27.5694) | Bit/dim 3.5495(3.5460) | Xent 0.0889(0.0994) | Loss 3.5940(3.5958) | Error 0.0344(0.0341) Steps 1132(1129.53) | Grad Norm 1.4423(1.5457) | Total Time 14.00(14.00)\n",
      "Iter 14560 | Time 27.3575(27.6329) | Bit/dim 3.5634(3.5477) | Xent 0.1010(0.0997) | Loss 3.6139(3.5976) | Error 0.0367(0.0345) Steps 1138(1130.07) | Grad Norm 1.2868(1.5669) | Total Time 14.00(14.00)\n",
      "Iter 14570 | Time 26.7440(27.5403) | Bit/dim 3.5783(3.5473) | Xent 0.1099(0.1019) | Loss 3.6333(3.5982) | Error 0.0344(0.0349) Steps 1126(1129.35) | Grad Norm 1.4217(1.6147) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0265 | Time 131.0339, Epoch Time 1668.1707(1624.0075), Bit/dim 3.5586(best: 3.5564), Xent 1.0292, Loss 4.0732, Error 0.2267(best: 0.2211)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14580 | Time 27.8892(27.5506) | Bit/dim 3.5280(3.5476) | Xent 0.0921(0.1014) | Loss 3.5741(3.5983) | Error 0.0311(0.0344) Steps 1114(1127.80) | Grad Norm 1.4313(1.5844) | Total Time 14.00(14.00)\n",
      "Iter 14590 | Time 27.5685(27.5776) | Bit/dim 3.5588(3.5484) | Xent 0.1071(0.0999) | Loss 3.6123(3.5983) | Error 0.0344(0.0337) Steps 1138(1128.94) | Grad Norm 1.8671(1.5811) | Total Time 14.00(14.00)\n",
      "Iter 14600 | Time 27.3082(27.5302) | Bit/dim 3.5268(3.5478) | Xent 0.0861(0.0988) | Loss 3.5699(3.5972) | Error 0.0300(0.0333) Steps 1114(1128.65) | Grad Norm 1.9462(1.6141) | Total Time 14.00(14.00)\n",
      "Iter 14610 | Time 28.1559(27.6516) | Bit/dim 3.5621(3.5489) | Xent 0.1053(0.0982) | Loss 3.6147(3.5980) | Error 0.0400(0.0330) Steps 1150(1128.51) | Grad Norm 1.3697(1.5773) | Total Time 14.00(14.00)\n",
      "Iter 14620 | Time 27.5722(27.6310) | Bit/dim 3.5327(3.5495) | Xent 0.1001(0.0983) | Loss 3.5828(3.5987) | Error 0.0367(0.0335) Steps 1126(1129.10) | Grad Norm 2.1983(1.5617) | Total Time 14.00(14.00)\n",
      "Iter 14630 | Time 27.4828(27.6394) | Bit/dim 3.5479(3.5481) | Xent 0.1209(0.1002) | Loss 3.6084(3.5982) | Error 0.0456(0.0344) Steps 1126(1129.61) | Grad Norm 1.3491(1.5363) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0266 | Time 130.6336, Epoch Time 1669.5694(1625.3744), Bit/dim 3.5573(best: 3.5564), Xent 1.0208, Loss 4.0677, Error 0.2296(best: 0.2211)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14640 | Time 27.7524(27.6880) | Bit/dim 3.5328(3.5483) | Xent 0.1071(0.1008) | Loss 3.5863(3.5987) | Error 0.0378(0.0344) Steps 1144(1128.57) | Grad Norm 1.8098(1.5176) | Total Time 14.00(14.00)\n",
      "Iter 14650 | Time 26.8061(27.6336) | Bit/dim 3.5634(3.5480) | Xent 0.0977(0.0983) | Loss 3.6122(3.5972) | Error 0.0256(0.0335) Steps 1138(1131.47) | Grad Norm 1.3921(1.4972) | Total Time 14.00(14.00)\n",
      "Iter 14660 | Time 27.0499(27.5380) | Bit/dim 3.5423(3.5495) | Xent 0.0841(0.0989) | Loss 3.5843(3.5990) | Error 0.0311(0.0336) Steps 1132(1131.44) | Grad Norm 1.4426(1.5008) | Total Time 14.00(14.00)\n",
      "Iter 14670 | Time 27.7738(27.6273) | Bit/dim 3.5443(3.5494) | Xent 0.1091(0.0994) | Loss 3.5988(3.5991) | Error 0.0367(0.0340) Steps 1132(1131.48) | Grad Norm 1.7099(1.5051) | Total Time 14.00(14.00)\n",
      "Iter 14680 | Time 27.7288(27.6181) | Bit/dim 3.5588(3.5472) | Xent 0.1113(0.1028) | Loss 3.6144(3.5986) | Error 0.0322(0.0353) Steps 1120(1131.73) | Grad Norm 1.5189(1.5308) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0267 | Time 132.0906, Epoch Time 1669.9208(1626.7108), Bit/dim 3.5571(best: 3.5564), Xent 1.0400, Loss 4.0771, Error 0.2262(best: 0.2211)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14690 | Time 26.8858(27.5740) | Bit/dim 3.5380(3.5477) | Xent 0.0960(0.0997) | Loss 3.5860(3.5976) | Error 0.0300(0.0343) Steps 1114(1132.46) | Grad Norm 2.1161(1.5427) | Total Time 14.00(14.00)\n",
      "Iter 14700 | Time 27.8249(27.6085) | Bit/dim 3.5285(3.5485) | Xent 0.0855(0.0978) | Loss 3.5712(3.5974) | Error 0.0267(0.0333) Steps 1138(1133.01) | Grad Norm 1.3838(1.5439) | Total Time 14.00(14.00)\n",
      "Iter 14710 | Time 28.1629(27.6122) | Bit/dim 3.5701(3.5513) | Xent 0.1088(0.0968) | Loss 3.6245(3.5997) | Error 0.0378(0.0332) Steps 1138(1133.11) | Grad Norm 1.2306(1.5347) | Total Time 14.00(14.00)\n",
      "Iter 14720 | Time 27.4839(27.6120) | Bit/dim 3.5411(3.5476) | Xent 0.1003(0.0970) | Loss 3.5913(3.5961) | Error 0.0256(0.0328) Steps 1120(1132.94) | Grad Norm 1.4602(1.5441) | Total Time 14.00(14.00)\n",
      "Iter 14730 | Time 27.5458(27.6164) | Bit/dim 3.5218(3.5458) | Xent 0.1139(0.0988) | Loss 3.5787(3.5952) | Error 0.0433(0.0337) Steps 1126(1132.27) | Grad Norm 1.6791(1.5189) | Total Time 14.00(14.00)\n",
      "Iter 14740 | Time 28.0596(27.6227) | Bit/dim 3.5599(3.5455) | Xent 0.1287(0.0994) | Loss 3.6243(3.5952) | Error 0.0444(0.0343) Steps 1150(1133.55) | Grad Norm 1.5793(1.5187) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0268 | Time 130.4889, Epoch Time 1666.7829(1627.9129), Bit/dim 3.5553(best: 3.5564), Xent 1.0137, Loss 4.0622, Error 0.2246(best: 0.2211)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14750 | Time 27.7700(27.5365) | Bit/dim 3.5193(3.5455) | Xent 0.0823(0.0976) | Loss 3.5604(3.5944) | Error 0.0322(0.0334) Steps 1126(1131.44) | Grad Norm 1.3375(1.4723) | Total Time 14.00(14.00)\n",
      "Iter 14760 | Time 27.3149(27.5116) | Bit/dim 3.5464(3.5465) | Xent 0.1142(0.0965) | Loss 3.6035(3.5947) | Error 0.0300(0.0327) Steps 1120(1130.89) | Grad Norm 1.5292(1.5068) | Total Time 14.00(14.00)\n",
      "Iter 14770 | Time 28.4307(27.6170) | Bit/dim 3.5597(3.5459) | Xent 0.1014(0.0970) | Loss 3.6104(3.5944) | Error 0.0289(0.0327) Steps 1120(1128.86) | Grad Norm 1.7243(1.5550) | Total Time 14.00(14.00)\n",
      "Iter 14780 | Time 28.0162(27.6403) | Bit/dim 3.5625(3.5490) | Xent 0.1032(0.0978) | Loss 3.6141(3.5979) | Error 0.0311(0.0328) Steps 1132(1130.49) | Grad Norm 1.4364(1.5685) | Total Time 14.00(14.00)\n",
      "Iter 14790 | Time 27.0841(27.6240) | Bit/dim 3.5541(3.5465) | Xent 0.0895(0.0982) | Loss 3.5989(3.5955) | Error 0.0322(0.0331) Steps 1120(1129.46) | Grad Norm 1.9804(1.6236) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0269 | Time 131.1666, Epoch Time 1665.9765(1629.0548), Bit/dim 3.5576(best: 3.5553), Xent 1.0411, Loss 4.0782, Error 0.2242(best: 0.2211)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14800 | Time 27.8658(27.6374) | Bit/dim 3.5604(3.5461) | Xent 0.0836(0.0978) | Loss 3.6023(3.5950) | Error 0.0278(0.0329) Steps 1156(1133.32) | Grad Norm 1.1715(1.6289) | Total Time 14.00(14.00)\n",
      "Iter 14810 | Time 28.0767(27.6222) | Bit/dim 3.5490(3.5450) | Xent 0.1038(0.0977) | Loss 3.6009(3.5939) | Error 0.0344(0.0332) Steps 1138(1133.18) | Grad Norm 1.4673(1.6259) | Total Time 14.00(14.00)\n",
      "Iter 14820 | Time 27.4001(27.6146) | Bit/dim 3.5394(3.5472) | Xent 0.1044(0.0971) | Loss 3.5916(3.5957) | Error 0.0356(0.0330) Steps 1120(1133.00) | Grad Norm 1.5762(1.5808) | Total Time 14.00(14.00)\n",
      "Iter 14830 | Time 28.1205(27.6373) | Bit/dim 3.5523(3.5496) | Xent 0.0940(0.0968) | Loss 3.5993(3.5980) | Error 0.0344(0.0333) Steps 1138(1133.51) | Grad Norm 1.8250(1.5448) | Total Time 14.00(14.00)\n",
      "Iter 14840 | Time 28.3056(27.6664) | Bit/dim 3.5532(3.5488) | Xent 0.0966(0.0958) | Loss 3.6015(3.5967) | Error 0.0333(0.0331) Steps 1114(1133.27) | Grad Norm 1.4662(1.4960) | Total Time 14.00(14.00)\n",
      "Iter 14850 | Time 27.4152(27.5543) | Bit/dim 3.5477(3.5461) | Xent 0.1007(0.0976) | Loss 3.5981(3.5949) | Error 0.0300(0.0334) Steps 1102(1129.84) | Grad Norm 1.9886(1.5327) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0270 | Time 129.1902, Epoch Time 1665.1584(1630.1379), Bit/dim 3.5572(best: 3.5553), Xent 1.0493, Loss 4.0818, Error 0.2226(best: 0.2211)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14860 | Time 27.4137(27.5442) | Bit/dim 3.5604(3.5466) | Xent 0.0819(0.0973) | Loss 3.6014(3.5952) | Error 0.0256(0.0335) Steps 1126(1131.82) | Grad Norm 1.5808(1.5780) | Total Time 14.00(14.00)\n",
      "Iter 14870 | Time 27.7887(27.5555) | Bit/dim 3.5299(3.5451) | Xent 0.0916(0.0950) | Loss 3.5757(3.5926) | Error 0.0278(0.0328) Steps 1114(1129.84) | Grad Norm 1.2431(1.5766) | Total Time 14.00(14.00)\n",
      "Iter 14880 | Time 27.2853(27.5314) | Bit/dim 3.5324(3.5436) | Xent 0.0969(0.0975) | Loss 3.5809(3.5923) | Error 0.0278(0.0330) Steps 1132(1129.69) | Grad Norm 1.7795(1.6202) | Total Time 14.00(14.00)\n",
      "Iter 14890 | Time 28.1079(27.5473) | Bit/dim 3.5372(3.5457) | Xent 0.0699(0.0963) | Loss 3.5721(3.5939) | Error 0.0200(0.0331) Steps 1126(1129.22) | Grad Norm 1.3333(1.5980) | Total Time 14.00(14.00)\n",
      "Iter 14900 | Time 28.2701(27.5769) | Bit/dim 3.5809(3.5486) | Xent 0.0763(0.0948) | Loss 3.6190(3.5960) | Error 0.0244(0.0329) Steps 1132(1129.71) | Grad Norm 1.0816(1.5447) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0271 | Time 131.1112, Epoch Time 1663.3697(1631.1349), Bit/dim 3.5565(best: 3.5553), Xent 1.0480, Loss 4.0805, Error 0.2284(best: 0.2211)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14910 | Time 27.4768(27.5164) | Bit/dim 3.5652(3.5479) | Xent 0.0880(0.0947) | Loss 3.6092(3.5953) | Error 0.0289(0.0327) Steps 1144(1132.34) | Grad Norm 1.2329(1.5192) | Total Time 14.00(14.00)\n",
      "Iter 14920 | Time 27.7488(27.5400) | Bit/dim 3.5557(3.5466) | Xent 0.1098(0.0944) | Loss 3.6106(3.5938) | Error 0.0411(0.0328) Steps 1126(1132.39) | Grad Norm 1.7142(1.5132) | Total Time 14.00(14.00)\n",
      "Iter 14930 | Time 27.8090(27.6329) | Bit/dim 3.5706(3.5487) | Xent 0.0931(0.0944) | Loss 3.6171(3.5959) | Error 0.0322(0.0325) Steps 1132(1133.32) | Grad Norm 1.4493(1.5047) | Total Time 14.00(14.00)\n",
      "Iter 14940 | Time 28.1858(27.6538) | Bit/dim 3.5454(3.5489) | Xent 0.1035(0.0956) | Loss 3.5971(3.5967) | Error 0.0344(0.0330) Steps 1150(1133.68) | Grad Norm 1.8696(1.5632) | Total Time 14.00(14.00)\n",
      "Iter 14950 | Time 26.9686(27.6563) | Bit/dim 3.5316(3.5441) | Xent 0.0996(0.0968) | Loss 3.5814(3.5925) | Error 0.0244(0.0331) Steps 1126(1133.37) | Grad Norm 1.9440(1.6174) | Total Time 14.00(14.00)\n",
      "Iter 14960 | Time 27.7764(27.6719) | Bit/dim 3.5615(3.5454) | Xent 0.1036(0.0966) | Loss 3.6132(3.5937) | Error 0.0400(0.0332) Steps 1144(1133.45) | Grad Norm 1.4759(1.6371) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0272 | Time 130.8023, Epoch Time 1671.8753(1632.3571), Bit/dim 3.5569(best: 3.5553), Xent 1.0311, Loss 4.0725, Error 0.2241(best: 0.2211)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14970 | Time 27.9827(27.6517) | Bit/dim 3.5648(3.5433) | Xent 0.1091(0.0949) | Loss 3.6194(3.5907) | Error 0.0389(0.0325) Steps 1120(1133.11) | Grad Norm 2.1452(1.6372) | Total Time 14.00(14.00)\n",
      "Iter 14980 | Time 27.1552(27.6251) | Bit/dim 3.5601(3.5458) | Xent 0.0982(0.0946) | Loss 3.6092(3.5931) | Error 0.0322(0.0328) Steps 1126(1133.32) | Grad Norm 2.0490(1.6542) | Total Time 14.00(14.00)\n",
      "Iter 14990 | Time 27.5515(27.6269) | Bit/dim 3.5694(3.5456) | Xent 0.0971(0.0945) | Loss 3.6179(3.5928) | Error 0.0322(0.0324) Steps 1132(1134.14) | Grad Norm 1.8667(1.6854) | Total Time 14.00(14.00)\n",
      "Iter 15000 | Time 27.5327(27.5621) | Bit/dim 3.5347(3.5451) | Xent 0.0897(0.0934) | Loss 3.5796(3.5918) | Error 0.0322(0.0317) Steps 1138(1134.20) | Grad Norm 1.4335(1.6570) | Total Time 14.00(14.00)\n",
      "Iter 15010 | Time 27.3550(27.5123) | Bit/dim 3.5552(3.5470) | Xent 0.0744(0.0948) | Loss 3.5924(3.5944) | Error 0.0278(0.0325) Steps 1144(1134.46) | Grad Norm 1.4237(1.6161) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0273 | Time 130.3589, Epoch Time 1661.5629(1633.2333), Bit/dim 3.5557(best: 3.5553), Xent 1.0645, Loss 4.0879, Error 0.2321(best: 0.2211)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15020 | Time 27.6850(27.5084) | Bit/dim 3.5582(3.5466) | Xent 0.0920(0.0963) | Loss 3.6042(3.5947) | Error 0.0289(0.0330) Steps 1120(1133.72) | Grad Norm 1.2331(1.6687) | Total Time 14.00(14.00)\n",
      "Iter 15030 | Time 27.9134(27.5347) | Bit/dim 3.5138(3.5433) | Xent 0.0917(0.0954) | Loss 3.5597(3.5910) | Error 0.0400(0.0334) Steps 1138(1134.17) | Grad Norm 1.5291(1.6577) | Total Time 14.00(14.00)\n",
      "Iter 15040 | Time 27.1841(27.5437) | Bit/dim 3.5628(3.5433) | Xent 0.0878(0.0957) | Loss 3.6067(3.5911) | Error 0.0256(0.0331) Steps 1150(1135.39) | Grad Norm 1.6773(1.6623) | Total Time 14.00(14.00)\n",
      "Iter 15050 | Time 27.2579(27.5293) | Bit/dim 3.5406(3.5441) | Xent 0.0965(0.0967) | Loss 3.5889(3.5925) | Error 0.0311(0.0334) Steps 1120(1134.08) | Grad Norm 1.3904(1.6802) | Total Time 14.00(14.00)\n",
      "Iter 15060 | Time 27.6733(27.5369) | Bit/dim 3.5565(3.5461) | Xent 0.0988(0.0952) | Loss 3.6059(3.5937) | Error 0.0422(0.0329) Steps 1126(1133.79) | Grad Norm 2.8425(1.7243) | Total Time 14.00(14.00)\n",
      "Iter 15070 | Time 27.8054(27.5150) | Bit/dim 3.5711(3.5477) | Xent 0.1055(0.0950) | Loss 3.6238(3.5951) | Error 0.0278(0.0327) Steps 1114(1132.57) | Grad Norm 1.5782(1.7115) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0274 | Time 131.8116, Epoch Time 1664.1888(1634.1619), Bit/dim 3.5572(best: 3.5553), Xent 1.0675, Loss 4.0910, Error 0.2286(best: 0.2211)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15080 | Time 27.8487(27.5885) | Bit/dim 3.5501(3.5458) | Xent 0.1095(0.0945) | Loss 3.6048(3.5931) | Error 0.0389(0.0328) Steps 1114(1133.88) | Grad Norm 1.3386(1.6667) | Total Time 14.00(14.00)\n",
      "Iter 15090 | Time 27.7124(27.6820) | Bit/dim 3.5783(3.5454) | Xent 0.0942(0.0924) | Loss 3.6254(3.5916) | Error 0.0344(0.0321) Steps 1138(1132.78) | Grad Norm 1.3996(1.6319) | Total Time 14.00(14.00)\n",
      "Iter 15100 | Time 26.9173(27.6737) | Bit/dim 3.5407(3.5482) | Xent 0.0917(0.0933) | Loss 3.5865(3.5948) | Error 0.0322(0.0322) Steps 1114(1132.85) | Grad Norm 1.3084(1.5837) | Total Time 14.00(14.00)\n",
      "Iter 15110 | Time 27.3714(27.6443) | Bit/dim 3.5570(3.5464) | Xent 0.0805(0.0947) | Loss 3.5972(3.5938) | Error 0.0211(0.0325) Steps 1132(1131.12) | Grad Norm 2.5707(1.6456) | Total Time 14.00(14.00)\n",
      "Iter 15120 | Time 27.4268(27.6705) | Bit/dim 3.5662(3.5469) | Xent 0.0802(0.0940) | Loss 3.6063(3.5939) | Error 0.0278(0.0325) Steps 1138(1132.00) | Grad Norm 1.3000(1.6114) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0275 | Time 131.4712, Epoch Time 1675.3124(1635.3965), Bit/dim 3.5555(best: 3.5553), Xent 1.0503, Loss 4.0807, Error 0.2272(best: 0.2211)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15130 | Time 27.1709(27.6336) | Bit/dim 3.5533(3.5468) | Xent 0.1219(0.0962) | Loss 3.6142(3.5949) | Error 0.0378(0.0330) Steps 1132(1130.63) | Grad Norm 1.6515(1.6273) | Total Time 14.00(14.00)\n",
      "Iter 15140 | Time 27.3417(27.6341) | Bit/dim 3.5533(3.5468) | Xent 0.0706(0.0941) | Loss 3.5886(3.5938) | Error 0.0244(0.0324) Steps 1138(1131.59) | Grad Norm 1.7088(1.6358) | Total Time 14.00(14.00)\n",
      "Iter 15150 | Time 27.3828(27.6302) | Bit/dim 3.5148(3.5448) | Xent 0.0730(0.0911) | Loss 3.5513(3.5904) | Error 0.0300(0.0315) Steps 1108(1130.24) | Grad Norm 2.0669(1.6307) | Total Time 14.00(14.00)\n",
      "Iter 15160 | Time 28.0094(27.6784) | Bit/dim 3.5328(3.5447) | Xent 0.0933(0.0916) | Loss 3.5795(3.5905) | Error 0.0344(0.0315) Steps 1144(1132.67) | Grad Norm 1.3982(1.6171) | Total Time 14.00(14.00)\n",
      "Iter 15170 | Time 27.1168(27.6196) | Bit/dim 3.5677(3.5467) | Xent 0.0839(0.0915) | Loss 3.6097(3.5924) | Error 0.0300(0.0322) Steps 1126(1131.99) | Grad Norm 2.9694(1.7351) | Total Time 14.00(14.00)\n",
      "Iter 15180 | Time 27.9831(27.6770) | Bit/dim 3.5239(3.5461) | Xent 0.0893(0.0928) | Loss 3.5686(3.5925) | Error 0.0322(0.0328) Steps 1138(1133.62) | Grad Norm 1.5316(1.7702) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0276 | Time 131.3721, Epoch Time 1671.2110(1636.4709), Bit/dim 3.5540(best: 3.5553), Xent 1.0440, Loss 4.0760, Error 0.2264(best: 0.2211)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15190 | Time 27.6030(27.6870) | Bit/dim 3.5379(3.5476) | Xent 0.1033(0.0924) | Loss 3.5896(3.5938) | Error 0.0311(0.0322) Steps 1114(1133.08) | Grad Norm 1.8829(1.7107) | Total Time 14.00(14.00)\n",
      "Iter 15200 | Time 27.8936(27.7902) | Bit/dim 3.5493(3.5471) | Xent 0.1016(0.0929) | Loss 3.6001(3.5936) | Error 0.0344(0.0320) Steps 1132(1132.75) | Grad Norm 1.5199(1.7005) | Total Time 14.00(14.00)\n",
      "Iter 15210 | Time 27.4407(27.7534) | Bit/dim 3.5700(3.5468) | Xent 0.0757(0.0923) | Loss 3.6078(3.5930) | Error 0.0311(0.0323) Steps 1120(1135.02) | Grad Norm 1.3007(1.6568) | Total Time 14.00(14.00)\n",
      "Iter 15220 | Time 27.9703(27.7521) | Bit/dim 3.4748(3.5404) | Xent 0.1055(0.0920) | Loss 3.5275(3.5864) | Error 0.0444(0.0327) Steps 1156(1136.73) | Grad Norm 2.4040(1.6296) | Total Time 14.00(14.00)\n",
      "Iter 15230 | Time 27.5952(27.6990) | Bit/dim 3.5895(3.5450) | Xent 0.1044(0.0920) | Loss 3.6417(3.5910) | Error 0.0389(0.0324) Steps 1132(1135.19) | Grad Norm 1.4323(1.5941) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0277 | Time 132.7390, Epoch Time 1676.3169(1637.6663), Bit/dim 3.5544(best: 3.5540), Xent 1.0621, Loss 4.0855, Error 0.2286(best: 0.2211)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15240 | Time 27.7070(27.6808) | Bit/dim 3.5829(3.5458) | Xent 0.0678(0.0925) | Loss 3.6168(3.5921) | Error 0.0222(0.0325) Steps 1126(1135.19) | Grad Norm 1.3890(1.5670) | Total Time 14.00(14.00)\n",
      "Iter 15250 | Time 27.2811(27.6845) | Bit/dim 3.5290(3.5455) | Xent 0.0895(0.0931) | Loss 3.5738(3.5921) | Error 0.0378(0.0326) Steps 1132(1134.65) | Grad Norm 2.1815(1.5859) | Total Time 14.00(14.00)\n",
      "Iter 15260 | Time 27.4550(27.7265) | Bit/dim 3.5606(3.5454) | Xent 0.1196(0.0933) | Loss 3.6204(3.5921) | Error 0.0433(0.0330) Steps 1150(1134.91) | Grad Norm 1.8846(1.5927) | Total Time 14.00(14.00)\n",
      "Iter 15270 | Time 27.8442(27.7587) | Bit/dim 3.5323(3.5456) | Xent 0.0716(0.0911) | Loss 3.5681(3.5911) | Error 0.0289(0.0318) Steps 1144(1135.18) | Grad Norm 1.3400(1.6062) | Total Time 14.00(14.00)\n",
      "Iter 15280 | Time 27.3827(27.7810) | Bit/dim 3.5589(3.5434) | Xent 0.0917(0.0912) | Loss 3.6047(3.5890) | Error 0.0333(0.0315) Steps 1138(1136.47) | Grad Norm 1.4797(1.5965) | Total Time 14.00(14.00)\n",
      "Iter 15290 | Time 27.3136(27.7086) | Bit/dim 3.5511(3.5459) | Xent 0.1113(0.0931) | Loss 3.6068(3.5924) | Error 0.0422(0.0321) Steps 1120(1136.43) | Grad Norm 1.3691(1.5786) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0278 | Time 131.4330, Epoch Time 1675.6544(1638.8059), Bit/dim 3.5549(best: 3.5540), Xent 1.0787, Loss 4.0943, Error 0.2283(best: 0.2211)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15300 | Time 27.6465(27.6929) | Bit/dim 3.5741(3.5476) | Xent 0.0889(0.0909) | Loss 3.6186(3.5930) | Error 0.0378(0.0316) Steps 1138(1134.97) | Grad Norm 1.5365(1.5870) | Total Time 14.00(14.00)\n",
      "Iter 15310 | Time 28.1322(27.7160) | Bit/dim 3.5312(3.5457) | Xent 0.0894(0.0905) | Loss 3.5759(3.5909) | Error 0.0344(0.0311) Steps 1150(1136.32) | Grad Norm 1.4542(1.5907) | Total Time 14.00(14.00)\n",
      "Iter 15320 | Time 27.2997(27.6304) | Bit/dim 3.5546(3.5455) | Xent 0.0847(0.0897) | Loss 3.5970(3.5904) | Error 0.0289(0.0303) Steps 1126(1136.07) | Grad Norm 1.4096(1.5562) | Total Time 14.00(14.00)\n",
      "Iter 15330 | Time 27.5111(27.6281) | Bit/dim 3.5428(3.5456) | Xent 0.0897(0.0920) | Loss 3.5876(3.5916) | Error 0.0344(0.0317) Steps 1126(1133.70) | Grad Norm 2.0180(1.5878) | Total Time 14.00(14.00)\n",
      "Iter 15340 | Time 27.4755(27.5663) | Bit/dim 3.5565(3.5441) | Xent 0.0943(0.0923) | Loss 3.6037(3.5903) | Error 0.0322(0.0321) Steps 1138(1134.51) | Grad Norm 1.4309(1.6242) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0279 | Time 133.1346, Epoch Time 1667.2150(1639.6582), Bit/dim 3.5533(best: 3.5540), Xent 1.0624, Loss 4.0845, Error 0.2285(best: 0.2211)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15350 | Time 27.5736(27.5314) | Bit/dim 3.5796(3.5449) | Xent 0.0655(0.0910) | Loss 3.6123(3.5904) | Error 0.0267(0.0316) Steps 1126(1132.80) | Grad Norm 1.3122(1.5994) | Total Time 14.00(14.00)\n",
      "Iter 15360 | Time 27.2226(27.5110) | Bit/dim 3.5423(3.5446) | Xent 0.1045(0.0904) | Loss 3.5945(3.5898) | Error 0.0400(0.0313) Steps 1144(1132.88) | Grad Norm 2.4734(1.6092) | Total Time 14.00(14.00)\n",
      "Iter 15370 | Time 27.0737(27.5158) | Bit/dim 3.5552(3.5424) | Xent 0.0852(0.0902) | Loss 3.5978(3.5875) | Error 0.0322(0.0316) Steps 1138(1133.17) | Grad Norm 1.3438(1.6438) | Total Time 14.00(14.00)\n",
      "Iter 15380 | Time 27.1901(27.4864) | Bit/dim 3.5444(3.5431) | Xent 0.1010(0.0909) | Loss 3.5949(3.5885) | Error 0.0400(0.0318) Steps 1120(1132.11) | Grad Norm 1.9391(1.6354) | Total Time 14.00(14.00)\n",
      "Iter 15390 | Time 27.1710(27.4870) | Bit/dim 3.5385(3.5447) | Xent 0.0961(0.0930) | Loss 3.5865(3.5912) | Error 0.0300(0.0322) Steps 1132(1133.21) | Grad Norm 2.1283(1.6956) | Total Time 14.00(14.00)\n",
      "Iter 15400 | Time 27.5216(27.4895) | Bit/dim 3.5576(3.5455) | Xent 0.1257(0.0935) | Loss 3.6204(3.5922) | Error 0.0378(0.0324) Steps 1144(1133.39) | Grad Norm 2.7385(1.7647) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0280 | Time 129.9369, Epoch Time 1659.5364(1640.2545), Bit/dim 3.5572(best: 3.5533), Xent 1.0914, Loss 4.1029, Error 0.2297(best: 0.2211)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15410 | Time 27.7048(27.4809) | Bit/dim 3.5086(3.5438) | Xent 0.1036(0.0947) | Loss 3.5604(3.5911) | Error 0.0389(0.0329) Steps 1132(1132.85) | Grad Norm 2.3472(1.8110) | Total Time 14.00(14.00)\n",
      "Iter 15420 | Time 26.9204(27.5116) | Bit/dim 3.5422(3.5447) | Xent 0.0969(0.0940) | Loss 3.5906(3.5917) | Error 0.0344(0.0329) Steps 1138(1134.57) | Grad Norm 1.5107(1.7984) | Total Time 14.00(14.00)\n",
      "Iter 15430 | Time 27.5746(27.5577) | Bit/dim 3.5401(3.5463) | Xent 0.1007(0.0951) | Loss 3.5904(3.5938) | Error 0.0311(0.0329) Steps 1126(1133.44) | Grad Norm 1.4332(1.8117) | Total Time 14.00(14.00)\n",
      "Iter 15440 | Time 27.9998(27.5975) | Bit/dim 3.5486(3.5453) | Xent 0.0969(0.0943) | Loss 3.5971(3.5925) | Error 0.0300(0.0327) Steps 1132(1133.43) | Grad Norm 2.1340(1.8309) | Total Time 14.00(14.00)\n",
      "Iter 15450 | Time 27.1202(27.5904) | Bit/dim 3.5320(3.5447) | Xent 0.0874(0.0934) | Loss 3.5757(3.5914) | Error 0.0278(0.0327) Steps 1138(1132.09) | Grad Norm 1.5283(1.8293) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0281 | Time 131.6943, Epoch Time 1666.6875(1641.0475), Bit/dim 3.5557(best: 3.5533), Xent 1.0832, Loss 4.0973, Error 0.2272(best: 0.2211)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15460 | Time 28.2790(27.5797) | Bit/dim 3.5474(3.5433) | Xent 0.0974(0.0953) | Loss 3.5961(3.5910) | Error 0.0356(0.0333) Steps 1156(1131.42) | Grad Norm 1.4023(1.8422) | Total Time 14.00(14.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_cifar.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_run2 --resume ../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_run2/epoch_250_checkpt.pth --seed 2 --lr 0.0001 --conditional True --controlled_tol False --train_mode semisup --log_freq 10 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
