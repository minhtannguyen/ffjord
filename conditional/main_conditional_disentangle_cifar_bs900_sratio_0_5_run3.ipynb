{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='4,5,6,7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_cifar.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "if args.controlled_tol:\n",
      "    import lib.odenvp_conditional_tol as odenvp\n",
      "else:\n",
      "    import lib.odenvp_conditional as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            loss.backward()\n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, condition_ratio=0.5, conditional=True, controlled_tol=False, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.0, imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.0001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume='../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_run3/current_checkpt.pth', rtol=1e-05, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_run3', seed=3, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      ")\n",
      "Number of trainable parameters: 1414198\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc.py:286: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "Iter 13590 | Time 26.2596(26.6912) | Bit/dim 3.5524(3.5529) | Xent 0.0610(0.0775) | Loss 3.5829(3.5916) | Error 0.0200(0.0285) Steps 1030(1049.42) | Grad Norm 1.7404(3.4200) | Total Time 14.00(14.00)\n",
      "Iter 13600 | Time 26.1098(26.4732) | Bit/dim 3.5762(3.5486) | Xent 0.0469(0.0687) | Loss 3.5996(3.5829) | Error 0.0156(0.0249) Steps 1006(1042.38) | Grad Norm 1.5190(3.0400) | Total Time 14.00(14.00)\n",
      "Iter 13610 | Time 26.1039(26.2740) | Bit/dim 3.5445(3.5451) | Xent 0.0274(0.0595) | Loss 3.5582(3.5748) | Error 0.0111(0.0216) Steps 1060(1038.80) | Grad Norm 0.9556(2.5844) | Total Time 14.00(14.00)\n",
      "Iter 13620 | Time 25.2664(26.1151) | Bit/dim 3.5378(3.5431) | Xent 0.0290(0.0518) | Loss 3.5524(3.5690) | Error 0.0089(0.0184) Steps 1036(1036.90) | Grad Norm 1.0036(2.2167) | Total Time 14.00(14.00)\n",
      "Iter 13630 | Time 25.9368(26.0442) | Bit/dim 3.5290(3.5376) | Xent 0.0271(0.0454) | Loss 3.5425(3.5603) | Error 0.0089(0.0158) Steps 1048(1035.80) | Grad Norm 0.8649(1.9242) | Total Time 14.00(14.00)\n",
      "Iter 13640 | Time 25.9990(25.9425) | Bit/dim 3.5336(3.5337) | Xent 0.0280(0.0410) | Loss 3.5476(3.5542) | Error 0.0089(0.0138) Steps 1036(1031.12) | Grad Norm 0.9181(1.6561) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0248 | Time 124.2490, Epoch Time 1580.5909(1507.5830), Bit/dim 3.5313(best: inf), Xent 1.7910, Loss 4.4268, Error 0.2671(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13650 | Time 25.7091(25.8850) | Bit/dim 3.5028(3.5321) | Xent 0.0316(0.0374) | Loss 3.5186(3.5508) | Error 0.0078(0.0122) Steps 1030(1028.68) | Grad Norm 1.0603(1.4722) | Total Time 14.00(14.00)\n",
      "Iter 13660 | Time 25.8959(25.9167) | Bit/dim 3.5683(3.5315) | Xent 0.0257(0.0347) | Loss 3.5811(3.5488) | Error 0.0056(0.0109) Steps 1012(1029.13) | Grad Norm 0.8424(1.3161) | Total Time 14.00(14.00)\n",
      "Iter 13670 | Time 25.7378(25.9526) | Bit/dim 3.5255(3.5272) | Xent 0.0270(0.0330) | Loss 3.5390(3.5436) | Error 0.0056(0.0101) Steps 1048(1028.97) | Grad Norm 0.7279(1.2107) | Total Time 14.00(14.00)\n",
      "Iter 13680 | Time 26.1095(25.9947) | Bit/dim 3.5295(3.5270) | Xent 0.0246(0.0311) | Loss 3.5418(3.5425) | Error 0.0067(0.0094) Steps 1048(1033.02) | Grad Norm 0.8866(1.1216) | Total Time 14.00(14.00)\n",
      "Iter 13690 | Time 26.6478(26.0448) | Bit/dim 3.5098(3.5269) | Xent 0.0209(0.0296) | Loss 3.5202(3.5417) | Error 0.0044(0.0086) Steps 1054(1035.44) | Grad Norm 0.6587(1.0510) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0249 | Time 120.2353, Epoch Time 1568.4506(1509.4090), Bit/dim 3.5283(best: 3.5313), Xent 1.7969, Loss 4.4268, Error 0.2631(best: 0.2671)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13700 | Time 25.7617(26.0665) | Bit/dim 3.5369(3.5254) | Xent 0.0196(0.0281) | Loss 3.5467(3.5395) | Error 0.0022(0.0079) Steps 1048(1037.44) | Grad Norm 0.7625(0.9885) | Total Time 14.00(14.00)\n",
      "Iter 13710 | Time 27.2513(26.1674) | Bit/dim 3.5379(3.5250) | Xent 0.0202(0.0268) | Loss 3.5480(3.5384) | Error 0.0067(0.0073) Steps 1054(1040.09) | Grad Norm 0.7796(0.9489) | Total Time 14.00(14.00)\n",
      "Iter 13720 | Time 25.8445(26.1946) | Bit/dim 3.5172(3.5257) | Xent 0.0228(0.0258) | Loss 3.5286(3.5386) | Error 0.0033(0.0068) Steps 1048(1043.55) | Grad Norm 0.8497(0.9208) | Total Time 14.00(14.00)\n",
      "Iter 13730 | Time 26.1360(26.1726) | Bit/dim 3.5622(3.5260) | Xent 0.0348(0.0250) | Loss 3.5796(3.5385) | Error 0.0122(0.0067) Steps 1060(1043.81) | Grad Norm 1.0558(0.8890) | Total Time 14.00(14.00)\n",
      "Iter 13740 | Time 25.8286(26.1602) | Bit/dim 3.4854(3.5229) | Xent 0.0197(0.0243) | Loss 3.4952(3.5350) | Error 0.0022(0.0063) Steps 1030(1043.72) | Grad Norm 1.0614(0.8670) | Total Time 14.00(14.00)\n",
      "Iter 13750 | Time 26.6567(26.2175) | Bit/dim 3.5192(3.5233) | Xent 0.0226(0.0241) | Loss 3.5305(3.5354) | Error 0.0067(0.0064) Steps 1054(1044.21) | Grad Norm 0.8455(0.8461) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0250 | Time 121.3619, Epoch Time 1584.9619(1511.6756), Bit/dim 3.5262(best: 3.5283), Xent 1.8006, Loss 4.4265, Error 0.2680(best: 0.2631)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13760 | Time 26.5094(26.2770) | Bit/dim 3.5115(3.5234) | Xent 0.0233(0.0236) | Loss 3.5232(3.5352) | Error 0.0067(0.0063) Steps 1060(1045.30) | Grad Norm 0.7596(0.8244) | Total Time 14.00(14.00)\n",
      "Iter 13770 | Time 27.0514(26.2793) | Bit/dim 3.5118(3.5227) | Xent 0.0169(0.0234) | Loss 3.5202(3.5343) | Error 0.0033(0.0061) Steps 1078(1046.30) | Grad Norm 0.7221(0.8197) | Total Time 14.00(14.00)\n",
      "Iter 13780 | Time 26.5601(26.3487) | Bit/dim 3.4889(3.5207) | Xent 0.0176(0.0233) | Loss 3.4978(3.5323) | Error 0.0067(0.0061) Steps 1030(1047.59) | Grad Norm 0.6763(0.8330) | Total Time 14.00(14.00)\n",
      "Iter 13790 | Time 26.6970(26.3825) | Bit/dim 3.5261(3.5219) | Xent 0.0226(0.0237) | Loss 3.5374(3.5338) | Error 0.0078(0.0064) Steps 1048(1047.83) | Grad Norm 0.7300(0.8546) | Total Time 14.00(14.00)\n",
      "Iter 13800 | Time 26.2129(26.3752) | Bit/dim 3.5609(3.5233) | Xent 0.0177(0.0240) | Loss 3.5698(3.5353) | Error 0.0033(0.0066) Steps 1030(1044.68) | Grad Norm 0.7028(0.8668) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0251 | Time 119.5145, Epoch Time 1590.1949(1514.0312), Bit/dim 3.5263(best: 3.5262), Xent 1.8129, Loss 4.4327, Error 0.2668(best: 0.2631)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13810 | Time 25.8349(26.3251) | Bit/dim 3.5237(3.5198) | Xent 0.0139(0.0239) | Loss 3.5306(3.5317) | Error 0.0044(0.0065) Steps 1048(1043.12) | Grad Norm 0.6989(0.8564) | Total Time 14.00(14.00)\n",
      "Iter 13820 | Time 26.4345(26.3339) | Bit/dim 3.5221(3.5212) | Xent 0.0237(0.0233) | Loss 3.5340(3.5329) | Error 0.0056(0.0063) Steps 1042(1043.35) | Grad Norm 0.8698(0.8392) | Total Time 14.00(14.00)\n",
      "Iter 13830 | Time 25.5510(26.2762) | Bit/dim 3.5150(3.5221) | Xent 0.0190(0.0231) | Loss 3.5245(3.5337) | Error 0.0056(0.0062) Steps 1030(1042.45) | Grad Norm 0.7116(0.8299) | Total Time 14.00(14.00)\n",
      "Iter 13840 | Time 26.8733(26.3129) | Bit/dim 3.4859(3.5207) | Xent 0.0248(0.0236) | Loss 3.4983(3.5325) | Error 0.0067(0.0062) Steps 1024(1042.64) | Grad Norm 0.7933(0.8219) | Total Time 14.00(14.00)\n",
      "Iter 13850 | Time 26.6291(26.3448) | Bit/dim 3.5234(3.5205) | Xent 0.0181(0.0232) | Loss 3.5324(3.5321) | Error 0.0033(0.0060) Steps 1054(1043.68) | Grad Norm 0.9212(0.8280) | Total Time 14.00(14.00)\n",
      "Iter 13860 | Time 25.6693(26.2381) | Bit/dim 3.5607(3.5202) | Xent 0.0260(0.0232) | Loss 3.5737(3.5318) | Error 0.0056(0.0060) Steps 1042(1044.17) | Grad Norm 1.0548(0.8381) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0252 | Time 118.8099, Epoch Time 1579.5589(1515.9970), Bit/dim 3.5244(best: 3.5262), Xent 1.8332, Loss 4.4410, Error 0.2675(best: 0.2631)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13870 | Time 26.7637(26.2680) | Bit/dim 3.5549(3.5208) | Xent 0.0183(0.0225) | Loss 3.5640(3.5321) | Error 0.0044(0.0060) Steps 1066(1044.21) | Grad Norm 0.6806(0.8012) | Total Time 14.00(14.00)\n",
      "Iter 13880 | Time 25.7711(26.2474) | Bit/dim 3.5150(3.5167) | Xent 0.0151(0.0224) | Loss 3.5225(3.5279) | Error 0.0033(0.0061) Steps 1054(1046.64) | Grad Norm 0.5315(0.8086) | Total Time 14.00(14.00)\n",
      "Iter 13890 | Time 26.4495(26.3126) | Bit/dim 3.5214(3.5200) | Xent 0.0182(0.0224) | Loss 3.5305(3.5312) | Error 0.0044(0.0062) Steps 1030(1047.63) | Grad Norm 0.7559(0.8207) | Total Time 14.00(14.00)\n",
      "Iter 13900 | Time 27.8431(26.3996) | Bit/dim 3.5112(3.5195) | Xent 0.0157(0.0216) | Loss 3.5191(3.5303) | Error 0.0033(0.0059) Steps 1060(1048.39) | Grad Norm 0.7339(0.8019) | Total Time 14.00(14.00)\n",
      "Iter 13910 | Time 25.9463(26.3511) | Bit/dim 3.5049(3.5197) | Xent 0.0182(0.0219) | Loss 3.5140(3.5306) | Error 0.0033(0.0060) Steps 1030(1046.81) | Grad Norm 0.7232(0.8284) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0253 | Time 120.0671, Epoch Time 1587.8636(1518.1530), Bit/dim 3.5228(best: 3.5244), Xent 1.8354, Loss 4.4405, Error 0.2706(best: 0.2631)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13920 | Time 25.7222(26.3059) | Bit/dim 3.5296(3.5200) | Xent 0.0173(0.0217) | Loss 3.5382(3.5308) | Error 0.0033(0.0059) Steps 1048(1045.87) | Grad Norm 0.6543(0.8244) | Total Time 14.00(14.00)\n",
      "Iter 13930 | Time 26.6650(26.3592) | Bit/dim 3.5040(3.5177) | Xent 0.0207(0.0212) | Loss 3.5144(3.5283) | Error 0.0056(0.0053) Steps 1036(1046.77) | Grad Norm 0.7531(0.7947) | Total Time 14.00(14.00)\n",
      "Iter 13940 | Time 26.5173(26.3495) | Bit/dim 3.5111(3.5187) | Xent 0.0234(0.0212) | Loss 3.5228(3.5293) | Error 0.0044(0.0055) Steps 1054(1045.84) | Grad Norm 0.7026(0.7965) | Total Time 14.00(14.00)\n",
      "Iter 13950 | Time 26.2398(26.3803) | Bit/dim 3.4996(3.5184) | Xent 0.0235(0.0212) | Loss 3.5114(3.5290) | Error 0.0056(0.0056) Steps 1060(1045.90) | Grad Norm 0.8614(0.7952) | Total Time 14.00(14.00)\n",
      "Iter 13960 | Time 26.4761(26.3624) | Bit/dim 3.4839(3.5192) | Xent 0.0165(0.0211) | Loss 3.4921(3.5298) | Error 0.0022(0.0056) Steps 1042(1046.16) | Grad Norm 0.5847(0.7954) | Total Time 14.00(14.00)\n",
      "Iter 13970 | Time 26.1037(26.3712) | Bit/dim 3.4941(3.5196) | Xent 0.0249(0.0211) | Loss 3.5066(3.5301) | Error 0.0056(0.0056) Steps 1030(1047.25) | Grad Norm 0.8929(0.7934) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0254 | Time 119.6456, Epoch Time 1588.9002(1520.2754), Bit/dim 3.5220(best: 3.5228), Xent 1.8327, Loss 4.4384, Error 0.2691(best: 0.2631)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13980 | Time 26.0018(26.3456) | Bit/dim 3.5238(3.5182) | Xent 0.0226(0.0214) | Loss 3.5351(3.5289) | Error 0.0044(0.0055) Steps 1042(1045.52) | Grad Norm 0.7741(0.7982) | Total Time 14.00(14.00)\n",
      "Iter 13990 | Time 25.9601(26.4063) | Bit/dim 3.5254(3.5170) | Xent 0.0163(0.0215) | Loss 3.5335(3.5277) | Error 0.0022(0.0054) Steps 1036(1046.54) | Grad Norm 0.6348(0.8015) | Total Time 14.00(14.00)\n",
      "Iter 14000 | Time 26.5060(26.3760) | Bit/dim 3.5526(3.5179) | Xent 0.0151(0.0209) | Loss 3.5601(3.5284) | Error 0.0033(0.0052) Steps 1048(1045.45) | Grad Norm 0.5576(0.7854) | Total Time 14.00(14.00)\n",
      "Iter 14010 | Time 26.5028(26.3067) | Bit/dim 3.5401(3.5191) | Xent 0.0265(0.0209) | Loss 3.5534(3.5296) | Error 0.0078(0.0052) Steps 1060(1046.63) | Grad Norm 1.0185(0.7887) | Total Time 14.00(14.00)\n",
      "Iter 14020 | Time 26.4572(26.3169) | Bit/dim 3.5139(3.5186) | Xent 0.0208(0.0209) | Loss 3.5243(3.5290) | Error 0.0044(0.0051) Steps 1024(1044.51) | Grad Norm 0.8672(0.7780) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0255 | Time 119.8890, Epoch Time 1585.0722(1522.2193), Bit/dim 3.5221(best: 3.5220), Xent 1.8530, Loss 4.4486, Error 0.2685(best: 0.2631)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14030 | Time 26.2829(26.2959) | Bit/dim 3.5148(3.5185) | Xent 0.0134(0.0204) | Loss 3.5215(3.5287) | Error 0.0000(0.0049) Steps 1030(1044.13) | Grad Norm 0.5132(0.7631) | Total Time 14.00(14.00)\n",
      "Iter 14040 | Time 25.2362(26.2408) | Bit/dim 3.5403(3.5180) | Xent 0.0140(0.0205) | Loss 3.5473(3.5282) | Error 0.0011(0.0051) Steps 1024(1043.10) | Grad Norm 0.5762(0.7627) | Total Time 14.00(14.00)\n",
      "Iter 14050 | Time 25.7978(26.2399) | Bit/dim 3.5113(3.5190) | Xent 0.0209(0.0208) | Loss 3.5217(3.5294) | Error 0.0056(0.0050) Steps 1060(1043.80) | Grad Norm 0.9983(0.7800) | Total Time 14.00(14.00)\n",
      "Iter 14060 | Time 26.4369(26.2623) | Bit/dim 3.5102(3.5193) | Xent 0.0193(0.0209) | Loss 3.5199(3.5297) | Error 0.0044(0.0051) Steps 1036(1041.73) | Grad Norm 0.7441(0.7741) | Total Time 14.00(14.00)\n",
      "Iter 14070 | Time 25.9616(26.2417) | Bit/dim 3.5253(3.5163) | Xent 0.0216(0.0211) | Loss 3.5361(3.5269) | Error 0.0044(0.0052) Steps 1030(1044.40) | Grad Norm 0.6324(0.7774) | Total Time 14.00(14.00)\n",
      "Iter 14080 | Time 26.6549(26.2566) | Bit/dim 3.5223(3.5164) | Xent 0.0215(0.0211) | Loss 3.5330(3.5269) | Error 0.0044(0.0051) Steps 1060(1046.24) | Grad Norm 0.8159(0.7669) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0256 | Time 119.6095, Epoch Time 1580.3324(1523.9627), Bit/dim 3.5221(best: 3.5220), Xent 1.8314, Loss 4.4378, Error 0.2667(best: 0.2631)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14090 | Time 26.6016(26.2672) | Bit/dim 3.5088(3.5172) | Xent 0.0163(0.0212) | Loss 3.5170(3.5278) | Error 0.0033(0.0051) Steps 1042(1045.39) | Grad Norm 0.8192(0.7778) | Total Time 14.00(14.00)\n",
      "Iter 14100 | Time 26.3110(26.2799) | Bit/dim 3.5078(3.5152) | Xent 0.0262(0.0205) | Loss 3.5209(3.5255) | Error 0.0056(0.0047) Steps 1054(1044.93) | Grad Norm 0.7561(0.7643) | Total Time 14.00(14.00)\n",
      "Iter 14110 | Time 26.7710(26.3869) | Bit/dim 3.4998(3.5151) | Xent 0.0217(0.0203) | Loss 3.5107(3.5252) | Error 0.0078(0.0050) Steps 1066(1046.18) | Grad Norm 0.9640(0.7885) | Total Time 14.00(14.00)\n",
      "Iter 14120 | Time 26.8895(26.4027) | Bit/dim 3.5233(3.5175) | Xent 0.0158(0.0198) | Loss 3.5312(3.5273) | Error 0.0033(0.0047) Steps 1054(1046.36) | Grad Norm 0.8928(0.7791) | Total Time 14.00(14.00)\n",
      "Iter 14130 | Time 26.3630(26.3544) | Bit/dim 3.5091(3.5156) | Xent 0.0249(0.0203) | Loss 3.5216(3.5258) | Error 0.0078(0.0050) Steps 1060(1047.11) | Grad Norm 1.0225(0.7985) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0257 | Time 119.2836, Epoch Time 1588.1280(1525.8877), Bit/dim 3.5207(best: 3.5220), Xent 1.8432, Loss 4.4424, Error 0.2698(best: 0.2631)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14140 | Time 26.3975(26.3124) | Bit/dim 3.5396(3.5176) | Xent 0.0166(0.0198) | Loss 3.5479(3.5276) | Error 0.0011(0.0047) Steps 1066(1048.08) | Grad Norm 0.5174(0.7741) | Total Time 14.00(14.00)\n",
      "Iter 14150 | Time 25.6972(26.2711) | Bit/dim 3.5245(3.5204) | Xent 0.0265(0.0202) | Loss 3.5377(3.5305) | Error 0.0078(0.0050) Steps 1042(1048.17) | Grad Norm 1.0408(0.7634) | Total Time 14.00(14.00)\n",
      "Iter 14160 | Time 26.3056(26.2960) | Bit/dim 3.5271(3.5188) | Xent 0.0215(0.0202) | Loss 3.5378(3.5289) | Error 0.0033(0.0051) Steps 1036(1048.12) | Grad Norm 0.7642(0.7750) | Total Time 14.00(14.00)\n",
      "Iter 14170 | Time 25.7238(26.2987) | Bit/dim 3.4874(3.5161) | Xent 0.0216(0.0203) | Loss 3.4982(3.5262) | Error 0.0056(0.0051) Steps 1054(1047.95) | Grad Norm 0.7921(0.7877) | Total Time 14.00(14.00)\n",
      "Iter 14180 | Time 26.4945(26.3316) | Bit/dim 3.4881(3.5137) | Xent 0.0206(0.0200) | Loss 3.4984(3.5237) | Error 0.0067(0.0050) Steps 1060(1049.58) | Grad Norm 0.7669(0.7774) | Total Time 14.00(14.00)\n",
      "Iter 14190 | Time 27.7220(26.3813) | Bit/dim 3.5274(3.5151) | Xent 0.0196(0.0195) | Loss 3.5372(3.5248) | Error 0.0056(0.0046) Steps 1054(1047.73) | Grad Norm 0.7592(0.7572) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0258 | Time 120.3407, Epoch Time 1586.9077(1527.7183), Bit/dim 3.5206(best: 3.5207), Xent 1.8746, Loss 4.4578, Error 0.2688(best: 0.2631)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14200 | Time 26.2639(26.3301) | Bit/dim 3.5097(3.5158) | Xent 0.0142(0.0195) | Loss 3.5168(3.5255) | Error 0.0011(0.0045) Steps 1048(1048.21) | Grad Norm 0.5728(0.7755) | Total Time 14.00(14.00)\n",
      "Iter 14210 | Time 26.5066(26.3594) | Bit/dim 3.5375(3.5178) | Xent 0.0126(0.0199) | Loss 3.5438(3.5277) | Error 0.0022(0.0047) Steps 1066(1049.62) | Grad Norm 0.6515(0.8016) | Total Time 14.00(14.00)\n",
      "Iter 14220 | Time 25.7133(26.2962) | Bit/dim 3.4874(3.5164) | Xent 0.0273(0.0202) | Loss 3.5011(3.5266) | Error 0.0056(0.0048) Steps 1036(1047.75) | Grad Norm 0.9531(0.8221) | Total Time 14.00(14.00)\n",
      "Iter 14230 | Time 26.6628(26.3256) | Bit/dim 3.5056(3.5158) | Xent 0.0151(0.0194) | Loss 3.5131(3.5256) | Error 0.0011(0.0043) Steps 1036(1045.67) | Grad Norm 0.6298(0.7877) | Total Time 14.00(14.00)\n",
      "Iter 14240 | Time 26.6554(26.4292) | Bit/dim 3.5455(3.5133) | Xent 0.0231(0.0196) | Loss 3.5571(3.5231) | Error 0.0078(0.0044) Steps 1030(1045.95) | Grad Norm 1.0413(0.7933) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0259 | Time 120.7047, Epoch Time 1590.6941(1529.6076), Bit/dim 3.5205(best: 3.5206), Xent 1.8707, Loss 4.4559, Error 0.2732(best: 0.2631)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14250 | Time 26.7181(26.4703) | Bit/dim 3.5580(3.5163) | Xent 0.0264(0.0201) | Loss 3.5712(3.5264) | Error 0.0078(0.0047) Steps 1066(1048.02) | Grad Norm 1.0028(0.8279) | Total Time 14.00(14.00)\n",
      "Iter 14260 | Time 26.5852(26.3880) | Bit/dim 3.4910(3.5144) | Xent 0.0229(0.0200) | Loss 3.5024(3.5244) | Error 0.0056(0.0048) Steps 1054(1047.05) | Grad Norm 1.1139(0.8343) | Total Time 14.00(14.00)\n",
      "Iter 14270 | Time 26.6784(26.3765) | Bit/dim 3.5050(3.5148) | Xent 0.0193(0.0199) | Loss 3.5146(3.5247) | Error 0.0033(0.0049) Steps 1048(1047.18) | Grad Norm 0.8142(0.8195) | Total Time 14.00(14.00)\n",
      "Iter 14280 | Time 26.5018(26.3828) | Bit/dim 3.5396(3.5125) | Xent 0.0156(0.0195) | Loss 3.5474(3.5222) | Error 0.0011(0.0045) Steps 1042(1047.77) | Grad Norm 0.6557(0.7976) | Total Time 14.00(14.00)\n",
      "Iter 14290 | Time 25.9940(26.3392) | Bit/dim 3.4644(3.5134) | Xent 0.0186(0.0200) | Loss 3.4737(3.5235) | Error 0.0056(0.0050) Steps 1054(1047.02) | Grad Norm 0.7600(0.8044) | Total Time 14.00(14.00)\n",
      "Iter 14300 | Time 26.4825(26.3423) | Bit/dim 3.5604(3.5149) | Xent 0.0219(0.0198) | Loss 3.5714(3.5248) | Error 0.0078(0.0050) Steps 1072(1046.47) | Grad Norm 1.1259(0.8150) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0260 | Time 119.4619, Epoch Time 1584.7984(1531.2633), Bit/dim 3.5193(best: 3.5205), Xent 1.8780, Loss 4.4583, Error 0.2693(best: 0.2631)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14310 | Time 26.5544(26.3553) | Bit/dim 3.5248(3.5134) | Xent 0.0195(0.0200) | Loss 3.5345(3.5234) | Error 0.0044(0.0051) Steps 1060(1046.50) | Grad Norm 0.8006(0.8151) | Total Time 14.00(14.00)\n",
      "Iter 14320 | Time 26.2798(26.3327) | Bit/dim 3.4929(3.5155) | Xent 0.0218(0.0198) | Loss 3.5038(3.5254) | Error 0.0033(0.0050) Steps 1054(1045.16) | Grad Norm 0.8358(0.8236) | Total Time 14.00(14.00)\n",
      "Iter 14330 | Time 27.1709(26.3407) | Bit/dim 3.4691(3.5147) | Xent 0.0144(0.0199) | Loss 3.4763(3.5246) | Error 0.0033(0.0049) Steps 1030(1043.69) | Grad Norm 0.6099(0.8166) | Total Time 14.00(14.00)\n",
      "Iter 14340 | Time 26.3262(26.3324) | Bit/dim 3.5131(3.5130) | Xent 0.0154(0.0194) | Loss 3.5208(3.5227) | Error 0.0022(0.0044) Steps 1054(1045.44) | Grad Norm 0.6162(0.7933) | Total Time 14.00(14.00)\n",
      "Iter 14350 | Time 26.0454(26.3265) | Bit/dim 3.5078(3.5122) | Xent 0.0156(0.0195) | Loss 3.5156(3.5219) | Error 0.0022(0.0046) Steps 1054(1045.84) | Grad Norm 0.8089(0.8069) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0261 | Time 120.2812, Epoch Time 1586.6557(1532.9251), Bit/dim 3.5193(best: 3.5193), Xent 1.8650, Loss 4.4517, Error 0.2696(best: 0.2631)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14360 | Time 25.4951(26.3334) | Bit/dim 3.5422(3.5137) | Xent 0.0154(0.0189) | Loss 3.5499(3.5232) | Error 0.0044(0.0044) Steps 1048(1045.21) | Grad Norm 0.5136(0.7841) | Total Time 14.00(14.00)\n",
      "Iter 14370 | Time 27.3674(26.3669) | Bit/dim 3.5112(3.5156) | Xent 0.0217(0.0188) | Loss 3.5220(3.5251) | Error 0.0033(0.0045) Steps 1054(1046.40) | Grad Norm 0.9035(0.8016) | Total Time 14.00(14.00)\n",
      "Iter 14380 | Time 26.3561(26.4355) | Bit/dim 3.5352(3.5161) | Xent 0.0163(0.0191) | Loss 3.5433(3.5257) | Error 0.0033(0.0044) Steps 1048(1046.04) | Grad Norm 0.8702(0.8144) | Total Time 14.00(14.00)\n",
      "Iter 14390 | Time 26.7438(26.4168) | Bit/dim 3.5064(3.5154) | Xent 0.0244(0.0190) | Loss 3.5186(3.5249) | Error 0.0078(0.0046) Steps 1072(1045.08) | Grad Norm 0.8982(0.8125) | Total Time 14.00(14.00)\n",
      "Iter 14400 | Time 25.8914(26.4354) | Bit/dim 3.5513(3.5167) | Xent 0.0212(0.0187) | Loss 3.5619(3.5261) | Error 0.0044(0.0042) Steps 1048(1047.74) | Grad Norm 0.7730(0.8005) | Total Time 14.00(14.00)\n",
      "Iter 14410 | Time 26.1985(26.4003) | Bit/dim 3.5373(3.5132) | Xent 0.0153(0.0188) | Loss 3.5449(3.5226) | Error 0.0033(0.0043) Steps 1018(1046.39) | Grad Norm 0.7036(0.8460) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0262 | Time 119.9002, Epoch Time 1591.4326(1534.6803), Bit/dim 3.5187(best: 3.5193), Xent 1.8759, Loss 4.4566, Error 0.2700(best: 0.2631)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14420 | Time 26.5254(26.3976) | Bit/dim 3.5106(3.5116) | Xent 0.0150(0.0184) | Loss 3.5181(3.5208) | Error 0.0011(0.0039) Steps 1048(1047.78) | Grad Norm 0.7002(0.8301) | Total Time 14.00(14.00)\n",
      "Iter 14430 | Time 26.5666(26.3920) | Bit/dim 3.5299(3.5129) | Xent 0.0241(0.0190) | Loss 3.5420(3.5224) | Error 0.0056(0.0042) Steps 1030(1047.23) | Grad Norm 1.1190(0.8410) | Total Time 14.00(14.00)\n",
      "Iter 14440 | Time 26.3243(26.4001) | Bit/dim 3.4883(3.5108) | Xent 0.0299(0.0189) | Loss 3.5032(3.5202) | Error 0.0067(0.0041) Steps 1030(1047.81) | Grad Norm 0.9894(0.8179) | Total Time 14.00(14.00)\n",
      "Iter 14450 | Time 26.1290(26.4252) | Bit/dim 3.5198(3.5136) | Xent 0.0160(0.0191) | Loss 3.5278(3.5231) | Error 0.0033(0.0045) Steps 1054(1045.80) | Grad Norm 0.7002(0.8069) | Total Time 14.00(14.00)\n",
      "Iter 14460 | Time 26.7965(26.4310) | Bit/dim 3.5427(3.5152) | Xent 0.0261(0.0193) | Loss 3.5557(3.5249) | Error 0.0100(0.0048) Steps 1060(1047.68) | Grad Norm 1.2180(0.8306) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0263 | Time 120.0764, Epoch Time 1590.2138(1536.3463), Bit/dim 3.5183(best: 3.5187), Xent 1.8980, Loss 4.4673, Error 0.2721(best: 0.2631)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14470 | Time 26.8267(26.3572) | Bit/dim 3.4914(3.5127) | Xent 0.0184(0.0194) | Loss 3.5006(3.5225) | Error 0.0056(0.0048) Steps 1054(1046.02) | Grad Norm 0.8354(0.8342) | Total Time 14.00(14.00)\n",
      "Iter 14480 | Time 25.9693(26.3186) | Bit/dim 3.5294(3.5132) | Xent 0.0126(0.0188) | Loss 3.5358(3.5226) | Error 0.0011(0.0046) Steps 1048(1044.30) | Grad Norm 0.5851(0.8343) | Total Time 14.00(14.00)\n",
      "Iter 14490 | Time 26.3149(26.3152) | Bit/dim 3.5484(3.5131) | Xent 0.0202(0.0189) | Loss 3.5585(3.5225) | Error 0.0033(0.0043) Steps 1012(1041.76) | Grad Norm 0.7385(0.8190) | Total Time 14.00(14.00)\n",
      "Iter 14500 | Time 25.7348(26.3619) | Bit/dim 3.5125(3.5139) | Xent 0.0203(0.0191) | Loss 3.5226(3.5235) | Error 0.0067(0.0043) Steps 1048(1042.62) | Grad Norm 0.9273(0.8274) | Total Time 14.00(14.00)\n",
      "Iter 14510 | Time 25.5853(26.3473) | Bit/dim 3.5411(3.5149) | Xent 0.0151(0.0192) | Loss 3.5486(3.5245) | Error 0.0011(0.0044) Steps 1042(1041.23) | Grad Norm 0.6963(0.8315) | Total Time 14.00(14.00)\n",
      "Iter 14520 | Time 26.7084(26.3240) | Bit/dim 3.5069(3.5140) | Xent 0.0182(0.0186) | Loss 3.5160(3.5233) | Error 0.0044(0.0041) Steps 1036(1041.43) | Grad Norm 0.7206(0.7892) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0264 | Time 120.0944, Epoch Time 1584.5955(1537.7938), Bit/dim 3.5176(best: 3.5183), Xent 1.9027, Loss 4.4690, Error 0.2698(best: 0.2631)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14530 | Time 26.6033(26.3693) | Bit/dim 3.5060(3.5129) | Xent 0.0314(0.0187) | Loss 3.5217(3.5222) | Error 0.0133(0.0042) Steps 1060(1042.05) | Grad Norm 0.9225(0.7846) | Total Time 14.00(14.00)\n",
      "Iter 14540 | Time 25.9933(26.3950) | Bit/dim 3.4747(3.5125) | Xent 0.0145(0.0182) | Loss 3.4819(3.5216) | Error 0.0022(0.0040) Steps 1042(1042.48) | Grad Norm 0.8875(0.7733) | Total Time 14.00(14.00)\n",
      "Iter 14550 | Time 26.3359(26.3597) | Bit/dim 3.4889(3.5135) | Xent 0.0158(0.0180) | Loss 3.4968(3.5225) | Error 0.0033(0.0040) Steps 1042(1042.71) | Grad Norm 0.6337(0.7785) | Total Time 14.00(14.00)\n",
      "Iter 14560 | Time 26.5513(26.3261) | Bit/dim 3.4588(3.5140) | Xent 0.0199(0.0182) | Loss 3.4687(3.5231) | Error 0.0067(0.0040) Steps 1060(1043.37) | Grad Norm 0.9041(0.8171) | Total Time 14.00(14.00)\n",
      "Iter 14570 | Time 26.0460(26.3372) | Bit/dim 3.4842(3.5104) | Xent 0.0174(0.0183) | Loss 3.4929(3.5196) | Error 0.0033(0.0039) Steps 1054(1045.56) | Grad Norm 0.8788(0.8116) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0265 | Time 119.9127, Epoch Time 1586.1042(1539.2431), Bit/dim 3.5170(best: 3.5176), Xent 1.9039, Loss 4.4690, Error 0.2715(best: 0.2631)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14580 | Time 26.3593(26.2904) | Bit/dim 3.5170(3.5121) | Xent 0.0213(0.0183) | Loss 3.5277(3.5212) | Error 0.0056(0.0041) Steps 1030(1046.07) | Grad Norm 1.1529(0.8336) | Total Time 14.00(14.00)\n",
      "Iter 14590 | Time 26.3590(26.3294) | Bit/dim 3.5328(3.5123) | Xent 0.0179(0.0182) | Loss 3.5417(3.5213) | Error 0.0022(0.0040) Steps 1042(1045.73) | Grad Norm 0.9922(0.8220) | Total Time 14.00(14.00)\n",
      "Iter 14600 | Time 26.1103(26.3179) | Bit/dim 3.5448(3.5119) | Xent 0.0138(0.0179) | Loss 3.5517(3.5209) | Error 0.0000(0.0037) Steps 1042(1046.28) | Grad Norm 0.6915(0.8069) | Total Time 14.00(14.00)\n",
      "Iter 14610 | Time 25.9537(26.3073) | Bit/dim 3.5239(3.5101) | Xent 0.0136(0.0182) | Loss 3.5307(3.5192) | Error 0.0033(0.0040) Steps 1048(1045.98) | Grad Norm 0.5210(0.7991) | Total Time 14.00(14.00)\n",
      "Iter 14620 | Time 26.1568(26.2818) | Bit/dim 3.5158(3.5115) | Xent 0.0193(0.0183) | Loss 3.5255(3.5206) | Error 0.0067(0.0041) Steps 1048(1045.87) | Grad Norm 0.9040(0.7871) | Total Time 14.00(14.00)\n",
      "Iter 14630 | Time 26.2684(26.2663) | Bit/dim 3.5476(3.5122) | Xent 0.0204(0.0189) | Loss 3.5578(3.5216) | Error 0.0044(0.0043) Steps 1060(1046.29) | Grad Norm 0.8718(0.8100) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0266 | Time 120.5517, Epoch Time 1583.9977(1540.5857), Bit/dim 3.5175(best: 3.5170), Xent 1.9030, Loss 4.4690, Error 0.2713(best: 0.2631)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14640 | Time 26.7530(26.3291) | Bit/dim 3.5190(3.5116) | Xent 0.0126(0.0188) | Loss 3.5253(3.5210) | Error 0.0033(0.0043) Steps 1024(1045.92) | Grad Norm 0.7095(0.8101) | Total Time 14.00(14.00)\n",
      "Iter 14650 | Time 26.6054(26.3057) | Bit/dim 3.5087(3.5125) | Xent 0.0243(0.0186) | Loss 3.5208(3.5218) | Error 0.0067(0.0043) Steps 1060(1045.70) | Grad Norm 0.9324(0.8032) | Total Time 14.00(14.00)\n",
      "Iter 14660 | Time 26.3203(26.2954) | Bit/dim 3.5523(3.5125) | Xent 0.0185(0.0186) | Loss 3.5616(3.5218) | Error 0.0044(0.0043) Steps 1054(1047.23) | Grad Norm 0.8170(0.8092) | Total Time 14.00(14.00)\n",
      "Iter 14670 | Time 27.0813(26.3122) | Bit/dim 3.5169(3.5118) | Xent 0.0157(0.0184) | Loss 3.5248(3.5210) | Error 0.0056(0.0042) Steps 1030(1047.08) | Grad Norm 0.6209(0.8128) | Total Time 14.00(14.00)\n",
      "Iter 14680 | Time 26.4312(26.2545) | Bit/dim 3.5106(3.5118) | Xent 0.0196(0.0189) | Loss 3.5203(3.5212) | Error 0.0044(0.0044) Steps 1060(1046.73) | Grad Norm 0.6878(0.8056) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0267 | Time 118.9593, Epoch Time 1584.9440(1541.9165), Bit/dim 3.5159(best: 3.5170), Xent 1.9209, Loss 4.4763, Error 0.2705(best: 0.2631)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14690 | Time 26.4864(26.3314) | Bit/dim 3.5049(3.5114) | Xent 0.0225(0.0185) | Loss 3.5162(3.5206) | Error 0.0067(0.0041) Steps 1030(1048.13) | Grad Norm 1.1700(0.8095) | Total Time 14.00(14.00)\n",
      "Iter 14700 | Time 25.7881(26.2869) | Bit/dim 3.5183(3.5106) | Xent 0.0200(0.0182) | Loss 3.5283(3.5197) | Error 0.0056(0.0042) Steps 1042(1046.16) | Grad Norm 0.8006(0.8002) | Total Time 14.00(14.00)\n",
      "Iter 14710 | Time 26.6860(26.3313) | Bit/dim 3.5306(3.5095) | Xent 0.0240(0.0186) | Loss 3.5426(3.5188) | Error 0.0056(0.0042) Steps 1048(1047.42) | Grad Norm 0.8062(0.8308) | Total Time 14.00(14.00)\n",
      "Iter 14720 | Time 25.6508(26.3164) | Bit/dim 3.4991(3.5098) | Xent 0.0174(0.0186) | Loss 3.5078(3.5191) | Error 0.0044(0.0041) Steps 1054(1048.22) | Grad Norm 0.8601(0.8458) | Total Time 14.00(14.00)\n",
      "Iter 14730 | Time 26.3434(26.2806) | Bit/dim 3.5124(3.5113) | Xent 0.0199(0.0188) | Loss 3.5223(3.5207) | Error 0.0044(0.0042) Steps 1042(1050.30) | Grad Norm 0.8079(0.8560) | Total Time 14.00(14.00)\n",
      "Iter 14740 | Time 26.4996(26.3030) | Bit/dim 3.4940(3.5126) | Xent 0.0149(0.0186) | Loss 3.5014(3.5218) | Error 0.0022(0.0042) Steps 1024(1049.31) | Grad Norm 0.5830(0.8383) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0268 | Time 119.2748, Epoch Time 1583.3734(1543.1602), Bit/dim 3.5168(best: 3.5159), Xent 1.9366, Loss 4.4851, Error 0.2695(best: 0.2631)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14750 | Time 25.8346(26.3294) | Bit/dim 3.4858(3.5111) | Xent 0.0208(0.0190) | Loss 3.4962(3.5206) | Error 0.0056(0.0043) Steps 1030(1048.02) | Grad Norm 0.9109(0.8714) | Total Time 14.00(14.00)\n",
      "Iter 14760 | Time 26.6657(26.3590) | Bit/dim 3.5211(3.5073) | Xent 0.0163(0.0183) | Loss 3.5292(3.5164) | Error 0.0044(0.0041) Steps 1030(1046.81) | Grad Norm 0.8368(0.8517) | Total Time 14.00(14.00)\n",
      "Iter 14770 | Time 26.3277(26.2991) | Bit/dim 3.5166(3.5115) | Xent 0.0243(0.0179) | Loss 3.5287(3.5205) | Error 0.0067(0.0040) Steps 1012(1043.35) | Grad Norm 1.2017(0.8494) | Total Time 14.00(14.00)\n",
      "Iter 14780 | Time 26.2024(26.3233) | Bit/dim 3.5335(3.5126) | Xent 0.0334(0.0182) | Loss 3.5502(3.5217) | Error 0.0089(0.0041) Steps 1012(1042.69) | Grad Norm 1.2756(0.8426) | Total Time 14.00(14.00)\n",
      "Iter 14790 | Time 27.0675(26.3204) | Bit/dim 3.5163(3.5137) | Xent 0.0179(0.0183) | Loss 3.5253(3.5229) | Error 0.0033(0.0042) Steps 1042(1041.10) | Grad Norm 0.7300(0.8425) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0269 | Time 119.9647, Epoch Time 1584.4775(1544.3997), Bit/dim 3.5158(best: 3.5159), Xent 1.9307, Loss 4.4811, Error 0.2697(best: 0.2631)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14800 | Time 25.4643(26.2582) | Bit/dim 3.4761(3.5117) | Xent 0.0171(0.0179) | Loss 3.4846(3.5206) | Error 0.0033(0.0040) Steps 1042(1042.73) | Grad Norm 0.8309(0.8286) | Total Time 14.00(14.00)\n",
      "Iter 14810 | Time 25.8428(26.2442) | Bit/dim 3.5276(3.5134) | Xent 0.0221(0.0176) | Loss 3.5386(3.5222) | Error 0.0056(0.0039) Steps 1054(1043.43) | Grad Norm 0.8221(0.8132) | Total Time 14.00(14.00)\n",
      "Iter 14820 | Time 26.2329(26.2296) | Bit/dim 3.5097(3.5122) | Xent 0.0145(0.0173) | Loss 3.5170(3.5208) | Error 0.0022(0.0037) Steps 1042(1044.21) | Grad Norm 0.6769(0.7807) | Total Time 14.00(14.00)\n",
      "Iter 14830 | Time 25.9011(26.1762) | Bit/dim 3.4963(3.5109) | Xent 0.0173(0.0177) | Loss 3.5050(3.5197) | Error 0.0056(0.0038) Steps 1048(1044.71) | Grad Norm 1.0876(0.7948) | Total Time 14.00(14.00)\n",
      "Iter 14840 | Time 26.0527(26.1981) | Bit/dim 3.5404(3.5131) | Xent 0.0254(0.0179) | Loss 3.5531(3.5221) | Error 0.0078(0.0039) Steps 1030(1046.08) | Grad Norm 1.2297(0.7966) | Total Time 14.00(14.00)\n",
      "Iter 14850 | Time 27.2198(26.2614) | Bit/dim 3.4962(3.5111) | Xent 0.0152(0.0176) | Loss 3.5039(3.5200) | Error 0.0033(0.0038) Steps 1030(1043.40) | Grad Norm 0.9937(0.7974) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0270 | Time 120.0300, Epoch Time 1580.4244(1545.4804), Bit/dim 3.5149(best: 3.5158), Xent 1.9344, Loss 4.4821, Error 0.2711(best: 0.2631)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14860 | Time 25.5331(26.2263) | Bit/dim 3.5199(3.5099) | Xent 0.0123(0.0171) | Loss 3.5260(3.5185) | Error 0.0022(0.0036) Steps 1030(1042.70) | Grad Norm 0.8279(0.8025) | Total Time 14.00(14.00)\n",
      "Iter 14870 | Time 26.7917(26.2591) | Bit/dim 3.5013(3.5093) | Xent 0.0126(0.0172) | Loss 3.5076(3.5179) | Error 0.0000(0.0035) Steps 1054(1043.82) | Grad Norm 0.6934(0.8070) | Total Time 14.00(14.00)\n",
      "Iter 14880 | Time 26.0116(26.2387) | Bit/dim 3.4841(3.5102) | Xent 0.0179(0.0172) | Loss 3.4930(3.5188) | Error 0.0044(0.0036) Steps 1036(1042.90) | Grad Norm 0.6540(0.7923) | Total Time 14.00(14.00)\n",
      "Iter 14890 | Time 27.5195(26.3198) | Bit/dim 3.5077(3.5096) | Xent 0.0142(0.0176) | Loss 3.5148(3.5184) | Error 0.0022(0.0038) Steps 1036(1042.79) | Grad Norm 0.6320(0.8060) | Total Time 14.00(14.00)\n",
      "Iter 14900 | Time 26.5631(26.3059) | Bit/dim 3.5172(3.5097) | Xent 0.0119(0.0177) | Loss 3.5231(3.5186) | Error 0.0000(0.0038) Steps 1012(1043.59) | Grad Norm 0.5310(0.7863) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0271 | Time 119.2076, Epoch Time 1582.7404(1546.5982), Bit/dim 3.5150(best: 3.5149), Xent 1.9327, Loss 4.4814, Error 0.2715(best: 0.2631)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14910 | Time 26.6827(26.3322) | Bit/dim 3.5337(3.5128) | Xent 0.0158(0.0177) | Loss 3.5416(3.5217) | Error 0.0078(0.0039) Steps 1060(1044.58) | Grad Norm 0.8499(0.7892) | Total Time 14.00(14.00)\n",
      "Iter 14920 | Time 26.7389(26.3033) | Bit/dim 3.5258(3.5137) | Xent 0.0190(0.0173) | Loss 3.5353(3.5223) | Error 0.0056(0.0036) Steps 1072(1045.19) | Grad Norm 0.8366(0.7929) | Total Time 14.00(14.00)\n",
      "Iter 14930 | Time 26.1441(26.2680) | Bit/dim 3.5322(3.5116) | Xent 0.0230(0.0178) | Loss 3.5437(3.5205) | Error 0.0056(0.0037) Steps 1054(1046.11) | Grad Norm 0.8494(0.8108) | Total Time 14.00(14.00)\n",
      "Iter 14940 | Time 26.7367(26.3605) | Bit/dim 3.4663(3.5085) | Xent 0.0218(0.0179) | Loss 3.4772(3.5175) | Error 0.0056(0.0038) Steps 1060(1046.99) | Grad Norm 1.1546(0.8303) | Total Time 14.00(14.00)\n",
      "Iter 14950 | Time 27.4122(26.4361) | Bit/dim 3.5116(3.5093) | Xent 0.0257(0.0184) | Loss 3.5244(3.5184) | Error 0.0089(0.0041) Steps 1042(1046.94) | Grad Norm 0.9713(0.8546) | Total Time 14.00(14.00)\n",
      "Iter 14960 | Time 26.1621(26.3820) | Bit/dim 3.5118(3.5103) | Xent 0.0190(0.0187) | Loss 3.5213(3.5196) | Error 0.0067(0.0042) Steps 1036(1045.73) | Grad Norm 0.8148(0.8595) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0272 | Time 119.6604, Epoch Time 1589.4557(1547.8840), Bit/dim 3.5153(best: 3.5149), Xent 1.9350, Loss 4.4828, Error 0.2745(best: 0.2631)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14970 | Time 26.2931(26.3906) | Bit/dim 3.5133(3.5113) | Xent 0.0221(0.0185) | Loss 3.5244(3.5205) | Error 0.0056(0.0041) Steps 1060(1045.26) | Grad Norm 0.9242(0.8698) | Total Time 14.00(14.00)\n",
      "Iter 14980 | Time 25.2581(26.3167) | Bit/dim 3.4710(3.5116) | Xent 0.0137(0.0180) | Loss 3.4778(3.5205) | Error 0.0033(0.0039) Steps 1030(1043.03) | Grad Norm 0.7187(0.8401) | Total Time 14.00(14.00)\n",
      "Iter 14990 | Time 26.7797(26.2899) | Bit/dim 3.4752(3.5091) | Xent 0.0223(0.0179) | Loss 3.4863(3.5181) | Error 0.0056(0.0039) Steps 1048(1042.31) | Grad Norm 0.9953(0.8116) | Total Time 14.00(14.00)\n",
      "Iter 15000 | Time 26.6026(26.3441) | Bit/dim 3.5080(3.5103) | Xent 0.0190(0.0180) | Loss 3.5175(3.5193) | Error 0.0033(0.0041) Steps 1054(1043.17) | Grad Norm 1.1658(0.8380) | Total Time 14.00(14.00)\n",
      "Iter 15010 | Time 26.9328(26.3578) | Bit/dim 3.4877(3.5096) | Xent 0.0139(0.0178) | Loss 3.4946(3.5185) | Error 0.0011(0.0038) Steps 1066(1044.79) | Grad Norm 0.7665(0.8271) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0273 | Time 119.4423, Epoch Time 1585.4428(1549.0107), Bit/dim 3.5142(best: 3.5149), Xent 1.9355, Loss 4.4819, Error 0.2743(best: 0.2631)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15020 | Time 26.9123(26.3514) | Bit/dim 3.4898(3.5078) | Xent 0.0126(0.0178) | Loss 3.4961(3.5167) | Error 0.0033(0.0038) Steps 1042(1045.13) | Grad Norm 0.7314(0.8244) | Total Time 14.00(14.00)\n",
      "Iter 15030 | Time 26.0237(26.3299) | Bit/dim 3.5565(3.5101) | Xent 0.0172(0.0176) | Loss 3.5652(3.5189) | Error 0.0044(0.0038) Steps 1054(1045.75) | Grad Norm 0.8134(0.8416) | Total Time 14.00(14.00)\n",
      "Iter 15040 | Time 26.4499(26.3671) | Bit/dim 3.4995(3.5100) | Xent 0.0173(0.0175) | Loss 3.5081(3.5187) | Error 0.0033(0.0039) Steps 1048(1045.09) | Grad Norm 0.9676(0.8319) | Total Time 14.00(14.00)\n",
      "Iter 15050 | Time 26.6018(26.4206) | Bit/dim 3.4880(3.5069) | Xent 0.0148(0.0174) | Loss 3.4954(3.5156) | Error 0.0033(0.0038) Steps 1054(1046.43) | Grad Norm 0.6276(0.8141) | Total Time 14.00(14.00)\n",
      "Iter 15060 | Time 25.9310(26.3272) | Bit/dim 3.5037(3.5082) | Xent 0.0168(0.0177) | Loss 3.5121(3.5170) | Error 0.0044(0.0039) Steps 1054(1045.93) | Grad Norm 0.8367(0.8209) | Total Time 14.00(14.00)\n",
      "Iter 15070 | Time 25.8006(26.2519) | Bit/dim 3.5523(3.5113) | Xent 0.0151(0.0175) | Loss 3.5599(3.5200) | Error 0.0022(0.0039) Steps 1060(1045.91) | Grad Norm 0.7498(0.8176) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0274 | Time 118.8695, Epoch Time 1582.7875(1550.0240), Bit/dim 3.5154(best: 3.5142), Xent 1.9518, Loss 4.4913, Error 0.2723(best: 0.2631)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15080 | Time 26.2947(26.3105) | Bit/dim 3.5350(3.5103) | Xent 0.0131(0.0175) | Loss 3.5415(3.5190) | Error 0.0011(0.0038) Steps 1042(1043.90) | Grad Norm 0.7284(0.8148) | Total Time 14.00(14.00)\n",
      "Iter 15090 | Time 26.1319(26.3240) | Bit/dim 3.5205(3.5123) | Xent 0.0142(0.0178) | Loss 3.5277(3.5212) | Error 0.0000(0.0039) Steps 1048(1042.98) | Grad Norm 0.6725(0.8302) | Total Time 14.00(14.00)\n",
      "Iter 15100 | Time 27.2444(26.4324) | Bit/dim 3.5090(3.5128) | Xent 0.0149(0.0173) | Loss 3.5164(3.5215) | Error 0.0022(0.0036) Steps 1048(1045.35) | Grad Norm 0.8116(0.8221) | Total Time 14.00(14.00)\n",
      "Iter 15110 | Time 26.2501(26.4328) | Bit/dim 3.4795(3.5112) | Xent 0.0200(0.0176) | Loss 3.4895(3.5200) | Error 0.0067(0.0038) Steps 1018(1046.19) | Grad Norm 1.0188(0.8806) | Total Time 14.00(14.00)\n",
      "Iter 15120 | Time 26.5028(26.5029) | Bit/dim 3.4725(3.5078) | Xent 0.0179(0.0173) | Loss 3.4815(3.5164) | Error 0.0033(0.0038) Steps 1042(1046.49) | Grad Norm 1.0621(0.8929) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0275 | Time 120.3181, Epoch Time 1597.8642(1551.4592), Bit/dim 3.5138(best: 3.5142), Xent 1.9456, Loss 4.4866, Error 0.2706(best: 0.2631)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15130 | Time 26.3751(26.4646) | Bit/dim 3.5096(3.5090) | Xent 0.0120(0.0177) | Loss 3.5156(3.5179) | Error 0.0022(0.0040) Steps 1054(1047.63) | Grad Norm 0.6483(0.8978) | Total Time 14.00(14.00)\n",
      "Iter 15140 | Time 26.1357(26.4922) | Bit/dim 3.5138(3.5111) | Xent 0.0222(0.0179) | Loss 3.5249(3.5201) | Error 0.0067(0.0042) Steps 1024(1047.68) | Grad Norm 0.9100(0.9156) | Total Time 14.00(14.00)\n",
      "Iter 15150 | Time 26.0537(26.5157) | Bit/dim 3.4866(3.5084) | Xent 0.0200(0.0178) | Loss 3.4966(3.5174) | Error 0.0056(0.0041) Steps 1048(1049.20) | Grad Norm 1.0460(0.9215) | Total Time 14.00(14.00)\n",
      "Iter 15160 | Time 26.4499(26.4857) | Bit/dim 3.5247(3.5067) | Xent 0.0112(0.0175) | Loss 3.5303(3.5154) | Error 0.0000(0.0037) Steps 1066(1049.53) | Grad Norm 0.5755(0.9016) | Total Time 14.00(14.00)\n",
      "Iter 15170 | Time 26.7088(26.5009) | Bit/dim 3.5353(3.5072) | Xent 0.0228(0.0176) | Loss 3.5467(3.5160) | Error 0.0067(0.0036) Steps 1042(1051.46) | Grad Norm 1.0265(0.8826) | Total Time 14.00(14.00)\n",
      "Iter 15180 | Time 26.3360(26.4931) | Bit/dim 3.5343(3.5094) | Xent 0.0171(0.0174) | Loss 3.5429(3.5181) | Error 0.0056(0.0036) Steps 1042(1051.14) | Grad Norm 0.7157(0.8524) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0276 | Time 120.0458, Epoch Time 1595.4565(1552.7792), Bit/dim 3.5132(best: 3.5138), Xent 1.9664, Loss 4.4964, Error 0.2738(best: 0.2631)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15190 | Time 26.9184(26.4845) | Bit/dim 3.5000(3.5091) | Xent 0.0142(0.0178) | Loss 3.5071(3.5180) | Error 0.0022(0.0038) Steps 1030(1051.77) | Grad Norm 0.6717(0.9100) | Total Time 14.00(14.00)\n",
      "Iter 15200 | Time 25.7983(26.5181) | Bit/dim 3.5253(3.5090) | Xent 0.0259(0.0178) | Loss 3.5383(3.5179) | Error 0.0067(0.0036) Steps 1012(1050.95) | Grad Norm 1.0580(0.9079) | Total Time 14.00(14.00)\n",
      "Iter 15210 | Time 26.3254(26.4664) | Bit/dim 3.5255(3.5083) | Xent 0.0129(0.0173) | Loss 3.5319(3.5169) | Error 0.0022(0.0034) Steps 1030(1048.42) | Grad Norm 0.5240(0.9108) | Total Time 14.00(14.00)\n",
      "Iter 15220 | Time 26.8628(26.4621) | Bit/dim 3.4763(3.5061) | Xent 0.0192(0.0178) | Loss 3.4859(3.5149) | Error 0.0078(0.0040) Steps 1060(1047.62) | Grad Norm 0.9626(0.9177) | Total Time 14.00(14.00)\n",
      "Iter 15230 | Time 27.2277(26.4490) | Bit/dim 3.4998(3.5077) | Xent 0.0134(0.0172) | Loss 3.5065(3.5163) | Error 0.0022(0.0037) Steps 1036(1050.14) | Grad Norm 0.6137(0.8660) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0277 | Time 120.2604, Epoch Time 1594.5893(1554.0335), Bit/dim 3.5141(best: 3.5132), Xent 1.9883, Loss 4.5083, Error 0.2723(best: 0.2631)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15240 | Time 27.6197(26.5235) | Bit/dim 3.4737(3.5092) | Xent 0.0248(0.0175) | Loss 3.4862(3.5180) | Error 0.0056(0.0038) Steps 1072(1051.69) | Grad Norm 1.0269(0.8566) | Total Time 14.00(14.00)\n",
      "Iter 15250 | Time 26.8666(26.5402) | Bit/dim 3.5063(3.5097) | Xent 0.0179(0.0174) | Loss 3.5152(3.5184) | Error 0.0033(0.0037) Steps 1060(1053.97) | Grad Norm 1.0925(0.8592) | Total Time 14.00(14.00)\n",
      "Iter 15260 | Time 26.2031(26.5008) | Bit/dim 3.4971(3.5103) | Xent 0.0146(0.0173) | Loss 3.5044(3.5190) | Error 0.0022(0.0036) Steps 1048(1052.24) | Grad Norm 0.7294(0.8769) | Total Time 14.00(14.00)\n",
      "Iter 15270 | Time 26.9856(26.5955) | Bit/dim 3.4925(3.5077) | Xent 0.0203(0.0177) | Loss 3.5027(3.5165) | Error 0.0044(0.0038) Steps 1036(1052.70) | Grad Norm 1.0639(0.8952) | Total Time 14.00(14.00)\n",
      "Iter 15280 | Time 27.0381(26.5923) | Bit/dim 3.5242(3.5096) | Xent 0.0174(0.0176) | Loss 3.5330(3.5184) | Error 0.0033(0.0039) Steps 1066(1052.38) | Grad Norm 0.8234(0.8932) | Total Time 14.00(14.00)\n",
      "Iter 15290 | Time 26.8719(26.5442) | Bit/dim 3.4856(3.5064) | Xent 0.0219(0.0185) | Loss 3.4965(3.5157) | Error 0.0067(0.0044) Steps 1060(1052.25) | Grad Norm 1.1376(0.9375) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0278 | Time 119.8579, Epoch Time 1599.4350(1555.3955), Bit/dim 3.5127(best: 3.5132), Xent 1.9590, Loss 4.4922, Error 0.2734(best: 0.2631)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15300 | Time 26.3472(26.5074) | Bit/dim 3.5037(3.5081) | Xent 0.0183(0.0186) | Loss 3.5128(3.5174) | Error 0.0067(0.0046) Steps 1060(1049.77) | Grad Norm 0.9290(0.9579) | Total Time 14.00(14.00)\n",
      "Iter 15310 | Time 27.6440(26.5476) | Bit/dim 3.5081(3.5080) | Xent 0.0129(0.0182) | Loss 3.5145(3.5172) | Error 0.0011(0.0045) Steps 1024(1049.77) | Grad Norm 1.0161(0.9481) | Total Time 14.00(14.00)\n",
      "Iter 15320 | Time 26.8730(26.5885) | Bit/dim 3.5190(3.5081) | Xent 0.0112(0.0181) | Loss 3.5246(3.5171) | Error 0.0011(0.0041) Steps 1048(1050.92) | Grad Norm 0.6708(0.9453) | Total Time 14.00(14.00)\n",
      "Iter 15330 | Time 27.0323(26.5926) | Bit/dim 3.5492(3.5087) | Xent 0.0184(0.0182) | Loss 3.5584(3.5178) | Error 0.0056(0.0043) Steps 1054(1052.89) | Grad Norm 1.0867(0.9340) | Total Time 14.00(14.00)\n",
      "Iter 15340 | Time 26.5871(26.5665) | Bit/dim 3.4914(3.5087) | Xent 0.0169(0.0186) | Loss 3.4999(3.5180) | Error 0.0056(0.0047) Steps 1054(1052.94) | Grad Norm 0.8743(0.9432) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0279 | Time 120.2063, Epoch Time 1600.6788(1556.7540), Bit/dim 3.5138(best: 3.5127), Xent 1.9701, Loss 4.4989, Error 0.2707(best: 0.2631)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15350 | Time 26.7200(26.5813) | Bit/dim 3.5135(3.5065) | Xent 0.0193(0.0179) | Loss 3.5231(3.5154) | Error 0.0067(0.0045) Steps 1066(1050.64) | Grad Norm 0.9679(0.9290) | Total Time 14.00(14.00)\n",
      "Iter 15360 | Time 26.1335(26.5564) | Bit/dim 3.4733(3.5067) | Xent 0.0170(0.0177) | Loss 3.4818(3.5156) | Error 0.0011(0.0044) Steps 1054(1052.02) | Grad Norm 0.6851(0.9174) | Total Time 14.00(14.00)\n",
      "Iter 15370 | Time 25.8508(26.5392) | Bit/dim 3.4808(3.5104) | Xent 0.0264(0.0176) | Loss 3.4940(3.5192) | Error 0.0067(0.0042) Steps 1066(1052.13) | Grad Norm 1.1120(0.9010) | Total Time 14.00(14.00)\n",
      "Iter 15380 | Time 26.9821(26.5279) | Bit/dim 3.4867(3.5095) | Xent 0.0189(0.0177) | Loss 3.4961(3.5184) | Error 0.0056(0.0043) Steps 1072(1050.57) | Grad Norm 0.7644(0.8988) | Total Time 14.00(14.00)\n",
      "Iter 15390 | Time 27.3323(26.5921) | Bit/dim 3.4918(3.5074) | Xent 0.0163(0.0173) | Loss 3.5000(3.5160) | Error 0.0022(0.0042) Steps 1048(1050.30) | Grad Norm 0.9068(0.9053) | Total Time 14.00(14.00)\n",
      "Iter 15400 | Time 26.1195(26.5811) | Bit/dim 3.5005(3.5067) | Xent 0.0185(0.0175) | Loss 3.5098(3.5154) | Error 0.0044(0.0040) Steps 1066(1051.51) | Grad Norm 0.9570(0.8986) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0280 | Time 119.8794, Epoch Time 1598.6622(1558.0113), Bit/dim 3.5132(best: 3.5127), Xent 1.9944, Loss 4.5104, Error 0.2738(best: 0.2631)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15410 | Time 26.4089(26.6218) | Bit/dim 3.5204(3.5074) | Xent 0.0121(0.0173) | Loss 3.5264(3.5160) | Error 0.0000(0.0038) Steps 1072(1050.40) | Grad Norm 0.7330(0.9038) | Total Time 14.00(14.00)\n",
      "Iter 15420 | Time 27.3245(26.5746) | Bit/dim 3.4818(3.5065) | Xent 0.0197(0.0177) | Loss 3.4917(3.5154) | Error 0.0056(0.0038) Steps 1030(1049.38) | Grad Norm 1.2353(0.9529) | Total Time 14.00(14.00)\n",
      "Iter 15430 | Time 27.0917(26.6387) | Bit/dim 3.5081(3.5087) | Xent 0.0127(0.0173) | Loss 3.5144(3.5173) | Error 0.0022(0.0037) Steps 1060(1050.20) | Grad Norm 0.8672(0.9536) | Total Time 14.00(14.00)\n",
      "Iter 15440 | Time 26.8902(26.5702) | Bit/dim 3.4727(3.5082) | Xent 0.0215(0.0175) | Loss 3.4834(3.5170) | Error 0.0056(0.0037) Steps 1066(1050.00) | Grad Norm 1.1067(0.9579) | Total Time 14.00(14.00)\n",
      "Iter 15450 | Time 25.9774(26.5716) | Bit/dim 3.5295(3.5079) | Xent 0.0133(0.0173) | Loss 3.5361(3.5166) | Error 0.0022(0.0037) Steps 1048(1051.12) | Grad Norm 0.6644(0.9348) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0281 | Time 120.4912, Epoch Time 1599.2884(1559.2496), Bit/dim 3.5120(best: 3.5127), Xent 1.9765, Loss 4.5002, Error 0.2692(best: 0.2631)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15460 | Time 26.0354(26.6096) | Bit/dim 3.5010(3.5074) | Xent 0.0204(0.0177) | Loss 3.5112(3.5162) | Error 0.0022(0.0039) Steps 1042(1052.34) | Grad Norm 0.8351(0.9254) | Total Time 14.00(14.00)\n",
      "Iter 15470 | Time 26.7267(26.6940) | Bit/dim 3.5113(3.5087) | Xent 0.0164(0.0173) | Loss 3.5195(3.5174) | Error 0.0044(0.0039) Steps 1048(1052.78) | Grad Norm 0.8533(0.9136) | Total Time 14.00(14.00)\n",
      "Iter 15480 | Time 26.9398(26.7241) | Bit/dim 3.5506(3.5089) | Xent 0.0146(0.0169) | Loss 3.5579(3.5173) | Error 0.0033(0.0037) Steps 1048(1050.78) | Grad Norm 0.7192(0.8768) | Total Time 14.00(14.00)\n",
      "Iter 15490 | Time 26.9544(26.7367) | Bit/dim 3.4886(3.5066) | Xent 0.0133(0.0171) | Loss 3.4952(3.5152) | Error 0.0022(0.0037) Steps 1036(1050.19) | Grad Norm 0.6541(0.8826) | Total Time 14.00(14.00)\n",
      "Iter 15500 | Time 27.2014(26.8369) | Bit/dim 3.4975(3.5059) | Xent 0.0198(0.0171) | Loss 3.5075(3.5144) | Error 0.0044(0.0037) Steps 1066(1051.90) | Grad Norm 0.9966(0.8687) | Total Time 14.00(14.00)\n",
      "Iter 15510 | Time 26.4483(26.8365) | Bit/dim 3.4831(3.5062) | Xent 0.0273(0.0174) | Loss 3.4968(3.5149) | Error 0.0100(0.0040) Steps 1024(1051.14) | Grad Norm 1.2528(0.8767) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0282 | Time 120.0488, Epoch Time 1618.3786(1561.0234), Bit/dim 3.5128(best: 3.5120), Xent 2.0041, Loss 4.5148, Error 0.2698(best: 0.2631)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15520 | Time 26.8192(26.7767) | Bit/dim 3.4935(3.5051) | Xent 0.0216(0.0175) | Loss 3.5043(3.5138) | Error 0.0078(0.0042) Steps 1084(1050.86) | Grad Norm 0.9376(0.8904) | Total Time 14.00(14.00)\n",
      "Iter 15530 | Time 26.5752(26.7384) | Bit/dim 3.4998(3.5042) | Xent 0.0202(0.0173) | Loss 3.5099(3.5129) | Error 0.0033(0.0040) Steps 1036(1051.03) | Grad Norm 1.2780(0.8960) | Total Time 14.00(14.00)\n",
      "Iter 15540 | Time 26.7971(26.6174) | Bit/dim 3.4765(3.5042) | Xent 0.0140(0.0171) | Loss 3.4834(3.5128) | Error 0.0022(0.0039) Steps 1042(1048.34) | Grad Norm 0.7788(0.9026) | Total Time 14.00(14.00)\n",
      "Iter 15550 | Time 26.2054(26.5551) | Bit/dim 3.5048(3.5061) | Xent 0.0202(0.0176) | Loss 3.5149(3.5149) | Error 0.0056(0.0043) Steps 1036(1047.20) | Grad Norm 1.2491(0.9376) | Total Time 14.00(14.00)\n",
      "Iter 15560 | Time 26.9273(26.5396) | Bit/dim 3.5217(3.5068) | Xent 0.0135(0.0178) | Loss 3.5284(3.5157) | Error 0.0033(0.0043) Steps 1042(1046.66) | Grad Norm 0.7373(0.9783) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0283 | Time 120.2440, Epoch Time 1596.4270(1562.0855), Bit/dim 3.5119(best: 3.5120), Xent 1.9996, Loss 4.5117, Error 0.2720(best: 0.2631)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15570 | Time 25.8523(26.5544) | Bit/dim 3.5232(3.5085) | Xent 0.0189(0.0177) | Loss 3.5326(3.5174) | Error 0.0056(0.0042) Steps 1042(1048.54) | Grad Norm 0.9098(1.0157) | Total Time 14.00(14.00)\n",
      "Iter 15580 | Time 26.1805(26.5140) | Bit/dim 3.4782(3.5075) | Xent 0.0160(0.0175) | Loss 3.4862(3.5163) | Error 0.0044(0.0043) Steps 1054(1047.65) | Grad Norm 0.9629(0.9807) | Total Time 14.00(14.00)\n",
      "Iter 15590 | Time 26.3754(26.5601) | Bit/dim 3.5037(3.5069) | Xent 0.0139(0.0179) | Loss 3.5106(3.5158) | Error 0.0000(0.0044) Steps 1042(1047.56) | Grad Norm 0.6503(0.9512) | Total Time 14.00(14.00)\n",
      "Iter 15600 | Time 26.8497(26.5591) | Bit/dim 3.5137(3.5071) | Xent 0.0185(0.0184) | Loss 3.5229(3.5163) | Error 0.0044(0.0048) Steps 1066(1049.67) | Grad Norm 0.8716(0.9594) | Total Time 14.00(14.00)\n",
      "Iter 15610 | Time 26.4833(26.5892) | Bit/dim 3.5186(3.5067) | Xent 0.0174(0.0178) | Loss 3.5273(3.5156) | Error 0.0033(0.0045) Steps 1054(1050.63) | Grad Norm 1.0491(0.9509) | Total Time 14.00(14.00)\n",
      "Iter 15620 | Time 27.0774(26.5909) | Bit/dim 3.4906(3.5077) | Xent 0.0212(0.0174) | Loss 3.5012(3.5164) | Error 0.0067(0.0041) Steps 1066(1052.28) | Grad Norm 1.0614(0.9190) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0284 | Time 119.3303, Epoch Time 1599.1411(1563.1972), Bit/dim 3.5122(best: 3.5119), Xent 2.0295, Loss 4.5269, Error 0.2714(best: 0.2631)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15630 | Time 26.6643(26.6618) | Bit/dim 3.4927(3.5070) | Xent 0.0139(0.0168) | Loss 3.4996(3.5154) | Error 0.0044(0.0040) Steps 1042(1051.48) | Grad Norm 0.7121(0.8972) | Total Time 14.00(14.00)\n",
      "Iter 15640 | Time 26.5966(26.7428) | Bit/dim 3.5466(3.5067) | Xent 0.0199(0.0167) | Loss 3.5565(3.5151) | Error 0.0078(0.0039) Steps 1042(1052.40) | Grad Norm 1.2321(0.8883) | Total Time 14.00(14.00)\n",
      "Iter 15650 | Time 26.8269(26.6987) | Bit/dim 3.4875(3.5048) | Xent 0.0179(0.0171) | Loss 3.4965(3.5134) | Error 0.0056(0.0040) Steps 1048(1052.10) | Grad Norm 0.9143(0.8801) | Total Time 14.00(14.00)\n",
      "Iter 15660 | Time 26.9284(26.6838) | Bit/dim 3.5080(3.5055) | Xent 0.0123(0.0170) | Loss 3.5142(3.5139) | Error 0.0022(0.0038) Steps 1024(1053.46) | Grad Norm 0.6743(0.8751) | Total Time 14.00(14.00)\n",
      "Iter 15670 | Time 26.6426(26.7474) | Bit/dim 3.5166(3.5083) | Xent 0.0239(0.0175) | Loss 3.5286(3.5170) | Error 0.0078(0.0040) Steps 1060(1051.87) | Grad Norm 1.3351(0.8796) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0285 | Time 120.0402, Epoch Time 1613.2355(1564.6984), Bit/dim 3.5108(best: 3.5119), Xent 2.0364, Loss 4.5290, Error 0.2746(best: 0.2631)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15680 | Time 26.4643(26.7953) | Bit/dim 3.5132(3.5076) | Xent 0.0229(0.0174) | Loss 3.5247(3.5163) | Error 0.0067(0.0038) Steps 1060(1054.06) | Grad Norm 1.0776(0.8630) | Total Time 14.00(14.00)\n",
      "Iter 15690 | Time 26.7421(26.7377) | Bit/dim 3.5196(3.5091) | Xent 0.0153(0.0169) | Loss 3.5273(3.5175) | Error 0.0033(0.0037) Steps 1066(1052.68) | Grad Norm 0.9536(0.8896) | Total Time 14.00(14.00)\n",
      "Iter 15700 | Time 26.6705(26.6722) | Bit/dim 3.5068(3.5070) | Xent 0.0241(0.0171) | Loss 3.5188(3.5155) | Error 0.0078(0.0038) Steps 1024(1050.04) | Grad Norm 1.0221(0.8822) | Total Time 14.00(14.00)\n",
      "Iter 15710 | Time 27.4759(26.7044) | Bit/dim 3.4970(3.5070) | Xent 0.0164(0.0167) | Loss 3.5052(3.5154) | Error 0.0022(0.0038) Steps 1042(1050.68) | Grad Norm 0.9109(0.8792) | Total Time 14.00(14.00)\n",
      "Iter 15720 | Time 26.5529(26.6933) | Bit/dim 3.5006(3.5056) | Xent 0.0129(0.0169) | Loss 3.5070(3.5140) | Error 0.0011(0.0037) Steps 1042(1052.77) | Grad Norm 0.7818(0.8727) | Total Time 14.00(14.00)\n",
      "Iter 15730 | Time 26.9030(26.6628) | Bit/dim 3.5029(3.5059) | Xent 0.0234(0.0169) | Loss 3.5146(3.5143) | Error 0.0022(0.0037) Steps 1060(1053.25) | Grad Norm 1.2098(0.8706) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0286 | Time 119.6655, Epoch Time 1602.0007(1565.8174), Bit/dim 3.5104(best: 3.5108), Xent 2.0203, Loss 4.5206, Error 0.2757(best: 0.2631)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15740 | Time 26.5183(26.5549) | Bit/dim 3.5236(3.5049) | Xent 0.0122(0.0171) | Loss 3.5297(3.5134) | Error 0.0011(0.0036) Steps 1036(1053.46) | Grad Norm 0.7983(0.8809) | Total Time 14.00(14.00)\n",
      "Iter 15750 | Time 26.7763(26.5386) | Bit/dim 3.5084(3.5045) | Xent 0.0223(0.0166) | Loss 3.5195(3.5128) | Error 0.0067(0.0036) Steps 1078(1053.48) | Grad Norm 1.0483(0.8812) | Total Time 14.00(14.00)\n",
      "Iter 15760 | Time 26.5044(26.4896) | Bit/dim 3.5185(3.5047) | Xent 0.0171(0.0167) | Loss 3.5270(3.5131) | Error 0.0067(0.0037) Steps 1018(1054.05) | Grad Norm 1.0226(0.9054) | Total Time 14.00(14.00)\n",
      "Iter 15770 | Time 26.5014(26.5174) | Bit/dim 3.5116(3.5042) | Xent 0.0149(0.0169) | Loss 3.5191(3.5126) | Error 0.0033(0.0039) Steps 1048(1053.92) | Grad Norm 0.7828(0.9137) | Total Time 14.00(14.00)\n",
      "Iter 15780 | Time 26.6225(26.6496) | Bit/dim 3.5205(3.5060) | Xent 0.0195(0.0171) | Loss 3.5302(3.5146) | Error 0.0044(0.0039) Steps 1060(1053.76) | Grad Norm 1.2115(0.9125) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0287 | Time 119.2961, Epoch Time 1596.7684(1566.7460), Bit/dim 3.5110(best: 3.5104), Xent 2.0244, Loss 4.5232, Error 0.2725(best: 0.2631)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15790 | Time 26.5476(26.6017) | Bit/dim 3.4549(3.5070) | Xent 0.0146(0.0166) | Loss 3.4622(3.5153) | Error 0.0033(0.0037) Steps 1054(1054.56) | Grad Norm 1.2873(0.8982) | Total Time 14.00(14.00)\n",
      "Iter 15800 | Time 26.9741(26.5807) | Bit/dim 3.5310(3.5078) | Xent 0.0124(0.0162) | Loss 3.5373(3.5159) | Error 0.0022(0.0035) Steps 1042(1054.35) | Grad Norm 0.8093(0.8964) | Total Time 14.00(14.00)\n",
      "Iter 15810 | Time 26.6469(26.6006) | Bit/dim 3.5231(3.5069) | Xent 0.0163(0.0162) | Loss 3.5313(3.5150) | Error 0.0033(0.0035) Steps 1042(1053.40) | Grad Norm 1.1699(0.9489) | Total Time 14.00(14.00)\n",
      "Iter 15820 | Time 27.4860(26.5760) | Bit/dim 3.5162(3.5053) | Xent 0.0192(0.0160) | Loss 3.5258(3.5133) | Error 0.0067(0.0034) Steps 1078(1054.77) | Grad Norm 1.4583(0.9732) | Total Time 14.00(14.00)\n",
      "Iter 15830 | Time 26.3697(26.5646) | Bit/dim 3.5320(3.5057) | Xent 0.0242(0.0163) | Loss 3.5441(3.5138) | Error 0.0111(0.0037) Steps 1048(1056.37) | Grad Norm 0.9886(0.9636) | Total Time 14.00(14.00)\n",
      "Iter 15840 | Time 25.2297(26.4944) | Bit/dim 3.4891(3.5054) | Xent 0.0199(0.0170) | Loss 3.4991(3.5139) | Error 0.0033(0.0039) Steps 1048(1056.00) | Grad Norm 0.9015(0.9701) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0288 | Time 119.5331, Epoch Time 1595.4718(1567.6077), Bit/dim 3.5105(best: 3.5104), Xent 2.0412, Loss 4.5311, Error 0.2780(best: 0.2631)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15850 | Time 26.0704(26.4781) | Bit/dim 3.4751(3.5027) | Xent 0.0164(0.0170) | Loss 3.4832(3.5112) | Error 0.0011(0.0039) Steps 1054(1056.35) | Grad Norm 1.1324(0.9692) | Total Time 14.00(14.00)\n",
      "Iter 15860 | Time 26.4591(26.5120) | Bit/dim 3.5090(3.5047) | Xent 0.0231(0.0169) | Loss 3.5205(3.5131) | Error 0.0044(0.0038) Steps 1066(1058.33) | Grad Norm 1.1499(0.9420) | Total Time 14.00(14.00)\n",
      "Iter 15870 | Time 26.7220(26.5859) | Bit/dim 3.5465(3.5058) | Xent 0.0165(0.0169) | Loss 3.5548(3.5143) | Error 0.0044(0.0037) Steps 1066(1059.64) | Grad Norm 1.1945(0.9621) | Total Time 14.00(14.00)\n",
      "Iter 15880 | Time 26.9703(26.6065) | Bit/dim 3.4997(3.5066) | Xent 0.0155(0.0164) | Loss 3.5075(3.5148) | Error 0.0022(0.0037) Steps 1048(1059.65) | Grad Norm 0.7047(0.9316) | Total Time 14.00(14.00)\n",
      "Iter 15890 | Time 25.8126(26.5546) | Bit/dim 3.4938(3.5068) | Xent 0.0139(0.0164) | Loss 3.5007(3.5150) | Error 0.0033(0.0037) Steps 1048(1058.83) | Grad Norm 1.1018(0.9367) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0289 | Time 120.6705, Epoch Time 1599.5147(1568.5649), Bit/dim 3.5108(best: 3.5104), Xent 2.0194, Loss 4.5205, Error 0.2766(best: 0.2631)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15900 | Time 26.3252(26.5325) | Bit/dim 3.4936(3.5035) | Xent 0.0191(0.0163) | Loss 3.5031(3.5116) | Error 0.0044(0.0037) Steps 1036(1056.13) | Grad Norm 0.9578(0.9181) | Total Time 14.00(14.00)\n",
      "Iter 15910 | Time 26.4874(26.5451) | Bit/dim 3.5051(3.5039) | Xent 0.0104(0.0162) | Loss 3.5103(3.5120) | Error 0.0011(0.0034) Steps 1024(1053.84) | Grad Norm 0.6411(0.9079) | Total Time 14.00(14.00)\n",
      "Iter 15920 | Time 27.1355(26.5819) | Bit/dim 3.4839(3.5004) | Xent 0.0188(0.0169) | Loss 3.4933(3.5089) | Error 0.0056(0.0039) Steps 1078(1054.70) | Grad Norm 1.0073(0.9181) | Total Time 14.00(14.00)\n",
      "Iter 15930 | Time 26.1056(26.6047) | Bit/dim 3.5323(3.5066) | Xent 0.0140(0.0168) | Loss 3.5393(3.5150) | Error 0.0022(0.0038) Steps 1030(1053.22) | Grad Norm 0.8270(0.9309) | Total Time 14.00(14.00)\n",
      "Iter 15940 | Time 26.0723(26.5961) | Bit/dim 3.5076(3.5055) | Xent 0.0222(0.0171) | Loss 3.5187(3.5140) | Error 0.0067(0.0039) Steps 1072(1055.26) | Grad Norm 1.1741(0.9404) | Total Time 14.00(14.00)\n",
      "Iter 15950 | Time 26.3273(26.6410) | Bit/dim 3.5329(3.5066) | Xent 0.0172(0.0170) | Loss 3.5415(3.5151) | Error 0.0033(0.0038) Steps 1048(1051.98) | Grad Norm 1.0065(0.9214) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0290 | Time 120.3571, Epoch Time 1604.0921(1569.6308), Bit/dim 3.5107(best: 3.5104), Xent 2.0319, Loss 4.5267, Error 0.2722(best: 0.2631)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15960 | Time 26.4842(26.6405) | Bit/dim 3.5010(3.5054) | Xent 0.0118(0.0163) | Loss 3.5069(3.5136) | Error 0.0022(0.0034) Steps 1060(1050.74) | Grad Norm 0.7803(0.9173) | Total Time 14.00(14.00)\n",
      "Iter 15970 | Time 26.9920(26.6771) | Bit/dim 3.4950(3.5058) | Xent 0.0186(0.0167) | Loss 3.5043(3.5141) | Error 0.0022(0.0034) Steps 1060(1051.12) | Grad Norm 1.1841(0.9566) | Total Time 14.00(14.00)\n",
      "Iter 15980 | Time 27.3687(26.6106) | Bit/dim 3.5079(3.5059) | Xent 0.0106(0.0168) | Loss 3.5133(3.5144) | Error 0.0000(0.0035) Steps 1054(1051.95) | Grad Norm 0.8862(0.9733) | Total Time 14.00(14.00)\n",
      "Iter 15990 | Time 26.9421(26.6295) | Bit/dim 3.5344(3.5057) | Xent 0.0165(0.0162) | Loss 3.5427(3.5138) | Error 0.0056(0.0034) Steps 1072(1054.18) | Grad Norm 0.8396(0.9418) | Total Time 14.00(14.00)\n",
      "Iter 16000 | Time 27.0667(26.5912) | Bit/dim 3.5079(3.5062) | Xent 0.0142(0.0164) | Loss 3.5150(3.5144) | Error 0.0022(0.0034) Steps 1060(1051.87) | Grad Norm 0.8322(0.9228) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0291 | Time 119.2951, Epoch Time 1600.8900(1570.5685), Bit/dim 3.5103(best: 3.5104), Xent 2.0382, Loss 4.5294, Error 0.2742(best: 0.2631)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16010 | Time 27.9043(26.6049) | Bit/dim 3.4864(3.5052) | Xent 0.0140(0.0162) | Loss 3.4934(3.5133) | Error 0.0033(0.0035) Steps 1066(1051.29) | Grad Norm 0.8156(0.9148) | Total Time 14.00(14.00)\n",
      "Iter 16020 | Time 26.1573(26.6655) | Bit/dim 3.5042(3.5060) | Xent 0.0116(0.0160) | Loss 3.5100(3.5140) | Error 0.0011(0.0036) Steps 1066(1054.54) | Grad Norm 0.7960(0.9053) | Total Time 14.00(14.00)\n",
      "Iter 16030 | Time 27.0463(26.6310) | Bit/dim 3.4910(3.5058) | Xent 0.0142(0.0161) | Loss 3.4982(3.5139) | Error 0.0033(0.0039) Steps 1066(1053.28) | Grad Norm 0.6591(0.9188) | Total Time 14.00(14.00)\n",
      "Iter 16040 | Time 26.3118(26.5657) | Bit/dim 3.5264(3.5044) | Xent 0.0138(0.0163) | Loss 3.5333(3.5125) | Error 0.0022(0.0036) Steps 1066(1050.98) | Grad Norm 0.6791(0.9075) | Total Time 14.00(14.00)\n",
      "Iter 16050 | Time 26.7953(26.5878) | Bit/dim 3.4799(3.5043) | Xent 0.0166(0.0164) | Loss 3.4882(3.5125) | Error 0.0044(0.0039) Steps 1066(1053.97) | Grad Norm 0.7069(0.9306) | Total Time 14.00(14.00)\n",
      "Iter 16060 | Time 26.6658(26.5624) | Bit/dim 3.5107(3.5045) | Xent 0.0184(0.0166) | Loss 3.5198(3.5128) | Error 0.0056(0.0041) Steps 1042(1054.31) | Grad Norm 0.8129(0.9303) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0292 | Time 119.3553, Epoch Time 1599.0670(1571.4235), Bit/dim 3.5093(best: 3.5103), Xent 2.0287, Loss 4.5236, Error 0.2739(best: 0.2631)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16070 | Time 25.6018(26.5012) | Bit/dim 3.4817(3.4998) | Xent 0.0229(0.0170) | Loss 3.4932(3.5083) | Error 0.0067(0.0043) Steps 1060(1055.47) | Grad Norm 1.4224(0.9805) | Total Time 14.00(14.00)\n",
      "Iter 16080 | Time 26.9178(26.5075) | Bit/dim 3.5202(3.5009) | Xent 0.0112(0.0168) | Loss 3.5258(3.5093) | Error 0.0011(0.0043) Steps 1036(1055.71) | Grad Norm 0.7017(0.9933) | Total Time 14.00(14.00)\n",
      "Iter 16090 | Time 26.9451(26.5543) | Bit/dim 3.5231(3.5022) | Xent 0.0186(0.0169) | Loss 3.5324(3.5107) | Error 0.0044(0.0042) Steps 1054(1055.70) | Grad Norm 1.6420(1.0367) | Total Time 14.00(14.00)\n",
      "Iter 16100 | Time 26.5519(26.5049) | Bit/dim 3.5227(3.5044) | Xent 0.0231(0.0174) | Loss 3.5342(3.5131) | Error 0.0067(0.0040) Steps 1060(1055.24) | Grad Norm 1.2148(1.0499) | Total Time 14.00(14.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_cifar.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_run3 --resume ../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_run3/current_checkpt.pth --seed 3 --lr 0.0001 --conditional True --controlled_tol False --train_mode semisup --log_freq 10 --weight_y 0.5 --condition_ratio 0.5\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
