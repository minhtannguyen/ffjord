{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='4,5,6,7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_rl.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "GATES = [\"cnn1\", \"cnn2\", \"rnn\"]\n",
      "\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--gate', type=str, default='cnn1', choices=GATES)\n",
      "parser.add_argument('--scale', type=float, default=1.0)\n",
      "parser.add_argument('--scale_fac', type=float, default=1.0)\n",
      "parser.add_argument('--scale_std', type=float, default=1.0)\n",
      "parser.add_argument('--eta', default=0.1, type=float,\n",
      "                        help='tuning parameter that allows us to trade-off the competing goals of' \n",
      "                                'minimizing the prediction loss and maximizing the gate rewards ')\n",
      "parser.add_argument('--rl-weight', default=0.01, type=float,\n",
      "                        help='rl weight')\n",
      "\n",
      "parser.add_argument('--gamma', default=0.99, type=float,\n",
      "                        help='discount factor, default: (0.99)')\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "import lib.odenvp_conditional_rl as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol, \"scale\": args.scale, \"scale_fac\": args.scale_fac, \"scale_std\": args.scale_std, \"gate\": args.gate},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            # re-weight the gate rewards\n",
      "            normalized_eta = args.eta / len(logp_actions)\n",
      "            \n",
      "            # collect cumulative future rewards\n",
      "            R = - loss\n",
      "            cum_rewards = []\n",
      "            for r in nfe[::-1]:\n",
      "                R = -normalized_eta * r.view(-1,1) + args.gamma * R\n",
      "                cum_rewards.insert(0,R)\n",
      "            \n",
      "            # apply REINFORCE\n",
      "            rl_loss = 0\n",
      "            for lpa, r in zip(logp_actions, cum_rewards):\n",
      "                rl_loss = rl_loss - lpa.view(-1,1) * args.rl_weight * r\n",
      "                \n",
      "            loss = loss + rl_loss.mean()\n",
      "            \n",
      "            loss.backward()\n",
      "            \n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "            \n",
      "            for idx in range(len(model.module.transforms)):\n",
      "                for layer in model.module.transforms[idx].chain:\n",
      "                    if hasattr(layer, 'atol'):\n",
      "                        layer.odefunc.after_odeint()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'train': atol[tol_indx].mean()}, itr)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'train': rtol[tol_indx].mean()}, itr)\n",
      "                    \n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples, atol, rtol, logp_actions, nfe = model(fixed_z, reverse=True)\n",
      "            generated_samples = generated_samples.view(-1, *data_shape)\n",
      "            for tol_indx in range(len(atol)):\n",
      "                writer.add_scalars('atol_gen_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                writer.add_scalars('rtol_gen_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, condition_ratio=0.5, conditional=True, controlled_tol=False, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, eta=0.1, gamma=0.99, gate='cnn1', imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.001, max_grad_norm=20.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume=None, rl_weight=10.0, rtol=1e-05, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_6_rlw_10_0_run1', scale=1.0, scale_fac=1.0, scale_std=6.0, seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1450732\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc_rl.py:291: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Iter 0010 | Time 13.1408(31.6453) | Bit/dim 8.6885(8.9519) | Xent 2.2803(2.3001) | Loss 11105.8955(11235.6884) | Error 0.7967(0.8602) Steps 0(0.00) | Grad Norm 11173.3009(14178.9205) | Total Time 0.00(0.00)\n",
      "Iter 0020 | Time 12.5269(26.6915) | Bit/dim 8.4696(8.8604) | Xent 2.2220(2.2869) | Loss 10848.7803(11120.9055) | Error 0.7278(0.8323) Steps 0(0.00) | Grad Norm 4248.0406(12202.0851) | Total Time 0.00(0.00)\n",
      "Iter 0030 | Time 12.8571(23.0787) | Bit/dim 8.3378(8.7398) | Xent 2.1630(2.2609) | Loss 10975.7441(10995.0565) | Error 0.7511(0.8094) Steps 0(0.00) | Grad Norm 4499.8271(10017.1388) | Total Time 0.00(0.00)\n",
      "Iter 0040 | Time 13.0323(20.3741) | Bit/dim 8.0768(8.5933) | Xent 2.0969(2.2286) | Loss 10359.4033(10844.4360) | Error 0.7133(0.7908) Steps 0(0.00) | Grad Norm 3101.1717(8171.6842) | Total Time 0.00(0.00)\n",
      "Iter 0050 | Time 12.9763(18.3584) | Bit/dim 7.7467(8.4056) | Xent 2.0858(2.1927) | Loss 9818.1514(10640.9278) | Error 0.7044(0.7726) Steps 0(0.00) | Grad Norm 2687.1593(6727.1723) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0001 | Time 77.0769, Epoch Time 825.2054(825.2054), Bit/dim 7.5020(best: inf), Xent 2.0604, Loss 8.5322, Error 0.6927(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0060 | Time 12.7726(16.9504) | Bit/dim 7.3832(8.1718) | Xent 2.0644(2.1606) | Loss 9413.2744(10945.1451) | Error 0.6978(0.7544) Steps 0(0.00) | Grad Norm 2061.9286(5574.1608) | Total Time 0.00(0.00)\n",
      "Iter 0070 | Time 13.4795(15.9368) | Bit/dim 7.1278(7.9197) | Xent 2.0651(2.1378) | Loss 9442.3789(10549.2984) | Error 0.6900(0.7386) Steps 0(0.00) | Grad Norm 1180.7377(4567.4689) | Total Time 0.00(0.00)\n",
      "Iter 0080 | Time 14.4901(15.2428) | Bit/dim 7.0178(7.6933) | Xent 2.0829(2.1232) | Loss 9305.9717(10201.7726) | Error 0.6933(0.7305) Steps 0(0.00) | Grad Norm 2126.4470(3882.1921) | Total Time 0.00(0.00)\n",
      "Iter 0090 | Time 13.4547(14.8533) | Bit/dim 6.9789(7.5108) | Xent 2.0744(2.1117) | Loss 9330.1660(9927.4296) | Error 0.7289(0.7286) Steps 0(0.00) | Grad Norm 2077.9443(3434.5092) | Total Time 0.00(0.00)\n",
      "Iter 0100 | Time 15.1266(14.6036) | Bit/dim 6.9058(7.3645) | Xent 2.0379(2.0965) | Loss 9263.2090(9719.7066) | Error 0.6900(0.7188) Steps 0(0.00) | Grad Norm 1845.0926(3156.1174) | Total Time 0.00(0.00)\n",
      "Iter 0110 | Time 14.4485(14.5443) | Bit/dim 6.8921(7.2446) | Xent 2.0335(2.0828) | Loss 9337.5205(9564.1450) | Error 0.7178(0.7141) Steps 0(0.00) | Grad Norm 1364.2816(2838.3346) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0002 | Time 78.1119, Epoch Time 845.1048(825.8024), Bit/dim 6.8820(best: 7.5020), Xent 2.0282, Loss 7.8961, Error 0.6833(best: 0.6927)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0120 | Time 14.4101(14.5851) | Bit/dim 6.7999(7.1395) | Xent 2.0350(2.0700) | Loss 8867.2705(9969.7655) | Error 0.7122(0.7088) Steps 0(0.00) | Grad Norm 5186.1039(2848.3769) | Total Time 0.00(0.00)\n",
      "Iter 0130 | Time 14.4428(14.5781) | Bit/dim 6.7365(7.0431) | Xent 2.0225(2.0573) | Loss 9012.1318(9693.3723) | Error 0.6822(0.7029) Steps 0(0.00) | Grad Norm 7574.2086(3426.4462) | Total Time 0.00(0.00)\n",
      "Iter 0140 | Time 16.1017(14.6033) | Bit/dim 6.6854(6.9519) | Xent 2.0366(2.0474) | Loss 8887.4639(9470.9047) | Error 0.6900(0.6989) Steps 0(0.00) | Grad Norm 10763.4410(4505.4557) | Total Time 0.00(0.00)\n",
      "Iter 0150 | Time 15.1865(14.6760) | Bit/dim 6.5669(6.8646) | Xent 1.9995(2.0398) | Loss 8495.5293(9266.4325) | Error 0.7022(0.6969) Steps 0(0.00) | Grad Norm 8506.2199(5882.3896) | Total Time 0.00(0.00)\n",
      "Iter 0160 | Time 15.8998(14.7981) | Bit/dim 6.4932(6.7779) | Xent 2.0266(2.0364) | Loss 8827.7432(9132.3533) | Error 0.7067(0.6972) Steps 0(0.00) | Grad Norm 15295.4448(7705.2755) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0003 | Time 79.3067, Epoch Time 910.5754(828.3456), Bit/dim 6.4502(best: 6.8820), Xent 2.0308, Loss 7.4657, Error 0.7028(best: 0.6833)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0170 | Time 15.3583(14.8564) | Bit/dim 6.4106(6.6916) | Xent 2.0591(2.0361) | Loss 8838.7754(9646.5452) | Error 0.7156(0.6997) Steps 0(0.00) | Grad Norm 30171.6838(11141.2692) | Total Time 0.00(0.00)\n",
      "Iter 0180 | Time 15.8861(15.0178) | Bit/dim 6.3279(6.6042) | Xent 2.0097(2.0335) | Loss 8415.6270(9361.7794) | Error 0.7000(0.7019) Steps 0(0.00) | Grad Norm 25383.3733(15345.1861) | Total Time 0.00(0.00)\n",
      "Iter 0190 | Time 14.1846(14.8951) | Bit/dim 6.2578(6.5189) | Xent 2.0463(2.0366) | Loss 8490.9805(9147.0250) | Error 0.7100(0.7055) Steps 0(0.00) | Grad Norm 35537.9698(20919.7616) | Total Time 0.00(0.00)\n",
      "Iter 0200 | Time 13.9608(14.9338) | Bit/dim 6.1979(6.4416) | Xent 2.0905(2.0422) | Loss 8389.9834(8968.3435) | Error 0.7211(0.7106) Steps 0(0.00) | Grad Norm 61462.6255(26819.4334) | Total Time 0.00(0.00)\n",
      "Iter 0210 | Time 15.1965(15.0606) | Bit/dim 6.1481(6.3690) | Xent 2.0636(2.0510) | Loss 8448.3594(8802.3670) | Error 0.7167(0.7155) Steps 0(0.00) | Grad Norm 38698.4128(33168.1327) | Total Time 0.00(0.00)\n",
      "Iter 0220 | Time 14.3763(14.9986) | Bit/dim 6.1049(6.3017) | Xent 2.1081(2.0547) | Loss 8501.0977(8674.4947) | Error 0.7467(0.7181) Steps 0(0.00) | Grad Norm 76096.2923(38729.9724) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0004 | Time 78.4196, Epoch Time 925.8227(831.2699), Bit/dim 6.0939(best: 6.4502), Xent 2.0686, Loss 7.1282, Error 0.7258(best: 0.6833)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0230 | Time 14.7134(15.1422) | Bit/dim 6.0721(6.2424) | Xent 2.0717(2.0552) | Loss 8154.6904(9087.3006) | Error 0.7467(0.7220) Steps 0(0.00) | Grad Norm 55656.4269(42523.1560) | Total Time 0.00(0.00)\n",
      "Iter 0240 | Time 15.3451(15.0531) | Bit/dim 6.0113(6.1869) | Xent 2.0480(2.0579) | Loss 8375.9609(8831.6993) | Error 0.7256(0.7260) Steps 0(0.00) | Grad Norm 45978.5109(44904.6986) | Total Time 0.00(0.00)\n",
      "Iter 0250 | Time 13.8878(14.9881) | Bit/dim 5.9866(6.1350) | Xent 2.0314(2.0597) | Loss 8092.1636(8663.8097) | Error 0.7233(0.7273) Steps 0(0.00) | Grad Norm 56332.0674(46903.8917) | Total Time 0.00(0.00)\n",
      "Iter 0260 | Time 13.6670(14.8915) | Bit/dim 5.9245(6.0859) | Xent 2.0233(2.0586) | Loss 8034.3281(8543.8827) | Error 0.7144(0.7268) Steps 0(0.00) | Grad Norm 38918.6768(47427.9787) | Total Time 0.00(0.00)\n",
      "Iter 0270 | Time 14.2757(14.9590) | Bit/dim 5.8807(6.0387) | Xent 2.0381(2.0640) | Loss 7517.8560(8435.3666) | Error 0.7178(0.7295) Steps 0(0.00) | Grad Norm 37110.3709(47464.4463) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0005 | Time 79.5090, Epoch Time 921.6913(833.9826), Bit/dim 5.8997(best: 6.0939), Xent 2.0832, Loss 6.9413, Error 0.7360(best: 0.6833)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0280 | Time 15.0305(14.9870) | Bit/dim 5.8221(5.9927) | Xent 2.0478(2.0687) | Loss 8199.8867(8926.7176) | Error 0.7322(0.7343) Steps 0(0.00) | Grad Norm 32794.3881(46959.9639) | Total Time 0.00(0.00)\n",
      "Iter 0290 | Time 14.3265(14.8732) | Bit/dim 5.8920(5.9626) | Xent 2.0754(2.0959) | Loss 8185.4121(8715.5329) | Error 0.7500(0.7486) Steps 0(0.00) | Grad Norm 48156.7142(48881.3672) | Total Time 0.00(0.00)\n",
      "Iter 0300 | Time 13.9488(14.7555) | Bit/dim 5.9078(5.9442) | Xent 2.0411(2.0825) | Loss 8047.6431(8556.5594) | Error 0.7356(0.7399) Steps 0(0.00) | Grad Norm 30871.8233(44557.1716) | Total Time 0.00(0.00)\n",
      "Iter 0310 | Time 14.0273(14.8147) | Bit/dim 5.9707(5.9802) | Xent 2.1438(2.1028) | Loss 7790.2783(8500.1664) | Error 0.7733(0.7446) Steps 0(0.00) | Grad Norm 55759.5181(45357.2274) | Total Time 0.00(0.00)\n",
      "Iter 0320 | Time 14.6850(14.8494) | Bit/dim 5.8255(5.9483) | Xent 2.0429(2.1070) | Loss 7969.9526(8380.4337) | Error 0.7211(0.7497) Steps 0(0.00) | Grad Norm 30507.8736(43492.0323) | Total Time 0.00(0.00)\n",
      "Iter 0330 | Time 14.3182(14.8945) | Bit/dim 5.7717(5.9053) | Xent 2.0138(2.1022) | Loss 7984.6650(8273.5452) | Error 0.6822(0.7483) Steps 0(0.00) | Grad Norm 12032.9898(39948.1873) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0006 | Time 77.8758, Epoch Time 911.0600(836.2949), Bit/dim 5.8649(best: 5.8997), Xent 2.1640, Loss 6.9469, Error 0.8062(best: 0.6833)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0340 | Time 14.8938(14.7824) | Bit/dim 6.1463(5.9248) | Xent 2.4061(2.1566) | Loss 8597.7139(8818.2544) | Error 0.8356(0.7657) Steps 0(0.00) | Grad Norm 83761.1211(49984.5988) | Total Time 0.00(0.00)\n",
      "Iter 0350 | Time 13.7800(14.6785) | Bit/dim 5.7111(5.8929) | Xent 2.1751(2.1605) | Loss 8012.3125(8623.5522) | Error 0.8211(0.7735) Steps 0(0.00) | Grad Norm 27983.9182(44480.4874) | Total Time 0.00(0.00)\n",
      "Iter 0360 | Time 13.6099(14.4884) | Bit/dim 5.7036(5.8512) | Xent 2.0769(2.1534) | Loss 7739.8057(8430.6793) | Error 0.7067(0.7731) Steps 0(0.00) | Grad Norm 14965.9190(39095.7723) | Total Time 0.00(0.00)\n",
      "Iter 0370 | Time 14.1713(14.4163) | Bit/dim 5.6739(5.8066) | Xent 2.0981(2.1612) | Loss 7746.3447(8291.3384) | Error 0.7689(0.7774) Steps 0(0.00) | Grad Norm 17464.5423(35868.4514) | Total Time 0.00(0.00)\n",
      "Iter 0380 | Time 13.5154(14.3301) | Bit/dim 5.6517(5.7661) | Xent 2.1535(2.1525) | Loss 7656.9062(8171.7783) | Error 0.8044(0.7733) Steps 0(0.00) | Grad Norm 15931.5304(29925.0328) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0007 | Time 76.4413, Epoch Time 872.8082(837.3903), Bit/dim 5.5971(best: 5.8649), Xent 2.0642, Loss 6.6292, Error 0.7046(best: 0.6833)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0390 | Time 15.0009(14.2059) | Bit/dim 5.5406(5.7219) | Xent 2.0455(2.1324) | Loss 7803.9614(8634.8237) | Error 0.6878(0.7587) Steps 0(0.00) | Grad Norm 7904.2315(23855.0505) | Total Time 0.00(0.00)\n",
      "Iter 0400 | Time 14.5985(14.1437) | Bit/dim 5.5046(5.6734) | Xent 2.0326(2.1091) | Loss 7883.8472(8386.6568) | Error 0.7200(0.7443) Steps 0(0.00) | Grad Norm 11355.1120(19612.6077) | Total Time 0.00(0.00)\n",
      "Iter 0410 | Time 13.9399(14.0959) | Bit/dim 5.5357(5.6265) | Xent 2.0013(2.0880) | Loss 7441.6250(8196.7933) | Error 0.6667(0.7345) Steps 0(0.00) | Grad Norm 13851.9809(18133.5593) | Total Time 0.00(0.00)\n",
      "Iter 0420 | Time 16.3508(14.1362) | Bit/dim 5.3974(5.5732) | Xent 1.9846(2.0710) | Loss 7662.1514(8037.4154) | Error 0.6967(0.7255) Steps 0(0.00) | Grad Norm 14714.7504(16620.4395) | Total Time 0.00(0.00)\n",
      "Iter 0430 | Time 14.6376(14.2645) | Bit/dim 5.4065(5.5276) | Xent 2.0288(2.0595) | Loss 7634.5488(7896.7467) | Error 0.7167(0.7230) Steps 0(0.00) | Grad Norm 13996.8952(16760.5578) | Total Time 0.00(0.00)\n",
      "Iter 0440 | Time 13.5323(14.3000) | Bit/dim 5.4726(5.4926) | Xent 1.9765(2.0445) | Loss 7458.3218(7795.1915) | Error 0.6933(0.7167) Steps 0(0.00) | Grad Norm 6897.8034(16187.7999) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0008 | Time 76.2156, Epoch Time 876.4937(838.5634), Bit/dim 5.4244(best: 5.5971), Xent 2.1274, Loss 6.4881, Error 0.7792(best: 0.6833)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0450 | Time 14.8681(14.2062) | Bit/dim 5.3685(5.4703) | Xent 2.0103(2.0462) | Loss 7242.1929(8257.6847) | Error 0.6833(0.7180) Steps 0(0.00) | Grad Norm 7244.5270(16141.3533) | Total Time 0.00(0.00)\n",
      "Iter 0460 | Time 15.9621(14.3584) | Bit/dim 5.2966(5.4240) | Xent 2.0574(2.0387) | Loss 7599.1240(8064.8653) | Error 0.7478(0.7161) Steps 0(0.00) | Grad Norm 13252.4117(14688.2255) | Total Time 0.00(0.00)\n",
      "Iter 0470 | Time 15.9766(14.4371) | Bit/dim 5.2504(5.3774) | Xent 2.0082(2.0289) | Loss 7300.7017(7903.1799) | Error 0.7222(0.7157) Steps 0(0.00) | Grad Norm 6514.2136(12742.9424) | Total Time 0.00(0.00)\n",
      "Iter 0480 | Time 15.2979(14.4226) | Bit/dim 5.1658(5.3280) | Xent 1.9844(2.0189) | Loss 7429.8311(7749.1621) | Error 0.7033(0.7131) Steps 0(0.00) | Grad Norm 9589.9514(11815.6817) | Total Time 0.00(0.00)\n",
      "Iter 0490 | Time 14.3260(14.4158) | Bit/dim 5.2023(5.2838) | Xent 1.9515(2.0052) | Loss 7449.0366(7634.3099) | Error 0.6856(0.7050) Steps 0(0.00) | Grad Norm 11942.3817(11429.5793) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0009 | Time 76.2765, Epoch Time 889.7300(840.0984), Bit/dim 5.1210(best: 5.4244), Xent 1.9274, Loss 6.0847, Error 0.6596(best: 0.6833)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0500 | Time 13.5845(14.4213) | Bit/dim 5.1208(5.2391) | Xent 1.9541(1.9894) | Loss 7034.9727(8048.8565) | Error 0.6900(0.6978) Steps 0(0.00) | Grad Norm 12768.1869(11171.3125) | Total Time 0.00(0.00)\n",
      "Iter 0510 | Time 14.2141(14.3825) | Bit/dim 5.1196(5.1981) | Xent 1.8991(1.9720) | Loss 7314.4717(7812.6304) | Error 0.6656(0.6918) Steps 0(0.00) | Grad Norm 11553.2565(10284.4672) | Total Time 0.00(0.00)\n",
      "Iter 0520 | Time 14.9339(14.5649) | Bit/dim 5.0519(5.1604) | Xent 1.8546(1.9527) | Loss 7272.0796(7643.0875) | Error 0.6189(0.6836) Steps 0(0.00) | Grad Norm 5783.0762(9867.5503) | Total Time 0.00(0.00)\n",
      "Iter 0530 | Time 13.2043(14.6261) | Bit/dim 5.1369(5.1387) | Xent 1.9644(1.9481) | Loss 7005.8965(7531.8121) | Error 0.6911(0.6815) Steps 0(0.00) | Grad Norm 13313.7065(10822.0415) | Total Time 0.00(0.00)\n",
      "Iter 0540 | Time 15.2915(14.7868) | Bit/dim 4.9733(5.1070) | Xent 1.9539(1.9473) | Loss 7214.1729(7419.9794) | Error 0.7178(0.6837) Steps 0(0.00) | Grad Norm 13214.3150(10575.6453) | Total Time 0.00(0.00)\n",
      "Iter 0550 | Time 16.7664(14.9780) | Bit/dim 4.9814(5.0794) | Xent 1.9905(1.9456) | Loss 7180.4219(7339.3495) | Error 0.6800(0.6811) Steps 0(0.00) | Grad Norm 13501.3704(10584.4819) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0010 | Time 78.2539, Epoch Time 915.6561(842.3651), Bit/dim 4.9600(best: 5.1210), Xent 1.8894, Loss 5.9047, Error 0.6440(best: 0.6596)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0560 | Time 14.9524(14.9375) | Bit/dim 4.9222(5.0441) | Xent 1.8969(1.9389) | Loss 6923.5620(7785.0230) | Error 0.6722(0.6805) Steps 0(0.00) | Grad Norm 7249.0287(9965.6003) | Total Time 0.00(0.00)\n",
      "Iter 0570 | Time 14.4943(14.8498) | Bit/dim 5.0076(5.0158) | Xent 1.9832(1.9501) | Loss 7016.2617(7570.6727) | Error 0.7211(0.6886) Steps 0(0.00) | Grad Norm 16414.2847(10405.4248) | Total Time 0.00(0.00)\n",
      "Iter 0580 | Time 15.2148(14.8941) | Bit/dim 4.9007(4.9849) | Xent 1.8940(1.9440) | Loss 6913.0039(7441.9792) | Error 0.6333(0.6854) Steps 0(0.00) | Grad Norm 2533.9464(9631.3193) | Total Time 0.00(0.00)\n",
      "Iter 0590 | Time 13.6305(14.9229) | Bit/dim 5.2207(4.9835) | Xent 1.9164(1.9344) | Loss 6959.3945(7335.1729) | Error 0.6611(0.6818) Steps 0(0.00) | Grad Norm 16701.4940(10784.8493) | Total Time 0.00(0.00)\n",
      "Iter 0600 | Time 15.5410(14.8654) | Bit/dim 4.9064(4.9706) | Xent 1.9698(1.9350) | Loss 6891.2095(7243.3490) | Error 0.6978(0.6818) Steps 0(0.00) | Grad Norm 16581.3368(11196.4576) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0011 | Time 79.1891, Epoch Time 913.2832(844.4927), Bit/dim 4.9037(best: 4.9600), Xent 1.9013, Loss 5.8543, Error 0.6750(best: 0.6440)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0610 | Time 13.9208(14.8796) | Bit/dim 4.8120(4.9396) | Xent 1.9734(1.9325) | Loss 6509.1021(7761.3900) | Error 0.7122(0.6830) Steps 0(0.00) | Grad Norm 12066.8342(11167.6678) | Total Time 0.00(0.00)\n",
      "Iter 0620 | Time 14.9800(14.9829) | Bit/dim 4.7403(4.8971) | Xent 1.7931(1.9133) | Loss 6812.5137(7521.2022) | Error 0.6156(0.6757) Steps 0(0.00) | Grad Norm 3740.0895(9979.4879) | Total Time 0.00(0.00)\n",
      "Iter 0630 | Time 15.7587(14.9729) | Bit/dim 4.7394(4.8640) | Xent 1.7692(1.8900) | Loss 6573.8384(7318.0272) | Error 0.6222(0.6682) Steps 0(0.00) | Grad Norm 2200.0542(9478.9173) | Total Time 0.00(0.00)\n",
      "Iter 0640 | Time 15.1861(15.0796) | Bit/dim 4.8506(4.8399) | Xent 1.8665(1.8810) | Loss 7093.2466(7194.9396) | Error 0.6444(0.6645) Steps 0(0.00) | Grad Norm 20531.5065(10625.9102) | Total Time 0.00(0.00)\n",
      "Iter 0650 | Time 15.1312(15.0105) | Bit/dim 4.7773(4.8185) | Xent 1.8628(1.8705) | Loss 6687.4307(7088.4764) | Error 0.6778(0.6632) Steps 0(0.00) | Grad Norm 16423.9508(10912.1946) | Total Time 0.00(0.00)\n",
      "Iter 0660 | Time 15.3521(14.9960) | Bit/dim 4.6792(4.7908) | Xent 1.8448(1.8663) | Loss 6804.5190(7009.9589) | Error 0.6567(0.6621) Steps 0(0.00) | Grad Norm 5273.8053(10459.2690) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0012 | Time 82.9999, Epoch Time 929.3743(847.0391), Bit/dim 4.6940(best: 4.9037), Xent 1.7988, Loss 5.5934, Error 0.6395(best: 0.6440)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0670 | Time 15.2895(15.1646) | Bit/dim 4.7050(4.7645) | Xent 1.9408(1.8509) | Loss 6860.6655(7421.5657) | Error 0.6856(0.6553) Steps 0(0.00) | Grad Norm 28290.8506(10577.5206) | Total Time 0.00(0.00)\n",
      "Iter 0680 | Time 14.9834(15.3849) | Bit/dim 4.6179(4.7420) | Xent 1.8180(1.8536) | Loss 6666.6382(7240.9142) | Error 0.6578(0.6564) Steps 0(0.00) | Grad Norm 11294.4479(11652.8983) | Total Time 0.00(0.00)\n",
      "Iter 0690 | Time 16.0752(15.4740) | Bit/dim 4.7149(4.7294) | Xent 1.9542(1.8579) | Loss 6622.0366(7110.2653) | Error 0.6878(0.6591) Steps 0(0.00) | Grad Norm 9400.6054(11977.6675) | Total Time 0.00(0.00)\n",
      "Iter 0700 | Time 16.2143(15.5332) | Bit/dim 4.6838(4.7069) | Xent 1.8187(1.8482) | Loss 6663.8701(6991.6123) | Error 0.6722(0.6555) Steps 0(0.00) | Grad Norm 5147.5531(10699.2633) | Total Time 0.00(0.00)\n",
      "Iter 0710 | Time 16.0111(15.6473) | Bit/dim 4.6517(4.6951) | Xent 1.7223(1.8377) | Loss 6800.7061(6935.0804) | Error 0.6044(0.6521) Steps 0(0.00) | Grad Norm 3327.1889(10509.5031) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0013 | Time 83.5043, Epoch Time 971.7725(850.7811), Bit/dim 4.6704(best: 4.6940), Xent 1.7457, Loss 5.5432, Error 0.6003(best: 0.6395)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0720 | Time 16.8421(15.7427) | Bit/dim 4.7079(4.6859) | Xent 1.8405(1.8420) | Loss 6716.6831(7498.7322) | Error 0.6622(0.6534) Steps 0(0.00) | Grad Norm 13873.9883(10850.9210) | Total Time 0.00(0.00)\n",
      "Iter 0730 | Time 14.7805(15.8728) | Bit/dim 4.6240(4.6786) | Xent 1.8272(1.8551) | Loss 6340.1704(7286.8412) | Error 0.6378(0.6557) Steps 0(0.00) | Grad Norm 10091.0042(12250.8715) | Total Time 0.00(0.00)\n",
      "Iter 0740 | Time 16.4843(16.1745) | Bit/dim 4.6280(4.6761) | Xent 1.8156(1.8523) | Loss 6800.9155(7156.7057) | Error 0.6578(0.6539) Steps 0(0.00) | Grad Norm 13656.7232(12481.2733) | Total Time 0.00(0.00)\n",
      "Iter 0750 | Time 15.0147(16.1780) | Bit/dim 4.5676(4.6555) | Xent 1.7538(1.8439) | Loss 6527.6904(7010.9257) | Error 0.6289(0.6533) Steps 0(0.00) | Grad Norm 3286.9754(11343.7412) | Total Time 0.00(0.00)\n",
      "Iter 0760 | Time 14.8024(16.1082) | Bit/dim 4.6595(4.6716) | Xent 1.7590(1.8358) | Loss 6870.4648(6959.7920) | Error 0.6100(0.6489) Steps 0(0.00) | Grad Norm 6628.4385(11837.7774) | Total Time 0.00(0.00)\n",
      "Iter 0770 | Time 15.5894(16.0942) | Bit/dim 4.5682(4.6488) | Xent 1.7081(1.8157) | Loss 6602.3921(6863.4445) | Error 0.5922(0.6426) Steps 0(0.00) | Grad Norm 7013.2438(10558.7240) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0014 | Time 84.7088, Epoch Time 999.4155(855.2401), Bit/dim 4.5593(best: 4.6704), Xent 1.7046, Loss 5.4116, Error 0.6018(best: 0.6003)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0780 | Time 17.4446(16.3337) | Bit/dim 4.5615(4.6280) | Xent 1.7484(1.7999) | Loss 6365.7490(7336.0431) | Error 0.6256(0.6392) Steps 0(0.00) | Grad Norm 7749.7317(10579.5289) | Total Time 0.00(0.00)\n",
      "Iter 0790 | Time 15.4688(16.2740) | Bit/dim 4.4965(4.6051) | Xent 1.7005(1.7810) | Loss 6670.8584(7142.5800) | Error 0.6144(0.6312) Steps 0(0.00) | Grad Norm 2753.2872(10205.0637) | Total Time 0.00(0.00)\n",
      "Iter 0800 | Time 16.1721(16.1640) | Bit/dim 4.5166(4.5818) | Xent 1.7432(1.7813) | Loss 6452.3618(6987.5639) | Error 0.6167(0.6310) Steps 0(0.00) | Grad Norm 7592.5515(10312.7095) | Total Time 0.00(0.00)\n",
      "Iter 0810 | Time 15.5688(16.2703) | Bit/dim 4.4438(4.5525) | Xent 1.7375(1.7680) | Loss 6407.0820(6861.5128) | Error 0.5911(0.6266) Steps 0(0.00) | Grad Norm 3845.3830(9404.8697) | Total Time 0.00(0.00)\n",
      "Iter 0820 | Time 19.6897(16.2763) | Bit/dim 4.5251(4.5582) | Xent 1.7902(1.7803) | Loss 6528.4502(6790.9046) | Error 0.6356(0.6285) Steps 0(0.00) | Grad Norm 7064.5907(10247.9943) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0015 | Time 84.6346, Epoch Time 1002.1898(859.6486), Bit/dim 4.5174(best: 4.5593), Xent 1.6979, Loss 5.3663, Error 0.6100(best: 0.6003)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0830 | Time 16.1963(16.2768) | Bit/dim 4.4580(4.5452) | Xent 1.8036(1.7728) | Loss 6443.9253(7368.8899) | Error 0.6644(0.6292) Steps 0(0.00) | Grad Norm 12895.4927(9459.3832) | Total Time 0.00(0.00)\n",
      "Iter 0840 | Time 16.3307(16.3408) | Bit/dim 4.4641(4.5258) | Xent 1.8808(1.7685) | Loss 6482.6758(7135.9817) | Error 0.6489(0.6282) Steps 0(0.00) | Grad Norm 14931.1392(9299.4252) | Total Time 0.00(0.00)\n",
      "Iter 0850 | Time 16.2402(16.2312) | Bit/dim 4.4072(4.5019) | Xent 1.6883(1.7575) | Loss 6402.6870(6962.0123) | Error 0.5956(0.6260) Steps 0(0.00) | Grad Norm 9205.0972(9484.2058) | Total Time 0.00(0.00)\n",
      "Iter 0860 | Time 14.8620(16.1558) | Bit/dim 4.4296(4.4855) | Xent 1.6821(1.7412) | Loss 6286.0947(6815.7522) | Error 0.5933(0.6214) Steps 0(0.00) | Grad Norm 8032.5095(9090.8092) | Total Time 0.00(0.00)\n",
      "Iter 0870 | Time 16.0121(16.3245) | Bit/dim 4.4692(4.4649) | Xent 1.8577(1.7514) | Loss 6583.4097(6758.2560) | Error 0.6600(0.6263) Steps 0(0.00) | Grad Norm 10787.3678(9715.7604) | Total Time 0.00(0.00)\n",
      "Iter 0880 | Time 16.9149(16.4134) | Bit/dim 4.4227(4.4508) | Xent 1.6527(1.7414) | Loss 6449.5215(6690.0964) | Error 0.5733(0.6207) Steps 0(0.00) | Grad Norm 7958.7689(9303.5049) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0016 | Time 83.4501, Epoch Time 1000.0431(863.8605), Bit/dim 4.4232(best: 4.5174), Xent 1.7025, Loss 5.2744, Error 0.6027(best: 0.6003)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0890 | Time 15.8885(16.4380) | Bit/dim 4.3678(4.4371) | Xent 1.6399(1.7290) | Loss 6284.6772(7154.6645) | Error 0.5744(0.6176) Steps 0(0.00) | Grad Norm 2991.4935(8905.4124) | Total Time 0.00(0.00)\n",
      "Iter 0900 | Time 15.6033(16.5036) | Bit/dim 4.3607(4.4206) | Xent 1.6548(1.7158) | Loss 6100.9136(6948.4942) | Error 0.5944(0.6127) Steps 0(0.00) | Grad Norm 8799.9035(8809.7289) | Total Time 0.00(0.00)\n",
      "Iter 0910 | Time 18.7462(16.6487) | Bit/dim 4.4037(4.4198) | Xent 1.8174(1.7251) | Loss 6647.9980(6847.7285) | Error 0.6589(0.6139) Steps 0(0.00) | Grad Norm 10469.6390(9415.2948) | Total Time 0.00(0.00)\n",
      "Iter 0920 | Time 15.6400(16.5286) | Bit/dim 4.3294(4.4041) | Xent 1.7151(1.7226) | Loss 6039.6099(6708.0081) | Error 0.6389(0.6149) Steps 0(0.00) | Grad Norm 5966.5452(8784.7614) | Total Time 0.00(0.00)\n",
      "Iter 0930 | Time 16.1645(16.4902) | Bit/dim 4.6503(4.4162) | Xent 1.9484(1.7411) | Loss 6766.3403(6663.8003) | Error 0.6944(0.6203) Steps 0(0.00) | Grad Norm 18594.6020(9941.7676) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0017 | Time 83.8131, Epoch Time 1012.6004(868.3227), Bit/dim 4.4020(best: 4.4232), Xent 1.6558, Loss 5.2299, Error 0.5911(best: 0.6003)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0940 | Time 17.3939(16.4450) | Bit/dim 4.3759(4.4100) | Xent 1.7226(1.7418) | Loss 6456.8994(7223.1025) | Error 0.6044(0.6207) Steps 0(0.00) | Grad Norm 10420.5236(9506.1139) | Total Time 0.00(0.00)\n",
      "Iter 0950 | Time 16.8118(16.3797) | Bit/dim 4.3101(4.3941) | Xent 1.5741(1.7269) | Loss 6394.3726(6982.6649) | Error 0.5689(0.6159) Steps 0(0.00) | Grad Norm 4793.1685(9071.7102) | Total Time 0.00(0.00)\n",
      "Iter 0960 | Time 15.4632(16.3424) | Bit/dim 4.3165(4.3712) | Xent 1.7075(1.7121) | Loss 6041.0444(6770.5607) | Error 0.6167(0.6119) Steps 0(0.00) | Grad Norm 8670.5563(8535.8684) | Total Time 0.00(0.00)\n",
      "Iter 0970 | Time 15.5540(16.4076) | Bit/dim 4.2424(4.3475) | Xent 1.6708(1.6913) | Loss 6206.2100(6644.1027) | Error 0.5811(0.6045) Steps 0(0.00) | Grad Norm 2591.5759(7481.7917) | Total Time 0.00(0.00)\n",
      "Iter 0980 | Time 16.9585(16.3975) | Bit/dim 4.3118(4.3349) | Xent 1.9091(1.7054) | Loss 6413.7007(6562.1093) | Error 0.6644(0.6070) Steps 0(0.00) | Grad Norm 13577.6998(8420.2774) | Total Time 0.00(0.00)\n",
      "Iter 0990 | Time 20.2135(16.4681) | Bit/dim 4.3048(4.3301) | Xent 1.8027(1.7331) | Loss 6507.6016(6517.5093) | Error 0.6400(0.6157) Steps 0(0.00) | Grad Norm 10095.5364(9536.4332) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0018 | Time 85.8457, Epoch Time 1003.7033(872.3841), Bit/dim 4.2630(best: 4.4020), Xent 1.5882, Loss 5.0572, Error 0.5655(best: 0.5911)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1000 | Time 15.8817(16.5081) | Bit/dim 4.2324(4.3084) | Xent 1.5518(1.7076) | Loss 6363.5083(7019.3497) | Error 0.5878(0.6097) Steps 0(0.00) | Grad Norm 5993.0111(8678.5430) | Total Time 0.00(0.00)\n",
      "Iter 1010 | Time 15.1895(16.4345) | Bit/dim 4.2572(4.3013) | Xent 1.6290(1.6963) | Loss 6106.0806(6815.8782) | Error 0.5956(0.6068) Steps 0(0.00) | Grad Norm 5889.6109(8457.8068) | Total Time 0.00(0.00)\n",
      "Iter 1020 | Time 17.3271(16.3274) | Bit/dim 4.2380(4.2868) | Xent 1.5787(1.6790) | Loss 6277.2158(6668.6112) | Error 0.5900(0.6037) Steps 0(0.00) | Grad Norm 6984.1060(8049.1435) | Total Time 0.00(0.00)\n",
      "Iter 1030 | Time 15.9590(16.2373) | Bit/dim 4.1792(4.2637) | Xent 1.5714(1.6583) | Loss 6050.5586(6540.3459) | Error 0.5644(0.5966) Steps 0(0.00) | Grad Norm 3635.3096(7345.8490) | Total Time 0.00(0.00)\n",
      "Iter 1040 | Time 16.0679(16.2418) | Bit/dim 4.2209(4.2471) | Xent 1.6777(1.6512) | Loss 6089.4712(6441.3495) | Error 0.5956(0.5933) Steps 0(0.00) | Grad Norm 7242.0841(7107.0866) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0019 | Time 83.4220, Epoch Time 997.3143(876.1320), Bit/dim 4.2267(best: 4.2630), Xent 1.6090, Loss 5.0312, Error 0.5657(best: 0.5655)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1050 | Time 16.9048(16.3875) | Bit/dim 4.1622(4.2377) | Xent 1.6048(1.6482) | Loss 6107.3398(6957.1061) | Error 0.5711(0.5925) Steps 0(0.00) | Grad Norm 5867.1190(7725.6821) | Total Time 0.00(0.00)\n",
      "Iter 1060 | Time 16.7238(16.5269) | Bit/dim 4.1430(4.2242) | Xent 1.5878(1.6427) | Loss 6260.0938(6764.6151) | Error 0.5600(0.5901) Steps 0(0.00) | Grad Norm 7668.6697(8064.9739) | Total Time 0.00(0.00)\n",
      "Iter 1070 | Time 16.1485(16.5871) | Bit/dim 4.1470(4.2034) | Xent 1.5194(1.6183) | Loss 6064.5474(6587.3117) | Error 0.5533(0.5814) Steps 0(0.00) | Grad Norm 4984.1238(7148.5510) | Total Time 0.00(0.00)\n",
      "Iter 1080 | Time 17.9024(16.5522) | Bit/dim 4.1452(4.1912) | Xent 1.6968(1.6099) | Loss 6277.8813(6468.8316) | Error 0.6100(0.5789) Steps 0(0.00) | Grad Norm 11660.3071(7135.1079) | Total Time 0.00(0.00)\n",
      "Iter 1090 | Time 16.2100(16.5249) | Bit/dim 4.1068(4.1777) | Xent 1.5881(1.6025) | Loss 6125.9048(6355.2486) | Error 0.5433(0.5759) Steps 0(0.00) | Grad Norm 3322.9611(6700.0239) | Total Time 0.00(0.00)\n",
      "Iter 1100 | Time 16.9033(16.6471) | Bit/dim 4.1247(4.1669) | Xent 1.6396(1.5967) | Loss 6031.7222(6277.0691) | Error 0.5900(0.5734) Steps 0(0.00) | Grad Norm 8138.5189(6489.7894) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0020 | Time 81.2358, Epoch Time 1018.1376(880.3922), Bit/dim 4.1113(best: 4.2267), Xent 1.5257, Loss 4.8742, Error 0.5474(best: 0.5655)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1110 | Time 15.8236(16.6793) | Bit/dim 4.1590(4.1598) | Xent 1.5116(1.5958) | Loss 6009.1108(6739.0219) | Error 0.5600(0.5732) Steps 0(0.00) | Grad Norm 6291.8272(7155.1149) | Total Time 0.00(0.00)\n",
      "Iter 1120 | Time 17.7525(16.5036) | Bit/dim 4.1168(4.1477) | Xent 1.4456(1.5856) | Loss 6041.5928(6554.1070) | Error 0.5089(0.5711) Steps 0(0.00) | Grad Norm 6722.3222(7206.3020) | Total Time 0.00(0.00)\n",
      "Iter 1130 | Time 19.2393(16.5668) | Bit/dim 4.1173(4.1383) | Xent 1.6289(1.5707) | Loss 6063.7510(6410.7795) | Error 0.5911(0.5677) Steps 0(0.00) | Grad Norm 11252.5823(7114.1172) | Total Time 0.00(0.00)\n",
      "Iter 1140 | Time 16.2393(16.4780) | Bit/dim 4.1226(4.1306) | Xent 1.4835(1.5543) | Loss 5820.0059(6297.5371) | Error 0.5256(0.5623) Steps 0(0.00) | Grad Norm 5570.4066(6757.7505) | Total Time 0.00(0.00)\n",
      "Iter 1150 | Time 14.8482(16.4019) | Bit/dim 4.0680(4.1171) | Xent 1.5034(1.5402) | Loss 5984.8164(6183.7847) | Error 0.5644(0.5581) Steps 0(0.00) | Grad Norm 7865.0434(6434.3399) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0021 | Time 81.7020, Epoch Time 1000.8863(884.0070), Bit/dim 4.0740(best: 4.1113), Xent 1.4055, Loss 4.7768, Error 0.5129(best: 0.5474)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1160 | Time 16.8388(16.4976) | Bit/dim 4.0913(4.1181) | Xent 1.5118(1.5326) | Loss 5983.5571(6744.7677) | Error 0.5478(0.5539) Steps 0(0.00) | Grad Norm 5963.9354(6935.2604) | Total Time 0.00(0.00)\n",
      "Iter 1170 | Time 15.7051(16.2979) | Bit/dim 4.0838(4.1159) | Xent 1.5336(1.5238) | Loss 6024.1362(6547.7150) | Error 0.5544(0.5495) Steps 0(0.00) | Grad Norm 8273.4481(6961.8249) | Total Time 0.00(0.00)\n",
      "Iter 1180 | Time 15.4179(16.3105) | Bit/dim 4.0963(4.1218) | Xent 1.5183(1.5312) | Loss 5680.5928(6407.9167) | Error 0.5600(0.5541) Steps 0(0.00) | Grad Norm 9107.3849(7365.3092) | Total Time 0.00(0.00)\n",
      "Iter 1190 | Time 15.9766(16.2472) | Bit/dim 4.0831(4.1133) | Xent 1.5960(1.5206) | Loss 5954.2075(6285.2740) | Error 0.5733(0.5507) Steps 0(0.00) | Grad Norm 7164.7696(7133.1083) | Total Time 0.00(0.00)\n",
      "Iter 1200 | Time 14.8299(16.1628) | Bit/dim 4.0626(4.0972) | Xent 1.4857(1.5081) | Loss 5952.0176(6195.4588) | Error 0.5378(0.5468) Steps 0(0.00) | Grad Norm 4421.5296(6615.7797) | Total Time 0.00(0.00)\n",
      "Iter 1210 | Time 15.1749(16.2295) | Bit/dim 4.0592(4.0859) | Xent 1.4874(1.5018) | Loss 5937.0356(6115.1140) | Error 0.5389(0.5458) Steps 0(0.00) | Grad Norm 8783.0017(6407.5714) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0022 | Time 82.2354, Epoch Time 990.6322(887.2057), Bit/dim 4.0645(best: 4.0740), Xent 1.4334, Loss 4.7811, Error 0.5232(best: 0.5129)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1220 | Time 15.9903(16.3908) | Bit/dim 4.0347(4.0737) | Xent 1.4866(1.4928) | Loss 6015.6753(6584.9772) | Error 0.5444(0.5414) Steps 0(0.00) | Grad Norm 8899.8680(6741.9217) | Total Time 0.00(0.00)\n",
      "Iter 1230 | Time 17.0324(16.3028) | Bit/dim 4.0179(4.0639) | Xent 1.4618(1.4830) | Loss 5665.6196(6380.4774) | Error 0.5133(0.5360) Steps 0(0.00) | Grad Norm 6062.4695(6468.2335) | Total Time 0.00(0.00)\n",
      "Iter 1240 | Time 16.7921(16.2615) | Bit/dim 4.0306(4.0520) | Xent 1.4089(1.4655) | Loss 5819.5210(6215.5315) | Error 0.5044(0.5310) Steps 0(0.00) | Grad Norm 5841.2958(6092.5239) | Total Time 0.00(0.00)\n",
      "Iter 1250 | Time 18.3208(16.3676) | Bit/dim 4.0117(4.0453) | Xent 1.3911(1.4552) | Loss 5999.3325(6134.2757) | Error 0.4956(0.5281) Steps 0(0.00) | Grad Norm 3569.1839(5757.1011) | Total Time 0.00(0.00)\n",
      "Iter 1260 | Time 14.6498(16.2467) | Bit/dim 4.0295(4.0377) | Xent 1.3936(1.4404) | Loss 5683.9014(6041.0947) | Error 0.5089(0.5244) Steps 0(0.00) | Grad Norm 5605.0150(5338.2754) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0023 | Time 79.8686, Epoch Time 996.7995(890.4935), Bit/dim 4.0232(best: 4.0645), Xent 1.3272, Loss 4.6868, Error 0.4884(best: 0.5129)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1270 | Time 17.0878(16.2677) | Bit/dim 3.9892(4.0320) | Xent 1.4560(1.4302) | Loss 6073.6216(6591.6267) | Error 0.5300(0.5201) Steps 0(0.00) | Grad Norm 10438.3030(5468.7865) | Total Time 0.00(0.00)\n",
      "Iter 1280 | Time 15.1176(16.1729) | Bit/dim 4.0217(4.0277) | Xent 1.4014(1.4240) | Loss 5580.0874(6402.1745) | Error 0.4978(0.5166) Steps 0(0.00) | Grad Norm 4762.8937(5915.1441) | Total Time 0.00(0.00)\n",
      "Iter 1290 | Time 15.8080(16.1324) | Bit/dim 4.0295(4.0235) | Xent 1.4443(1.4247) | Loss 5907.1250(6271.6088) | Error 0.5289(0.5155) Steps 0(0.00) | Grad Norm 5171.1785(6342.6901) | Total Time 0.00(0.00)\n",
      "Iter 1300 | Time 14.4208(15.9564) | Bit/dim 4.0100(4.0156) | Xent 1.4262(1.4251) | Loss 5756.1978(6147.7127) | Error 0.5122(0.5161) Steps 0(0.00) | Grad Norm 7127.2811(5909.6513) | Total Time 0.00(0.00)\n",
      "Iter 1310 | Time 16.2297(15.9813) | Bit/dim 4.0081(4.0140) | Xent 1.3809(1.4122) | Loss 6068.8521(6064.0736) | Error 0.4833(0.5128) Steps 0(0.00) | Grad Norm 5692.1154(5956.0761) | Total Time 0.00(0.00)\n",
      "Iter 1320 | Time 16.2512(16.0644) | Bit/dim 3.9792(4.0035) | Xent 1.3897(1.4056) | Loss 5917.1787(5973.1637) | Error 0.4978(0.5095) Steps 0(0.00) | Grad Norm 5038.7788(5902.9904) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0024 | Time 81.9290, Epoch Time 978.4952(893.1336), Bit/dim 3.9716(best: 4.0232), Xent 1.2764, Loss 4.6098, Error 0.4680(best: 0.4884)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1330 | Time 15.1912(16.0694) | Bit/dim 3.9842(3.9984) | Xent 1.2668(1.3924) | Loss 5833.8159(6452.5517) | Error 0.4444(0.5035) Steps 0(0.00) | Grad Norm 2829.3084(5772.2990) | Total Time 0.00(0.00)\n",
      "Iter 1340 | Time 15.2401(16.0905) | Bit/dim 3.9735(3.9903) | Xent 1.3823(1.3855) | Loss 5514.4204(6267.0914) | Error 0.5000(0.5002) Steps 0(0.00) | Grad Norm 4993.5364(5601.5344) | Total Time 0.00(0.00)\n",
      "Iter 1350 | Time 16.6832(16.1198) | Bit/dim 3.9781(3.9848) | Xent 1.3700(1.3744) | Loss 5738.5146(6142.8479) | Error 0.4989(0.4967) Steps 0(0.00) | Grad Norm 4542.7125(5361.3515) | Total Time 0.00(0.00)\n",
      "Iter 1360 | Time 15.1484(16.0870) | Bit/dim 3.9905(3.9791) | Xent 1.3873(1.3703) | Loss 5826.4058(6048.0486) | Error 0.4822(0.4967) Steps 0(0.00) | Grad Norm 4902.1841(5063.9513) | Total Time 0.00(0.00)\n",
      "Iter 1370 | Time 16.1836(16.0715) | Bit/dim 3.9524(3.9704) | Xent 1.3716(1.3664) | Loss 5869.3550(5983.9452) | Error 0.5078(0.4962) Steps 0(0.00) | Grad Norm 7569.8483(5061.8585) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0025 | Time 81.3833, Epoch Time 986.2081(895.9258), Bit/dim 3.9713(best: 3.9716), Xent 1.3027, Loss 4.6226, Error 0.4779(best: 0.4680)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1380 | Time 17.3172(16.1450) | Bit/dim 3.9253(3.9665) | Xent 1.4531(1.3650) | Loss 5979.5894(6508.1914) | Error 0.5222(0.4941) Steps 0(0.00) | Grad Norm 11932.2899(5749.5219) | Total Time 0.00(0.00)\n",
      "Iter 1390 | Time 16.6426(16.2222) | Bit/dim 4.0136(3.9648) | Xent 1.4435(1.3848) | Loss 6027.0659(6323.0871) | Error 0.5378(0.4993) Steps 0(0.00) | Grad Norm 7491.2766(6409.5127) | Total Time 0.00(0.00)\n",
      "Iter 1400 | Time 15.0151(16.1607) | Bit/dim 3.9745(3.9625) | Xent 1.3322(1.3845) | Loss 5770.8994(6179.9648) | Error 0.4822(0.4997) Steps 0(0.00) | Grad Norm 7053.0696(6355.1170) | Total Time 0.00(0.00)\n",
      "Iter 1410 | Time 17.0632(16.1053) | Bit/dim 3.9691(3.9598) | Xent 1.2869(1.3758) | Loss 5696.9790(6059.8667) | Error 0.4656(0.4975) Steps 0(0.00) | Grad Norm 5704.6434(6011.2282) | Total Time 0.00(0.00)\n",
      "Iter 1420 | Time 15.5042(16.1213) | Bit/dim 3.9297(3.9571) | Xent 1.3043(1.3588) | Loss 5826.6035(5984.2819) | Error 0.4678(0.4916) Steps 0(0.00) | Grad Norm 3744.4606(5718.6698) | Total Time 0.00(0.00)\n",
      "Iter 1430 | Time 17.5815(16.1342) | Bit/dim 3.9202(3.9541) | Xent 1.3337(1.3488) | Loss 5861.5977(5922.4952) | Error 0.4933(0.4880) Steps 0(0.00) | Grad Norm 5690.8236(5570.9444) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0026 | Time 82.5213, Epoch Time 988.6112(898.7064), Bit/dim 3.9332(best: 3.9713), Xent 1.2633, Loss 4.5648, Error 0.4616(best: 0.4680)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1440 | Time 15.5792(16.1424) | Bit/dim 3.9549(3.9475) | Xent 1.2142(1.3374) | Loss 5639.4419(6380.2187) | Error 0.4444(0.4847) Steps 0(0.00) | Grad Norm 4592.6353(5253.5867) | Total Time 0.00(0.00)\n",
      "Iter 1450 | Time 15.5335(16.0314) | Bit/dim 3.9097(3.9415) | Xent 1.2669(1.3222) | Loss 5489.7998(6188.1467) | Error 0.4644(0.4788) Steps 0(0.00) | Grad Norm 3991.0205(4908.7433) | Total Time 0.00(0.00)\n",
      "Iter 1460 | Time 16.4258(16.2043) | Bit/dim 3.8760(3.9350) | Xent 1.3119(1.3303) | Loss 5544.8672(6067.6279) | Error 0.4700(0.4805) Steps 0(0.00) | Grad Norm 3985.6867(5127.5646) | Total Time 0.00(0.00)\n",
      "Iter 1470 | Time 15.8803(16.2673) | Bit/dim 3.9296(3.9360) | Xent 1.3426(1.3279) | Loss 5997.2119(6010.9674) | Error 0.4944(0.4826) Steps 0(0.00) | Grad Norm 7952.0345(5583.6969) | Total Time 0.00(0.00)\n",
      "Iter 1480 | Time 14.2555(16.1637) | Bit/dim 3.9071(3.9322) | Xent 1.3317(1.3247) | Loss 5309.5806(5918.3934) | Error 0.4900(0.4816) Steps 0(0.00) | Grad Norm 3967.8850(5482.4563) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0027 | Time 80.8976, Epoch Time 990.4415(901.4584), Bit/dim 3.9043(best: 3.9332), Xent 1.2326, Loss 4.5206, Error 0.4505(best: 0.4616)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1490 | Time 16.1052(16.1612) | Bit/dim 3.8805(3.9254) | Xent 1.3631(1.3180) | Loss 5700.7090(6448.5366) | Error 0.4844(0.4767) Steps 0(0.00) | Grad Norm 7400.5940(5487.8427) | Total Time 0.00(0.00)\n",
      "Iter 1500 | Time 16.1853(16.1097) | Bit/dim 3.8865(3.9178) | Xent 1.3002(1.3099) | Loss 5582.0635(6239.4152) | Error 0.4722(0.4730) Steps 0(0.00) | Grad Norm 3725.5439(5142.1095) | Total Time 0.00(0.00)\n",
      "Iter 1510 | Time 15.1790(16.2873) | Bit/dim 3.8745(3.9095) | Xent 1.2869(1.2973) | Loss 5601.6904(6101.0462) | Error 0.4533(0.4677) Steps 0(0.00) | Grad Norm 3750.7626(4733.8005) | Total Time 0.00(0.00)\n",
      "Iter 1520 | Time 16.0026(16.2350) | Bit/dim 3.9121(3.9080) | Xent 1.3196(1.3020) | Loss 5701.5122(5991.2625) | Error 0.4656(0.4707) Steps 0(0.00) | Grad Norm 7378.2304(4997.6398) | Total Time 0.00(0.00)\n",
      "Iter 1530 | Time 17.9575(16.2158) | Bit/dim 3.9088(3.9040) | Xent 1.2413(1.2981) | Loss 5864.5356(5909.3600) | Error 0.4500(0.4698) Steps 0(0.00) | Grad Norm 3528.1530(5265.7588) | Total Time 0.00(0.00)\n",
      "Iter 1540 | Time 14.8169(16.1509) | Bit/dim 3.9061(3.9010) | Xent 1.2793(1.2980) | Loss 5416.6455(5824.3267) | Error 0.4578(0.4700) Steps 0(0.00) | Grad Norm 2769.5253(4916.5769) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0028 | Time 81.1724, Epoch Time 987.6576(904.0444), Bit/dim 3.8836(best: 3.9043), Xent 1.2264, Loss 4.4967, Error 0.4480(best: 0.4505)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1550 | Time 18.2336(16.3049) | Bit/dim 3.9192(3.8992) | Xent 1.2726(1.2866) | Loss 5739.6411(6310.0897) | Error 0.4656(0.4642) Steps 0(0.00) | Grad Norm 4462.4850(4651.4729) | Total Time 0.00(0.00)\n",
      "Iter 1560 | Time 15.6182(16.3662) | Bit/dim 3.8596(3.8954) | Xent 1.2633(1.2909) | Loss 5440.8164(6124.6345) | Error 0.4644(0.4674) Steps 0(0.00) | Grad Norm 6437.3729(4772.8603) | Total Time 0.00(0.00)\n",
      "Iter 1570 | Time 15.9177(16.2638) | Bit/dim 3.8514(3.8910) | Xent 1.1912(1.2770) | Loss 5638.4238(6006.2565) | Error 0.4322(0.4633) Steps 0(0.00) | Grad Norm 2091.5577(4509.3265) | Total Time 0.00(0.00)\n",
      "Iter 1580 | Time 15.8794(16.1994) | Bit/dim 3.8702(3.8915) | Xent 1.3436(1.2844) | Loss 5623.0098(5926.0283) | Error 0.4733(0.4656) Steps 0(0.00) | Grad Norm 9066.5523(4921.6903) | Total Time 0.00(0.00)\n",
      "Iter 1590 | Time 16.1245(16.2598) | Bit/dim 3.9422(3.8939) | Xent 1.2554(1.2846) | Loss 5678.9160(5859.4156) | Error 0.4400(0.4652) Steps 0(0.00) | Grad Norm 4669.8022(5062.7926) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0029 | Time 81.2484, Epoch Time 997.5226(906.8488), Bit/dim 3.8849(best: 3.8836), Xent 1.2017, Loss 4.4857, Error 0.4366(best: 0.4480)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1600 | Time 15.4736(16.2541) | Bit/dim 3.8810(3.8918) | Xent 1.2745(1.2775) | Loss 5469.2905(6396.8882) | Error 0.4511(0.4610) Steps 0(0.00) | Grad Norm 3782.8468(4906.6011) | Total Time 0.00(0.00)\n",
      "Iter 1610 | Time 15.0313(16.0954) | Bit/dim 3.8587(3.8860) | Xent 1.1916(1.2680) | Loss 5261.4971(6167.1704) | Error 0.4256(0.4565) Steps 0(0.00) | Grad Norm 3304.8564(5263.5298) | Total Time 0.00(0.00)\n",
      "Iter 1620 | Time 15.9029(16.0617) | Bit/dim 3.8671(3.8817) | Xent 1.2506(1.2645) | Loss 5530.6436(6021.4771) | Error 0.4556(0.4548) Steps 0(0.00) | Grad Norm 2949.7342(5190.3058) | Total Time 0.00(0.00)\n",
      "Iter 1630 | Time 15.0887(16.1471) | Bit/dim 3.8502(3.8788) | Xent 1.2368(1.2627) | Loss 5450.2500(5917.1994) | Error 0.4289(0.4536) Steps 0(0.00) | Grad Norm 4183.2589(5304.1757) | Total Time 0.00(0.00)\n",
      "Iter 1640 | Time 15.1366(15.9964) | Bit/dim 3.8855(3.8758) | Xent 1.2550(1.2568) | Loss 5474.4409(5818.5799) | Error 0.4533(0.4521) Steps 0(0.00) | Grad Norm 4054.5637(5018.0665) | Total Time 0.00(0.00)\n",
      "Iter 1650 | Time 16.6388(16.1635) | Bit/dim 3.8640(3.8728) | Xent 1.1898(1.2560) | Loss 5571.3672(5763.8002) | Error 0.4200(0.4526) Steps 0(0.00) | Grad Norm 2932.1042(4800.2582) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0030 | Time 80.7592, Epoch Time 982.2604(909.1111), Bit/dim 3.8775(best: 3.8836), Xent 1.2239, Loss 4.4894, Error 0.4438(best: 0.4366)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1660 | Time 16.0177(16.0657) | Bit/dim 3.8591(3.8712) | Xent 1.2277(1.2540) | Loss 5574.0278(6209.7181) | Error 0.4522(0.4512) Steps 0(0.00) | Grad Norm 4682.0407(5088.9888) | Total Time 0.00(0.00)\n",
      "Iter 1670 | Time 14.8730(15.9377) | Bit/dim 3.8658(3.8695) | Xent 1.2461(1.2478) | Loss 5599.5464(6019.6128) | Error 0.4578(0.4507) Steps 0(0.00) | Grad Norm 7534.3639(5238.8325) | Total Time 0.00(0.00)\n",
      "Iter 1680 | Time 16.8538(16.0472) | Bit/dim 3.8513(3.8686) | Xent 1.1786(1.2388) | Loss 5381.0181(5913.3428) | Error 0.4533(0.4486) Steps 0(0.00) | Grad Norm 3257.6489(4982.5494) | Total Time 0.00(0.00)\n",
      "Iter 1690 | Time 15.5683(16.0625) | Bit/dim 3.8853(3.8668) | Xent 1.2659(1.2367) | Loss 5731.5347(5832.7543) | Error 0.4489(0.4479) Steps 0(0.00) | Grad Norm 7951.6089(4941.5753) | Total Time 0.00(0.00)\n",
      "Iter 1700 | Time 16.3614(16.0337) | Bit/dim 3.8310(3.8621) | Xent 1.2387(1.2439) | Loss 5475.5908(5758.2142) | Error 0.4489(0.4501) Steps 0(0.00) | Grad Norm 3288.5142(5026.0504) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0031 | Time 81.2853, Epoch Time 977.8964(911.1747), Bit/dim 3.8545(best: 3.8775), Xent 1.1947, Loss 4.4518, Error 0.4303(best: 0.4366)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1710 | Time 17.0916(16.0139) | Bit/dim 3.8371(3.8560) | Xent 1.1535(1.2399) | Loss 5713.4692(6298.4754) | Error 0.4078(0.4486) Steps 0(0.00) | Grad Norm 3674.8038(5222.0530) | Total Time 0.00(0.00)\n",
      "Iter 1720 | Time 16.0180(16.0067) | Bit/dim 3.8714(3.8550) | Xent 1.2235(1.2312) | Loss 5650.5591(6104.2954) | Error 0.4244(0.4441) Steps 0(0.00) | Grad Norm 3282.4402(4950.7342) | Total Time 0.00(0.00)\n",
      "Iter 1730 | Time 16.5817(16.0857) | Bit/dim 3.8522(3.8538) | Xent 1.1846(1.2269) | Loss 5646.7754(5967.1257) | Error 0.4278(0.4429) Steps 0(0.00) | Grad Norm 5439.3751(5051.4116) | Total Time 0.00(0.00)\n",
      "Iter 1740 | Time 15.5533(16.0660) | Bit/dim 3.8795(3.8499) | Xent 1.2126(1.2248) | Loss 5495.0190(5863.1045) | Error 0.4422(0.4427) Steps 0(0.00) | Grad Norm 4213.7898(5134.4753) | Total Time 0.00(0.00)\n",
      "Iter 1750 | Time 16.6740(16.2303) | Bit/dim 3.8741(3.8471) | Xent 1.2059(1.2227) | Loss 5622.8833(5787.8070) | Error 0.4367(0.4416) Steps 0(0.00) | Grad Norm 3551.9230(5004.4761) | Total Time 0.00(0.00)\n",
      "Iter 1760 | Time 15.6869(16.3812) | Bit/dim 3.8205(3.8406) | Xent 1.2560(1.2228) | Loss 5369.8052(5728.0055) | Error 0.4667(0.4427) Steps 0(0.00) | Grad Norm 10208.5051(4998.3724) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0032 | Time 81.0603, Epoch Time 995.5834(913.7069), Bit/dim 3.8353(best: 3.8545), Xent 1.2681, Loss 4.4694, Error 0.4516(best: 0.4303)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1770 | Time 15.2374(16.3674) | Bit/dim 3.8287(3.8406) | Xent 1.1189(1.2210) | Loss 5473.6519(6192.8084) | Error 0.4078(0.4409) Steps 0(0.00) | Grad Norm 3813.3020(5193.3962) | Total Time 0.00(0.00)\n",
      "Iter 1780 | Time 15.5916(16.2146) | Bit/dim 3.8581(3.8403) | Xent 1.2100(1.2102) | Loss 5493.4844(6018.7523) | Error 0.4367(0.4363) Steps 0(0.00) | Grad Norm 3975.9364(4973.2172) | Total Time 0.00(0.00)\n",
      "Iter 1790 | Time 16.3358(16.1697) | Bit/dim 3.8347(3.8349) | Xent 1.2136(1.2070) | Loss 5372.2598(5875.1252) | Error 0.4367(0.4350) Steps 0(0.00) | Grad Norm 4163.2949(5102.0042) | Total Time 0.00(0.00)\n",
      "Iter 1800 | Time 15.6336(16.2165) | Bit/dim 3.8327(3.8328) | Xent 1.2126(1.2012) | Loss 5437.7764(5779.7764) | Error 0.4522(0.4323) Steps 0(0.00) | Grad Norm 7995.3096(5057.9493) | Total Time 0.00(0.00)\n",
      "Iter 1810 | Time 15.6002(16.0189) | Bit/dim 3.8302(3.8295) | Xent 1.1270(1.1918) | Loss 5570.0635(5685.2680) | Error 0.3900(0.4294) Steps 0(0.00) | Grad Norm 2506.8127(4930.3888) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0033 | Time 81.2279, Epoch Time 985.4700(915.8598), Bit/dim 3.8211(best: 3.8353), Xent 1.1254, Loss 4.3838, Error 0.4132(best: 0.4303)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1820 | Time 16.5433(16.2169) | Bit/dim 3.8504(3.8298) | Xent 1.1393(1.1860) | Loss 5666.6680(6219.8102) | Error 0.4256(0.4286) Steps 0(0.00) | Grad Norm 4794.7128(4778.2584) | Total Time 0.00(0.00)\n",
      "Iter 1830 | Time 15.9435(16.2162) | Bit/dim 3.7879(3.8268) | Xent 1.1397(1.1810) | Loss 5430.5093(6037.6168) | Error 0.4178(0.4260) Steps 0(0.00) | Grad Norm 4528.0308(4687.5020) | Total Time 0.00(0.00)\n",
      "Iter 1840 | Time 15.6986(16.2336) | Bit/dim 3.7802(3.8216) | Xent 1.2100(1.1734) | Loss 5052.2520(5879.9321) | Error 0.4433(0.4231) Steps 0(0.00) | Grad Norm 4500.5061(4610.5544) | Total Time 0.00(0.00)\n",
      "Iter 1850 | Time 16.3608(16.0526) | Bit/dim 3.7966(3.8227) | Xent 1.1377(1.1723) | Loss 5622.7520(5781.8056) | Error 0.4300(0.4239) Steps 0(0.00) | Grad Norm 4005.3671(4839.1366) | Total Time 0.00(0.00)\n",
      "Iter 1860 | Time 15.1452(15.9575) | Bit/dim 3.8120(3.8194) | Xent 1.1120(1.1687) | Loss 5306.6816(5694.1916) | Error 0.3956(0.4218) Steps 0(0.00) | Grad Norm 5184.8813(5064.6565) | Total Time 0.00(0.00)\n",
      "Iter 1870 | Time 14.3360(15.9297) | Bit/dim 3.8126(3.8190) | Xent 1.1501(1.1683) | Loss 5321.2114(5651.4841) | Error 0.4167(0.4210) Steps 0(0.00) | Grad Norm 3958.2281(5035.2635) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0034 | Time 80.5895, Epoch Time 975.9690(917.6631), Bit/dim 3.7967(best: 3.8211), Xent 1.0842, Loss 4.3388, Error 0.3951(best: 0.4132)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1880 | Time 15.3308(15.9704) | Bit/dim 3.8305(3.8174) | Xent 1.1154(1.1596) | Loss 5457.6304(6101.2663) | Error 0.4122(0.4177) Steps 0(0.00) | Grad Norm 5837.9260(4818.2036) | Total Time 0.00(0.00)\n",
      "Iter 1890 | Time 15.9172(16.0413) | Bit/dim 3.8170(3.8161) | Xent 1.1446(1.1534) | Loss 5323.0059(5933.3230) | Error 0.3911(0.4143) Steps 0(0.00) | Grad Norm 6410.1065(4909.7217) | Total Time 0.00(0.00)\n",
      "Iter 1900 | Time 16.9506(16.1276) | Bit/dim 3.7862(3.8142) | Xent 1.1929(1.1560) | Loss 5238.0762(5815.4095) | Error 0.4089(0.4155) Steps 0(0.00) | Grad Norm 4896.4616(5151.3361) | Total Time 0.00(0.00)\n",
      "Iter 1910 | Time 15.8000(16.0171) | Bit/dim 3.8018(3.8131) | Xent 1.1641(1.1570) | Loss 5625.5781(5736.0084) | Error 0.4133(0.4170) Steps 0(0.00) | Grad Norm 4542.1009(5002.0419) | Total Time 0.00(0.00)\n",
      "Iter 1920 | Time 16.7483(15.9962) | Bit/dim 3.8173(3.8081) | Xent 1.1240(1.1493) | Loss 5458.4204(5658.9734) | Error 0.4100(0.4135) Steps 0(0.00) | Grad Norm 2373.5852(4791.0853) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0035 | Time 80.1524, Epoch Time 983.3051(919.6324), Bit/dim 3.8056(best: 3.7967), Xent 1.0899, Loss 4.3505, Error 0.3909(best: 0.3951)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1930 | Time 16.7192(16.1812) | Bit/dim 3.8188(3.8097) | Xent 1.2243(1.1513) | Loss 5530.0039(6199.4763) | Error 0.4533(0.4141) Steps 0(0.00) | Grad Norm 4881.0081(4491.1891) | Total Time 0.00(0.00)\n",
      "Iter 1940 | Time 16.3604(16.1280) | Bit/dim 3.8064(3.8067) | Xent 1.1779(1.1543) | Loss 5387.1631(6002.7448) | Error 0.4267(0.4158) Steps 0(0.00) | Grad Norm 9963.9619(4833.9029) | Total Time 0.00(0.00)\n",
      "Iter 1950 | Time 16.6150(16.1999) | Bit/dim 3.8265(3.8090) | Xent 1.1260(1.1542) | Loss 5580.2896(5863.7650) | Error 0.3944(0.4148) Steps 0(0.00) | Grad Norm 3682.6971(5140.5811) | Total Time 0.00(0.00)\n",
      "Iter 1960 | Time 15.4532(16.2801) | Bit/dim 3.7938(3.8066) | Xent 1.1131(1.1504) | Loss 5459.4072(5759.8220) | Error 0.4067(0.4120) Steps 0(0.00) | Grad Norm 5302.0557(5140.3212) | Total Time 0.00(0.00)\n",
      "Iter 1970 | Time 17.0222(16.1905) | Bit/dim 3.7963(3.8031) | Xent 1.0822(1.1401) | Loss 5353.6802(5663.8057) | Error 0.3789(0.4083) Steps 0(0.00) | Grad Norm 4848.8588(4933.3880) | Total Time 0.00(0.00)\n",
      "Iter 1980 | Time 15.0628(16.1829) | Bit/dim 3.7986(3.8041) | Xent 1.1017(1.1289) | Loss 5488.2451(5589.1141) | Error 0.3956(0.4054) Steps 0(0.00) | Grad Norm 4381.0996(4973.4668) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0036 | Time 80.4453, Epoch Time 992.4683(921.8174), Bit/dim 3.7892(best: 3.7967), Xent 1.0566, Loss 4.3175, Error 0.3811(best: 0.3909)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1990 | Time 16.5488(16.1540) | Bit/dim 3.7969(3.8006) | Xent 1.0776(1.1183) | Loss 5671.2192(6055.3866) | Error 0.3967(0.4019) Steps 0(0.00) | Grad Norm 4883.0231(4876.5474) | Total Time 0.00(0.00)\n",
      "Iter 2000 | Time 17.6201(16.1783) | Bit/dim 3.7818(3.7993) | Xent 1.1065(1.1073) | Loss 5500.2451(5901.1497) | Error 0.4056(0.3986) Steps 0(0.00) | Grad Norm 4665.0709(4958.0361) | Total Time 0.00(0.00)\n",
      "Iter 2010 | Time 15.7591(16.0808) | Bit/dim 3.8103(3.8010) | Xent 1.1659(1.1189) | Loss 5428.3047(5784.3723) | Error 0.4167(0.4032) Steps 0(0.00) | Grad Norm 4888.8965(5339.4428) | Total Time 0.00(0.00)\n",
      "Iter 2020 | Time 16.4142(16.0775) | Bit/dim 3.7968(3.7980) | Xent 1.1544(1.1129) | Loss 5529.2881(5687.6578) | Error 0.4078(0.4007) Steps 0(0.00) | Grad Norm 5485.5291(5112.8138) | Total Time 0.00(0.00)\n",
      "Iter 2030 | Time 15.4945(16.1641) | Bit/dim 3.8660(3.7988) | Xent 1.1209(1.1193) | Loss 5242.4604(5622.8940) | Error 0.3911(0.4032) Steps 0(0.00) | Grad Norm 3786.9580(5198.1353) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0037 | Time 80.0631, Epoch Time 986.8522(923.7685), Bit/dim 3.7963(best: 3.7892), Xent 1.0640, Loss 4.3283, Error 0.3817(best: 0.3811)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2040 | Time 16.0569(16.2060) | Bit/dim 3.8027(3.7975) | Xent 1.0762(1.1096) | Loss 5439.3506(6145.0379) | Error 0.3889(0.4005) Steps 0(0.00) | Grad Norm 2757.9257(4877.8424) | Total Time 0.00(0.00)\n",
      "Iter 2050 | Time 15.0369(16.1332) | Bit/dim 3.8143(3.7988) | Xent 1.0873(1.1121) | Loss 5502.8608(5957.8024) | Error 0.3822(0.4007) Steps 0(0.00) | Grad Norm 7911.0804(5457.0057) | Total Time 0.00(0.00)\n",
      "Iter 2060 | Time 15.5578(16.2663) | Bit/dim 3.7842(3.7982) | Xent 1.1465(1.1083) | Loss 5372.6338(5840.9791) | Error 0.4244(0.3995) Steps 0(0.00) | Grad Norm 4271.0423(5142.2760) | Total Time 0.00(0.00)\n",
      "Iter 2070 | Time 15.3582(16.2335) | Bit/dim 3.7818(3.7956) | Xent 1.0742(1.0911) | Loss 5345.6460(5719.6300) | Error 0.3789(0.3934) Steps 0(0.00) | Grad Norm 3777.3921(4855.7671) | Total Time 0.00(0.00)\n",
      "Iter 2080 | Time 15.9401(16.3806) | Bit/dim 3.7953(3.7958) | Xent 1.0746(1.0934) | Loss 5356.4077(5654.9043) | Error 0.3878(0.3943) Steps 0(0.00) | Grad Norm 3155.8762(4686.2828) | Total Time 0.00(0.00)\n",
      "Iter 2090 | Time 16.7555(16.4618) | Bit/dim 3.7729(3.7909) | Xent 1.1554(1.0971) | Loss 5573.4902(5615.8287) | Error 0.3956(0.3951) Steps 0(0.00) | Grad Norm 5316.5017(4724.6495) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0038 | Time 80.3934, Epoch Time 1001.3202(926.0950), Bit/dim 3.8087(best: 3.7892), Xent 1.0516, Loss 4.3345, Error 0.3757(best: 0.3811)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2100 | Time 16.9365(16.4115) | Bit/dim 3.8048(3.7927) | Xent 1.0873(1.0974) | Loss 5486.9102(6091.3305) | Error 0.3789(0.3949) Steps 0(0.00) | Grad Norm 4260.3878(4855.1564) | Total Time 0.00(0.00)\n",
      "Iter 2110 | Time 17.6864(16.4612) | Bit/dim 3.7846(3.7943) | Xent 1.1694(1.0932) | Loss 5467.9043(5896.4972) | Error 0.4211(0.3952) Steps 0(0.00) | Grad Norm 8486.1522(4755.3461) | Total Time 0.00(0.00)\n",
      "Iter 2120 | Time 16.1234(16.4388) | Bit/dim 3.7808(3.7924) | Xent 1.0429(1.0865) | Loss 5475.5200(5778.5610) | Error 0.3844(0.3929) Steps 0(0.00) | Grad Norm 3776.2620(5039.0501) | Total Time 0.00(0.00)\n",
      "Iter 2130 | Time 15.2235(16.2828) | Bit/dim 3.7745(3.7906) | Xent 1.0565(1.0865) | Loss 5318.0615(5688.9615) | Error 0.3911(0.3909) Steps 0(0.00) | Grad Norm 5489.7454(4884.2333) | Total Time 0.00(0.00)\n",
      "Iter 2140 | Time 16.2480(16.2434) | Bit/dim 3.8003(3.7894) | Xent 1.0602(1.0846) | Loss 5285.6528(5611.5252) | Error 0.4000(0.3898) Steps 0(0.00) | Grad Norm 2038.0006(4584.3758) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0039 | Time 83.3604, Epoch Time 999.0460(928.2836), Bit/dim 3.7911(best: 3.7892), Xent 1.0505, Loss 4.3163, Error 0.3780(best: 0.3757)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2150 | Time 15.8151(16.3283) | Bit/dim 3.7819(3.7897) | Xent 1.0896(1.0913) | Loss 5459.3501(6185.5021) | Error 0.3811(0.3910) Steps 0(0.00) | Grad Norm 2489.5151(4600.3025) | Total Time 0.00(0.00)\n",
      "Iter 2160 | Time 15.7332(16.2671) | Bit/dim 3.7973(3.7911) | Xent 1.0496(1.0831) | Loss 5449.3706(6002.6614) | Error 0.3600(0.3864) Steps 0(0.00) | Grad Norm 5175.3456(4685.6031) | Total Time 0.00(0.00)\n",
      "Iter 2170 | Time 16.5919(16.3553) | Bit/dim 3.7735(3.7916) | Xent 1.0617(1.0699) | Loss 5392.9053(5839.7163) | Error 0.3756(0.3822) Steps 0(0.00) | Grad Norm 3892.4652(4450.6156) | Total Time 0.00(0.00)\n",
      "Iter 2180 | Time 17.2779(16.2822) | Bit/dim 3.7566(3.7888) | Xent 1.1009(1.0664) | Loss 5465.6968(5717.7859) | Error 0.3811(0.3819) Steps 0(0.00) | Grad Norm 4768.7774(4462.4233) | Total Time 0.00(0.00)\n",
      "Iter 2190 | Time 17.1546(16.4061) | Bit/dim 3.7421(3.7839) | Xent 1.0083(1.0594) | Loss 5531.2090(5652.6327) | Error 0.3689(0.3806) Steps 0(0.00) | Grad Norm 3586.6722(4534.3130) | Total Time 0.00(0.00)\n",
      "Iter 2200 | Time 16.9431(16.4442) | Bit/dim 3.7578(3.7806) | Xent 0.9860(1.0546) | Loss 5296.4429(5590.8449) | Error 0.3656(0.3804) Steps 0(0.00) | Grad Norm 4806.0719(4406.2533) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0040 | Time 83.5175, Epoch Time 1001.9274(930.4929), Bit/dim 3.7737(best: 3.7892), Xent 0.9833, Loss 4.2654, Error 0.3570(best: 0.3757)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2210 | Time 15.0705(16.3632) | Bit/dim 3.7815(3.7771) | Xent 0.9915(1.0426) | Loss 5277.0068(6012.5905) | Error 0.3611(0.3763) Steps 0(0.00) | Grad Norm 2163.4353(4315.7367) | Total Time 0.00(0.00)\n",
      "Iter 2220 | Time 15.6550(16.4805) | Bit/dim 3.7526(3.7725) | Xent 1.0946(1.0358) | Loss 5263.9189(5843.8187) | Error 0.4044(0.3732) Steps 0(0.00) | Grad Norm 6404.5111(4275.2012) | Total Time 0.00(0.00)\n",
      "Iter 2230 | Time 17.1976(16.4658) | Bit/dim 3.7781(3.7719) | Xent 1.0337(1.0345) | Loss 5400.3022(5737.3241) | Error 0.3756(0.3724) Steps 0(0.00) | Grad Norm 4285.7233(4443.8422) | Total Time 0.00(0.00)\n",
      "Iter 2240 | Time 15.2352(16.2866) | Bit/dim 3.7531(3.7701) | Xent 0.9687(1.0399) | Loss 5045.4727(5613.4426) | Error 0.3667(0.3756) Steps 0(0.00) | Grad Norm 3211.0974(4729.3642) | Total Time 0.00(0.00)\n",
      "Iter 2250 | Time 16.1921(16.2318) | Bit/dim 3.7724(3.7714) | Xent 1.1022(1.0413) | Loss 5479.6465(5576.3956) | Error 0.3967(0.3753) Steps 0(0.00) | Grad Norm 4829.8233(4653.9400) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0041 | Time 81.3875, Epoch Time 993.7982(932.3920), Bit/dim 3.7687(best: 3.7737), Xent 0.9938, Loss 4.2656, Error 0.3546(best: 0.3570)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2260 | Time 15.3286(16.2258) | Bit/dim 3.7699(3.7732) | Xent 0.9876(1.0389) | Loss 5236.9727(6100.4661) | Error 0.3611(0.3735) Steps 0(0.00) | Grad Norm 7846.9250(5010.2465) | Total Time 0.00(0.00)\n",
      "Iter 2270 | Time 16.2958(16.2302) | Bit/dim 3.7674(3.7727) | Xent 1.0268(1.0257) | Loss 5270.9302(5901.4016) | Error 0.3778(0.3677) Steps 0(0.00) | Grad Norm 4235.1032(4785.6619) | Total Time 0.00(0.00)\n",
      "Iter 2280 | Time 17.6686(16.3571) | Bit/dim 3.7923(3.7744) | Xent 0.9860(1.0229) | Loss 5413.3311(5761.7640) | Error 0.3622(0.3668) Steps 0(0.00) | Grad Norm 5411.9007(4998.0347) | Total Time 0.00(0.00)\n",
      "Iter 2290 | Time 15.5439(16.4851) | Bit/dim 3.7293(3.7732) | Xent 1.0340(1.0204) | Loss 5280.3896(5665.9125) | Error 0.3756(0.3657) Steps 0(0.00) | Grad Norm 4817.8232(5079.0561) | Total Time 0.00(0.00)\n",
      "Iter 2300 | Time 17.0467(16.4739) | Bit/dim 3.7652(3.7684) | Xent 1.0064(1.0223) | Loss 5257.5645(5585.7630) | Error 0.3556(0.3675) Steps 0(0.00) | Grad Norm 2626.3610(4717.4998) | Total Time 0.00(0.00)\n",
      "Iter 2310 | Time 18.2685(16.6551) | Bit/dim 3.7566(3.7658) | Xent 1.0515(1.0160) | Loss 5509.5264(5546.8540) | Error 0.3722(0.3656) Steps 0(0.00) | Grad Norm 6183.2617(4549.8892) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0042 | Time 81.6062, Epoch Time 1014.2511(934.8478), Bit/dim 3.7633(best: 3.7687), Xent 0.9915, Loss 4.2590, Error 0.3548(best: 0.3546)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2320 | Time 17.1550(16.6930) | Bit/dim 3.7738(3.7665) | Xent 0.9603(1.0298) | Loss 5441.1338(6028.8092) | Error 0.3478(0.3705) Steps 0(0.00) | Grad Norm 2876.2765(4521.8157) | Total Time 0.00(0.00)\n",
      "Iter 2330 | Time 17.0888(16.5977) | Bit/dim 3.7407(3.7657) | Xent 1.0872(1.0281) | Loss 5702.9053(5861.0027) | Error 0.4011(0.3704) Steps 0(0.00) | Grad Norm 8604.5623(4674.1263) | Total Time 0.00(0.00)\n",
      "Iter 2340 | Time 16.2579(16.4918) | Bit/dim 3.7637(3.7651) | Xent 1.0446(1.0271) | Loss 5517.3003(5749.0120) | Error 0.3800(0.3698) Steps 0(0.00) | Grad Norm 5113.1523(4631.4601) | Total Time 0.00(0.00)\n",
      "Iter 2350 | Time 17.0459(16.4026) | Bit/dim 3.7601(3.7644) | Xent 1.0368(1.0233) | Loss 5529.8809(5656.9559) | Error 0.3878(0.3685) Steps 0(0.00) | Grad Norm 3633.9253(4454.5059) | Total Time 0.00(0.00)\n",
      "Iter 2360 | Time 16.2888(16.4173) | Bit/dim 3.7067(3.7611) | Xent 0.9558(1.0159) | Loss 5284.9712(5593.6958) | Error 0.3622(0.3661) Steps 0(0.00) | Grad Norm 3451.7425(4558.0681) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0043 | Time 83.0993, Epoch Time 999.4556(936.7860), Bit/dim 3.7553(best: 3.7633), Xent 0.9475, Loss 4.2290, Error 0.3339(best: 0.3546)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2370 | Time 17.0690(16.3956) | Bit/dim 3.7538(3.7586) | Xent 0.9075(1.0040) | Loss 5472.7979(6132.7624) | Error 0.3278(0.3634) Steps 0(0.00) | Grad Norm 5078.6910(4641.9556) | Total Time 0.00(0.00)\n",
      "Iter 2380 | Time 16.5112(16.4096) | Bit/dim 3.8023(3.7615) | Xent 0.9614(0.9972) | Loss 5524.2939(5928.4442) | Error 0.3378(0.3590) Steps 0(0.00) | Grad Norm 3838.2941(4496.8402) | Total Time 0.00(0.00)\n",
      "Iter 2390 | Time 17.1401(16.4489) | Bit/dim 3.7261(3.7586) | Xent 1.0125(0.9995) | Loss 5337.6626(5795.2117) | Error 0.3656(0.3590) Steps 0(0.00) | Grad Norm 5119.3034(4535.6940) | Total Time 0.00(0.00)\n",
      "Iter 2400 | Time 16.2049(16.4782) | Bit/dim 3.7498(3.7574) | Xent 1.0317(1.0134) | Loss 5426.1484(5688.7136) | Error 0.3733(0.3633) Steps 0(0.00) | Grad Norm 7274.0185(4980.3175) | Total Time 0.00(0.00)\n",
      "Iter 2410 | Time 17.4674(16.4559) | Bit/dim 3.7303(3.7530) | Xent 0.9949(1.0094) | Loss 5435.6094(5580.3892) | Error 0.3611(0.3626) Steps 0(0.00) | Grad Norm 3177.3294(4985.4203) | Total Time 0.00(0.00)\n",
      "Iter 2420 | Time 16.8137(16.5185) | Bit/dim 3.7356(3.7495) | Xent 0.8838(0.9961) | Loss 5403.7324(5529.7038) | Error 0.3267(0.3583) Steps 0(0.00) | Grad Norm 5029.5813(4772.1530) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0044 | Time 81.7103, Epoch Time 1008.6456(938.9418), Bit/dim 3.7546(best: 3.7553), Xent 0.9402, Loss 4.2247, Error 0.3350(best: 0.3339)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2430 | Time 18.4589(16.5922) | Bit/dim 3.7446(3.7492) | Xent 0.9358(0.9939) | Loss 5481.3306(5999.1326) | Error 0.3578(0.3575) Steps 0(0.00) | Grad Norm 3780.5576(4759.6933) | Total Time 0.00(0.00)\n",
      "Iter 2440 | Time 18.0732(16.5732) | Bit/dim 3.7514(3.7478) | Xent 0.9749(0.9868) | Loss 5433.6934(5833.9587) | Error 0.3567(0.3554) Steps 0(0.00) | Grad Norm 3743.3638(4460.2860) | Total Time 0.00(0.00)\n",
      "Iter 2450 | Time 16.6377(16.5861) | Bit/dim 3.7427(3.7474) | Xent 0.9472(0.9849) | Loss 5382.0581(5713.9329) | Error 0.3356(0.3541) Steps 0(0.00) | Grad Norm 4354.9068(4395.9222) | Total Time 0.00(0.00)\n",
      "Iter 2460 | Time 16.6615(16.6191) | Bit/dim 3.7614(3.7494) | Xent 0.9840(0.9834) | Loss 5437.5181(5636.5790) | Error 0.3733(0.3539) Steps 0(0.00) | Grad Norm 5396.9823(4476.0454) | Total Time 0.00(0.00)\n",
      "Iter 2470 | Time 18.4779(16.6842) | Bit/dim 3.7431(3.7514) | Xent 1.0243(0.9922) | Loss 5412.3882(5559.5247) | Error 0.3767(0.3568) Steps 0(0.00) | Grad Norm 6586.4900(4883.7218) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0045 | Time 81.8515, Epoch Time 1016.1090(941.2569), Bit/dim 3.7603(best: 3.7546), Xent 1.0390, Loss 4.2797, Error 0.3730(best: 0.3339)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2480 | Time 15.7641(16.7002) | Bit/dim 3.7100(3.7516) | Xent 0.9908(1.0018) | Loss 4969.7793(6106.7682) | Error 0.3722(0.3604) Steps 0(0.00) | Grad Norm 4655.2134(4879.5867) | Total Time 0.00(0.00)\n",
      "Iter 2490 | Time 16.1757(16.6009) | Bit/dim 3.7724(3.7513) | Xent 0.9583(0.9906) | Loss 5387.6890(5899.3604) | Error 0.3578(0.3554) Steps 0(0.00) | Grad Norm 3083.6804(4595.0120) | Total Time 0.00(0.00)\n",
      "Iter 2500 | Time 16.2467(16.6143) | Bit/dim 3.7491(3.7535) | Xent 0.9991(0.9864) | Loss 5365.0908(5751.6737) | Error 0.3656(0.3543) Steps 0(0.00) | Grad Norm 4649.2266(4560.8587) | Total Time 0.00(0.00)\n",
      "Iter 2510 | Time 19.0782(16.8067) | Bit/dim 3.7513(3.7544) | Xent 0.9585(0.9777) | Loss 5610.1943(5660.3826) | Error 0.3422(0.3514) Steps 0(0.00) | Grad Norm 3885.5430(4291.4869) | Total Time 0.00(0.00)\n",
      "Iter 2520 | Time 17.1341(16.7592) | Bit/dim 3.7444(3.7520) | Xent 0.9385(0.9713) | Loss 5195.4375(5569.1328) | Error 0.3356(0.3487) Steps 0(0.00) | Grad Norm 3494.1973(4215.1300) | Total Time 0.00(0.00)\n",
      "Iter 2530 | Time 16.5618(16.6286) | Bit/dim 3.7160(3.7493) | Xent 0.8826(0.9608) | Loss 5283.5029(5496.6970) | Error 0.3189(0.3442) Steps 0(0.00) | Grad Norm 3094.5736(3980.3049) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0046 | Time 80.8978, Epoch Time 1016.1622(943.5040), Bit/dim 3.7444(best: 3.7546), Xent 0.9072, Loss 4.1980, Error 0.3220(best: 0.3339)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2540 | Time 16.5801(16.5061) | Bit/dim 3.7426(3.7481) | Xent 0.9432(0.9601) | Loss 5136.6406(5956.6182) | Error 0.3244(0.3435) Steps 0(0.00) | Grad Norm 2522.2419(3962.9129) | Total Time 0.00(0.00)\n",
      "Iter 2550 | Time 18.0189(16.7683) | Bit/dim 3.7359(3.7509) | Xent 1.0124(0.9611) | Loss 5533.6636(5806.1795) | Error 0.3589(0.3428) Steps 0(0.00) | Grad Norm 3121.1572(4202.7553) | Total Time 0.00(0.00)\n",
      "Iter 2560 | Time 16.0533(16.7950) | Bit/dim 3.7725(3.7483) | Xent 0.9651(0.9529) | Loss 5399.0078(5673.5543) | Error 0.3633(0.3405) Steps 0(0.00) | Grad Norm 7366.7890(4057.7539) | Total Time 0.00(0.00)\n",
      "Iter 2570 | Time 16.0979(16.6815) | Bit/dim 3.7869(3.7504) | Xent 1.1076(0.9622) | Loss 5291.1489(5598.0260) | Error 0.3900(0.3438) Steps 0(0.00) | Grad Norm 6123.3532(4352.5467) | Total Time 0.00(0.00)\n",
      "Iter 2580 | Time 18.1238(16.7211) | Bit/dim 3.7406(3.7486) | Xent 1.0090(0.9637) | Loss 5559.7178(5548.6857) | Error 0.3767(0.3448) Steps 0(0.00) | Grad Norm 9500.6711(4399.6803) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0047 | Time 82.3983, Epoch Time 1026.0816(945.9813), Bit/dim 3.7416(best: 3.7444), Xent 0.9394, Loss 4.2113, Error 0.3345(best: 0.3220)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2590 | Time 17.4081(16.7202) | Bit/dim 3.7002(3.7427) | Xent 0.8675(0.9653) | Loss 5268.0791(6119.8377) | Error 0.3144(0.3451) Steps 0(0.00) | Grad Norm 3363.8039(4424.2928) | Total Time 0.00(0.00)\n",
      "Iter 2600 | Time 16.6211(16.6763) | Bit/dim 3.7175(3.7421) | Xent 0.8734(0.9530) | Loss 5057.5376(5880.7999) | Error 0.3067(0.3411) Steps 0(0.00) | Grad Norm 2373.2418(4425.8514) | Total Time 0.00(0.00)\n",
      "Iter 2610 | Time 15.5454(16.6086) | Bit/dim 3.7505(3.7405) | Xent 0.8946(0.9450) | Loss 5474.8755(5737.1380) | Error 0.3156(0.3393) Steps 0(0.00) | Grad Norm 6611.6989(4228.9397) | Total Time 0.00(0.00)\n",
      "Iter 2620 | Time 16.4112(16.5789) | Bit/dim 3.7340(3.7383) | Xent 0.9051(0.9400) | Loss 5234.8467(5630.8809) | Error 0.3167(0.3360) Steps 0(0.00) | Grad Norm 3385.9181(4295.9189) | Total Time 0.00(0.00)\n",
      "Iter 2630 | Time 16.6735(16.5326) | Bit/dim 3.7132(3.7378) | Xent 0.9806(0.9377) | Loss 5292.8687(5535.0465) | Error 0.3444(0.3356) Steps 0(0.00) | Grad Norm 2958.2088(4167.1091) | Total Time 0.00(0.00)\n",
      "Iter 2640 | Time 16.1719(16.6241) | Bit/dim 3.7547(3.7370) | Xent 0.9986(0.9381) | Loss 5158.0498(5458.6240) | Error 0.3789(0.3364) Steps 0(0.00) | Grad Norm 6747.9848(4433.1608) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0048 | Time 82.0272, Epoch Time 1008.4105(947.8542), Bit/dim 3.7298(best: 3.7416), Xent 0.9245, Loss 4.1920, Error 0.3279(best: 0.3220)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2650 | Time 16.1888(16.4639) | Bit/dim 3.7031(3.7333) | Xent 0.8769(0.9324) | Loss 5512.5137(5926.2540) | Error 0.3089(0.3338) Steps 0(0.00) | Grad Norm 3936.0300(4286.8360) | Total Time 0.00(0.00)\n",
      "Iter 2660 | Time 16.7890(16.5820) | Bit/dim 3.7207(3.7330) | Xent 0.9059(0.9263) | Loss 5389.8438(5766.8420) | Error 0.3256(0.3311) Steps 0(0.00) | Grad Norm 5731.6252(4355.8987) | Total Time 0.00(0.00)\n",
      "Iter 2670 | Time 16.0240(16.5022) | Bit/dim 3.7466(3.7350) | Xent 0.9717(0.9193) | Loss 5348.6504(5641.7726) | Error 0.3556(0.3290) Steps 0(0.00) | Grad Norm 4111.8801(4360.4169) | Total Time 0.00(0.00)\n",
      "Iter 2680 | Time 17.1967(16.5815) | Bit/dim 3.7416(3.7355) | Xent 0.9075(0.9133) | Loss 5350.1548(5554.8097) | Error 0.3078(0.3264) Steps 0(0.00) | Grad Norm 6837.3461(4347.6212) | Total Time 0.00(0.00)\n",
      "Iter 2690 | Time 15.3972(16.7099) | Bit/dim 3.6788(3.7337) | Xent 0.9623(0.9159) | Loss 5379.0269(5489.4076) | Error 0.3344(0.3266) Steps 0(0.00) | Grad Norm 3621.6111(4420.5885) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0049 | Time 83.3844, Epoch Time 1014.2667(949.8466), Bit/dim 3.7221(best: 3.7298), Xent 0.8838, Loss 4.1639, Error 0.3147(best: 0.3220)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2700 | Time 16.8697(16.7243) | Bit/dim 3.7407(3.7335) | Xent 0.9327(0.9105) | Loss 5388.5137(6059.8327) | Error 0.3300(0.3250) Steps 0(0.00) | Grad Norm 4596.0628(4255.7712) | Total Time 0.00(0.00)\n",
      "Iter 2710 | Time 16.8882(16.8470) | Bit/dim 3.7238(3.7334) | Xent 0.8662(0.9089) | Loss 5345.8086(5855.9394) | Error 0.3033(0.3238) Steps 0(0.00) | Grad Norm 5510.2804(4324.2950) | Total Time 0.00(0.00)\n",
      "Iter 2720 | Time 15.2418(16.8353) | Bit/dim 3.7152(3.7342) | Xent 0.8790(0.9040) | Loss 5323.4253(5709.7336) | Error 0.3200(0.3212) Steps 0(0.00) | Grad Norm 3357.8963(4259.6242) | Total Time 0.00(0.00)\n",
      "Iter 2730 | Time 15.1948(16.6723) | Bit/dim 3.7035(3.7305) | Xent 0.8937(0.9060) | Loss 5242.8501(5595.4418) | Error 0.3222(0.3220) Steps 0(0.00) | Grad Norm 4735.6899(4543.5394) | Total Time 0.00(0.00)\n",
      "Iter 2740 | Time 16.7036(16.6938) | Bit/dim 3.7411(3.7280) | Xent 0.9693(0.9222) | Loss 5335.0234(5538.7787) | Error 0.3444(0.3287) Steps 0(0.00) | Grad Norm 3242.1695(4437.3513) | Total Time 0.00(0.00)\n",
      "Iter 2750 | Time 16.2827(16.6729) | Bit/dim 3.7688(3.7291) | Xent 0.9707(0.9202) | Loss 5167.7568(5466.7671) | Error 0.3522(0.3302) Steps 0(0.00) | Grad Norm 8018.7375(4439.6049) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0050 | Time 82.5150, Epoch Time 1020.7418(951.9734), Bit/dim 3.7396(best: 3.7221), Xent 0.9265, Loss 4.2029, Error 0.3334(best: 0.3147)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2760 | Time 15.7008(16.7745) | Bit/dim 3.7323(3.7273) | Xent 0.8898(0.9171) | Loss 5405.4526(5934.6823) | Error 0.3122(0.3280) Steps 0(0.00) | Grad Norm 4772.1838(4393.7039) | Total Time 0.00(0.00)\n",
      "Iter 2770 | Time 16.4761(16.7526) | Bit/dim 3.7403(3.7270) | Xent 0.9189(0.9155) | Loss 5330.1401(5743.3399) | Error 0.3311(0.3286) Steps 0(0.00) | Grad Norm 4507.9483(4233.3079) | Total Time 0.00(0.00)\n",
      "Iter 2780 | Time 16.6605(16.7501) | Bit/dim 3.7273(3.7291) | Xent 0.9021(0.9126) | Loss 5209.9214(5644.5887) | Error 0.3233(0.3278) Steps 0(0.00) | Grad Norm 3321.5950(4329.7020) | Total Time 0.00(0.00)\n",
      "Iter 2790 | Time 16.7717(16.6384) | Bit/dim 3.7179(3.7277) | Xent 0.8987(0.9093) | Loss 5442.5825(5568.4090) | Error 0.3189(0.3263) Steps 0(0.00) | Grad Norm 3368.9703(4240.7295) | Total Time 0.00(0.00)\n",
      "Iter 2800 | Time 17.0836(16.6586) | Bit/dim 3.7178(3.7274) | Xent 0.8896(0.9088) | Loss 5165.8140(5510.1440) | Error 0.3044(0.3255) Steps 0(0.00) | Grad Norm 5671.9970(4450.8636) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0051 | Time 82.0300, Epoch Time 1017.8305(953.9492), Bit/dim 3.7251(best: 3.7221), Xent 0.8766, Loss 4.1634, Error 0.3101(best: 0.3147)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2810 | Time 16.6656(16.6161) | Bit/dim 3.7279(3.7284) | Xent 0.9365(0.9073) | Loss 5454.7842(6058.3602) | Error 0.3433(0.3249) Steps 0(0.00) | Grad Norm 2710.6226(4505.8922) | Total Time 0.00(0.00)\n",
      "Iter 2820 | Time 15.8524(16.5038) | Bit/dim 3.7172(3.7286) | Xent 0.8627(0.9047) | Loss 5195.1733(5839.3842) | Error 0.3133(0.3248) Steps 0(0.00) | Grad Norm 4877.9680(4427.3455) | Total Time 0.00(0.00)\n",
      "Iter 2830 | Time 15.4928(16.3628) | Bit/dim 3.7115(3.7280) | Xent 0.8545(0.9015) | Loss 5204.0952(5681.0829) | Error 0.3044(0.3226) Steps 0(0.00) | Grad Norm 4072.4852(4462.3933) | Total Time 0.00(0.00)\n",
      "Iter 2840 | Time 16.6936(16.4716) | Bit/dim 3.7139(3.7271) | Xent 0.9212(0.8992) | Loss 5315.5747(5575.4820) | Error 0.3122(0.3209) Steps 0(0.00) | Grad Norm 6122.9983(4405.1154) | Total Time 0.00(0.00)\n",
      "Iter 2850 | Time 14.3801(16.4708) | Bit/dim 3.7103(3.7289) | Xent 0.9140(0.9002) | Loss 5217.5786(5493.1927) | Error 0.3244(0.3219) Steps 0(0.00) | Grad Norm 4393.3620(4326.0433) | Total Time 0.00(0.00)\n",
      "Iter 2860 | Time 18.2117(16.5297) | Bit/dim 3.7499(3.7272) | Xent 0.9395(0.9050) | Loss 5403.7866(5444.5305) | Error 0.3278(0.3237) Steps 0(0.00) | Grad Norm 7386.5938(4518.3278) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0052 | Time 81.1600, Epoch Time 1003.7152(955.4421), Bit/dim 3.7294(best: 3.7221), Xent 0.8872, Loss 4.1730, Error 0.3165(best: 0.3101)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2870 | Time 17.1736(16.4979) | Bit/dim 3.6971(3.7270) | Xent 0.8683(0.8969) | Loss 5347.6450(5915.0919) | Error 0.3111(0.3208) Steps 0(0.00) | Grad Norm 4534.4704(4617.5139) | Total Time 0.00(0.00)\n",
      "Iter 2880 | Time 16.8493(16.5511) | Bit/dim 3.6989(3.7270) | Xent 0.8626(0.8902) | Loss 5385.6030(5749.4356) | Error 0.3144(0.3180) Steps 0(0.00) | Grad Norm 3707.1744(4477.6273) | Total Time 0.00(0.00)\n",
      "Iter 2890 | Time 17.1591(16.6121) | Bit/dim 3.7014(3.7271) | Xent 0.8980(0.8901) | Loss 5443.4834(5638.2957) | Error 0.3167(0.3202) Steps 0(0.00) | Grad Norm 5778.5074(4589.7428) | Total Time 0.00(0.00)\n",
      "Iter 2900 | Time 16.1946(16.6030) | Bit/dim 3.7196(3.7259) | Xent 0.8708(0.8850) | Loss 5220.5083(5542.0896) | Error 0.2867(0.3167) Steps 0(0.00) | Grad Norm 3432.5637(4363.6023) | Total Time 0.00(0.00)\n",
      "Iter 2910 | Time 16.4368(16.8099) | Bit/dim 3.7300(3.7249) | Xent 0.9111(0.8767) | Loss 5409.7100(5457.8802) | Error 0.3256(0.3138) Steps 0(0.00) | Grad Norm 5781.6454(4250.6493) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0053 | Time 83.2645, Epoch Time 1020.9005(957.4059), Bit/dim 3.7202(best: 3.7221), Xent 0.8759, Loss 4.1582, Error 0.3075(best: 0.3101)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2920 | Time 18.8487(16.7842) | Bit/dim 3.7320(3.7268) | Xent 0.8376(0.8727) | Loss 5352.0156(6022.5909) | Error 0.3111(0.3129) Steps 0(0.00) | Grad Norm 3073.7417(4106.9208) | Total Time 0.00(0.00)\n",
      "Iter 2930 | Time 16.4231(16.6609) | Bit/dim 3.6900(3.7236) | Xent 0.8778(0.8711) | Loss 5240.3057(5839.1975) | Error 0.3211(0.3117) Steps 0(0.00) | Grad Norm 4188.4324(4058.5895) | Total Time 0.00(0.00)\n",
      "Iter 2940 | Time 17.4237(16.6533) | Bit/dim 3.7232(3.7205) | Xent 0.8757(0.8744) | Loss 5295.4883(5689.0221) | Error 0.3267(0.3135) Steps 0(0.00) | Grad Norm 3895.7053(4254.0879) | Total Time 0.00(0.00)\n",
      "Iter 2950 | Time 15.6956(16.6602) | Bit/dim 3.7082(3.7189) | Xent 0.8545(0.8745) | Loss 4997.9302(5571.5560) | Error 0.3089(0.3132) Steps 0(0.00) | Grad Norm 4066.9254(4031.9534) | Total Time 0.00(0.00)\n",
      "Iter 2960 | Time 17.8909(16.7113) | Bit/dim 3.6921(3.7161) | Xent 0.8597(0.8746) | Loss 5355.4375(5487.6930) | Error 0.3089(0.3131) Steps 0(0.00) | Grad Norm 4397.5077(4054.6515) | Total Time 0.00(0.00)\n",
      "Iter 2970 | Time 18.0443(16.7582) | Bit/dim 3.6988(3.7165) | Xent 0.8536(0.8689) | Loss 5387.7876(5424.1766) | Error 0.3078(0.3114) Steps 0(0.00) | Grad Norm 3298.2234(3933.8818) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0054 | Time 85.3710, Epoch Time 1022.0915(959.3465), Bit/dim 3.7249(best: 3.7202), Xent 0.8396, Loss 4.1447, Error 0.2989(best: 0.3075)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2980 | Time 15.1991(16.7380) | Bit/dim 3.6998(3.7189) | Xent 0.8748(0.8647) | Loss 5187.4526(5910.8780) | Error 0.3211(0.3098) Steps 0(0.00) | Grad Norm 4707.7465(4100.8511) | Total Time 0.00(0.00)\n",
      "Iter 2990 | Time 17.9629(16.7409) | Bit/dim 3.7205(3.7184) | Xent 0.7861(0.8515) | Loss 5259.5278(5725.6733) | Error 0.2744(0.3052) Steps 0(0.00) | Grad Norm 2629.3874(3942.3392) | Total Time 0.00(0.00)\n",
      "Iter 3000 | Time 15.8036(16.7005) | Bit/dim 3.7585(3.7216) | Xent 0.8735(0.8534) | Loss 5181.1074(5594.1192) | Error 0.3289(0.3051) Steps 0(0.00) | Grad Norm 6370.0273(4120.3042) | Total Time 0.00(0.00)\n",
      "Iter 3010 | Time 16.4073(16.5942) | Bit/dim 3.7134(3.7172) | Xent 0.9118(0.8580) | Loss 5425.8037(5516.4149) | Error 0.3189(0.3051) Steps 0(0.00) | Grad Norm 4509.4278(4231.4348) | Total Time 0.00(0.00)\n",
      "Iter 3020 | Time 18.1717(16.5684) | Bit/dim 3.7081(3.7178) | Xent 0.9059(0.8600) | Loss 5388.8740(5458.9677) | Error 0.3222(0.3057) Steps 0(0.00) | Grad Norm 3370.7301(4275.5133) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0055 | Time 83.0893, Epoch Time 1010.9522(960.8946), Bit/dim 3.7140(best: 3.7202), Xent 0.8557, Loss 4.1419, Error 0.3020(best: 0.2989)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3030 | Time 18.4547(16.5522) | Bit/dim 3.7196(3.7154) | Xent 0.9122(0.8611) | Loss 5432.0869(6000.5535) | Error 0.3311(0.3075) Steps 0(0.00) | Grad Norm 4815.0694(4147.2960) | Total Time 0.00(0.00)\n",
      "Iter 3040 | Time 16.3266(16.4429) | Bit/dim 3.7256(3.7152) | Xent 0.8072(0.8526) | Loss 5189.6323(5795.3478) | Error 0.2956(0.3051) Steps 0(0.00) | Grad Norm 3355.2579(3978.2198) | Total Time 0.00(0.00)\n",
      "Iter 3050 | Time 16.5345(16.5314) | Bit/dim 3.7191(3.7135) | Xent 0.7946(0.8493) | Loss 5129.6958(5650.0056) | Error 0.2700(0.3043) Steps 0(0.00) | Grad Norm 3120.9338(3948.9841) | Total Time 0.00(0.00)\n",
      "Iter 3060 | Time 16.9559(16.5506) | Bit/dim 3.7234(3.7125) | Xent 0.8921(0.8552) | Loss 5574.8394(5557.6134) | Error 0.3267(0.3080) Steps 0(0.00) | Grad Norm 3269.9477(4212.2520) | Total Time 0.00(0.00)\n",
      "Iter 3070 | Time 16.4670(16.6171) | Bit/dim 3.7427(3.7112) | Xent 0.8743(0.8516) | Loss 5212.8252(5466.8919) | Error 0.3078(0.3051) Steps 0(0.00) | Grad Norm 3228.3459(4216.9492) | Total Time 0.00(0.00)\n",
      "Iter 3080 | Time 18.3574(16.7173) | Bit/dim 3.7172(3.7112) | Xent 0.8313(0.8471) | Loss 5287.8882(5405.0281) | Error 0.2956(0.3050) Steps 0(0.00) | Grad Norm 2472.7357(3888.3986) | Total Time 0.00(0.00)\n",
      "validating...\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_rl.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_6_rlw_10_0_run1 --seed 1 --lr 0.001 --conditional True --controlled_tol False --train_mode semisup --log_freq 10 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5 --scale_fac 1.0 --scale_std 6.0 --rl-weight 10.0 --max_grad_norm 20.0\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
