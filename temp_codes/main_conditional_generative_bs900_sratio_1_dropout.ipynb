{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0,1,2,3,4,5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_generative.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "if args.controlled_tol:\n",
      "    import lib.odenvp_conditional_tol as odenvp\n",
      "else:\n",
      "    import lib.odenvp_conditional as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    logpz_sup_per_dim = torch.sum(logpz_sup) / z[:, 0:dim_sup].nelement()  # averaged over batches\n",
      "    bits_sup_per_dim = -(logpz_sup_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute the predicted labels\n",
      "    all_y = torch.from_numpy(np.arange(model.module.y_class)).type(torch.long).to(x.get_device(), non_blocking=True)\n",
      "    all_y_onehot = thops.onehot(all_y, num_classes=model.module.y_class)\n",
      "    all_mean, all_logs = model.module._prior(all_y_onehot)\n",
      "    \n",
      "    zsup = zsup.view(zsup.shape[0], 1, zsup.shape[1])\n",
      "    zsup = zsup.repeat(1,model.module.y_class,1)\n",
      "    likelihood = modules.GaussianDiag.likelihood(all_mean, all_logs, zsup)\n",
      "    dims = [i+2 for i in range(len(likelihood.size()) - 2)]\n",
      "    all_logpz_sup = thops.sum(likelihood, dim=dims)\n",
      "    y_predicted = np.argmax(all_logpz_sup.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, bits_sup_per_dim, y_predicted\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    if args.conditional: best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            loss.backward()\n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}, Xent {:.4f}, Loss {:.4f}, Error {:.4f}\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, loss_xent, loss, error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, condition_ratio=1.0, conditional=True, controlled_tol=False, conv=True, data='mnist', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume=None, rtol=1e-05, save='../experiments/cnf_conditional_generative_bs900_sratio_1_0_drop_0_5_seed_1', seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5)\n",
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=1568, bias=True)\n",
      "  (project_class): LinearZeros(in_features=784, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 828890\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc.py:286: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "Iter 0010 | Time 16.0343(40.9517) | Bit/dim 17.7868(19.1112) | Xent 26.2920(27.6203) | Loss 30.9328(32.9214) | Error 0.4411(0.8059) Steps 410(410.00) | Grad Norm 191.4958(213.8992) | Total Time 10.00(10.00)\n",
      "Iter 0020 | Time 14.7171(34.0121) | Bit/dim 14.5762(18.2982) | Xent 23.0741(26.8070) | Loss 26.1133(31.7017) | Error 0.6556(0.7461) Steps 410(410.00) | Grad Norm 130.6178(199.2996) | Total Time 10.00(10.00)\n",
      "Iter 0030 | Time 14.3776(28.9291) | Bit/dim 12.0898(16.9312) | Xent 20.5809(25.4370) | Loss 22.3803(29.6497) | Error 0.4856(0.7000) Steps 410(410.00) | Grad Norm 64.3932(170.6951) | Total Time 10.00(10.00)\n",
      "Iter 0040 | Time 13.9174(25.1326) | Bit/dim 10.1844(15.3632) | Xent 18.6716(23.8647) | Loss 19.5202(27.2956) | Error 0.3400(0.6210) Steps 410(410.00) | Grad Norm 31.8722(137.2381) | Total Time 10.00(10.00)\n",
      "Iter 0050 | Time 14.7564(22.3831) | Bit/dim 8.7189(13.7773) | Xent 17.1824(22.2700) | Loss 17.3101(24.9123) | Error 0.3733(0.5536) Steps 410(410.00) | Grad Norm 21.6950(107.5754) | Total Time 10.00(10.00)\n",
      "Iter 0060 | Time 14.6632(20.2889) | Bit/dim 7.2851(12.2342) | Xent 15.7384(20.7169) | Loss 15.1543(22.5926) | Error 0.4144(0.5082) Steps 410(410.00) | Grad Norm 22.4608(85.2414) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0001 | Time 100.5956, Epoch Time 1116.9474(1116.9474), Bit/dim 6.4670, Xent 14.8849, Loss 13.9095, Error 0.3229\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0070 | Time 14.5644(18.8272) | Bit/dim 5.9993(10.7509) | Xent 14.4115(19.2175) | Loss 13.2051(20.3597) | Error 0.4222(0.4815) Steps 410(410.00) | Grad Norm 18.0139(68.0473) | Total Time 10.00(10.00)\n",
      "Iter 0080 | Time 15.1450(17.7338) | Bit/dim 4.8048(9.3167) | Xent 13.1640(17.7613) | Loss 11.3868(18.1974) | Error 0.3889(0.4619) Steps 410(410.00) | Grad Norm 15.9052(54.5532) | Total Time 10.00(10.00)\n",
      "Iter 0090 | Time 14.2041(16.8803) | Bit/dim 3.7848(7.9703) | Xent 12.0857(16.3838) | Loss 9.8276(16.1622) | Error 0.4056(0.4458) Steps 416(410.35) | Grad Norm 13.1255(44.0416) | Total Time 10.00(10.00)\n",
      "Iter 0100 | Time 14.5098(16.3195) | Bit/dim 3.1284(6.7683) | Xent 11.3428(15.1373) | Loss 8.7998(14.3369) | Error 0.4000(0.4324) Steps 422(413.28) | Grad Norm 8.7958(35.2542) | Total Time 10.00(10.00)\n",
      "Iter 0110 | Time 14.9558(16.0626) | Bit/dim 2.7599(5.7476) | Xent 10.8654(14.0617) | Loss 8.1926(12.7784) | Error 0.3933(0.4244) Steps 428(416.09) | Grad Norm 5.9562(27.8416) | Total Time 10.00(10.00)\n",
      "Iter 0120 | Time 16.3440(16.0130) | Bit/dim 2.5267(4.9210) | Xent 10.5438(13.1695) | Loss 7.7986(11.5057) | Error 0.4122(0.4212) Steps 440(421.67) | Grad Norm 3.8736(21.7550) | Total Time 10.00(10.00)\n",
      "Iter 0130 | Time 16.4711(15.9610) | Bit/dim 2.3863(4.2693) | Xent 10.3292(12.4449) | Loss 7.5509(10.4918) | Error 0.4411(0.4168) Steps 446(426.84) | Grad Norm 2.6906(16.8654) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0002 | Time 104.7099, Epoch Time 1121.4216(1117.0817), Bit/dim 2.3584, Xent 10.2687, Loss 7.4927, Error 0.2513\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0140 | Time 15.8857(15.9407) | Bit/dim 2.3164(3.7649) | Xent 10.1837(11.8659) | Loss 7.4082(9.6979) | Error 0.4311(0.4175) Steps 446(431.87) | Grad Norm 1.9431(13.0340) | Total Time 10.00(10.00)\n",
      "Iter 0150 | Time 15.9118(16.0150) | Bit/dim 2.2784(3.3800) | Xent 10.0717(11.4061) | Loss 7.3143(9.0830) | Error 0.4111(0.4163) Steps 446(435.58) | Grad Norm 1.3775(10.0298) | Total Time 10.00(10.00)\n",
      "Iter 0160 | Time 16.5233(16.1024) | Bit/dim 2.2653(3.0890) | Xent 10.0033(11.0450) | Loss 7.2669(8.6115) | Error 0.4200(0.4129) Steps 458(438.67) | Grad Norm 1.0093(7.7036) | Total Time 10.00(10.00)\n",
      "Iter 0170 | Time 16.2572(16.1979) | Bit/dim 2.2556(2.8719) | Xent 9.9552(10.7627) | Loss 7.2332(8.2533) | Error 0.3978(0.4079) Steps 458(443.75) | Grad Norm 0.8724(5.9235) | Total Time 10.00(10.00)\n",
      "Iter 0180 | Time 17.2710(16.2812) | Bit/dim 2.2473(2.7090) | Xent 9.9171(10.5436) | Loss 7.2058(7.9808) | Error 0.3733(0.4065) Steps 458(447.49) | Grad Norm 0.7270(4.5705) | Total Time 10.00(10.00)\n",
      "Iter 0190 | Time 16.2922(16.2568) | Bit/dim 2.2537(2.5881) | Xent 9.8845(10.3733) | Loss 7.1960(7.7747) | Error 0.3789(0.3984) Steps 452(449.40) | Grad Norm 0.6501(3.5503) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0003 | Time 100.8248, Epoch Time 1194.2598(1119.3970), Bit/dim 2.2333, Xent 9.8630, Loss 7.1648, Error 0.3092\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0200 | Time 17.2550(16.3344) | Bit/dim 2.2397(2.4966) | Xent 9.8697(10.2418) | Loss 7.1745(7.6175) | Error 0.3911(0.3943) Steps 452(450.08) | Grad Norm 0.6140(2.7863) | Total Time 10.00(10.00)\n",
      "Iter 0210 | Time 16.4880(16.3094) | Bit/dim 2.2300(2.4269) | Xent 9.8515(10.1405) | Loss 7.1558(7.4971) | Error 0.3767(0.3866) Steps 452(450.59) | Grad Norm 0.5888(2.2111) | Total Time 10.00(10.00)\n",
      "Iter 0220 | Time 15.7196(16.2865) | Bit/dim 2.2099(2.3728) | Xent 9.8382(10.0626) | Loss 7.1290(7.4041) | Error 0.3467(0.3766) Steps 440(448.95) | Grad Norm 0.5689(1.7813) | Total Time 10.00(10.00)\n",
      "Iter 0230 | Time 15.0791(16.1855) | Bit/dim 2.2180(2.3316) | Xent 9.8312(10.0023) | Loss 7.1336(7.3328) | Error 0.3211(0.3672) Steps 440(446.60) | Grad Norm 0.5848(1.4605) | Total Time 10.00(10.00)\n",
      "Iter 0240 | Time 16.0133(16.1415) | Bit/dim 2.2082(2.2988) | Xent 9.8251(9.9561) | Loss 7.1208(7.2769) | Error 0.3367(0.3601) Steps 440(444.87) | Grad Norm 0.4836(1.2085) | Total Time 10.00(10.00)\n",
      "Iter 0250 | Time 16.0683(15.9997) | Bit/dim 2.1947(2.2726) | Xent 9.8198(9.9207) | Loss 7.1045(7.2329) | Error 0.3456(0.3538) Steps 440(443.59) | Grad Norm 0.4703(1.0174) | Total Time 10.00(10.00)\n",
      "Iter 0260 | Time 15.9601(16.0063) | Bit/dim 2.1796(2.2496) | Xent 9.8122(9.8933) | Loss 7.0857(7.1963) | Error 0.3233(0.3468) Steps 440(442.82) | Grad Norm 0.4441(0.8737) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0004 | Time 102.6032, Epoch Time 1177.6503(1121.1446), Bit/dim 2.1727, Xent 9.8100, Loss 7.0777, Error 0.3223\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0270 | Time 15.6297(15.9353) | Bit/dim 2.1625(2.2295) | Xent 9.8114(9.8720) | Loss 7.0682(7.1655) | Error 0.3489(0.3419) Steps 446(443.66) | Grad Norm 0.4382(0.7632) | Total Time 10.00(10.00)\n",
      "Iter 0280 | Time 15.1918(15.9135) | Bit/dim 2.1492(2.2113) | Xent 9.8098(9.8559) | Loss 7.0542(7.1393) | Error 0.3478(0.3395) Steps 446(444.27) | Grad Norm 0.4348(0.6768) | Total Time 10.00(10.00)\n",
      "Iter 0290 | Time 16.7833(15.9044) | Bit/dim 2.1554(2.1953) | Xent 9.8070(9.8434) | Loss 7.0589(7.1169) | Error 0.3322(0.3389) Steps 446(444.73) | Grad Norm 0.4234(0.6104) | Total Time 10.00(10.00)\n",
      "Iter 0300 | Time 16.1395(15.9034) | Bit/dim 2.1323(2.1817) | Xent 9.8069(9.8335) | Loss 7.0358(7.0985) | Error 0.3567(0.3374) Steps 458(445.72) | Grad Norm 0.4558(0.5681) | Total Time 10.00(10.00)\n",
      "Iter 0310 | Time 15.7693(15.9825) | Bit/dim 2.1275(2.1671) | Xent 9.8058(9.8261) | Loss 7.0304(7.0802) | Error 0.3200(0.3348) Steps 452(447.52) | Grad Norm 0.4316(0.5314) | Total Time 10.00(10.00)\n",
      "Iter 0320 | Time 15.7755(16.0812) | Bit/dim 2.1015(2.1522) | Xent 9.8051(9.8201) | Loss 7.0041(7.0622) | Error 0.3322(0.3330) Steps 452(448.70) | Grad Norm 0.4091(0.4965) | Total Time 10.00(10.00)\n",
      "Iter 0330 | Time 15.8158(16.1029) | Bit/dim 2.0911(2.1391) | Xent 9.8004(9.8154) | Loss 6.9913(7.0468) | Error 0.3489(0.3334) Steps 452(449.56) | Grad Norm 0.3997(0.4715) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0005 | Time 103.9902, Epoch Time 1177.9668(1122.8493), Bit/dim 2.0889, Xent 9.7986, Loss 6.9882, Error 0.3178\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0340 | Time 15.8348(16.1098) | Bit/dim 2.0962(2.1252) | Xent 9.8014(9.8115) | Loss 6.9969(7.0310) | Error 0.3511(0.3356) Steps 452(450.20) | Grad Norm 0.3940(0.4502) | Total Time 10.00(10.00)\n",
      "Iter 0350 | Time 16.2890(16.1485) | Bit/dim 2.0713(2.1117) | Xent 9.7983(9.8085) | Loss 6.9704(7.0159) | Error 0.3267(0.3380) Steps 452(450.67) | Grad Norm 0.4143(0.4382) | Total Time 10.00(10.00)\n",
      "Iter 0360 | Time 16.6235(16.2416) | Bit/dim 2.0466(2.0989) | Xent 9.8011(9.8062) | Loss 6.9471(7.0020) | Error 0.3278(0.3356) Steps 452(451.02) | Grad Norm 0.3966(0.4299) | Total Time 10.00(10.00)\n",
      "Iter 0370 | Time 16.6834(16.2066) | Bit/dim 2.0437(2.0848) | Xent 9.7955(9.8041) | Loss 6.9415(6.9869) | Error 0.3333(0.3357) Steps 452(451.28) | Grad Norm 0.3706(0.4208) | Total Time 10.00(10.00)\n",
      "Iter 0380 | Time 15.7376(16.2260) | Bit/dim 2.0257(2.0726) | Xent 9.7992(9.8023) | Loss 6.9253(6.9738) | Error 0.3367(0.3375) Steps 452(451.47) | Grad Norm 0.3638(0.4062) | Total Time 10.00(10.00)\n",
      "Iter 0390 | Time 16.7501(16.2136) | Bit/dim 2.0126(2.0591) | Xent 9.7978(9.8007) | Loss 6.9115(6.9594) | Error 0.3644(0.3390) Steps 452(451.61) | Grad Norm 0.5637(0.4081) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0006 | Time 102.5861, Epoch Time 1192.3118(1124.9331), Bit/dim 2.0079, Xent 9.7945, Loss 6.9051, Error 0.3047\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0400 | Time 15.8621(16.1927) | Bit/dim 2.0122(2.0472) | Xent 9.7973(9.8000) | Loss 6.9109(6.9472) | Error 0.3533(0.3419) Steps 452(451.71) | Grad Norm 0.4258(0.4217) | Total Time 10.00(10.00)\n",
      "Iter 0410 | Time 16.7025(16.2077) | Bit/dim 2.0006(2.0357) | Xent 9.7968(9.7993) | Loss 6.8990(6.9353) | Error 0.3444(0.3431) Steps 452(451.79) | Grad Norm 0.4667(0.4153) | Total Time 10.00(10.00)\n",
      "Iter 0420 | Time 16.5196(16.1425) | Bit/dim 1.9809(2.0235) | Xent 9.8022(9.7990) | Loss 6.8821(6.9230) | Error 0.3656(0.3460) Steps 458(453.12) | Grad Norm 0.3424(0.4085) | Total Time 10.00(10.00)\n",
      "Iter 0430 | Time 16.0962(16.1729) | Bit/dim 1.9719(2.0121) | Xent 9.7986(9.7989) | Loss 6.8712(6.9116) | Error 0.3522(0.3506) Steps 458(454.40) | Grad Norm 0.3043(0.4104) | Total Time 10.00(10.00)\n",
      "Iter 0440 | Time 16.9105(16.3282) | Bit/dim 1.9593(2.0006) | Xent 9.7963(9.7992) | Loss 6.8574(6.9002) | Error 0.3589(0.3526) Steps 464(456.79) | Grad Norm 0.3430(0.4165) | Total Time 10.00(10.00)\n",
      "Iter 0450 | Time 16.2588(16.4052) | Bit/dim 1.9539(1.9896) | Xent 9.7996(9.7996) | Loss 6.8537(6.8894) | Error 0.3478(0.3571) Steps 464(458.68) | Grad Norm 0.2903(0.4318) | Total Time 10.00(10.00)\n",
      "Iter 0460 | Time 17.5937(16.4898) | Bit/dim 1.9557(1.9797) | Xent 9.8032(9.8000) | Loss 6.8573(6.8797) | Error 0.3700(0.3604) Steps 464(460.08) | Grad Norm 0.6838(0.4812) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0007 | Time 107.3115, Epoch Time 1208.0723(1127.4273), Bit/dim 1.9444, Xent 9.7970, Loss 6.8429, Error 0.2800\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0470 | Time 16.6720(16.5876) | Bit/dim 1.9437(1.9713) | Xent 9.8028(9.7999) | Loss 6.8450(6.8713) | Error 0.3600(0.3623) Steps 470(462.68) | Grad Norm 0.5757(0.5049) | Total Time 10.00(10.00)\n",
      "Iter 0480 | Time 17.3362(16.7562) | Bit/dim 1.9259(1.9633) | Xent 9.7983(9.7998) | Loss 6.8250(6.8632) | Error 0.3967(0.3630) Steps 470(464.60) | Grad Norm 0.4141(0.4760) | Total Time 10.00(10.00)\n",
      "Iter 0490 | Time 16.9980(16.7896) | Bit/dim 1.9293(1.9544) | Xent 9.8019(9.7998) | Loss 6.8303(6.8543) | Error 0.4056(0.3670) Steps 470(466.02) | Grad Norm 0.3260(0.4667) | Total Time 10.00(10.00)\n",
      "Iter 0500 | Time 16.4101(16.8249) | Bit/dim 1.9097(1.9455) | Xent 9.7967(9.7990) | Loss 6.8080(6.8450) | Error 0.3933(0.3713) Steps 470(467.07) | Grad Norm 0.2687(0.4409) | Total Time 10.00(10.00)\n",
      "Iter 0510 | Time 16.7849(16.7938) | Bit/dim 1.9069(1.9370) | Xent 9.7952(9.7978) | Loss 6.8045(6.8358) | Error 0.3778(0.3714) Steps 470(467.84) | Grad Norm 0.4518(0.4315) | Total Time 10.00(10.00)\n",
      "Iter 0520 | Time 16.0225(16.7891) | Bit/dim 1.8878(1.9278) | Xent 9.7901(9.7961) | Loss 6.7829(6.8258) | Error 0.3844(0.3716) Steps 470(468.75) | Grad Norm 0.4015(0.4453) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0008 | Time 108.4567, Epoch Time 1244.3719(1130.9357), Bit/dim 1.8976, Xent 9.7824, Loss 6.7889, Error 0.2574\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0530 | Time 18.0303(16.9703) | Bit/dim 1.9129(1.9217) | Xent 9.7859(9.7939) | Loss 6.8059(6.8186) | Error 0.3778(0.3713) Steps 476(470.39) | Grad Norm 0.7940(0.4572) | Total Time 10.00(10.00)\n",
      "Iter 0540 | Time 17.4082(17.0865) | Bit/dim 1.8938(1.9153) | Xent 9.7840(9.7912) | Loss 6.7858(6.8109) | Error 0.3700(0.3717) Steps 488(473.99) | Grad Norm 0.7796(0.4830) | Total Time 10.00(10.00)\n",
      "Iter 0550 | Time 16.9934(17.1725) | Bit/dim 1.8736(1.9080) | Xent 9.7779(9.7885) | Loss 6.7625(6.8022) | Error 0.3989(0.3731) Steps 488(477.67) | Grad Norm 0.2854(0.4758) | Total Time 10.00(10.00)\n",
      "Iter 0560 | Time 17.4843(17.2477) | Bit/dim 1.9001(1.9016) | Xent 9.7735(9.7849) | Loss 6.7869(6.7940) | Error 0.3700(0.3746) Steps 488(480.05) | Grad Norm 0.3682(0.4399) | Total Time 10.00(10.00)\n",
      "Iter 0570 | Time 17.3885(17.3130) | Bit/dim 1.8558(1.8925) | Xent 9.7716(9.7818) | Loss 6.7416(6.7834) | Error 0.3689(0.3738) Steps 488(481.31) | Grad Norm 0.7864(0.4692) | Total Time 10.00(10.00)\n",
      "Iter 0580 | Time 17.3631(17.3285) | Bit/dim 1.8600(1.8831) | Xent 9.7691(9.7789) | Loss 6.7445(6.7725) | Error 0.3900(0.3720) Steps 482(481.81) | Grad Norm 0.6614(0.4905) | Total Time 10.00(10.00)\n",
      "Iter 0590 | Time 17.0129(17.2665) | Bit/dim 1.8146(1.8717) | Xent 9.7647(9.7754) | Loss 6.6969(6.7594) | Error 0.3489(0.3716) Steps 482(482.00) | Grad Norm 0.8301(0.5074) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0009 | Time 117.4022, Epoch Time 1283.3458(1135.5080), Bit/dim 1.8200, Xent 9.7625, Loss 6.7013, Error 0.2463\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0600 | Time 17.8436(17.3106) | Bit/dim 1.7965(1.8596) | Xent 9.7672(9.7727) | Loss 6.6801(6.7459) | Error 0.3589(0.3718) Steps 482(482.00) | Grad Norm 0.5122(0.5565) | Total Time 10.00(10.00)\n",
      "Iter 0610 | Time 17.1078(17.2612) | Bit/dim 1.7916(1.8423) | Xent 9.7533(9.7699) | Loss 6.6682(6.7273) | Error 0.3422(0.3710) Steps 482(482.00) | Grad Norm 0.8536(0.5809) | Total Time 10.00(10.00)\n",
      "Iter 0620 | Time 17.5704(17.2926) | Bit/dim 1.7444(1.8202) | Xent 9.7560(9.7666) | Loss 6.6224(6.7035) | Error 0.3956(0.3740) Steps 482(482.33) | Grad Norm 0.4150(0.5919) | Total Time 10.00(10.00)\n",
      "Iter 0630 | Time 17.2554(17.3675) | Bit/dim 1.6971(1.7925) | Xent 9.7414(9.7617) | Loss 6.5678(6.6734) | Error 0.3622(0.3727) Steps 488(483.67) | Grad Norm 1.0482(0.6339) | Total Time 10.00(10.00)\n",
      "Iter 0640 | Time 18.3587(17.4698) | Bit/dim 1.6958(1.7730) | Xent 9.7160(9.7596) | Loss 6.5538(6.6528) | Error 0.4322(0.3847) Steps 488(484.66) | Grad Norm 7.6884(3.3900) | Total Time 10.00(10.00)\n",
      "Iter 0650 | Time 18.2663(17.4947) | Bit/dim 1.6720(1.7500) | Xent 9.7081(9.7508) | Loss 6.5260(6.6254) | Error 0.3667(0.3887) Steps 488(485.20) | Grad Norm 3.2995(4.2709) | Total Time 10.00(10.00)\n",
      "Iter 0660 | Time 16.7915(17.4281) | Bit/dim 1.6554(1.7242) | Xent 9.7092(9.7406) | Loss 6.5099(6.5945) | Error 0.3600(0.3844) Steps 482(484.78) | Grad Norm 4.5013(4.2681) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0010 | Time 111.2054, Epoch Time 1278.4702(1139.7968), Bit/dim 1.6319, Xent 9.7047, Loss 6.4843, Error 0.2263\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0670 | Time 17.4327(17.3390) | Bit/dim 1.6211(1.6981) | Xent 9.6945(9.7304) | Loss 6.4684(6.5632) | Error 0.3878(0.3820) Steps 482(484.19) | Grad Norm 3.0679(4.0027) | Total Time 10.00(10.00)\n",
      "Iter 0680 | Time 17.7315(17.3213) | Bit/dim 1.6317(1.6800) | Xent 9.6704(9.7203) | Loss 6.4669(6.5401) | Error 0.3444(0.3845) Steps 482(483.62) | Grad Norm 12.1172(5.6623) | Total Time 10.00(10.00)\n",
      "Iter 0690 | Time 16.7164(17.2387) | Bit/dim 1.5975(1.6608) | Xent 9.6597(9.7091) | Loss 6.4273(6.5153) | Error 0.3678(0.3919) Steps 476(482.23) | Grad Norm 2.1507(6.0673) | Total Time 10.00(10.00)\n",
      "Iter 0700 | Time 17.4077(17.2020) | Bit/dim 1.5984(1.6415) | Xent 9.6560(9.6966) | Loss 6.4265(6.4898) | Error 0.4056(0.3916) Steps 482(481.28) | Grad Norm 2.3837(5.7014) | Total Time 10.00(10.00)\n",
      "Iter 0710 | Time 17.4685(17.1975) | Bit/dim 1.5572(1.6259) | Xent 9.6499(9.6834) | Loss 6.3822(6.4676) | Error 0.4711(0.3937) Steps 482(481.47) | Grad Norm 7.5619(5.7793) | Total Time 10.00(10.00)\n",
      "Iter 0720 | Time 16.3987(17.1999) | Bit/dim 1.5862(1.6187) | Xent 9.6192(9.6703) | Loss 6.3958(6.4539) | Error 0.3889(0.4048) Steps 482(482.10) | Grad Norm 1.7593(7.7582) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0011 | Time 112.7737, Epoch Time 1262.0875(1143.4656), Bit/dim 1.5550, Xent 9.5952, Loss 6.3526, Error 0.2354\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0730 | Time 17.1792(17.2177) | Bit/dim 1.5434(1.6029) | Xent 9.6211(9.6535) | Loss 6.3540(6.4296) | Error 0.4667(0.4081) Steps 482(482.85) | Grad Norm 8.8031(7.6552) | Total Time 10.00(10.00)\n",
      "Iter 0740 | Time 16.4448(17.1672) | Bit/dim 1.5580(1.5892) | Xent 9.5612(9.6345) | Loss 6.3386(6.4064) | Error 0.3756(0.4067) Steps 482(482.95) | Grad Norm 5.3485(7.2319) | Total Time 10.00(10.00)\n",
      "Iter 0750 | Time 17.4616(17.2568) | Bit/dim 1.5746(1.5876) | Xent 9.6187(9.6182) | Loss 6.3839(6.3967) | Error 0.6011(0.4241) Steps 494(484.38) | Grad Norm 22.4825(9.9978) | Total Time 10.00(10.00)\n",
      "Iter 0760 | Time 17.0335(17.2948) | Bit/dim 1.5453(1.5790) | Xent 9.4844(9.5953) | Loss 6.2875(6.3767) | Error 0.4344(0.4307) Steps 494(486.90) | Grad Norm 3.5158(10.4166) | Total Time 10.00(10.00)\n",
      "Iter 0770 | Time 17.0298(17.2936) | Bit/dim 1.5160(1.5680) | Xent 9.4768(9.5706) | Loss 6.2544(6.3533) | Error 0.4256(0.4344) Steps 494(488.28) | Grad Norm 2.4169(9.8337) | Total Time 10.00(10.00)\n",
      "Iter 0780 | Time 17.3087(17.3589) | Bit/dim 1.5149(1.5573) | Xent 9.4731(9.5441) | Loss 6.2515(6.3293) | Error 0.5244(0.4371) Steps 494(488.84) | Grad Norm 12.1848(9.7219) | Total Time 10.00(10.00)\n",
      "Iter 0790 | Time 17.2751(17.4195) | Bit/dim 1.5326(1.5476) | Xent 9.3824(9.5118) | Loss 6.2238(6.3035) | Error 0.4356(0.4391) Steps 494(489.91) | Grad Norm 6.3546(8.1745) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0012 | Time 116.2400, Epoch Time 1281.8331(1147.6166), Bit/dim 1.6331, Xent 9.6434, Loss 6.4548, Error 0.4665\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0800 | Time 18.1960(17.4755) | Bit/dim 1.6137(1.5712) | Xent 9.3367(9.4886) | Loss 6.2820(6.3155) | Error 0.3911(0.4644) Steps 494(491.66) | Grad Norm 20.8245(12.6749) | Total Time 10.00(10.00)\n",
      "Iter 0810 | Time 17.4047(17.5329) | Bit/dim 1.5381(1.5709) | Xent 9.3279(9.4551) | Loss 6.2020(6.2984) | Error 0.4333(0.4751) Steps 494(493.35) | Grad Norm 3.8481(12.9078) | Total Time 10.00(10.00)\n",
      "Iter 0820 | Time 18.2978(17.5614) | Bit/dim 1.5074(1.5613) | Xent 9.2805(9.4179) | Loss 6.1477(6.2702) | Error 0.4567(0.4718) Steps 494(494.19) | Grad Norm 2.7140(11.7664) | Total Time 10.00(10.00)\n",
      "Iter 0830 | Time 18.1991(17.5943) | Bit/dim 1.4878(1.5487) | Xent 9.2144(9.3746) | Loss 6.0950(6.2360) | Error 0.4589(0.4704) Steps 494(494.14) | Grad Norm 4.6538(9.9652) | Total Time 10.00(10.00)\n",
      "Iter 0840 | Time 17.4408(17.5620) | Bit/dim 1.5331(1.5392) | Xent 9.1312(9.3257) | Loss 6.0987(6.2020) | Error 0.4078(0.4653) Steps 494(494.10) | Grad Norm 8.7604(8.5036) | Total Time 10.00(10.00)\n",
      "Iter 0850 | Time 18.1780(17.6589) | Bit/dim 1.6936(1.6073) | Xent 9.1264(9.3237) | Loss 6.2568(6.2691) | Error 0.5233(0.5087) Steps 524(497.55) | Grad Norm 18.4242(15.1145) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0013 | Time 138.6742, Epoch Time 1341.2756(1153.4263), Bit/dim 1.5715, Xent 9.1785, Loss 6.1608, Error 0.3474\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0860 | Time 20.5819(18.3403) | Bit/dim 1.5976(1.6088) | Xent 9.1439(9.2827) | Loss 6.1696(6.2501) | Error 0.3878(0.5124) Steps 596(519.66) | Grad Norm 7.8349(14.0358) | Total Time 10.00(10.00)\n",
      "Iter 0870 | Time 19.7318(18.9204) | Bit/dim 1.5270(1.5951) | Xent 9.1185(9.2412) | Loss 6.0862(6.2157) | Error 0.5322(0.5015) Steps 584(538.81) | Grad Norm 4.7657(11.9203) | Total Time 10.00(10.00)\n",
      "Iter 0880 | Time 20.0972(19.1673) | Bit/dim 1.5422(1.5789) | Xent 9.0093(9.1940) | Loss 6.0469(6.1759) | Error 0.4067(0.4910) Steps 554(545.13) | Grad Norm 8.9898(11.2072) | Total Time 10.00(10.00)\n",
      "Iter 0890 | Time 18.9966(19.0162) | Bit/dim 1.4992(1.5660) | Xent 9.0559(9.1432) | Loss 6.0272(6.1376) | Error 0.6089(0.4894) Steps 512(540.45) | Grad Norm 22.4211(12.2859) | Total Time 10.00(10.00)\n",
      "Iter 0900 | Time 18.2717(18.7682) | Bit/dim 1.5709(1.5796) | Xent 9.0673(9.0889) | Loss 6.1045(6.1240) | Error 0.7122(0.4940) Steps 530(533.56) | Grad Norm 32.4469(15.2089) | Total Time 10.00(10.00)\n",
      "Iter 0910 | Time 18.2194(18.5284) | Bit/dim 1.6761(1.5979) | Xent 8.6688(9.0147) | Loss 6.0105(6.1053) | Error 0.3333(0.4875) Steps 500(526.04) | Grad Norm 15.3440(16.6612) | Total Time 10.00(10.00)\n",
      "Iter 0920 | Time 17.8889(18.4155) | Bit/dim 1.6020(1.5945) | Xent 8.6648(8.9469) | Loss 5.9344(6.0680) | Error 0.3667(0.4907) Steps 512(522.48) | Grad Norm 8.6982(15.5240) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0014 | Time 122.5490, Epoch Time 1383.7185(1160.3351), Bit/dim 1.5558, Xent 8.6487, Loss 5.8802, Error 0.4538\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0930 | Time 17.9741(18.3369) | Bit/dim 1.5570(1.5858) | Xent 8.5798(8.8656) | Loss 5.8469(6.0186) | Error 0.3933(0.4859) Steps 506(519.07) | Grad Norm 7.1832(13.6542) | Total Time 10.00(10.00)\n",
      "Iter 0940 | Time 17.4067(18.1934) | Bit/dim 1.5406(1.5755) | Xent 8.4868(8.7746) | Loss 5.7840(5.9628) | Error 0.4844(0.4800) Steps 506(516.23) | Grad Norm 5.8252(11.3815) | Total Time 10.00(10.00)\n",
      "Iter 0950 | Time 19.7699(18.2089) | Bit/dim 2.0141(1.6570) | Xent 8.4232(8.7371) | Loss 6.2256(6.0255) | Error 0.6256(0.5088) Steps 536(515.97) | Grad Norm 26.0996(18.4362) | Total Time 10.00(10.00)\n",
      "Iter 0960 | Time 21.5585(18.8712) | Bit/dim 1.7618(1.6883) | Xent 8.3352(8.6639) | Loss 5.9294(6.0203) | Error 0.5200(0.5277) Steps 602(534.78) | Grad Norm 11.8154(18.3697) | Total Time 10.00(10.00)\n",
      "Iter 0970 | Time 20.0561(19.3767) | Bit/dim 1.5683(1.6735) | Xent 8.4626(8.6031) | Loss 5.7996(5.9751) | Error 0.6089(0.5425) Steps 590(549.71) | Grad Norm 11.6459(16.1699) | Total Time 10.00(10.00)\n",
      "Iter 0980 | Time 19.0067(19.4321) | Bit/dim 1.5660(1.6492) | Xent 8.2526(8.5281) | Loss 5.6923(5.9132) | Error 0.4556(0.5314) Steps 542(550.48) | Grad Norm 3.9219(13.6608) | Total Time 10.00(10.00)\n",
      "Iter 0990 | Time 18.5464(19.2051) | Bit/dim 1.5371(1.6225) | Xent 8.1213(8.4395) | Loss 5.5977(5.8423) | Error 0.4944(0.5206) Steps 518(544.70) | Grad Norm 2.0947(11.1429) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0015 | Time 122.5750, Epoch Time 1405.6766(1167.6954), Bit/dim 1.5345, Xent 8.1103, Loss 5.5896, Error 0.3589\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1000 | Time 18.6152(18.9611) | Bit/dim 1.5916(1.6032) | Xent 7.9545(8.3298) | Loss 5.5688(5.7681) | Error 0.5222(0.5120) Steps 524(537.91) | Grad Norm 17.5857(9.7946) | Total Time 10.00(10.00)\n",
      "Iter 1010 | Time 24.0391(19.5960) | Bit/dim 1.9079(1.8149) | Xent 8.3954(8.4233) | Loss 6.1056(6.0265) | Error 0.7689(0.5766) Steps 704(557.57) | Grad Norm 34.0199(23.3211) | Total Time 10.00(10.00)\n",
      "Iter 1020 | Time 25.3679(21.0658) | Bit/dim 1.6713(1.8194) | Xent 8.3742(8.3864) | Loss 5.8584(6.0126) | Error 0.6467(0.5953) Steps 746(603.21) | Grad Norm 13.5140(22.1641) | Total Time 10.00(10.00)\n",
      "Iter 1030 | Time 23.2297(21.8963) | Bit/dim 1.5588(1.7707) | Xent 8.2823(8.3542) | Loss 5.7000(5.9478) | Error 0.5778(0.5898) Steps 668(628.17) | Grad Norm 8.6712(18.3705) | Total Time 10.00(10.00)\n",
      "Iter 1040 | Time 20.0838(21.8225) | Bit/dim 1.4959(1.7066) | Xent 8.1633(8.3127) | Loss 5.5776(5.8629) | Error 0.4844(0.5696) Steps 560(622.56) | Grad Norm 2.9923(14.6599) | Total Time 10.00(10.00)\n",
      "Iter 1050 | Time 19.2182(21.2216) | Bit/dim 1.4809(1.6473) | Xent 7.9932(8.2472) | Loss 5.4775(5.7709) | Error 0.4644(0.5516) Steps 548(604.35) | Grad Norm 2.9138(11.5691) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0016 | Time 127.6159, Epoch Time 1563.5634(1179.5714), Bit/dim 1.4476, Xent 7.8964, Loss 5.3958, Error 0.3914\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1060 | Time 19.5637(20.7371) | Bit/dim 1.4544(1.5988) | Xent 7.8262(8.1564) | Loss 5.3675(5.6770) | Error 0.4811(0.5344) Steps 542(587.30) | Grad Norm 2.2110(9.1129) | Total Time 10.00(10.00)\n",
      "Iter 1070 | Time 18.4656(20.2726) | Bit/dim 1.4537(1.5601) | Xent 7.6266(8.0410) | Loss 5.2670(5.5806) | Error 0.4589(0.5201) Steps 530(573.34) | Grad Norm 2.7002(7.3543) | Total Time 10.00(10.00)\n",
      "Iter 1080 | Time 19.5391(19.9430) | Bit/dim 1.4922(1.5320) | Xent 7.3872(7.8995) | Loss 5.1858(5.4818) | Error 0.5567(0.5162) Steps 530(563.06) | Grad Norm 21.4423(6.9917) | Total Time 10.00(10.00)\n",
      "Iter 1090 | Time 24.0538(20.6513) | Bit/dim 2.0676(1.7402) | Xent 7.4858(7.9097) | Loss 5.8105(5.6951) | Error 0.6989(0.5807) Steps 710(584.36) | Grad Norm 24.2314(20.9078) | Total Time 10.00(10.00)\n",
      "Iter 1100 | Time 25.5055(22.0603) | Bit/dim 1.6773(1.7598) | Xent 7.7177(7.8562) | Loss 5.5362(5.6879) | Error 0.8667(0.6372) Steps 734(627.34) | Grad Norm 9.5867(20.1969) | Total Time 10.00(10.00)\n",
      "Iter 1110 | Time 25.1501(23.0633) | Bit/dim 1.5778(1.7211) | Xent 7.4964(7.7801) | Loss 5.3260(5.6111) | Error 0.6722(0.6760) Steps 734(657.71) | Grad Norm 5.9840(16.8752) | Total Time 10.00(10.00)\n",
      "Iter 1120 | Time 24.4589(23.7067) | Bit/dim 1.4927(1.6677) | Xent 7.3101(7.6805) | Loss 5.1478(5.5079) | Error 0.6567(0.6821) Steps 704(673.24) | Grad Norm 3.3351(13.8038) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0017 | Time 159.9347, Epoch Time 1684.9272(1194.7321), Bit/dim 1.4704, Xent 7.2792, Loss 5.1099, Error 0.4520\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1130 | Time 24.4546(23.8415) | Bit/dim 1.4695(1.6182) | Xent 7.1011(7.5546) | Loss 5.0200(5.3955) | Error 0.6389(0.6713) Steps 662(675.66) | Grad Norm 2.8208(11.1901) | Total Time 10.00(10.00)\n",
      "Iter 1140 | Time 22.5851(23.7804) | Bit/dim 1.4706(1.5752) | Xent 6.8199(7.3937) | Loss 4.8806(5.2720) | Error 0.5722(0.6522) Steps 650(673.16) | Grad Norm 5.1324(9.1954) | Total Time 10.00(10.00)\n",
      "Iter 1150 | Time 22.8396(23.5768) | Bit/dim 3.5777(1.7401) | Xent 8.1238(7.3727) | Loss 7.6396(5.4265) | Error 0.8989(0.6671) Steps 644(666.29) | Grad Norm 116.1493(23.0191) | Total Time 10.00(10.00)\n",
      "Iter 1160 | Time 22.6577(23.4500) | Bit/dim 1.7607(1.8433) | Xent 7.1961(7.3093) | Loss 5.3588(5.4980) | Error 0.9033(0.7114) Steps 602(660.17) | Grad Norm 22.9282(25.5120) | Total Time 10.00(10.00)\n",
      "Iter 1170 | Time 20.9422(22.7484) | Bit/dim 1.6431(1.8056) | Xent 7.0125(7.2375) | Loss 5.1493(5.4243) | Error 0.7167(0.7005) Steps 584(642.40) | Grad Norm 7.6846(21.2174) | Total Time 10.00(10.00)\n",
      "Iter 1180 | Time 19.7585(22.0532) | Bit/dim 1.5156(1.7402) | Xent 6.9211(7.1601) | Loss 4.9762(5.3202) | Error 0.5011(0.6670) Steps 566(623.28) | Grad Norm 7.1003(17.2888) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0018 | Time 125.5935, Epoch Time 1602.9685(1206.9792), Bit/dim 1.4512, Xent 6.7764, Loss 4.8394, Error 0.2527\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1190 | Time 19.7888(21.5381) | Bit/dim 1.4393(1.6730) | Xent 6.7577(7.0652) | Loss 4.8182(5.2055) | Error 0.3767(0.6027) Steps 566(608.40) | Grad Norm 3.6880(13.9431) | Total Time 10.00(10.00)\n",
      "Iter 1200 | Time 18.9557(21.0239) | Bit/dim 1.4525(1.6142) | Xent 6.5143(6.9444) | Loss 4.7096(5.0864) | Error 0.3789(0.5454) Steps 548(593.26) | Grad Norm 1.5789(11.1504) | Total Time 10.00(10.00)\n",
      "Iter 1210 | Time 19.7835(20.6118) | Bit/dim 1.4341(1.5680) | Xent 6.2618(6.7940) | Loss 4.5650(4.9650) | Error 0.3400(0.4964) Steps 542(580.73) | Grad Norm 2.2505(9.0711) | Total Time 10.00(10.00)\n",
      "Iter 1220 | Time 22.7491(20.5914) | Bit/dim 2.5452(1.7137) | Xent 6.7554(6.7692) | Loss 5.9229(5.0983) | Error 0.8433(0.5364) Steps 668(580.06) | Grad Norm 45.1686(21.2011) | Total Time 10.00(10.00)\n",
      "Iter 1230 | Time 24.8072(21.7412) | Bit/dim 1.9030(1.8146) | Xent 6.5406(6.7015) | Loss 5.1733(5.1654) | Error 0.8800(0.6110) Steps 716(613.78) | Grad Norm 17.7410(22.6237) | Total Time 10.00(10.00)\n",
      "Iter 1240 | Time 22.3928(22.3317) | Bit/dim 1.6582(1.7921) | Xent 6.3751(6.6251) | Loss 4.8457(5.1047) | Error 0.7700(0.6485) Steps 656(630.39) | Grad Norm 9.3193(19.4955) | Total Time 10.00(10.00)\n",
      "Iter 1250 | Time 21.8269(22.4553) | Bit/dim 1.5355(1.7344) | Xent 6.1904(6.5330) | Loss 4.6307(5.0010) | Error 0.4578(0.6318) Steps 632(633.38) | Grad Norm 3.6094(16.0296) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0019 | Time 138.4360, Epoch Time 1598.6624(1218.7297), Bit/dim 1.5094, Xent 6.0521, Loss 4.5355, Error 0.2701\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1260 | Time 21.0455(22.2795) | Bit/dim 1.4909(1.6719) | Xent 5.9482(6.4098) | Loss 4.4650(4.8768) | Error 0.3633(0.5757) Steps 596(629.58) | Grad Norm 3.1022(12.9264) | Total Time 10.00(10.00)\n",
      "Iter 1270 | Time 21.2839(21.9709) | Bit/dim 1.4367(1.6158) | Xent 5.7177(6.2548) | Loss 4.2956(4.7432) | Error 0.3633(0.5223) Steps 596(621.20) | Grad Norm 2.2216(10.2222) | Total Time 10.00(10.00)\n",
      "Iter 1280 | Time 22.7762(21.7954) | Bit/dim 1.4471(1.5709) | Xent 5.4101(6.0663) | Loss 4.1522(4.6041) | Error 0.3733(0.4809) Steps 602(615.19) | Grad Norm 1.9133(8.0186) | Total Time 10.00(10.00)\n",
      "Iter 1290 | Time 21.3199(21.6055) | Bit/dim 1.4404(1.5355) | Xent 5.0855(5.8457) | Loss 3.9831(4.4584) | Error 0.4067(0.4576) Steps 584(608.29) | Grad Norm 2.8588(6.3764) | Total Time 10.00(10.00)\n",
      "Iter 1300 | Time 30.2947(22.1914) | Bit/dim 3.8924(2.1005) | Xent 6.5663(6.0732) | Loss 7.1756(5.1371) | Error 0.8611(0.5357) Steps 824(622.40) | Grad Norm 78.0590(33.5742) | Total Time 10.00(10.00)\n",
      "Iter 1310 | Time 27.9800(23.9056) | Bit/dim 2.1209(2.2388) | Xent 6.1064(6.0812) | Loss 5.1741(5.2794) | Error 0.8189(0.6146) Steps 800(672.77) | Grad Norm 15.0499(32.6853) | Total Time 10.00(10.00)\n",
      "Iter 1320 | Time 25.4280(24.4347) | Bit/dim 1.7654(2.1321) | Xent 6.0699(6.0933) | Loss 4.8003(5.1788) | Error 0.8267(0.6538) Steps 704(688.31) | Grad Norm 7.0810(27.0627) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0020 | Time 154.0025, Epoch Time 1716.6936(1233.6686), Bit/dim 1.7555, Xent 6.1029, Loss 4.8070, Error 0.8508\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1330 | Time 24.4208(24.3318) | Bit/dim 1.5973(2.0020) | Xent 6.0801(6.1084) | Loss 4.6374(5.0561) | Error 0.5778(0.6596) Steps 680(686.99) | Grad Norm 4.6845(21.7745) | Total Time 10.00(10.00)\n",
      "Iter 1340 | Time 22.7160(23.9241) | Bit/dim 1.5359(1.8831) | Xent 5.9613(6.0875) | Loss 4.5166(4.9269) | Error 0.4533(0.6116) Steps 620(674.23) | Grad Norm 5.1192(17.3224) | Total Time 10.00(10.00)\n",
      "Iter 1350 | Time 21.6936(23.3924) | Bit/dim 1.4836(1.7839) | Xent 5.8307(6.0347) | Loss 4.3990(4.8013) | Error 0.3789(0.5550) Steps 596(657.79) | Grad Norm 3.2278(13.6866) | Total Time 10.00(10.00)\n",
      "Iter 1360 | Time 21.1407(22.9097) | Bit/dim 1.4636(1.7024) | Xent 5.6448(5.9510) | Loss 4.2860(4.6779) | Error 0.3344(0.5055) Steps 602(643.40) | Grad Norm 1.5903(10.6680) | Total Time 10.00(10.00)\n",
      "Iter 1370 | Time 21.7467(22.4773) | Bit/dim 1.4417(1.6374) | Xent 5.4599(5.8409) | Loss 4.1716(4.5579) | Error 0.3656(0.4645) Steps 590(631.18) | Grad Norm 2.1765(8.3025) | Total Time 10.00(10.00)\n",
      "Iter 1380 | Time 20.8652(21.9983) | Bit/dim 1.4587(1.5879) | Xent 5.2436(5.7078) | Loss 4.0805(4.4418) | Error 0.4300(0.4438) Steps 578(617.49) | Grad Norm 2.5139(6.6382) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0021 | Time 135.8379, Epoch Time 1598.9494(1244.6270), Bit/dim 1.8894, Xent 5.6311, Loss 4.7050, Error 0.5301\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1390 | Time 21.6465(21.8105) | Bit/dim 1.8441(1.6417) | Xent 5.5658(5.6442) | Loss 4.6269(4.4638) | Error 0.8067(0.4774) Steps 614(611.88) | Grad Norm 55.0100(19.6039) | Total Time 10.00(10.00)\n",
      "Iter 1400 | Time 24.2584(22.3201) | Bit/dim 1.7611(1.6958) | Xent 5.1932(5.5815) | Loss 4.3577(4.4865) | Error 0.7878(0.5498) Steps 674(624.95) | Grad Norm 17.4335(23.0994) | Total Time 10.00(10.00)\n",
      "Iter 1410 | Time 23.6954(22.6259) | Bit/dim 1.5725(1.6747) | Xent 5.1560(5.4957) | Loss 4.1505(4.4225) | Error 0.6311(0.5850) Steps 656(634.08) | Grad Norm 9.0438(20.7961) | Total Time 10.00(10.00)\n",
      "Iter 1420 | Time 23.8926(22.7096) | Bit/dim 1.4978(1.6323) | Xent 5.0663(5.3999) | Loss 4.0310(4.3322) | Error 0.6067(0.5945) Steps 650(636.63) | Grad Norm 9.6634(17.8124) | Total Time 10.00(10.00)\n",
      "Iter 1430 | Time 22.1288(22.7796) | Bit/dim 1.4631(1.5935) | Xent 4.8441(5.2775) | Loss 3.8851(4.2322) | Error 0.5389(0.5849) Steps 650(640.82) | Grad Norm 6.9451(15.2040) | Total Time 10.00(10.00)\n",
      "Iter 1440 | Time 23.1916(22.8044) | Bit/dim 1.4720(1.5611) | Xent 4.6151(5.1302) | Loss 3.7796(4.1262) | Error 0.6278(0.5873) Steps 638(642.27) | Grad Norm 5.0549(12.8290) | Total Time 10.00(10.00)\n",
      "Iter 1450 | Time 21.9801(22.6684) | Bit/dim 1.4537(1.5351) | Xent 4.3829(4.9613) | Loss 3.6452(4.0157) | Error 0.6600(0.6018) Steps 632(639.08) | Grad Norm 3.7915(10.6539) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0022 | Time 139.0259, Epoch Time 1672.0161(1257.4487), Bit/dim 1.4466, Xent 4.3089, Loss 3.6011, Error 0.2827\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1460 | Time 22.3692(22.5299) | Bit/dim 1.4361(1.5129) | Xent 4.1430(4.7696) | Loss 3.5076(3.8977) | Error 0.7200(0.6261) Steps 626(635.78) | Grad Norm 7.7666(9.0047) | Total Time 10.00(10.00)\n",
      "Iter 1470 | Time 27.8749(22.7545) | Bit/dim 4.2922(2.0392) | Xent 6.2476(5.0502) | Loss 7.4160(4.5643) | Error 0.9033(0.6795) Steps 764(641.07) | Grad Norm 123.4523(58.2991) | Total Time 10.00(10.00)\n",
      "Iter 1480 | Time 33.6475(24.8181) | Bit/dim 2.5918(2.3215) | Xent 5.1762(5.1354) | Loss 5.1799(4.8892) | Error 0.8811(0.7339) Steps 920(695.98) | Grad Norm 23.7153(56.6496) | Total Time 10.00(10.00)\n",
      "Iter 1490 | Time 30.2821(26.3668) | Bit/dim 1.8501(2.2435) | Xent 5.4562(5.2097) | Loss 4.5782(4.8484) | Error 0.7256(0.7546) Steps 818(735.08) | Grad Norm 11.0035(46.1691) | Total Time 10.00(10.00)\n",
      "Iter 1500 | Time 29.8817(27.3431) | Bit/dim 1.6571(2.1196) | Xent 5.4202(5.2597) | Loss 4.3672(4.7494) | Error 0.6167(0.7324) Steps 860(763.02) | Grad Norm 7.3990(36.4398) | Total Time 10.00(10.00)\n",
      "Iter 1510 | Time 30.6056(28.0511) | Bit/dim 1.6307(1.9965) | Xent 5.2586(5.2803) | Loss 4.2599(4.6367) | Error 0.4344(0.6727) Steps 830(783.63) | Grad Norm 7.0429(28.4494) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0023 | Time 164.5648, Epoch Time 2033.8954(1280.7421), Bit/dim 1.5572, Xent 5.1824, Loss 4.1484, Error 0.2184\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1520 | Time 28.2924(28.2286) | Bit/dim 1.5698(1.8890) | Xent 5.1298(5.2608) | Loss 4.1347(4.5195) | Error 0.3911(0.6030) Steps 800(790.58) | Grad Norm 2.6065(21.9582) | Total Time 10.00(10.00)\n",
      "Iter 1530 | Time 27.7701(28.1838) | Bit/dim 1.5413(1.8008) | Xent 4.9751(5.2043) | Loss 4.0289(4.4029) | Error 0.3767(0.5481) Steps 794(792.15) | Grad Norm 1.8196(16.8193) | Total Time 10.00(10.00)\n",
      "Iter 1540 | Time 28.2609(28.0931) | Bit/dim 1.5341(1.7293) | Xent 4.8031(5.1201) | Loss 3.9356(4.2894) | Error 0.3811(0.5072) Steps 764(786.93) | Grad Norm 2.0799(12.9280) | Total Time 10.00(10.00)\n",
      "Iter 1550 | Time 26.0982(27.7473) | Bit/dim 1.4946(1.6698) | Xent 4.6226(5.0114) | Loss 3.8059(4.1755) | Error 0.4556(0.4824) Steps 710(774.64) | Grad Norm 1.7833(10.0032) | Total Time 10.00(10.00)\n",
      "Iter 1560 | Time 25.4124(27.2121) | Bit/dim 1.4833(1.6233) | Xent 4.4513(4.8840) | Loss 3.7090(4.0653) | Error 0.5100(0.4784) Steps 716(758.06) | Grad Norm 1.1725(7.7441) | Total Time 10.00(10.00)\n",
      "Iter 1570 | Time 24.6783(26.8313) | Bit/dim 1.4649(1.5857) | Xent 4.2443(4.7387) | Loss 3.5871(3.9550) | Error 0.5467(0.4941) Steps 710(746.45) | Grad Norm 0.9649(6.0227) | Total Time 10.00(10.00)\n",
      "Iter 1580 | Time 25.6097(26.6565) | Bit/dim 1.4669(1.5553) | Xent 4.0400(4.5782) | Loss 3.4869(3.8444) | Error 0.5522(0.5119) Steps 722(738.25) | Grad Norm 1.4106(4.8231) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0024 | Time 144.0859, Epoch Time 1926.6340(1300.1188), Bit/dim 1.4602, Xent 3.9321, Loss 3.4263, Error 0.2569\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1590 | Time 25.0394(26.4329) | Bit/dim 1.4557(1.5315) | Xent 3.8248(4.4043) | Loss 3.3681(3.7336) | Error 0.6200(0.5393) Steps 704(731.10) | Grad Norm 2.3614(4.0691) | Total Time 10.00(10.00)\n",
      "Iter 1600 | Time 24.7438(26.1808) | Bit/dim 1.4571(1.5128) | Xent 3.5912(4.2171) | Loss 3.2527(3.6214) | Error 0.6944(0.5705) Steps 686(721.81) | Grad Norm 3.0978(3.6211) | Total Time 10.00(10.00)\n",
      "Iter 1610 | Time 25.5306(25.9493) | Bit/dim 1.7770(1.5237) | Xent 3.7178(4.0462) | Loss 3.6359(3.5468) | Error 0.7189(0.6106) Steps 686(714.90) | Grad Norm 124.8388(16.3914) | Total Time 10.00(10.00)\n",
      "Iter 1620 | Time 27.5647(26.5252) | Bit/dim 2.7985(2.1114) | Xent 4.6414(4.3780) | Loss 5.1192(4.3004) | Error 0.8478(0.6789) Steps 788(731.70) | Grad Norm 75.1745(61.5670) | Total Time 10.00(10.00)\n",
      "Iter 1630 | Time 27.3060(27.0930) | Bit/dim 1.9242(2.1375) | Xent 4.4230(4.3910) | Loss 4.1357(4.3330) | Error 0.7878(0.7185) Steps 740(745.62) | Grad Norm 18.1578(56.4667) | Total Time 10.00(10.00)\n",
      "Iter 1640 | Time 28.8840(27.0720) | Bit/dim 1.6169(2.0346) | Xent 4.6105(4.4326) | Loss 3.9221(4.2509) | Error 0.7500(0.7359) Steps 734(745.26) | Grad Norm 15.1954(46.9069) | Total Time 10.00(10.00)\n",
      "Iter 1650 | Time 27.8219(27.2536) | Bit/dim 1.5946(1.9214) | Xent 4.3796(4.4436) | Loss 3.7844(4.1432) | Error 0.5544(0.7115) Steps 752(747.54) | Grad Norm 12.2025(37.5135) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0025 | Time 152.7997, Epoch Time 1947.3170(1319.5348), Bit/dim 1.5629, Xent 4.3944, Loss 3.7601, Error 0.2303\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1660 | Time 26.6545(27.2498) | Bit/dim 1.4818(1.8153) | Xent 4.3499(4.4233) | Loss 3.6568(4.0270) | Error 0.4989(0.6642) Steps 734(746.70) | Grad Norm 8.7897(29.8597) | Total Time 10.00(10.00)\n",
      "Iter 1670 | Time 27.2442(27.1119) | Bit/dim 1.5448(1.7407) | Xent 4.2256(4.3737) | Loss 3.6576(3.9275) | Error 0.6522(0.6407) Steps 716(742.60) | Grad Norm 72.2199(32.6459) | Total Time 10.00(10.00)\n",
      "Iter 1680 | Time 28.9614(27.2793) | Bit/dim 1.5906(1.7006) | Xent 3.9066(4.3043) | Loss 3.5439(3.8527) | Error 0.6900(0.6491) Steps 800(751.51) | Grad Norm 13.5167(32.9582) | Total Time 10.00(10.00)\n",
      "Iter 1690 | Time 28.1488(27.5426) | Bit/dim 1.5539(1.6536) | Xent 3.7605(4.2048) | Loss 3.4341(3.7560) | Error 0.6533(0.6536) Steps 764(758.31) | Grad Norm 13.4458(28.4663) | Total Time 10.00(10.00)\n",
      "Iter 1700 | Time 27.9991(27.6303) | Bit/dim 1.4800(1.6090) | Xent 3.6626(4.0848) | Loss 3.3113(3.6514) | Error 0.6433(0.6475) Steps 764(761.44) | Grad Norm 8.9632(23.5809) | Total Time 10.00(10.00)\n",
      "Iter 1710 | Time 26.2429(27.4160) | Bit/dim 1.4554(1.5688) | Xent 3.4633(3.9453) | Loss 3.1870(3.5414) | Error 0.6022(0.6387) Steps 758(757.79) | Grad Norm 7.0174(19.2678) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0026 | Time 152.1376, Epoch Time 1977.8131(1339.2831), Bit/dim 1.4463, Xent 3.3135, Loss 3.1031, Error 0.2332\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1720 | Time 26.6780(27.2780) | Bit/dim 1.4627(1.5389) | Xent 3.2431(3.7868) | Loss 3.0843(3.4323) | Error 0.6744(0.6404) Steps 752(756.43) | Grad Norm 5.2012(15.8210) | Total Time 10.00(10.00)\n",
      "Iter 1730 | Time 26.9374(27.1964) | Bit/dim 3.0571(1.6891) | Xent 4.5021(3.7791) | Loss 5.3081(3.5786) | Error 0.8822(0.6717) Steps 782(752.65) | Grad Norm 228.6426(43.3898) | Total Time 10.00(10.00)\n",
      "Iter 1740 | Time 27.8428(27.2618) | Bit/dim 1.8682(1.8140) | Xent 3.3922(3.7789) | Loss 3.5643(3.7034) | Error 0.8144(0.7153) Steps 746(753.07) | Grad Norm 47.1982(64.7235) | Total Time 10.00(10.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_generative.py --data mnist --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments/cnf_conditional_generative_bs900_sratio_1_0_drop_0_5_seed_1 --seed 1 --conditional True --controlled_tol False --train_mode semisup --log_freq 10 --weight_y 0.5 --condition_ratio 1.0 --dropout_rate 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
