{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='4,5,6,7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_rl_2cond.py\n",
      "from __future__ import print_function\n",
      "\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import torch.utils.data as data\n",
      "from torch.utils.data import Dataset\n",
      "\n",
      "from PIL import Image\n",
      "import os.path\n",
      "import errno\n",
      "import codecs\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time, count_nfe_gate\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "GATES = [\"cnn1\", \"cnn2\", \"rnn\"]\n",
      "\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"colormnist\", \"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--gate', type=str, default='cnn1', choices=GATES)\n",
      "parser.add_argument('--scale', type=float, default=1.0)\n",
      "parser.add_argument('--scale_fac', type=float, default=1.0)\n",
      "parser.add_argument('--scale_std', type=float, default=1.0)\n",
      "parser.add_argument('--eta', default=0.1, type=float,\n",
      "                        help='tuning parameter that allows us to trade-off the competing goals of' \n",
      "                                'minimizing the prediction loss and maximizing the gate rewards ')\n",
      "parser.add_argument('--rl-weight', default=0.01, type=float,\n",
      "                        help='rl weight')\n",
      "\n",
      "parser.add_argument('--gamma', default=0.99, type=float,\n",
      "                        help='discount factor, default: (0.99)')\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=500)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "parser.add_argument(\"--annealing_std\", type=eval, default=False, choices=[True, False])\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "parser.add_argument(\"--cond_nn\", choices=[\"linear\", \"mlp\"], type=str, default=\"linear\")\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "import lib.odenvp_conditional_rl_2cond as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "class ColorMNIST(data.Dataset):\n",
      "    \"\"\"`MNIST <http://yann.lecun.com/exdb/mnist/>`_ Dataset.\n",
      "\n",
      "    Args:\n",
      "        root (string): Root directory of dataset where ``processed/training.pt``\n",
      "            and  ``processed/test.pt`` exist.\n",
      "        train (bool, optional): If True, creates dataset from ``training.pt``,\n",
      "            otherwise from ``test.pt``.\n",
      "        download (bool, optional): If true, downloads the dataset from the internet and\n",
      "            puts it in root directory. If dataset is already downloaded, it is not\n",
      "            downloaded again.\n",
      "        transform (callable, optional): A function/transform that  takes in an PIL image\n",
      "            and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
      "        target_transform (callable, optional): A function/transform that takes in the\n",
      "            target and transforms it.\n",
      "    \"\"\"\n",
      "    urls = [\n",
      "        'http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz',\n",
      "    ]\n",
      "    raw_folder = 'raw'\n",
      "    processed_folder = 'processed'\n",
      "    training_file = 'training.pt'\n",
      "    test_file = 'test.pt'\n",
      "\n",
      "    def __init__(self, root, train=True, transform=None, target_transform=None, download=False):\n",
      "        self.root = os.path.expanduser(root)\n",
      "        self.transform = transform\n",
      "        self.target_transform = target_transform\n",
      "        self.train = train  # training set or test set\n",
      "\n",
      "        if download:\n",
      "            self.download()\n",
      "\n",
      "        if not self._check_exists():\n",
      "            raise RuntimeError('Dataset not found.' +\n",
      "                               ' You can use download=True to download it')\n",
      "\n",
      "        if self.train:\n",
      "            self.train_data, self.train_labels = torch.load(\n",
      "                os.path.join(self.root, self.processed_folder, self.training_file))\n",
      "            \n",
      "            self.train_data = np.tile(self.train_data[:, :, :, np.newaxis], 3)\n",
      "        else:\n",
      "            self.test_data, self.test_labels = torch.load(\n",
      "                os.path.join(self.root, self.processed_folder, self.test_file))\n",
      "            \n",
      "            self.test_data = np.tile(self.test_data[:, :, :, np.newaxis], 3)\n",
      "        \n",
      "        self.pallette = [[31, 119, 180],\n",
      "                         [255, 127, 14],\n",
      "                         [44, 160, 44],\n",
      "                         [214, 39, 40],\n",
      "                         [148, 103, 189],\n",
      "                         [140, 86, 75],\n",
      "                         [227, 119, 194],\n",
      "                         [127, 127, 127],\n",
      "                         [188, 189, 34],\n",
      "                         [23, 190, 207]]\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        \"\"\"\n",
      "        Args:\n",
      "            index (int): Index\n",
      "\n",
      "        Returns:\n",
      "            tuple: (image, target) where target is index of the target class.\n",
      "        \"\"\"\n",
      "        if self.train:\n",
      "            img, target = self.train_data[index], self.train_labels[index]\n",
      "        else:\n",
      "            img, target = self.test_data[index], self.test_labels[index]\n",
      "        \n",
      "        # doing this so that it is consistent with all other datasets\n",
      "        # to return a PIL Image\n",
      "        y_color_digit = np.random.randint(0, 10)\n",
      "        c_digit = self.pallette[y_color_digit]\n",
      "        \n",
      "        img[:, :, 0] = img[:, :, 0] / 255 * c_digit[0]\n",
      "        img[:, :, 1] = img[:, :, 1] / 255 * c_digit[1]\n",
      "        img[:, :, 2] = img[:, :, 2] / 255 * c_digit[2]\n",
      "        \n",
      "        img = Image.fromarray(img)\n",
      "\n",
      "        if self.transform is not None:\n",
      "            img = self.transform(img)\n",
      "\n",
      "        if self.target_transform is not None:\n",
      "            target = self.target_transform(target)\n",
      "\n",
      "        return img, [target,torch.from_numpy(np.array(y_color_digit))]\n",
      "\n",
      "    def __len__(self):\n",
      "        if self.train:\n",
      "            return len(self.train_data)\n",
      "        else:\n",
      "            return len(self.test_data)\n",
      "\n",
      "    def _check_exists(self):\n",
      "        return os.path.exists(os.path.join(self.root, self.processed_folder, self.training_file)) and \\\n",
      "            os.path.exists(os.path.join(self.root, self.processed_folder, self.test_file))\n",
      "\n",
      "    def download(self):\n",
      "        \"\"\"Download the MNIST data if it doesn't exist in processed_folder already.\"\"\"\n",
      "        from six.moves import urllib\n",
      "        import gzip\n",
      "\n",
      "        if self._check_exists():\n",
      "            return\n",
      "\n",
      "        # download files\n",
      "        try:\n",
      "            os.makedirs(os.path.join(self.root, self.raw_folder))\n",
      "            os.makedirs(os.path.join(self.root, self.processed_folder))\n",
      "        except OSError as e:\n",
      "            if e.errno == errno.EEXIST:\n",
      "                pass\n",
      "            else:\n",
      "                raise\n",
      "\n",
      "        for url in self.urls:\n",
      "            print('Downloading ' + url)\n",
      "            data = urllib.request.urlopen(url)\n",
      "            filename = url.rpartition('/')[2]\n",
      "            file_path = os.path.join(self.root, self.raw_folder, filename)\n",
      "            with open(file_path, 'wb') as f:\n",
      "                f.write(data.read())\n",
      "            with open(file_path.replace('.gz', ''), 'wb') as out_f, \\\n",
      "                    gzip.GzipFile(file_path) as zip_f:\n",
      "                out_f.write(zip_f.read())\n",
      "            os.unlink(file_path)\n",
      "\n",
      "        # process and save as torch files\n",
      "        print('Processing...')\n",
      "\n",
      "        training_set = (\n",
      "            read_image_file(os.path.join(self.root, self.raw_folder, 'train-images-idx3-ubyte')),\n",
      "            read_label_file(os.path.join(self.root, self.raw_folder, 'train-labels-idx1-ubyte'))\n",
      "        )\n",
      "        test_set = (\n",
      "            read_image_file(os.path.join(self.root, self.raw_folder, 't10k-images-idx3-ubyte')),\n",
      "            read_label_file(os.path.join(self.root, self.raw_folder, 't10k-labels-idx1-ubyte'))\n",
      "        )\n",
      "        with open(os.path.join(self.root, self.processed_folder, self.training_file), 'wb') as f:\n",
      "            torch.save(training_set, f)\n",
      "        with open(os.path.join(self.root, self.processed_folder, self.test_file), 'wb') as f:\n",
      "            torch.save(test_set, f)\n",
      "\n",
      "        print('Done!')\n",
      "\n",
      "    def __repr__(self):\n",
      "        fmt_str = 'Dataset ' + self.__class__.__name__ + '\\n'\n",
      "        fmt_str += '    Number of datapoints: {}\\n'.format(self.__len__())\n",
      "        tmp = 'train' if self.train is True else 'test'\n",
      "        fmt_str += '    Split: {}\\n'.format(tmp)\n",
      "        fmt_str += '    Root Location: {}\\n'.format(self.root)\n",
      "        tmp = '    Transforms (if any): '\n",
      "        fmt_str += '{0}{1}\\n'.format(tmp, self.transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
      "        tmp = '    Target Transforms (if any): '\n",
      "        fmt_str += '{0}{1}'.format(tmp, self.target_transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
      "        return fmt_str\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "        \n",
      "def update_scale_std(model, epoch):\n",
      "    epoch_frac = 1.0 - float(epoch - 1) / max(args.num_epochs + 1, 1)\n",
      "    scale_std = args.scale_std * epoch_frac\n",
      "    model.set_scale_std(scale_std)\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    if args.data == \"colormnist\":\n",
      "        im_dim = 3\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = ColorMNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = ColorMNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, y_color, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "    y_onehot_color = thops.onehot(y_color, num_classes=model.module.y_color).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "    mean_color, logs_color = model.module._prior_color(y_onehot_color)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_color_sup = modules.GaussianDiag.logp(mean_color, logs_color, z[:, dim_sup:(2*dim_sup)]).view(-1,1)  # logp(z)_color_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, (2*dim_sup):]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_color_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "        zcolorsup = model.module.dropout_color(z[:, dim_sup:(2*dim_sup)])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "        zcolorsup = z[:, dim_sup:(2*dim_sup)]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "    \n",
      "    y_logits_color = model.module.project_color(zcolorsup)\n",
      "    loss_xent_color = model.module.loss_class(y_logits_color, y_color.to(x.get_device()))\n",
      "    y_color_predicted = np.argmax(y_logits_color.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, loss_xent_color, y_predicted, y_color_predicted, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol, \"scale\": args.scale, \"scale_fac\": args.scale_fac, \"scale_std\": args.scale_std, \"gate\": args.gate},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,\n",
      "            cond_nn=args.cond_nn)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    xent_color_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    error_color_meter = utils.RunningAverageMeter(0.97)\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        xent_color_meter.set(checkpt['xent_train_color'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        error_color_meter.set(checkpt['error_train_color'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        \n",
      "        fixed_y_color = torch.from_numpy(np.arange(model.module.y_color)).repeat(model.module.y_color).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot_color = thops.onehot(fixed_y_color, num_classes=model.module.y_color)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            mean_color, logs_color = model.module._prior_color(fixed_y_onehot_color)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_color_sup = modules.GaussianDiag.sample(mean_color, logs_color)\n",
      "            dim_unsup = np.prod(data_shape) - np.prod(fixed_z_sup.shape[1:]) - np.prod(fixed_z_color_sup.shape[1:])\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_color_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    best_error_score_color = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        if args.annealing_std:\n",
      "            update_scale_std(model.module, epoch)\n",
      "            \n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y_all) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            \n",
      "            y = y_all[0]\n",
      "            y_color = y_all[1]\n",
      "            \n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, loss_xent_color, y_predicted, y_color_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, y_color, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * 0.5 * (loss_xent + loss_xent_color)\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  0.5 * (loss_xent + loss_xent_color)\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy()) \n",
      "                error_score_color = 1. - np.mean(y_color_predicted.astype(int) == y_color.numpy())\n",
      "                \n",
      "            else:\n",
      "                loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, loss_xent_color, error_score, error_score_color = loss, 0., 0., 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            # re-weight the gate rewards\n",
      "            normalized_eta = args.eta / len(logp_actions)\n",
      "            \n",
      "            # collect cumulative future rewards\n",
      "            R = - loss\n",
      "            cum_rewards = []\n",
      "            for r in nfe[::-1]:\n",
      "                R = -normalized_eta * r.view(-1,1) + args.gamma * R\n",
      "                cum_rewards.insert(0,R)\n",
      "            \n",
      "            # apply REINFORCE\n",
      "            rl_loss = 0\n",
      "            for lpa, r in zip(logp_actions, cum_rewards):\n",
      "                rl_loss = rl_loss - lpa.view(-1,1) * args.rl_weight * r\n",
      "                \n",
      "            loss = loss + rl_loss.mean()\n",
      "            \n",
      "            loss.backward()\n",
      "            \n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "                xent_color_meter.update(loss_xent_color.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "                xent_color_meter.update(loss_xent_color)\n",
      "            error_meter.update(error_score)\n",
      "            error_color_meter.update(error_score_color)\n",
      "            steps_meter.update(count_nfe_gate(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            for idx in range(len(model.module.transforms)):\n",
      "                for layer in model.module.transforms[idx].chain:\n",
      "                    if hasattr(layer, 'atol'):\n",
      "                        layer.odefunc.after_odeint()\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('xent_color', {'train_iter': xent_color_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('error_color', {'train_iter': error_color_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'train': atol[tol_indx].mean()}, itr)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'train': rtol[tol_indx].mean()}, itr)\n",
      "                    \n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Xent Color {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) | Error Color {:.4f}({:.4f}) |\"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, xent_color_meter.val, xent_color_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, error_color_meter.val, error_color_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent_color', {'train_epoch': xent_color_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('error_color', {'train_epoch': error_color_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses_xent_color = []; losses = []\n",
      "                total_correct = 0\n",
      "                total_correct_color = 0\n",
      "                \n",
      "                for (x, y_all) in test_loader:\n",
      "                    y = y_all[0]\n",
      "                    y_color = y_all[1]\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, loss_xent_color, y_predicted, y_color_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, y_color, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * 0.5 * (loss_xent + loss_xent_color)\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  0.5 * (loss_xent + loss_xent_color)\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                        total_correct_color += np.sum(y_color_predicted.astype(int) == y_color.numpy())\n",
      "                    else:\n",
      "                        loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent, loss_xent_color = loss, 0., 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                        losses_xent_color.append(loss_xent_color.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                        losses_xent_color.append(loss_xent_color)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss_xent_color = np.mean(losses_xent_color); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                error_score_color =  1. - total_correct_color / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('xent_color', {'validation': loss_xent_color}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                writer.add_scalars('error_color', {'validation': error_score_color}, epoch)\n",
      "                \n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Xent Color {:.4f}. Loss {:.4f}, Error {:.4f}(best: {:.4f}), Error Color {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss_xent_color, loss, error_score, best_error_score, error_score_color, best_error_score_color)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"error_color\": error_score_color,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"xent_color\": loss_xent_color,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"best_error_score_color\": best_error_score_color,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"error_train_color\": error_color_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"xent_train_color\": xent_color_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"error_color\": error_score_color,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"xent_color\": loss_xent_color,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"best_error_score_color\": best_error_score_color,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"error_train_color\": error_color_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"xent_train_color\": xent_color_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"error_color\": error_score_color,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"xent_color\": loss_xent_color,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"best_error_score_color\": best_error_score_color,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"error_train_color\": error_color_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"xent_train_color\": xent_color_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"error_color\": error_score_color,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"xent_color\": loss_xent_color,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"best_error_score_color\": best_error_score_color,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"error_train_color\": error_color_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"xent_train_color\": xent_color_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "                    if error_score_color < best_error_score_color:\n",
      "                        best_error_score_color = error_score_color\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"error_color\": error_score_color,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"xent_color\": loss_xent_color,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"best_error_score_color\": best_error_score_color,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"error_train_color\": error_color_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"xent_train_color\": xent_color_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_color_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples, atol, rtol, logp_actions, nfe = model(fixed_z, reverse=True)\n",
      "            generated_samples = generated_samples.view(-1, *data_shape)\n",
      "            for tol_indx in range(len(atol)):\n",
      "                writer.add_scalars('atol_gen_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                writer.add_scalars('rtol_gen_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, annealing_std=False, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, cond_nn='mlp', condition_ratio=0.083333, conditional=True, controlled_tol=False, conv=True, data='colormnist', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, eta=0.1, gamma=0.99, gate='cnn1', imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=500, parallel=False, rademacher=True, residual=False, resume=None, rl_weight=0.01, rtol=1e-05, save='../experiments_published/cnf_conditional_disentangle_colormnist_bs900_sratio_1_12th_drop_0_5_rl_stdscale_6_2cond_mlp_run1', scale=1.0, scale_fac=1.0, scale_std=6.0, seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5)\n",
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): MultiLinearZeros(\n",
      "    (fc1): Linear(in_features=10, out_features=195, bias=True)\n",
      "    (relu1): ReLU()\n",
      "    (fc2): Linear(in_features=195, out_features=390, bias=True)\n",
      "  )\n",
      "  (project_ycond_color): MultiLinearZeros(\n",
      "    (fc1): Linear(in_features=10, out_features=195, bias=True)\n",
      "    (relu1): ReLU()\n",
      "    (fc2): Linear(in_features=195, out_features=390, bias=True)\n",
      "  )\n",
      "  (project_class): MultiLinearZeros(\n",
      "    (fc1): Linear(in_features=195, out_features=97, bias=True)\n",
      "    (relu1): ReLU()\n",
      "    (fc2): Linear(in_features=97, out_features=10, bias=True)\n",
      "  )\n",
      "  (project_color): MultiLinearZeros(\n",
      "    (fc1): Linear(in_features=195, out_features=97, bias=True)\n",
      "    (relu1): ReLU()\n",
      "    (fc2): Linear(in_features=97, out_features=10, bias=True)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.5)\n",
      "  (dropout_color): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1107462\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc_rl.py:291: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Iter 0010 | Time 7.1043(24.6718) | Bit/dim 25.4643(27.3861) | Xent 2.3026(2.3026) | Xent Color 2.3026(2.3026) | Loss 48.8743(52.2691) | Error 0.8700(0.8955) | Error Color 0.9067(0.8916) |Steps 302(311.17) | Grad Norm 262.2161(277.4435) | Total Time 0.00(0.00)\n",
      "Iter 0020 | Time 7.8198(20.1486) | Bit/dim 20.1556(26.1190) | Xent 2.3026(2.3026) | Xent Color 2.3026(2.3026) | Loss 39.0214(49.9625) | Error 0.8856(0.8937) | Error Color 0.9111(0.8933) |Steps 290(308.91) | Grad Norm 215.8387(266.7707) | Total Time 0.00(0.00)\n",
      "Iter 0030 | Time 8.1758(16.8761) | Bit/dim 13.8830(23.6000) | Xent 2.3026(2.3026) | Xent Color 2.3026(2.3026) | Loss 27.5036(45.3938) | Error 0.8967(0.8931) | Error Color 0.8978(0.8931) |Steps 338(310.64) | Grad Norm 157.4838(245.1488) | Total Time 0.00(0.00)\n",
      "Iter 0040 | Time 8.6904(14.6194) | Bit/dim 9.0006(20.2609) | Xent 2.3026(2.3026) | Xent Color 2.3026(2.3026) | Loss 19.1696(39.3634) | Error 0.8778(0.8923) | Error Color 0.9078(0.8958) |Steps 362(319.50) | Grad Norm 96.0639(213.0633) | Total Time 0.00(0.00)\n",
      "Iter 0050 | Time 9.5125(13.1843) | Bit/dim 6.7504(16.9036) | Xent 2.3026(2.3026) | Xent Color 2.3026(2.3026) | Loss 15.0755(33.3240) | Error 0.8900(0.8910) | Error Color 0.8722(0.8959) |Steps 404(335.06) | Grad Norm 33.8161(172.5463) | Total Time 0.00(0.00)\n",
      "Iter 0060 | Time 9.1545(12.2102) | Bit/dim 6.1091(14.1291) | Xent 2.3025(2.3026) | Xent Color 2.3026(2.3026) | Loss 13.7437(28.3315) | Error 0.8967(0.8903) | Error Color 0.9111(0.8977) |Steps 374(349.92) | Grad Norm 24.4139(133.2279) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0001 | Time 56.8210, Epoch Time 657.0456(657.0456), Bit/dim 5.7523(best: inf), Xent 2.3025, Xent Color 2.3026. Loss 6.9036, Error 0.8865(best: inf), Error Color 0.9008(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0070 | Time 9.4556(11.4919) | Bit/dim 4.3046(11.7967) | Xent 2.3026(2.3026) | Xent Color 2.3026(2.3026) | Loss 10.6809(24.5808) | Error 0.8911(0.8895) | Error Color 0.8967(0.8974) |Steps 368(357.01) | Grad Norm 16.0928(103.9586) | Total Time 0.00(0.00)\n",
      "Iter 0080 | Time 9.2098(10.9253) | Bit/dim 4.0569(9.7935) | Xent 2.3025(2.3026) | Xent Color 2.3026(2.3026) | Loss 10.2414(20.8609) | Error 0.8933(0.8898) | Error Color 0.9156(0.8989) |Steps 380(365.05) | Grad Norm 12.3252(80.1189) | Total Time 0.00(0.00)\n",
      "Iter 0090 | Time 9.0773(10.4597) | Bit/dim 3.7297(8.2435) | Xent 2.3024(2.3025) | Xent Color 2.3026(2.3026) | Loss 9.5410(17.9881) | Error 0.8789(0.8888) | Error Color 0.9033(0.8999) |Steps 368(369.34) | Grad Norm 9.7745(62.1160) | Total Time 0.00(0.00)\n",
      "Iter 0100 | Time 9.4522(10.1588) | Bit/dim 3.5245(7.0330) | Xent 2.3024(2.3025) | Xent Color 2.3026(2.3026) | Loss 9.2294(15.7390) | Error 0.8678(0.8884) | Error Color 0.9011(0.8995) |Steps 350(370.63) | Grad Norm 7.4980(47.7947) | Total Time 0.00(0.00)\n",
      "Iter 0110 | Time 9.0274(9.9101) | Bit/dim 3.2721(6.0734) | Xent 2.3024(2.3025) | Xent Color 2.3026(2.3026) | Loss 8.7904(13.9620) | Error 0.8733(0.8871) | Error Color 0.8878(0.8982) |Steps 362(373.21) | Grad Norm 6.7762(37.1346) | Total Time 0.00(0.00)\n",
      "Iter 0120 | Time 9.3897(9.7207) | Bit/dim 3.0116(5.2962) | Xent 2.3024(2.3025) | Xent Color 2.3026(2.3026) | Loss 8.3356(12.5209) | Error 0.8856(0.8871) | Error Color 0.9011(0.8992) |Steps 392(376.15) | Grad Norm 6.8400(29.2162) | Total Time 0.00(0.00)\n",
      "Iter 0130 | Time 9.4688(9.6318) | Bit/dim 2.7839(4.6584) | Xent 2.3025(2.3025) | Xent Color 2.3026(2.3026) | Loss 7.8609(11.3551) | Error 0.8933(0.8875) | Error Color 0.8944(0.8994) |Steps 398(378.52) | Grad Norm 5.7007(23.1789) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0002 | Time 56.7287, Epoch Time 693.6002(658.1422), Bit/dim 2.6970(best: 5.7523), Xent 2.3024, Xent Color 2.3026. Loss 3.8483, Error 0.8865(best: 0.8865), Error Color 0.9019(best: 0.9008)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0140 | Time 9.6026(9.5720) | Bit/dim 2.1657(4.0428) | Xent 2.3024(2.3024) | Xent Color 2.3026(2.3026) | Loss 6.9336(10.6692) | Error 0.8878(0.8876) | Error Color 0.9100(0.8994) |Steps 398(381.55) | Grad Norm 4.2503(18.5000) | Total Time 0.00(0.00)\n",
      "Iter 0150 | Time 10.0051(9.5969) | Bit/dim 2.0773(3.5313) | Xent 2.3024(2.3024) | Xent Color 2.3026(2.3026) | Loss 6.7644(9.6286) | Error 0.9033(0.8866) | Error Color 0.8956(0.9006) |Steps 416(385.76) | Grad Norm 4.2179(14.6645) | Total Time 0.00(0.00)\n",
      "Iter 0160 | Time 9.6155(9.6639) | Bit/dim 1.9662(3.1284) | Xent 2.3022(2.3024) | Xent Color 2.3026(2.3026) | Loss 6.4398(8.8194) | Error 0.8800(0.8864) | Error Color 0.8967(0.8993) |Steps 398(389.59) | Grad Norm 3.4424(11.7457) | Total Time 0.00(0.00)\n",
      "Iter 0170 | Time 9.5457(9.6743) | Bit/dim 1.8878(2.8105) | Xent 2.3020(2.3024) | Xent Color 2.3026(2.3026) | Loss 6.3432(8.1854) | Error 0.8789(0.8871) | Error Color 0.9122(0.8994) |Steps 398(391.78) | Grad Norm 2.8169(9.4732) | Total Time 0.00(0.00)\n",
      "Iter 0180 | Time 9.5632(9.6897) | Bit/dim 1.8087(2.5550) | Xent 2.3028(2.3023) | Xent Color 2.3026(2.3026) | Loss 6.1751(7.6820) | Error 0.9111(0.8874) | Error Color 0.9044(0.9007) |Steps 398(393.60) | Grad Norm 2.5673(7.6769) | Total Time 0.00(0.00)\n",
      "Iter 0190 | Time 9.7823(9.7137) | Bit/dim 1.7849(2.3552) | Xent 2.3024(2.3023) | Xent Color 2.3026(2.3026) | Loss 6.1567(7.2834) | Error 0.8989(0.8884) | Error Color 0.8889(0.8990) |Steps 416(397.43) | Grad Norm 1.8696(6.2171) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0003 | Time 58.9360, Epoch Time 726.1618(660.1828), Bit/dim 1.7245(best: 2.6970), Xent 2.3022, Xent Color 2.3026. Loss 2.8757, Error 0.8865(best: 0.8865), Error Color 0.9035(best: 0.9008)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0200 | Time 9.4742(9.7524) | Bit/dim 1.4780(2.1783) | Xent 2.3023(2.3023) | Xent Color 2.3026(2.3026) | Loss 5.6282(7.4686) | Error 0.8856(0.8883) | Error Color 0.8900(0.8985) |Steps 410(399.36) | Grad Norm 1.8857(5.0474) | Total Time 0.00(0.00)\n",
      "Iter 0210 | Time 10.0255(9.8078) | Bit/dim 1.4466(1.9880) | Xent 2.3024(2.3023) | Xent Color 2.3026(2.3026) | Loss 5.5147(6.9646) | Error 0.9044(0.8877) | Error Color 0.8900(0.8965) |Steps 422(400.50) | Grad Norm 1.4851(4.1313) | Total Time 0.00(0.00)\n",
      "Iter 0220 | Time 9.7551(9.7875) | Bit/dim 1.4209(1.8397) | Xent 2.3018(2.3022) | Xent Color 2.3025(2.3026) | Loss 5.4499(6.5820) | Error 0.8900(0.8873) | Error Color 0.8911(0.8970) |Steps 398(401.13) | Grad Norm 0.8847(3.3636) | Total Time 0.00(0.00)\n",
      "Iter 0230 | Time 9.5767(9.8020) | Bit/dim 1.3977(1.7265) | Xent 2.3014(2.3022) | Xent Color 2.3024(2.3026) | Loss 5.4212(6.2882) | Error 0.8700(0.8876) | Error Color 0.8900(0.8977) |Steps 404(402.43) | Grad Norm 1.1384(2.7583) | Total Time 0.00(0.00)\n",
      "Iter 0240 | Time 9.4662(9.7482) | Bit/dim 1.4035(1.6401) | Xent 2.3019(2.3021) | Xent Color 2.3026(2.3026) | Loss 5.4768(6.0599) | Error 0.8767(0.8881) | Error Color 0.9156(0.8985) |Steps 392(401.05) | Grad Norm 1.0260(2.3086) | Total Time 0.00(0.00)\n",
      "Iter 0250 | Time 9.5617(9.7848) | Bit/dim 1.3678(1.5703) | Xent 2.3017(2.3021) | Xent Color 2.3026(2.3026) | Loss 5.3092(5.8895) | Error 0.8756(0.8875) | Error Color 0.9033(0.8989) |Steps 386(401.19) | Grad Norm 0.9475(2.0118) | Total Time 0.00(0.00)\n",
      "Iter 0260 | Time 9.3777(9.7976) | Bit/dim 1.3715(1.5183) | Xent 2.3026(2.3021) | Xent Color 2.3025(2.3026) | Loss 5.4046(5.7626) | Error 0.8944(0.8876) | Error Color 0.8778(0.8985) |Steps 392(401.75) | Grad Norm 1.3239(1.7426) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0004 | Time 57.6909, Epoch Time 729.3041(662.2564), Bit/dim 1.3609(best: 1.7245), Xent 2.3019, Xent Color 2.3026. Loss 2.5121, Error 0.8865(best: 0.8865), Error Color 0.9023(best: 0.9008)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0270 | Time 9.8097(9.7673) | Bit/dim 1.1373(1.4412) | Xent 2.3022(2.3021) | Xent Color 2.3026(2.3026) | Loss 4.9407(6.0712) | Error 0.8844(0.8868) | Error Color 0.9133(0.9007) |Steps 392(401.18) | Grad Norm 0.8971(1.5705) | Total Time 0.00(0.00)\n",
      "Iter 0280 | Time 9.5050(9.6890) | Bit/dim 1.1439(1.3622) | Xent 2.3022(2.3020) | Xent Color 2.3026(2.3026) | Loss 4.9575(5.7737) | Error 0.9011(0.8887) | Error Color 0.9100(0.9006) |Steps 392(398.66) | Grad Norm 1.3013(1.3838) | Total Time 0.00(0.00)\n",
      "Iter 0290 | Time 9.4776(9.6420) | Bit/dim 1.1266(1.3025) | Xent 2.3014(2.3020) | Xent Color 2.3026(2.3026) | Loss 4.8944(5.5592) | Error 0.8800(0.8891) | Error Color 0.8956(0.9006) |Steps 398(394.80) | Grad Norm 0.4843(1.1913) | Total Time 0.00(0.00)\n",
      "Iter 0300 | Time 9.0181(9.5802) | Bit/dim 1.1378(1.2565) | Xent 2.3020(2.3019) | Xent Color 2.3024(2.3026) | Loss 4.9021(5.3938) | Error 0.8944(0.8879) | Error Color 0.8822(0.8988) |Steps 368(392.43) | Grad Norm 0.3544(1.0133) | Total Time 0.00(0.00)\n",
      "Iter 0310 | Time 9.4342(9.5410) | Bit/dim 1.1222(1.2218) | Xent 2.3008(2.3019) | Xent Color 2.3027(2.3026) | Loss 4.9046(5.2686) | Error 0.8822(0.8880) | Error Color 0.9033(0.8998) |Steps 398(390.73) | Grad Norm 0.8128(0.8895) | Total Time 0.00(0.00)\n",
      "Iter 0320 | Time 9.3419(9.5207) | Bit/dim 1.1196(1.1956) | Xent 2.3013(2.3019) | Xent Color 2.3027(2.3026) | Loss 5.0245(5.1889) | Error 0.8789(0.8884) | Error Color 0.9144(0.9002) |Steps 410(392.03) | Grad Norm 0.4142(0.7945) | Total Time 0.00(0.00)\n",
      "Iter 0330 | Time 9.4973(9.5155) | Bit/dim 1.1367(1.1760) | Xent 2.3016(2.3018) | Xent Color 2.3026(2.3026) | Loss 4.9530(5.1206) | Error 0.8856(0.8873) | Error Color 0.9122(0.9006) |Steps 386(391.32) | Grad Norm 0.3034(0.6899) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0005 | Time 58.0712, Epoch Time 709.1818(663.6642), Bit/dim 1.1082(best: 1.3609), Xent 2.3016, Xent Color 2.3025. Loss 2.2593, Error 0.8865(best: 0.8865), Error Color 0.8988(best: 0.9008)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0340 | Time 9.0736(9.4799) | Bit/dim 0.9404(1.1131) | Xent 2.3011(2.3017) | Xent Color 2.3026(2.3026) | Loss 4.5740(5.3989) | Error 0.8856(0.8879) | Error Color 0.9044(0.9017) |Steps 374(391.67) | Grad Norm 0.3677(0.6422) | Total Time 0.00(0.00)\n",
      "Iter 0350 | Time 9.4770(9.4617) | Bit/dim 0.9269(1.0652) | Xent 2.3012(2.3018) | Xent Color 2.3025(2.3026) | Loss 4.5882(5.1843) | Error 0.8822(0.8883) | Error Color 0.8900(0.9006) |Steps 392(391.55) | Grad Norm 0.4449(0.5979) | Total Time 0.00(0.00)\n",
      "Iter 0360 | Time 9.6192(9.4760) | Bit/dim 0.9224(1.0301) | Xent 2.3014(2.3018) | Xent Color 2.3025(2.3026) | Loss 4.5843(5.0353) | Error 0.8878(0.8887) | Error Color 0.8756(0.8995) |Steps 380(391.64) | Grad Norm 0.3140(0.5604) | Total Time 0.00(0.00)\n",
      "Iter 0370 | Time 9.2935(9.4549) | Bit/dim 0.9286(1.0025) | Xent 2.3021(2.3017) | Xent Color 2.3027(2.3026) | Loss 4.5981(4.9197) | Error 0.8933(0.8879) | Error Color 0.8900(0.8989) |Steps 392(391.68) | Grad Norm 0.4617(0.5251) | Total Time 0.00(0.00)\n",
      "Iter 0380 | Time 9.3072(9.4316) | Bit/dim 0.9124(0.9820) | Xent 2.3018(2.3016) | Xent Color 2.3024(2.3026) | Loss 4.5982(4.8318) | Error 0.8867(0.8873) | Error Color 0.9033(0.9001) |Steps 398(393.36) | Grad Norm 0.3944(0.5048) | Total Time 0.00(0.00)\n",
      "Iter 0390 | Time 9.4949(9.4542) | Bit/dim 0.9183(0.9657) | Xent 2.3013(2.3016) | Xent Color 2.3025(2.3026) | Loss 4.6275(4.7701) | Error 0.8778(0.8875) | Error Color 0.9067(0.9012) |Steps 410(393.13) | Grad Norm 0.6749(0.5258) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0006 | Time 57.9195, Epoch Time 705.2032(664.9104), Bit/dim 0.9099(best: 1.1082), Xent 2.3013, Xent Color 2.3026. Loss 2.0609, Error 0.8865(best: 0.8865), Error Color 0.8991(best: 0.8988)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0400 | Time 9.7045(9.4417) | Bit/dim 0.7811(0.9381) | Xent 2.3000(2.3015) | Xent Color 2.3024(2.3026) | Loss 4.3775(5.1860) | Error 0.8767(0.8871) | Error Color 0.8911(0.9007) |Steps 410(393.70) | Grad Norm 1.9741(0.6290) | Total Time 0.00(0.00)\n",
      "Iter 0410 | Time 9.5484(9.4743) | Bit/dim 0.7829(0.8982) | Xent 2.2988(2.3014) | Xent Color 2.3026(2.3026) | Loss 4.3225(4.9673) | Error 0.8756(0.8871) | Error Color 0.8911(0.9005) |Steps 386(392.01) | Grad Norm 0.7165(0.7652) | Total Time 0.00(0.00)\n",
      "Iter 0420 | Time 9.5512(9.4732) | Bit/dim 0.7805(0.8676) | Xent 2.2994(2.3011) | Xent Color 2.3026(2.3026) | Loss 4.2562(4.7949) | Error 0.8744(0.8860) | Error Color 0.9078(0.8995) |Steps 404(389.99) | Grad Norm 0.3864(0.7578) | Total Time 0.00(0.00)\n",
      "Iter 0430 | Time 9.7647(9.5037) | Bit/dim 0.7829(0.8445) | Xent 2.3016(2.3012) | Xent Color 2.3028(2.3026) | Loss 4.2962(4.6752) | Error 0.8900(0.8868) | Error Color 0.9144(0.9001) |Steps 380(391.89) | Grad Norm 0.2196(0.6796) | Total Time 0.00(0.00)\n",
      "Iter 0440 | Time 8.9507(9.5067) | Bit/dim 0.7745(0.8264) | Xent 2.3012(2.3013) | Xent Color 2.3025(2.3026) | Loss 4.2808(4.5856) | Error 0.8744(0.8870) | Error Color 0.9022(0.9012) |Steps 392(392.66) | Grad Norm 0.5171(0.6628) | Total Time 0.00(0.00)\n",
      "Iter 0450 | Time 9.3411(9.5439) | Bit/dim 0.7796(0.8137) | Xent 2.3013(2.3014) | Xent Color 2.3025(2.3026) | Loss 4.3483(4.5178) | Error 0.9022(0.8883) | Error Color 0.8811(0.9000) |Steps 398(393.12) | Grad Norm 1.6007(0.7662) | Total Time 0.00(0.00)\n",
      "Iter 0460 | Time 9.3008(9.5691) | Bit/dim 0.7666(0.8031) | Xent 2.3007(2.3015) | Xent Color 2.3024(2.3025) | Loss 4.3734(4.4681) | Error 0.8856(0.8874) | Error Color 0.9144(0.8998) |Steps 416(394.44) | Grad Norm 2.0966(1.2758) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0007 | Time 58.5548, Epoch Time 714.5315(666.3990), Bit/dim 0.7660(best: 0.9099), Xent 2.3011, Xent Color 2.3027. Loss 1.9169, Error 0.8865(best: 0.8865), Error Color 0.9026(best: 0.8988)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0470 | Time 9.5616(9.5729) | Bit/dim 0.6864(0.7781) | Xent 2.3021(2.3016) | Xent Color 2.3026(2.3026) | Loss 4.2066(4.8391) | Error 0.8967(0.8886) | Error Color 0.9067(0.9013) |Steps 410(394.22) | Grad Norm 1.5113(1.6260) | Total Time 0.00(0.00)\n",
      "Iter 0480 | Time 9.8667(9.5343) | Bit/dim 0.6905(0.7551) | Xent 2.3000(2.3016) | Xent Color 2.3031(2.3026) | Loss 4.1703(4.6610) | Error 0.8733(0.8887) | Error Color 0.9100(0.9014) |Steps 380(394.55) | Grad Norm 7.2234(2.7402) | Total Time 0.00(0.00)\n",
      "Iter 0490 | Time 9.0684(9.5320) | Bit/dim 0.6852(0.7368) | Xent 2.3001(2.3014) | Xent Color 2.3019(2.3026) | Loss 4.1281(4.5314) | Error 0.8789(0.8891) | Error Color 0.8867(0.9018) |Steps 398(394.95) | Grad Norm 2.1278(3.1798) | Total Time 0.00(0.00)\n",
      "Iter 0500 | Time 9.2512(9.5109) | Bit/dim 0.6894(0.7231) | Xent 2.3012(2.3014) | Xent Color 2.3029(2.3026) | Loss 4.1096(4.4282) | Error 0.8878(0.8891) | Error Color 0.9089(0.9025) |Steps 380(393.54) | Grad Norm 5.2017(3.8114) | Total Time 0.00(0.00)\n",
      "Iter 0510 | Time 9.5960(9.5400) | Bit/dim 0.6811(0.7128) | Xent 2.3011(2.3013) | Xent Color 2.3025(2.3026) | Loss 4.1967(4.3594) | Error 0.8922(0.8882) | Error Color 0.8956(0.9028) |Steps 392(393.97) | Grad Norm 2.1808(4.1994) | Total Time 0.00(0.00)\n",
      "Iter 0520 | Time 9.6768(9.5672) | Bit/dim 0.6798(0.7052) | Xent 2.3000(2.3012) | Xent Color 2.3028(2.3026) | Loss 4.1900(4.3100) | Error 0.8856(0.8872) | Error Color 0.9000(0.9032) |Steps 392(394.74) | Grad Norm 5.9713(5.1181) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0008 | Time 57.4463, Epoch Time 712.5045(667.7822), Bit/dim 0.6736(best: 0.7660), Xent 2.3011, Xent Color 2.3026. Loss 1.8246, Error 0.8865(best: 0.8865), Error Color 0.9029(best: 0.8988)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0530 | Time 9.4553(9.5746) | Bit/dim 0.6363(0.6960) | Xent 2.3044(2.3013) | Xent Color 2.3025(2.3026) | Loss 3.9343(4.7758) | Error 0.9078(0.8869) | Error Color 0.9133(0.9024) |Steps 374(392.87) | Grad Norm 5.9269(4.8079) | Total Time 0.00(0.00)\n",
      "Iter 0540 | Time 9.7644(9.5766) | Bit/dim 0.6406(0.6808) | Xent 2.3004(2.3011) | Xent Color 2.3026(2.3026) | Loss 4.1000(4.5919) | Error 0.8844(0.8869) | Error Color 0.8944(0.9020) |Steps 362(392.65) | Grad Norm 15.5740(5.2144) | Total Time 0.00(0.00)\n",
      "Iter 0550 | Time 9.8777(9.5569) | Bit/dim 0.6406(0.6737) | Xent 2.3009(2.3012) | Xent Color 2.3027(2.3026) | Loss 4.1537(4.4625) | Error 0.8911(0.8873) | Error Color 0.9078(0.9005) |Steps 404(392.15) | Grad Norm 6.8326(8.2468) | Total Time 0.00(0.00)\n",
      "Iter 0560 | Time 9.2696(9.5217) | Bit/dim 0.6414(0.6653) | Xent 2.2999(2.3011) | Xent Color 2.3026(2.3026) | Loss 4.1637(4.3621) | Error 0.8722(0.8863) | Error Color 0.9011(0.9000) |Steps 386(391.49) | Grad Norm 7.0735(8.0248) | Total Time 0.00(0.00)\n",
      "Iter 0570 | Time 9.9869(9.5330) | Bit/dim 0.6301(0.6572) | Xent 2.2999(2.3013) | Xent Color 2.3028(2.3026) | Loss 4.1128(4.2801) | Error 0.8911(0.8878) | Error Color 0.9167(0.8992) |Steps 404(390.48) | Grad Norm 3.9065(6.9429) | Total Time 0.00(0.00)\n",
      "Iter 0580 | Time 9.4485(9.5624) | Bit/dim 0.6356(0.6506) | Xent 2.3015(2.3014) | Xent Color 2.3027(2.3026) | Loss 4.0291(4.2201) | Error 0.8922(0.8880) | Error Color 0.9011(0.9007) |Steps 368(391.49) | Grad Norm 1.9380(5.8174) | Total Time 0.00(0.00)\n",
      "Iter 0590 | Time 9.6035(9.5775) | Bit/dim 0.6280(0.6453) | Xent 2.3022(2.3011) | Xent Color 2.3025(2.3026) | Loss 4.0040(4.1698) | Error 0.8933(0.8867) | Error Color 0.9056(0.9014) |Steps 380(391.46) | Grad Norm 0.8581(4.8211) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0009 | Time 57.9241, Epoch Time 713.9249(669.1664), Bit/dim 0.6260(best: 0.6736), Xent 2.3010, Xent Color 2.3025. Loss 1.7769, Error 0.8865(best: 0.8865), Error Color 0.8995(best: 0.8988)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0600 | Time 9.2861(9.5933) | Bit/dim 0.6118(0.6376) | Xent 2.3040(2.3011) | Xent Color 2.3025(2.3026) | Loss 3.9734(4.5961) | Error 0.8989(0.8879) | Error Color 0.8900(0.8997) |Steps 392(390.35) | Grad Norm 10.4949(4.8144) | Total Time 0.00(0.00)\n",
      "Iter 0610 | Time 9.8164(9.5791) | Bit/dim 0.6203(0.6386) | Xent 2.3005(2.3009) | Xent Color 2.3027(2.3026) | Loss 4.0917(4.4541) | Error 0.9044(0.8875) | Error Color 0.8978(0.9006) |Steps 398(388.81) | Grad Norm 9.8877(9.4354) | Total Time 0.00(0.00)\n",
      "Iter 0620 | Time 9.7001(9.6325) | Bit/dim 0.6166(0.6342) | Xent 2.2991(2.3009) | Xent Color 2.3028(2.3026) | Loss 4.0148(4.3470) | Error 0.8878(0.8869) | Error Color 0.9167(0.9011) |Steps 368(391.85) | Grad Norm 5.2119(9.2498) | Total Time 0.00(0.00)\n",
      "Iter 0630 | Time 9.8186(9.6337) | Bit/dim 0.6130(0.6291) | Xent 2.3014(2.3010) | Xent Color 2.3026(2.3026) | Loss 4.1520(4.2659) | Error 0.8833(0.8866) | Error Color 0.9167(0.9001) |Steps 422(393.98) | Grad Norm 3.7957(7.9894) | Total Time 0.00(0.00)\n",
      "Iter 0640 | Time 9.0107(9.6908) | Bit/dim 0.6122(0.6245) | Xent 2.3022(2.3013) | Xent Color 2.3026(2.3026) | Loss 3.9828(4.2075) | Error 0.8956(0.8877) | Error Color 0.8956(0.9001) |Steps 362(393.07) | Grad Norm 2.8144(6.6029) | Total Time 0.00(0.00)\n",
      "Iter 0650 | Time 10.0771(9.6416) | Bit/dim 0.6136(0.6205) | Xent 2.3033(2.3014) | Xent Color 2.3025(2.3026) | Loss 4.0670(4.1632) | Error 0.9011(0.8875) | Error Color 0.9022(0.8992) |Steps 428(391.54) | Grad Norm 2.2338(5.3823) | Total Time 0.00(0.00)\n",
      "Iter 0660 | Time 9.0717(9.6589) | Bit/dim 0.6040(0.6164) | Xent 2.3026(2.3012) | Xent Color 2.3027(2.3026) | Loss 3.9603(4.1399) | Error 0.9056(0.8874) | Error Color 0.9000(0.8996) |Steps 374(393.24) | Grad Norm 1.6003(4.4605) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0010 | Time 58.0369, Epoch Time 720.6858(670.7120), Bit/dim 0.6034(best: 0.6260), Xent 2.3010, Xent Color 2.3026. Loss 1.7543, Error 0.8865(best: 0.8865), Error Color 0.9038(best: 0.8988)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0670 | Time 9.4791(9.6440) | Bit/dim 0.5925(0.6106) | Xent 2.3014(2.3012) | Xent Color 2.3025(2.3026) | Loss 3.9897(4.5343) | Error 0.8956(0.8872) | Error Color 0.8956(0.9003) |Steps 380(393.80) | Grad Norm 2.0565(3.7216) | Total Time 0.00(0.00)\n",
      "Iter 0680 | Time 9.9302(9.5969) | Bit/dim 0.6031(0.6089) | Xent 2.3034(2.3013) | Xent Color 2.3024(2.3026) | Loss 4.1406(4.4053) | Error 0.8956(0.8876) | Error Color 0.8844(0.9008) |Steps 416(393.53) | Grad Norm 10.4273(5.8214) | Total Time 0.00(0.00)\n",
      "Iter 0690 | Time 9.6206(9.5704) | Bit/dim 0.6115(0.6069) | Xent 2.3043(2.3013) | Xent Color 2.3027(2.3026) | Loss 4.0052(4.2958) | Error 0.9044(0.8882) | Error Color 0.9000(0.9009) |Steps 380(392.35) | Grad Norm 12.6912(7.2019) | Total Time 0.00(0.00)\n",
      "Iter 0700 | Time 9.5448(9.5862) | Bit/dim 0.6109(0.6078) | Xent 2.3016(2.3011) | Xent Color 2.3025(2.3026) | Loss 4.0464(4.2274) | Error 0.8878(0.8873) | Error Color 0.9089(0.8998) |Steps 410(392.55) | Grad Norm 15.1814(8.6383) | Total Time 0.00(0.00)\n",
      "Iter 0710 | Time 9.6310(9.5778) | Bit/dim 0.5959(0.6056) | Xent 2.3006(2.3012) | Xent Color 2.3025(2.3026) | Loss 4.0756(4.1735) | Error 0.8789(0.8873) | Error Color 0.8944(0.8996) |Steps 416(394.50) | Grad Norm 4.4694(8.0053) | Total Time 0.00(0.00)\n",
      "Iter 0720 | Time 9.8566(9.5796) | Bit/dim 0.5955(0.6027) | Xent 2.2995(2.3011) | Xent Color 2.3025(2.3026) | Loss 4.0232(4.1313) | Error 0.8867(0.8868) | Error Color 0.8978(0.8985) |Steps 416(394.44) | Grad Norm 2.8041(6.8747) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0011 | Time 58.6374, Epoch Time 713.4795(671.9950), Bit/dim 0.5900(best: 0.6034), Xent 2.3010, Xent Color 2.3027. Loss 1.7409, Error 0.8865(best: 0.8865), Error Color 0.9014(best: 0.8988)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0730 | Time 9.3344(9.5490) | Bit/dim 0.5857(0.5993) | Xent 2.3025(2.3012) | Xent Color 2.3023(2.3026) | Loss 3.9733(4.6062) | Error 0.8889(0.8882) | Error Color 0.8922(0.8987) |Steps 392(393.50) | Grad Norm 1.7839(5.6071) | Total Time 0.00(0.00)\n",
      "Iter 0740 | Time 9.2378(9.5514) | Bit/dim 0.5846(0.5953) | Xent 2.3021(2.3015) | Xent Color 2.3022(2.3026) | Loss 4.0682(4.4513) | Error 0.8933(0.8902) | Error Color 0.8922(0.9006) |Steps 398(395.36) | Grad Norm 2.9637(4.5749) | Total Time 0.00(0.00)\n",
      "Iter 0750 | Time 9.4246(9.5518) | Bit/dim 0.6338(0.6166) | Xent 2.3012(2.3014) | Xent Color 2.3027(2.3026) | Loss 4.0318(4.3815) | Error 0.8878(0.8896) | Error Color 0.9233(0.9010) |Steps 380(396.59) | Grad Norm 13.0635(11.6288) | Total Time 0.00(0.00)\n",
      "Iter 0760 | Time 9.8476(9.6347) | Bit/dim 0.6174(0.6198) | Xent 2.2987(2.3012) | Xent Color 2.3025(2.3026) | Loss 4.1078(4.3045) | Error 0.8633(0.8880) | Error Color 0.8867(0.8996) |Steps 410(399.90) | Grad Norm 4.6020(11.2343) | Total Time 0.00(0.00)\n",
      "Iter 0770 | Time 9.0392(9.6514) | Bit/dim 0.6086(0.6178) | Xent 2.2996(2.3011) | Xent Color 2.3027(2.3026) | Loss 4.0125(4.2420) | Error 0.8844(0.8871) | Error Color 0.8922(0.8996) |Steps 392(399.99) | Grad Norm 6.0938(9.8096) | Total Time 0.00(0.00)\n",
      "Iter 0780 | Time 9.3297(9.6064) | Bit/dim 0.5973(0.6136) | Xent 2.3001(2.3013) | Xent Color 2.3029(2.3026) | Loss 4.0381(4.1887) | Error 0.8778(0.8893) | Error Color 0.9233(0.9013) |Steps 368(398.06) | Grad Norm 2.2356(8.0345) | Total Time 0.00(0.00)\n",
      "Iter 0790 | Time 9.6194(9.6030) | Bit/dim 0.5894(0.6081) | Xent 2.3009(2.3011) | Xent Color 2.3026(2.3026) | Loss 4.0929(4.1607) | Error 0.8867(0.8869) | Error Color 0.9078(0.9018) |Steps 410(398.21) | Grad Norm 0.6348(6.3729) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0012 | Time 60.1018, Epoch Time 719.3915(673.4169), Bit/dim 0.5890(best: 0.5900), Xent 2.3010, Xent Color 2.3026. Loss 1.7399, Error 0.8865(best: 0.8865), Error Color 0.9000(best: 0.8988)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0800 | Time 9.6359(9.6064) | Bit/dim 0.5811(0.6021) | Xent 2.2999(2.3012) | Xent Color 2.3023(2.3026) | Loss 4.0622(4.6061) | Error 0.8911(0.8872) | Error Color 0.9022(0.9002) |Steps 416(397.24) | Grad Norm 1.2571(5.0511) | Total Time 0.00(0.00)\n",
      "Iter 0810 | Time 9.6274(9.6256) | Bit/dim 0.5797(0.5965) | Xent 2.3006(2.3013) | Xent Color 2.3028(2.3026) | Loss 4.0483(4.4714) | Error 0.8878(0.8877) | Error Color 0.9122(0.9000) |Steps 404(399.83) | Grad Norm 3.2035(4.2227) | Total Time 0.00(0.00)\n",
      "Iter 0820 | Time 9.7786(9.6857) | Bit/dim 0.5747(0.5913) | Xent 2.3011(2.3012) | Xent Color 2.3022(2.3026) | Loss 4.0816(4.3733) | Error 0.8844(0.8856) | Error Color 0.8811(0.8997) |Steps 416(403.83) | Grad Norm 5.5147(3.9352) | Total Time 0.00(0.00)\n",
      "Iter 0830 | Time 9.4538(9.7047) | Bit/dim 0.5918(0.5892) | Xent 2.2992(2.3011) | Xent Color 2.3024(2.3026) | Loss 3.9268(4.2813) | Error 0.8756(0.8867) | Error Color 0.8978(0.8996) |Steps 398(403.07) | Grad Norm 15.5820(6.1930) | Total Time 0.00(0.00)\n",
      "Iter 0840 | Time 9.6227(9.7000) | Bit/dim 0.5714(0.5857) | Xent 2.2993(2.3010) | Xent Color 2.3027(2.3026) | Loss 4.0855(4.2324) | Error 0.8856(0.8869) | Error Color 0.9033(0.8991) |Steps 404(404.69) | Grad Norm 7.4044(6.9751) | Total Time 0.00(0.00)\n",
      "Iter 0850 | Time 9.6974(9.7340) | Bit/dim 0.5651(0.5810) | Xent 2.2994(2.3011) | Xent Color 2.3025(2.3026) | Loss 4.0928(4.1976) | Error 0.8800(0.8874) | Error Color 0.8933(0.8995) |Steps 422(410.93) | Grad Norm 6.2834(6.6079) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0013 | Time 61.2204, Epoch Time 729.4732(675.0986), Bit/dim 0.5563(best: 0.5890), Xent 2.3010, Xent Color 2.3026. Loss 1.7072, Error 0.8865(best: 0.8865), Error Color 0.9018(best: 0.8988)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0860 | Time 9.6754(9.7468) | Bit/dim 0.5548(0.5754) | Xent 2.3051(2.3013) | Xent Color 2.3026(2.3026) | Loss 4.0645(4.7774) | Error 0.9078(0.8881) | Error Color 0.9144(0.9008) |Steps 410(412.92) | Grad Norm 4.2769(5.7762) | Total Time 0.00(0.00)\n",
      "Iter 0870 | Time 9.8466(9.7208) | Bit/dim 0.5434(0.5682) | Xent 2.2998(2.3013) | Xent Color 2.3025(2.3026) | Loss 4.0559(4.5883) | Error 0.8811(0.8884) | Error Color 0.8856(0.8990) |Steps 440(411.49) | Grad Norm 2.9607(4.9241) | Total Time 0.00(0.00)\n",
      "Iter 0880 | Time 10.0286(9.6970) | Bit/dim 0.5265(0.5602) | Xent 2.3021(2.3014) | Xent Color 2.3027(2.3026) | Loss 4.0693(4.4331) | Error 0.8944(0.8893) | Error Color 0.9044(0.8985) |Steps 410(410.39) | Grad Norm 3.4747(5.3732) | Total Time 0.00(0.00)\n",
      "Iter 0890 | Time 9.4262(9.6887) | Bit/dim 0.5076(0.5488) | Xent 2.2948(2.3011) | Xent Color 2.3022(2.3026) | Loss 3.9409(4.3041) | Error 0.8444(0.8869) | Error Color 0.8811(0.8982) |Steps 422(407.09) | Grad Norm 5.1396(5.4761) | Total Time 0.00(0.00)\n",
      "Iter 0900 | Time 9.2773(9.6263) | Bit/dim 0.4883(0.5355) | Xent 2.3007(2.3010) | Xent Color 2.3023(2.3026) | Loss 3.8213(4.2078) | Error 0.8722(0.8860) | Error Color 0.8922(0.8974) |Steps 374(405.19) | Grad Norm 8.5385(6.6387) | Total Time 0.00(0.00)\n",
      "Iter 0910 | Time 9.4730(9.5718) | Bit/dim 0.5890(0.5319) | Xent 2.3031(2.3011) | Xent Color 2.3026(2.3026) | Loss 4.0591(4.1336) | Error 0.8922(0.8872) | Error Color 0.8967(0.8982) |Steps 404(403.07) | Grad Norm 48.3417(11.3739) | Total Time 0.00(0.00)\n",
      "Iter 0920 | Time 9.8311(9.6327) | Bit/dim 0.5201(0.5365) | Xent 2.3022(2.3013) | Xent Color 2.3024(2.3026) | Loss 3.9553(4.0845) | Error 0.8878(0.8885) | Error Color 0.8911(0.8990) |Steps 416(404.87) | Grad Norm 10.6079(11.9185) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0014 | Time 56.5622, Epoch Time 716.7709(676.3488), Bit/dim 0.4898(best: 0.5563), Xent 2.3010, Xent Color 2.3027. Loss 1.6407, Error 0.8865(best: 0.8865), Error Color 0.8993(best: 0.8988)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0930 | Time 9.9276(9.7107) | Bit/dim 0.4803(0.5254) | Xent 2.3029(2.3012) | Xent Color 2.3028(2.3026) | Loss 3.9456(4.4851) | Error 0.9000(0.8882) | Error Color 0.9033(0.8988) |Steps 434(405.27) | Grad Norm 5.4924(10.1640) | Total Time 0.00(0.00)\n",
      "Iter 0940 | Time 10.3709(9.8209) | Bit/dim 0.4637(0.5110) | Xent 2.3035(2.3012) | Xent Color 2.3027(2.3026) | Loss 3.9067(4.3153) | Error 0.8811(0.8881) | Error Color 0.8911(0.8986) |Steps 428(408.60) | Grad Norm 4.0815(8.5174) | Total Time 0.00(0.00)\n",
      "Iter 0950 | Time 10.2571(9.8693) | Bit/dim 0.4464(0.4957) | Xent 2.2998(2.3012) | Xent Color 2.3023(2.3026) | Loss 3.9120(4.1996) | Error 0.8911(0.8875) | Error Color 0.8733(0.8996) |Steps 428(412.68) | Grad Norm 2.5590(6.9255) | Total Time 0.00(0.00)\n",
      "Iter 0960 | Time 9.8283(9.8610) | Bit/dim 0.4663(0.4846) | Xent 2.3002(2.3011) | Xent Color 2.3024(2.3026) | Loss 3.9385(4.1125) | Error 0.8867(0.8875) | Error Color 0.8956(0.9000) |Steps 416(412.68) | Grad Norm 22.8997(7.8095) | Total Time 0.00(0.00)\n",
      "Iter 0970 | Time 9.8094(9.8541) | Bit/dim 0.4386(0.4730) | Xent 2.3000(2.3010) | Xent Color 2.3024(2.3026) | Loss 3.8152(4.0432) | Error 0.8867(0.8871) | Error Color 0.8778(0.8991) |Steps 380(410.67) | Grad Norm 10.5376(8.0859) | Total Time 0.00(0.00)\n",
      "Iter 0980 | Time 9.0954(9.8220) | Bit/dim 0.4238(0.4616) | Xent 2.3006(2.3012) | Xent Color 2.3027(2.3026) | Loss 3.7137(3.9881) | Error 0.8856(0.8879) | Error Color 0.8967(0.8979) |Steps 386(410.72) | Grad Norm 2.3560(7.5085) | Total Time 0.00(0.00)\n",
      "Iter 0990 | Time 9.2859(9.7527) | Bit/dim 0.4167(0.4513) | Xent 2.2981(2.3012) | Xent Color 2.3027(2.3026) | Loss 3.7607(3.9430) | Error 0.8678(0.8876) | Error Color 0.8989(0.8994) |Steps 386(407.12) | Grad Norm 3.1768(7.0995) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0015 | Time 60.6347, Epoch Time 735.8197(678.1329), Bit/dim 0.4183(best: 0.4898), Xent 2.3010, Xent Color 2.3026. Loss 1.5692, Error 0.8865(best: 0.8865), Error Color 0.8998(best: 0.8988)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1000 | Time 8.8676(9.7245) | Bit/dim 0.4146(0.4439) | Xent 2.3040(2.3012) | Xent Color 2.3025(2.3026) | Loss 3.6586(4.3582) | Error 0.8933(0.8876) | Error Color 0.8900(0.8988) |Steps 380(406.85) | Grad Norm 5.1712(8.1292) | Total Time 0.00(0.00)\n",
      "Iter 1010 | Time 9.6096(9.7431) | Bit/dim 0.4305(0.4391) | Xent 2.3013(2.3015) | Xent Color 2.3027(2.3026) | Loss 3.8155(4.2129) | Error 0.8767(0.8885) | Error Color 0.9067(0.8998) |Steps 410(408.14) | Grad Norm 19.1749(9.8548) | Total Time 0.00(0.00)\n",
      "Iter 1020 | Time 9.6069(9.7443) | Bit/dim 0.4221(0.4359) | Xent 2.3025(2.3015) | Xent Color 2.3025(2.3026) | Loss 3.6307(4.0905) | Error 0.8900(0.8887) | Error Color 0.9033(0.8997) |Steps 362(406.87) | Grad Norm 12.7272(10.7292) | Total Time 0.00(0.00)\n",
      "Iter 1030 | Time 9.6116(9.6707) | Bit/dim 0.3931(0.4268) | Xent 2.2983(2.3014) | Xent Color 2.3025(2.3026) | Loss 3.7586(3.9866) | Error 0.8767(0.8880) | Error Color 0.8967(0.8995) |Steps 404(402.44) | Grad Norm 4.3817(9.5948) | Total Time 0.00(0.00)\n",
      "Iter 1040 | Time 9.4563(9.6495) | Bit/dim 0.3729(0.4149) | Xent 2.2993(2.3011) | Xent Color 2.3026(2.3026) | Loss 3.7171(3.9201) | Error 0.8856(0.8870) | Error Color 0.8933(0.9005) |Steps 374(400.53) | Grad Norm 2.4999(8.3625) | Total Time 0.00(0.00)\n",
      "Iter 1050 | Time 9.6058(9.5902) | Bit/dim 0.3802(0.4071) | Xent 2.2995(2.3009) | Xent Color 2.3025(2.3026) | Loss 3.6431(3.8607) | Error 0.8822(0.8865) | Error Color 0.8944(0.8995) |Steps 368(399.22) | Grad Norm 18.6073(10.5156) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0016 | Time 59.0443, Epoch Time 717.1011(679.3020), Bit/dim 0.4002(best: 0.4183), Xent 2.3010, Xent Color 2.3026. Loss 1.5511, Error 0.8865(best: 0.8865), Error Color 0.9035(best: 0.8988)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1060 | Time 9.1325(9.5557) | Bit/dim 0.3595(0.3998) | Xent 2.3027(2.3012) | Xent Color 2.3025(2.3026) | Loss 3.5758(4.3328) | Error 0.8956(0.8866) | Error Color 0.9033(0.8990) |Steps 398(402.35) | Grad Norm 15.4926(12.7470) | Total Time 0.00(0.00)\n",
      "Iter 1070 | Time 9.3676(9.5631) | Bit/dim 0.3286(0.3845) | Xent 2.3033(2.3013) | Xent Color 2.3030(2.3026) | Loss 3.6101(4.1458) | Error 0.8922(0.8875) | Error Color 0.9122(0.8996) |Steps 362(401.47) | Grad Norm 4.9783(11.7210) | Total Time 0.00(0.00)\n",
      "Iter 1080 | Time 9.8647(9.6049) | Bit/dim 0.3134(0.3675) | Xent 2.2981(2.3014) | Xent Color 2.3027(2.3026) | Loss 3.6073(3.9982) | Error 0.8689(0.8881) | Error Color 0.9022(0.8997) |Steps 416(400.94) | Grad Norm 5.9566(10.1476) | Total Time 0.00(0.00)\n",
      "Iter 1090 | Time 9.1951(9.5772) | Bit/dim 0.3546(0.3625) | Xent 2.3019(2.3013) | Xent Color 2.3027(2.3026) | Loss 3.5622(3.8989) | Error 0.8878(0.8880) | Error Color 0.9056(0.8999) |Steps 392(401.35) | Grad Norm 14.5593(12.0850) | Total Time 0.00(0.00)\n",
      "Iter 1100 | Time 8.8846(9.5472) | Bit/dim 0.3444(0.3605) | Xent 2.2998(2.3011) | Xent Color 2.3024(2.3026) | Loss 3.4260(3.8179) | Error 0.8756(0.8873) | Error Color 0.9011(0.9005) |Steps 380(399.89) | Grad Norm 10.3235(12.3647) | Total Time 0.00(0.00)\n",
      "Iter 1110 | Time 9.0642(9.5320) | Bit/dim 0.3079(0.3491) | Xent 2.3016(2.3009) | Xent Color 2.3025(2.3026) | Loss 3.4771(3.7373) | Error 0.8911(0.8867) | Error Color 0.8956(0.9001) |Steps 362(394.44) | Grad Norm 4.4289(10.8202) | Total Time 0.00(0.00)\n",
      "Iter 1120 | Time 9.4480(9.5148) | Bit/dim 0.2873(0.3349) | Xent 2.3010(2.3010) | Xent Color 2.3023(2.3026) | Loss 3.4982(3.6820) | Error 0.8833(0.8876) | Error Color 0.8944(0.8998) |Steps 368(393.85) | Grad Norm 5.4862(9.1377) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0017 | Time 59.9764, Epoch Time 714.6687(680.3630), Bit/dim 0.2827(best: 0.4002), Xent 2.3010, Xent Color 2.3027. Loss 1.4336, Error 0.8865(best: 0.8865), Error Color 0.9032(best: 0.8988)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1130 | Time 9.9833(9.5454) | Bit/dim 0.2719(0.3200) | Xent 2.3035(2.3012) | Xent Color 2.3023(2.3026) | Loss 3.4717(4.1014) | Error 0.8956(0.8873) | Error Color 0.8956(0.8994) |Steps 398(396.08) | Grad Norm 3.2943(7.6574) | Total Time 0.00(0.00)\n",
      "Iter 1140 | Time 9.5863(9.5586) | Bit/dim 0.2555(0.3050) | Xent 2.3008(2.3011) | Xent Color 2.3023(2.3026) | Loss 3.4899(3.9398) | Error 0.8889(0.8874) | Error Color 0.8800(0.8983) |Steps 404(394.51) | Grad Norm 4.3457(6.5180) | Total Time 0.00(0.00)\n",
      "Iter 1150 | Time 8.7042(9.4995) | Bit/dim 0.3051(0.2974) | Xent 2.2996(2.3010) | Xent Color 2.3025(2.3026) | Loss 3.4545(3.8148) | Error 0.8967(0.8865) | Error Color 0.8756(0.8972) |Steps 386(394.23) | Grad Norm 17.2032(8.8051) | Total Time 0.00(0.00)\n",
      "Iter 1160 | Time 9.6643(9.5197) | Bit/dim 0.2797(0.3009) | Xent 2.3016(2.3011) | Xent Color 2.3028(2.3026) | Loss 3.3812(3.7313) | Error 0.8911(0.8861) | Error Color 0.9100(0.8983) |Steps 374(394.78) | Grad Norm 7.3780(10.7730) | Total Time 0.00(0.00)\n",
      "Iter 1170 | Time 9.4252(9.5043) | Bit/dim 0.2459(0.2899) | Xent 2.3018(2.3012) | Xent Color 2.3023(2.3025) | Loss 3.4927(3.6547) | Error 0.8944(0.8883) | Error Color 0.8978(0.8968) |Steps 404(392.86) | Grad Norm 8.3925(10.1761) | Total Time 0.00(0.00)\n",
      "Iter 1180 | Time 9.9996(9.4801) | Bit/dim 0.2237(0.2738) | Xent 2.3011(2.3013) | Xent Color 2.3023(2.3025) | Loss 3.4850(3.5897) | Error 0.8922(0.8885) | Error Color 0.9078(0.8977) |Steps 380(393.24) | Grad Norm 8.2114(9.1476) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0018 | Time 59.3764, Epoch Time 709.3639(681.2330), Bit/dim 0.2582(best: 0.2827), Xent 2.3010, Xent Color 2.3026. Loss 1.4091, Error 0.8865(best: 0.8865), Error Color 0.8980(best: 0.8988)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1190 | Time 9.9537(9.4481) | Bit/dim 0.2478(0.2629) | Xent 2.3027(2.3011) | Xent Color 2.3028(2.3026) | Loss 3.4810(4.1057) | Error 0.8978(0.8880) | Error Color 0.8967(0.8982) |Steps 428(391.99) | Grad Norm 18.0648(10.4702) | Total Time 0.00(0.00)\n",
      "Iter 1200 | Time 9.7291(9.4476) | Bit/dim 0.2116(0.2537) | Xent 2.3016(2.3011) | Xent Color 2.3020(2.3025) | Loss 3.3790(3.9203) | Error 0.8922(0.8875) | Error Color 0.9022(0.8969) |Steps 404(394.85) | Grad Norm 8.9061(11.6305) | Total Time 0.00(0.00)\n",
      "Iter 1210 | Time 9.4615(9.4268) | Bit/dim 0.1938(0.2402) | Xent 2.2990(2.3010) | Xent Color 2.3024(2.3026) | Loss 3.3587(3.7719) | Error 0.8778(0.8865) | Error Color 0.8911(0.8981) |Steps 410(395.99) | Grad Norm 6.2134(10.7503) | Total Time 0.00(0.00)\n",
      "Iter 1220 | Time 9.4533(9.4179) | Bit/dim 0.1798(0.2266) | Xent 2.3035(2.3011) | Xent Color 2.3025(2.3026) | Loss 3.3006(3.6583) | Error 0.8900(0.8870) | Error Color 0.8911(0.8984) |Steps 380(393.89) | Grad Norm 6.5856(9.6629) | Total Time 0.00(0.00)\n",
      "Iter 1230 | Time 9.7362(9.4287) | Bit/dim 0.1758(0.2139) | Xent 2.2999(2.3012) | Xent Color 2.3027(2.3026) | Loss 3.3197(3.5741) | Error 0.8767(0.8876) | Error Color 0.9000(0.8979) |Steps 404(394.24) | Grad Norm 5.7739(8.7436) | Total Time 0.00(0.00)\n",
      "Iter 1240 | Time 9.1510(9.4922) | Bit/dim 0.1708(0.2028) | Xent 2.3052(2.3011) | Xent Color 2.3025(2.3026) | Loss 3.2558(3.5026) | Error 0.9089(0.8871) | Error Color 0.8822(0.8975) |Steps 380(395.86) | Grad Norm 8.9220(7.9461) | Total Time 0.00(0.00)\n",
      "Iter 1250 | Time 9.1653(9.5109) | Bit/dim 0.3102(0.2056) | Xent 2.3031(2.3012) | Xent Color 2.3025(2.3026) | Loss 3.5243(3.4691) | Error 0.8978(0.8879) | Error Color 0.8944(0.8989) |Steps 374(397.50) | Grad Norm 32.2914(10.9939) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0019 | Time 61.8212, Epoch Time 712.6614(682.1758), Bit/dim 0.2290(best: 0.2582), Xent 2.3010, Xent Color 2.3026. Loss 1.3799, Error 0.8865(best: 0.8865), Error Color 0.8993(best: 0.8980)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1260 | Time 9.2806(9.4920) | Bit/dim 0.2108(0.2117) | Xent 2.3025(2.3015) | Xent Color 2.3024(2.3026) | Loss 3.3749(3.9613) | Error 0.9000(0.8895) | Error Color 0.8978(0.8999) |Steps 380(396.39) | Grad Norm 11.2322(13.1734) | Total Time 0.00(0.00)\n",
      "Iter 1270 | Time 9.0683(9.3889) | Bit/dim 0.1748(0.2052) | Xent 2.3026(2.3013) | Xent Color 2.3030(2.3027) | Loss 3.2952(3.7872) | Error 0.8956(0.8889) | Error Color 0.9033(0.9008) |Steps 386(395.51) | Grad Norm 4.1921(11.5403) | Total Time 0.00(0.00)\n",
      "Iter 1280 | Time 8.9624(9.4047) | Bit/dim 0.1646(0.1961) | Xent 2.3028(2.3012) | Xent Color 2.3028(2.3027) | Loss 3.2743(3.6568) | Error 0.8933(0.8875) | Error Color 0.9144(0.9014) |Steps 368(393.62) | Grad Norm 4.8267(9.7119) | Total Time 0.00(0.00)\n",
      "Iter 1290 | Time 8.8082(9.4046) | Bit/dim 0.1559(0.1869) | Xent 2.2996(2.3013) | Xent Color 2.3026(2.3026) | Loss 3.2877(3.5549) | Error 0.8800(0.8876) | Error Color 0.8900(0.9005) |Steps 404(391.43) | Grad Norm 2.3350(8.0694) | Total Time 0.00(0.00)\n",
      "Iter 1300 | Time 9.0459(9.3920) | Bit/dim 0.1533(0.1787) | Xent 2.3030(2.3015) | Xent Color 2.3025(2.3026) | Loss 3.1451(3.4737) | Error 0.9000(0.8890) | Error Color 0.9200(0.9019) |Steps 398(389.49) | Grad Norm 2.9285(6.8008) | Total Time 0.00(0.00)\n",
      "Iter 1310 | Time 8.8327(9.3606) | Bit/dim 0.2464(0.1841) | Xent 2.3041(2.3013) | Xent Color 2.3027(2.3026) | Loss 3.2722(3.4280) | Error 0.9033(0.8878) | Error Color 0.8956(0.9010) |Steps 374(387.32) | Grad Norm 26.6000(9.4317) | Total Time 0.00(0.00)\n",
      "Iter 1320 | Time 9.3481(9.3589) | Bit/dim 0.1993(0.1939) | Xent 2.3017(2.3011) | Xent Color 2.3029(2.3026) | Loss 3.3281(3.4148) | Error 0.8911(0.8872) | Error Color 0.8978(0.9014) |Steps 374(388.61) | Grad Norm 10.0622(10.2485) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0020 | Time 56.9208, Epoch Time 697.4772(682.6349), Bit/dim 0.1894(best: 0.2290), Xent 2.3010, Xent Color 2.3027. Loss 1.3403, Error 0.8865(best: 0.8865), Error Color 0.9024(best: 0.8980)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1330 | Time 9.2578(9.3955) | Bit/dim 0.1645(0.1895) | Xent 2.3018(2.3010) | Xent Color 2.3025(2.3026) | Loss 3.2856(3.7737) | Error 0.8933(0.8867) | Error Color 0.8867(0.9008) |Steps 398(390.32) | Grad Norm 6.3897(9.3244) | Total Time 0.00(0.00)\n",
      "Iter 1340 | Time 9.6785(9.3111) | Bit/dim 0.1546(0.1816) | Xent 2.3043(2.3012) | Xent Color 2.3024(2.3026) | Loss 3.3052(3.6382) | Error 0.8922(0.8870) | Error Color 0.9067(0.9010) |Steps 386(389.13) | Grad Norm 2.7699(7.7943) | Total Time 0.00(0.00)\n",
      "Iter 1350 | Time 9.4823(9.3476) | Bit/dim 0.1468(0.1737) | Xent 2.3028(2.3013) | Xent Color 2.3028(2.3025) | Loss 3.2942(3.5388) | Error 0.8889(0.8866) | Error Color 0.8978(0.9002) |Steps 410(389.78) | Grad Norm 1.6773(6.3912) | Total Time 0.00(0.00)\n",
      "Iter 1360 | Time 8.9644(9.3408) | Bit/dim 0.1459(0.1663) | Xent 2.3018(2.3011) | Xent Color 2.3023(2.3026) | Loss 3.2749(3.4691) | Error 0.8900(0.8866) | Error Color 0.8789(0.9012) |Steps 392(391.48) | Grad Norm 1.6260(5.2055) | Total Time 0.00(0.00)\n",
      "Iter 1370 | Time 9.0670(9.3621) | Bit/dim 0.1600(0.1609) | Xent 2.3023(2.3014) | Xent Color 2.3027(2.3026) | Loss 3.2136(3.4130) | Error 0.8967(0.8885) | Error Color 0.9011(0.9020) |Steps 386(390.45) | Grad Norm 16.4540(5.1157) | Total Time 0.00(0.00)\n",
      "Iter 1380 | Time 9.3217(9.3759) | Bit/dim 0.2447(0.1799) | Xent 2.2986(2.3013) | Xent Color 2.3026(2.3026) | Loss 3.3017(3.4020) | Error 0.8733(0.8877) | Error Color 0.8900(0.9005) |Steps 380(388.83) | Grad Norm 22.5749(9.5616) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0021 | Time 57.2085, Epoch Time 700.1640(683.1608), Bit/dim 0.1695(best: 0.1894), Xent 2.3010, Xent Color 2.3027. Loss 1.3204, Error 0.8865(best: 0.8865), Error Color 0.9005(best: 0.8980)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1390 | Time 9.4044(9.3809) | Bit/dim 0.1634(0.1825) | Xent 2.3020(2.3012) | Xent Color 2.3025(2.3026) | Loss 3.2698(3.8499) | Error 0.9000(0.8880) | Error Color 0.8978(0.9009) |Steps 398(388.39) | Grad Norm 4.7415(9.7365) | Total Time 0.00(0.00)\n",
      "Iter 1400 | Time 8.8739(9.3697) | Bit/dim 0.1546(0.1770) | Xent 2.3053(2.3013) | Xent Color 2.3023(2.3026) | Loss 3.2009(3.6865) | Error 0.9078(0.8891) | Error Color 0.8889(0.9015) |Steps 392(388.72) | Grad Norm 3.0589(8.7049) | Total Time 0.00(0.00)\n",
      "Iter 1410 | Time 8.8513(9.3664) | Bit/dim 0.1432(0.1696) | Xent 2.2990(2.3012) | Xent Color 2.3027(2.3026) | Loss 3.1372(3.5671) | Error 0.8889(0.8888) | Error Color 0.8967(0.9011) |Steps 356(388.79) | Grad Norm 2.3417(7.4187) | Total Time 0.00(0.00)\n",
      "Iter 1420 | Time 9.2289(9.3141) | Bit/dim 0.1376(0.1619) | Xent 2.2989(2.3012) | Xent Color 2.3027(2.3026) | Loss 3.2130(3.4745) | Error 0.8778(0.8883) | Error Color 0.8956(0.8998) |Steps 398(389.06) | Grad Norm 1.8213(6.2147) | Total Time 0.00(0.00)\n",
      "Iter 1430 | Time 9.5846(9.2932) | Bit/dim 0.1331(0.1553) | Xent 2.2989(2.3010) | Xent Color 2.3028(2.3026) | Loss 3.1737(3.4007) | Error 0.8733(0.8872) | Error Color 0.9167(0.9004) |Steps 404(388.83) | Grad Norm 1.3963(5.1163) | Total Time 0.00(0.00)\n",
      "Iter 1440 | Time 9.7419(9.3821) | Bit/dim 0.1297(0.1494) | Xent 2.2974(2.3010) | Xent Color 2.3027(2.3026) | Loss 3.2173(3.3494) | Error 0.8756(0.8873) | Error Color 0.8989(0.9007) |Steps 416(392.18) | Grad Norm 2.7633(4.2827) | Total Time 0.00(0.00)\n",
      "Iter 1450 | Time 9.5197(9.4343) | Bit/dim 0.1537(0.1468) | Xent 2.3006(2.3013) | Xent Color 2.3027(2.3026) | Loss 3.2500(3.3137) | Error 0.8789(0.8876) | Error Color 0.8989(0.9003) |Steps 380(393.24) | Grad Norm 16.6490(5.5871) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0022 | Time 57.7649, Epoch Time 702.0368(683.7270), Bit/dim 0.1798(best: 0.1695), Xent 2.3010, Xent Color 2.3026. Loss 1.3307, Error 0.8865(best: 0.8865), Error Color 0.8985(best: 0.8980)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1460 | Time 8.9830(9.4150) | Bit/dim 0.1639(0.1538) | Xent 2.3031(2.3014) | Xent Color 2.3026(2.3026) | Loss 3.1478(3.7136) | Error 0.8944(0.8882) | Error Color 0.9022(0.9006) |Steps 368(390.58) | Grad Norm 16.8876(8.4091) | Total Time 0.00(0.00)\n",
      "Iter 1470 | Time 9.4544(9.3676) | Bit/dim 0.1934(0.1667) | Xent 2.3035(2.3014) | Xent Color 2.3027(2.3026) | Loss 3.1991(3.5935) | Error 0.9089(0.8891) | Error Color 0.9044(0.9007) |Steps 380(389.05) | Grad Norm 18.6224(10.9015) | Total Time 0.00(0.00)\n",
      "Iter 1480 | Time 9.1197(9.3429) | Bit/dim 0.1612(0.1663) | Xent 2.3010(2.3012) | Xent Color 2.3027(2.3026) | Loss 3.1989(3.4978) | Error 0.8833(0.8880) | Error Color 0.9033(0.9006) |Steps 392(387.71) | Grad Norm 7.6152(11.0179) | Total Time 0.00(0.00)\n",
      "Iter 1490 | Time 8.9249(9.3598) | Bit/dim 0.1373(0.1605) | Xent 2.3004(2.3013) | Xent Color 2.3026(2.3026) | Loss 3.1865(3.4270) | Error 0.8789(0.8885) | Error Color 0.8944(0.9007) |Steps 392(388.39) | Grad Norm 4.7682(9.9329) | Total Time 0.00(0.00)\n",
      "Iter 1500 | Time 9.9938(9.4063) | Bit/dim 0.1322(0.1539) | Xent 2.2995(2.3011) | Xent Color 2.3026(2.3026) | Loss 3.2342(3.3752) | Error 0.8911(0.8878) | Error Color 0.8922(0.9010) |Steps 392(391.29) | Grad Norm 4.4915(8.4809) | Total Time 0.00(0.00)\n",
      "Iter 1510 | Time 10.3572(9.4398) | Bit/dim 0.1298(0.1476) | Xent 2.3015(2.3011) | Xent Color 2.3026(2.3026) | Loss 3.2736(3.3288) | Error 0.8978(0.8889) | Error Color 0.9067(0.9010) |Steps 410(390.29) | Grad Norm 2.0205(7.1327) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0023 | Time 62.2642, Epoch Time 706.9578(684.4240), Bit/dim 0.1242(best: 0.1695), Xent 2.3010, Xent Color 2.3026. Loss 1.2751, Error 0.8865(best: 0.8865), Error Color 0.9066(best: 0.8980)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1520 | Time 9.6475(9.4524) | Bit/dim 0.1198(0.1412) | Xent 2.3023(2.3014) | Xent Color 2.3026(2.3026) | Loss 3.2348(3.8637) | Error 0.8978(0.8879) | Error Color 0.8989(0.9019) |Steps 392(391.36) | Grad Norm 3.1889(5.9093) | Total Time 0.00(0.00)\n",
      "Iter 1530 | Time 10.1159(9.4971) | Bit/dim 0.1217(0.1360) | Xent 2.3004(2.3013) | Xent Color 2.3026(2.3026) | Loss 3.2409(3.6880) | Error 0.8878(0.8876) | Error Color 0.8833(0.9008) |Steps 440(394.87) | Grad Norm 2.7377(5.0074) | Total Time 0.00(0.00)\n",
      "Iter 1540 | Time 10.1407(9.5225) | Bit/dim 0.1229(0.1344) | Xent 2.2994(2.3011) | Xent Color 2.3025(2.3026) | Loss 3.2412(3.5628) | Error 0.8767(0.8869) | Error Color 0.8989(0.9009) |Steps 422(396.83) | Grad Norm 8.6739(6.4255) | Total Time 0.00(0.00)\n",
      "Iter 1550 | Time 10.2072(9.5506) | Bit/dim 0.1912(0.1452) | Xent 2.3002(2.3010) | Xent Color 2.3026(2.3026) | Loss 3.2528(3.4783) | Error 0.8767(0.8864) | Error Color 0.8889(0.8996) |Steps 392(396.52) | Grad Norm 25.6413(9.9895) | Total Time 0.00(0.00)\n",
      "Iter 1560 | Time 9.0985(9.5897) | Bit/dim 0.1753(0.1586) | Xent 2.2981(2.3012) | Xent Color 2.3028(2.3026) | Loss 3.3274(3.4359) | Error 0.8767(0.8882) | Error Color 0.9122(0.9004) |Steps 410(401.24) | Grad Norm 13.0123(11.9037) | Total Time 0.00(0.00)\n",
      "Iter 1570 | Time 10.2652(9.5512) | Bit/dim 0.1359(0.1568) | Xent 2.2999(2.3012) | Xent Color 2.3025(2.3026) | Loss 3.2928(3.3891) | Error 0.8789(0.8876) | Error Color 0.9022(0.9006) |Steps 428(400.37) | Grad Norm 4.2485(10.7859) | Total Time 0.00(0.00)\n",
      "Iter 1580 | Time 9.9772(9.5686) | Bit/dim 0.1308(0.1509) | Xent 2.3021(2.3012) | Xent Color 2.3024(2.3026) | Loss 3.3155(3.3524) | Error 0.8856(0.8873) | Error Color 0.8889(0.9001) |Steps 410(402.39) | Grad Norm 3.5525(9.1719) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0024 | Time 62.5737, Epoch Time 719.9875(685.4909), Bit/dim 0.1262(best: 0.1242), Xent 2.3010, Xent Color 2.3026. Loss 1.2771, Error 0.8865(best: 0.8865), Error Color 0.8997(best: 0.8980)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1590 | Time 9.6000(9.5534) | Bit/dim 0.1260(0.1440) | Xent 2.3015(2.3010) | Xent Color 2.3028(2.3026) | Loss 3.2046(3.8066) | Error 0.8733(0.8865) | Error Color 0.9067(0.9003) |Steps 428(403.59) | Grad Norm 2.2253(7.4836) | Total Time 0.00(0.00)\n",
      "Iter 1600 | Time 9.8731(9.5769) | Bit/dim 0.1179(0.1372) | Xent 2.3025(2.3009) | Xent Color 2.3028(2.3026) | Loss 3.2333(3.6389) | Error 0.8833(0.8860) | Error Color 0.9167(0.9008) |Steps 410(402.02) | Grad Norm 2.2729(6.1360) | Total Time 0.00(0.00)\n",
      "Iter 1610 | Time 10.3625(9.6207) | Bit/dim 0.1136(0.1308) | Xent 2.3027(2.3011) | Xent Color 2.3027(2.3026) | Loss 3.2922(3.5181) | Error 0.8967(0.8876) | Error Color 0.9133(0.9006) |Steps 428(402.77) | Grad Norm 2.5533(5.2113) | Total Time 0.00(0.00)\n",
      "Iter 1620 | Time 9.6902(9.6632) | Bit/dim 0.1158(0.1253) | Xent 2.3010(2.3012) | Xent Color 2.3026(2.3026) | Loss 3.1942(3.4302) | Error 0.8700(0.8870) | Error Color 0.8944(0.8994) |Steps 374(402.81) | Grad Norm 6.0905(4.6710) | Total Time 0.00(0.00)\n",
      "Iter 1630 | Time 10.0855(9.7261) | Bit/dim 0.1170(0.1220) | Xent 2.2987(2.3012) | Xent Color 2.3026(2.3026) | Loss 3.1981(3.3767) | Error 0.8722(0.8865) | Error Color 0.9067(0.8988) |Steps 398(406.29) | Grad Norm 9.8317(5.5197) | Total Time 0.00(0.00)\n",
      "Iter 1640 | Time 9.9535(9.7689) | Bit/dim 0.1105(0.1207) | Xent 2.3004(2.3013) | Xent Color 2.3022(2.3026) | Loss 3.1823(3.3290) | Error 0.8833(0.8874) | Error Color 0.8844(0.8979) |Steps 398(408.16) | Grad Norm 5.5824(6.6692) | Total Time 0.00(0.00)\n",
      "Iter 1650 | Time 10.0249(9.8065) | Bit/dim 0.2583(0.1300) | Xent 2.2998(2.3013) | Xent Color 2.3025(2.3026) | Loss 3.4350(3.3109) | Error 0.8856(0.8885) | Error Color 0.8900(0.8980) |Steps 404(408.39) | Grad Norm 16.5466(9.4770) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0025 | Time 64.8465, Epoch Time 735.4995(686.9911), Bit/dim 0.3135(best: 0.1242), Xent 2.3010, Xent Color 2.3025. Loss 1.4644, Error 0.8865(best: 0.8865), Error Color 0.8940(best: 0.8980)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1660 | Time 9.5307(9.8376) | Bit/dim 0.2525(0.1655) | Xent 2.3013(2.3014) | Xent Color 2.3027(2.3026) | Loss 3.4674(3.7847) | Error 0.8878(0.8883) | Error Color 0.9122(0.8987) |Steps 392(407.00) | Grad Norm 6.5484(8.7843) | Total Time 0.00(0.00)\n",
      "Iter 1670 | Time 9.8539(9.7710) | Bit/dim 0.1987(0.1798) | Xent 2.3024(2.3011) | Xent Color 2.3026(2.3026) | Loss 3.3195(3.6748) | Error 0.8922(0.8874) | Error Color 0.9056(0.8990) |Steps 410(406.59) | Grad Norm 3.4756(7.6381) | Total Time 0.00(0.00)\n",
      "Iter 1680 | Time 9.2025(9.7045) | Bit/dim 0.1572(0.1793) | Xent 2.3010(2.3009) | Xent Color 2.3027(2.3026) | Loss 3.1750(3.5675) | Error 0.8867(0.8859) | Error Color 0.9033(0.8991) |Steps 386(404.94) | Grad Norm 3.7966(6.5701) | Total Time 0.00(0.00)\n",
      "Iter 1690 | Time 9.7210(9.6827) | Bit/dim 0.1319(0.1701) | Xent 2.3019(2.3012) | Xent Color 2.3028(2.3026) | Loss 3.2714(3.4772) | Error 0.8878(0.8873) | Error Color 0.9044(0.9007) |Steps 404(404.68) | Grad Norm 3.2616(5.9135) | Total Time 0.00(0.00)\n",
      "Iter 1700 | Time 9.8821(9.7093) | Bit/dim 0.1246(0.1599) | Xent 2.2999(2.3010) | Xent Color 2.3028(2.3026) | Loss 3.2220(3.3980) | Error 0.8767(0.8869) | Error Color 0.8944(0.9001) |Steps 404(404.16) | Grad Norm 6.0396(6.0535) | Total Time 0.00(0.00)\n",
      "Iter 1710 | Time 9.4797(9.6803) | Bit/dim 0.3351(0.1771) | Xent 2.3020(2.3012) | Xent Color 2.3028(2.3026) | Loss 3.5593(3.3781) | Error 0.9067(0.8879) | Error Color 0.9000(0.8996) |Steps 368(400.95) | Grad Norm 12.0591(9.8053) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0026 | Time 57.8921, Epoch Time 721.9060(688.0386), Bit/dim 0.1829(best: 0.1242), Xent 2.3010, Xent Color 2.3025. Loss 1.3338, Error 0.8865(best: 0.8865), Error Color 0.9005(best: 0.8940)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1720 | Time 9.9498(9.7117) | Bit/dim 0.1959(0.1889) | Xent 2.2992(2.3012) | Xent Color 2.3024(2.3026) | Loss 3.3369(3.8330) | Error 0.8711(0.8878) | Error Color 0.8900(0.8997) |Steps 380(401.54) | Grad Norm 9.3589(11.4110) | Total Time 0.00(0.00)\n",
      "Iter 1730 | Time 9.5387(9.7332) | Bit/dim 0.1521(0.1827) | Xent 2.3002(2.3011) | Xent Color 2.3025(2.3026) | Loss 3.2929(3.6953) | Error 0.8844(0.8871) | Error Color 0.9033(0.9003) |Steps 428(403.93) | Grad Norm 5.8280(10.3925) | Total Time 0.00(0.00)\n",
      "Iter 1740 | Time 9.7910(9.7397) | Bit/dim 0.1329(0.1707) | Xent 2.3022(2.3014) | Xent Color 2.3026(2.3026) | Loss 3.1965(3.5779) | Error 0.8900(0.8886) | Error Color 0.8867(0.9007) |Steps 392(404.85) | Grad Norm 8.5353(9.3681) | Total Time 0.00(0.00)\n",
      "Iter 1750 | Time 9.7572(9.7893) | Bit/dim 0.1176(0.1580) | Xent 2.2992(2.3012) | Xent Color 2.3024(2.3026) | Loss 3.2038(3.4854) | Error 0.8767(0.8876) | Error Color 0.9011(0.8999) |Steps 422(406.33) | Grad Norm 4.4216(8.0216) | Total Time 0.00(0.00)\n",
      "Iter 1760 | Time 10.0892(9.8449) | Bit/dim 0.1109(0.1467) | Xent 2.3002(2.3012) | Xent Color 2.3025(2.3026) | Loss 3.1622(3.4197) | Error 0.8833(0.8876) | Error Color 0.8867(0.8996) |Steps 440(412.29) | Grad Norm 3.3249(6.6661) | Total Time 0.00(0.00)\n",
      "Iter 1770 | Time 9.8739(9.8700) | Bit/dim 0.1085(0.1373) | Xent 2.3009(2.3015) | Xent Color 2.3025(2.3026) | Loss 3.2537(3.3643) | Error 0.8800(0.8895) | Error Color 0.9133(0.8993) |Steps 452(412.26) | Grad Norm 2.4792(5.6572) | Total Time 0.00(0.00)\n",
      "Iter 1780 | Time 9.8452(9.9028) | Bit/dim 0.1047(0.1295) | Xent 2.3029(2.3014) | Xent Color 2.3026(2.3026) | Loss 3.1811(3.3180) | Error 0.8922(0.8886) | Error Color 0.9022(0.8999) |Steps 404(410.08) | Grad Norm 4.7918(4.8997) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0027 | Time 63.1465, Epoch Time 740.0138(689.5978), Bit/dim 0.1264(best: 0.1242), Xent 2.3010, Xent Color 2.3026. Loss 1.2773, Error 0.8865(best: 0.8865), Error Color 0.8981(best: 0.8940)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1790 | Time 9.4898(9.8303) | Bit/dim 0.1723(0.1348) | Xent 2.3007(2.3013) | Xent Color 2.3024(2.3026) | Loss 3.3163(3.7659) | Error 0.8833(0.8879) | Error Color 0.8878(0.8995) |Steps 416(407.84) | Grad Norm 9.9527(7.0938) | Total Time 0.00(0.00)\n",
      "Iter 1800 | Time 8.9313(9.7406) | Bit/dim 0.3382(0.1872) | Xent 2.3023(2.3010) | Xent Color 2.3026(2.3026) | Loss 3.6536(3.7140) | Error 0.8789(0.8865) | Error Color 0.9000(0.8997) |Steps 404(402.99) | Grad Norm 11.0387(11.0295) | Total Time 0.00(0.00)\n",
      "Iter 1810 | Time 9.5215(9.6884) | Bit/dim 0.2682(0.2186) | Xent 2.3035(2.3012) | Xent Color 2.3027(2.3026) | Loss 3.3323(3.6532) | Error 0.9011(0.8869) | Error Color 0.9022(0.8992) |Steps 404(402.55) | Grad Norm 2.7545(9.9435) | Total Time 0.00(0.00)\n",
      "Iter 1820 | Time 9.3355(9.5482) | Bit/dim 0.2282(0.2265) | Xent 2.3004(2.3012) | Xent Color 2.3025(2.3026) | Loss 3.3657(3.5827) | Error 0.8778(0.8871) | Error Color 0.8933(0.8992) |Steps 386(395.63) | Grad Norm 2.9403(8.5640) | Total Time 0.00(0.00)\n",
      "Iter 1830 | Time 9.7741(9.5913) | Bit/dim 0.2036(0.2223) | Xent 2.3013(2.3013) | Xent Color 2.3028(2.3026) | Loss 3.2990(3.5250) | Error 0.8978(0.8881) | Error Color 0.9000(0.8987) |Steps 380(397.65) | Grad Norm 4.1233(7.3577) | Total Time 0.00(0.00)\n",
      "Iter 1840 | Time 9.8627(9.6044) | Bit/dim 0.1855(0.2135) | Xent 2.3010(2.3013) | Xent Color 2.3027(2.3026) | Loss 3.2996(3.4648) | Error 0.8967(0.8877) | Error Color 0.9044(0.8998) |Steps 422(398.96) | Grad Norm 2.5065(6.2396) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0028 | Time 62.6628, Epoch Time 717.6687(690.4400), Bit/dim 0.1700(best: 0.1242), Xent 2.3010, Xent Color 2.3026. Loss 1.3209, Error 0.8865(best: 0.8865), Error Color 0.8972(best: 0.8940)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1850 | Time 9.8924(9.6172) | Bit/dim 0.1743(0.2031) | Xent 2.2993(2.3011) | Xent Color 2.3024(2.3026) | Loss 3.3555(3.9221) | Error 0.8800(0.8877) | Error Color 0.8922(0.8977) |Steps 410(399.85) | Grad Norm 5.9089(5.3711) | Total Time 0.00(0.00)\n",
      "Iter 1860 | Time 9.6299(9.6024) | Bit/dim 0.1588(0.1935) | Xent 2.3076(2.3013) | Xent Color 2.3030(2.3026) | Loss 3.2685(3.7416) | Error 0.9111(0.8883) | Error Color 0.9033(0.8987) |Steps 416(401.08) | Grad Norm 4.8413(5.5685) | Total Time 0.00(0.00)\n",
      "Iter 1870 | Time 9.6520(9.6407) | Bit/dim 0.1513(0.1846) | Xent 2.3030(2.3013) | Xent Color 2.3027(2.3026) | Loss 3.2468(3.6151) | Error 0.9000(0.8885) | Error Color 0.9111(0.8997) |Steps 416(402.00) | Grad Norm 5.9673(5.9104) | Total Time 0.00(0.00)\n",
      "Iter 1880 | Time 9.6394(9.6208) | Bit/dim 0.1449(0.1755) | Xent 2.2991(2.3013) | Xent Color 2.3026(2.3026) | Loss 3.1756(3.5140) | Error 0.8856(0.8884) | Error Color 0.8967(0.9006) |Steps 398(401.38) | Grad Norm 3.6338(5.9317) | Total Time 0.00(0.00)\n",
      "Iter 1890 | Time 9.2344(9.6414) | Bit/dim 0.1371(0.1666) | Xent 2.3001(2.3013) | Xent Color 2.3028(2.3026) | Loss 3.2330(3.4386) | Error 0.8789(0.8884) | Error Color 0.9133(0.8989) |Steps 416(401.89) | Grad Norm 5.0635(5.5982) | Total Time 0.00(0.00)\n",
      "Iter 1900 | Time 9.5877(9.6765) | Bit/dim 0.1324(0.1577) | Xent 2.3021(2.3012) | Xent Color 2.3023(2.3026) | Loss 3.1641(3.3744) | Error 0.8933(0.8878) | Error Color 0.8944(0.8997) |Steps 398(403.78) | Grad Norm 1.0996(4.5020) | Total Time 0.00(0.00)\n",
      "Iter 1910 | Time 9.9645(9.6858) | Bit/dim 0.4236(0.1839) | Xent 2.3037(2.3011) | Xent Color 2.3025(2.3026) | Loss 3.7448(3.3832) | Error 0.9000(0.8871) | Error Color 0.8944(0.9006) |Steps 422(403.53) | Grad Norm 12.7054(7.7955) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0029 | Time 64.2523, Epoch Time 728.7680(691.5898), Bit/dim 0.3441(best: 0.1242), Xent 2.3010, Xent Color 2.3026. Loss 1.4950, Error 0.8865(best: 0.8865), Error Color 0.9007(best: 0.8940)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1920 | Time 10.5996(9.7930) | Bit/dim 0.3211(0.2269) | Xent 2.3013(2.3012) | Xent Color 2.3024(2.3026) | Loss 3.6611(3.9164) | Error 0.8867(0.8872) | Error Color 0.8900(0.8994) |Steps 452(408.17) | Grad Norm 5.3166(7.5219) | Total Time 0.00(0.00)\n",
      "Iter 1930 | Time 10.4362(9.9434) | Bit/dim 0.2975(0.2486) | Xent 2.2993(2.3013) | Xent Color 2.3031(2.3026) | Loss 3.6144(3.8285) | Error 0.8856(0.8876) | Error Color 0.9178(0.9004) |Steps 458(415.81) | Grad Norm 3.1562(6.5849) | Total Time 0.00(0.00)\n",
      "Iter 1940 | Time 9.4061(9.8860) | Bit/dim 0.2813(0.2587) | Xent 2.2981(2.3014) | Xent Color 2.3023(2.3026) | Loss 3.3772(3.7321) | Error 0.8700(0.8882) | Error Color 0.8922(0.8999) |Steps 392(412.32) | Grad Norm 1.9043(5.5021) | Total Time 0.00(0.00)\n",
      "Iter 1950 | Time 9.3013(9.7312) | Bit/dim 0.2666(0.2622) | Xent 2.3017(2.3010) | Xent Color 2.3029(2.3026) | Loss 3.3455(3.6295) | Error 0.8922(0.8859) | Error Color 0.9067(0.9007) |Steps 392(406.62) | Grad Norm 1.5475(4.5729) | Total Time 0.00(0.00)\n",
      "Iter 1960 | Time 9.2122(9.6641) | Bit/dim 0.2519(0.2614) | Xent 2.3018(2.3009) | Xent Color 2.3029(2.3026) | Loss 3.3394(3.5501) | Error 0.8811(0.8864) | Error Color 0.9167(0.9026) |Steps 392(401.91) | Grad Norm 3.6220(3.9019) | Total Time 0.00(0.00)\n",
      "Iter 1970 | Time 9.5348(9.5946) | Bit/dim 0.2456(0.2586) | Xent 2.3020(2.3010) | Xent Color 2.3027(2.3026) | Loss 3.2831(3.4853) | Error 0.8900(0.8876) | Error Color 0.9189(0.9044) |Steps 392(396.39) | Grad Norm 6.9488(4.3869) | Total Time 0.00(0.00)\n",
      "Iter 1980 | Time 9.2998(9.5464) | Bit/dim 0.2301(0.2529) | Xent 2.3013(2.3013) | Xent Color 2.3023(2.3026) | Loss 3.2370(3.4274) | Error 0.8878(0.8882) | Error Color 0.9067(0.9033) |Steps 386(393.10) | Grad Norm 1.6224(4.2371) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0030 | Time 58.4118, Epoch Time 720.6828(692.4626), Bit/dim 0.2313(best: 0.1242), Xent 2.3010, Xent Color 2.3025. Loss 1.3821, Error 0.8865(best: 0.8865), Error Color 0.8966(best: 0.8940)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1990 | Time 8.7927(9.4713) | Bit/dim 0.2283(0.2478) | Xent 2.3042(2.3014) | Xent Color 2.3025(2.3026) | Loss 3.1854(3.7426) | Error 0.9044(0.8895) | Error Color 0.9022(0.9028) |Steps 368(392.20) | Grad Norm 7.2885(4.8763) | Total Time 0.00(0.00)\n",
      "Iter 2000 | Time 8.9025(9.4488) | Bit/dim 0.2384(0.2435) | Xent 2.3003(2.3011) | Xent Color 2.3023(2.3026) | Loss 3.1978(3.6123) | Error 0.8800(0.8880) | Error Color 0.8811(0.9022) |Steps 368(387.88) | Grad Norm 13.6005(6.2467) | Total Time 0.00(0.00)\n",
      "Iter 2010 | Time 8.9652(9.3597) | Bit/dim 0.2789(0.2536) | Xent 2.2994(2.3010) | Xent Color 2.3025(2.3026) | Loss 3.2761(3.5364) | Error 0.8900(0.8880) | Error Color 0.8922(0.9015) |Steps 350(381.63) | Grad Norm 22.3209(9.5157) | Total Time 0.00(0.00)\n",
      "Iter 2020 | Time 9.1598(9.4024) | Bit/dim 0.2195(0.2507) | Xent 2.3028(2.3012) | Xent Color 2.3028(2.3026) | Loss 3.2075(3.4623) | Error 0.8922(0.8871) | Error Color 0.8989(0.9012) |Steps 374(380.59) | Grad Norm 5.8490(9.4656) | Total Time 0.00(0.00)\n",
      "Iter 2030 | Time 9.5026(9.3793) | Bit/dim 0.1986(0.2395) | Xent 2.3005(2.3013) | Xent Color 2.3022(2.3026) | Loss 3.1647(3.3875) | Error 0.8844(0.8872) | Error Color 0.8911(0.9003) |Steps 374(376.10) | Grad Norm 4.6661(8.3469) | Total Time 0.00(0.00)\n",
      "Iter 2040 | Time 9.4275(9.3970) | Bit/dim 0.1836(0.2262) | Xent 2.3022(2.3012) | Xent Color 2.3025(2.3026) | Loss 3.1940(3.3401) | Error 0.8967(0.8873) | Error Color 0.9022(0.8993) |Steps 350(379.27) | Grad Norm 1.9152(6.9112) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0031 | Time 57.9497, Epoch Time 699.8699(692.6848), Bit/dim 0.1755(best: 0.1242), Xent 2.3010, Xent Color 2.3025. Loss 1.3263, Error 0.8865(best: 0.8865), Error Color 0.9011(best: 0.8940)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2050 | Time 9.6054(9.3967) | Bit/dim 0.1772(0.2128) | Xent 2.2976(2.3014) | Xent Color 2.3026(2.3026) | Loss 3.1734(3.7524) | Error 0.8600(0.8880) | Error Color 0.8967(0.8991) |Steps 374(378.79) | Grad Norm 4.5179(5.7399) | Total Time 0.00(0.00)\n",
      "Iter 2060 | Time 9.7323(9.4332) | Bit/dim 0.1659(0.2018) | Xent 2.2993(2.3012) | Xent Color 2.3025(2.3026) | Loss 3.2155(3.6052) | Error 0.8811(0.8881) | Error Color 0.8978(0.8990) |Steps 410(384.84) | Grad Norm 7.9676(6.0181) | Total Time 0.00(0.00)\n",
      "Iter 2070 | Time 9.0575(9.4850) | Bit/dim 0.1785(0.1935) | Xent 2.3014(2.3012) | Xent Color 2.3026(2.3026) | Loss 3.1358(3.4994) | Error 0.8878(0.8879) | Error Color 0.9111(0.8988) |Steps 374(386.31) | Grad Norm 16.0371(7.2223) | Total Time 0.00(0.00)\n",
      "Iter 2080 | Time 10.2156(9.4369) | Bit/dim 0.2710(0.2058) | Xent 2.2993(2.3012) | Xent Color 2.3027(2.3026) | Loss 3.4515(3.4514) | Error 0.8856(0.8884) | Error Color 0.8989(0.8990) |Steps 416(383.34) | Grad Norm 29.5234(9.9101) | Total Time 0.00(0.00)\n",
      "Iter 2090 | Time 8.7018(9.2818) | Bit/dim 0.1956(0.2102) | Xent 2.3002(2.3013) | Xent Color 2.3025(2.3026) | Loss 3.1663(3.3934) | Error 0.8822(0.8888) | Error Color 0.8922(0.8990) |Steps 356(376.15) | Grad Norm 8.6736(10.1989) | Total Time 0.00(0.00)\n",
      "Iter 2100 | Time 9.3625(9.2290) | Bit/dim 0.1546(0.2005) | Xent 2.2983(2.3012) | Xent Color 2.3025(2.3026) | Loss 3.1726(3.3330) | Error 0.8722(0.8875) | Error Color 0.8922(0.8994) |Steps 368(374.29) | Grad Norm 3.8866(9.3523) | Total Time 0.00(0.00)\n",
      "Iter 2110 | Time 9.4445(9.2779) | Bit/dim 0.1469(0.1873) | Xent 2.3006(2.3011) | Xent Color 2.3024(2.3026) | Loss 3.1276(3.2841) | Error 0.8933(0.8868) | Error Color 0.8789(0.8995) |Steps 368(374.74) | Grad Norm 1.8689(8.1350) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0032 | Time 57.7558, Epoch Time 697.5380(692.8304), Bit/dim 0.1443(best: 0.1242), Xent 2.3010, Xent Color 2.3025. Loss 1.2952, Error 0.8865(best: 0.8865), Error Color 0.8941(best: 0.8940)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2120 | Time 9.4311(9.3048) | Bit/dim 0.1346(0.1744) | Xent 2.3022(2.3013) | Xent Color 2.3027(2.3026) | Loss 3.0707(3.6578) | Error 0.8933(0.8882) | Error Color 0.8944(0.8997) |Steps 392(375.85) | Grad Norm 3.9035(6.9670) | Total Time 0.00(0.00)\n",
      "Iter 2130 | Time 9.2237(9.3267) | Bit/dim 0.1280(0.1637) | Xent 2.2997(2.3014) | Xent Color 2.3027(2.3026) | Loss 3.1476(3.5258) | Error 0.8789(0.8886) | Error Color 0.9022(0.9006) |Steps 392(378.21) | Grad Norm 4.9656(6.2954) | Total Time 0.00(0.00)\n",
      "Iter 2140 | Time 9.4703(9.3827) | Bit/dim 0.1762(0.1580) | Xent 2.2994(2.3012) | Xent Color 2.3024(2.3026) | Loss 3.2215(3.4222) | Error 0.8878(0.8881) | Error Color 0.9011(0.9002) |Steps 386(380.84) | Grad Norm 29.0995(8.1015) | Total Time 0.00(0.00)\n",
      "Iter 2150 | Time 9.9952(9.4647) | Bit/dim 0.1640(0.1605) | Xent 2.3032(2.3012) | Xent Color 2.3025(2.3026) | Loss 3.2662(3.3656) | Error 0.8978(0.8876) | Error Color 0.8911(0.9014) |Steps 416(384.32) | Grad Norm 15.2399(10.9988) | Total Time 0.00(0.00)\n",
      "Iter 2160 | Time 9.8471(9.5397) | Bit/dim 0.1448(0.1587) | Xent 2.3008(2.3012) | Xent Color 2.3026(2.3026) | Loss 3.2221(3.3309) | Error 0.8878(0.8874) | Error Color 0.9000(0.9026) |Steps 386(387.46) | Grad Norm 8.0537(11.0083) | Total Time 0.00(0.00)\n",
      "Iter 2170 | Time 9.7093(9.5360) | Bit/dim 0.1279(0.1525) | Xent 2.2977(2.3011) | Xent Color 2.3023(2.3026) | Loss 3.1813(3.2968) | Error 0.8744(0.8874) | Error Color 0.8956(0.9024) |Steps 404(389.91) | Grad Norm 5.7732(9.9055) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0033 | Time 60.1373, Epoch Time 714.1810(693.4709), Bit/dim 0.1206(best: 0.1242), Xent 2.3010, Xent Color 2.3026. Loss 1.2715, Error 0.8865(best: 0.8865), Error Color 0.9014(best: 0.8940)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2180 | Time 10.0100(9.5416) | Bit/dim 0.1258(0.1454) | Xent 2.3012(2.3011) | Xent Color 2.3028(2.3026) | Loss 3.2225(3.7981) | Error 0.8989(0.8872) | Error Color 0.9100(0.9027) |Steps 422(389.65) | Grad Norm 3.6409(8.4347) | Total Time 0.00(0.00)\n",
      "Iter 2190 | Time 9.8485(9.4905) | Bit/dim 0.1202(0.1381) | Xent 2.2992(2.3011) | Xent Color 2.3023(2.3026) | Loss 3.1727(3.6222) | Error 0.8756(0.8878) | Error Color 0.8833(0.9015) |Steps 398(388.95) | Grad Norm 4.7621(7.1588) | Total Time 0.00(0.00)\n",
      "Iter 2200 | Time 8.8652(9.5199) | Bit/dim 0.1546(0.1337) | Xent 2.3007(2.3012) | Xent Color 2.3026(2.3026) | Loss 3.1646(3.4968) | Error 0.8878(0.8886) | Error Color 0.8933(0.9003) |Steps 374(386.62) | Grad Norm 18.1414(7.0249) | Total Time 0.00(0.00)\n",
      "Iter 2210 | Time 9.9191(9.5797) | Bit/dim 0.1790(0.1458) | Xent 2.3009(2.3010) | Xent Color 2.3025(2.3026) | Loss 3.1661(3.4310) | Error 0.8744(0.8869) | Error Color 0.8889(0.9004) |Steps 368(386.92) | Grad Norm 26.0389(10.4646) | Total Time 0.00(0.00)\n",
      "Iter 2220 | Time 8.6720(9.5184) | Bit/dim 0.1252(0.1465) | Xent 2.3025(2.3014) | Xent Color 2.3027(2.3026) | Loss 3.0823(3.3646) | Error 0.8989(0.8871) | Error Color 0.9011(0.9003) |Steps 386(387.13) | Grad Norm 4.9821(10.8092) | Total Time 0.00(0.00)\n",
      "Iter 2230 | Time 9.4059(9.4581) | Bit/dim 0.1215(0.1404) | Xent 2.2984(2.3011) | Xent Color 2.3025(2.3025) | Loss 3.1806(3.3037) | Error 0.8622(0.8859) | Error Color 0.8944(0.8986) |Steps 374(384.59) | Grad Norm 5.0723(9.6412) | Total Time 0.00(0.00)\n",
      "Iter 2240 | Time 9.5538(9.4080) | Bit/dim 0.1098(0.1325) | Xent 2.3014(2.3011) | Xent Color 2.3027(2.3025) | Loss 3.1520(3.2376) | Error 0.9033(0.8870) | Error Color 0.9033(0.8974) |Steps 398(382.24) | Grad Norm 6.2015(8.5349) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0034 | Time 56.3584, Epoch Time 703.4692(693.7709), Bit/dim 0.1065(best: 0.1206), Xent 2.3010, Xent Color 2.3026. Loss 1.2574, Error 0.8865(best: 0.8865), Error Color 0.8982(best: 0.8940)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2250 | Time 9.4264(9.3708) | Bit/dim 0.1093(0.1259) | Xent 2.3002(2.3010) | Xent Color 2.3028(2.3026) | Loss 3.0048(3.6100) | Error 0.8811(0.8872) | Error Color 0.9189(0.8980) |Steps 356(380.91) | Grad Norm 7.7460(7.9236) | Total Time 0.00(0.00)\n",
      "Iter 2260 | Time 8.7097(9.3498) | Bit/dim 0.1029(0.1214) | Xent 2.3000(2.3010) | Xent Color 2.3033(2.3026) | Loss 2.9934(3.4623) | Error 0.8822(0.8883) | Error Color 0.9278(0.8978) |Steps 362(377.98) | Grad Norm 7.4320(8.0862) | Total Time 0.00(0.00)\n",
      "Iter 2270 | Time 9.3380(9.3951) | Bit/dim 0.0953(0.1162) | Xent 2.3026(2.3011) | Xent Color 2.3026(2.3026) | Loss 2.9829(3.3535) | Error 0.9056(0.8873) | Error Color 0.8989(0.8983) |Steps 392(376.72) | Grad Norm 3.1955(7.5737) | Total Time 0.00(0.00)\n",
      "Iter 2280 | Time 9.2111(9.4110) | Bit/dim 0.1546(0.1163) | Xent 2.3041(2.3012) | Xent Color 2.3025(2.3026) | Loss 3.1030(3.2765) | Error 0.8944(0.8878) | Error Color 0.8978(0.8982) |Steps 368(375.66) | Grad Norm 28.2426(8.9698) | Total Time 0.00(0.00)\n",
      "Iter 2290 | Time 9.8158(9.4449) | Bit/dim 0.1286(0.1253) | Xent 2.3016(2.3012) | Xent Color 2.3027(2.3026) | Loss 3.1787(3.2536) | Error 0.8878(0.8873) | Error Color 0.9000(0.8987) |Steps 404(380.18) | Grad Norm 10.8888(11.1385) | Total Time 0.00(0.00)\n",
      "Iter 2300 | Time 9.3814(9.4367) | Bit/dim 0.1111(0.1242) | Xent 2.3017(2.3015) | Xent Color 2.3028(2.3026) | Loss 3.1008(3.2232) | Error 0.8900(0.8889) | Error Color 0.9056(0.9000) |Steps 374(381.80) | Grad Norm 7.2647(11.2109) | Total Time 0.00(0.00)\n",
      "Iter 2310 | Time 9.6310(9.4524) | Bit/dim 0.1340(0.1263) | Xent 2.2996(2.3013) | Xent Color 2.3026(2.3026) | Loss 3.1934(3.1995) | Error 0.8856(0.8879) | Error Color 0.9067(0.9003) |Steps 374(381.07) | Grad Norm 11.0146(11.9432) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0035 | Time 55.7961, Epoch Time 703.8237(694.0725), Bit/dim 0.1304(best: 0.1065), Xent 2.3010, Xent Color 2.3025. Loss 1.2813, Error 0.8865(best: 0.8865), Error Color 0.8993(best: 0.8940)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2320 | Time 9.2926(9.4238) | Bit/dim 0.1019(0.1234) | Xent 2.3030(2.3014) | Xent Color 2.3028(2.3026) | Loss 2.9323(3.5129) | Error 0.9000(0.8886) | Error Color 0.8933(0.8996) |Steps 374(380.52) | Grad Norm 6.4851(11.3857) | Total Time 0.00(0.00)\n",
      "Iter 2330 | Time 9.5734(9.4460) | Bit/dim 0.0995(0.1185) | Xent 2.3010(2.3013) | Xent Color 2.3026(2.3026) | Loss 3.0473(3.3959) | Error 0.8733(0.8885) | Error Color 0.8900(0.8979) |Steps 362(380.54) | Grad Norm 5.8079(10.1725) | Total Time 0.00(0.00)\n",
      "Iter 2340 | Time 9.5168(9.4273) | Bit/dim 0.0974(0.1129) | Xent 2.3021(2.3011) | Xent Color 2.3026(2.3026) | Loss 3.0444(3.2981) | Error 0.8944(0.8871) | Error Color 0.8989(0.8985) |Steps 380(380.65) | Grad Norm 4.7873(8.9469) | Total Time 0.00(0.00)\n",
      "Iter 2350 | Time 9.6155(9.4493) | Bit/dim 0.1421(0.1099) | Xent 2.3005(2.3012) | Xent Color 2.3024(2.3026) | Loss 3.0384(3.2295) | Error 0.8800(0.8877) | Error Color 0.8900(0.8989) |Steps 338(377.70) | Grad Norm 16.6671(8.5587) | Total Time 0.00(0.00)\n",
      "Iter 2360 | Time 9.2865(9.4979) | Bit/dim 0.1417(0.1110) | Xent 2.3004(2.3012) | Xent Color 2.3026(2.3026) | Loss 3.1127(3.1934) | Error 0.8789(0.8869) | Error Color 0.8989(0.8987) |Steps 368(378.48) | Grad Norm 26.5389(9.6434) | Total Time 0.00(0.00)\n",
      "Iter 2370 | Time 9.9242(9.5065) | Bit/dim 0.3120(0.1278) | Xent 2.3011(2.3011) | Xent Color 2.3024(2.3026) | Loss 3.5858(3.2040) | Error 0.8978(0.8875) | Error Color 0.8878(0.8986) |Steps 398(381.56) | Grad Norm 15.2039(11.5825) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0036 | Time 56.9111, Epoch Time 708.3124(694.4997), Bit/dim 0.1921(best: 0.1065), Xent 2.3010, Xent Color 2.3026. Loss 1.3430, Error 0.8865(best: 0.8865), Error Color 0.9039(best: 0.8940)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2380 | Time 9.7631(9.5116) | Bit/dim 0.1782(0.1480) | Xent 2.2993(2.3012) | Xent Color 2.3026(2.3026) | Loss 3.2256(3.7026) | Error 0.8644(0.8870) | Error Color 0.8900(0.8988) |Steps 410(381.98) | Grad Norm 12.0651(11.5463) | Total Time 0.00(0.00)\n",
      "Iter 2390 | Time 9.1142(9.4952) | Bit/dim 0.1229(0.1465) | Xent 2.2998(2.3011) | Xent Color 2.3024(2.3026) | Loss 3.0963(3.5696) | Error 0.8722(0.8866) | Error Color 0.8889(0.9004) |Steps 362(384.56) | Grad Norm 4.2534(10.3500) | Total Time 0.00(0.00)\n",
      "Iter 2400 | Time 9.8343(9.4634) | Bit/dim 0.1108(0.1379) | Xent 2.3004(2.3011) | Xent Color 2.3026(2.3026) | Loss 3.1079(3.4393) | Error 0.8778(0.8866) | Error Color 0.8989(0.9011) |Steps 416(382.23) | Grad Norm 8.1412(8.8758) | Total Time 0.00(0.00)\n",
      "Iter 2410 | Time 9.6864(9.4725) | Bit/dim 0.1185(0.1307) | Xent 2.3009(2.3012) | Xent Color 2.3026(2.3026) | Loss 3.1202(3.3433) | Error 0.8867(0.8870) | Error Color 0.9033(0.9014) |Steps 380(380.48) | Grad Norm 10.6815(8.7367) | Total Time 0.00(0.00)\n",
      "Iter 2420 | Time 9.9918(9.3966) | Bit/dim 0.1256(0.1312) | Xent 2.3016(2.3012) | Xent Color 2.3027(2.3026) | Loss 3.1341(3.2802) | Error 0.8967(0.8878) | Error Color 0.9044(0.9005) |Steps 434(379.41) | Grad Norm 13.5195(10.0432) | Total Time 0.00(0.00)\n",
      "Iter 2430 | Time 9.4227(9.4633) | Bit/dim 0.1148(0.1269) | Xent 2.3010(2.3013) | Xent Color 2.3027(2.3026) | Loss 3.0865(3.2320) | Error 0.8911(0.8881) | Error Color 0.8989(0.8991) |Steps 398(383.08) | Grad Norm 8.1247(9.7113) | Total Time 0.00(0.00)\n",
      "Iter 2440 | Time 9.4442(9.4896) | Bit/dim 0.0920(0.1195) | Xent 2.3030(2.3011) | Xent Color 2.3028(2.3026) | Loss 3.0716(3.1843) | Error 0.9033(0.8884) | Error Color 0.9100(0.8994) |Steps 398(382.72) | Grad Norm 4.1220(8.6783) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0037 | Time 59.7962, Epoch Time 708.4041(694.9168), Bit/dim 0.0947(best: 0.1065), Xent 2.3010, Xent Color 2.3026. Loss 1.2456, Error 0.8865(best: 0.8865), Error Color 0.9044(best: 0.8940)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2450 | Time 9.0974(9.4552) | Bit/dim 0.0924(0.1125) | Xent 2.3004(2.3013) | Xent Color 2.3026(2.3026) | Loss 2.9560(3.5477) | Error 0.8767(0.8886) | Error Color 0.8989(0.8989) |Steps 368(381.08) | Grad Norm 7.1214(7.9727) | Total Time 0.00(0.00)\n",
      "Iter 2460 | Time 9.3579(9.4705) | Bit/dim 0.0912(0.1082) | Xent 2.2988(2.3012) | Xent Color 2.3025(2.3026) | Loss 3.0186(3.4140) | Error 0.8733(0.8885) | Error Color 0.8922(0.8986) |Steps 374(381.30) | Grad Norm 7.8447(8.0232) | Total Time 0.00(0.00)\n",
      "Iter 2470 | Time 9.2210(9.4374) | Bit/dim 0.0960(0.1032) | Xent 2.2996(2.3010) | Xent Color 2.3028(2.3026) | Loss 2.9335(3.3044) | Error 0.8733(0.8871) | Error Color 0.9144(0.8972) |Steps 350(380.26) | Grad Norm 11.1511(7.3114) | Total Time 0.00(0.00)\n",
      "Iter 2480 | Time 9.0853(9.3823) | Bit/dim 0.1186(0.1064) | Xent 2.3055(2.3011) | Xent Color 2.3025(2.3026) | Loss 3.0522(3.2410) | Error 0.9078(0.8867) | Error Color 0.9044(0.8981) |Steps 374(378.21) | Grad Norm 10.6521(8.6593) | Total Time 0.00(0.00)\n",
      "Iter 2490 | Time 9.7229(9.4597) | Bit/dim 0.1188(0.1089) | Xent 2.3039(2.3011) | Xent Color 2.3025(2.3026) | Loss 3.0882(3.1944) | Error 0.8922(0.8869) | Error Color 0.8933(0.8975) |Steps 386(377.14) | Grad Norm 11.8639(9.1060) | Total Time 0.00(0.00)\n",
      "Iter 2500 | Time 9.2572(9.4698) | Bit/dim 0.0990(0.1062) | Xent 2.2969(2.3010) | Xent Color 2.3027(2.3026) | Loss 2.9787(3.1492) | Error 0.8633(0.8862) | Error Color 0.9011(0.8980) |Steps 368(378.76) | Grad Norm 8.0953(8.4195) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0038 | Time 55.5969, Epoch Time 703.4378(695.1724), Bit/dim 0.0903(best: 0.0947), Xent 2.3010, Xent Color 2.3027. Loss 1.2412, Error 0.8865(best: 0.8865), Error Color 0.9020(best: 0.8940)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2510 | Time 9.1209(9.4394) | Bit/dim 0.0865(0.1018) | Xent 2.3024(2.3014) | Xent Color 2.3026(2.3026) | Loss 3.0240(3.5593) | Error 0.8978(0.8888) | Error Color 0.9011(0.9000) |Steps 374(379.40) | Grad Norm 3.6432(7.6424) | Total Time 0.00(0.00)\n",
      "Iter 2520 | Time 9.2654(9.3892) | Bit/dim 0.0817(0.0973) | Xent 2.3025(2.3014) | Xent Color 2.3025(2.3026) | Loss 2.9597(3.4029) | Error 0.8944(0.8882) | Error Color 0.8944(0.8993) |Steps 362(376.46) | Grad Norm 4.9139(7.1760) | Total Time 0.00(0.00)\n",
      "Iter 2530 | Time 9.3020(9.3769) | Bit/dim 0.0817(0.0932) | Xent 2.3037(2.3012) | Xent Color 2.3022(2.3026) | Loss 2.9296(3.2883) | Error 0.8933(0.8875) | Error Color 0.8778(0.8978) |Steps 362(374.20) | Grad Norm 2.9696(6.3458) | Total Time 0.00(0.00)\n",
      "Iter 2540 | Time 9.7052(9.3737) | Bit/dim 0.1421(0.0923) | Xent 2.2985(2.3009) | Xent Color 2.3023(2.3026) | Loss 3.1871(3.2079) | Error 0.8644(0.8855) | Error Color 0.8922(0.8983) |Steps 386(370.77) | Grad Norm 16.7609(6.5556) | Total Time 0.00(0.00)\n",
      "Iter 2550 | Time 9.8988(9.3848) | Bit/dim 0.2671(0.1172) | Xent 2.3049(2.3013) | Xent Color 2.3024(2.3026) | Loss 3.4328(3.2110) | Error 0.9111(0.8878) | Error Color 0.8900(0.8996) |Steps 398(373.68) | Grad Norm 50.4724(10.7457) | Total Time 0.00(0.00)\n",
      "Iter 2560 | Time 9.4209(9.3865) | Bit/dim 0.2102(0.1514) | Xent 2.3025(2.3013) | Xent Color 2.3027(2.3026) | Loss 3.2754(3.2512) | Error 0.8944(0.8878) | Error Color 0.9122(0.9008) |Steps 398(376.76) | Grad Norm 3.7201(10.3742) | Total Time 0.00(0.00)\n",
      "Iter 2570 | Time 9.9608(9.4207) | Bit/dim 0.1483(0.1573) | Xent 2.3019(2.3014) | Xent Color 2.3026(2.3026) | Loss 3.2156(3.2519) | Error 0.8944(0.8889) | Error Color 0.8867(0.9006) |Steps 392(380.74) | Grad Norm 6.8890(9.3207) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0039 | Time 60.6666, Epoch Time 704.0350(695.4383), Bit/dim 0.1354(best: 0.0903), Xent 2.3010, Xent Color 2.3026. Loss 1.2864, Error 0.8865(best: 0.8865), Error Color 0.9035(best: 0.8940)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2580 | Time 9.2153(9.3948) | Bit/dim 0.1237(0.1509) | Xent 2.3005(2.3012) | Xent Color 2.3026(2.3026) | Loss 3.2124(3.7171) | Error 0.8833(0.8876) | Error Color 0.8911(0.9001) |Steps 368(382.25) | Grad Norm 5.1149(8.3699) | Total Time 0.00(0.00)\n",
      "Iter 2590 | Time 9.1570(9.3144) | Bit/dim 0.1140(0.1428) | Xent 2.3019(2.3011) | Xent Color 2.3029(2.3026) | Loss 3.1557(3.5716) | Error 0.8933(0.8875) | Error Color 0.9300(0.9011) |Steps 368(381.83) | Grad Norm 3.2618(7.2067) | Total Time 0.00(0.00)\n",
      "Iter 2600 | Time 8.7937(9.2441) | Bit/dim 0.1035(0.1337) | Xent 2.3041(2.3013) | Xent Color 2.3025(2.3026) | Loss 3.1050(3.4511) | Error 0.9067(0.8882) | Error Color 0.9089(0.9006) |Steps 362(379.18) | Grad Norm 2.4181(5.8938) | Total Time 0.00(0.00)\n",
      "Iter 2610 | Time 8.8632(9.2027) | Bit/dim 0.0987(0.1254) | Xent 2.3002(2.3013) | Xent Color 2.3028(2.3026) | Loss 3.1029(3.3591) | Error 0.8667(0.8870) | Error Color 0.8789(0.9001) |Steps 386(378.50) | Grad Norm 0.7697(4.7898) | Total Time 0.00(0.00)\n",
      "Iter 2620 | Time 9.6321(9.1879) | Bit/dim 0.1073(0.1193) | Xent 2.3022(2.3012) | Xent Color 2.3027(2.3026) | Loss 3.0924(3.2874) | Error 0.8911(0.8863) | Error Color 0.9056(0.8995) |Steps 392(376.70) | Grad Norm 11.8203(5.1827) | Total Time 0.00(0.00)\n",
      "Iter 2630 | Time 9.4533(9.1702) | Bit/dim 0.2256(0.1250) | Xent 2.3029(2.3014) | Xent Color 2.3030(2.3026) | Loss 3.3252(3.2550) | Error 0.8944(0.8873) | Error Color 0.9067(0.8997) |Steps 386(377.37) | Grad Norm 13.4764(7.4666) | Total Time 0.00(0.00)\n",
      "Iter 2640 | Time 9.3025(9.2344) | Bit/dim 0.1584(0.1345) | Xent 2.3010(2.3012) | Xent Color 2.3027(2.3026) | Loss 3.1885(3.2555) | Error 0.8867(0.8889) | Error Color 0.9044(0.8997) |Steps 380(380.67) | Grad Norm 23.2606(9.2413) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0040 | Time 63.8558, Epoch Time 693.4250(695.3779), Bit/dim 0.1695(best: 0.0903), Xent 2.3010, Xent Color 2.3027. Loss 1.3204, Error 0.8865(best: 0.8865), Error Color 0.9028(best: 0.8940)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2650 | Time 9.3359(9.3125) | Bit/dim 0.1194(0.1354) | Xent 2.3026(2.3012) | Xent Color 2.3025(2.3026) | Loss 3.2988(3.7250) | Error 0.8833(0.8883) | Error Color 0.8967(0.8999) |Steps 404(384.54) | Grad Norm 6.7152(8.9302) | Total Time 0.00(0.00)\n",
      "Iter 2660 | Time 9.4873(9.3762) | Bit/dim 0.0941(0.1262) | Xent 2.3014(2.3011) | Xent Color 2.3026(2.3026) | Loss 3.2018(3.5910) | Error 0.8944(0.8878) | Error Color 0.8911(0.9000) |Steps 386(386.74) | Grad Norm 4.5695(7.9791) | Total Time 0.00(0.00)\n",
      "Iter 2670 | Time 9.6861(9.4205) | Bit/dim 0.0845(0.1160) | Xent 2.3038(2.3012) | Xent Color 2.3026(2.3026) | Loss 3.1630(3.4861) | Error 0.9078(0.8880) | Error Color 0.9111(0.8998) |Steps 398(390.73) | Grad Norm 3.9826(6.8777) | Total Time 0.00(0.00)\n",
      "Iter 2680 | Time 9.7454(9.4893) | Bit/dim 0.0768(0.1065) | Xent 2.2977(2.3013) | Xent Color 2.3026(2.3026) | Loss 3.1293(3.3977) | Error 0.8789(0.8889) | Error Color 0.8933(0.8986) |Steps 410(393.45) | Grad Norm 3.2200(5.7904) | Total Time 0.00(0.00)\n",
      "Iter 2690 | Time 9.7699(9.5003) | Bit/dim 0.0783(0.0990) | Xent 2.3013(2.3012) | Xent Color 2.3024(2.3026) | Loss 3.1452(3.3261) | Error 0.8989(0.8879) | Error Color 0.8900(0.8973) |Steps 404(395.68) | Grad Norm 2.8919(4.8388) | Total Time 0.00(0.00)\n",
      "Iter 2700 | Time 9.8971(9.5416) | Bit/dim 0.0770(0.0935) | Xent 2.3012(2.3013) | Xent Color 2.3026(2.3026) | Loss 3.1720(3.2755) | Error 0.8856(0.8885) | Error Color 0.8944(0.8970) |Steps 374(395.25) | Grad Norm 3.9108(4.7657) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0041 | Time 61.8860, Epoch Time 720.8789(696.1429), Bit/dim 0.0798(best: 0.0903), Xent 2.3010, Xent Color 2.3027. Loss 1.2307, Error 0.8865(best: 0.8865), Error Color 0.9008(best: 0.8940)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2710 | Time 9.2593(9.5833) | Bit/dim 0.0875(0.0903) | Xent 2.2999(2.3011) | Xent Color 2.3023(2.3026) | Loss 3.1570(3.7700) | Error 0.8733(0.8869) | Error Color 0.8856(0.8983) |Steps 380(398.07) | Grad Norm 10.2402(5.3654) | Total Time 0.00(0.00)\n",
      "Iter 2720 | Time 9.1677(9.6092) | Bit/dim 0.2707(0.1065) | Xent 2.3029(2.3008) | Xent Color 2.3027(2.3026) | Loss 3.4265(3.6280) | Error 0.8900(0.8865) | Error Color 0.9044(0.8997) |Steps 362(396.70) | Grad Norm 17.5524(8.8596) | Total Time 0.00(0.00)\n",
      "Iter 2730 | Time 9.2659(9.6076) | Bit/dim 0.1781(0.1377) | Xent 2.2981(2.3008) | Xent Color 2.3028(2.3026) | Loss 3.3684(3.5597) | Error 0.8700(0.8863) | Error Color 0.9000(0.8994) |Steps 386(396.59) | Grad Norm 9.3083(10.4961) | Total Time 0.00(0.00)\n",
      "Iter 2740 | Time 9.7268(9.6677) | Bit/dim 0.1349(0.1503) | Xent 2.3027(2.3009) | Xent Color 2.3024(2.3026) | Loss 3.1291(3.4893) | Error 0.8956(0.8866) | Error Color 0.8989(0.9001) |Steps 386(397.48) | Grad Norm 2.9804(10.5761) | Total Time 0.00(0.00)\n",
      "Iter 2750 | Time 9.4095(9.6576) | Bit/dim 0.1102(0.1431) | Xent 2.3029(2.3011) | Xent Color 2.3028(2.3026) | Loss 3.1286(3.4086) | Error 0.9033(0.8871) | Error Color 0.8833(0.8974) |Steps 428(397.76) | Grad Norm 5.7305(8.9900) | Total Time 0.00(0.00)\n",
      "Iter 2760 | Time 9.6158(9.6019) | Bit/dim 0.0940(0.1318) | Xent 2.3035(2.3013) | Xent Color 2.3025(2.3026) | Loss 3.1262(3.3291) | Error 0.8944(0.8892) | Error Color 0.8867(0.8976) |Steps 368(396.68) | Grad Norm 3.5862(7.5089) | Total Time 0.00(0.00)\n",
      "Iter 2770 | Time 9.5379(9.5280) | Bit/dim 0.0827(0.1201) | Xent 2.3040(2.3015) | Xent Color 2.3028(2.3026) | Loss 2.9650(3.2472) | Error 0.8933(0.8886) | Error Color 0.9189(0.8989) |Steps 356(390.45) | Grad Norm 2.0962(6.1037) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0042 | Time 56.0167, Epoch Time 712.4170(696.6312), Bit/dim 0.0822(best: 0.0798), Xent 2.3010, Xent Color 2.3027. Loss 1.2331, Error 0.8865(best: 0.8865), Error Color 0.9043(best: 0.8940)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2780 | Time 10.0151(9.4315) | Bit/dim 0.0776(0.1097) | Xent 2.3062(2.3014) | Xent Color 2.3029(2.3026) | Loss 3.0323(3.5715) | Error 0.8967(0.8885) | Error Color 0.9111(0.9003) |Steps 332(383.80) | Grad Norm 1.2786(4.9054) | Total Time 0.00(0.00)\n",
      "Iter 2790 | Time 9.6766(9.3486) | Bit/dim 0.0750(0.1009) | Xent 2.3028(2.3013) | Xent Color 2.3026(2.3026) | Loss 2.9641(3.4118) | Error 0.8889(0.8880) | Error Color 0.9033(0.8999) |Steps 398(378.28) | Grad Norm 0.7356(3.8739) | Total Time 0.00(0.00)\n",
      "Iter 2800 | Time 9.7014(9.3052) | Bit/dim 0.0746(0.0939) | Xent 2.3003(2.3013) | Xent Color 2.3020(2.3026) | Loss 2.9253(3.2877) | Error 0.8867(0.8889) | Error Color 0.8700(0.8981) |Steps 338(373.37) | Grad Norm 3.0920(3.3640) | Total Time 0.00(0.00)\n",
      "Iter 2810 | Time 8.9810(9.2904) | Bit/dim 0.0797(0.0885) | Xent 2.2996(2.3013) | Xent Color 2.3026(2.3026) | Loss 2.9319(3.1955) | Error 0.8733(0.8882) | Error Color 0.8967(0.8998) |Steps 338(368.86) | Grad Norm 7.4425(3.3133) | Total Time 0.00(0.00)\n",
      "Iter 2820 | Time 8.4857(9.2915) | Bit/dim 0.1000(0.0917) | Xent 2.3023(2.3013) | Xent Color 2.3026(2.3026) | Loss 2.9112(3.1483) | Error 0.8978(0.8871) | Error Color 0.8833(0.9005) |Steps 338(368.27) | Grad Norm 12.0781(5.9246) | Total Time 0.00(0.00)\n",
      "Iter 2830 | Time 9.3333(9.3062) | Bit/dim 0.1622(0.1091) | Xent 2.3004(2.3015) | Xent Color 2.3026(2.3026) | Loss 3.1921(3.1488) | Error 0.8844(0.8886) | Error Color 0.9000(0.8991) |Steps 380(368.85) | Grad Norm 14.6787(9.0964) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0043 | Time 55.8363, Epoch Time 689.9510(696.4307), Bit/dim 0.0904(best: 0.0798), Xent 2.3010, Xent Color 2.3027. Loss 1.2414, Error 0.8865(best: 0.8865), Error Color 0.8997(best: 0.8940)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2840 | Time 9.0968(9.2682) | Bit/dim 0.1011(0.1096) | Xent 2.3002(2.3011) | Xent Color 2.3027(2.3026) | Loss 3.0718(3.6229) | Error 0.8833(0.8869) | Error Color 0.9067(0.8993) |Steps 380(369.57) | Grad Norm 9.8223(9.2463) | Total Time 0.00(0.00)\n",
      "Iter 2850 | Time 9.2571(9.2508) | Bit/dim 0.0814(0.1044) | Xent 2.2998(2.3012) | Xent Color 2.3029(2.3027) | Loss 2.9602(3.4636) | Error 0.8811(0.8874) | Error Color 0.9100(0.9005) |Steps 380(371.16) | Grad Norm 4.9394(8.5429) | Total Time 0.00(0.00)\n",
      "Iter 2860 | Time 8.9644(9.1941) | Bit/dim 0.0798(0.0986) | Xent 2.3008(2.3012) | Xent Color 2.3026(2.3026) | Loss 3.0338(3.3478) | Error 0.8844(0.8878) | Error Color 0.9033(0.9004) |Steps 380(369.08) | Grad Norm 4.1872(7.6873) | Total Time 0.00(0.00)\n",
      "Iter 2870 | Time 9.4275(9.1596) | Bit/dim 0.0771(0.0926) | Xent 2.3020(2.3013) | Xent Color 2.3025(2.3026) | Loss 2.9671(3.2491) | Error 0.8900(0.8882) | Error Color 0.8900(0.9010) |Steps 368(368.81) | Grad Norm 2.0353(6.5145) | Total Time 0.00(0.00)\n",
      "Iter 2880 | Time 8.4475(9.1554) | Bit/dim 0.0729(0.0876) | Xent 2.3004(2.3012) | Xent Color 2.3027(2.3026) | Loss 2.9520(3.1766) | Error 0.8756(0.8871) | Error Color 0.9078(0.9008) |Steps 368(366.94) | Grad Norm 2.0558(5.3121) | Total Time 0.00(0.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_rl_2cond.py --data colormnist --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/cnf_conditional_disentangle_colormnist_bs900_sratio_1_12th_drop_0_5_rl_stdscale_6_2cond_mlp_run1 --seed 1 --lr 0.001 --conditional True --controlled_tol False --train_mode semisup --log_freq 10 --weight_y 0.5 --condition_ratio 0.083333 --dropout_rate 0.5 --scale_fac 1.0 --scale_std 6.0 --cond_nn mlp\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
