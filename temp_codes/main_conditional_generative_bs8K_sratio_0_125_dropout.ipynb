{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_generative.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "if args.controlled_tol:\n",
      "    import lib.odenvp_conditional_tol as odenvp\n",
      "else:\n",
      "    import lib.odenvp_conditional as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    logpz_sup_per_dim = torch.sum(logpz_sup) / z[:, 0:dim_sup].nelement()  # averaged over batches\n",
      "    bits_sup_per_dim = -(logpz_sup_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute the predicted labels\n",
      "    all_y = torch.from_numpy(np.arange(model.module.y_class)).type(torch.long).to(x.get_device(), non_blocking=True)\n",
      "    all_y_onehot = thops.onehot(all_y, num_classes=model.module.y_class)\n",
      "    all_mean, all_logs = model.module._prior(all_y_onehot)\n",
      "    \n",
      "    zsup = zsup.view(zsup.shape[0], 1, zsup.shape[1])\n",
      "    zsup = zsup.repeat(1,model.module.y_class,1)\n",
      "    likelihood = modules.GaussianDiag.likelihood(all_mean, all_logs, zsup)\n",
      "    dims = [i+2 for i in range(len(likelihood.size()) - 2)]\n",
      "    all_logpz_sup = thops.sum(likelihood, dim=dims)\n",
      "    y_predicted = np.argmax(all_logpz_sup.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, bits_sup_per_dim, y_predicted\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    if args.conditional: best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            loss.backward()\n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}, Xent {:.4f}, Loss {:.4f}, Error {:.4f}\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, loss_xent, loss, error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=0.0001, autoencode=False, batch_norm=False, batch_size=8000, batch_size_schedule='', begin_epoch=1, condition_ratio=0.125, conditional=True, controlled_tol=True, conv=True, data='mnist', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=1, lr=0.01, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume=None, rtol=0.0001, save='../experiments/cnf_conditional_generative_bs8K_sratio_0_125_drop_0_5_seed_1', seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=5000, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=113.0, weight_decay=0.0, weight_y=0.5)\n",
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=196, bias=True)\n",
      "  (project_class): LinearZeros(in_features=98, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 804194\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc.py:286: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "Iter 0001 | Time 58.0590(58.0590) | Bit/dim 19.2733(19.2733) | Xent 27.5362(27.5362) | Loss 33.0414(33.0414) | Error 0.9021(0.9021) Steps 296(296.00) | Grad Norm 223.3186(223.3186) | Total Time 10.00(10.00)\n",
      "Iter 0002 | Time 21.4016(56.9592) | Bit/dim 15.1748(19.1504) | Xent 23.4478(27.4136) | Loss 26.8987(32.8571) | Error 0.5099(0.8904) Steps 302(296.18) | Grad Norm 149.1170(221.0926) | Total Time 10.00(10.00)\n",
      "Iter 0003 | Time 25.6577(56.0202) | Bit/dim 12.5646(18.9528) | Xent 19.8702(27.1873) | Loss 22.4997(32.5464) | Error 0.5328(0.8796) Steps 386(298.87) | Grad Norm 59.6894(216.2505) | Total Time 10.00(10.00)\n",
      "Iter 0004 | Time 26.8306(55.1445) | Bit/dim 12.1802(18.7496) | Xent 18.7685(26.9347) | Loss 21.5645(32.2170) | Error 0.5881(0.8709) Steps 404(302.03) | Grad Norm 63.6947(211.6738) | Total Time 10.00(10.00)\n",
      "Iter 0005 | Time 26.7977(54.2941) | Bit/dim 11.8942(18.5440) | Xent 19.9183(26.7242) | Loss 21.8533(31.9061) | Error 0.6450(0.8641) Steps 410(305.27) | Grad Norm 111.4667(208.6676) | Total Time 10.00(10.00)\n",
      "Iter 0006 | Time 26.7335(53.4673) | Bit/dim 10.2679(18.2957) | Xent 19.9973(26.5224) | Loss 20.2665(31.5569) | Error 0.6580(0.8579) Steps 410(308.41) | Grad Norm 108.3976(205.6595) | Total Time 10.00(10.00)\n",
      "Iter 0007 | Time 25.4124(52.6256) | Bit/dim 8.6119(18.0052) | Xent 18.2973(26.2756) | Loss 17.7606(31.1430) | Error 0.6264(0.8510) Steps 380(310.56) | Grad Norm 75.7327(201.7617) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0001 | Time 24.2495, Epoch Time 247.6400(247.6400), Bit/dim 7.9695, Xent 16.2277, Loss 16.0833, Error 0.5009\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0008 | Time 24.9278(51.7947) | Bit/dim 7.8848(17.7016) | Xent 16.1054(25.9705) | Loss 15.9375(30.6868) | Error 0.5991(0.8434) Steps 338(311.38) | Grad Norm 46.2513(197.0964) | Total Time 10.00(10.00)\n",
      "Iter 0009 | Time 23.3290(50.9407) | Bit/dim 7.6016(17.3986) | Xent 14.4851(25.6260) | Loss 14.8441(30.2115) | Error 0.5320(0.8341) Steps 344(312.36) | Grad Norm 39.0238(192.3542) | Total Time 10.00(10.00)\n",
      "Iter 0010 | Time 23.5097(50.1178) | Bit/dim 6.8602(17.0824) | Xent 13.4997(25.2622) | Loss 13.6100(29.7135) | Error 0.5050(0.8242) Steps 350(313.49) | Grad Norm 38.9856(187.7532) | Total Time 10.00(10.00)\n",
      "Iter 0011 | Time 24.3640(49.3452) | Bit/dim 5.6699(16.7400) | Xent 12.8719(24.8905) | Loss 12.1059(29.1853) | Error 0.5186(0.8150) Steps 356(314.76) | Grad Norm 34.1969(183.1465) | Total Time 10.00(10.00)\n",
      "Iter 0012 | Time 25.4695(48.6289) | Bit/dim 4.6305(16.3767) | Xent 12.3699(24.5149) | Loss 10.8154(28.6342) | Error 0.5488(0.8071) Steps 374(316.54) | Grad Norm 26.6718(178.4522) | Total Time 10.00(10.00)\n",
      "Iter 0013 | Time 25.8667(47.9460) | Bit/dim 3.9980(16.0054) | Xent 11.8360(24.1345) | Loss 9.9160(28.0726) | Error 0.5829(0.8003) Steps 386(318.62) | Grad Norm 21.2309(173.7356) | Total Time 10.00(10.00)\n",
      "Iter 0014 | Time 25.1619(47.2625) | Bit/dim 3.7658(15.6382) | Xent 11.3787(23.7518) | Loss 9.4552(27.5141) | Error 0.6009(0.7943) Steps 374(320.29) | Grad Norm 19.5336(169.1095) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0002 | Time 14.2212, Epoch Time 199.2538(246.1884), Bit/dim 3.5997, Xent 10.9969, Loss 9.0981, Error 0.4447\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0015 | Time 24.5566(46.5813) | Bit/dim 3.6076(15.2773) | Xent 11.0130(23.3696) | Loss 9.1141(26.9621) | Error 0.6285(0.7894) Steps 362(321.54) | Grad Norm 18.6872(164.5969) | Total Time 10.00(10.00)\n",
      "Iter 0016 | Time 27.2239(46.0006) | Bit/dim 3.3566(14.9197) | Xent 10.7100(22.9899) | Loss 8.7116(26.4146) | Error 0.6709(0.7858) Steps 398(323.83) | Grad Norm 16.0372(160.1401) | Total Time 10.00(10.00)\n",
      "Iter 0017 | Time 29.5046(45.5057) | Bit/dim 3.0654(14.5640) | Xent 10.5088(22.6154) | Loss 8.3198(25.8717) | Error 0.7145(0.7837) Steps 422(326.78) | Grad Norm 11.6100(155.6842) | Total Time 10.00(10.00)\n",
      "Iter 0018 | Time 30.1993(45.0466) | Bit/dim 2.8860(14.2137) | Xent 10.3634(22.2479) | Loss 8.0677(25.3376) | Error 0.7374(0.7823) Steps 434(329.99) | Grad Norm 7.0038(151.2238) | Total Time 10.00(10.00)\n",
      "Iter 0019 | Time 30.2762(44.6034) | Bit/dim 2.9194(13.8749) | Xent 10.2718(21.8886) | Loss 8.0553(24.8192) | Error 0.7776(0.7821) Steps 440(333.29) | Grad Norm 5.5657(146.8540) | Total Time 10.00(10.00)\n",
      "Iter 0020 | Time 33.9218(44.2830) | Bit/dim 3.0987(13.5516) | Xent 10.2116(21.5383) | Loss 8.2045(24.3207) | Error 0.8030(0.7828) Steps 458(337.03) | Grad Norm 7.9018(142.6854) | Total Time 10.00(10.00)\n",
      "Iter 0021 | Time 35.4409(44.0177) | Bit/dim 3.2903(13.2437) | Xent 10.1734(21.1973) | Loss 8.3770(23.8424) | Error 0.8300(0.7842) Steps 464(340.84) | Grad Norm 9.8959(138.7018) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0003 | Time 17.6964, Epoch Time 240.9625(246.0317), Bit/dim 3.3595, Xent 10.1180, Loss 8.4185, Error 0.8013\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0022 | Time 36.2346(43.7842) | Bit/dim 3.3684(12.9475) | Xent 10.1208(20.8650) | Loss 8.4288(23.3800) | Error 0.8435(0.7860) Steps 476(344.90) | Grad Norm 10.0319(134.8417) | Total Time 10.00(10.00)\n",
      "Iter 0023 | Time 33.3693(43.4718) | Bit/dim 3.3452(12.6594) | Xent 10.0529(20.5407) | Loss 8.3716(22.9297) | Error 0.8526(0.7880) Steps 458(348.29) | Grad Norm 8.8904(131.0631) | Total Time 10.00(10.00)\n",
      "Iter 0024 | Time 35.3304(43.2275) | Bit/dim 3.2885(12.3783) | Xent 9.9704(20.2236) | Loss 8.2737(22.4901) | Error 0.8462(0.7897) Steps 464(351.76) | Grad Norm 7.5740(127.3585) | Total Time 10.00(10.00)\n",
      "Iter 0025 | Time 35.4380(42.9939) | Bit/dim 3.2492(12.1044) | Xent 9.8964(19.9137) | Loss 8.1974(22.0613) | Error 0.8448(0.7914) Steps 464(355.13) | Grad Norm 6.6346(123.7367) | Total Time 10.00(10.00)\n",
      "Iter 0026 | Time 35.8073(42.7783) | Bit/dim 3.2190(11.8378) | Xent 9.8350(19.6114) | Loss 8.1365(21.6435) | Error 0.8254(0.7924) Steps 464(358.40) | Grad Norm 6.0686(120.2067) | Total Time 10.00(10.00)\n",
      "Iter 0027 | Time 38.8316(42.6599) | Bit/dim 3.1768(11.5780) | Xent 9.8036(19.3171) | Loss 8.0786(21.2366) | Error 0.8321(0.7936) Steps 488(362.28) | Grad Norm 5.8005(116.7745) | Total Time 10.00(10.00)\n",
      "Iter 0028 | Time 39.3651(42.5610) | Bit/dim 3.1337(11.3247) | Xent 9.7967(19.0315) | Loss 8.0321(20.8405) | Error 0.8197(0.7944) Steps 488(366.06) | Grad Norm 5.9088(113.4485) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0004 | Time 18.4833, Epoch Time 285.0617(247.2026), Bit/dim 3.0826, Xent 9.8111, Loss 7.9881, Error 0.8357\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0029 | Time 38.7468(42.4466) | Bit/dim 3.0848(11.0775) | Xent 9.8161(18.7551) | Loss 7.9929(20.4550) | Error 0.8167(0.7950) Steps 482(369.53) | Grad Norm 6.2608(110.2329) | Total Time 10.00(10.00)\n",
      "Iter 0030 | Time 36.5698(42.2703) | Bit/dim 3.0216(10.8358) | Xent 9.8419(18.4877) | Loss 7.9426(20.0797) | Error 0.8101(0.7955) Steps 476(372.73) | Grad Norm 6.4829(107.1204) | Total Time 10.00(10.00)\n",
      "Iter 0031 | Time 33.1107(41.9955) | Bit/dim 2.9234(10.5984) | Xent 9.8679(18.2291) | Loss 7.8573(19.7130) | Error 0.7929(0.7954) Steps 446(374.93) | Grad Norm 6.2017(104.0928) | Total Time 10.00(10.00)\n",
      "Iter 0032 | Time 32.2419(41.7029) | Bit/dim 2.8063(10.3647) | Xent 9.8842(17.9787) | Loss 7.7484(19.3540) | Error 0.7956(0.7954) Steps 428(376.52) | Grad Norm 5.4026(101.1321) | Total Time 10.00(10.00)\n",
      "Iter 0033 | Time 30.7474(41.3742) | Bit/dim 2.6761(10.1340) | Xent 9.9096(17.7367) | Loss 7.6309(19.0024) | Error 0.7722(0.7947) Steps 416(377.70) | Grad Norm 4.6441(98.2375) | Total Time 10.00(10.00)\n",
      "Iter 0034 | Time 29.7861(41.0266) | Bit/dim 2.5675(9.9070) | Xent 9.9451(17.5029) | Loss 7.5401(18.6585) | Error 0.7622(0.7937) Steps 398(378.31) | Grad Norm 4.3901(95.4221) | Total Time 10.00(10.00)\n",
      "Iter 0035 | Time 30.9830(40.7253) | Bit/dim 2.4634(9.6837) | Xent 9.9814(17.2773) | Loss 7.4541(18.3224) | Error 0.7511(0.7925) Steps 398(378.90) | Grad Norm 4.1957(92.6853) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0005 | Time 15.1767, Epoch Time 259.5266(247.5723), Bit/dim 2.3949, Xent 10.0131, Loss 7.4014, Error 0.5813\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0036 | Time 29.7921(40.3973) | Bit/dim 2.4022(9.4653) | Xent 10.0170(17.0595) | Loss 7.4107(17.9950) | Error 0.7292(0.7906) Steps 398(379.47) | Grad Norm 3.3197(90.0043) | Total Time 10.00(10.00)\n",
      "Iter 0037 | Time 26.8991(39.9923) | Bit/dim 2.3764(9.2526) | Xent 10.0335(16.8487) | Loss 7.3931(17.6769) | Error 0.7103(0.7882) Steps 380(379.49) | Grad Norm 2.6208(87.3828) | Total Time 10.00(10.00)\n",
      "Iter 0038 | Time 28.2793(39.6409) | Bit/dim 2.3706(9.0461) | Xent 10.0332(16.6442) | Loss 7.3872(17.3683) | Error 0.6730(0.7847) Steps 380(379.51) | Grad Norm 3.8199(84.8759) | Total Time 10.00(10.00)\n",
      "Iter 0039 | Time 29.0550(39.3234) | Bit/dim 2.3702(8.8459) | Xent 10.0181(16.4454) | Loss 7.3792(17.0686) | Error 0.6584(0.7809) Steps 374(379.34) | Grad Norm 4.6303(82.4685) | Total Time 10.00(10.00)\n",
      "Iter 0040 | Time 27.9996(38.9837) | Bit/dim 2.3607(8.6513) | Xent 9.9888(16.2517) | Loss 7.3550(16.7772) | Error 0.6524(0.7771) Steps 374(379.18) | Grad Norm 3.9791(80.1139) | Total Time 10.00(10.00)\n",
      "Iter 0041 | Time 26.7932(38.6179) | Bit/dim 2.3386(8.4619) | Xent 9.9517(16.0627) | Loss 7.3144(16.4933) | Error 0.6649(0.7737) Steps 368(378.85) | Grad Norm 3.8770(77.8268) | Total Time 10.00(10.00)\n",
      "Iter 0042 | Time 27.0450(38.2708) | Bit/dim 2.3291(8.2779) | Xent 9.9121(15.8782) | Loss 7.2851(16.2170) | Error 0.6719(0.7706) Steps 374(378.70) | Grad Norm 4.8146(75.6364) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0006 | Time 13.8301, Epoch Time 222.1364(246.8092), Bit/dim 2.3107, Xent 9.8585, Loss 7.2399, Error 0.5367\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0043 | Time 27.7770(37.9559) | Bit/dim 2.3170(8.0991) | Xent 9.8604(15.6977) | Loss 7.2472(15.9480) | Error 0.6751(0.7678) Steps 368(378.38) | Grad Norm 3.6001(73.4753) | Total Time 10.00(10.00)\n",
      "Iter 0044 | Time 26.2884(37.6059) | Bit/dim 2.2986(7.9251) | Xent 9.8137(15.5212) | Loss 7.2055(15.6857) | Error 0.6726(0.7649) Steps 362(377.89) | Grad Norm 2.2111(71.3374) | Total Time 10.00(10.00)\n",
      "Iter 0045 | Time 27.9516(37.3163) | Bit/dim 2.2853(7.7559) | Xent 9.7931(15.3493) | Loss 7.1819(15.4306) | Error 0.7009(0.7630) Steps 368(377.59) | Grad Norm 3.3364(69.2973) | Total Time 10.00(10.00)\n",
      "Iter 0046 | Time 27.3718(37.0180) | Bit/dim 2.2853(7.5918) | Xent 9.7801(15.1822) | Loss 7.1754(15.1829) | Error 0.7211(0.7617) Steps 374(377.48) | Grad Norm 3.4990(67.3234) | Total Time 10.00(10.00)\n",
      "Iter 0047 | Time 25.4196(36.6700) | Bit/dim 2.2736(7.4322) | Xent 9.7635(15.0197) | Loss 7.1553(14.9421) | Error 0.7225(0.7606) Steps 362(377.02) | Grad Norm 2.6688(65.3838) | Total Time 10.00(10.00)\n",
      "Iter 0048 | Time 26.1337(36.3539) | Bit/dim 2.2668(7.2773) | Xent 9.7526(14.8617) | Loss 7.1431(14.7081) | Error 0.7326(0.7597) Steps 362(376.57) | Grad Norm 3.1099(63.5155) | Total Time 10.00(10.00)\n",
      "Iter 0049 | Time 25.5222(36.0290) | Bit/dim 2.2393(7.1261) | Xent 9.7496(14.7083) | Loss 7.1141(14.4803) | Error 0.7221(0.7586) Steps 368(376.31) | Grad Norm 3.2637(61.7080) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0007 | Time 14.5204, Epoch Time 213.2970(245.8038), Bit/dim 2.2109, Xent 9.7583, Loss 7.0901, Error 0.7127\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0050 | Time 26.5016(35.7431) | Bit/dim 2.2108(6.9787) | Xent 9.7607(14.5599) | Loss 7.0912(14.2586) | Error 0.7085(0.7571) Steps 368(376.06) | Grad Norm 1.7672(59.9098) | Total Time 10.00(10.00)\n",
      "Iter 0051 | Time 25.1595(35.4256) | Bit/dim 2.1932(6.8351) | Xent 9.7816(14.4165) | Loss 7.0840(14.0434) | Error 0.6926(0.7552) Steps 362(375.64) | Grad Norm 2.9723(58.2016) | Total Time 10.00(10.00)\n",
      "Iter 0052 | Time 25.2841(35.1214) | Bit/dim 2.1805(6.6955) | Xent 9.8014(14.2781) | Loss 7.0812(13.8345) | Error 0.6903(0.7532) Steps 362(375.23) | Grad Norm 4.2526(56.5832) | Total Time 10.00(10.00)\n",
      "Iter 0053 | Time 24.8190(34.8123) | Bit/dim 2.1717(6.5598) | Xent 9.8000(14.1437) | Loss 7.0717(13.6316) | Error 0.6744(0.7508) Steps 350(374.47) | Grad Norm 2.9398(54.9739) | Total Time 10.00(10.00)\n",
      "Iter 0054 | Time 25.4736(34.5322) | Bit/dim 2.1459(6.4273) | Xent 9.8027(14.0135) | Loss 7.0472(13.4341) | Error 0.6771(0.7486) Steps 362(374.10) | Grad Norm 3.1105(53.4180) | Total Time 10.00(10.00)\n",
      "Iter 0055 | Time 25.7844(34.2697) | Bit/dim 2.1261(6.2983) | Xent 9.7960(13.8870) | Loss 7.0241(13.2418) | Error 0.6949(0.7470) Steps 362(373.74) | Grad Norm 3.2992(51.9144) | Total Time 10.00(10.00)\n",
      "Iter 0056 | Time 26.1072(34.0248) | Bit/dim 2.1164(6.1729) | Xent 9.7812(13.7638) | Loss 7.0070(13.0548) | Error 0.7026(0.7457) Steps 368(373.56) | Grad Norm 1.5721(50.4041) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0008 | Time 14.2752, Epoch Time 205.6009(244.5978), Bit/dim 2.1004, Xent 9.7667, Loss 6.9837, Error 0.6688\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0057 | Time 26.1695(33.7892) | Bit/dim 2.1010(6.0507) | Xent 9.7697(13.6440) | Loss 6.9858(12.8727) | Error 0.7121(0.7447) Steps 362(373.22) | Grad Norm 1.8988(48.9490) | Total Time 10.00(10.00)\n",
      "Iter 0058 | Time 25.9875(33.5551) | Bit/dim 2.0964(5.9321) | Xent 9.7607(13.5275) | Loss 6.9767(12.6958) | Error 0.7241(0.7441) Steps 362(372.88) | Grad Norm 2.2377(47.5476) | Total Time 10.00(10.00)\n",
      "Iter 0059 | Time 25.3642(33.3094) | Bit/dim 2.0929(5.8169) | Xent 9.7524(13.4142) | Loss 6.9691(12.5240) | Error 0.7325(0.7437) Steps 362(372.55) | Grad Norm 1.9247(46.1790) | Total Time 10.00(10.00)\n",
      "Iter 0060 | Time 26.6009(33.1081) | Bit/dim 2.0872(5.7050) | Xent 9.7493(13.3043) | Loss 6.9619(12.3571) | Error 0.7349(0.7435) Steps 362(372.24) | Grad Norm 2.0495(44.8551) | Total Time 10.00(10.00)\n",
      "Iter 0061 | Time 26.0889(32.8976) | Bit/dim 2.0769(5.5962) | Xent 9.7507(13.1977) | Loss 6.9523(12.1950) | Error 0.7350(0.7432) Steps 368(372.11) | Grad Norm 1.9043(43.5665) | Total Time 10.00(10.00)\n",
      "Iter 0062 | Time 25.7048(32.6818) | Bit/dim 2.0561(5.4900) | Xent 9.7614(13.0946) | Loss 6.9368(12.0373) | Error 0.7180(0.7424) Steps 368(371.99) | Grad Norm 1.2787(42.2979) | Total Time 10.00(10.00)\n",
      "Iter 0063 | Time 25.9459(32.4797) | Bit/dim 2.0365(5.3864) | Xent 9.7710(12.9949) | Loss 6.9220(11.8838) | Error 0.7134(0.7416) Steps 368(371.87) | Grad Norm 1.7895(41.0827) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0009 | Time 14.5396, Epoch Time 208.6478(243.5193), Bit/dim 2.0245, Xent 9.7731, Loss 6.9111, Error 0.6391\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0064 | Time 26.8721(32.3115) | Bit/dim 2.0254(5.2855) | Xent 9.7750(12.8983) | Loss 6.9129(11.7347) | Error 0.7055(0.7405) Steps 374(371.93) | Grad Norm 1.8996(39.9072) | Total Time 10.00(10.00)\n",
      "Iter 0065 | Time 27.6075(32.1704) | Bit/dim 2.0189(5.1875) | Xent 9.7693(12.8044) | Loss 6.9035(11.5897) | Error 0.7159(0.7398) Steps 380(372.17) | Grad Norm 1.7341(38.7620) | Total Time 10.00(10.00)\n",
      "Iter 0066 | Time 26.7369(32.0074) | Bit/dim 2.0038(5.0920) | Xent 9.7631(12.7132) | Loss 6.8854(11.4486) | Error 0.7206(0.7392) Steps 380(372.41) | Grad Norm 1.9084(37.6564) | Total Time 10.00(10.00)\n",
      "Iter 0067 | Time 26.6618(31.8470) | Bit/dim 2.0084(4.9995) | Xent 9.7561(12.6245) | Loss 6.8864(11.3117) | Error 0.7319(0.7390) Steps 380(372.64) | Grad Norm 1.0396(36.5579) | Total Time 10.00(10.00)\n",
      "Iter 0068 | Time 26.7776(31.6949) | Bit/dim 1.9957(4.9094) | Xent 9.7573(12.5385) | Loss 6.8744(11.1786) | Error 0.7398(0.7390) Steps 380(372.86) | Grad Norm 1.2247(35.4979) | Total Time 10.00(10.00)\n",
      "Iter 0069 | Time 26.9234(31.5518) | Bit/dim 1.9967(4.8220) | Xent 9.7566(12.4550) | Loss 6.8750(11.0495) | Error 0.7535(0.7394) Steps 380(373.07) | Grad Norm 1.7756(34.4862) | Total Time 10.00(10.00)\n",
      "Iter 0070 | Time 27.1797(31.4206) | Bit/dim 1.9933(4.7371) | Xent 9.7538(12.3740) | Loss 6.8702(10.9241) | Error 0.7480(0.7397) Steps 380(373.28) | Grad Norm 1.3366(33.4917) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0010 | Time 15.0860, Epoch Time 215.9665(242.6927), Bit/dim 1.9733, Xent 9.7492, Loss 6.8479, Error 0.7126\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0071 | Time 27.6460(31.3074) | Bit/dim 1.9687(4.6541) | Xent 9.7530(12.2953) | Loss 6.8452(10.8018) | Error 0.7506(0.7400) Steps 374(373.30) | Grad Norm 0.9531(32.5156) | Total Time 10.00(10.00)\n",
      "Iter 0072 | Time 28.3504(31.2187) | Bit/dim 1.9659(4.5735) | Xent 9.7543(12.2191) | Loss 6.8431(10.6830) | Error 0.7558(0.7405) Steps 374(373.32) | Grad Norm 1.1243(31.5738) | Total Time 10.00(10.00)\n",
      "Iter 0073 | Time 26.3914(31.0738) | Bit/dim 1.9602(4.4951) | Xent 9.7609(12.1454) | Loss 6.8406(10.5677) | Error 0.7499(0.7408) Steps 374(373.34) | Grad Norm 1.3152(30.6661) | Total Time 10.00(10.00)\n",
      "Iter 0074 | Time 26.5521(30.9382) | Bit/dim 1.9627(4.4191) | Xent 9.7581(12.0737) | Loss 6.8417(10.4560) | Error 0.7536(0.7411) Steps 362(373.00) | Grad Norm 1.4130(29.7885) | Total Time 10.00(10.00)\n",
      "Iter 0075 | Time 26.6384(30.8092) | Bit/dim 1.9513(4.3450) | Xent 9.7465(12.0039) | Loss 6.8246(10.3470) | Error 0.7545(0.7415) Steps 374(373.03) | Grad Norm 0.8655(28.9208) | Total Time 10.00(10.00)\n",
      "Iter 0076 | Time 28.4021(30.7370) | Bit/dim 1.9565(4.2734) | Xent 9.7343(11.9358) | Loss 6.8236(10.2413) | Error 0.7689(0.7424) Steps 374(373.06) | Grad Norm 1.0402(28.0844) | Total Time 10.00(10.00)\n",
      "Iter 0077 | Time 27.3040(30.6340) | Bit/dim 1.9515(4.2037) | Xent 9.7261(11.8695) | Loss 6.8145(10.1385) | Error 0.7791(0.7435) Steps 374(373.09) | Grad Norm 1.0416(27.2731) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0011 | Time 14.5011, Epoch Time 218.0067(241.9521), Bit/dim 1.9406, Xent 9.7238, Loss 6.8025, Error 0.7356\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0078 | Time 26.4173(30.5075) | Bit/dim 1.9433(4.1359) | Xent 9.7255(11.8052) | Loss 6.8061(10.0385) | Error 0.7780(0.7445) Steps 362(372.76) | Grad Norm 1.1145(26.4883) | Total Time 10.00(10.00)\n",
      "Iter 0079 | Time 25.6846(30.3628) | Bit/dim 1.9383(4.0700) | Xent 9.7213(11.7427) | Loss 6.7989(9.9413) | Error 0.7670(0.7452) Steps 362(372.43) | Grad Norm 1.1640(25.7286) | Total Time 10.00(10.00)\n",
      "Iter 0080 | Time 25.9224(30.2296) | Bit/dim 1.9326(4.0059) | Xent 9.7182(11.6820) | Loss 6.7917(9.8469) | Error 0.7730(0.7460) Steps 362(372.12) | Grad Norm 1.3918(24.9985) | Total Time 10.00(10.00)\n",
      "Iter 0081 | Time 27.0758(30.1350) | Bit/dim 1.9282(3.9435) | Xent 9.7198(11.6231) | Loss 6.7881(9.7551) | Error 0.7804(0.7470) Steps 368(372.00) | Grad Norm 0.8554(24.2742) | Total Time 10.00(10.00)\n",
      "Iter 0082 | Time 26.5514(30.0275) | Bit/dim 1.9245(3.8830) | Xent 9.7113(11.5657) | Loss 6.7801(9.6658) | Error 0.7771(0.7479) Steps 380(372.24) | Grad Norm 1.2376(23.5831) | Total Time 10.00(10.00)\n",
      "Iter 0083 | Time 25.6011(29.8947) | Bit/dim 1.9181(3.8240) | Xent 9.7035(11.5099) | Loss 6.7699(9.5790) | Error 0.7770(0.7488) Steps 374(372.29) | Grad Norm 1.1156(22.9091) | Total Time 10.00(10.00)\n",
      "Iter 0084 | Time 25.4274(29.7607) | Bit/dim 1.9176(3.7668) | Xent 9.7017(11.4556) | Loss 6.7685(9.4947) | Error 0.7782(0.7497) Steps 362(371.98) | Grad Norm 0.5305(22.2377) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0012 | Time 14.6578, Epoch Time 209.5177(240.9791), Bit/dim 1.9072, Xent 9.6972, Loss 6.7558, Error 0.7214\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0085 | Time 25.1884(29.6235) | Bit/dim 1.9147(3.7113) | Xent 9.6996(11.4030) | Loss 6.7645(9.4127) | Error 0.7724(0.7504) Steps 368(371.86) | Grad Norm 1.4046(21.6127) | Total Time 10.00(10.00)\n",
      "Iter 0086 | Time 25.9276(29.5126) | Bit/dim 1.9041(3.6571) | Xent 9.6904(11.3516) | Loss 6.7493(9.3328) | Error 0.7680(0.7509) Steps 380(372.11) | Grad Norm 0.9029(20.9914) | Total Time 10.00(10.00)\n",
      "Iter 0087 | Time 25.6162(29.3957) | Bit/dim 1.8979(3.6043) | Xent 9.6892(11.3017) | Loss 6.7425(9.2551) | Error 0.7695(0.7515) Steps 374(372.16) | Grad Norm 1.3094(20.4010) | Total Time 10.00(10.00)\n",
      "Iter 0088 | Time 25.7215(29.2855) | Bit/dim 1.8955(3.5530) | Xent 9.6829(11.2531) | Loss 6.7369(9.1796) | Error 0.7820(0.7524) Steps 374(372.22) | Grad Norm 1.7083(19.8402) | Total Time 10.00(10.00)\n",
      "Iter 0089 | Time 25.0805(29.1593) | Bit/dim 1.8858(3.5030) | Xent 9.6865(11.2061) | Loss 6.7290(9.1061) | Error 0.7588(0.7526) Steps 356(371.73) | Grad Norm 2.9597(19.3338) | Total Time 10.00(10.00)\n",
      "Iter 0090 | Time 27.3944(29.1064) | Bit/dim 1.9069(3.4551) | Xent 9.6680(11.1600) | Loss 6.7409(9.0351) | Error 0.7929(0.7538) Steps 386(372.16) | Grad Norm 8.0534(18.9954) | Total Time 10.00(10.00)\n",
      "Iter 0091 | Time 27.0793(29.0456) | Bit/dim 2.0252(3.4122) | Xent 9.8061(11.1194) | Loss 6.9283(8.9719) | Error 0.7305(0.7531) Steps 380(372.39) | Grad Norm 31.3652(19.3665) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0013 | Time 16.4785, Epoch Time 210.8175(240.0742), Bit/dim 2.8586, Xent 10.0877, Loss 7.9024, Error 0.8578\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0092 | Time 33.7707(29.1873) | Bit/dim 2.8639(3.3958) | Xent 10.0910(11.0885) | Loss 7.9094(8.9400) | Error 0.8715(0.7566) Steps 428(374.06) | Grad Norm 45.5468(20.1519) | Total Time 10.00(10.00)\n",
      "Iter 0093 | Time 31.8046(29.2659) | Bit/dim 2.6686(3.3740) | Xent 9.7389(11.0480) | Loss 7.5380(8.8980) | Error 0.8468(0.7593) Steps 416(375.32) | Grad Norm 14.1001(19.9703) | Total Time 10.00(10.00)\n",
      "Iter 0094 | Time 32.1395(29.3521) | Bit/dim 2.4472(3.3461) | Xent 9.7531(11.0092) | Loss 7.3237(8.8507) | Error 0.8066(0.7608) Steps 422(376.72) | Grad Norm 8.2314(19.6182) | Total Time 10.00(10.00)\n",
      "Iter 0095 | Time 28.3961(29.3234) | Bit/dim 2.4526(3.3193) | Xent 9.8436(10.9742) | Loss 7.3744(8.8065) | Error 0.6983(0.7589) Steps 392(377.18) | Grad Norm 34.5378(20.0657) | Total Time 10.00(10.00)\n",
      "Iter 0096 | Time 30.6166(29.3622) | Bit/dim 2.2383(3.2869) | Xent 9.7154(10.9365) | Loss 7.0960(8.7551) | Error 0.8254(0.7609) Steps 422(378.52) | Grad Norm 5.1035(19.6169) | Total Time 10.00(10.00)\n",
      "Iter 0097 | Time 35.3308(29.5412) | Bit/dim 2.2794(3.2567) | Xent 9.7580(10.9011) | Loss 7.1584(8.7072) | Error 0.8139(0.7625) Steps 470(381.27) | Grad Norm 4.2575(19.1561) | Total Time 10.00(10.00)\n",
      "Iter 0098 | Time 35.4256(29.7178) | Bit/dim 2.3101(3.2283) | Xent 9.7814(10.8675) | Loss 7.2008(8.6620) | Error 0.7934(0.7634) Steps 476(384.11) | Grad Norm 3.8406(18.6966) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0014 | Time 17.7337, Epoch Time 257.5459(240.5984), Bit/dim 2.2938, Xent 9.7597, Loss 7.1737, Error 0.6815\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0099 | Time 35.9655(29.9052) | Bit/dim 2.2953(3.2003) | Xent 9.7630(10.8344) | Loss 7.1768(8.6175) | Error 0.7574(0.7632) Steps 476(386.87) | Grad Norm 3.7912(18.2495) | Total Time 10.00(10.00)\n",
      "Iter 0100 | Time 36.4314(30.1010) | Bit/dim 2.3095(3.1736) | Xent 9.6953(10.8002) | Loss 7.1571(8.5737) | Error 0.7140(0.7617) Steps 470(389.36) | Grad Norm 4.3894(17.8337) | Total Time 10.00(10.00)\n",
      "Iter 0101 | Time 35.7188(30.2695) | Bit/dim 2.2864(3.1470) | Xent 9.6253(10.7650) | Loss 7.0991(8.5294) | Error 0.7070(0.7601) Steps 464(391.60) | Grad Norm 3.3299(17.3986) | Total Time 10.00(10.00)\n",
      "Iter 0102 | Time 35.1875(30.4171) | Bit/dim 2.2214(3.1192) | Xent 9.5901(10.7297) | Loss 7.0164(8.4840) | Error 0.7228(0.7590) Steps 440(393.05) | Grad Norm 1.6752(16.9268) | Total Time 10.00(10.00)\n",
      "Iter 0103 | Time 35.3749(30.5658) | Bit/dim 2.2123(3.0920) | Xent 9.5800(10.6952) | Loss 7.0023(8.4396) | Error 0.7578(0.7589) Steps 446(394.64) | Grad Norm 1.8476(16.4745) | Total Time 10.00(10.00)\n",
      "Iter 0104 | Time 36.2066(30.7350) | Bit/dim 2.2234(3.0659) | Xent 9.5761(10.6616) | Loss 7.0115(8.3968) | Error 0.7630(0.7591) Steps 458(396.54) | Grad Norm 2.4122(16.0526) | Total Time 10.00(10.00)\n",
      "Iter 0105 | Time 35.0240(30.8637) | Bit/dim 2.1772(3.0393) | Xent 9.5836(10.6293) | Loss 6.9690(8.3539) | Error 0.7501(0.7588) Steps 446(398.03) | Grad Norm 2.5254(15.6468) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0015 | Time 17.0166, Epoch Time 279.1535(241.7550), Bit/dim 2.1056, Xent 9.6097, Loss 6.9104, Error 0.6746\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0106 | Time 35.5784(31.0051) | Bit/dim 2.1101(3.0114) | Xent 9.6105(10.5987) | Loss 6.9153(8.3108) | Error 0.7505(0.7585) Steps 440(399.28) | Grad Norm 1.6514(15.2269) | Total Time 10.00(10.00)\n",
      "Iter 0107 | Time 34.3314(31.1049) | Bit/dim 2.0806(2.9835) | Xent 9.6344(10.5698) | Loss 6.8978(8.2684) | Error 0.7420(0.7580) Steps 464(401.23) | Grad Norm 1.8170(14.8246) | Total Time 10.00(10.00)\n",
      "Iter 0108 | Time 36.8120(31.2761) | Bit/dim 2.0781(2.9563) | Xent 9.6415(10.5420) | Loss 6.8989(8.2273) | Error 0.7291(0.7572) Steps 464(403.11) | Grad Norm 3.3604(14.4807) | Total Time 10.00(10.00)\n",
      "Iter 0109 | Time 35.0392(31.3890) | Bit/dim 2.0639(2.9295) | Xent 9.5860(10.5133) | Loss 6.8569(8.1862) | Error 0.7544(0.7571) Steps 452(404.58) | Grad Norm 1.6818(14.0967) | Total Time 10.00(10.00)\n",
      "Iter 0110 | Time 34.1582(31.4721) | Bit/dim 2.0525(2.9032) | Xent 9.5177(10.4834) | Loss 6.8114(8.1449) | Error 0.7561(0.7571) Steps 446(405.82) | Grad Norm 1.4493(13.7173) | Total Time 10.00(10.00)\n",
      "Iter 0111 | Time 32.7328(31.5099) | Bit/dim 2.0805(2.8785) | Xent 9.4478(10.4523) | Loss 6.8044(8.1047) | Error 0.7459(0.7567) Steps 428(406.48) | Grad Norm 4.2733(13.4340) | Total Time 10.00(10.00)\n",
      "Iter 0112 | Time 32.3904(31.5363) | Bit/dim 2.1012(2.8552) | Xent 9.4300(10.4217) | Loss 6.8162(8.0661) | Error 0.7791(0.7574) Steps 434(407.31) | Grad Norm 5.0474(13.1824) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0016 | Time 15.6554, Epoch Time 268.9174(242.5699), Bit/dim 2.0386, Xent 9.4276, Loss 6.7524, Error 0.7431\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0113 | Time 32.6711(31.5704) | Bit/dim 2.0424(2.8308) | Xent 9.4272(10.3918) | Loss 6.7560(8.0268) | Error 0.7728(0.7579) Steps 422(407.75) | Grad Norm 3.4062(12.8891) | Total Time 10.00(10.00)\n",
      "Iter 0114 | Time 31.7319(31.5752) | Bit/dim 2.0168(2.8064) | Xent 9.4755(10.3644) | Loss 6.7545(7.9886) | Error 0.7290(0.7570) Steps 422(408.18) | Grad Norm 9.1766(12.7777) | Total Time 10.00(10.00)\n",
      "Iter 0115 | Time 30.2235(31.5347) | Bit/dim 2.0054(2.7824) | Xent 9.4307(10.3363) | Loss 6.7207(7.9506) | Error 0.7742(0.7575) Steps 404(408.05) | Grad Norm 3.4719(12.4986) | Total Time 10.00(10.00)\n",
      "Iter 0116 | Time 31.1581(31.5234) | Bit/dim 2.0015(2.7590) | Xent 9.3825(10.3077) | Loss 6.6928(7.9128) | Error 0.7997(0.7588) Steps 404(407.93) | Grad Norm 3.7621(12.2365) | Total Time 10.00(10.00)\n",
      "Iter 0117 | Time 29.3044(31.4568) | Bit/dim 1.9888(2.7359) | Xent 9.3036(10.2776) | Loss 6.6406(7.8747) | Error 0.7869(0.7596) Steps 398(407.63) | Grad Norm 2.2783(11.9377) | Total Time 10.00(10.00)\n",
      "Iter 0118 | Time 29.0418(31.3844) | Bit/dim 2.0207(2.7144) | Xent 9.2253(10.2460) | Loss 6.6333(7.8374) | Error 0.7508(0.7594) Steps 392(407.16) | Grad Norm 6.0618(11.7614) | Total Time 10.00(10.00)\n",
      "Iter 0119 | Time 30.3610(31.3537) | Bit/dim 1.9758(2.6922) | Xent 9.2220(10.2153) | Loss 6.5868(7.7999) | Error 0.8019(0.7606) Steps 386(406.53) | Grad Norm 3.0625(11.5005) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0017 | Time 14.5971, Epoch Time 241.0672(242.5248), Bit/dim 1.9440, Xent 9.2119, Loss 6.5500, Error 0.7770\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0120 | Time 29.7569(31.3057) | Bit/dim 1.9527(2.6701) | Xent 9.2138(10.1853) | Loss 6.5596(7.7627) | Error 0.8191(0.7624) Steps 386(405.91) | Grad Norm 3.0627(11.2473) | Total Time 10.00(10.00)\n",
      "Iter 0121 | Time 28.7160(31.2281) | Bit/dim 1.9401(2.6482) | Xent 9.2037(10.1558) | Loss 6.5419(7.7261) | Error 0.8083(0.7638) Steps 386(405.32) | Grad Norm 5.2559(11.0676) | Total Time 10.00(10.00)\n",
      "Iter 0122 | Time 30.0145(31.1917) | Bit/dim 1.9221(2.6264) | Xent 9.1216(10.1248) | Loss 6.4829(7.6888) | Error 0.8210(0.7655) Steps 398(405.10) | Grad Norm 1.9226(10.7932) | Total Time 10.00(10.00)\n",
      "Iter 0123 | Time 29.2656(31.1339) | Bit/dim 1.9462(2.6060) | Xent 9.0269(10.0919) | Loss 6.4596(7.6519) | Error 0.8223(0.7672) Steps 398(404.88) | Grad Norm 3.9325(10.5874) | Total Time 10.00(10.00)\n",
      "Iter 0124 | Time 29.9314(31.0978) | Bit/dim 1.9835(2.5873) | Xent 8.9334(10.0571) | Loss 6.4501(7.6158) | Error 0.7855(0.7677) Steps 386(404.32) | Grad Norm 5.8565(10.4455) | Total Time 10.00(10.00)\n",
      "Iter 0125 | Time 27.5272(30.9907) | Bit/dim 1.9119(2.5670) | Xent 8.9919(10.0251) | Loss 6.4079(7.5796) | Error 0.8369(0.7698) Steps 386(403.77) | Grad Norm 5.0860(10.2847) | Total Time 10.00(10.00)\n",
      "Iter 0126 | Time 28.4241(30.9137) | Bit/dim 1.9306(2.5479) | Xent 9.0240(9.9951) | Loss 6.4425(7.5455) | Error 0.8438(0.7720) Steps 380(403.05) | Grad Norm 14.8634(10.4221) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0018 | Time 14.5823, Epoch Time 230.3477(242.1595), Bit/dim 2.2584, Xent 9.6181, Loss 7.0674, Error 0.8772\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0127 | Time 28.6219(30.8449) | Bit/dim 2.2613(2.5393) | Xent 9.6204(9.9839) | Loss 7.0714(7.5313) | Error 0.8898(0.7756) Steps 398(402.90) | Grad Norm 16.9035(10.6165) | Total Time 10.00(10.00)\n",
      "Iter 0128 | Time 35.4940(30.9844) | Bit/dim 2.5386(2.5393) | Xent 10.5655(10.0013) | Loss 7.8214(7.5400) | Error 0.8669(0.7783) Steps 506(406.00) | Grad Norm 38.0341(11.4390) | Total Time 10.00(10.00)\n",
      "Iter 0129 | Time 29.5696(30.9420) | Bit/dim 2.4939(2.5379) | Xent 9.4958(9.9862) | Loss 7.2418(7.5310) | Error 0.8698(0.7810) Steps 404(405.94) | Grad Norm 22.3382(11.7660) | Total Time 10.00(10.00)\n",
      "Iter 0130 | Time 28.4254(30.8665) | Bit/dim 2.1359(2.5259) | Xent 8.9738(9.9558) | Loss 6.6228(7.5038) | Error 0.7104(0.7789) Steps 380(405.16) | Grad Norm 6.2149(11.5995) | Total Time 10.00(10.00)\n",
      "Iter 0131 | Time 30.2947(30.8493) | Bit/dim 2.0114(2.5105) | Xent 9.2481(9.9346) | Loss 6.6354(7.4777) | Error 0.7904(0.7793) Steps 386(404.58) | Grad Norm 5.3067(11.4107) | Total Time 10.00(10.00)\n",
      "Iter 0132 | Time 30.6082(30.8421) | Bit/dim 2.0664(2.4971) | Xent 9.2830(9.9150) | Loss 6.7079(7.4546) | Error 0.8174(0.7804) Steps 422(405.11) | Grad Norm 2.8446(11.1537) | Total Time 10.00(10.00)\n",
      "Iter 0133 | Time 31.8506(30.8723) | Bit/dim 2.1645(2.4872) | Xent 9.2013(9.8936) | Loss 6.7651(7.4339) | Error 0.8344(0.7820) Steps 428(405.79) | Grad Norm 3.3552(10.9198) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0019 | Time 15.6194, Epoch Time 242.9153(242.1822), Bit/dim 2.1961, Xent 9.1527, Loss 6.7724, Error 0.8355\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0134 | Time 32.0163(30.9066) | Bit/dim 2.1994(2.4785) | Xent 9.1529(9.8714) | Loss 6.7759(7.4142) | Error 0.8287(0.7834) Steps 422(406.28) | Grad Norm 3.5639(10.6991) | Total Time 10.00(10.00)\n",
      "Iter 0135 | Time 31.8850(30.9360) | Bit/dim 2.1383(2.4683) | Xent 9.1909(9.8510) | Loss 6.7337(7.3938) | Error 0.8265(0.7847) Steps 428(406.93) | Grad Norm 8.9219(10.6458) | Total Time 10.00(10.00)\n",
      "Iter 0136 | Time 30.0114(30.9083) | Bit/dim 2.1431(2.4586) | Xent 9.0980(9.8284) | Loss 6.6921(7.3727) | Error 0.8086(0.7854) Steps 422(407.38) | Grad Norm 7.0397(10.5376) | Total Time 10.00(10.00)\n",
      "Iter 0137 | Time 30.3215(30.8907) | Bit/dim 2.0091(2.4451) | Xent 9.1280(9.8074) | Loss 6.5731(7.3488) | Error 0.7403(0.7841) Steps 428(408.00) | Grad Norm 2.2436(10.2888) | Total Time 10.00(10.00)\n",
      "Iter 0138 | Time 32.2743(30.9322) | Bit/dim 2.0226(2.4324) | Xent 9.2413(9.7904) | Loss 6.6432(7.3276) | Error 0.7330(0.7826) Steps 428(408.60) | Grad Norm 3.8971(10.0970) | Total Time 10.00(10.00)\n",
      "Iter 0139 | Time 32.4258(30.9770) | Bit/dim 1.9895(2.4191) | Xent 9.2987(9.7756) | Loss 6.6388(7.3069) | Error 0.7604(0.7819) Steps 416(408.82) | Grad Norm 3.3573(9.8948) | Total Time 10.00(10.00)\n",
      "Iter 0140 | Time 29.3982(30.9296) | Bit/dim 1.9692(2.4056) | Xent 9.2348(9.7594) | Loss 6.5866(7.2853) | Error 0.7724(0.7816) Steps 410(408.86) | Grad Norm 2.1657(9.6630) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0020 | Time 15.0701, Epoch Time 245.7226(242.2884), Bit/dim 1.9704, Xent 9.2087, Loss 6.5747, Error 0.7704\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0141 | Time 30.0979(30.9047) | Bit/dim 1.9762(2.3927) | Xent 9.2124(9.7430) | Loss 6.5823(7.2642) | Error 0.8545(0.7838) Steps 404(408.71) | Grad Norm 5.2024(9.5291) | Total Time 10.00(10.00)\n",
      "Iter 0142 | Time 29.6992(30.8685) | Bit/dim 1.9767(2.3802) | Xent 9.1689(9.7258) | Loss 6.5612(7.2431) | Error 0.8325(0.7852) Steps 410(408.75) | Grad Norm 5.6003(9.4113) | Total Time 10.00(10.00)\n",
      "Iter 0143 | Time 29.4474(30.8259) | Bit/dim 1.9012(2.3659) | Xent 9.2210(9.7106) | Loss 6.5117(7.2212) | Error 0.8772(0.7880) Steps 398(408.43) | Grad Norm 3.8958(9.2458) | Total Time 10.00(10.00)\n",
      "Iter 0144 | Time 27.8983(30.7380) | Bit/dim 1.8926(2.3517) | Xent 9.1502(9.6938) | Loss 6.4677(7.1986) | Error 0.8317(0.7893) Steps 392(407.94) | Grad Norm 1.6954(9.0193) | Total Time 10.00(10.00)\n",
      "Iter 0145 | Time 29.2893(30.6946) | Bit/dim 1.9119(2.3385) | Xent 9.0952(9.6759) | Loss 6.4595(7.1764) | Error 0.7943(0.7895) Steps 392(407.46) | Grad Norm 4.1109(8.8720) | Total Time 10.00(10.00)\n",
      "Iter 0146 | Time 29.1783(30.6491) | Bit/dim 1.9373(2.3265) | Xent 9.0675(9.6576) | Loss 6.4711(7.1553) | Error 0.8397(0.7910) Steps 398(407.17) | Grad Norm 7.0545(8.8175) | Total Time 10.00(10.00)\n",
      "Iter 0147 | Time 28.4597(30.5834) | Bit/dim 1.9376(2.3148) | Xent 9.0299(9.6388) | Loss 6.4525(7.1342) | Error 0.7488(0.7897) Steps 392(406.72) | Grad Norm 6.2230(8.7397) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0021 | Time 14.4350, Epoch Time 230.5017(241.9348), Bit/dim 1.8811, Xent 8.9679, Loss 6.3650, Error 0.7141\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0148 | Time 29.2816(30.5444) | Bit/dim 1.8830(2.3018) | Xent 8.9688(9.6187) | Loss 6.3674(7.1112) | Error 0.7984(0.7900) Steps 392(406.28) | Grad Norm 2.9276(8.5653) | Total Time 10.00(10.00)\n",
      "Iter 0149 | Time 28.6934(30.4888) | Bit/dim 1.8574(2.2885) | Xent 8.9778(9.5994) | Loss 6.3463(7.0882) | Error 0.8125(0.7906) Steps 398(406.03) | Grad Norm 1.5286(8.3542) | Total Time 10.00(10.00)\n",
      "Iter 0150 | Time 30.6379(30.4933) | Bit/dim 1.8664(2.2758) | Xent 8.9778(9.5808) | Loss 6.3553(7.0662) | Error 0.7990(0.7909) Steps 392(405.61) | Grad Norm 4.3762(8.2349) | Total Time 10.00(10.00)\n",
      "Iter 0151 | Time 28.4977(30.4334) | Bit/dim 1.8494(2.2630) | Xent 8.8996(9.5604) | Loss 6.2992(7.0432) | Error 0.7939(0.7910) Steps 392(405.20) | Grad Norm 2.9058(8.0750) | Total Time 10.00(10.00)\n",
      "Iter 0152 | Time 29.1583(30.3952) | Bit/dim 1.8623(2.2510) | Xent 8.7883(9.5372) | Loss 6.2564(7.0196) | Error 0.7681(0.7903) Steps 392(404.80) | Grad Norm 1.6854(7.8833) | Total Time 10.00(10.00)\n",
      "Iter 0153 | Time 27.7426(30.3156) | Bit/dim 1.8848(2.2400) | Xent 8.7304(9.5130) | Loss 6.2500(6.9965) | Error 0.7526(0.7892) Steps 380(404.06) | Grad Norm 3.7037(7.7579) | Total Time 10.00(10.00)\n",
      "Iter 0154 | Time 27.7440(30.2384) | Bit/dim 1.8666(2.2288) | Xent 8.7289(9.4895) | Loss 6.2310(6.9736) | Error 0.7959(0.7894) Steps 380(403.34) | Grad Norm 7.4311(7.7481) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0022 | Time 14.0707, Epoch Time 228.3300(241.5266), Bit/dim 1.8869, Xent 8.8236, Loss 6.2987, Error 0.7635\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0155 | Time 27.7535(30.1639) | Bit/dim 1.8907(2.2187) | Xent 8.8256(9.4696) | Loss 6.3035(6.9535) | Error 0.7784(0.7890) Steps 380(402.64) | Grad Norm 10.6047(7.8338) | Total Time 10.00(10.00)\n",
      "Iter 0156 | Time 28.4870(30.1136) | Bit/dim 1.8911(2.2089) | Xent 8.9255(9.4532) | Loss 6.3538(6.9355) | Error 0.8465(0.7908) Steps 386(402.14) | Grad Norm 15.5155(8.0643) | Total Time 10.00(10.00)\n",
      "Iter 0157 | Time 29.8773(30.1065) | Bit/dim 1.9907(2.2023) | Xent 9.1568(9.4443) | Loss 6.5691(6.9245) | Error 0.8386(0.7922) Steps 392(401.83) | Grad Norm 16.2399(8.3095) | Total Time 10.00(10.00)\n",
      "Iter 0158 | Time 30.0205(30.1039) | Bit/dim 1.9129(2.1936) | Xent 8.5820(9.4185) | Loss 6.2039(6.9029) | Error 0.8336(0.7934) Steps 392(401.54) | Grad Norm 6.3195(8.2498) | Total Time 10.00(10.00)\n",
      "Iter 0159 | Time 27.8517(30.0363) | Bit/dim 1.8929(2.1846) | Xent 8.5630(9.3928) | Loss 6.1744(6.8810) | Error 0.8017(0.7937) Steps 386(401.07) | Grad Norm 7.3017(8.2214) | Total Time 10.00(10.00)\n",
      "Iter 0160 | Time 30.7139(30.0567) | Bit/dim 1.8756(2.1753) | Xent 8.5649(9.3680) | Loss 6.1580(6.8593) | Error 0.7334(0.7919) Steps 410(401.34) | Grad Norm 3.8406(8.0900) | Total Time 10.00(10.00)\n",
      "Iter 0161 | Time 32.1052(30.1181) | Bit/dim 1.9157(2.1675) | Xent 8.5005(9.3419) | Loss 6.1660(6.8385) | Error 0.7070(0.7893) Steps 446(402.68) | Grad Norm 3.3161(7.9468) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0023 | Time 15.8008, Epoch Time 234.5854(241.3184), Bit/dim 1.9306, Xent 8.3764, Loss 6.1188, Error 0.6637\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0162 | Time 33.2017(30.2106) | Bit/dim 1.9370(2.1606) | Xent 8.3801(9.3131) | Loss 6.1270(6.8172) | Error 0.6900(0.7864) Steps 470(404.70) | Grad Norm 3.2125(7.8047) | Total Time 10.00(10.00)\n",
      "Iter 0163 | Time 31.9023(30.2614) | Bit/dim 1.8993(2.1528) | Xent 8.3196(9.2833) | Loss 6.0591(6.7944) | Error 0.7272(0.7846) Steps 446(405.94) | Grad Norm 1.5901(7.6183) | Total Time 10.00(10.00)\n",
      "Iter 0164 | Time 30.2985(30.2625) | Bit/dim 1.8914(2.1449) | Xent 8.2606(9.2526) | Loss 6.0217(6.7713) | Error 0.7460(0.7834) Steps 428(406.60) | Grad Norm 1.7138(7.4412) | Total Time 10.00(10.00)\n",
      "Iter 0165 | Time 29.9076(30.2519) | Bit/dim 1.8974(2.1375) | Xent 8.2102(9.2213) | Loss 6.0025(6.7482) | Error 0.7549(0.7826) Steps 422(407.06) | Grad Norm 2.6560(7.2976) | Total Time 10.00(10.00)\n",
      "Iter 0166 | Time 28.4387(30.1975) | Bit/dim 1.8973(2.1303) | Xent 8.0930(9.1875) | Loss 5.9437(6.7241) | Error 0.7245(0.7808) Steps 392(406.61) | Grad Norm 1.8295(7.1336) | Total Time 10.00(10.00)\n",
      "Iter 0167 | Time 29.8160(30.1860) | Bit/dim 1.8836(2.1229) | Xent 8.0181(9.1524) | Loss 5.8926(6.6991) | Error 0.6912(0.7781) Steps 398(406.35) | Grad Norm 2.5194(6.9951) | Total Time 10.00(10.00)\n",
      "Iter 0168 | Time 30.1716(30.1856) | Bit/dim 1.8856(2.1158) | Xent 7.9257(9.1156) | Loss 5.8485(6.6736) | Error 0.6458(0.7742) Steps 404(406.28) | Grad Norm 2.4161(6.8578) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0024 | Time 14.5645, Epoch Time 240.5477(241.2953), Bit/dim 1.8645, Xent 7.8629, Loss 5.7960, Error 0.5535\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0169 | Time 29.8314(30.1750) | Bit/dim 1.8621(2.1082) | Xent 7.8604(9.0779) | Loss 5.7923(6.6472) | Error 0.7196(0.7725) Steps 392(405.85) | Grad Norm 2.3101(6.7213) | Total Time 10.00(10.00)\n",
      "Iter 0170 | Time 27.8660(30.1057) | Bit/dim 1.9241(2.1027) | Xent 7.6545(9.0352) | Loss 5.7513(6.6203) | Error 0.7414(0.7716) Steps 386(405.26) | Grad Norm 3.7778(6.6330) | Total Time 10.00(10.00)\n",
      "Iter 0171 | Time 28.7957(30.0664) | Bit/dim 1.9414(2.0978) | Xent 7.6717(8.9943) | Loss 5.7772(6.5950) | Error 0.7995(0.7724) Steps 386(404.68) | Grad Norm 11.2409(6.7713) | Total Time 10.00(10.00)\n",
      "Iter 0172 | Time 27.7828(29.9979) | Bit/dim 2.2586(2.1026) | Xent 10.4641(9.0384) | Loss 7.4906(6.6219) | Error 0.8728(0.7754) Steps 392(404.30) | Grad Norm 42.9522(7.8567) | Total Time 10.00(10.00)\n",
      "Iter 0173 | Time 30.8341(30.0230) | Bit/dim 5.8855(2.2161) | Xent 31.4271(9.7101) | Loss 21.5991(7.0712) | Error 0.9364(0.7803) Steps 410(404.47) | Grad Norm 425.7077(20.3922) | Total Time 10.00(10.00)\n",
      "Iter 0174 | Time 46.1886(30.5079) | Bit/dim 7.0003(2.3597) | Xent 17.1527(9.9334) | Loss 15.5766(7.3263) | Error 0.8942(0.7837) Steps 620(410.94) | Grad Norm 151.8587(24.3362) | Total Time 10.00(10.00)\n",
      "Iter 0175 | Time 112.0097(32.9530) | Bit/dim 8.1033(2.5320) | Xent 11.8979(9.9923) | Loss 14.0522(7.5281) | Error 0.9029(0.7873) Steps 1496(443.49) | Grad Norm inf(inf) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0025 | Time 75.6983, Epoch Time 391.3362(245.7965), Bit/dim 245323501637842459688960.0000, Xent 679698964318255524610048.0000, Loss 585172983796970221993984.0000, Error 0.9042\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_generative.py --data mnist --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 8000 --test_batch_size 5000 --save ../experiments/cnf_conditional_generative_bs8K_sratio_0_125_drop_0_5_seed_1 --seed 1 --conditional True --controlled_tol True --train_mode semisup --lr 0.01 --warmup_iters 113 --atol 1e-4  --rtol 1e-4 --weight_y 0.5 --condition_ratio 0.125 --dropout_rate 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
