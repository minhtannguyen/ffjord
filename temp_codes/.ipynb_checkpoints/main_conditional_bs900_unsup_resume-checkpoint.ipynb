{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0,1,2,3,4,5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_conditional_unsup.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.odenvp_conditional as odenvp\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=True, choices=[True, False])\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"./data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"./data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"./data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"./data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"./data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"./data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            './data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            './data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz = modules.GaussianDiag.logp(mean, logs, z).view(-1,1)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(z)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol},)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    # fixed_y = torch.randint(high=10, size=(100,)).type(torch.long).to(device, non_blocking=True)\n",
      "    fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "    fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "    with torch.no_grad():\n",
      "        mean, logs = model.module._prior(fixed_y_onehot)\n",
      "        fixed_z = modules.GaussianDiag.sample(mean, logs)\n",
      "    \n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    if args.conditional: best_error_score = float(\"inf\")\n",
      "    \n",
      "    itr = 0\n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                loss = loss_nll\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            loss.backward()\n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            xent_meter.update(loss_xent.item())\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits/dim', {'train': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        loss = loss_nll\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses_xent.append(loss_xent.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'validation': time.time() - start}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits/dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}, Xent {:.4f}, Loss {:.4f}, Error {:.4f}\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, loss_xent, loss, error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, conditional=True, conv=True, data='mnist', dims='64,64,64', divergence_fn='approximate', dl2int=None, imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume=None, rtol=1e-05, save='experiments/cnf_cond_bs900_unsup', solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=900, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.0)\n",
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=1568, bias=True)\n",
      "  (project_class): LinearZeros(in_features=784, out_features=10, bias=True)\n",
      ")\n",
      "Number of trainable parameters: 828890\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc.py:286: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "Iter 0000 | Time 39.2155(39.2155) | Bit/dim 21.5489(21.5489) | Xent 2.3026(2.3026) | Loss 21.5489(21.5489) | Error 0.8967(0.8967) Steps 410(410.00) | Grad Norm 174.9348(174.9348) | Total Time 10.00(10.00)\n",
      "Iter 0010 | Time 12.9601(32.3076) | Bit/dim 19.5097(21.3115) | Xent 2.3026(2.3026) | Loss 19.5097(21.3115) | Error 0.9144(0.8975) Steps 410(410.00) | Grad Norm 155.2186(172.6265) | Total Time 10.00(10.00)\n",
      "Iter 0020 | Time 13.1174(27.2197) | Bit/dim 15.1328(20.1940) | Xent 2.3026(2.3026) | Loss 15.1328(20.1940) | Error 0.9044(0.8979) Steps 410(410.00) | Grad Norm 105.6132(160.7293) | Total Time 10.00(10.00)\n",
      "Iter 0030 | Time 12.8943(23.4905) | Bit/dim 11.9312(18.3415) | Xent 2.3026(2.3026) | Loss 11.9312(18.3415) | Error 0.9078(0.8990) Steps 410(410.00) | Grad Norm 46.5219(136.9860) | Total Time 10.00(10.00)\n",
      "Iter 0040 | Time 12.7717(20.7165) | Bit/dim 10.2059(16.3676) | Xent 2.3026(2.3026) | Loss 10.2059(16.3676) | Error 0.9000(0.9002) Steps 410(410.00) | Grad Norm 22.2411(108.6890) | Total Time 10.00(10.00)\n",
      "Iter 0050 | Time 13.0377(18.6800) | Bit/dim 8.4924(14.4729) | Xent 2.3026(2.3026) | Loss 8.4924(14.4729) | Error 0.8967(0.9016) Steps 410(410.00) | Grad Norm 15.2391(84.8797) | Total Time 10.00(10.00)\n",
      "Iter 0060 | Time 12.8800(17.1731) | Bit/dim 7.1094(12.6938) | Xent 2.3026(2.3026) | Loss 7.1094(12.6938) | Error 0.9144(0.9018) Steps 410(410.00) | Grad Norm 16.3738(66.6098) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0001 | Time 48.3243, Epoch Time 942.6313(942.6313), Bit/dim 6.4618, Xent 2.3026, Loss 6.4618, Error 0.9031\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0070 | Time 12.8436(16.0700) | Bit/dim 5.9251(11.0545) | Xent 2.3026(2.3026) | Loss 5.9251(11.0545) | Error 0.8967(0.9014) Steps 410(410.00) | Grad Norm 14.8281(53.3490) | Total Time 10.00(10.00)\n",
      "Iter 0080 | Time 12.7364(15.2021) | Bit/dim 4.7158(9.5222) | Xent 2.3026(2.3026) | Loss 4.7158(9.5222) | Error 0.9089(0.9006) Steps 410(410.00) | Grad Norm 10.9716(42.5998) | Total Time 10.00(10.00)\n",
      "Iter 0090 | Time 12.9640(14.6086) | Bit/dim 3.7959(8.1131) | Xent 2.3026(2.3026) | Loss 3.7959(8.1131) | Error 0.8989(0.9013) Steps 416(410.85) | Grad Norm 9.1007(33.9927) | Total Time 10.00(10.00)\n",
      "Iter 0100 | Time 13.4375(14.3105) | Bit/dim 3.1382(6.8769) | Xent 2.3026(2.3026) | Loss 3.1382(6.8769) | Error 0.9000(0.9020) Steps 428(415.08) | Grad Norm 6.5894(27.1201) | Total Time 10.00(10.00)\n",
      "Iter 0110 | Time 14.0291(14.1484) | Bit/dim 2.8096(5.8451) | Xent 2.3026(2.3026) | Loss 2.8096(5.8451) | Error 0.8889(0.9020) Steps 440(420.46) | Grad Norm 4.0011(21.3064) | Total Time 10.00(10.00)\n",
      "Iter 0120 | Time 13.6343(14.0372) | Bit/dim 2.5765(5.0112) | Xent 2.3026(2.3026) | Loss 2.5765(5.0112) | Error 0.8944(0.9013) Steps 440(425.44) | Grad Norm 2.5409(16.5216) | Total Time 10.00(10.00)\n",
      "Iter 0130 | Time 13.7653(13.9625) | Bit/dim 2.4596(4.3549) | Xent 2.3026(2.3026) | Loss 2.4596(4.3549) | Error 0.8856(0.9009) Steps 440(429.27) | Grad Norm 1.7772(12.7259) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0002 | Time 52.6046, Epoch Time 948.7009(942.8134), Bit/dim 2.4260, Xent 2.3026, Loss 2.4260, Error 0.9031\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0140 | Time 14.4058(14.0088) | Bit/dim 2.3767(3.8418) | Xent 2.3026(2.3026) | Loss 2.3767(3.8418) | Error 0.8856(0.9009) Steps 452(434.08) | Grad Norm 1.2991(9.7785) | Total Time 10.00(10.00)\n",
      "Iter 0150 | Time 14.2195(14.0579) | Bit/dim 2.3276(3.4477) | Xent 2.3026(2.3026) | Loss 2.3276(3.4477) | Error 0.9067(0.9010) Steps 452(438.78) | Grad Norm 1.0559(7.5097) | Total Time 10.00(10.00)\n",
      "Iter 0160 | Time 14.4470(14.1419) | Bit/dim 2.2669(3.1446) | Xent 2.3026(2.3026) | Loss 2.2669(3.1446) | Error 0.8944(0.9003) Steps 452(442.25) | Grad Norm 0.8570(5.7841) | Total Time 10.00(10.00)\n",
      "Iter 0170 | Time 14.1538(14.1642) | Bit/dim 2.2242(2.9093) | Xent 2.3026(2.3026) | Loss 2.2242(2.9093) | Error 0.9267(0.9015) Steps 452(444.81) | Grad Norm 0.7668(4.4765) | Total Time 10.00(10.00)\n",
      "Iter 0180 | Time 14.2698(14.1600) | Bit/dim 2.2104(2.7289) | Xent 2.3026(2.3026) | Loss 2.2104(2.7289) | Error 0.9078(0.9012) Steps 452(445.45) | Grad Norm 0.6775(3.4874) | Total Time 10.00(10.00)\n",
      "Iter 0190 | Time 13.7613(14.0754) | Bit/dim 2.1975(2.5924) | Xent 2.3026(2.3026) | Loss 2.1975(2.5924) | Error 0.9056(0.9010) Steps 440(444.30) | Grad Norm 0.6342(2.7434) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0003 | Time 52.2913, Epoch Time 997.6770(944.4593), Bit/dim 2.1824, Xent 2.3026, Loss 2.1824, Error 0.9031\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0200 | Time 13.7787(13.9946) | Bit/dim 2.1733(2.4873) | Xent 2.3026(2.3026) | Loss 2.1733(2.4873) | Error 0.9178(0.9015) Steps 440(443.17) | Grad Norm 0.6061(2.1814) | Total Time 10.00(10.00)\n",
      "Iter 0210 | Time 13.8212(13.9246) | Bit/dim 2.1793(2.4066) | Xent 2.3026(2.3026) | Loss 2.1793(2.4066) | Error 0.8900(0.9006) Steps 440(442.34) | Grad Norm 0.5570(1.7577) | Total Time 10.00(10.00)\n",
      "Iter 0220 | Time 13.5174(13.8503) | Bit/dim 2.1466(2.3414) | Xent 2.3026(2.3026) | Loss 2.1466(2.3414) | Error 0.9178(0.9009) Steps 434(441.38) | Grad Norm 0.5283(1.4366) | Total Time 10.00(10.00)\n",
      "Iter 0230 | Time 13.8537(13.8312) | Bit/dim 2.1380(2.2904) | Xent 2.3026(2.3026) | Loss 2.1380(2.2904) | Error 0.8911(0.9001) Steps 434(439.44) | Grad Norm 0.5116(1.1936) | Total Time 10.00(10.00)\n",
      "Iter 0240 | Time 13.6773(13.8085) | Bit/dim 2.1271(2.2494) | Xent 2.3026(2.3026) | Loss 2.1271(2.2494) | Error 0.9144(0.9012) Steps 434(438.01) | Grad Norm 0.4983(1.0115) | Total Time 10.00(10.00)\n",
      "Iter 0250 | Time 13.6276(13.7933) | Bit/dim 2.1056(2.2156) | Xent 2.3026(2.3026) | Loss 2.1056(2.2156) | Error 0.9111(0.9013) Steps 434(436.96) | Grad Norm 0.4867(0.8721) | Total Time 10.00(10.00)\n",
      "Iter 0260 | Time 13.8969(13.7760) | Bit/dim 2.0931(2.1874) | Xent 2.3026(2.3026) | Loss 2.0931(2.1874) | Error 0.9089(0.9011) Steps 434(436.18) | Grad Norm 0.4468(0.7648) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0004 | Time 50.7025, Epoch Time 970.3074(945.2347), Bit/dim 2.0919, Xent 2.3026, Loss 2.0919, Error 0.9031\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0270 | Time 13.7374(13.7340) | Bit/dim 2.0622(2.1620) | Xent 2.3026(2.3026) | Loss 2.0622(2.1620) | Error 0.9089(0.9016) Steps 440(435.79) | Grad Norm 0.4764(0.6831) | Total Time 10.00(10.00)\n",
      "Iter 0280 | Time 13.7851(13.7315) | Bit/dim 2.0900(2.1409) | Xent 2.3026(2.3026) | Loss 2.0900(2.1409) | Error 0.8922(0.9028) Steps 440(436.89) | Grad Norm 0.4244(0.6183) | Total Time 10.00(10.00)\n",
      "Iter 0290 | Time 13.7423(13.7399) | Bit/dim 2.0577(2.1212) | Xent 2.3026(2.3026) | Loss 2.0577(2.1212) | Error 0.9111(0.9022) Steps 440(437.71) | Grad Norm 0.4072(0.5668) | Total Time 10.00(10.00)\n",
      "Iter 0300 | Time 13.8018(13.7578) | Bit/dim 2.0459(2.1034) | Xent 2.3026(2.3026) | Loss 2.0459(2.1034) | Error 0.9144(0.9018) Steps 440(438.31) | Grad Norm 0.4404(0.5299) | Total Time 10.00(10.00)\n",
      "Iter 0310 | Time 13.8732(13.7353) | Bit/dim 2.0268(2.0863) | Xent 2.3026(2.3026) | Loss 2.0268(2.0863) | Error 0.9000(0.9010) Steps 440(438.75) | Grad Norm 0.4187(0.4969) | Total Time 10.00(10.00)\n",
      "Iter 0320 | Time 13.8898(13.7470) | Bit/dim 2.0330(2.0708) | Xent 2.3026(2.3026) | Loss 2.0330(2.0708) | Error 0.9011(0.9025) Steps 440(439.08) | Grad Norm 0.4007(0.4687) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0005 | Time 52.5063, Epoch Time 973.4607(946.0815), Bit/dim 2.0049, Xent 2.3026, Loss 2.0049, Error 0.9031\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0330 | Time 14.1608(13.7813) | Bit/dim 2.0101(2.0561) | Xent 2.3026(2.3026) | Loss 2.0101(2.0561) | Error 0.9111(0.9016) Steps 440(439.32) | Grad Norm 0.3659(0.4430) | Total Time 10.00(10.00)\n",
      "Iter 0340 | Time 13.8632(13.8080) | Bit/dim 2.0172(2.0427) | Xent 2.3026(2.3026) | Loss 2.0172(2.0427) | Error 0.8900(0.9009) Steps 440(439.50) | Grad Norm 0.3535(0.4254) | Total Time 10.00(10.00)\n",
      "Iter 0350 | Time 14.1691(13.8619) | Bit/dim 1.9623(2.0288) | Xent 2.3026(2.3026) | Loss 1.9623(2.0288) | Error 0.9100(0.9009) Steps 440(439.63) | Grad Norm 0.4120(0.4074) | Total Time 10.00(10.00)\n",
      "Iter 0360 | Time 13.9193(13.8536) | Bit/dim 1.9521(2.0146) | Xent 2.3026(2.3026) | Loss 1.9521(2.0146) | Error 0.9044(0.9024) Steps 440(439.73) | Grad Norm 0.3566(0.3951) | Total Time 10.00(10.00)\n",
      "Iter 0370 | Time 13.7965(13.8550) | Bit/dim 1.9518(2.0023) | Xent 2.3026(2.3026) | Loss 1.9518(2.0023) | Error 0.8967(0.9020) Steps 440(439.80) | Grad Norm 0.3661(0.3891) | Total Time 10.00(10.00)\n",
      "Iter 0380 | Time 13.7330(13.8600) | Bit/dim 1.9499(1.9887) | Xent 2.3026(2.3026) | Loss 1.9499(1.9887) | Error 0.8956(0.9007) Steps 440(439.85) | Grad Norm 0.3587(0.3820) | Total Time 10.00(10.00)\n",
      "Iter 0390 | Time 13.8212(13.8500) | Bit/dim 1.9360(1.9752) | Xent 2.3026(2.3026) | Loss 1.9360(1.9752) | Error 0.9022(0.9008) Steps 440(439.56) | Grad Norm 0.3079(0.3772) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0006 | Time 52.3238, Epoch Time 981.9274(947.1569), Bit/dim 1.9259, Xent 2.3026, Loss 1.9259, Error 0.9031\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0400 | Time 14.0015(13.8419) | Bit/dim 1.9180(1.9627) | Xent 2.3026(2.3026) | Loss 1.9180(1.9627) | Error 0.9033(0.9015) Steps 440(439.67) | Grad Norm 0.3204(0.3725) | Total Time 10.00(10.00)\n",
      "Iter 0410 | Time 13.6801(13.8093) | Bit/dim 1.9193(1.9521) | Xent 2.3026(2.3026) | Loss 1.9193(1.9521) | Error 0.9033(0.9015) Steps 440(439.76) | Grad Norm 0.2851(0.3598) | Total Time 10.00(10.00)\n",
      "Iter 0420 | Time 13.9632(13.8178) | Bit/dim 1.9098(1.9417) | Xent 2.3026(2.3026) | Loss 1.9098(1.9417) | Error 0.9033(0.9005) Steps 446(440.18) | Grad Norm 0.2482(0.3439) | Total Time 10.00(10.00)\n",
      "Iter 0430 | Time 14.0156(13.8420) | Bit/dim 1.8970(1.9303) | Xent 2.3026(2.3026) | Loss 1.8970(1.9303) | Error 0.9022(0.9005) Steps 446(441.71) | Grad Norm 0.2959(0.3302) | Total Time 10.00(10.00)\n",
      "Iter 0440 | Time 14.0129(13.8871) | Bit/dim 1.8885(1.9202) | Xent 2.3026(2.3026) | Loss 1.8885(1.9202) | Error 0.8989(0.9007) Steps 446(442.83) | Grad Norm 0.2766(0.3165) | Total Time 10.00(10.00)\n",
      "Iter 0450 | Time 14.0778(13.9322) | Bit/dim 1.8756(1.9100) | Xent 2.3026(2.3026) | Loss 1.8756(1.9100) | Error 0.9022(0.9016) Steps 452(444.02) | Grad Norm 0.2175(0.2944) | Total Time 10.00(10.00)\n",
      "Iter 0460 | Time 14.5112(14.0246) | Bit/dim 1.8806(1.9004) | Xent 2.3026(2.3026) | Loss 1.8806(1.9004) | Error 0.8911(0.9014) Steps 458(446.17) | Grad Norm 0.2010(0.2844) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0007 | Time 53.3876, Epoch Time 988.4574(948.3959), Bit/dim 1.8665, Xent 2.3026, Loss 1.8665, Error 0.9031\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0470 | Time 14.2971(14.1018) | Bit/dim 1.8563(1.8917) | Xent 2.3026(2.3026) | Loss 1.8563(1.8917) | Error 0.8967(0.9009) Steps 458(449.14) | Grad Norm 0.3543(0.2876) | Total Time 10.00(10.00)\n",
      "Iter 0480 | Time 14.3022(14.1634) | Bit/dim 1.8563(1.8836) | Xent 2.3026(2.3026) | Loss 1.8563(1.8836) | Error 0.8889(0.8999) Steps 458(451.19) | Grad Norm 0.3022(0.2840) | Total Time 10.00(10.00)\n",
      "Iter 0490 | Time 14.2677(14.1893) | Bit/dim 1.8465(1.8774) | Xent 2.3026(2.3026) | Loss 1.8465(1.8774) | Error 0.8989(0.9003) Steps 458(452.98) | Grad Norm 0.2660(0.2808) | Total Time 10.00(10.00)\n",
      "Iter 0500 | Time 14.3912(14.2432) | Bit/dim 1.8450(1.8697) | Xent 2.3026(2.3026) | Loss 1.8450(1.8697) | Error 0.9056(0.9002) Steps 458(454.30) | Grad Norm 0.2780(0.2814) | Total Time 10.00(10.00)\n",
      "Iter 0510 | Time 14.2523(14.2843) | Bit/dim 1.8270(1.8616) | Xent 2.3026(2.3026) | Loss 1.8270(1.8616) | Error 0.9011(0.9011) Steps 458(455.27) | Grad Norm 0.4355(0.2923) | Total Time 10.00(10.00)\n",
      "Iter 0520 | Time 14.4938(14.3138) | Bit/dim 1.8141(1.8537) | Xent 2.3026(2.3026) | Loss 1.8141(1.8537) | Error 0.8933(0.9015) Steps 458(455.99) | Grad Norm 0.4521(0.3085) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0008 | Time 53.2806, Epoch Time 1013.6729(950.3542), Bit/dim 1.8174, Xent 2.3026, Loss 1.8174, Error 0.9031\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0530 | Time 14.4452(14.3113) | Bit/dim 1.8189(1.8457) | Xent 2.3026(2.3026) | Loss 1.8189(1.8457) | Error 0.9067(0.9023) Steps 458(456.52) | Grad Norm 0.3307(0.3176) | Total Time 10.00(10.00)\n",
      "Iter 0540 | Time 14.1218(14.3148) | Bit/dim 1.8253(1.8382) | Xent 2.3026(2.3026) | Loss 1.8253(1.8382) | Error 0.8844(0.9010) Steps 458(456.75) | Grad Norm 0.4218(0.3160) | Total Time 10.00(10.00)\n",
      "Iter 0550 | Time 14.3807(14.3168) | Bit/dim 1.8038(1.8305) | Xent 2.3026(2.3026) | Loss 1.8038(1.8305) | Error 0.8856(0.9002) Steps 452(455.84) | Grad Norm 0.3238(0.3270) | Total Time 10.00(10.00)\n",
      "Iter 0560 | Time 14.2067(14.3066) | Bit/dim 1.8010(1.8219) | Xent 2.3026(2.3026) | Loss 1.8010(1.8219) | Error 0.8978(0.9007) Steps 458(456.26) | Grad Norm 0.3472(0.3387) | Total Time 10.00(10.00)\n",
      "Iter 0570 | Time 14.2291(14.3163) | Bit/dim 1.7667(1.8115) | Xent 2.3026(2.3026) | Loss 1.7667(1.8115) | Error 0.9122(0.9017) Steps 458(456.72) | Grad Norm 0.4431(0.3705) | Total Time 10.00(10.00)\n",
      "Iter 0580 | Time 14.2231(14.3030) | Bit/dim 1.7486(1.7996) | Xent 2.3026(2.3026) | Loss 1.7486(1.7996) | Error 0.9144(0.9016) Steps 458(457.06) | Grad Norm 0.5404(0.4337) | Total Time 10.00(10.00)\n",
      "Iter 0590 | Time 14.2258(14.2663) | Bit/dim 1.7404(1.7883) | Xent 2.3026(2.3026) | Loss 1.7404(1.7883) | Error 0.9033(0.9021) Steps 458(457.30) | Grad Norm 0.4290(0.4805) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0009 | Time 54.9427, Epoch Time 1011.4004(952.1856), Bit/dim 1.7349, Xent 2.3026, Loss 1.7349, Error 0.9031\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0600 | Time 14.1870(14.2554) | Bit/dim 1.7343(1.7735) | Xent 2.3026(2.3026) | Loss 1.7343(1.7735) | Error 0.9078(0.9020) Steps 458(457.32) | Grad Norm 0.6008(0.4700) | Total Time 10.00(10.00)\n",
      "Iter 0610 | Time 14.2617(14.2417) | Bit/dim 1.7025(1.7588) | Xent 2.3026(2.3026) | Loss 1.7025(1.7588) | Error 0.9100(0.9009) Steps 458(456.84) | Grad Norm 0.3465(0.4929) | Total Time 10.00(10.00)\n",
      "Iter 0620 | Time 14.2595(14.2300) | Bit/dim 1.6592(1.7362) | Xent 2.3026(2.3026) | Loss 1.6592(1.7362) | Error 0.8978(0.9026) Steps 458(456.49) | Grad Norm 5.9263(0.7629) | Total Time 10.00(10.00)\n",
      "Iter 0630 | Time 14.5455(14.2503) | Bit/dim 1.6500(1.7162) | Xent 2.3026(2.3026) | Loss 1.6500(1.7162) | Error 0.9033(0.9009) Steps 464(456.70) | Grad Norm 14.3468(3.7154) | Total Time 10.00(10.00)\n",
      "Iter 0640 | Time 14.6450(14.2801) | Bit/dim 1.6071(1.6921) | Xent 2.3026(2.3026) | Loss 1.6071(1.6921) | Error 0.9111(0.9021) Steps 470(456.65) | Grad Norm 10.3586(5.3631) | Total Time 10.00(10.00)\n",
      "Iter 0650 | Time 14.7166(14.3356) | Bit/dim 1.5417(1.6592) | Xent 2.3026(2.3026) | Loss 1.5417(1.6592) | Error 0.9178(0.9013) Steps 470(458.37) | Grad Norm 6.5541(5.4720) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0010 | Time 57.3587, Epoch Time 1016.1814(954.1055), Bit/dim 1.5166, Xent 2.3026, Loss 1.5166, Error 0.9031\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0660 | Time 14.4539(14.3706) | Bit/dim 1.5308(1.6259) | Xent 2.3026(2.3026) | Loss 1.5308(1.6259) | Error 0.9067(0.9016) Steps 470(461.28) | Grad Norm 3.1519(5.0262) | Total Time 10.00(10.00)\n",
      "Iter 0670 | Time 14.5302(14.4189) | Bit/dim 1.5281(1.5969) | Xent 2.3026(2.3026) | Loss 1.5281(1.5969) | Error 0.8856(0.9009) Steps 470(463.75) | Grad Norm 10.3073(4.9336) | Total Time 10.00(10.00)\n",
      "Iter 0680 | Time 14.7836(14.5084) | Bit/dim 1.4868(1.5759) | Xent 2.3026(2.3026) | Loss 1.4868(1.5759) | Error 0.9100(0.9009) Steps 476(466.16) | Grad Norm 8.6324(7.0077) | Total Time 10.00(10.00)\n",
      "Iter 0690 | Time 14.7261(14.5864) | Bit/dim 1.4721(1.5513) | Xent 2.3026(2.3026) | Loss 1.4721(1.5513) | Error 0.8989(0.9025) Steps 476(468.45) | Grad Norm 4.4859(6.8200) | Total Time 10.00(10.00)\n",
      "Iter 0700 | Time 14.7858(14.6345) | Bit/dim 1.4537(1.5339) | Xent 2.3026(2.3026) | Loss 1.4537(1.5339) | Error 0.8967(0.9011) Steps 476(470.28) | Grad Norm 5.7150(7.5172) | Total Time 10.00(10.00)\n",
      "Iter 0710 | Time 14.9269(14.6796) | Bit/dim 1.4728(1.5206) | Xent 2.3026(2.3026) | Loss 1.4728(1.5206) | Error 0.9000(0.9007) Steps 476(471.78) | Grad Norm 13.4293(8.7218) | Total Time 10.00(10.00)\n",
      "Iter 0720 | Time 14.4719(14.6979) | Bit/dim 1.4475(1.5051) | Xent 2.3026(2.3026) | Loss 1.4475(1.5051) | Error 0.8922(0.9015) Steps 476(472.92) | Grad Norm 5.3504(8.5328) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0011 | Time 56.6724, Epoch Time 1042.9609(956.7711), Bit/dim 1.4408, Xent 2.3026, Loss 1.4408, Error 0.9031\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0730 | Time 14.8911(14.7204) | Bit/dim 1.4628(1.4910) | Xent 2.3026(2.3026) | Loss 1.4628(1.4910) | Error 0.8822(0.8998) Steps 476(473.73) | Grad Norm 8.2092(7.8764) | Total Time 10.00(10.00)\n",
      "Iter 0740 | Time 15.0869(14.7381) | Bit/dim 1.4722(1.4855) | Xent 2.3026(2.3026) | Loss 1.4722(1.4855) | Error 0.8944(0.8993) Steps 482(474.82) | Grad Norm 13.2789(9.6849) | Total Time 10.00(10.00)\n",
      "Iter 0750 | Time 14.9595(14.7487) | Bit/dim 1.4278(1.4730) | Xent 2.3026(2.3026) | Loss 1.4278(1.4730) | Error 0.9122(0.9014) Steps 476(474.71) | Grad Norm 2.2651(8.9106) | Total Time 10.00(10.00)\n",
      "Iter 0760 | Time 15.0614(14.7287) | Bit/dim 1.4312(1.4635) | Xent 2.3026(2.3026) | Loss 1.4312(1.4635) | Error 0.9144(0.9006) Steps 488(475.43) | Grad Norm 14.5556(8.8494) | Total Time 10.00(10.00)\n",
      "Iter 0770 | Time 14.6931(14.7659) | Bit/dim 1.4106(1.4548) | Xent 2.3026(2.3026) | Loss 1.4106(1.4548) | Error 0.9067(0.9018) Steps 470(476.69) | Grad Norm 14.0239(9.5511) | Total Time 10.00(10.00)\n",
      "Iter 0780 | Time 15.1529(14.7961) | Bit/dim 1.4125(1.4477) | Xent 2.3026(2.3026) | Loss 1.4125(1.4477) | Error 0.9056(0.9011) Steps 488(477.30) | Grad Norm 6.3238(9.4399) | Total Time 10.00(10.00)\n",
      "Iter 0790 | Time 14.7383(14.8273) | Bit/dim 1.4358(1.4428) | Xent 2.3026(2.3026) | Loss 1.4358(1.4428) | Error 0.8933(0.9024) Steps 470(477.63) | Grad Norm 16.8241(10.5129) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0012 | Time 60.3889, Epoch Time 1052.0964(959.6309), Bit/dim 1.4211, Xent 2.3026, Loss 1.4211, Error 0.9031\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0800 | Time 15.4555(14.8682) | Bit/dim 1.4251(1.4372) | Xent 2.3026(2.3026) | Loss 1.4251(1.4372) | Error 0.9122(0.9025) Steps 494(478.63) | Grad Norm 8.6992(10.0778) | Total Time 10.00(10.00)\n",
      "Iter 0810 | Time 14.9243(14.8890) | Bit/dim 1.4139(1.4295) | Xent 2.3026(2.3026) | Loss 1.4139(1.4295) | Error 0.8878(0.9016) Steps 488(479.56) | Grad Norm 11.3239(9.2889) | Total Time 10.00(10.00)\n",
      "Iter 0820 | Time 14.9797(14.8879) | Bit/dim 1.4167(1.4240) | Xent 2.3026(2.3026) | Loss 1.4167(1.4240) | Error 0.9067(0.9015) Steps 494(480.85) | Grad Norm 12.5267(9.9659) | Total Time 10.00(10.00)\n",
      "Iter 0830 | Time 14.9124(14.9010) | Bit/dim 1.3970(1.4164) | Xent 2.3026(2.3026) | Loss 1.3970(1.4164) | Error 0.9067(0.9011) Steps 488(481.49) | Grad Norm 10.6017(9.8085) | Total Time 10.00(10.00)\n",
      "Iter 0840 | Time 14.9857(14.9059) | Bit/dim 1.4026(1.4131) | Xent 2.3026(2.3026) | Loss 1.4026(1.4131) | Error 0.9044(0.9006) Steps 476(482.30) | Grad Norm 11.8950(10.3837) | Total Time 10.00(10.00)\n",
      "Iter 0850 | Time 15.3050(14.9487) | Bit/dim 1.3846(1.4068) | Xent 2.3026(2.3026) | Loss 1.3846(1.4068) | Error 0.9033(0.9018) Steps 494(483.61) | Grad Norm 13.3225(10.3810) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0013 | Time 60.4766, Epoch Time 1062.4452(962.7153), Bit/dim 1.3956, Xent 2.3026, Loss 1.3956, Error 0.9031\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0860 | Time 15.3585(14.9910) | Bit/dim 1.4341(1.4032) | Xent 2.3026(2.3026) | Loss 1.4341(1.4032) | Error 0.9078(0.9027) Steps 500(485.22) | Grad Norm 22.7134(11.0168) | Total Time 10.00(10.00)\n",
      "Iter 0870 | Time 15.2003(15.0333) | Bit/dim 1.3766(1.4000) | Xent 2.3026(2.3026) | Loss 1.3766(1.4000) | Error 0.9133(0.9028) Steps 494(485.98) | Grad Norm 14.9596(11.4459) | Total Time 10.00(10.00)\n",
      "Iter 0880 | Time 14.9590(15.0415) | Bit/dim 1.3762(1.3948) | Xent 2.3026(2.3026) | Loss 1.3762(1.3948) | Error 0.9211(0.9022) Steps 494(487.50) | Grad Norm 14.4444(10.6834) | Total Time 10.00(10.00)\n",
      "Iter 0890 | Time 15.1397(15.0561) | Bit/dim 1.3536(1.3912) | Xent 2.3026(2.3026) | Loss 1.3536(1.3912) | Error 0.8967(0.9030) Steps 494(488.70) | Grad Norm 4.8717(10.9242) | Total Time 10.00(10.00)\n",
      "Iter 0900 | Time 14.8664(15.0473) | Bit/dim 1.3790(1.3891) | Xent 2.3026(2.3026) | Loss 1.3790(1.3891) | Error 0.9089(0.9027) Steps 494(489.15) | Grad Norm 5.8691(11.2977) | Total Time 10.00(10.00)\n",
      "Iter 0910 | Time 14.9203(15.0545) | Bit/dim 1.3616(1.3853) | Xent 2.3026(2.3026) | Loss 1.3616(1.3853) | Error 0.9044(0.9011) Steps 494(490.13) | Grad Norm 6.4370(10.8397) | Total Time 10.00(10.00)\n",
      "Iter 0920 | Time 14.7666(15.0534) | Bit/dim 1.3667(1.3809) | Xent 2.3026(2.3026) | Loss 1.3667(1.3809) | Error 0.9022(0.9008) Steps 494(490.99) | Grad Norm 13.7209(10.6843) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0014 | Time 60.9478, Epoch Time 1069.3989(965.9158), Bit/dim 1.3504, Xent 2.3026, Loss 1.3504, Error 0.9031\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0930 | Time 14.8817(15.0587) | Bit/dim 1.3879(1.3797) | Xent 2.3026(2.3026) | Loss 1.3879(1.3797) | Error 0.9089(0.9006) Steps 494(491.02) | Grad Norm 16.4334(11.5255) | Total Time 10.00(10.00)\n",
      "Iter 0940 | Time 15.1137(15.0530) | Bit/dim 1.3673(1.3745) | Xent 2.3026(2.3026) | Loss 1.3673(1.3745) | Error 0.9111(0.9005) Steps 494(491.80) | Grad Norm 12.5427(10.6057) | Total Time 10.00(10.00)\n",
      "Iter 0950 | Time 15.2895(15.0331) | Bit/dim 1.3793(1.3690) | Xent 2.3026(2.3026) | Loss 1.3793(1.3690) | Error 0.9078(0.9007) Steps 500(492.56) | Grad Norm 18.6850(10.0809) | Total Time 10.00(10.00)\n",
      "Iter 0960 | Time 14.8578(15.0310) | Bit/dim 1.3535(1.3685) | Xent 2.3026(2.3026) | Loss 1.3535(1.3685) | Error 0.9178(0.9020) Steps 494(492.15) | Grad Norm 10.1756(10.9715) | Total Time 10.00(10.00)\n",
      "Iter 0970 | Time 15.1445(15.0430) | Bit/dim 1.3428(1.3639) | Xent 2.3026(2.3026) | Loss 1.3428(1.3639) | Error 0.8989(0.9022) Steps 494(492.36) | Grad Norm 8.4813(10.5311) | Total Time 10.00(10.00)\n",
      "Iter 0980 | Time 15.1137(15.0942) | Bit/dim 1.3622(1.3601) | Xent 2.3026(2.3026) | Loss 1.3622(1.3601) | Error 0.8922(0.9010) Steps 494(493.11) | Grad Norm 18.9366(10.3340) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0015 | Time 59.1678, Epoch Time 1068.1579(968.9831), Bit/dim 1.3806, Xent 2.3026, Loss 1.3806, Error 0.9031\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0990 | Time 15.1175(15.1050) | Bit/dim 1.3909(1.3626) | Xent 2.3026(2.3026) | Loss 1.3909(1.3626) | Error 0.9011(0.9010) Steps 482(492.83) | Grad Norm 19.3523(11.6530) | Total Time 10.00(10.00)\n",
      "Iter 1000 | Time 15.3330(15.1097) | Bit/dim 1.3480(1.3586) | Xent 2.3026(2.3026) | Loss 1.3480(1.3586) | Error 0.9033(0.8994) Steps 500(493.34) | Grad Norm 13.7343(11.3406) | Total Time 10.00(10.00)\n",
      "Iter 1010 | Time 15.3876(15.1155) | Bit/dim 1.3538(1.3539) | Xent 2.3026(2.3026) | Loss 1.3538(1.3539) | Error 0.9022(0.9012) Steps 500(493.85) | Grad Norm 16.4881(11.1195) | Total Time 10.00(10.00)\n",
      "Iter 1020 | Time 15.2445(15.1234) | Bit/dim 1.3425(1.3500) | Xent 2.3026(2.3026) | Loss 1.3425(1.3500) | Error 0.8989(0.9018) Steps 500(494.22) | Grad Norm 13.7899(10.9145) | Total Time 10.00(10.00)\n",
      "Iter 1030 | Time 15.3779(15.1724) | Bit/dim 1.3289(1.3465) | Xent 2.3026(2.3026) | Loss 1.3289(1.3465) | Error 0.9011(0.9017) Steps 500(494.80) | Grad Norm 11.0914(10.8151) | Total Time 10.00(10.00)\n",
      "Iter 1040 | Time 15.2211(15.2022) | Bit/dim 1.3319(1.3448) | Xent 2.3026(2.3026) | Loss 1.3319(1.3448) | Error 0.8889(0.8993) Steps 494(495.05) | Grad Norm 2.4338(10.5206) | Total Time 10.00(10.00)\n",
      "Iter 1050 | Time 15.1704(15.2200) | Bit/dim 1.3606(1.3416) | Xent 2.3026(2.3026) | Loss 1.3606(1.3416) | Error 0.9156(0.9007) Steps 494(495.41) | Grad Norm 19.1178(10.9455) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0016 | Time 61.2064, Epoch Time 1078.5335(972.2696), Bit/dim 1.3219, Xent 2.3026, Loss 1.3219, Error 0.9031\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1060 | Time 15.4067(15.2229) | Bit/dim 1.3280(1.3409) | Xent 2.3026(2.3026) | Loss 1.3280(1.3409) | Error 0.8867(0.9020) Steps 506(496.03) | Grad Norm 11.3774(11.4772) | Total Time 10.00(10.00)\n",
      "Iter 1070 | Time 15.0220(15.1914) | Bit/dim 1.3183(1.3374) | Xent 2.3026(2.3026) | Loss 1.3183(1.3374) | Error 0.8911(0.9015) Steps 494(496.32) | Grad Norm 2.0439(10.8909) | Total Time 10.00(10.00)\n",
      "Iter 1080 | Time 15.1126(15.2035) | Bit/dim 1.3136(1.3344) | Xent 2.3026(2.3026) | Loss 1.3136(1.3344) | Error 0.8900(0.9009) Steps 500(497.26) | Grad Norm 5.3395(11.0292) | Total Time 10.00(10.00)\n",
      "Iter 1090 | Time 15.4957(15.2167) | Bit/dim 1.3092(1.3308) | Xent 2.3026(2.3026) | Loss 1.3092(1.3308) | Error 0.8978(0.9006) Steps 506(498.30) | Grad Norm 11.4844(10.7215) | Total Time 10.00(10.00)\n",
      "Iter 1100 | Time 15.4002(15.2157) | Bit/dim 1.3144(1.3281) | Xent 2.3026(2.3026) | Loss 1.3144(1.3281) | Error 0.9167(0.9020) Steps 506(498.77) | Grad Norm 14.6201(10.3741) | Total Time 10.00(10.00)\n",
      "Iter 1110 | Time 15.3041(15.2212) | Bit/dim 1.3284(1.3277) | Xent 2.3026(2.3026) | Loss 1.3284(1.3277) | Error 0.8844(0.9011) Steps 506(499.29) | Grad Norm 15.0483(11.1356) | Total Time 10.00(10.00)\n",
      "Iter 1120 | Time 15.3712(15.2500) | Bit/dim 1.3131(1.3209) | Xent 2.3026(2.3026) | Loss 1.3131(1.3209) | Error 0.8922(0.9012) Steps 506(500.12) | Grad Norm 5.2258(9.7979) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0017 | Time 61.8505, Epoch Time 1080.8776(975.5278), Bit/dim 1.3027, Xent 2.3026, Loss 1.3027, Error 0.9031\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1130 | Time 15.0715(15.2577) | Bit/dim 1.2951(1.3197) | Xent 2.3026(2.3026) | Loss 1.2951(1.3197) | Error 0.8944(0.9013) Steps 500(500.56) | Grad Norm 15.1560(10.2123) | Total Time 10.00(10.00)\n",
      "Iter 1140 | Time 15.6510(15.3093) | Bit/dim 1.2897(1.3198) | Xent 2.3026(2.3026) | Loss 1.2897(1.3198) | Error 0.8922(0.9009) Steps 506(501.38) | Grad Norm 12.8268(11.1725) | Total Time 10.00(10.00)\n",
      "Iter 1150 | Time 15.1140(15.3524) | Bit/dim 1.2938(1.3183) | Xent 2.3026(2.3026) | Loss 1.2938(1.3183) | Error 0.9078(0.9016) Steps 500(501.78) | Grad Norm 2.3946(10.8215) | Total Time 10.00(10.00)\n",
      "Iter 1160 | Time 15.4031(15.3435) | Bit/dim 1.2910(1.3132) | Xent 2.3026(2.3026) | Loss 1.2910(1.3132) | Error 0.9122(0.9016) Steps 506(502.26) | Grad Norm 11.8357(10.5549) | Total Time 10.00(10.00)\n",
      "Iter 1170 | Time 15.3628(15.3246) | Bit/dim 1.3308(1.3119) | Xent 2.3026(2.3026) | Loss 1.3308(1.3119) | Error 0.9022(0.9019) Steps 506(502.62) | Grad Norm 21.2837(11.1866) | Total Time 10.00(10.00)\n",
      "Iter 1180 | Time 15.0156(15.3264) | Bit/dim 1.3702(1.3160) | Xent 2.3026(2.3026) | Loss 1.3702(1.3160) | Error 0.8922(0.9018) Steps 500(502.74) | Grad Norm 21.1722(11.9442) | Total Time 10.00(10.00)\n",
      "Iter 1190 | Time 15.1212(15.3455) | Bit/dim 1.3286(1.3248) | Xent 2.3026(2.3026) | Loss 1.3286(1.3248) | Error 0.8989(0.9003) Steps 500(503.26) | Grad Norm 11.1152(12.9873) | Total Time 10.00(10.00)\n",
      "Iter 1200 | Time 15.4986(15.3459) | Bit/dim 1.3125(1.3233) | Xent 2.3026(2.3026) | Loss 1.3125(1.3233) | Error 0.8989(0.8999) Steps 506(503.36) | Grad Norm 12.8159(12.2879) | Total Time 10.00(10.00)\n",
      "Iter 1210 | Time 15.3322(15.3305) | Bit/dim 1.2909(1.3185) | Xent 2.3026(2.3026) | Loss 1.2909(1.3185) | Error 0.9056(0.8999) Steps 500(502.96) | Grad Norm 5.5624(11.3832) | Total Time 10.00(10.00)\n",
      "Iter 1220 | Time 15.8682(15.3664) | Bit/dim 1.2725(1.3093) | Xent 2.3026(2.3026) | Loss 1.2725(1.3093) | Error 0.8756(0.9008) Steps 506(502.99) | Grad Norm 2.8982(9.9954) | Total Time 10.00(10.00)\n",
      "Iter 1230 | Time 15.6499(15.4253) | Bit/dim 1.2942(1.3022) | Xent 2.3026(2.3026) | Loss 1.2942(1.3022) | Error 0.8978(0.9005) Steps 500(503.30) | Grad Norm 6.1481(8.8764) | Total Time 10.00(10.00)\n",
      "Iter 1250 | Time 15.5551(15.3850) | Bit/dim 1.2707(1.2919) | Xent 2.3026(2.3026) | Loss 1.2707(1.2919) | Error 0.9067(0.9016) Steps 506(503.87) | Grad Norm 13.3887(8.4806) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0019 | Time 61.9357, Epoch Time 1090.8404(982.3144), Bit/dim 1.2664, Xent 2.3026, Loss 1.2664, Error 0.9031\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1260 | Time 15.5328(15.3624) | Bit/dim 1.2952(1.2933) | Xent 2.3026(2.3026) | Loss 1.2952(1.2933) | Error 0.9067(0.9027) Steps 506(503.96) | Grad Norm 14.8866(9.7291) | Total Time 10.00(10.00)\n",
      "Iter 1270 | Time 15.4247(15.3708) | Bit/dim 1.2811(1.2902) | Xent 2.3026(2.3026) | Loss 1.2811(1.2902) | Error 0.8944(0.9017) Steps 506(504.20) | Grad Norm 2.5392(8.7167) | Total Time 10.00(10.00)\n",
      "Iter 1280 | Time 15.0545(15.3621) | Bit/dim 1.3386(1.2898) | Xent 2.3026(2.3026) | Loss 1.3386(1.2898) | Error 0.8933(0.9022) Steps 500(504.19) | Grad Norm 19.0405(9.5342) | Total Time 10.00(10.00)\n",
      "Iter 1290 | Time 15.4883(15.3629) | Bit/dim 1.2757(1.2904) | Xent 2.3026(2.3026) | Loss 1.2757(1.2904) | Error 0.9111(0.9012) Steps 506(504.67) | Grad Norm 7.8509(10.2461) | Total Time 10.00(10.00)\n",
      "Iter 1300 | Time 15.5635(15.3769) | Bit/dim 1.2678(1.2877) | Xent 2.3026(2.3026) | Loss 1.2678(1.2877) | Error 0.8989(0.9018) Steps 506(504.55) | Grad Norm 9.7662(10.5310) | Total Time 10.00(10.00)\n",
      "Iter 1310 | Time 15.3738(15.3819) | Bit/dim 1.2835(1.2834) | Xent 2.3026(2.3026) | Loss 1.2835(1.2834) | Error 0.8889(0.9008) Steps 506(504.60) | Grad Norm 15.9566(10.3327) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0020 | Time 62.3908, Epoch Time 1091.6455(985.5943), Bit/dim 1.2612, Xent 2.3026, Loss 1.2612, Error 0.9031\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1320 | Time 15.3058(15.4039) | Bit/dim 1.2676(1.2818) | Xent 2.3026(2.3026) | Loss 1.2676(1.2818) | Error 0.8900(0.9006) Steps 500(504.48) | Grad Norm 6.6123(10.3911) | Total Time 10.00(10.00)\n",
      "Iter 1330 | Time 15.1708(15.3888) | Bit/dim 1.2932(1.2827) | Xent 2.3026(2.3026) | Loss 1.2932(1.2827) | Error 0.8978(0.9004) Steps 500(504.24) | Grad Norm 18.5578(11.0005) | Total Time 10.00(10.00)\n",
      "Iter 1340 | Time 15.2517(15.3883) | Bit/dim 1.2794(1.2786) | Xent 2.3026(2.3026) | Loss 1.2794(1.2786) | Error 0.8878(0.9013) Steps 506(504.37) | Grad Norm 10.6472(10.8176) | Total Time 10.00(10.00)\n",
      "Iter 1350 | Time 15.3308(15.3935) | Bit/dim 1.2504(1.2776) | Xent 2.3026(2.3026) | Loss 1.2504(1.2776) | Error 0.8956(0.8999) Steps 506(504.48) | Grad Norm 2.9657(10.6825) | Total Time 10.00(10.00)\n",
      "Iter 1360 | Time 15.5450(15.4264) | Bit/dim 1.2496(1.2733) | Xent 2.3026(2.3026) | Loss 1.2496(1.2733) | Error 0.9111(0.9014) Steps 506(504.56) | Grad Norm 3.2907(10.3413) | Total Time 10.00(10.00)\n",
      "Iter 1370 | Time 15.5323(15.4572) | Bit/dim 1.2645(1.2705) | Xent 2.3026(2.3026) | Loss 1.2645(1.2705) | Error 0.8878(0.9016) Steps 500(504.45) | Grad Norm 14.6207(10.6565) | Total Time 10.00(10.00)\n",
      "Iter 1380 | Time 15.5959(15.4859) | Bit/dim 1.2511(1.2707) | Xent 2.3026(2.3026) | Loss 1.2511(1.2707) | Error 0.9056(0.9014) Steps 506(504.39) | Grad Norm 1.0374(10.7582) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0021 | Time 61.9248, Epoch Time 1095.8797(988.9029), Bit/dim 1.2552, Xent 2.3026, Loss 1.2552, Error 0.9031\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1390 | Time 15.5515(15.4785) | Bit/dim 1.2477(1.2673) | Xent 2.3026(2.3026) | Loss 1.2477(1.2673) | Error 0.9033(0.9016) Steps 506(504.49) | Grad Norm 9.5708(10.5086) | Total Time 10.00(10.00)\n",
      "Iter 1400 | Time 15.5028(15.4541) | Bit/dim 1.2677(1.2692) | Xent 2.3026(2.3026) | Loss 1.2677(1.2692) | Error 0.8900(0.9012) Steps 506(504.43) | Grad Norm 12.6968(11.2374) | Total Time 10.00(10.00)\n",
      "Iter 1410 | Time 15.2742(15.4338) | Bit/dim 1.2513(1.2678) | Xent 2.3026(2.3026) | Loss 1.2513(1.2678) | Error 0.8956(0.9018) Steps 506(504.50) | Grad Norm 5.6048(11.0182) | Total Time 10.00(10.00)\n",
      "Iter 1420 | Time 15.5234(15.4052) | Bit/dim 1.2321(1.2650) | Xent 2.3026(2.3026) | Loss 1.2321(1.2650) | Error 0.9078(0.9033) Steps 506(504.44) | Grad Norm 7.3231(10.8054) | Total Time 10.00(10.00)\n",
      "Iter 1430 | Time 15.1594(15.4191) | Bit/dim 1.2916(1.2640) | Xent 2.3026(2.3026) | Loss 1.2916(1.2640) | Error 0.9022(0.9019) Steps 500(504.50) | Grad Norm 19.1483(11.0950) | Total Time 10.00(10.00)\n",
      "Iter 1440 | Time 15.7318(15.4610) | Bit/dim 1.2530(1.2622) | Xent 2.3026(2.3026) | Loss 1.2530(1.2622) | Error 0.8878(0.9011) Steps 506(504.72) | Grad Norm 7.2006(11.1501) | Total Time 10.00(10.00)\n",
      "Iter 1450 | Time 15.2409(15.4562) | Bit/dim 1.2418(1.2599) | Xent 2.3026(2.3026) | Loss 1.2418(1.2599) | Error 0.9189(0.9005) Steps 506(504.74) | Grad Norm 2.0982(10.6757) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0022 | Time 62.6020, Epoch Time 1094.7089(992.0771), Bit/dim 1.2547, Xent 2.3026, Loss 1.2547, Error 0.9031\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1460 | Time 15.2405(15.4620) | Bit/dim 1.2678(1.2583) | Xent 2.3026(2.3026) | Loss 1.2678(1.2583) | Error 0.9022(0.9007) Steps 506(504.60) | Grad Norm 12.5324(10.8340) | Total Time 10.00(10.00)\n",
      "Iter 1470 | Time 15.4732(15.4606) | Bit/dim 1.2551(1.2573) | Xent 2.3026(2.3026) | Loss 1.2551(1.2573) | Error 0.9033(0.8993) Steps 506(504.82) | Grad Norm 15.2849(10.8847) | Total Time 10.00(10.00)\n",
      "Iter 1480 | Time 15.3041(15.4380) | Bit/dim 1.2545(1.2578) | Xent 2.3026(2.3026) | Loss 1.2545(1.2578) | Error 0.9033(0.9006) Steps 506(504.67) | Grad Norm 8.7472(11.0665) | Total Time 10.00(10.00)\n",
      "Iter 1490 | Time 15.2974(15.4134) | Bit/dim 1.2192(1.2553) | Xent 2.3026(2.3026) | Loss 1.2192(1.2553) | Error 0.8956(0.8996) Steps 506(504.88) | Grad Norm 7.2035(10.6696) | Total Time 10.00(10.00)\n",
      "Iter 1500 | Time 15.5802(15.3953) | Bit/dim 1.2424(1.2530) | Xent 2.3026(2.3026) | Loss 1.2424(1.2530) | Error 0.9089(0.9008) Steps 506(504.87) | Grad Norm 11.3044(10.4797) | Total Time 10.00(10.00)\n",
      "Iter 1510 | Time 15.2355(15.4040) | Bit/dim 1.2697(1.2520) | Xent 2.3026(2.3026) | Loss 1.2697(1.2520) | Error 0.8933(0.9004) Steps 500(504.86) | Grad Norm 17.2222(10.9000) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0023 | Time 61.8183, Epoch Time 1091.8746(995.0710), Bit/dim 1.2301, Xent 2.3026, Loss 1.2301, Error 0.9031\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1520 | Time 15.5973(15.4196) | Bit/dim 1.2444(1.2503) | Xent 2.3026(2.3026) | Loss 1.2444(1.2503) | Error 0.8878(0.9014) Steps 506(505.02) | Grad Norm 8.8162(10.6314) | Total Time 10.00(10.00)\n",
      "Iter 1530 | Time 15.5813(15.4254) | Bit/dim 1.2321(1.2468) | Xent 2.3026(2.3026) | Loss 1.2321(1.2468) | Error 0.8889(0.9015) Steps 506(505.11) | Grad Norm 1.2619(10.0654) | Total Time 10.00(10.00)\n",
      "Iter 1540 | Time 15.4426(15.4194) | Bit/dim 1.2449(1.2475) | Xent 2.3026(2.3026) | Loss 1.2449(1.2475) | Error 0.9100(0.9009) Steps 506(504.87) | Grad Norm 10.9149(10.5576) | Total Time 10.00(10.00)\n",
      "Iter 1550 | Time 15.4705(15.4278) | Bit/dim 1.2253(1.2471) | Xent 2.3026(2.3026) | Loss 1.2253(1.2471) | Error 0.9100(0.9011) Steps 506(504.85) | Grad Norm 11.0593(10.9727) | Total Time 10.00(10.00)\n",
      "Iter 1560 | Time 15.6943(15.4361) | Bit/dim 1.2166(1.2441) | Xent 2.3026(2.3026) | Loss 1.2166(1.2441) | Error 0.8911(0.9008) Steps 506(505.01) | Grad Norm 1.9501(10.3056) | Total Time 10.00(10.00)\n",
      "Iter 1570 | Time 15.5936(15.4531) | Bit/dim 1.2359(1.2412) | Xent 2.3026(2.3026) | Loss 1.2359(1.2412) | Error 0.8956(0.9008) Steps 506(505.27) | Grad Norm 13.0456(9.6293) | Total Time 10.00(10.00)\n",
      "Iter 1580 | Time 15.4032(15.4625) | Bit/dim 1.2429(1.2392) | Xent 2.3026(2.3026) | Loss 1.2429(1.2392) | Error 0.9056(0.9024) Steps 506(505.17) | Grad Norm 13.2821(9.5826) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0024 | Time 61.6812, Epoch Time 1095.6815(998.0893), Bit/dim 1.2217, Xent 2.3026, Loss 1.2217, Error 0.9031\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1590 | Time 15.6369(15.4762) | Bit/dim 1.2279(1.2414) | Xent 2.3026(2.3026) | Loss 1.2279(1.2414) | Error 0.8956(0.9018) Steps 506(506.46) | Grad Norm 8.0478(10.5810) | Total Time 10.00(10.00)\n",
      "Iter 1600 | Time 15.4261(15.4768) | Bit/dim 1.2312(1.2398) | Xent 2.3026(2.3026) | Loss 1.2312(1.2398) | Error 0.8956(0.9013) Steps 506(506.20) | Grad Norm 4.6136(10.2968) | Total Time 10.00(10.00)\n",
      "Iter 1610 | Time 15.4609(15.4739) | Bit/dim 1.2243(1.2361) | Xent 2.3026(2.3026) | Loss 1.2243(1.2361) | Error 0.9167(0.9012) Steps 506(505.84) | Grad Norm 3.5518(10.0121) | Total Time 10.00(10.00)\n",
      "Iter 1620 | Time 15.5706(15.5062) | Bit/dim 1.2249(1.2365) | Xent 2.3026(2.3026) | Loss 1.2249(1.2365) | Error 0.9167(0.9021) Steps 506(506.38) | Grad Norm 3.1876(10.4922) | Total Time 10.00(10.00)\n",
      "Iter 1630 | Time 15.8878(15.5755) | Bit/dim 1.2603(1.2451) | Xent 2.3026(2.3026) | Loss 1.2603(1.2451) | Error 0.8967(0.9019) Steps 524(508.78) | Grad Norm 13.8928(11.9729) | Total Time 10.00(10.00)\n",
      "Iter 1640 | Time 15.8279(15.6103) | Bit/dim 1.2357(1.2467) | Xent 2.3026(2.3026) | Loss 1.2357(1.2467) | Error 0.8922(0.9008) Steps 512(509.94) | Grad Norm 7.1231(11.9437) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0025 | Time 64.4466, Epoch Time 1107.6292(1001.3755), Bit/dim 1.2451, Xent 2.3026, Loss 1.2451, Error 0.9031\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1650 | Time 16.1016(15.6250) | Bit/dim 1.2640(1.2458) | Xent 2.3026(2.3026) | Loss 1.2640(1.2458) | Error 0.9067(0.9020) Steps 524(510.34) | Grad Norm 17.1072(11.5488) | Total Time 10.00(10.00)\n",
      "Iter 1660 | Time 15.6719(15.6127) | Bit/dim 1.2291(1.2444) | Xent 2.3026(2.3026) | Loss 1.2291(1.2444) | Error 0.8878(0.9013) Steps 506(510.49) | Grad Norm 7.4373(11.5366) | Total Time 10.00(10.00)\n",
      "Iter 1670 | Time 15.4551(15.6366) | Bit/dim 1.2160(1.2401) | Xent 2.3026(2.3026) | Loss 1.2160(1.2401) | Error 0.8967(0.9002) Steps 506(510.87) | Grad Norm 2.6394(10.8887) | Total Time 10.00(10.00)\n",
      "Iter 1680 | Time 16.2032(15.6596) | Bit/dim 1.2364(1.2383) | Xent 2.3026(2.3026) | Loss 1.2364(1.2383) | Error 0.9044(0.9015) Steps 524(512.27) | Grad Norm 17.4084(10.9041) | Total Time 10.00(10.00)\n",
      "Iter 1690 | Time 15.4533(15.6598) | Bit/dim 1.2127(1.2381) | Xent 2.3026(2.3026) | Loss 1.2127(1.2381) | Error 0.9122(0.9022) Steps 506(512.38) | Grad Norm 1.6082(11.1946) | Total Time 10.00(10.00)\n",
      "Iter 1700 | Time 15.5966(15.7088) | Bit/dim 1.2145(1.2333) | Xent 2.3026(2.3026) | Loss 1.2145(1.2333) | Error 0.8978(0.9005) Steps 506(512.45) | Grad Norm 6.1692(9.9682) | Total Time 10.00(10.00)\n",
      "Iter 1710 | Time 15.1973(15.7001) | Bit/dim 1.2223(1.2288) | Xent 2.3026(2.3026) | Loss 1.2223(1.2288) | Error 0.9144(0.9011) Steps 506(512.62) | Grad Norm 7.5174(8.6318) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0026 | Time 64.7403, Epoch Time 1114.4301(1004.7671), Bit/dim 1.2116, Xent 2.3026, Loss 1.2116, Error 0.9031\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1720 | Time 15.6326(15.6797) | Bit/dim 1.2131(1.2246) | Xent 2.3026(2.3026) | Loss 1.2131(1.2246) | Error 0.8800(0.9010) Steps 518(512.76) | Grad Norm 4.1744(8.0743) | Total Time 10.00(10.00)\n",
      "Iter 1730 | Time 15.6272(15.6630) | Bit/dim 1.2106(1.2217) | Xent 2.3026(2.3026) | Loss 1.2106(1.2217) | Error 0.9111(0.9015) Steps 518(512.79) | Grad Norm 3.9931(7.7470) | Total Time 10.00(10.00)\n",
      "Iter 1740 | Time 16.2661(15.6927) | Bit/dim 1.2434(1.2245) | Xent 2.3026(2.3026) | Loss 1.2434(1.2245) | Error 0.9078(0.9004) Steps 524(513.57) | Grad Norm 13.2487(8.8849) | Total Time 10.00(10.00)\n",
      "Iter 1750 | Time 15.7503(15.7130) | Bit/dim 1.2212(1.2328) | Xent 2.3026(2.3026) | Loss 1.2212(1.2328) | Error 0.9144(0.9016) Steps 530(514.67) | Grad Norm 5.4354(11.0174) | Total Time 10.00(10.00)\n",
      "Iter 1760 | Time 16.2205(15.7477) | Bit/dim 1.2291(1.2354) | Xent 2.3026(2.3026) | Loss 1.2291(1.2354) | Error 0.9100(0.9015) Steps 524(515.90) | Grad Norm 14.5749(11.2864) | Total Time 10.00(10.00)\n",
      "Iter 1770 | Time 15.8783(15.7792) | Bit/dim 1.2149(1.2327) | Xent 2.3026(2.3026) | Loss 1.2149(1.2327) | Error 0.9033(0.9017) Steps 518(515.64) | Grad Norm 2.8496(10.6581) | Total Time 10.00(10.00)\n",
      "Iter 1780 | Time 15.8801(15.7989) | Bit/dim 1.2060(1.2278) | Xent 2.3026(2.3026) | Loss 1.2060(1.2278) | Error 0.9078(0.9024) Steps 518(516.61) | Grad Norm 1.0885(9.0313) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0027 | Time 63.1578, Epoch Time 1117.6932(1008.1549), Bit/dim 1.2051, Xent 2.3026, Loss 1.2051, Error 0.9031\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1790 | Time 15.6519(15.7528) | Bit/dim 1.2195(1.2240) | Xent 2.3026(2.3026) | Loss 1.2195(1.2240) | Error 0.8933(0.9012) Steps 518(516.97) | Grad Norm 3.8460(7.7510) | Total Time 10.00(10.00)\n",
      "Iter 1800 | Time 15.7052(15.7479) | Bit/dim 1.1905(1.2196) | Xent 2.3026(2.3026) | Loss 1.1905(1.2196) | Error 0.9000(0.9012) Steps 506(516.23) | Grad Norm 6.5466(7.3918) | Total Time 10.00(10.00)\n",
      "Iter 1810 | Time 15.9756(15.7568) | Bit/dim 1.1872(1.2176) | Xent 2.3026(2.3026) | Loss 1.1872(1.2176) | Error 0.9222(0.9015) Steps 524(515.76) | Grad Norm 9.2261(7.7063) | Total Time 10.00(10.00)\n",
      "Iter 1820 | Time 15.8118(15.7450) | Bit/dim 1.2284(1.2254) | Xent 2.3026(2.3026) | Loss 1.2284(1.2254) | Error 0.8978(0.9009) Steps 524(516.59) | Grad Norm 9.3528(9.8073) | Total Time 10.00(10.00)\n",
      "Iter 1830 | Time 15.8309(15.7575) | Bit/dim 1.2340(1.2299) | Xent 2.3026(2.3026) | Loss 1.2340(1.2299) | Error 0.8978(0.9010) Steps 524(517.31) | Grad Norm 10.2605(10.6563) | Total Time 10.00(10.00)\n",
      "Iter 1840 | Time 15.9289(15.7863) | Bit/dim 1.2278(1.2300) | Xent 2.3026(2.3026) | Loss 1.2278(1.2300) | Error 0.8989(0.9012) Steps 530(518.56) | Grad Norm 5.7460(10.8300) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0028 | Time 64.9868, Epoch Time 1117.4274(1011.4331), Bit/dim 1.2039, Xent 2.3026, Loss 1.2039, Error 0.9031\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1850 | Time 15.6215(15.7724) | Bit/dim 1.1953(1.2247) | Xent 2.3026(2.3026) | Loss 1.1953(1.2247) | Error 0.9044(0.9026) Steps 518(518.64) | Grad Norm 4.9582(9.6432) | Total Time 10.00(10.00)\n",
      "Iter 1860 | Time 15.8604(15.7937) | Bit/dim 1.1799(1.2195) | Xent 2.3026(2.3026) | Loss 1.1799(1.2195) | Error 0.9100(0.9019) Steps 524(518.79) | Grad Norm 3.6705(7.9704) | Total Time 10.00(10.00)\n",
      "Iter 1870 | Time 15.5915(15.7901) | Bit/dim 1.1934(1.2132) | Xent 2.3026(2.3026) | Loss 1.1934(1.2132) | Error 0.8867(0.9004) Steps 518(518.72) | Grad Norm 1.7980(6.2655) | Total Time 10.00(10.00)\n",
      "Iter 1880 | Time 15.6383(15.7838) | Bit/dim 1.2124(1.2119) | Xent 2.3026(2.3026) | Loss 1.2124(1.2119) | Error 0.9011(0.8995) Steps 524(518.66) | Grad Norm 11.8706(6.8379) | Total Time 10.00(10.00)\n",
      "Iter 1890 | Time 15.9460(15.8046) | Bit/dim 1.1982(1.2093) | Xent 2.3026(2.3026) | Loss 1.1982(1.2093) | Error 0.9133(0.9014) Steps 518(518.98) | Grad Norm 2.5598(7.0227) | Total Time 10.00(10.00)\n",
      "Iter 1900 | Time 15.6674(15.8232) | Bit/dim 1.2197(1.2096) | Xent 2.3026(2.3026) | Loss 1.2197(1.2096) | Error 0.9133(0.9019) Steps 506(518.42) | Grad Norm 12.8297(7.6972) | Total Time 10.00(10.00)\n",
      "Iter 1910 | Time 15.4962(15.8232) | Bit/dim 1.1984(1.2082) | Xent 2.3026(2.3026) | Loss 1.1984(1.2082) | Error 0.9056(0.9022) Steps 506(517.95) | Grad Norm 9.9752(7.4841) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0029 | Time 64.3747, Epoch Time 1123.1849(1014.7856), Bit/dim 1.1946, Xent 2.3026, Loss 1.1946, Error 0.9031\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1920 | Time 15.4202(15.8226) | Bit/dim 1.2773(1.2165) | Xent 2.3026(2.3026) | Loss 1.2773(1.2165) | Error 0.9156(0.9021) Steps 500(517.91) | Grad Norm 17.7622(9.9549) | Total Time 10.00(10.00)\n",
      "Iter 1930 | Time 16.1473(15.8161) | Bit/dim 1.2155(1.2230) | Xent 2.3026(2.3026) | Loss 1.2155(1.2230) | Error 0.8967(0.9011) Steps 530(518.11) | Grad Norm 7.3956(11.0346) | Total Time 10.00(10.00)\n",
      "Iter 1940 | Time 16.1186(15.8486) | Bit/dim 1.2092(1.2243) | Xent 2.3026(2.3026) | Loss 1.2092(1.2243) | Error 0.9167(0.9013) Steps 524(518.96) | Grad Norm 5.7780(11.2978) | Total Time 10.00(10.00)\n",
      "Iter 1950 | Time 15.8621(15.8597) | Bit/dim 1.1984(1.2195) | Xent 2.3026(2.3026) | Loss 1.1984(1.2195) | Error 0.8944(0.9005) Steps 524(520.47) | Grad Norm 3.8016(9.8783) | Total Time 10.00(10.00)\n",
      "Iter 1960 | Time 15.6749(15.8915) | Bit/dim 1.2000(1.2135) | Xent 2.3026(2.3026) | Loss 1.2000(1.2135) | Error 0.8944(0.9005) Steps 524(521.69) | Grad Norm 8.0361(8.6716) | Total Time 10.00(10.00)\n",
      "Iter 1970 | Time 15.7941(15.8755) | Bit/dim 1.1930(1.2090) | Xent 2.3026(2.3026) | Loss 1.1930(1.2090) | Error 0.9111(0.9008) Steps 524(522.00) | Grad Norm 7.4850(7.9682) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0030 | Time 61.5617, Epoch Time 1123.0075(1018.0323), Bit/dim 1.2273, Xent 2.3026, Loss 1.2273, Error 0.9031\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1980 | Time 15.5557(15.8703) | Bit/dim 1.2328(1.2089) | Xent 2.3026(2.3026) | Loss 1.2328(1.2089) | Error 0.9067(0.9022) Steps 506(521.56) | Grad Norm 18.2664(8.8444) | Total Time 10.00(10.00)\n",
      "Iter 1990 | Time 15.5319(15.8714) | Bit/dim 1.2109(1.2078) | Xent 2.3026(2.3026) | Loss 1.2109(1.2078) | Error 0.8811(0.9021) Steps 512(521.51) | Grad Norm 13.0415(9.0953) | Total Time 10.00(10.00)\n",
      "Iter 2000 | Time 15.7894(15.8580) | Bit/dim 1.2122(1.2050) | Xent 2.3026(2.3026) | Loss 1.2122(1.2050) | Error 0.8856(0.9017) Steps 512(521.51) | Grad Norm 15.1655(9.0927) | Total Time 10.00(10.00)\n",
      "Iter 2010 | Time 15.4026(15.8689) | Bit/dim 1.2517(1.2070) | Xent 2.3026(2.3026) | Loss 1.2517(1.2070) | Error 0.8944(0.9005) Steps 506(521.61) | Grad Norm 19.3170(9.7692) | Total Time 10.00(10.00)\n",
      "Iter 2020 | Time 15.4633(15.8743) | Bit/dim 1.2462(1.2184) | Xent 2.3026(2.3026) | Loss 1.2462(1.2184) | Error 0.9133(0.8998) Steps 506(522.07) | Grad Norm 16.8670(11.5150) | Total Time 10.00(10.00)\n",
      "Iter 2030 | Time 15.7368(15.9152) | Bit/dim 1.2195(1.2199) | Xent 2.3026(2.3026) | Loss 1.2195(1.2199) | Error 0.8889(0.9009) Steps 518(522.77) | Grad Norm 6.0391(11.4176) | Total Time 10.00(10.00)\n",
      "Iter 2040 | Time 16.0945(15.9663) | Bit/dim 1.1971(1.2157) | Xent 2.3026(2.3026) | Loss 1.1971(1.2157) | Error 0.9089(0.9012) Steps 524(524.18) | Grad Norm 2.9390(10.0229) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0031 | Time 64.3724, Epoch Time 1130.9899(1021.4210), Bit/dim 1.1916, Xent 2.3026, Loss 1.1916, Error 0.9031\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2050 | Time 16.0582(15.9929) | Bit/dim 1.2002(1.2114) | Xent 2.3026(2.3026) | Loss 1.2002(1.2114) | Error 0.8844(0.9022) Steps 518(524.40) | Grad Norm 5.8065(8.6832) | Total Time 10.00(10.00)\n",
      "Iter 2060 | Time 15.8876(15.9915) | Bit/dim 1.1879(1.2057) | Xent 2.3026(2.3026) | Loss 1.1879(1.2057) | Error 0.9067(0.9012) Steps 524(524.59) | Grad Norm 3.3938(7.7780) | Total Time 10.00(10.00)\n",
      "Iter 2070 | Time 15.8560(15.9941) | Bit/dim 1.1880(1.2024) | Xent 2.3026(2.3026) | Loss 1.1880(1.2024) | Error 0.9067(0.9018) Steps 524(524.44) | Grad Norm 1.8799(7.7425) | Total Time 10.00(10.00)\n",
      "Iter 2080 | Time 15.2267(15.9616) | Bit/dim 1.3099(1.2125) | Xent 2.3026(2.3026) | Loss 1.3099(1.2125) | Error 0.9089(0.9018) Steps 506(523.76) | Grad Norm 14.4931(9.9067) | Total Time 10.00(10.00)\n",
      "Iter 2090 | Time 16.6841(16.0119) | Bit/dim 1.2207(1.2214) | Xent 2.3026(2.3026) | Loss 1.2207(1.2214) | Error 0.9144(0.9011) Steps 536(526.01) | Grad Norm 15.1258(10.7488) | Total Time 10.00(10.00)\n",
      "Iter 2100 | Time 16.5289(16.0639) | Bit/dim 1.2208(1.2207) | Xent 2.3026(2.3026) | Loss 1.2208(1.2207) | Error 0.9022(0.9020) Steps 536(527.21) | Grad Norm 13.8188(10.8250) | Total Time 10.00(10.00)\n",
      "Iter 2110 | Time 16.1488(16.0938) | Bit/dim 1.1810(1.2168) | Xent 2.3026(2.3026) | Loss 1.1810(1.2168) | Error 0.9078(0.9014) Steps 530(528.26) | Grad Norm 1.9285(10.1184) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0032 | Time 64.5933, Epoch Time 1138.1928(1024.9242), Bit/dim 1.1887, Xent 2.3026, Loss 1.1887, Error 0.9031\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2120 | Time 16.4803(16.1305) | Bit/dim 1.1925(1.2107) | Xent 2.3026(2.3026) | Loss 1.1925(1.2107) | Error 0.9022(0.9023) Steps 530(529.01) | Grad Norm 5.2533(8.7546) | Total Time 10.00(10.00)\n",
      "Iter 2130 | Time 16.0517(16.1037) | Bit/dim 1.2046(1.2042) | Xent 2.3026(2.3026) | Loss 1.2046(1.2042) | Error 0.8856(0.9019) Steps 530(528.94) | Grad Norm 2.3577(7.1889) | Total Time 10.00(10.00)\n",
      "Iter 2140 | Time 16.2964(16.0783) | Bit/dim 1.1928(1.1999) | Xent 2.3026(2.3026) | Loss 1.1928(1.1999) | Error 0.9033(0.9019) Steps 530(527.77) | Grad Norm 9.5946(7.1941) | Total Time 10.00(10.00)\n",
      "Iter 2150 | Time 15.9652(16.0438) | Bit/dim 1.2046(1.1982) | Xent 2.3026(2.3026) | Loss 1.2046(1.1982) | Error 0.8933(0.9015) Steps 530(526.97) | Grad Norm 17.8697(7.7766) | Total Time 10.00(10.00)\n",
      "Iter 2160 | Time 17.2971(16.0234) | Bit/dim 1.3076(1.2131) | Xent 2.3026(2.3026) | Loss 1.3076(1.2131) | Error 0.9100(0.9022) Steps 572(526.80) | Grad Norm 26.4844(10.0185) | Total Time 10.00(10.00)\n",
      "Iter 2170 | Time 16.1692(16.0297) | Bit/dim 1.2104(1.2201) | Xent 2.3026(2.3026) | Loss 1.2104(1.2201) | Error 0.9011(0.9008) Steps 536(526.93) | Grad Norm 11.8420(11.0976) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0033 | Time 64.5207, Epoch Time 1138.5304(1028.3324), Bit/dim 1.2019, Xent 2.3026, Loss 1.2019, Error 0.9031\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2180 | Time 15.8428(16.1056) | Bit/dim 1.2193(1.2182) | Xent 2.3026(2.3026) | Loss 1.2193(1.2182) | Error 0.8956(0.9003) Steps 524(528.49) | Grad Norm 11.1402(11.2139) | Total Time 10.00(10.00)\n",
      "Iter 2190 | Time 16.1478(16.1700) | Bit/dim 1.1871(1.2140) | Xent 2.3026(2.3026) | Loss 1.1871(1.2140) | Error 0.9144(0.9003) Steps 536(530.33) | Grad Norm 8.5696(10.2925) | Total Time 10.00(10.00)\n",
      "Iter 2200 | Time 16.2568(16.1864) | Bit/dim 1.1859(1.2080) | Xent 2.3026(2.3026) | Loss 1.1859(1.2080) | Error 0.8922(0.8999) Steps 524(530.53) | Grad Norm 1.2973(9.2997) | Total Time 10.00(10.00)\n",
      "Iter 2210 | Time 16.3676(16.1848) | Bit/dim 1.2122(1.2078) | Xent 2.3026(2.3026) | Loss 1.2122(1.2078) | Error 0.9122(0.9010) Steps 542(530.59) | Grad Norm 18.9073(10.5493) | Total Time 10.00(10.00)\n",
      "Iter 2220 | Time 16.2869(16.1959) | Bit/dim 1.2163(1.2062) | Xent 2.3026(2.3026) | Loss 1.2163(1.2062) | Error 0.9189(0.9004) Steps 536(531.36) | Grad Norm 18.0197(10.7382) | Total Time 10.00(10.00)\n",
      "Iter 2230 | Time 16.1344(16.2175) | Bit/dim 1.1979(1.2016) | Xent 2.3026(2.3026) | Loss 1.1979(1.2016) | Error 0.8978(0.9011) Steps 530(532.20) | Grad Norm 3.5161(9.6558) | Total Time 10.00(10.00)\n",
      "Iter 2240 | Time 16.4895(16.2360) | Bit/dim 1.1958(1.1976) | Xent 2.3026(2.3026) | Loss 1.1958(1.1976) | Error 0.9133(0.9016) Steps 542(532.89) | Grad Norm 6.3868(8.4769) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0034 | Time 64.7346, Epoch Time 1151.5034(1032.0275), Bit/dim 1.1754, Xent 2.3026, Loss 1.1754, Error 0.9031\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2250 | Time 16.4091(16.2256) | Bit/dim 1.1736(1.1943) | Xent 2.3026(2.3026) | Loss 1.1736(1.1943) | Error 0.9022(0.9015) Steps 536(532.49) | Grad Norm 5.0674(7.4081) | Total Time 10.00(10.00)\n",
      "Iter 2260 | Time 16.1450(16.2156) | Bit/dim 1.2068(1.1921) | Xent 2.3026(2.3026) | Loss 1.2068(1.1921) | Error 0.8800(0.9013) Steps 530(531.96) | Grad Norm 13.3535(7.6593) | Total Time 10.00(10.00)\n",
      "Iter 2270 | Time 16.7094(16.2064) | Bit/dim 1.2749(1.1959) | Xent 2.3026(2.3026) | Loss 1.2749(1.1959) | Error 0.9067(0.9026) Steps 572(532.84) | Grad Norm 32.2224(9.1457) | Total Time 10.00(10.00)\n",
      "Iter 2280 | Time 15.8260(16.2003) | Bit/dim 1.2462(1.2117) | Xent 2.3026(2.3026) | Loss 1.2462(1.2117) | Error 0.8967(0.9005) Steps 518(532.95) | Grad Norm 11.3329(10.5868) | Total Time 10.00(10.00)\n",
      "Iter 2290 | Time 16.2000(16.2607) | Bit/dim 1.2046(1.2106) | Xent 2.3026(2.3026) | Loss 1.2046(1.2106) | Error 0.9200(0.9017) Steps 530(534.54) | Grad Norm 11.5906(10.9769) | Total Time 10.00(10.00)\n",
      "Iter 2300 | Time 16.2371(16.2878) | Bit/dim 1.1799(1.2065) | Xent 2.3026(2.3026) | Loss 1.1799(1.2065) | Error 0.9167(0.9011) Steps 536(536.00) | Grad Norm 7.9454(10.1903) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0035 | Time 64.4130, Epoch Time 1152.0619(1035.6285), Bit/dim 1.1743, Xent 2.3026, Loss 1.1743, Error 0.9031\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2310 | Time 16.2777(16.3154) | Bit/dim 1.1850(1.2019) | Xent 2.3026(2.3026) | Loss 1.1850(1.2019) | Error 0.8900(0.9012) Steps 536(536.47) | Grad Norm 3.0937(9.6087) | Total Time 10.00(10.00)\n",
      "Iter 2320 | Time 16.6906(16.3665) | Bit/dim 1.2053(1.2013) | Xent 2.3026(2.3026) | Loss 1.2053(1.2013) | Error 0.9078(0.9004) Steps 542(537.60) | Grad Norm 13.2234(10.4158) | Total Time 10.00(10.00)\n",
      "Iter 2330 | Time 16.0699(16.3900) | Bit/dim 1.1751(1.1966) | Xent 2.3026(2.3026) | Loss 1.1751(1.1966) | Error 0.9000(0.9008) Steps 536(537.83) | Grad Norm 2.8156(9.0564) | Total Time 10.00(10.00)\n",
      "Iter 2340 | Time 16.4110(16.3712) | Bit/dim 1.1854(1.1928) | Xent 2.3026(2.3026) | Loss 1.1854(1.1928) | Error 0.8767(0.8993) Steps 530(537.14) | Grad Norm 1.5603(7.7107) | Total Time 10.00(10.00)\n",
      "Iter 2350 | Time 16.2075(16.3482) | Bit/dim 1.1713(1.1879) | Xent 2.3026(2.3026) | Loss 1.1713(1.1879) | Error 0.9222(0.9010) Steps 530(535.88) | Grad Norm 3.6396(6.7562) | Total Time 10.00(10.00)\n",
      "Iter 2360 | Time 16.1244(16.3093) | Bit/dim 1.1843(1.1853) | Xent 2.3026(2.3026) | Loss 1.1843(1.1853) | Error 0.9111(0.9022) Steps 536(535.12) | Grad Norm 12.3338(7.1550) | Total Time 10.00(10.00)\n",
      "Iter 2370 | Time 16.3258(16.2886) | Bit/dim 1.2053(1.1893) | Xent 2.3026(2.3026) | Loss 1.2053(1.1893) | Error 0.9000(0.9019) Steps 524(533.90) | Grad Norm 8.9820(8.4863) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0036 | Time 64.5763, Epoch Time 1158.0907(1039.3024), Bit/dim 1.1992, Xent 2.3026, Loss 1.1992, Error 0.9031\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2380 | Time 16.4784(16.3193) | Bit/dim 1.2084(1.2059) | Xent 2.3026(2.3026) | Loss 1.2084(1.2059) | Error 0.9178(0.9014) Steps 542(536.23) | Grad Norm 7.1286(10.2476) | Total Time 10.00(10.00)\n",
      "Iter 2390 | Time 16.5556(16.3477) | Bit/dim 1.2077(1.2063) | Xent 2.3026(2.3026) | Loss 1.2077(1.2063) | Error 0.8978(0.9007) Steps 542(536.99) | Grad Norm 15.2206(10.3407) | Total Time 10.00(10.00)\n",
      "Iter 2400 | Time 16.6031(16.3995) | Bit/dim 1.1828(1.2027) | Xent 2.3026(2.3026) | Loss 1.1828(1.2027) | Error 0.8989(0.9011) Steps 542(537.39) | Grad Norm 4.8958(9.8101) | Total Time 10.00(10.00)\n",
      "Iter 2410 | Time 16.4633(16.4488) | Bit/dim 1.1741(1.1964) | Xent 2.3026(2.3026) | Loss 1.1741(1.1964) | Error 0.9078(0.9023) Steps 536(538.32) | Grad Norm 7.8671(8.7153) | Total Time 10.00(10.00)\n",
      "Iter 2420 | Time 16.3823(16.4469) | Bit/dim 1.1742(1.1913) | Xent 2.3026(2.3026) | Loss 1.1742(1.1913) | Error 0.8967(0.9015) Steps 536(538.15) | Grad Norm 4.7637(7.4713) | Total Time 10.00(10.00)\n",
      "Iter 2430 | Time 16.7708(16.4613) | Bit/dim 1.1700(1.1866) | Xent 2.3026(2.3026) | Loss 1.1700(1.1866) | Error 0.9033(0.9026) Steps 542(538.23) | Grad Norm 6.3956(6.4777) | Total Time 10.00(10.00)\n",
      "Iter 2440 | Time 16.2825(16.4296) | Bit/dim 1.1838(1.1843) | Xent 2.3026(2.3026) | Loss 1.1838(1.1843) | Error 0.8844(0.9011) Steps 536(537.95) | Grad Norm 3.6306(6.5851) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0037 | Time 64.5665, Epoch Time 1163.9009(1043.0404), Bit/dim 1.1732, Xent 2.3026, Loss 1.1732, Error 0.9031\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2450 | Time 16.4543(16.3939) | Bit/dim 1.1824(1.1855) | Xent 2.3026(2.3026) | Loss 1.1824(1.1855) | Error 0.9078(0.8999) Steps 542(537.25) | Grad Norm 6.0370(7.6198) | Total Time 10.00(10.00)\n",
      "Iter 2460 | Time 16.1944(16.4017) | Bit/dim 1.2709(1.2035) | Xent 2.3026(2.3026) | Loss 1.2709(1.2035) | Error 0.9067(0.9009) Steps 524(538.04) | Grad Norm 17.5385(9.9226) | Total Time 10.00(10.00)\n",
      "Iter 2470 | Time 17.0244(16.4374) | Bit/dim 1.2380(1.2086) | Xent 2.3026(2.3026) | Loss 1.2380(1.2086) | Error 0.9133(0.9025) Steps 548(538.07) | Grad Norm 16.6534(10.3223) | Total Time 10.00(10.00)\n",
      "Iter 2480 | Time 16.5706(16.4615) | Bit/dim 1.1820(1.2036) | Xent 2.3026(2.3026) | Loss 1.1820(1.2036) | Error 0.8967(0.9020) Steps 542(538.96) | Grad Norm 6.5956(9.5457) | Total Time 10.00(10.00)\n",
      "Iter 2490 | Time 16.3272(16.4582) | Bit/dim 1.1793(1.1966) | Xent 2.3026(2.3026) | Loss 1.1793(1.1966) | Error 0.9089(0.9019) Steps 536(538.97) | Grad Norm 11.2693(8.7961) | Total Time 10.00(10.00)\n",
      "Iter 2500 | Time 16.5002(16.4551) | Bit/dim 1.1735(1.1906) | Xent 2.3026(2.3026) | Loss 1.1735(1.1906) | Error 0.8989(0.9018) Steps 542(539.25) | Grad Norm 4.0391(7.9264) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0038 | Time 64.8451, Epoch Time 1163.5680(1046.6562), Bit/dim 1.1684, Xent 2.3026, Loss 1.1684, Error 0.9031\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2510 | Time 16.4572(16.4501) | Bit/dim 1.1660(1.1862) | Xent 2.3026(2.3026) | Loss 1.1660(1.1862) | Error 0.8911(0.9003) Steps 530(539.17) | Grad Norm 8.6034(7.4094) | Total Time 10.00(10.00)\n",
      "Iter 2520 | Time 16.5878(16.4555) | Bit/dim 1.1922(1.1823) | Xent 2.3026(2.3026) | Loss 1.1922(1.1823) | Error 0.8844(0.9009) Steps 542(538.82) | Grad Norm 14.6236(6.8241) | Total Time 10.00(10.00)\n",
      "Iter 2530 | Time 16.1128(16.4454) | Bit/dim 1.2898(1.1916) | Xent 2.3026(2.3026) | Loss 1.2898(1.1916) | Error 0.8944(0.9003) Steps 518(538.46) | Grad Norm 15.6287(9.0903) | Total Time 10.00(10.00)\n",
      "Iter 2540 | Time 16.6532(16.4333) | Bit/dim 1.2277(1.2033) | Xent 2.3026(2.3026) | Loss 1.2277(1.2033) | Error 0.9067(0.8999) Steps 542(538.96) | Grad Norm 15.4633(10.3258) | Total Time 10.00(10.00)\n",
      "Iter 2550 | Time 16.5807(16.4345) | Bit/dim 1.1742(1.1986) | Xent 2.3026(2.3026) | Loss 1.1742(1.1986) | Error 0.9078(0.9004) Steps 542(538.62) | Grad Norm 8.6722(10.0414) | Total Time 10.00(10.00)\n",
      "Iter 2560 | Time 16.5996(16.4777) | Bit/dim 1.1677(1.1938) | Xent 2.3026(2.3026) | Loss 1.1677(1.1938) | Error 0.9200(0.9014) Steps 548(539.68) | Grad Norm 2.1348(8.4946) | Total Time 10.00(10.00)\n",
      "Iter 2570 | Time 16.7738(16.5062) | Bit/dim 1.1727(1.1876) | Xent 2.3026(2.3026) | Loss 1.1727(1.1876) | Error 0.8822(0.9011) Steps 548(539.83) | Grad Norm 4.3338(7.1284) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0039 | Time 65.3936, Epoch Time 1167.4495(1050.2800), Bit/dim 1.1604, Xent 2.3026, Loss 1.1604, Error 0.9031\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2580 | Time 16.3763(16.5263) | Bit/dim 1.1686(1.1838) | Xent 2.3026(2.3026) | Loss 1.1686(1.1838) | Error 0.8933(0.9009) Steps 536(539.30) | Grad Norm 8.8124(6.5557) | Total Time 10.00(10.00)\n",
      "Iter 2590 | Time 16.5838(16.5302) | Bit/dim 1.1736(1.1795) | Xent 2.3026(2.3026) | Loss 1.1736(1.1795) | Error 0.9033(0.9020) Steps 542(539.54) | Grad Norm 12.4625(6.6852) | Total Time 10.00(10.00)\n",
      "Iter 2600 | Time 16.1714(16.4973) | Bit/dim 1.1851(1.1773) | Xent 2.3026(2.3026) | Loss 1.1851(1.1773) | Error 0.9011(0.9032) Steps 536(539.71) | Grad Norm 15.5711(7.2958) | Total Time 10.00(10.00)\n",
      "Iter 2610 | Time 16.9235(16.4874) | Bit/dim 1.2086(1.1883) | Xent 2.3026(2.3026) | Loss 1.2086(1.1883) | Error 0.9056(0.9027) Steps 548(539.48) | Grad Norm 7.2909(9.3258) | Total Time 10.00(10.00)\n",
      "Iter 2620 | Time 16.7039(16.5177) | Bit/dim 1.1927(1.1977) | Xent 2.3026(2.3026) | Loss 1.1927(1.1977) | Error 0.8911(0.9017) Steps 548(540.65) | Grad Norm 5.8865(10.2990) | Total Time 10.00(10.00)\n",
      "Iter 2630 | Time 16.9001(16.5389) | Bit/dim 1.1855(1.1960) | Xent 2.3026(2.3026) | Loss 1.1855(1.1960) | Error 0.9122(0.9016) Steps 548(540.70) | Grad Norm 7.0228(10.3789) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0040 | Time 64.7223, Epoch Time 1169.6124(1053.8600), Bit/dim 1.1663, Xent 2.3026, Loss 1.1663, Error 0.9031\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2640 | Time 16.3199(16.5391) | Bit/dim 1.1920(1.1907) | Xent 2.3026(2.3026) | Loss 1.1920(1.1907) | Error 0.8978(0.9007) Steps 542(541.40) | Grad Norm 6.4772(9.2469) | Total Time 10.00(10.00)\n",
      "Iter 2650 | Time 16.5741(16.5819) | Bit/dim 1.1632(1.1851) | Xent 2.3026(2.3026) | Loss 1.1632(1.1851) | Error 0.9111(0.8999) Steps 536(541.82) | Grad Norm 1.5148(8.0484) | Total Time 10.00(10.00)\n",
      "Iter 2660 | Time 16.3673(16.5942) | Bit/dim 1.1607(1.1801) | Xent 2.3026(2.3026) | Loss 1.1607(1.1801) | Error 0.9133(0.9006) Steps 536(542.19) | Grad Norm 8.5153(7.2530) | Total Time 10.00(10.00)\n",
      "Iter 2670 | Time 16.4179(16.6098) | Bit/dim 1.1885(1.1763) | Xent 2.3026(2.3026) | Loss 1.1885(1.1763) | Error 0.9044(0.9002) Steps 542(542.44) | Grad Norm 12.2027(7.4365) | Total Time 10.00(10.00)\n",
      "Iter 2680 | Time 16.3672(16.6082) | Bit/dim 1.1619(1.1750) | Xent 2.3026(2.3026) | Loss 1.1619(1.1750) | Error 0.8900(0.9008) Steps 548(541.80) | Grad Norm 3.0787(7.7914) | Total Time 10.00(10.00)\n",
      "Iter 2690 | Time 15.9510(16.6143) | Bit/dim 1.2827(1.1822) | Xent 2.3026(2.3026) | Loss 1.2827(1.1822) | Error 0.9144(0.9026) Steps 524(541.52) | Grad Norm 20.6943(9.7046) | Total Time 10.00(10.00)\n",
      "Iter 2700 | Time 16.4542(16.6321) | Bit/dim 1.2041(1.1962) | Xent 2.3026(2.3026) | Loss 1.2041(1.1962) | Error 0.8978(0.9013) Steps 542(542.03) | Grad Norm 8.7668(10.4886) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0041 | Time 64.8068, Epoch Time 1177.1134(1057.5576), Bit/dim 1.1775, Xent 2.3026, Loss 1.1775, Error 0.9031\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2710 | Time 16.2321(16.6244) | Bit/dim 1.1687(1.1933) | Xent 2.3026(2.3026) | Loss 1.1687(1.1933) | Error 0.9100(0.9021) Steps 542(542.17) | Grad Norm 7.8950(10.3530) | Total Time 10.00(10.00)\n",
      "Iter 2720 | Time 16.9210(16.6417) | Bit/dim 1.1540(1.1878) | Xent 2.3026(2.3026) | Loss 1.1540(1.1878) | Error 0.9233(0.9013) Steps 548(542.30) | Grad Norm 2.1999(8.9342) | Total Time 10.00(10.00)\n",
      "Iter 2730 | Time 16.8564(16.6817) | Bit/dim 1.1503(1.1818) | Xent 2.3026(2.3026) | Loss 1.1503(1.1818) | Error 0.8922(0.9013) Steps 542(543.49) | Grad Norm 5.3247(7.7454) | Total Time 10.00(10.00)\n",
      "Iter 2740 | Time 16.9508(16.6736) | Bit/dim 1.1674(1.1761) | Xent 2.3026(2.3026) | Loss 1.1674(1.1761) | Error 0.9044(0.9021) Steps 554(544.03) | Grad Norm 9.9152(7.2497) | Total Time 10.00(10.00)\n",
      "Iter 2750 | Time 16.8562(16.6818) | Bit/dim 1.1739(1.1730) | Xent 2.3026(2.3026) | Loss 1.1739(1.1730) | Error 0.9033(0.9016) Steps 542(544.76) | Grad Norm 12.0566(7.8370) | Total Time 10.00(10.00)\n",
      "Iter 2760 | Time 16.6228(16.6828) | Bit/dim 1.1756(1.1713) | Xent 2.3026(2.3026) | Loss 1.1756(1.1713) | Error 0.9056(0.9025) Steps 548(545.08) | Grad Norm 17.8239(8.0044) | Total Time 10.00(10.00)\n",
      "Iter 2770 | Time 16.8745(16.7263) | Bit/dim 1.1993(1.1819) | Xent 2.3026(2.3026) | Loss 1.1993(1.1819) | Error 0.9011(0.9010) Steps 554(545.45) | Grad Norm 16.9552(9.7361) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0042 | Time 65.5575, Epoch Time 1181.8966(1061.2877), Bit/dim 1.1890, Xent 2.3026, Loss 1.1890, Error 0.9031\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2780 | Time 16.8489(16.7258) | Bit/dim 1.1733(1.1835) | Xent 2.3026(2.3026) | Loss 1.1733(1.1835) | Error 0.9022(0.9005) Steps 554(545.74) | Grad Norm 4.5360(9.5044) | Total Time 10.00(10.00)\n",
      "Iter 2790 | Time 17.3845(16.7647) | Bit/dim 1.1543(1.1791) | Xent 2.3026(2.3026) | Loss 1.1543(1.1791) | Error 0.8933(0.9013) Steps 548(546.24) | Grad Norm 3.5862(8.3451) | Total Time 10.00(10.00)\n",
      "Iter 2800 | Time 16.6138(16.7436) | Bit/dim 1.1419(1.1737) | Xent 2.3026(2.3026) | Loss 1.1419(1.1737) | Error 0.9078(0.9021) Steps 536(546.50) | Grad Norm 1.3149(6.6684) | Total Time 10.00(10.00)\n",
      "Iter 2810 | Time 16.7209(16.6903) | Bit/dim 1.1708(1.1727) | Xent 2.3026(2.3026) | Loss 1.1708(1.1727) | Error 0.9122(0.9009) Steps 542(545.25) | Grad Norm 9.8607(7.1939) | Total Time 10.00(10.00)\n",
      "Iter 2820 | Time 17.2572(16.7093) | Bit/dim 1.2450(1.1968) | Xent 2.3026(2.3026) | Loss 1.2450(1.1968) | Error 0.9144(0.9015) Steps 566(546.04) | Grad Norm 17.6225(9.6433) | Total Time 10.00(10.00)\n",
      "Iter 2830 | Time 16.3110(16.6769) | Bit/dim 1.2133(1.2004) | Xent 2.3026(2.3026) | Loss 1.2133(1.2004) | Error 0.8867(0.9010) Steps 536(545.85) | Grad Norm 16.4356(10.1147) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0043 | Time 64.8882, Epoch Time 1180.1174(1064.8526), Bit/dim 1.1642, Xent 2.3026, Loss 1.1642, Error 0.9031\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2840 | Time 16.4108(16.6420) | Bit/dim 1.1754(1.1967) | Xent 2.3026(2.3026) | Loss 1.1754(1.1967) | Error 0.8967(0.9014) Steps 542(545.17) | Grad Norm 6.9029(9.7962) | Total Time 10.00(10.00)\n",
      "Iter 2850 | Time 16.7508(16.7095) | Bit/dim 1.1817(1.1907) | Xent 2.3026(2.3026) | Loss 1.1817(1.1907) | Error 0.9033(0.9011) Steps 548(546.89) | Grad Norm 7.8970(8.8390) | Total Time 10.00(10.00)\n",
      "Iter 2860 | Time 16.5716(16.7172) | Bit/dim 1.1573(1.1832) | Xent 2.3026(2.3026) | Loss 1.1573(1.1832) | Error 0.9144(0.9017) Steps 542(547.71) | Grad Norm 1.2352(7.5252) | Total Time 10.00(10.00)\n",
      "Iter 2870 | Time 16.5247(16.6922) | Bit/dim 1.1719(1.1778) | Xent 2.3026(2.3026) | Loss 1.1719(1.1778) | Error 0.8933(0.9005) Steps 542(547.93) | Grad Norm 12.4775(7.3841) | Total Time 10.00(10.00)\n",
      "Iter 2880 | Time 17.1470(16.7139) | Bit/dim 1.2011(1.1815) | Xent 2.3026(2.3026) | Loss 1.2011(1.1815) | Error 0.8933(0.9005) Steps 560(548.18) | Grad Norm 13.3877(8.8676) | Total Time 10.00(10.00)\n",
      "Iter 2890 | Time 16.5724(16.7010) | Bit/dim 1.1883(1.1908) | Xent 2.3026(2.3026) | Loss 1.1883(1.1908) | Error 0.9111(0.9012) Steps 542(547.64) | Grad Norm 12.6945(10.6332) | Total Time 10.00(10.00)\n",
      "Iter 2900 | Time 16.9167(16.6865) | Bit/dim 1.1971(1.1902) | Xent 2.3026(2.3026) | Loss 1.1971(1.1902) | Error 0.8978(0.9012) Steps 548(546.46) | Grad Norm 15.4627(10.8711) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0044 | Time 65.2883, Epoch Time 1181.2286(1068.3439), Bit/dim 1.1556, Xent 2.3026, Loss 1.1556, Error 0.9031\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2910 | Time 17.4061(16.7235) | Bit/dim 1.1620(1.1843) | Xent 2.3026(2.3026) | Loss 1.1620(1.1843) | Error 0.8878(0.9008) Steps 554(547.54) | Grad Norm 6.8065(9.7007) | Total Time 10.00(10.00)\n",
      "Iter 2920 | Time 16.1404(16.7562) | Bit/dim 1.1879(1.1796) | Xent 2.3026(2.3026) | Loss 1.1879(1.1796) | Error 0.9067(0.9002) Steps 530(548.09) | Grad Norm 13.1026(9.2954) | Total Time 10.00(10.00)\n",
      "Iter 2930 | Time 16.4790(16.7646) | Bit/dim 1.1771(1.1801) | Xent 2.3026(2.3026) | Loss 1.1771(1.1801) | Error 0.9056(0.9006) Steps 524(547.86) | Grad Norm 13.2847(10.2835) | Total Time 10.00(10.00)\n",
      "Iter 2940 | Time 16.6056(16.7268) | Bit/dim 1.1652(1.1773) | Xent 2.3026(2.3026) | Loss 1.1652(1.1773) | Error 0.9089(0.9019) Steps 548(547.92) | Grad Norm 7.9472(10.2139) | Total Time 10.00(10.00)\n",
      "Iter 2950 | Time 17.0274(16.7438) | Bit/dim 1.1606(1.1731) | Xent 2.3026(2.3026) | Loss 1.1606(1.1731) | Error 0.8911(0.9017) Steps 560(549.06) | Grad Norm 4.8513(9.1405) | Total Time 10.00(10.00)\n",
      "Iter 2960 | Time 16.9224(16.7905) | Bit/dim 1.1653(1.1687) | Xent 2.3026(2.3026) | Loss 1.1653(1.1687) | Error 0.9122(0.9015) Steps 554(550.39) | Grad Norm 4.1229(8.0105) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0045 | Time 64.1543, Epoch Time 1189.0583(1071.9653), Bit/dim 1.1626, Xent 2.3026, Loss 1.1626, Error 0.9031\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2970 | Time 16.7097(16.8393) | Bit/dim 1.1615(1.1656) | Xent 2.3026(2.3026) | Loss 1.1615(1.1656) | Error 0.8878(0.9011) Steps 542(551.26) | Grad Norm 12.2512(7.9642) | Total Time 10.00(10.00)\n",
      "Iter 2980 | Time 17.0860(16.9100) | Bit/dim 1.1510(1.1625) | Xent 2.3026(2.3026) | Loss 1.1510(1.1625) | Error 0.8967(0.9014) Steps 554(552.76) | Grad Norm 7.9375(7.8117) | Total Time 10.00(10.00)\n",
      "Iter 2990 | Time 17.1250(16.8830) | Bit/dim 1.1722(1.1700) | Xent 2.3026(2.3026) | Loss 1.1722(1.1700) | Error 0.8922(0.9022) Steps 560(552.79) | Grad Norm 8.5049(9.5554) | Total Time 10.00(10.00)\n",
      "Iter 3000 | Time 17.2246(16.8800) | Bit/dim 1.1682(1.1758) | Xent 2.3026(2.3026) | Loss 1.1682(1.1758) | Error 0.9011(0.9032) Steps 560(551.83) | Grad Norm 9.0336(10.5957) | Total Time 10.00(10.00)\n",
      "Iter 3010 | Time 16.2827(16.8567) | Bit/dim 1.1661(1.1802) | Xent 2.3026(2.3026) | Loss 1.1661(1.1802) | Error 0.9011(0.9013) Steps 536(551.41) | Grad Norm 12.0116(10.9720) | Total Time 10.00(10.00)\n",
      "Iter 3020 | Time 16.5677(16.8688) | Bit/dim 1.1670(1.1778) | Xent 2.3026(2.3026) | Loss 1.1670(1.1778) | Error 0.8944(0.9003) Steps 542(551.24) | Grad Norm 6.0490(10.3872) | Total Time 10.00(10.00)\n",
      "Iter 3030 | Time 17.2404(16.8972) | Bit/dim 1.1650(1.1737) | Xent 2.3026(2.3026) | Loss 1.1650(1.1737) | Error 0.9033(0.9011) Steps 560(551.99) | Grad Norm 17.3177(10.3433) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0046 | Time 65.6147, Epoch Time 1195.2227(1075.6630), Bit/dim 1.1883, Xent 2.3026, Loss 1.1883, Error 0.9031\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3040 | Time 16.9842(16.9022) | Bit/dim 1.1496(1.1723) | Xent 2.3026(2.3026) | Loss 1.1496(1.1723) | Error 0.9089(0.9016) Steps 548(551.31) | Grad Norm 0.8968(10.6689) | Total Time 10.00(10.00)\n",
      "Iter 3050 | Time 16.6779(16.9549) | Bit/dim 1.1582(1.1683) | Xent 2.3026(2.3026) | Loss 1.1582(1.1683) | Error 0.8956(0.9017) Steps 554(552.33) | Grad Norm 6.8915(9.4062) | Total Time 10.00(10.00)\n",
      "Iter 3060 | Time 16.7942(16.9476) | Bit/dim 1.1361(1.1641) | Xent 2.3026(2.3026) | Loss 1.1361(1.1641) | Error 0.9144(0.9027) Steps 554(553.38) | Grad Norm 4.0290(8.2231) | Total Time 10.00(10.00)\n",
      "Iter 3070 | Time 16.8752(16.9475) | Bit/dim 1.1369(1.1600) | Xent 2.3026(2.3026) | Loss 1.1369(1.1600) | Error 0.9056(0.9018) Steps 548(554.00) | Grad Norm 6.9931(7.6984) | Total Time 10.00(10.00)\n",
      "Iter 3080 | Time 16.6623(16.9445) | Bit/dim 1.1516(1.1576) | Xent 2.3026(2.3026) | Loss 1.1516(1.1576) | Error 0.9033(0.9020) Steps 554(554.15) | Grad Norm 3.7822(7.5653) | Total Time 10.00(10.00)\n",
      "Iter 3090 | Time 17.9639(16.9451) | Bit/dim 1.3392(1.1734) | Xent 2.3026(2.3026) | Loss 1.3392(1.1734) | Error 0.9111(0.9012) Steps 596(554.52) | Grad Norm 33.9602(9.8197) | Total Time 10.00(10.00)\n",
      "Iter 3100 | Time 16.9536(16.9054) | Bit/dim 1.1732(1.1819) | Xent 2.3026(2.3026) | Loss 1.1732(1.1819) | Error 0.8978(0.9000) Steps 548(552.65) | Grad Norm 7.1448(10.1662) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0047 | Time 65.6577, Epoch Time 1197.0760(1079.3054), Bit/dim 1.1692, Xent 2.3026, Loss 1.1692, Error 0.9031\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3110 | Time 16.7659(16.8201) | Bit/dim 1.1623(1.1794) | Xent 2.3026(2.3026) | Loss 1.1623(1.1794) | Error 0.8967(0.9011) Steps 554(552.07) | Grad Norm 7.6536(9.9734) | Total Time 10.00(10.00)\n",
      "Iter 3120 | Time 16.6387(16.7767) | Bit/dim 1.1470(1.1745) | Xent 2.3026(2.3026) | Loss 1.1470(1.1745) | Error 0.9200(0.9026) Steps 554(551.85) | Grad Norm 5.0376(9.2788) | Total Time 10.00(10.00)\n",
      "Iter 3130 | Time 16.9068(16.8162) | Bit/dim 1.1432(1.1688) | Xent 2.3026(2.3026) | Loss 1.1432(1.1688) | Error 0.9011(0.9033) Steps 554(553.36) | Grad Norm 1.2076(8.4497) | Total Time 10.00(10.00)\n",
      "Iter 3140 | Time 16.8684(16.8657) | Bit/dim 1.1492(1.1651) | Xent 2.3026(2.3026) | Loss 1.1492(1.1651) | Error 0.9011(0.9000) Steps 554(554.47) | Grad Norm 3.5580(7.9582) | Total Time 10.00(10.00)\n",
      "Iter 3150 | Time 17.1071(16.9203) | Bit/dim 1.1402(1.1607) | Xent 2.3026(2.3026) | Loss 1.1402(1.1607) | Error 0.8967(0.8994) Steps 566(555.17) | Grad Norm 1.8423(7.3006) | Total Time 10.00(10.00)\n",
      "Iter 3160 | Time 17.1427(16.9545) | Bit/dim 1.1497(1.1605) | Xent 2.3026(2.3026) | Loss 1.1497(1.1605) | Error 0.9133(0.9012) Steps 554(554.54) | Grad Norm 8.4303(8.3045) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0048 | Time 62.8382, Epoch Time 1192.2182(1082.6928), Bit/dim 1.2002, Xent 2.3026, Loss 1.2002, Error 0.9031\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3170 | Time 17.2801(16.9659) | Bit/dim 1.1857(1.1773) | Xent 2.3026(2.3026) | Loss 1.1857(1.1773) | Error 0.8967(0.9009) Steps 560(555.10) | Grad Norm 6.6495(10.1859) | Total Time 10.00(10.00)\n",
      "Iter 3180 | Time 16.8399(16.9240) | Bit/dim 1.1726(1.1752) | Xent 2.3026(2.3026) | Loss 1.1726(1.1752) | Error 0.8989(0.9007) Steps 560(554.39) | Grad Norm 15.0473(10.0086) | Total Time 10.00(10.00)\n",
      "Iter 3190 | Time 16.9535(16.8607) | Bit/dim 1.1704(1.1751) | Xent 2.3026(2.3026) | Loss 1.1704(1.1751) | Error 0.8911(0.9007) Steps 554(553.42) | Grad Norm 4.1706(10.1984) | Total Time 10.00(10.00)\n",
      "Iter 3200 | Time 16.5708(16.8631) | Bit/dim 1.1590(1.1701) | Xent 2.3026(2.3026) | Loss 1.1590(1.1701) | Error 0.8811(0.9008) Steps 536(553.22) | Grad Norm 12.4731(9.8693) | Total Time 10.00(10.00)\n",
      "Iter 3210 | Time 16.5580(16.8465) | Bit/dim 1.1735(1.1710) | Xent 2.3026(2.3026) | Loss 1.1735(1.1710) | Error 0.8911(0.9007) Steps 536(552.76) | Grad Norm 13.8220(10.7035) | Total Time 10.00(10.00)\n",
      "Iter 3220 | Time 16.7342(16.8516) | Bit/dim 1.1531(1.1680) | Xent 2.3026(2.3026) | Loss 1.1531(1.1680) | Error 0.9078(0.9011) Steps 548(552.31) | Grad Norm 8.3710(10.3818) | Total Time 10.00(10.00)\n",
      "Iter 3230 | Time 16.8327(16.8937) | Bit/dim 1.1492(1.1625) | Xent 2.3026(2.3026) | Loss 1.1492(1.1625) | Error 0.9156(0.9013) Steps 554(553.20) | Grad Norm 4.8782(8.9367) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0049 | Time 65.8106, Epoch Time 1191.2710(1085.9502), Bit/dim 1.1369, Xent 2.3026, Loss 1.1369, Error 0.9031\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3240 | Time 16.9822(16.9204) | Bit/dim 1.1369(1.1585) | Xent 2.3026(2.3026) | Loss 1.1369(1.1585) | Error 0.8822(0.8994) Steps 554(553.28) | Grad Norm 4.1122(7.8498) | Total Time 10.00(10.00)\n",
      "Iter 3250 | Time 17.0650(16.9532) | Bit/dim 1.1377(1.1542) | Xent 2.3026(2.3026) | Loss 1.1377(1.1542) | Error 0.9211(0.9012) Steps 554(553.59) | Grad Norm 6.5948(7.5817) | Total Time 10.00(10.00)\n",
      "Iter 3260 | Time 17.0712(16.9497) | Bit/dim 1.1592(1.1536) | Xent 2.3026(2.3026) | Loss 1.1592(1.1536) | Error 0.8911(0.9010) Steps 560(554.20) | Grad Norm 12.8235(8.5441) | Total Time 10.00(10.00)\n",
      "Iter 3270 | Time 16.8355(16.9225) | Bit/dim 1.1645(1.1608) | Xent 2.3026(2.3026) | Loss 1.1645(1.1608) | Error 0.8922(0.9023) Steps 554(553.19) | Grad Norm 3.6508(9.6067) | Total Time 10.00(10.00)\n",
      "Iter 3280 | Time 17.5826(16.9488) | Bit/dim 1.1685(1.1589) | Xent 2.3026(2.3026) | Loss 1.1685(1.1589) | Error 0.8878(0.9013) Steps 566(554.24) | Grad Norm 7.8182(8.5623) | Total Time 10.00(10.00)\n",
      "Iter 3290 | Time 17.0000(16.9529) | Bit/dim 1.1544(1.1560) | Xent 2.3026(2.3026) | Loss 1.1544(1.1560) | Error 0.8856(0.9005) Steps 554(554.82) | Grad Norm 1.7674(8.0889) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0050 | Time 63.6949, Epoch Time 1197.9440(1089.3100), Bit/dim 1.2298, Xent 2.3026, Loss 1.2298, Error 0.9031\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3300 | Time 16.7606(16.9682) | Bit/dim 1.2359(1.1658) | Xent 2.3026(2.3026) | Loss 1.2359(1.1658) | Error 0.8933(0.9011) Steps 536(553.81) | Grad Norm 15.1883(10.2219) | Total Time 10.00(10.00)\n",
      "Iter 3310 | Time 17.5317(16.9648) | Bit/dim 1.1884(1.1711) | Xent 2.3026(2.3026) | Loss 1.1884(1.1711) | Error 0.8933(0.9010) Steps 572(553.88) | Grad Norm 17.5978(10.6274) | Total Time 10.00(10.00)\n",
      "Iter 3320 | Time 17.1869(16.9301) | Bit/dim 1.1749(1.1674) | Xent 2.3026(2.3026) | Loss 1.1749(1.1674) | Error 0.9111(0.9013) Steps 560(553.76) | Grad Norm 14.1327(10.0276) | Total Time 10.00(10.00)\n",
      "Iter 3330 | Time 17.0494(16.9390) | Bit/dim 1.1458(1.1645) | Xent 2.3026(2.3026) | Loss 1.1458(1.1645) | Error 0.8933(0.9022) Steps 548(552.89) | Grad Norm 4.2074(9.9243) | Total Time 10.00(10.00)\n",
      "Iter 3340 | Time 17.0969(16.9370) | Bit/dim 1.1319(1.1593) | Xent 2.3026(2.3026) | Loss 1.1319(1.1593) | Error 0.8900(0.9013) Steps 554(553.47) | Grad Norm 5.4805(9.1932) | Total Time 10.00(10.00)\n",
      "Iter 3350 | Time 16.6774(16.9254) | Bit/dim 1.1360(1.1548) | Xent 2.3026(2.3026) | Loss 1.1360(1.1548) | Error 0.9111(0.9016) Steps 560(554.57) | Grad Norm 2.6010(8.3519) | Total Time 10.00(10.00)\n",
      "Iter 3360 | Time 17.1961(16.9325) | Bit/dim 1.1549(1.1532) | Xent 2.3026(2.3026) | Loss 1.1549(1.1532) | Error 0.8922(0.9007) Steps 566(555.07) | Grad Norm 11.2720(8.4158) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0051 | Time 64.6910, Epoch Time 1195.8263(1092.5055), Bit/dim 1.1601, Xent 2.3026, Loss 1.1601, Error 0.9031\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3370 | Time 17.2206(16.9531) | Bit/dim 1.1397(1.1515) | Xent 2.3026(2.3026) | Loss 1.1397(1.1515) | Error 0.9200(0.9020) Steps 560(555.84) | Grad Norm 17.7039(8.7492) | Total Time 10.00(10.00)\n",
      "Iter 3380 | Time 17.9210(16.9515) | Bit/dim 1.2632(1.1636) | Xent 2.3026(2.3026) | Loss 1.2632(1.1636) | Error 0.8867(0.9009) Steps 584(556.11) | Grad Norm 28.2966(10.9742) | Total Time 10.00(10.00)\n",
      "Iter 3390 | Time 16.7274(16.9311) | Bit/dim 1.1740(1.1668) | Xent 2.3026(2.3026) | Loss 1.1740(1.1668) | Error 0.9078(0.8999) Steps 548(554.95) | Grad Norm 9.6307(10.6953) | Total Time 10.00(10.00)\n",
      "Iter 3400 | Time 16.5619(16.8953) | Bit/dim 1.1487(1.1637) | Xent 2.3026(2.3026) | Loss 1.1487(1.1637) | Error 0.9111(0.9003) Steps 542(553.97) | Grad Norm 6.9963(10.3333) | Total Time 10.00(10.00)\n",
      "Iter 3410 | Time 17.2893(16.8803) | Bit/dim 1.1428(1.1606) | Xent 2.3026(2.3026) | Loss 1.1428(1.1606) | Error 0.8944(0.9005) Steps 560(553.68) | Grad Norm 7.6517(9.5246) | Total Time 10.00(10.00)\n",
      "Iter 3420 | Time 17.0446(16.8834) | Bit/dim 1.1387(1.1544) | Xent 2.3026(2.3026) | Loss 1.1387(1.1544) | Error 0.9033(0.9020) Steps 560(553.93) | Grad Norm 3.7235(8.3424) | Total Time 10.00(10.00)\n",
      "Iter 3430 | Time 16.7473(16.9003) | Bit/dim 1.1408(1.1503) | Xent 2.3026(2.3026) | Loss 1.1408(1.1503) | Error 0.9089(0.9015) Steps 554(555.18) | Grad Norm 9.8642(7.5682) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0052 | Time 64.8772, Epoch Time 1193.3757(1095.5316), Bit/dim 1.1403, Xent 2.3026, Loss 1.1403, Error 0.9031\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3440 | Time 16.7413(16.9338) | Bit/dim 1.1474(1.1480) | Xent 2.3026(2.3026) | Loss 1.1474(1.1480) | Error 0.8944(0.9006) Steps 542(554.76) | Grad Norm 12.0308(8.2698) | Total Time 10.00(10.00)\n",
      "Iter 3450 | Time 16.9030(16.9731) | Bit/dim 1.1535(1.1472) | Xent 2.3026(2.3026) | Loss 1.1535(1.1472) | Error 0.9122(0.9002) Steps 542(556.36) | Grad Norm 17.4478(7.9807) | Total Time 10.00(10.00)\n",
      "Iter 3460 | Time 17.0872(17.0220) | Bit/dim 1.1290(1.1481) | Xent 2.3026(2.3026) | Loss 1.1290(1.1481) | Error 0.9078(0.9023) Steps 554(556.95) | Grad Norm 2.2060(8.4278) | Total Time 10.00(10.00)\n",
      "Iter 3470 | Time 17.2553(17.0322) | Bit/dim 1.1353(1.1485) | Xent 2.3026(2.3026) | Loss 1.1353(1.1485) | Error 0.8867(0.9018) Steps 572(558.40) | Grad Norm 9.5762(8.9970) | Total Time 10.00(10.00)\n",
      "Iter 3480 | Time 17.0994(17.0372) | Bit/dim 1.1409(1.1466) | Xent 2.3026(2.3026) | Loss 1.1409(1.1466) | Error 0.8956(0.9028) Steps 560(558.39) | Grad Norm 2.1761(8.7530) | Total Time 10.00(10.00)\n",
      "Iter 3490 | Time 16.6418(17.0392) | Bit/dim 1.2089(1.1484) | Xent 2.3026(2.3026) | Loss 1.2089(1.1484) | Error 0.8900(0.9022) Steps 548(558.00) | Grad Norm 19.7867(9.3494) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0053 | Time 64.4492, Epoch Time 1204.4152(1098.7981), Bit/dim 1.1752, Xent 2.3026, Loss 1.1752, Error 0.9031\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3500 | Time 16.4381(16.9901) | Bit/dim 1.1773(1.1673) | Xent 2.3026(2.3026) | Loss 1.1773(1.1673) | Error 0.8911(0.9013) Steps 536(556.22) | Grad Norm 5.9184(10.5166) | Total Time 10.00(10.00)\n",
      "Iter 3510 | Time 16.2553(16.9745) | Bit/dim 1.1689(1.1682) | Xent 2.3026(2.3026) | Loss 1.1689(1.1682) | Error 0.8911(0.9010) Steps 554(556.13) | Grad Norm 12.5211(10.1438) | Total Time 10.00(10.00)\n",
      "Iter 3520 | Time 17.1156(16.9436) | Bit/dim 1.1555(1.1636) | Xent 2.3026(2.3026) | Loss 1.1555(1.1636) | Error 0.8889(0.9004) Steps 548(554.25) | Grad Norm 2.8134(8.8974) | Total Time 10.00(10.00)\n",
      "Iter 3530 | Time 17.2836(16.9693) | Bit/dim 1.1446(1.1564) | Xent 2.3026(2.3026) | Loss 1.1446(1.1564) | Error 0.9067(0.9014) Steps 560(554.89) | Grad Norm 2.3925(7.4520) | Total Time 10.00(10.00)\n",
      "Iter 3540 | Time 16.4269(16.9379) | Bit/dim 1.1340(1.1520) | Xent 2.3026(2.3026) | Loss 1.1340(1.1520) | Error 0.9000(0.9029) Steps 548(554.96) | Grad Norm 11.3463(7.4550) | Total Time 10.00(10.00)\n",
      "Iter 3550 | Time 16.9348(16.9258) | Bit/dim 1.1254(1.1474) | Xent 2.3026(2.3026) | Loss 1.1254(1.1474) | Error 0.9044(0.9020) Steps 560(554.75) | Grad Norm 3.1365(7.4439) | Total Time 10.00(10.00)\n",
      "Iter 3560 | Time 16.5651(16.9247) | Bit/dim 1.1360(1.1455) | Xent 2.3026(2.3026) | Loss 1.1360(1.1455) | Error 0.8844(0.9014) Steps 554(555.48) | Grad Norm 11.4330(7.7140) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0054 | Time 66.6848, Epoch Time 1196.1528(1101.7187), Bit/dim 1.1320, Xent 2.3026, Loss 1.1320, Error 0.9031\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3570 | Time 17.2901(16.9643) | Bit/dim 1.2077(1.1520) | Xent 2.3026(2.3026) | Loss 1.2077(1.1520) | Error 0.8878(0.9005) Steps 560(555.66) | Grad Norm 25.0172(9.7072) | Total Time 10.00(10.00)\n",
      "Iter 3580 | Time 16.5814(16.9525) | Bit/dim 1.1723(1.1533) | Xent 2.3026(2.3026) | Loss 1.1723(1.1533) | Error 0.9222(0.9014) Steps 542(555.20) | Grad Norm 15.7734(10.3011) | Total Time 10.00(10.00)\n",
      "Iter 3590 | Time 16.6971(16.9291) | Bit/dim 1.1712(1.1552) | Xent 2.3026(2.3026) | Loss 1.1712(1.1552) | Error 0.9078(0.9019) Steps 542(553.85) | Grad Norm 10.7410(10.8541) | Total Time 10.00(10.00)\n",
      "Iter 3600 | Time 16.6210(16.8959) | Bit/dim 1.1270(1.1512) | Xent 2.3026(2.3026) | Loss 1.1270(1.1512) | Error 0.8978(0.9004) Steps 548(554.00) | Grad Norm 6.6086(9.3513) | Total Time 10.00(10.00)\n",
      "Iter 3610 | Time 16.8300(16.8762) | Bit/dim 1.1265(1.1473) | Xent 2.3026(2.3026) | Loss 1.1265(1.1473) | Error 0.9022(0.9004) Steps 560(554.62) | Grad Norm 1.9558(8.5138) | Total Time 10.00(10.00)\n",
      "Iter 3620 | Time 16.5643(16.8582) | Bit/dim 1.1445(1.1443) | Xent 2.3026(2.3026) | Loss 1.1445(1.1443) | Error 0.9100(0.9021) Steps 536(554.76) | Grad Norm 12.9901(8.2699) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0055 | Time 64.4425, Epoch Time 1192.7763(1104.4505), Bit/dim 1.1467, Xent 2.3026, Loss 1.1467, Error 0.9031\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3630 | Time 16.3872(16.8790) | Bit/dim 1.1489(1.1434) | Xent 2.3026(2.3026) | Loss 1.1489(1.1434) | Error 0.8889(0.9011) Steps 542(555.76) | Grad Norm 14.0984(8.8552) | Total Time 10.00(10.00)\n",
      "Iter 3640 | Time 17.0604(16.9169) | Bit/dim 1.2108(1.1456) | Xent 2.3026(2.3026) | Loss 1.2108(1.1456) | Error 0.8989(0.9022) Steps 560(555.73) | Grad Norm 32.5142(9.8258) | Total Time 10.00(10.00)\n",
      "Iter 3650 | Time 17.3902(16.8951) | Bit/dim 1.2631(1.1717) | Xent 2.3026(2.3026) | Loss 1.2631(1.1717) | Error 0.9078(0.9020) Steps 584(554.48) | Grad Norm 23.0106(11.0844) | Total Time 10.00(10.00)\n",
      "Iter 3660 | Time 16.5679(16.8469) | Bit/dim 1.1895(1.1759) | Xent 2.3026(2.3026) | Loss 1.1895(1.1759) | Error 0.8800(0.9020) Steps 536(551.91) | Grad Norm 14.7877(11.2575) | Total Time 10.00(10.00)\n",
      "Iter 3670 | Time 17.1479(16.8342) | Bit/dim 1.1470(1.1722) | Xent 2.3026(2.3026) | Loss 1.1470(1.1722) | Error 0.8989(0.9006) Steps 554(550.71) | Grad Norm 8.7780(10.7232) | Total Time 10.00(10.00)\n",
      "Iter 3680 | Time 16.4066(16.7361) | Bit/dim 1.1444(1.1648) | Xent 2.3026(2.3026) | Loss 1.1444(1.1648) | Error 0.8989(0.9005) Steps 548(548.96) | Grad Norm 7.3670(9.3045) | Total Time 10.00(10.00)\n",
      "Iter 3690 | Time 16.5134(16.6794) | Bit/dim 1.1494(1.1593) | Xent 2.3026(2.3026) | Loss 1.1494(1.1593) | Error 0.8922(0.9009) Steps 542(548.28) | Grad Norm 10.6548(8.8582) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0056 | Time 64.3871, Epoch Time 1181.9831(1106.7764), Bit/dim 1.1379, Xent 2.3026, Loss 1.1379, Error 0.9031\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3700 | Time 17.1668(16.7142) | Bit/dim 1.1502(1.1532) | Xent 2.3026(2.3026) | Loss 1.1502(1.1532) | Error 0.8833(0.9011) Steps 566(549.71) | Grad Norm 13.3277(8.5778) | Total Time 10.00(10.00)\n",
      "Iter 3710 | Time 16.7377(16.7400) | Bit/dim 1.1309(1.1497) | Xent 2.3026(2.3026) | Loss 1.1309(1.1497) | Error 0.8767(0.8995) Steps 554(550.98) | Grad Norm 1.3725(8.4800) | Total Time 10.00(10.00)\n",
      "Iter 3720 | Time 16.8249(16.7836) | Bit/dim 1.1364(1.1444) | Xent 2.3026(2.3026) | Loss 1.1364(1.1444) | Error 0.8944(0.9010) Steps 554(552.12) | Grad Norm 1.6561(7.6650) | Total Time 10.00(10.00)\n",
      "Iter 3730 | Time 17.0896(16.8240) | Bit/dim 1.1277(1.1411) | Xent 2.3026(2.3026) | Loss 1.1277(1.1411) | Error 0.9078(0.9017) Steps 560(553.60) | Grad Norm 14.3282(7.7208) | Total Time 10.00(10.00)\n",
      "Iter 3740 | Time 16.4867(16.8281) | Bit/dim 1.2736(1.1618) | Xent 2.3026(2.3026) | Loss 1.2736(1.1618) | Error 0.8889(0.9009) Steps 524(552.28) | Grad Norm 10.1307(9.9485) | Total Time 10.00(10.00)\n",
      "Iter 3750 | Time 16.2426(16.7852) | Bit/dim 1.1693(1.1709) | Xent 2.3026(2.3026) | Loss 1.1693(1.1709) | Error 0.9089(0.9012) Steps 536(549.70) | Grad Norm 10.1748(10.1477) | Total Time 10.00(10.00)\n",
      "Iter 3760 | Time 16.6620(16.7811) | Bit/dim 1.1466(1.1678) | Xent 2.3026(2.3026) | Loss 1.1466(1.1678) | Error 0.8967(0.9017) Steps 542(549.22) | Grad Norm 8.4178(10.2638) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0057 | Time 65.7465, Epoch Time 1189.6584(1109.2629), Bit/dim 1.1330, Xent 2.3026, Loss 1.1330, Error 0.9031\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3770 | Time 16.5598(16.7226) | Bit/dim 1.1226(1.1610) | Xent 2.3026(2.3026) | Loss 1.1226(1.1610) | Error 0.9011(0.9007) Steps 548(548.45) | Grad Norm 1.8961(8.9627) | Total Time 10.00(10.00)\n",
      "Iter 3780 | Time 16.6301(16.6942) | Bit/dim 1.1261(1.1535) | Xent 2.3026(2.3026) | Loss 1.1261(1.1535) | Error 0.9056(0.9006) Steps 548(547.85) | Grad Norm 3.3963(8.1266) | Total Time 10.00(10.00)\n",
      "Iter 3790 | Time 16.7294(16.6931) | Bit/dim 1.1397(1.1468) | Xent 2.3026(2.3026) | Loss 1.1397(1.1468) | Error 0.9122(0.9016) Steps 548(548.46) | Grad Norm 9.9907(7.3261) | Total Time 10.00(10.00)\n",
      "Iter 3800 | Time 16.6336(16.7339) | Bit/dim 1.1477(1.1439) | Xent 2.3026(2.3026) | Loss 1.1477(1.1439) | Error 0.8889(0.9017) Steps 542(550.80) | Grad Norm 15.7116(7.9760) | Total Time 10.00(10.00)\n",
      "Iter 3810 | Time 16.7959(16.7664) | Bit/dim 1.1265(1.1414) | Xent 2.3026(2.3026) | Loss 1.1265(1.1414) | Error 0.9111(0.9022) Steps 548(551.47) | Grad Norm 11.2957(8.2589) | Total Time 10.00(10.00)\n",
      "Iter 3820 | Time 16.5973(16.7710) | Bit/dim 1.2008(1.1522) | Xent 2.3026(2.3026) | Loss 1.2008(1.1522) | Error 0.9000(0.9014) Steps 536(550.76) | Grad Norm 12.1782(10.5329) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0058 | Time 63.1477, Epoch Time 1180.2756(1111.3933), Bit/dim 1.1624, Xent 2.3026, Loss 1.1624, Error 0.9031\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3830 | Time 17.0440(16.7464) | Bit/dim 1.1715(1.1586) | Xent 2.3026(2.3026) | Loss 1.1715(1.1586) | Error 0.9033(0.9013) Steps 554(550.02) | Grad Norm 19.1200(11.2353) | Total Time 10.00(10.00)\n",
      "Iter 3840 | Time 17.0510(16.7526) | Bit/dim 1.1413(1.1555) | Xent 2.3026(2.3026) | Loss 1.1413(1.1555) | Error 0.9022(0.9013) Steps 554(549.19) | Grad Norm 11.5651(10.9207) | Total Time 10.00(10.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p train_cnf_conditional_unsup.py --data mnist --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 900 --save experiments/cnf_cond_bs900_unsup --conditional True --log_freq 10 --weight_y 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
