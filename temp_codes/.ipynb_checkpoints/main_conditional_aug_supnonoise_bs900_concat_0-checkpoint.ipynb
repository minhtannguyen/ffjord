{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0,1,2,3,4,5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_augmented_nonoise.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "parser.add_argument(\"--concat_size\", type=int, default=0)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "if args.controlled_tol:\n",
      "    import lib.odenvp_conditional_tol as odenvp\n",
      "else:\n",
      "    import lib.odenvp_conditional as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise_concat(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    x = torch.cat((x, torch.zeros(args.concat_size, x.shape[1], x.shape[2]).to(x)),0)\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise_concat])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1 + args.concat_size\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise_concat,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise_concat,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise_concat,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise_concat,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise_concat,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise_concat,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise_concat,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz = modules.GaussianDiag.logp(mean, logs, z).view(-1,1)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(z)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol},)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z = modules.GaussianDiag.sample(mean, logs)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    if args.conditional: best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            loss.backward()\n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}, Xent {:.4f}, Loss {:.4f}, Error {:.4f}\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, loss_xent, loss, error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)\n",
      "            generated_samples = generated_samples[:,0:1,:,:]\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, concat_size=5, conditional=True, controlled_tol=False, conv=True, data='mnist', dims='64,64,64', divergence_fn='approximate', dl2int=None, imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume=None, rtol=1e-05, save='../experiments_published/cnf_published_conditional_nonoise_bs900_ccat_5', seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='sup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5)\n",
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=9408, bias=True)\n",
      "  (project_class): LinearZeros(in_features=4704, out_features=10, bias=True)\n",
      ")\n",
      "Number of trainable parameters: 1190790\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc.py:286: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "Iter 0010 | Time 14.9687(39.0944) | Bit/dim 122.5203(127.1002) | Xent 2.2670(2.2987) | Loss 2.2670(2.2987) | Error 0.8178(0.8850) Steps 410(410.00) | Grad Norm 68.5241(42.8004) | Total Time 10.00(10.00)\n",
      "Iter 0020 | Time 14.6844(32.7965) | Bit/dim 109.4285(124.0466) | Xent 2.1147(2.2701) | Loss 2.1147(2.2701) | Error 0.6622(0.8554) Steps 410(410.00) | Grad Norm 64.6653(49.9028) | Total Time 10.00(10.00)\n",
      "Iter 0030 | Time 14.8295(28.1145) | Bit/dim 94.2057(117.8816) | Xent 1.8462(2.1928) | Loss 1.8462(2.1928) | Error 0.3089(0.7596) Steps 410(410.00) | Grad Norm 55.1764(53.3031) | Total Time 10.00(10.00)\n",
      "Iter 0040 | Time 15.2793(24.6738) | Bit/dim 82.1537(109.7089) | Xent 1.4096(2.0466) | Loss 1.4096(2.0466) | Error 0.1978(0.6442) Steps 410(410.00) | Grad Norm 34.9666(53.1966) | Total Time 10.00(10.00)\n",
      "Iter 0050 | Time 15.4179(22.0807) | Bit/dim 89.0015(102.7462) | Xent 0.8654(1.8041) | Loss 0.8654(1.8041) | Error 0.2022(0.5341) Steps 410(410.00) | Grad Norm 35.3266(50.8809) | Total Time 10.00(10.00)\n",
      "Iter 0060 | Time 14.8736(20.1787) | Bit/dim 156.2634(108.5235) | Xent 0.5388(1.4935) | Loss 0.5388(1.4935) | Error 0.1600(0.4425) Steps 410(410.00) | Grad Norm 41.6669(48.1258) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0001 | Time 101.7600, Epoch Time 1139.2663(1139.2663), Bit/dim 173.5730, Xent 0.4426, Loss 0.4426, Error 0.1266\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0070 | Time 15.1487(18.8399) | Bit/dim 151.3211(124.9713) | Xent 0.4935(1.2316) | Loss 0.4935(1.2316) | Error 0.1500(0.3649) Steps 410(410.00) | Grad Norm 23.0574(44.8946) | Total Time 10.00(10.00)\n",
      "Iter 0080 | Time 15.5129(17.9584) | Bit/dim 104.9233(123.7900) | Xent 0.4653(1.0182) | Loss 0.4653(1.0182) | Error 0.1378(0.3018) Steps 416(410.51) | Grad Norm 31.2202(40.7844) | Total Time 10.00(10.00)\n",
      "Iter 0090 | Time 15.6197(17.2918) | Bit/dim 91.6314(116.3517) | Xent 0.3124(0.8532) | Loss 0.3124(0.8532) | Error 0.0956(0.2532) Steps 416(411.95) | Grad Norm 18.0633(35.7464) | Total Time 10.00(10.00)\n",
      "Iter 0100 | Time 15.6974(16.7113) | Bit/dim 100.9789(111.3632) | Xent 0.3524(0.7235) | Loss 0.3524(0.7235) | Error 0.1033(0.2152) Steps 416(413.02) | Grad Norm 13.1263(31.6828) | Total Time 10.00(10.00)\n",
      "Iter 0110 | Time 15.5563(16.2987) | Bit/dim 99.3639(108.3936) | Xent 0.2883(0.6186) | Loss 0.2883(0.6186) | Error 0.0967(0.1851) Steps 416(413.80) | Grad Norm 15.4974(27.9112) | Total Time 10.00(10.00)\n",
      "Iter 0120 | Time 14.8366(15.9831) | Bit/dim 96.7169(105.3851) | Xent 0.2822(0.5375) | Loss 0.2822(0.5375) | Error 0.0844(0.1597) Steps 416(414.38) | Grad Norm 19.0164(24.2591) | Total Time 10.00(10.00)\n",
      "Iter 0130 | Time 14.8107(15.7635) | Bit/dim 113.0530(104.6448) | Xent 0.3168(0.4708) | Loss 0.3168(0.4708) | Error 0.0989(0.1406) Steps 416(414.80) | Grad Norm 16.2974(21.8897) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0002 | Time 93.8715, Epoch Time 1114.7964(1138.5322), Bit/dim 112.5539, Xent 0.2645, Loss 0.2645, Error 0.0788\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0140 | Time 15.1106(15.5933) | Bit/dim 102.5819(105.8788) | Xent 0.2741(0.4161) | Loss 0.2741(0.4161) | Error 0.0756(0.1247) Steps 416(415.12) | Grad Norm 13.3834(19.8411) | Total Time 10.00(10.00)\n",
      "Iter 0150 | Time 16.5145(15.5332) | Bit/dim 105.5860(105.4182) | Xent 0.2556(0.3759) | Loss 0.2556(0.3759) | Error 0.0667(0.1120) Steps 416(415.35) | Grad Norm 11.9841(18.4932) | Total Time 10.00(10.00)\n",
      "Iter 0160 | Time 14.7615(15.5321) | Bit/dim 101.2717(105.1780) | Xent 0.3162(0.3430) | Loss 0.3162(0.3430) | Error 0.0733(0.1017) Steps 410(414.52) | Grad Norm 14.8081(17.7943) | Total Time 10.00(10.00)\n",
      "Iter 0170 | Time 14.4781(15.3589) | Bit/dim 111.1749(106.4527) | Xent 0.2251(0.3093) | Loss 0.2251(0.3093) | Error 0.0733(0.0927) Steps 410(413.33) | Grad Norm 12.1798(16.8760) | Total Time 10.00(10.00)\n",
      "Iter 0180 | Time 14.9576(15.2354) | Bit/dim 116.9945(108.3560) | Xent 0.1992(0.2792) | Loss 0.1992(0.2792) | Error 0.0633(0.0842) Steps 410(412.46) | Grad Norm 17.1159(16.3919) | Total Time 10.00(10.00)\n",
      "Iter 0190 | Time 14.7259(15.1162) | Bit/dim 103.1112(108.7618) | Xent 0.1357(0.2529) | Loss 0.1357(0.2529) | Error 0.0422(0.0761) Steps 410(411.81) | Grad Norm 12.1168(15.7018) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0003 | Time 93.5109, Epoch Time 1103.8811(1137.4926), Bit/dim 111.2592, Xent 0.1326, Loss 0.1326, Error 0.0405\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0200 | Time 15.0073(15.0539) | Bit/dim 112.6735(107.2763) | Xent 0.1405(0.2282) | Loss 0.1405(0.2282) | Error 0.0400(0.0689) Steps 410(411.34) | Grad Norm 13.7383(14.7305) | Total Time 10.00(10.00)\n",
      "Iter 0210 | Time 14.9947(14.9926) | Bit/dim 103.5122(107.4848) | Xent 0.1273(0.2046) | Loss 0.1273(0.2046) | Error 0.0400(0.0622) Steps 410(410.99) | Grad Norm 10.0524(13.6750) | Total Time 10.00(10.00)\n",
      "Iter 0220 | Time 14.3910(14.9638) | Bit/dim 109.7637(106.0074) | Xent 0.0809(0.1811) | Loss 0.0809(0.1811) | Error 0.0244(0.0544) Steps 410(410.73) | Grad Norm 8.0296(12.2317) | Total Time 10.00(10.00)\n",
      "Iter 0230 | Time 14.8568(15.1064) | Bit/dim 102.2602(106.9722) | Xent 0.1107(0.1614) | Loss 0.1107(0.1614) | Error 0.0311(0.0486) Steps 410(410.54) | Grad Norm 9.9524(11.3703) | Total Time 10.00(10.00)\n",
      "Iter 0240 | Time 14.4874(15.0656) | Bit/dim 98.9065(106.0573) | Xent 0.1105(0.1470) | Loss 0.1105(0.1470) | Error 0.0400(0.0443) Steps 410(410.40) | Grad Norm 10.7537(10.8640) | Total Time 10.00(10.00)\n",
      "Iter 0250 | Time 15.0342(15.0150) | Bit/dim 103.9809(106.5492) | Xent 0.1070(0.1338) | Loss 0.1070(0.1338) | Error 0.0378(0.0412) Steps 410(410.79) | Grad Norm 8.0134(10.3475) | Total Time 10.00(10.00)\n",
      "Iter 0260 | Time 14.6337(14.9978) | Bit/dim 105.2708(105.0701) | Xent 0.0699(0.1212) | Loss 0.0699(0.1212) | Error 0.0222(0.0368) Steps 410(410.58) | Grad Norm 5.8641(9.5317) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0004 | Time 93.8752, Epoch Time 1096.8776(1136.2742), Bit/dim 100.3563, Xent 0.0675, Loss 0.0675, Error 0.0219\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0270 | Time 14.8510(15.0122) | Bit/dim 90.0841(101.4334) | Xent 0.0724(0.1136) | Loss 0.0724(0.1136) | Error 0.0211(0.0346) Steps 410(410.43) | Grad Norm 6.8180(9.0363) | Total Time 10.00(10.00)\n",
      "Iter 0280 | Time 15.0104(14.9790) | Bit/dim 119.0533(105.2628) | Xent 0.0776(0.1032) | Loss 0.0776(0.1032) | Error 0.0233(0.0319) Steps 416(411.00) | Grad Norm 6.9717(8.5522) | Total Time 10.00(10.00)\n",
      "Iter 0290 | Time 15.1191(14.9787) | Bit/dim 85.2892(104.1294) | Xent 0.0818(0.0959) | Loss 0.0818(0.0959) | Error 0.0233(0.0298) Steps 410(411.31) | Grad Norm 7.9133(8.3642) | Total Time 10.00(10.00)\n",
      "Iter 0300 | Time 15.2236(14.9609) | Bit/dim 88.3133(100.9246) | Xent 0.0765(0.0912) | Loss 0.0765(0.0912) | Error 0.0256(0.0285) Steps 410(410.97) | Grad Norm 7.3741(7.9069) | Total Time 10.00(10.00)\n",
      "Iter 0310 | Time 15.2667(15.1111) | Bit/dim 74.7735(95.1668) | Xent 0.0640(0.0858) | Loss 0.0640(0.0858) | Error 0.0178(0.0265) Steps 410(410.71) | Grad Norm 4.9314(7.3131) | Total Time 10.00(10.00)\n",
      "Iter 0320 | Time 14.7549(15.1371) | Bit/dim 80.7652(91.6574) | Xent 0.0501(0.0809) | Loss 0.0501(0.0809) | Error 0.0144(0.0247) Steps 416(411.65) | Grad Norm 3.5086(6.7353) | Total Time 10.00(10.00)\n",
      "Iter 0330 | Time 14.6774(15.0956) | Bit/dim 90.2914(90.5882) | Xent 0.0560(0.0750) | Loss 0.0560(0.0750) | Error 0.0144(0.0231) Steps 416(413.34) | Grad Norm 6.1410(6.4947) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0005 | Time 93.5345, Epoch Time 1104.4466(1135.3194), Bit/dim 86.9564, Xent 0.0573, Loss 0.0573, Error 0.0194\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0340 | Time 15.6247(15.0858) | Bit/dim 101.8031(90.9292) | Xent 0.0450(0.0681) | Loss 0.0450(0.0681) | Error 0.0133(0.0210) Steps 428(414.26) | Grad Norm 3.4661(6.0801) | Total Time 10.00(10.00)\n",
      "Iter 0350 | Time 15.6620(15.0806) | Bit/dim 74.9143(89.0492) | Xent 0.0476(0.0657) | Loss 0.0476(0.0657) | Error 0.0167(0.0200) Steps 416(414.30) | Grad Norm 4.9413(5.8282) | Total Time 10.00(10.00)\n",
      "Iter 0360 | Time 15.6228(15.1083) | Bit/dim 83.5556(86.3071) | Xent 0.0419(0.0631) | Loss 0.0419(0.0631) | Error 0.0144(0.0194) Steps 422(414.50) | Grad Norm 3.7995(5.7704) | Total Time 10.00(10.00)\n",
      "Iter 0370 | Time 15.0053(15.0893) | Bit/dim 93.4235(87.1437) | Xent 0.0553(0.0612) | Loss 0.0553(0.0612) | Error 0.0167(0.0188) Steps 416(415.65) | Grad Norm 5.0721(5.7464) | Total Time 10.00(10.00)\n",
      "Iter 0380 | Time 15.2931(15.0960) | Bit/dim 74.5687(87.5483) | Xent 0.0474(0.0595) | Loss 0.0474(0.0595) | Error 0.0156(0.0179) Steps 416(415.74) | Grad Norm 4.9970(5.6563) | Total Time 10.00(10.00)\n",
      "Iter 0390 | Time 15.2490(15.1613) | Bit/dim 86.1772(84.2649) | Xent 0.0518(0.0565) | Loss 0.0518(0.0565) | Error 0.0122(0.0172) Steps 422(416.34) | Grad Norm 5.1519(5.3541) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0006 | Time 96.8416, Epoch Time 1112.2211(1134.6264), Bit/dim 85.5208, Xent 0.0418, Loss 0.0418, Error 0.0143\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0400 | Time 14.8905(15.1346) | Bit/dim 87.8455(85.0493) | Xent 0.0703(0.0541) | Loss 0.0703(0.0541) | Error 0.0244(0.0165) Steps 416(416.67) | Grad Norm 10.3171(5.3499) | Total Time 10.00(10.00)\n",
      "Iter 0410 | Time 14.9816(15.1118) | Bit/dim 77.9016(83.1813) | Xent 0.0746(0.0533) | Loss 0.0746(0.0533) | Error 0.0200(0.0166) Steps 410(415.99) | Grad Norm 5.2736(5.4697) | Total Time 10.00(10.00)\n",
      "Iter 0420 | Time 15.4073(15.1126) | Bit/dim 71.8660(79.4296) | Xent 0.0488(0.0530) | Loss 0.0488(0.0530) | Error 0.0089(0.0158) Steps 422(416.59) | Grad Norm 3.1262(5.2487) | Total Time 10.00(10.00)\n",
      "Iter 0430 | Time 14.9620(15.1143) | Bit/dim 60.3381(75.0994) | Xent 0.0321(0.0506) | Loss 0.0321(0.0506) | Error 0.0133(0.0152) Steps 416(416.44) | Grad Norm 3.6741(4.9252) | Total Time 10.00(10.00)\n",
      "Iter 0440 | Time 15.4391(15.0935) | Bit/dim 80.7627(73.3231) | Xent 0.0697(0.0478) | Loss 0.0697(0.0478) | Error 0.0178(0.0144) Steps 422(416.50) | Grad Norm 4.6111(4.6003) | Total Time 10.00(10.00)\n",
      "Iter 0450 | Time 15.4831(15.1589) | Bit/dim 78.1884(75.6902) | Xent 0.0523(0.0447) | Loss 0.0523(0.0447) | Error 0.0144(0.0138) Steps 446(419.21) | Grad Norm 4.8392(4.4380) | Total Time 10.00(10.00)\n",
      "Iter 0460 | Time 15.6719(15.2287) | Bit/dim 67.2826(74.7615) | Xent 0.0389(0.0443) | Loss 0.0389(0.0443) | Error 0.0133(0.0139) Steps 416(419.14) | Grad Norm 3.7875(4.4540) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0007 | Time 96.6355, Epoch Time 1111.8542(1133.9432), Bit/dim 65.3114, Xent 0.0344, Loss 0.0344, Error 0.0112\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0470 | Time 15.0131(15.2436) | Bit/dim 72.2717(72.7867) | Xent 0.0366(0.0432) | Loss 0.0366(0.0432) | Error 0.0122(0.0135) Steps 422(420.29) | Grad Norm 3.8363(4.3440) | Total Time 10.00(10.00)\n",
      "Iter 0480 | Time 15.7515(15.2153) | Bit/dim 75.6160(72.6323) | Xent 0.0322(0.0412) | Loss 0.0322(0.0412) | Error 0.0156(0.0132) Steps 416(419.75) | Grad Norm 3.1571(4.2736) | Total Time 10.00(10.00)\n",
      "Iter 0490 | Time 16.1184(15.5571) | Bit/dim 68.9038(75.5904) | Xent 0.0254(0.0407) | Loss 0.0254(0.0407) | Error 0.0089(0.0132) Steps 428(422.90) | Grad Norm 3.6283(4.2396) | Total Time 10.00(10.00)\n",
      "Iter 0500 | Time 15.0248(15.4429) | Bit/dim 53.7006(69.1734) | Xent 0.0393(0.0395) | Loss 0.0393(0.0395) | Error 0.0111(0.0128) Steps 416(421.36) | Grad Norm 3.5977(4.0429) | Total Time 10.00(10.00)\n",
      "Iter 0510 | Time 16.9908(15.6227) | Bit/dim 71.3191(68.2019) | Xent 0.0460(0.0391) | Loss 0.0460(0.0391) | Error 0.0089(0.0126) Steps 434(424.36) | Grad Norm 2.9690(3.8863) | Total Time 10.00(10.00)\n",
      "Iter 0520 | Time 15.9098(15.6432) | Bit/dim 49.2236(65.3469) | Xent 0.0271(0.0387) | Loss 0.0271(0.0387) | Error 0.0089(0.0124) Steps 428(426.37) | Grad Norm 2.4900(3.7978) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0008 | Time 103.9814, Epoch Time 1155.1905(1134.5807), Bit/dim 52.1930, Xent 0.0383, Loss 0.0383, Error 0.0129\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0530 | Time 15.9648(15.7355) | Bit/dim 54.6230(61.6678) | Xent 0.0239(0.0373) | Loss 0.0239(0.0373) | Error 0.0078(0.0121) Steps 428(427.08) | Grad Norm 2.7165(3.5703) | Total Time 10.00(10.00)\n",
      "Iter 0540 | Time 17.3749(15.8339) | Bit/dim 60.2354(60.2478) | Xent 0.0255(0.0362) | Loss 0.0255(0.0362) | Error 0.0044(0.0116) Steps 464(430.54) | Grad Norm 1.8930(3.4850) | Total Time 10.00(10.00)\n",
      "Iter 0550 | Time 15.8342(16.0137) | Bit/dim 54.5992(60.2510) | Xent 0.0245(0.0339) | Loss 0.0245(0.0339) | Error 0.0111(0.0111) Steps 434(435.77) | Grad Norm 2.8845(3.3452) | Total Time 10.00(10.00)\n",
      "Iter 0560 | Time 15.6878(15.9877) | Bit/dim 51.1477(58.7518) | Xent 0.0479(0.0338) | Loss 0.0479(0.0338) | Error 0.0100(0.0107) Steps 434(434.71) | Grad Norm 3.0236(3.2241) | Total Time 10.00(10.00)\n",
      "Iter 0570 | Time 15.6353(16.0451) | Bit/dim 53.7166(57.5184) | Xent 0.0341(0.0325) | Loss 0.0341(0.0325) | Error 0.0122(0.0104) Steps 440(435.61) | Grad Norm 3.2319(3.1308) | Total Time 10.00(10.00)\n",
      "Iter 0580 | Time 17.0076(16.0414) | Bit/dim 55.1435(58.6637) | Xent 0.0164(0.0316) | Loss 0.0164(0.0316) | Error 0.0033(0.0101) Steps 434(437.35) | Grad Norm 2.0919(3.0907) | Total Time 10.00(10.00)\n",
      "Iter 0590 | Time 15.4732(16.0546) | Bit/dim 43.9694(56.2890) | Xent 0.0374(0.0314) | Loss 0.0374(0.0314) | Error 0.0089(0.0099) Steps 428(436.24) | Grad Norm 3.0839(3.0576) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0009 | Time 97.5943, Epoch Time 1176.1881(1135.8289), Bit/dim 48.5064, Xent 0.0308, Loss 0.0308, Error 0.0097\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0600 | Time 17.4991(16.0708) | Bit/dim 47.3750(53.8060) | Xent 0.0268(0.0310) | Loss 0.0268(0.0310) | Error 0.0056(0.0099) Steps 434(435.08) | Grad Norm 2.0719(2.9652) | Total Time 10.00(10.00)\n",
      "Iter 0610 | Time 16.5332(16.0849) | Bit/dim 54.1058(54.0168) | Xent 0.0581(0.0309) | Loss 0.0581(0.0309) | Error 0.0122(0.0099) Steps 440(437.09) | Grad Norm 3.3910(2.9871) | Total Time 10.00(10.00)\n",
      "Iter 0620 | Time 16.6537(16.1108) | Bit/dim 46.2521(51.8375) | Xent 0.0326(0.0299) | Loss 0.0326(0.0299) | Error 0.0144(0.0098) Steps 440(438.16) | Grad Norm 2.7617(2.9094) | Total Time 10.00(10.00)\n",
      "Iter 0630 | Time 15.5520(16.0958) | Bit/dim 48.3890(50.9218) | Xent 0.0425(0.0294) | Loss 0.0425(0.0294) | Error 0.0067(0.0094) Steps 440(438.64) | Grad Norm 2.9330(2.8233) | Total Time 10.00(10.00)\n",
      "Iter 0640 | Time 16.5157(16.1449) | Bit/dim 37.2793(48.0051) | Xent 0.0179(0.0294) | Loss 0.0179(0.0294) | Error 0.0044(0.0095) Steps 440(438.08) | Grad Norm 1.4565(2.8946) | Total Time 10.00(10.00)\n",
      "Iter 0650 | Time 16.7281(16.3159) | Bit/dim 62.9888(48.5669) | Xent 0.0577(0.0296) | Loss 0.0577(0.0296) | Error 0.0122(0.0096) Steps 458(440.75) | Grad Norm 4.3716(2.8449) | Total Time 10.00(10.00)\n",
      "Iter 0660 | Time 16.0117(16.3129) | Bit/dim 42.7845(48.7351) | Xent 0.0405(0.0298) | Loss 0.0405(0.0298) | Error 0.0089(0.0095) Steps 452(444.01) | Grad Norm 2.5879(2.8826) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0010 | Time 101.8334, Epoch Time 1192.5776(1137.5313), Bit/dim 40.2974, Xent 0.0298, Loss 0.0298, Error 0.0100\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0670 | Time 17.8094(16.4420) | Bit/dim 44.9035(46.1098) | Xent 0.0237(0.0290) | Loss 0.0237(0.0290) | Error 0.0033(0.0092) Steps 452(446.11) | Grad Norm 1.9627(2.8011) | Total Time 10.00(10.00)\n",
      "Iter 0680 | Time 16.2811(16.3750) | Bit/dim 46.4083(47.2040) | Xent 0.0223(0.0281) | Loss 0.0223(0.0281) | Error 0.0056(0.0090) Steps 434(445.47) | Grad Norm 2.6952(2.7798) | Total Time 10.00(10.00)\n",
      "Iter 0690 | Time 16.1371(16.2557) | Bit/dim 40.7223(45.8526) | Xent 0.0294(0.0285) | Loss 0.0294(0.0285) | Error 0.0078(0.0091) Steps 434(443.41) | Grad Norm 2.5165(2.7225) | Total Time 10.00(10.00)\n",
      "Iter 0700 | Time 17.6269(16.5139) | Bit/dim 50.1439(45.7653) | Xent 0.0244(0.0273) | Loss 0.0244(0.0273) | Error 0.0067(0.0088) Steps 458(445.93) | Grad Norm 2.1980(2.6576) | Total Time 10.00(10.00)\n",
      "Iter 0710 | Time 17.8858(16.7556) | Bit/dim 37.1229(45.1879) | Xent 0.0199(0.0269) | Loss 0.0199(0.0269) | Error 0.0078(0.0084) Steps 440(448.05) | Grad Norm 2.0340(2.6509) | Total Time 10.00(10.00)\n",
      "Iter 0720 | Time 16.4908(16.5470) | Bit/dim 29.3866(41.4227) | Xent 0.0252(0.0274) | Loss 0.0252(0.0274) | Error 0.0111(0.0088) Steps 446(445.47) | Grad Norm 2.8426(2.6008) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0011 | Time 101.4375, Epoch Time 1216.0091(1139.8857), Bit/dim 31.2247, Xent 0.0360, Loss 0.0360, Error 0.0118\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0730 | Time 17.2158(16.6888) | Bit/dim 41.6323(39.1315) | Xent 0.0176(0.0275) | Loss 0.0176(0.0275) | Error 0.0056(0.0088) Steps 452(447.67) | Grad Norm 2.2216(2.6111) | Total Time 10.00(10.00)\n",
      "Iter 0740 | Time 16.8345(16.8256) | Bit/dim 54.2010(42.6684) | Xent 0.0260(0.0261) | Loss 0.0260(0.0261) | Error 0.0122(0.0084) Steps 458(449.12) | Grad Norm 2.9527(2.5876) | Total Time 10.00(10.00)\n",
      "Iter 0750 | Time 16.0031(16.6946) | Bit/dim 26.7707(40.7326) | Xent 0.0142(0.0254) | Loss 0.0142(0.0254) | Error 0.0067(0.0082) Steps 446(450.25) | Grad Norm 1.4136(2.5004) | Total Time 10.00(10.00)\n",
      "Iter 0760 | Time 17.8173(16.6817) | Bit/dim 37.3371(38.7940) | Xent 0.0117(0.0248) | Loss 0.0117(0.0248) | Error 0.0056(0.0082) Steps 452(449.49) | Grad Norm 1.7240(2.3945) | Total Time 10.00(10.00)\n",
      "Iter 0770 | Time 16.0543(16.6645) | Bit/dim 31.9441(39.5389) | Xent 0.0114(0.0239) | Loss 0.0114(0.0239) | Error 0.0033(0.0078) Steps 446(448.71) | Grad Norm 1.2879(2.3346) | Total Time 10.00(10.00)\n",
      "Iter 0780 | Time 16.2944(16.5103) | Bit/dim 34.9177(37.6642) | Xent 0.0212(0.0233) | Loss 0.0212(0.0233) | Error 0.0067(0.0077) Steps 446(447.74) | Grad Norm 1.8316(2.2661) | Total Time 10.00(10.00)\n",
      "Iter 0790 | Time 17.5466(16.6960) | Bit/dim 35.3786(37.4819) | Xent 0.0343(0.0228) | Loss 0.0343(0.0228) | Error 0.0122(0.0073) Steps 446(447.83) | Grad Norm 2.2251(2.2060) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0012 | Time 101.5276, Epoch Time 1219.6136(1142.2775), Bit/dim 29.8143, Xent 0.0337, Loss 0.0337, Error 0.0108\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0800 | Time 16.6355(16.7350) | Bit/dim 39.4789(36.9129) | Xent 0.0183(0.0222) | Loss 0.0183(0.0222) | Error 0.0056(0.0071) Steps 458(450.53) | Grad Norm 1.9076(2.1531) | Total Time 10.00(10.00)\n",
      "Iter 0810 | Time 16.4239(16.7325) | Bit/dim 30.5078(35.0399) | Xent 0.0124(0.0210) | Loss 0.0124(0.0210) | Error 0.0056(0.0067) Steps 446(449.67) | Grad Norm 1.4947(2.0430) | Total Time 10.00(10.00)\n",
      "Iter 0820 | Time 17.0093(16.6831) | Bit/dim 38.9199(35.5535) | Xent 0.0128(0.0202) | Loss 0.0128(0.0202) | Error 0.0044(0.0065) Steps 458(450.69) | Grad Norm 1.6819(2.0293) | Total Time 10.00(10.00)\n",
      "Iter 0830 | Time 16.4244(16.7322) | Bit/dim 34.4780(36.6088) | Xent 0.0319(0.0193) | Loss 0.0319(0.0193) | Error 0.0100(0.0063) Steps 452(452.43) | Grad Norm 1.9962(1.9632) | Total Time 10.00(10.00)\n",
      "Iter 0840 | Time 16.4318(16.8256) | Bit/dim 26.7258(34.7416) | Xent 0.0165(0.0191) | Loss 0.0165(0.0191) | Error 0.0056(0.0064) Steps 464(456.15) | Grad Norm 1.5466(1.9080) | Total Time 10.00(10.00)\n",
      "Iter 0850 | Time 17.2304(16.8153) | Bit/dim 35.2096(35.3431) | Xent 0.0227(0.0196) | Loss 0.0227(0.0196) | Error 0.0044(0.0062) Steps 446(455.55) | Grad Norm 1.7762(1.9025) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0013 | Time 101.8061, Epoch Time 1228.6559(1144.8689), Bit/dim 27.8899, Xent 0.0295, Loss 0.0295, Error 0.0091\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0860 | Time 17.7124(16.9155) | Bit/dim 29.3574(32.5223) | Xent 0.0199(0.0200) | Loss 0.0199(0.0200) | Error 0.0056(0.0062) Steps 452(454.20) | Grad Norm 1.7817(1.8920) | Total Time 10.00(10.00)\n",
      "Iter 0870 | Time 17.1319(17.0886) | Bit/dim 30.1065(32.5241) | Xent 0.0119(0.0193) | Loss 0.0119(0.0193) | Error 0.0044(0.0059) Steps 458(455.82) | Grad Norm 1.7752(1.8179) | Total Time 10.00(10.00)\n",
      "Iter 0880 | Time 17.5096(17.0236) | Bit/dim 34.6431(31.4111) | Xent 0.0153(0.0178) | Loss 0.0153(0.0178) | Error 0.0056(0.0055) Steps 458(453.27) | Grad Norm 2.1033(1.7662) | Total Time 10.00(10.00)\n",
      "Iter 0890 | Time 17.0945(17.1974) | Bit/dim 56.3353(36.0295) | Xent 0.0259(0.0161) | Loss 0.0259(0.0161) | Error 0.0067(0.0051) Steps 452(455.77) | Grad Norm 1.9879(1.7059) | Total Time 10.00(10.00)\n",
      "Iter 0900 | Time 16.1620(16.9184) | Bit/dim 26.2112(36.1011) | Xent 0.0133(0.0157) | Loss 0.0133(0.0157) | Error 0.0044(0.0049) Steps 434(452.34) | Grad Norm 1.4256(1.6769) | Total Time 10.00(10.00)\n",
      "Iter 0910 | Time 16.0132(16.7312) | Bit/dim 26.5341(33.6938) | Xent 0.0128(0.0165) | Loss 0.0128(0.0165) | Error 0.0044(0.0052) Steps 452(450.32) | Grad Norm 1.6256(1.7146) | Total Time 10.00(10.00)\n",
      "Iter 0920 | Time 15.8826(16.6751) | Bit/dim 30.8701(32.6048) | Xent 0.0060(0.0162) | Loss 0.0060(0.0162) | Error 0.0011(0.0051) Steps 446(449.47) | Grad Norm 1.0790(1.6546) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0014 | Time 104.3846, Epoch Time 1227.9752(1147.3621), Bit/dim 35.4828, Xent 0.0388, Loss 0.0388, Error 0.0090\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0930 | Time 17.0688(16.6802) | Bit/dim 34.4673(33.1138) | Xent 0.0233(0.0162) | Loss 0.0233(0.0162) | Error 0.0078(0.0051) Steps 458(450.35) | Grad Norm 3.0427(1.7034) | Total Time 10.00(10.00)\n",
      "Iter 0940 | Time 16.7728(16.6053) | Bit/dim 25.9967(31.8958) | Xent 0.0051(0.0157) | Loss 0.0051(0.0157) | Error 0.0022(0.0047) Steps 470(451.79) | Grad Norm 0.9694(1.6134) | Total Time 10.00(10.00)\n",
      "Iter 0950 | Time 16.9891(16.7260) | Bit/dim 27.1115(30.2425) | Xent 0.0181(0.0153) | Loss 0.0181(0.0153) | Error 0.0044(0.0045) Steps 452(452.99) | Grad Norm 1.7220(1.5927) | Total Time 10.00(10.00)\n",
      "Iter 0960 | Time 16.1569(16.7182) | Bit/dim 22.6166(29.2472) | Xent 0.0148(0.0148) | Loss 0.0148(0.0148) | Error 0.0056(0.0045) Steps 452(453.06) | Grad Norm 1.5218(1.5340) | Total Time 10.00(10.00)\n",
      "Iter 0970 | Time 18.4985(16.6624) | Bit/dim 37.1742(28.3405) | Xent 0.0246(0.0143) | Loss 0.0246(0.0143) | Error 0.0044(0.0043) Steps 476(453.25) | Grad Norm 1.4459(1.4572) | Total Time 10.00(10.00)\n",
      "Iter 0980 | Time 17.3527(17.0346) | Bit/dim 24.1332(30.5725) | Xent 0.0245(0.0150) | Loss 0.0245(0.0150) | Error 0.0067(0.0045) Steps 476(461.42) | Grad Norm 2.4033(1.6056) | Total Time 10.00(10.00)\n",
      "Iter 0990 | Time 16.6833(17.0643) | Bit/dim 18.6057(27.4658) | Xent 0.0173(0.0160) | Loss 0.0173(0.0160) | Error 0.0067(0.0051) Steps 458(460.61) | Grad Norm 1.5537(1.6008) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0015 | Time 104.1892, Epoch Time 1242.9917(1150.2309), Bit/dim 19.3578, Xent 0.0319, Loss 0.0319, Error 0.0091\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1000 | Time 17.1949(16.9318) | Bit/dim 29.1771(27.2815) | Xent 0.0160(0.0162) | Loss 0.0160(0.0162) | Error 0.0056(0.0053) Steps 452(458.95) | Grad Norm 1.6715(1.6359) | Total Time 10.00(10.00)\n",
      "Iter 1010 | Time 17.8866(17.1868) | Bit/dim 24.6651(26.7275) | Xent 0.0093(0.0161) | Loss 0.0093(0.0161) | Error 0.0033(0.0055) Steps 470(461.68) | Grad Norm 1.1655(1.6260) | Total Time 10.00(10.00)\n",
      "Iter 1020 | Time 18.1852(17.3345) | Bit/dim 26.7748(26.5272) | Xent 0.0194(0.0157) | Loss 0.0194(0.0157) | Error 0.0044(0.0052) Steps 470(464.18) | Grad Norm 1.9562(1.6048) | Total Time 10.00(10.00)\n",
      "Iter 1030 | Time 17.0767(17.5495) | Bit/dim 15.6528(25.6617) | Xent 0.0143(0.0159) | Loss 0.0143(0.0159) | Error 0.0056(0.0053) Steps 458(466.85) | Grad Norm 1.3136(1.5719) | Total Time 10.00(10.00)\n",
      "Iter 1040 | Time 16.3976(17.2760) | Bit/dim 20.7954(22.9856) | Xent 0.0112(0.0148) | Loss 0.0112(0.0148) | Error 0.0044(0.0053) Steps 458(465.96) | Grad Norm 1.3099(1.4639) | Total Time 10.00(10.00)\n",
      "Iter 1050 | Time 17.8018(17.3643) | Bit/dim 33.2231(24.2192) | Xent 0.0193(0.0137) | Loss 0.0193(0.0137) | Error 0.0033(0.0047) Steps 476(467.16) | Grad Norm 1.0946(1.3757) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0016 | Time 108.4193, Epoch Time 1280.7364(1154.1461), Bit/dim 34.3289, Xent 0.0360, Loss 0.0360, Error 0.0087\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1060 | Time 17.6113(17.5170) | Bit/dim 35.4681(27.1682) | Xent 0.0132(0.0124) | Loss 0.0132(0.0124) | Error 0.0056(0.0044) Steps 500(473.74) | Grad Norm 1.8276(1.3677) | Total Time 10.00(10.00)\n",
      "Iter 1070 | Time 17.8580(17.5696) | Bit/dim 34.8283(29.1407) | Xent 0.0138(0.0120) | Loss 0.0138(0.0120) | Error 0.0067(0.0043) Steps 482(477.59) | Grad Norm 1.6803(1.3867) | Total Time 10.00(10.00)\n",
      "Iter 1080 | Time 18.4251(17.8175) | Bit/dim 25.5489(28.7365) | Xent 0.0104(0.0117) | Loss 0.0104(0.0117) | Error 0.0022(0.0041) Steps 488(480.77) | Grad Norm 1.0320(1.3486) | Total Time 10.00(10.00)\n",
      "Iter 1090 | Time 17.2201(17.6959) | Bit/dim 22.7997(27.3507) | Xent 0.0214(0.0121) | Loss 0.0214(0.0121) | Error 0.0067(0.0043) Steps 482(482.72) | Grad Norm 2.3047(1.3972) | Total Time 10.00(10.00)\n",
      "Iter 1100 | Time 18.0869(17.6932) | Bit/dim 20.3249(25.8095) | Xent 0.0092(0.0129) | Loss 0.0092(0.0129) | Error 0.0022(0.0041) Steps 476(481.24) | Grad Norm 1.1348(1.4098) | Total Time 10.00(10.00)\n",
      "Iter 1110 | Time 17.8279(17.8271) | Bit/dim 23.9204(24.5131) | Xent 0.0208(0.0132) | Loss 0.0208(0.0132) | Error 0.0067(0.0043) Steps 476(480.08) | Grad Norm 1.9168(1.4120) | Total Time 10.00(10.00)\n",
      "Iter 1120 | Time 17.5804(17.7478) | Bit/dim 27.0867(25.4981) | Xent 0.0127(0.0126) | Loss 0.0127(0.0126) | Error 0.0044(0.0042) Steps 482(480.10) | Grad Norm 1.2789(1.3787) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0017 | Time 107.4508, Epoch Time 1298.4066(1158.4739), Bit/dim 21.8787, Xent 0.0305, Loss 0.0305, Error 0.0086\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1130 | Time 18.6494(17.8879) | Bit/dim 24.7270(24.8261) | Xent 0.0094(0.0119) | Loss 0.0094(0.0119) | Error 0.0033(0.0039) Steps 494(482.47) | Grad Norm 1.2425(1.3031) | Total Time 10.00(10.00)\n",
      "Iter 1140 | Time 18.4607(18.0162) | Bit/dim 28.4975(25.5708) | Xent 0.0036(0.0102) | Loss 0.0036(0.0102) | Error 0.0022(0.0033) Steps 482(482.94) | Grad Norm 0.7162(1.1736) | Total Time 10.00(10.00)\n",
      "Iter 1150 | Time 18.3603(18.1406) | Bit/dim 29.1928(26.4984) | Xent 0.0147(0.0097) | Loss 0.0147(0.0097) | Error 0.0067(0.0032) Steps 494(484.61) | Grad Norm 1.3371(1.1296) | Total Time 10.00(10.00)\n",
      "Iter 1160 | Time 18.1711(18.1989) | Bit/dim 18.3860(25.5634) | Xent 0.0057(0.0099) | Loss 0.0057(0.0099) | Error 0.0022(0.0032) Steps 470(485.46) | Grad Norm 0.9381(1.1276) | Total Time 10.00(10.00)\n",
      "Iter 1170 | Time 18.9493(17.8930) | Bit/dim 20.4842(23.7410) | Xent 0.0077(0.0093) | Loss 0.0077(0.0093) | Error 0.0022(0.0031) Steps 482(482.10) | Grad Norm 1.0484(1.0738) | Total Time 10.00(10.00)\n",
      "Iter 1180 | Time 19.3915(18.2090) | Bit/dim 33.9599(25.4611) | Xent 0.0080(0.0092) | Loss 0.0080(0.0092) | Error 0.0022(0.0029) Steps 524(489.23) | Grad Norm 0.8580(1.0782) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0018 | Time 117.3767, Epoch Time 1342.7602(1164.0025), Bit/dim 31.2635, Xent 0.0345, Loss 0.0345, Error 0.0079\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1190 | Time 18.1328(18.2459) | Bit/dim 31.0437(27.0676) | Xent 0.0083(0.0082) | Loss 0.0083(0.0082) | Error 0.0011(0.0026) Steps 524(496.84) | Grad Norm 1.1389(1.0438) | Total Time 10.00(10.00)\n",
      "Iter 1200 | Time 19.0597(18.2791) | Bit/dim 35.0959(28.8723) | Xent 0.0096(0.0081) | Loss 0.0096(0.0081) | Error 0.0022(0.0023) Steps 512(501.84) | Grad Norm 1.3034(1.0009) | Total Time 10.00(10.00)\n",
      "Iter 1210 | Time 19.1598(18.3077) | Bit/dim 20.7286(27.6773) | Xent 0.0057(0.0091) | Loss 0.0057(0.0091) | Error 0.0011(0.0027) Steps 482(497.60) | Grad Norm 1.0125(1.0918) | Total Time 10.00(10.00)\n",
      "Iter 1220 | Time 17.9140(18.3905) | Bit/dim 22.4033(26.0752) | Xent 0.0088(0.0088) | Loss 0.0088(0.0088) | Error 0.0033(0.0027) Steps 506(501.15) | Grad Norm 0.8988(1.0572) | Total Time 10.00(10.00)\n",
      "Iter 1230 | Time 18.9301(18.4128) | Bit/dim 26.6024(25.6918) | Xent 0.0073(0.0087) | Loss 0.0073(0.0087) | Error 0.0044(0.0028) Steps 500(501.73) | Grad Norm 1.0457(1.0526) | Total Time 10.00(10.00)\n",
      "Iter 1240 | Time 17.7199(18.2733) | Bit/dim 17.8784(24.8299) | Xent 0.0110(0.0088) | Loss 0.0110(0.0088) | Error 0.0011(0.0028) Steps 476(497.15) | Grad Norm 1.0163(1.0479) | Total Time 10.00(10.00)\n",
      "Iter 1250 | Time 18.2920(18.2287) | Bit/dim 21.0202(23.2897) | Xent 0.0044(0.0086) | Loss 0.0044(0.0086) | Error 0.0011(0.0028) Steps 488(494.85) | Grad Norm 0.7691(1.0264) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0019 | Time 110.6768, Epoch Time 1333.5960(1169.0903), Bit/dim 24.7191, Xent 0.0363, Loss 0.0363, Error 0.0088\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1260 | Time 19.6474(18.4139) | Bit/dim 30.1765(23.8982) | Xent 0.0071(0.0076) | Loss 0.0071(0.0076) | Error 0.0033(0.0025) Steps 494(495.76) | Grad Norm 1.0884(0.9562) | Total Time 10.00(10.00)\n",
      "Iter 1270 | Time 17.9354(18.3758) | Bit/dim 31.8312(26.0707) | Xent 0.0118(0.0070) | Loss 0.0118(0.0070) | Error 0.0033(0.0022) Steps 482(494.30) | Grad Norm 1.2260(0.9201) | Total Time 10.00(10.00)\n",
      "Iter 1280 | Time 18.8237(18.4935) | Bit/dim 30.3624(27.3777) | Xent 0.0109(0.0063) | Loss 0.0109(0.0063) | Error 0.0044(0.0021) Steps 518(496.79) | Grad Norm 1.4323(0.8918) | Total Time 10.00(10.00)\n",
      "Iter 1290 | Time 19.5389(18.5244) | Bit/dim 27.2152(27.3815) | Xent 0.0023(0.0065) | Loss 0.0023(0.0065) | Error 0.0000(0.0020) Steps 512(500.26) | Grad Norm 0.5436(0.9006) | Total Time 10.00(10.00)\n",
      "Iter 1300 | Time 18.5507(18.7280) | Bit/dim 19.7767(26.5505) | Xent 0.0119(0.0074) | Loss 0.0119(0.0074) | Error 0.0033(0.0023) Steps 500(504.06) | Grad Norm 1.0909(0.9554) | Total Time 10.00(10.00)\n",
      "Iter 1310 | Time 19.7811(18.8752) | Bit/dim 20.6978(25.3341) | Xent 0.0104(0.0075) | Loss 0.0104(0.0075) | Error 0.0044(0.0023) Steps 500(505.06) | Grad Norm 1.1788(0.9266) | Total Time 10.00(10.00)\n",
      "Iter 1320 | Time 17.9078(18.8329) | Bit/dim 19.3237(23.6483) | Xent 0.0030(0.0078) | Loss 0.0030(0.0078) | Error 0.0011(0.0025) Steps 482(502.50) | Grad Norm 0.4293(0.9473) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0020 | Time 110.7671, Epoch Time 1374.4528(1175.2512), Bit/dim 19.2243, Xent 0.0284, Loss 0.0284, Error 0.0076\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1330 | Time 17.6716(18.5683) | Bit/dim 25.3551(23.1646) | Xent 0.0032(0.0069) | Loss 0.0032(0.0069) | Error 0.0011(0.0023) Steps 488(498.18) | Grad Norm 0.6661(0.8882) | Total Time 10.00(10.00)\n",
      "Iter 1340 | Time 18.0850(18.4348) | Bit/dim 32.1631(24.6224) | Xent 0.0012(0.0057) | Loss 0.0012(0.0057) | Error 0.0011(0.0019) Steps 494(497.68) | Grad Norm 0.4335(0.7969) | Total Time 10.00(10.00)\n",
      "Iter 1350 | Time 18.5746(18.5780) | Bit/dim 37.0053(27.3843) | Xent 0.0070(0.0053) | Loss 0.0070(0.0053) | Error 0.0033(0.0017) Steps 518(501.57) | Grad Norm 1.1985(0.7768) | Total Time 10.00(10.00)\n",
      "Iter 1360 | Time 19.1975(18.6922) | Bit/dim 24.3760(28.2355) | Xent 0.0069(0.0055) | Loss 0.0069(0.0055) | Error 0.0022(0.0018) Steps 524(508.78) | Grad Norm 0.9139(0.8298) | Total Time 10.00(10.00)\n",
      "Iter 1370 | Time 19.3681(18.9544) | Bit/dim 21.6111(26.6483) | Xent 0.0077(0.0062) | Loss 0.0077(0.0062) | Error 0.0033(0.0021) Steps 512(510.13) | Grad Norm 1.0011(0.8657) | Total Time 10.00(10.00)\n",
      "Iter 1380 | Time 17.1988(18.8614) | Bit/dim 21.5681(25.2649) | Xent 0.0098(0.0067) | Loss 0.0098(0.0067) | Error 0.0044(0.0022) Steps 470(505.81) | Grad Norm 1.2191(0.9046) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0021 | Time 108.5494, Epoch Time 1349.4159(1180.4761), Bit/dim 21.7460, Xent 0.0314, Loss 0.0314, Error 0.0072\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1390 | Time 18.5430(18.5720) | Bit/dim 22.7009(24.3040) | Xent 0.0033(0.0064) | Loss 0.0033(0.0064) | Error 0.0022(0.0022) Steps 518(503.71) | Grad Norm 0.7445(0.8868) | Total Time 10.00(10.00)\n",
      "Iter 1400 | Time 18.8342(18.7955) | Bit/dim 26.6877(24.1532) | Xent 0.0072(0.0064) | Loss 0.0072(0.0064) | Error 0.0022(0.0022) Steps 500(505.32) | Grad Norm 1.0063(0.8979) | Total Time 10.00(10.00)\n",
      "Iter 1410 | Time 19.5861(18.7747) | Bit/dim 25.4452(24.9129) | Xent 0.0074(0.0066) | Loss 0.0074(0.0066) | Error 0.0033(0.0023) Steps 524(506.55) | Grad Norm 1.2217(0.9238) | Total Time 10.00(10.00)\n",
      "Iter 1420 | Time 19.1985(19.1050) | Bit/dim 25.0449(25.0203) | Xent 0.0041(0.0064) | Loss 0.0041(0.0064) | Error 0.0011(0.0023) Steps 518(512.93) | Grad Norm 0.7377(0.9238) | Total Time 10.00(10.00)\n",
      "Iter 1430 | Time 18.9622(19.1561) | Bit/dim 30.8336(25.8353) | Xent 0.0071(0.0060) | Loss 0.0071(0.0060) | Error 0.0011(0.0021) Steps 506(512.84) | Grad Norm 0.6380(0.8902) | Total Time 10.00(10.00)\n",
      "Iter 1440 | Time 17.3651(18.8945) | Bit/dim 19.6783(25.2655) | Xent 0.0016(0.0060) | Loss 0.0016(0.0060) | Error 0.0000(0.0019) Steps 494(509.68) | Grad Norm 0.3563(0.8607) | Total Time 10.00(10.00)\n",
      "Iter 1450 | Time 18.9016(18.7821) | Bit/dim 22.8437(24.0161) | Xent 0.0021(0.0056) | Loss 0.0021(0.0056) | Error 0.0000(0.0019) Steps 524(511.35) | Grad Norm 0.5578(0.8177) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0022 | Time 114.3099, Epoch Time 1384.1500(1186.5863), Bit/dim 26.0032, Xent 0.0284, Loss 0.0284, Error 0.0069\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1460 | Time 19.5748(18.8962) | Bit/dim 35.3743(25.4466) | Xent 0.0020(0.0045) | Loss 0.0020(0.0045) | Error 0.0011(0.0016) Steps 512(511.47) | Grad Norm 0.3907(0.6930) | Total Time 10.00(10.00)\n",
      "Iter 1470 | Time 19.4378(19.1858) | Bit/dim 39.5825(28.8038) | Xent 0.0016(0.0040) | Loss 0.0016(0.0040) | Error 0.0000(0.0014) Steps 524(514.49) | Grad Norm 0.6055(0.6751) | Total Time 10.00(10.00)\n",
      "Iter 1480 | Time 19.6356(19.4149) | Bit/dim 35.6716(31.5625) | Xent 0.0043(0.0040) | Loss 0.0043(0.0040) | Error 0.0022(0.0014) Steps 518(518.31) | Grad Norm 0.8931(0.7388) | Total Time 10.00(10.00)\n",
      "Iter 1490 | Time 20.2547(19.7320) | Bit/dim 25.9009(30.6744) | Xent 0.0049(0.0042) | Loss 0.0049(0.0042) | Error 0.0022(0.0016) Steps 548(526.58) | Grad Norm 0.9491(0.7770) | Total Time 10.00(10.00)\n",
      "Iter 1500 | Time 19.9557(19.9077) | Bit/dim 22.7093(28.7831) | Xent 0.0012(0.0045) | Loss 0.0012(0.0045) | Error 0.0000(0.0017) Steps 542(530.57) | Grad Norm 0.3436(0.7765) | Total Time 10.00(10.00)\n",
      "Iter 1510 | Time 20.3903(19.9865) | Bit/dim 26.8522(28.0013) | Xent 0.0016(0.0043) | Loss 0.0016(0.0043) | Error 0.0011(0.0017) Steps 542(531.03) | Grad Norm 0.4224(0.7618) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0023 | Time 119.2623, Epoch Time 1464.2273(1194.9156), Bit/dim 28.2257, Xent 0.0351, Loss 0.0351, Error 0.0076\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1520 | Time 18.9051(19.9252) | Bit/dim 28.4915(27.7055) | Xent 0.0043(0.0044) | Loss 0.0043(0.0044) | Error 0.0011(0.0018) Steps 536(533.32) | Grad Norm 0.7729(0.7599) | Total Time 10.00(10.00)\n",
      "Iter 1530 | Time 19.8190(19.8381) | Bit/dim 19.0159(27.5360) | Xent 0.0020(0.0048) | Loss 0.0020(0.0048) | Error 0.0011(0.0018) Steps 512(531.40) | Grad Norm 0.5012(0.7938) | Total Time 10.00(10.00)\n",
      "Iter 1540 | Time 19.4089(19.7883) | Bit/dim 24.0465(25.5323) | Xent 0.0013(0.0046) | Loss 0.0013(0.0046) | Error 0.0000(0.0017) Steps 542(528.52) | Grad Norm 0.2470(0.7469) | Total Time 10.00(10.00)\n",
      "Iter 1550 | Time 19.9725(19.9458) | Bit/dim 27.1753(26.2622) | Xent 0.0046(0.0046) | Loss 0.0046(0.0046) | Error 0.0022(0.0017) Steps 542(533.91) | Grad Norm 0.9077(0.7456) | Total Time 10.00(10.00)\n",
      "Iter 1560 | Time 20.1857(20.0112) | Bit/dim 31.0844(26.5204) | Xent 0.0185(0.0050) | Loss 0.0185(0.0050) | Error 0.0044(0.0017) Steps 560(537.41) | Grad Norm 1.5877(0.7482) | Total Time 10.00(10.00)\n",
      "Iter 1570 | Time 20.1758(20.0014) | Bit/dim 23.7785(26.2953) | Xent 0.0096(0.0049) | Loss 0.0096(0.0049) | Error 0.0011(0.0016) Steps 560(538.88) | Grad Norm 1.1002(0.7440) | Total Time 10.00(10.00)\n",
      "Iter 1580 | Time 19.1926(19.8498) | Bit/dim 23.4253(25.4974) | Xent 0.0071(0.0047) | Loss 0.0071(0.0047) | Error 0.0033(0.0016) Steps 542(539.52) | Grad Norm 1.1417(0.7242) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0024 | Time 117.5241, Epoch Time 1441.1956(1202.3040), Bit/dim 23.3114, Xent 0.0362, Loss 0.0362, Error 0.0074\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1590 | Time 20.5452(19.9034) | Bit/dim 25.6644(25.1522) | Xent 0.0006(0.0043) | Loss 0.0006(0.0043) | Error 0.0000(0.0014) Steps 542(540.37) | Grad Norm 0.1412(0.6924) | Total Time 10.00(10.00)\n",
      "Iter 1600 | Time 22.0280(20.1769) | Bit/dim 31.6778(26.0361) | Xent 0.0017(0.0040) | Loss 0.0017(0.0040) | Error 0.0000(0.0013) Steps 560(541.82) | Grad Norm 0.5385(0.6858) | Total Time 10.00(10.00)\n",
      "Iter 1610 | Time 18.9628(20.2137) | Bit/dim 24.5811(27.0851) | Xent 0.0025(0.0043) | Loss 0.0025(0.0043) | Error 0.0022(0.0014) Steps 524(543.98) | Grad Norm 0.7027(0.7208) | Total Time 10.00(10.00)\n",
      "Iter 1620 | Time 18.6154(19.7847) | Bit/dim 18.6436(25.5377) | Xent 0.0145(0.0052) | Loss 0.0145(0.0052) | Error 0.0033(0.0016) Steps 518(535.81) | Grad Norm 1.1808(0.7599) | Total Time 10.00(10.00)\n",
      "Iter 1630 | Time 20.2093(19.7321) | Bit/dim 14.3892(23.0518) | Xent 0.0045(0.0055) | Loss 0.0045(0.0055) | Error 0.0011(0.0017) Steps 518(531.33) | Grad Norm 0.6251(0.7628) | Total Time 10.00(10.00)\n",
      "Iter 1640 | Time 20.6168(20.0047) | Bit/dim 17.0382(21.0496) | Xent 0.0022(0.0049) | Loss 0.0022(0.0049) | Error 0.0000(0.0015) Steps 566(535.65) | Grad Norm 0.5272(0.6790) | Total Time 10.00(10.00)\n",
      "Iter 1650 | Time 18.9409(19.9705) | Bit/dim 19.3077(20.3742) | Xent 0.0015(0.0043) | Loss 0.0015(0.0043) | Error 0.0011(0.0013) Steps 536(541.75) | Grad Norm 0.3778(0.6145) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0025 | Time 114.8992, Epoch Time 1456.2030(1209.9209), Bit/dim 19.3221, Xent 0.0313, Loss 0.0313, Error 0.0080\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1660 | Time 20.6531(19.7209) | Bit/dim 23.6843(20.5687) | Xent 0.0040(0.0036) | Loss 0.0040(0.0036) | Error 0.0011(0.0011) Steps 536(540.09) | Grad Norm 0.6779(0.5494) | Total Time 10.00(10.00)\n",
      "Iter 1670 | Time 19.9284(20.0002) | Bit/dim 33.5621(22.9781) | Xent 0.0010(0.0030) | Loss 0.0010(0.0030) | Error 0.0000(0.0009) Steps 566(543.14) | Grad Norm 0.4129(0.5032) | Total Time 10.00(10.00)\n",
      "Iter 1680 | Time 20.1492(20.1212) | Bit/dim 34.4446(26.0615) | Xent 0.0002(0.0025) | Loss 0.0002(0.0025) | Error 0.0000(0.0008) Steps 548(545.34) | Grad Norm 0.0746(0.4643) | Total Time 10.00(10.00)\n",
      "Iter 1690 | Time 20.4485(20.1565) | Bit/dim 38.0286(28.7305) | Xent 0.0004(0.0022) | Loss 0.0004(0.0022) | Error 0.0000(0.0007) Steps 560(546.90) | Grad Norm 0.2054(0.4249) | Total Time 10.00(10.00)\n",
      "Iter 1700 | Time 19.9594(20.2407) | Bit/dim 35.4987(31.3794) | Xent 0.0050(0.0023) | Loss 0.0050(0.0023) | Error 0.0011(0.0007) Steps 548(548.87) | Grad Norm 0.8864(0.4379) | Total Time 10.00(10.00)\n",
      "Iter 1710 | Time 20.7438(20.2555) | Bit/dim 20.8258(30.8158) | Xent 0.0100(0.0031) | Loss 0.0100(0.0031) | Error 0.0022(0.0009) Steps 536(547.76) | Grad Norm 0.7859(0.5483) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0026 | Time 110.4370, Epoch Time 1458.6345(1217.3824), Bit/dim 13.5206, Xent 0.0313, Loss 0.0313, Error 0.0080\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1720 | Time 18.1927(19.9464) | Bit/dim 12.9709(26.4649) | Xent 0.0025(0.0034) | Loss 0.0025(0.0034) | Error 0.0011(0.0010) Steps 524(540.61) | Grad Norm 0.5700(0.5660) | Total Time 10.00(10.00)\n",
      "Iter 1730 | Time 20.1120(19.7700) | Bit/dim 21.6711(24.1085) | Xent 0.0007(0.0030) | Loss 0.0007(0.0030) | Error 0.0000(0.0010) Steps 554(540.01) | Grad Norm 0.2430(0.5233) | Total Time 10.00(10.00)\n",
      "Iter 1740 | Time 20.0207(19.9999) | Bit/dim 26.0891(24.2212) | Xent 0.0033(0.0029) | Loss 0.0033(0.0029) | Error 0.0011(0.0010) Steps 536(543.12) | Grad Norm 0.6613(0.5247) | Total Time 10.00(10.00)\n",
      "Iter 1750 | Time 21.1458(20.2180) | Bit/dim 25.8316(24.5272) | Xent 0.0016(0.0027) | Loss 0.0016(0.0027) | Error 0.0011(0.0009) Steps 572(545.73) | Grad Norm 0.5692(0.5236) | Total Time 10.00(10.00)\n",
      "Iter 1760 | Time 20.5455(20.3243) | Bit/dim 30.0232(25.2180) | Xent 0.0003(0.0024) | Loss 0.0003(0.0024) | Error 0.0000(0.0009) Steps 578(554.06) | Grad Norm 0.1016(0.4821) | Total Time 10.00(10.00)\n",
      "Iter 1770 | Time 20.8739(20.3602) | Bit/dim 26.2598(26.4216) | Xent 0.0027(0.0028) | Loss 0.0027(0.0028) | Error 0.0022(0.0010) Steps 566(557.93) | Grad Norm 0.8722(0.5246) | Total Time 10.00(10.00)\n",
      "Iter 1780 | Time 21.1270(20.4780) | Bit/dim 18.2534(25.1318) | Xent 0.0096(0.0034) | Loss 0.0096(0.0034) | Error 0.0033(0.0012) Steps 548(558.21) | Grad Norm 1.3622(0.5902) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0027 | Time 120.6917, Epoch Time 1476.0039(1225.1410), Bit/dim 16.9249, Xent 0.0299, Loss 0.0299, Error 0.0072\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1790 | Time 19.9207(20.3123) | Bit/dim 18.0731(23.1338) | Xent 0.0014(0.0029) | Loss 0.0014(0.0029) | Error 0.0011(0.0011) Steps 572(557.55) | Grad Norm 0.4131(0.5372) | Total Time 10.00(10.00)\n",
      "Iter 1800 | Time 21.6451(20.5177) | Bit/dim 23.2340(22.4853) | Xent 0.0005(0.0027) | Loss 0.0005(0.0027) | Error 0.0000(0.0010) Steps 578(560.59) | Grad Norm 0.2893(0.5111) | Total Time 10.00(10.00)\n",
      "Iter 1810 | Time 20.4628(20.7239) | Bit/dim 25.9333(23.2432) | Xent 0.0027(0.0027) | Loss 0.0027(0.0027) | Error 0.0022(0.0010) Steps 584(566.89) | Grad Norm 0.5913(0.5038) | Total Time 10.00(10.00)\n",
      "Iter 1820 | Time 21.0922(20.8875) | Bit/dim 25.6545(23.8325) | Xent 0.0077(0.0028) | Loss 0.0077(0.0028) | Error 0.0022(0.0010) Steps 566(569.26) | Grad Norm 1.0831(0.5142) | Total Time 10.00(10.00)\n",
      "Iter 1830 | Time 21.6999(20.9853) | Bit/dim 16.2396(22.5349) | Xent 0.0020(0.0030) | Loss 0.0020(0.0030) | Error 0.0000(0.0010) Steps 560(565.67) | Grad Norm 0.4280(0.5198) | Total Time 10.00(10.00)\n",
      "Iter 1840 | Time 21.6044(21.1038) | Bit/dim 21.8117(21.7177) | Xent 0.0100(0.0031) | Loss 0.0100(0.0031) | Error 0.0011(0.0009) Steps 590(566.37) | Grad Norm 0.5986(0.4929) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0028 | Time 123.7880, Epoch Time 1528.3874(1234.2384), Bit/dim 22.7184, Xent 0.0367, Loss 0.0367, Error 0.0073\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1850 | Time 21.8519(21.1345) | Bit/dim 22.6555(21.8288) | Xent 0.0016(0.0028) | Loss 0.0016(0.0028) | Error 0.0011(0.0008) Steps 584(571.95) | Grad Norm 0.4995(0.4655) | Total Time 10.00(10.00)\n",
      "Iter 1860 | Time 21.0874(21.1882) | Bit/dim 29.1404(23.0018) | Xent 0.0003(0.0023) | Loss 0.0003(0.0023) | Error 0.0000(0.0007) Steps 590(572.71) | Grad Norm 0.1442(0.4076) | Total Time 10.00(10.00)\n",
      "Iter 1870 | Time 21.5261(21.1051) | Bit/dim 32.3033(25.3991) | Xent 0.0000(0.0021) | Loss 0.0000(0.0021) | Error 0.0000(0.0007) Steps 578(575.57) | Grad Norm 0.0084(0.4197) | Total Time 10.00(10.00)\n",
      "Iter 1880 | Time 21.9229(21.0492) | Bit/dim 33.8033(27.4260) | Xent 0.0014(0.0020) | Loss 0.0014(0.0020) | Error 0.0011(0.0006) Steps 572(575.42) | Grad Norm 0.4514(0.4177) | Total Time 10.00(10.00)\n",
      "Iter 1890 | Time 20.6573(21.0624) | Bit/dim 33.1934(29.2916) | Xent 0.0006(0.0020) | Loss 0.0006(0.0020) | Error 0.0000(0.0006) Steps 572(573.85) | Grad Norm 0.2233(0.4217) | Total Time 10.00(10.00)\n",
      "Iter 1900 | Time 20.1021(20.9959) | Bit/dim 24.1759(28.5867) | Xent 0.0039(0.0021) | Loss 0.0039(0.0021) | Error 0.0011(0.0007) Steps 560(567.35) | Grad Norm 0.8189(0.4414) | Total Time 10.00(10.00)\n",
      "Iter 1910 | Time 21.4956(21.0738) | Bit/dim 26.0000(27.8183) | Xent 0.0006(0.0020) | Loss 0.0006(0.0020) | Error 0.0000(0.0007) Steps 566(566.34) | Grad Norm 0.2717(0.4405) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0029 | Time 120.2574, Epoch Time 1531.3829(1243.1527), Bit/dim 27.0458, Xent 0.0407, Loss 0.0407, Error 0.0082\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1920 | Time 22.5043(21.1569) | Bit/dim 28.5825(27.6406) | Xent 0.0002(0.0018) | Loss 0.0002(0.0018) | Error 0.0000(0.0007) Steps 584(569.81) | Grad Norm 0.0681(0.4104) | Total Time 10.00(10.00)\n",
      "Iter 1930 | Time 20.6032(21.1390) | Bit/dim 29.8270(28.0133) | Xent 0.0013(0.0019) | Loss 0.0013(0.0019) | Error 0.0011(0.0007) Steps 578(569.49) | Grad Norm 0.4874(0.4397) | Total Time 10.00(10.00)\n",
      "Iter 1940 | Time 19.6654(21.0789) | Bit/dim 23.8960(27.7500) | Xent 0.0016(0.0026) | Loss 0.0016(0.0026) | Error 0.0000(0.0009) Steps 542(567.41) | Grad Norm 0.3866(0.5235) | Total Time 10.00(10.00)\n",
      "Iter 1950 | Time 19.0627(20.6317) | Bit/dim 20.7039(26.1225) | Xent 0.0031(0.0029) | Loss 0.0031(0.0029) | Error 0.0022(0.0010) Steps 548(562.04) | Grad Norm 0.5909(0.5709) | Total Time 10.00(10.00)\n",
      "Iter 1960 | Time 20.3697(20.5971) | Bit/dim 14.2383(24.0975) | Xent 0.0073(0.0044) | Loss 0.0073(0.0044) | Error 0.0033(0.0014) Steps 554(560.26) | Grad Norm 0.9643(0.6635) | Total Time 10.00(10.00)\n",
      "Iter 1970 | Time 21.0052(20.8021) | Bit/dim 17.8992(21.6192) | Xent 0.0019(0.0043) | Loss 0.0019(0.0043) | Error 0.0011(0.0013) Steps 560(560.09) | Grad Norm 0.5245(0.6379) | Total Time 10.00(10.00)\n",
      "Iter 1980 | Time 20.6663(20.7451) | Bit/dim 27.3850(22.2383) | Xent 0.0034(0.0046) | Loss 0.0034(0.0046) | Error 0.0011(0.0015) Steps 548(558.50) | Grad Norm 0.7443(0.6479) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0030 | Time 123.9731, Epoch Time 1507.3103(1251.0774), Bit/dim 26.4341, Xent 0.0311, Loss 0.0311, Error 0.0069\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1990 | Time 22.2254(21.0289) | Bit/dim 22.9177(22.8908) | Xent 0.0043(0.0042) | Loss 0.0043(0.0042) | Error 0.0011(0.0014) Steps 590(564.66) | Grad Norm 0.7727(0.6248) | Total Time 10.00(10.00)\n",
      "Iter 2000 | Time 21.2919(21.1913) | Bit/dim 19.2751(22.3166) | Xent 0.0019(0.0044) | Loss 0.0019(0.0044) | Error 0.0011(0.0014) Steps 578(569.08) | Grad Norm 0.4584(0.6411) | Total Time 10.00(10.00)\n",
      "Iter 2010 | Time 20.6828(21.1114) | Bit/dim 20.5428(21.6555) | Xent 0.0034(0.0041) | Loss 0.0034(0.0041) | Error 0.0011(0.0014) Steps 566(567.86) | Grad Norm 0.6242(0.6376) | Total Time 10.00(10.00)\n",
      "Iter 2020 | Time 19.0132(20.7704) | Bit/dim 15.3196(20.3978) | Xent 0.0034(0.0050) | Loss 0.0034(0.0050) | Error 0.0011(0.0017) Steps 536(561.30) | Grad Norm 0.5005(0.6874) | Total Time 10.00(10.00)\n",
      "Iter 2030 | Time 21.2637(20.5939) | Bit/dim 13.6298(19.1002) | Xent 0.0053(0.0055) | Loss 0.0053(0.0055) | Error 0.0011(0.0018) Steps 566(559.44) | Grad Norm 0.5607(0.7286) | Total Time 10.00(10.00)\n",
      "Iter 2040 | Time 21.8628(20.7645) | Bit/dim 15.9627(17.6716) | Xent 0.0013(0.0052) | Loss 0.0013(0.0052) | Error 0.0000(0.0017) Steps 572(561.42) | Grad Norm 0.2670(0.6880) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0031 | Time 118.4598, Epoch Time 1517.6383(1259.0743), Bit/dim 18.9963, Xent 0.0372, Loss 0.0372, Error 0.0077\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2050 | Time 20.3842(20.8272) | Bit/dim 18.3925(17.9256) | Xent 0.0004(0.0048) | Loss 0.0004(0.0048) | Error 0.0000(0.0016) Steps 548(561.15) | Grad Norm 0.0924(0.6319) | Total Time 10.00(10.00)\n",
      "Iter 2060 | Time 19.7245(20.5345) | Bit/dim 19.6322(18.2357) | Xent 0.0053(0.0041) | Loss 0.0053(0.0041) | Error 0.0033(0.0015) Steps 560(556.55) | Grad Norm 1.0042(0.5827) | Total Time 10.00(10.00)\n",
      "Iter 2070 | Time 20.4757(20.4539) | Bit/dim 20.0684(18.6972) | Xent 0.0046(0.0037) | Loss 0.0046(0.0037) | Error 0.0011(0.0013) Steps 560(556.67) | Grad Norm 0.6261(0.5376) | Total Time 10.00(10.00)\n",
      "Iter 2080 | Time 20.3335(20.4123) | Bit/dim 22.0345(19.1571) | Xent 0.0070(0.0037) | Loss 0.0070(0.0037) | Error 0.0033(0.0013) Steps 560(557.56) | Grad Norm 1.0854(0.5644) | Total Time 10.00(10.00)\n",
      "Iter 2090 | Time 20.3858(20.3498) | Bit/dim 17.1955(18.9943) | Xent 0.0007(0.0035) | Loss 0.0007(0.0035) | Error 0.0000(0.0012) Steps 554(557.23) | Grad Norm 0.1324(0.5381) | Total Time 10.00(10.00)\n",
      "Iter 2100 | Time 20.0132(20.2423) | Bit/dim 15.5613(18.4334) | Xent 0.0055(0.0039) | Loss 0.0055(0.0039) | Error 0.0022(0.0013) Steps 566(557.14) | Grad Norm 0.8237(0.5680) | Total Time 10.00(10.00)\n",
      "Iter 2110 | Time 19.9187(20.0844) | Bit/dim 19.4594(18.4227) | Xent 0.0008(0.0039) | Loss 0.0008(0.0039) | Error 0.0000(0.0013) Steps 560(558.48) | Grad Norm 0.2094(0.5826) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0032 | Time 118.5924, Epoch Time 1457.0950(1265.0149), Bit/dim 19.7343, Xent 0.0454, Loss 0.0454, Error 0.0096\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2120 | Time 20.4880(20.2058) | Bit/dim 19.1504(18.6698) | Xent 0.0037(0.0035) | Loss 0.0037(0.0035) | Error 0.0022(0.0012) Steps 566(556.80) | Grad Norm 0.8732(0.5458) | Total Time 10.00(10.00)\n",
      "Iter 2130 | Time 21.6335(20.4172) | Bit/dim 17.6459(18.4601) | Xent 0.0024(0.0030) | Loss 0.0024(0.0030) | Error 0.0011(0.0010) Steps 560(557.27) | Grad Norm 0.6585(0.4966) | Total Time 10.00(10.00)\n",
      "Iter 2140 | Time 22.1482(20.7010) | Bit/dim 15.3984(18.0017) | Xent 0.0007(0.0028) | Loss 0.0007(0.0028) | Error 0.0000(0.0010) Steps 578(560.70) | Grad Norm 0.2726(0.4805) | Total Time 10.00(10.00)\n",
      "Iter 2150 | Time 21.4799(20.8608) | Bit/dim 19.3473(18.0941) | Xent 0.0007(0.0029) | Loss 0.0007(0.0029) | Error 0.0000(0.0010) Steps 566(563.80) | Grad Norm 0.1698(0.4991) | Total Time 10.00(10.00)\n",
      "Iter 2160 | Time 21.5524(21.0379) | Bit/dim 25.9655(19.5246) | Xent 0.0031(0.0028) | Loss 0.0031(0.0028) | Error 0.0022(0.0011) Steps 620(570.39) | Grad Norm 0.7091(0.5228) | Total Time 10.00(10.00)\n",
      "Iter 2170 | Time 22.0944(21.3036) | Bit/dim 22.5520(20.9184) | Xent 0.0024(0.0029) | Loss 0.0024(0.0029) | Error 0.0011(0.0012) Steps 590(576.52) | Grad Norm 0.6581(0.5537) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0033 | Time 121.7369, Epoch Time 1549.1075(1273.5377), Bit/dim 12.5822, Xent 0.0343, Loss 0.0343, Error 0.0081\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2180 | Time 20.7177(21.3599) | Bit/dim 12.6216(19.7301) | Xent 0.0071(0.0036) | Loss 0.0071(0.0036) | Error 0.0011(0.0012) Steps 584(578.43) | Grad Norm 0.6542(0.5731) | Total Time 10.00(10.00)\n",
      "Iter 2190 | Time 22.2951(21.4238) | Bit/dim 14.8767(18.0865) | Xent 0.0027(0.0039) | Loss 0.0027(0.0039) | Error 0.0011(0.0013) Steps 596(580.00) | Grad Norm 0.4658(0.5792) | Total Time 10.00(10.00)\n",
      "Iter 2200 | Time 22.1169(21.4834) | Bit/dim 18.9652(17.9440) | Xent 0.0088(0.0039) | Loss 0.0088(0.0039) | Error 0.0033(0.0013) Steps 596(583.00) | Grad Norm 1.2698(0.5901) | Total Time 10.00(10.00)\n",
      "Iter 2210 | Time 21.9307(21.4573) | Bit/dim 19.5962(18.3845) | Xent 0.0011(0.0040) | Loss 0.0011(0.0040) | Error 0.0000(0.0014) Steps 608(586.02) | Grad Norm 0.2804(0.6099) | Total Time 10.00(10.00)\n",
      "Iter 2220 | Time 21.0991(21.4507) | Bit/dim 12.3880(17.5878) | Xent 0.0094(0.0054) | Loss 0.0094(0.0054) | Error 0.0033(0.0018) Steps 566(585.90) | Grad Norm 0.8009(0.7063) | Total Time 10.00(10.00)\n",
      "Iter 2230 | Time 21.6675(21.4332) | Bit/dim 8.4537(15.0685) | Xent 0.0011(0.0062) | Loss 0.0011(0.0062) | Error 0.0000(0.0021) Steps 566(580.16) | Grad Norm 0.1988(0.7466) | Total Time 10.00(10.00)\n",
      "Iter 2240 | Time 22.1600(21.5472) | Bit/dim 10.1480(13.8538) | Xent 0.0032(0.0065) | Loss 0.0032(0.0065) | Error 0.0022(0.0023) Steps 602(582.64) | Grad Norm 0.6393(0.7500) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0034 | Time 122.2617, Epoch Time 1559.6761(1282.1218), Bit/dim 10.6432, Xent 0.0373, Loss 0.0373, Error 0.0092\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2250 | Time 21.6493(21.5592) | Bit/dim 11.8978(13.0624) | Xent 0.0026(0.0060) | Loss 0.0026(0.0060) | Error 0.0011(0.0020) Steps 584(582.97) | Grad Norm 0.5711(0.6958) | Total Time 10.00(10.00)\n",
      "Iter 2260 | Time 21.7851(21.6711) | Bit/dim 14.0099(13.2853) | Xent 0.0007(0.0060) | Loss 0.0007(0.0060) | Error 0.0000(0.0019) Steps 578(583.36) | Grad Norm 0.1351(0.6933) | Total Time 10.00(10.00)\n",
      "Iter 2270 | Time 22.9167(21.7990) | Bit/dim 18.7492(13.8427) | Xent 0.0004(0.0050) | Loss 0.0004(0.0050) | Error 0.0000(0.0017) Steps 572(584.78) | Grad Norm 0.1030(0.6269) | Total Time 10.00(10.00)\n",
      "Iter 2280 | Time 21.5333(21.8413) | Bit/dim 19.6988(15.4626) | Xent 0.0059(0.0050) | Loss 0.0059(0.0050) | Error 0.0022(0.0017) Steps 572(583.88) | Grad Norm 0.8153(0.6581) | Total Time 10.00(10.00)\n",
      "Iter 2290 | Time 22.2250(21.9280) | Bit/dim 15.7463(16.2805) | Xent 0.0053(0.0049) | Loss 0.0053(0.0049) | Error 0.0011(0.0016) Steps 596(585.02) | Grad Norm 0.7429(0.6299) | Total Time 10.00(10.00)\n",
      "Iter 2300 | Time 22.5714(22.0489) | Bit/dim 12.9630(15.5997) | Xent 0.0023(0.0049) | Loss 0.0023(0.0049) | Error 0.0011(0.0016) Steps 590(588.62) | Grad Norm 0.4155(0.6332) | Total Time 10.00(10.00)\n",
      "Iter 2310 | Time 22.0329(22.0627) | Bit/dim 14.4293(15.1279) | Xent 0.0067(0.0044) | Loss 0.0067(0.0044) | Error 0.0011(0.0015) Steps 584(590.81) | Grad Norm 0.6572(0.5961) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0035 | Time 121.1345, Epoch Time 1597.1934(1291.5740), Bit/dim 14.0998, Xent 0.0398, Loss 0.0398, Error 0.0076\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2320 | Time 21.2991(21.7607) | Bit/dim 14.8044(15.1302) | Xent 0.0002(0.0039) | Loss 0.0002(0.0039) | Error 0.0000(0.0013) Steps 554(585.66) | Grad Norm 0.0360(0.5488) | Total Time 10.00(10.00)\n",
      "Iter 2330 | Time 23.1232(21.8470) | Bit/dim 16.1922(15.4158) | Xent 0.0001(0.0035) | Loss 0.0001(0.0035) | Error 0.0000(0.0011) Steps 590(583.90) | Grad Norm 0.0384(0.5096) | Total Time 10.00(10.00)\n",
      "Iter 2340 | Time 21.8602(21.9699) | Bit/dim 14.2637(15.3949) | Xent 0.0051(0.0040) | Loss 0.0051(0.0040) | Error 0.0011(0.0012) Steps 578(585.44) | Grad Norm 0.5184(0.5090) | Total Time 10.00(10.00)\n",
      "Iter 2350 | Time 22.9643(22.0640) | Bit/dim 12.5919(14.7547) | Xent 0.0016(0.0034) | Loss 0.0016(0.0034) | Error 0.0000(0.0011) Steps 584(585.98) | Grad Norm 0.3519(0.4846) | Total Time 10.00(10.00)\n",
      "Iter 2360 | Time 23.1417(22.2994) | Bit/dim 13.3184(14.1711) | Xent 0.0010(0.0030) | Loss 0.0010(0.0030) | Error 0.0000(0.0009) Steps 596(588.54) | Grad Norm 0.2766(0.4515) | Total Time 10.00(10.00)\n",
      "Iter 2370 | Time 22.4839(22.4568) | Bit/dim 16.9223(14.4568) | Xent 0.0027(0.0024) | Loss 0.0027(0.0024) | Error 0.0022(0.0008) Steps 608(593.50) | Grad Norm 0.4843(0.3800) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0036 | Time 130.0001, Epoch Time 1621.0773(1301.4591), Bit/dim 18.3097, Xent 0.0382, Loss 0.0382, Error 0.0070\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2380 | Time 21.6208(22.3863) | Bit/dim 18.0685(15.3717) | Xent 0.0007(0.0021) | Loss 0.0007(0.0021) | Error 0.0000(0.0007) Steps 620(599.13) | Grad Norm 0.2143(0.3417) | Total Time 10.00(10.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_augmented_nonoise.py --data mnist --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/cnf_published_conditional_nonoise_bs900_ccat_5 --seed 1 --conditional True --controlled_tol False --train_mode sup --concat_size 5 --log_freq 10 --weight_y 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
