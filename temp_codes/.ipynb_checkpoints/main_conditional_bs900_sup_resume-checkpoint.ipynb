{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0,1,2,3,4,5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_conditional_sup.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.odenvp_conditional as odenvp\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=True, choices=[True, False])\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"./data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"./data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"./data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"./data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"./data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"./data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            './data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            './data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz = modules.GaussianDiag.logp(mean, logs, z).view(-1,1)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(z)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol},)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    # fixed_y = torch.randint(high=10, size=(100,)).type(torch.long).to(device, non_blocking=True)\n",
      "    fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "    fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "    with torch.no_grad():\n",
      "        mean, logs = model.module._prior(fixed_y_onehot)\n",
      "        fixed_z = modules.GaussianDiag.sample(mean, logs)\n",
      "    \n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    if args.conditional: best_error_score = float(\"inf\")\n",
      "    \n",
      "    itr = 0\n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                loss = loss_xent\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            loss.backward()\n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            xent_meter.update(loss_xent.item())\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits/dim', {'train': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        loss = loss_xent\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses_xent.append(loss_xent.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'validation': time.time() - start}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits/dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}, Xent {:.4f}, Loss {:.4f}, Error {:.4f}\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, loss_xent, loss, error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, conditional=True, conv=True, data='mnist', dims='64,64,64', divergence_fn='approximate', dl2int=None, imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume=None, rtol=1e-05, save='experiments/cnf_cond_bs900_wy_0_5_sup', solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=900, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5)\n",
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=1568, bias=True)\n",
      "  (project_class): LinearZeros(in_features=784, out_features=10, bias=True)\n",
      ")\n",
      "Number of trainable parameters: 828890\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc.py:286: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "Iter 0000 | Time 39.2804(39.2804) | Bit/dim 30.4294(30.4294) | Xent 2.3026(2.3026) | Loss 2.3026(2.3026) | Error 0.8878(0.8878) Steps 410(410.00) | Grad Norm 13.0524(13.0524) | Total Time 10.00(10.00)\n",
      "Iter 0010 | Time 13.0277(32.4082) | Bit/dim 31.5135(30.5651) | Xent 2.2688(2.2988) | Loss 2.2688(2.2988) | Error 0.8922(0.8805) Steps 410(410.00) | Grad Norm 13.3121(13.2544) | Total Time 10.00(10.00)\n",
      "Iter 0020 | Time 13.0144(27.3246) | Bit/dim 30.8377(30.7852) | Xent 2.1793(2.2801) | Loss 2.1793(2.2801) | Error 0.4856(0.8461) Steps 410(410.00) | Grad Norm 14.2279(13.4111) | Total Time 10.00(10.00)\n",
      "Iter 0030 | Time 13.1149(23.6047) | Bit/dim 32.7901(30.9271) | Xent 2.0068(2.2299) | Loss 2.0068(2.2299) | Error 0.3922(0.7266) Steps 410(410.00) | Grad Norm 13.8259(13.6386) | Total Time 10.00(10.00)\n",
      "Iter 0040 | Time 13.1921(20.8576) | Bit/dim 41.5959(32.5854) | Xent 1.7087(2.1304) | Loss 1.7087(2.1304) | Error 0.2556(0.6263) Steps 410(410.00) | Grad Norm 15.8510(13.9805) | Total Time 10.00(10.00)\n",
      "Iter 0050 | Time 12.9695(18.8100) | Bit/dim 85.8979(39.9617) | Xent 1.1006(1.9409) | Loss 1.1006(1.9409) | Error 0.2144(0.5241) Steps 410(410.00) | Grad Norm 14.8588(14.5066) | Total Time 10.00(10.00)\n",
      "Iter 0060 | Time 13.4272(17.3220) | Bit/dim 475.6706(94.6635) | Xent 0.5403(1.6274) | Loss 0.5403(1.6274) | Error 0.1711(0.4388) Steps 422(410.70) | Grad Norm 16.3304(15.6897) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0001 | Time 50.7457, Epoch Time 957.0635(957.0635), Bit/dim 793.5859, Xent 0.5004, Loss 0.5004, Error 0.1519\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0070 | Time 13.4250(16.3263) | Bit/dim 566.3337(256.4657) | Xent 0.4112(1.3443) | Loss 0.4112(1.3443) | Error 0.1256(0.3660) Steps 422(413.67) | Grad Norm 15.5944(17.9099) | Total Time 10.00(10.00)\n",
      "Iter 0080 | Time 13.2562(15.5686) | Bit/dim 327.0846(292.6875) | Xent 0.4007(1.1039) | Loss 0.4007(1.1039) | Error 0.1122(0.3025) Steps 416(415.33) | Grad Norm 10.3750(16.6429) | Total Time 10.00(10.00)\n",
      "Iter 0090 | Time 13.5928(15.0245) | Bit/dim 471.4794(317.5209) | Xent 0.2769(0.9065) | Loss 0.2769(0.9065) | Error 0.0811(0.2504) Steps 422(416.81) | Grad Norm 7.6850(15.0632) | Total Time 10.00(10.00)\n",
      "Iter 0100 | Time 13.5531(14.6588) | Bit/dim 484.3053(374.8006) | Xent 0.3017(0.7584) | Loss 0.3017(0.7584) | Error 0.0889(0.2110) Steps 422(418.17) | Grad Norm 14.9140(14.7711) | Total Time 10.00(10.00)\n",
      "Iter 0110 | Time 13.3623(14.3600) | Bit/dim 400.2272(386.8843) | Xent 0.2861(0.6414) | Loss 0.2861(0.6414) | Error 0.0889(0.1795) Steps 416(419.00) | Grad Norm 10.9269(14.3858) | Total Time 10.00(10.00)\n",
      "Iter 0120 | Time 13.5683(14.1209) | Bit/dim 456.5534(399.7796) | Xent 0.3178(0.5516) | Loss 0.3178(0.5516) | Error 0.0956(0.1554) Steps 416(419.26) | Grad Norm 10.1473(13.8889) | Total Time 10.00(10.00)\n",
      "Iter 0130 | Time 13.6663(13.9592) | Bit/dim 520.7526(422.8698) | Xent 0.2521(0.4760) | Loss 0.2521(0.4760) | Error 0.0711(0.1353) Steps 416(418.40) | Grad Norm 8.6210(13.1214) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0002 | Time 49.3257, Epoch Time 955.1988(957.0076), Bit/dim 562.0774, Xent 0.2456, Loss 0.2456, Error 0.0811\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0140 | Time 13.8079(13.8788) | Bit/dim 537.0402(452.3669) | Xent 0.2016(0.4135) | Loss 0.2016(0.4135) | Error 0.0744(0.1183) Steps 422(419.35) | Grad Norm 13.3194(12.6834) | Total Time 10.00(10.00)\n",
      "Iter 0150 | Time 13.6045(13.8146) | Bit/dim 563.2435(482.6946) | Xent 0.2557(0.3651) | Loss 0.2557(0.3651) | Error 0.0756(0.1040) Steps 422(420.01) | Grad Norm 11.0921(12.1114) | Total Time 10.00(10.00)\n",
      "Iter 0160 | Time 13.6979(13.7750) | Bit/dim 521.9376(500.0886) | Xent 0.1598(0.3243) | Loss 0.1598(0.3243) | Error 0.0489(0.0931) Steps 428(422.11) | Grad Norm 11.2398(11.9624) | Total Time 10.00(10.00)\n",
      "Iter 0170 | Time 13.7579(13.8150) | Bit/dim 559.6538(532.4591) | Xent 0.1907(0.2889) | Loss 0.1907(0.2889) | Error 0.0544(0.0827) Steps 434(424.81) | Grad Norm 11.1203(11.5883) | Total Time 10.00(10.00)\n",
      "Iter 0180 | Time 13.9021(13.8468) | Bit/dim 553.2464(536.2869) | Xent 0.1546(0.2580) | Loss 0.1546(0.2580) | Error 0.0511(0.0747) Steps 434(427.22) | Grad Norm 6.8791(10.8946) | Total Time 10.00(10.00)\n",
      "Iter 0190 | Time 13.9028(13.8698) | Bit/dim 650.0605(563.2967) | Xent 0.1302(0.2290) | Loss 0.1302(0.2290) | Error 0.0344(0.0668) Steps 434(429.00) | Grad Norm 10.5515(10.6112) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0003 | Time 52.2723, Epoch Time 977.6830(957.6278), Bit/dim 687.0215, Xent 0.1215, Loss 0.1215, Error 0.0453\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0200 | Time 14.0335(13.9035) | Bit/dim 632.8247(580.5442) | Xent 0.1334(0.2057) | Loss 0.1334(0.2057) | Error 0.0389(0.0598) Steps 440(430.84) | Grad Norm 7.8497(10.1425) | Total Time 10.00(10.00)\n",
      "Iter 0210 | Time 13.6786(13.8748) | Bit/dim 507.1450(578.2583) | Xent 0.0994(0.1839) | Loss 0.0994(0.1839) | Error 0.0289(0.0539) Steps 434(431.81) | Grad Norm 10.6877(9.8207) | Total Time 10.00(10.00)\n",
      "Iter 0220 | Time 14.1670(13.9154) | Bit/dim 651.0758(619.4611) | Xent 0.1117(0.1650) | Loss 0.1117(0.1650) | Error 0.0389(0.0487) Steps 440(433.38) | Grad Norm 9.0263(9.4027) | Total Time 10.00(10.00)\n",
      "Iter 0230 | Time 13.9144(13.9523) | Bit/dim 501.3438(615.2376) | Xent 0.1065(0.1501) | Loss 0.1065(0.1501) | Error 0.0311(0.0446) Steps 434(434.27) | Grad Norm 8.2873(9.1184) | Total Time 10.00(10.00)\n",
      "Iter 0240 | Time 14.1760(13.9908) | Bit/dim 629.6219(623.4648) | Xent 0.0872(0.1362) | Loss 0.0872(0.1362) | Error 0.0278(0.0410) Steps 440(435.64) | Grad Norm 6.9453(8.6033) | Total Time 10.00(10.00)\n",
      "Iter 0250 | Time 13.9957(13.9930) | Bit/dim 500.3300(616.3261) | Xent 0.0584(0.1241) | Loss 0.0584(0.1241) | Error 0.0156(0.0372) Steps 434(435.63) | Grad Norm 5.7660(8.1986) | Total Time 10.00(10.00)\n",
      "Iter 0260 | Time 13.6540(14.0201) | Bit/dim 489.3736(623.4662) | Xent 0.0969(0.1148) | Loss 0.0969(0.1148) | Error 0.0267(0.0345) Steps 434(436.48) | Grad Norm 6.6944(8.1324) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0004 | Time 52.2063, Epoch Time 990.7174(958.6205), Bit/dim 572.8646, Xent 0.0877, Loss 0.0877, Error 0.0378\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0270 | Time 14.2341(14.0319) | Bit/dim 577.7491(600.1321) | Xent 0.0770(0.1062) | Loss 0.0770(0.1062) | Error 0.0189(0.0317) Steps 446(437.31) | Grad Norm 5.0825(7.7175) | Total Time 10.00(10.00)\n",
      "Iter 0280 | Time 13.9542(14.1020) | Bit/dim 625.2756(605.3780) | Xent 0.0583(0.0968) | Loss 0.0583(0.0968) | Error 0.0133(0.0290) Steps 440(438.69) | Grad Norm 4.6982(7.1668) | Total Time 10.00(10.00)\n",
      "Iter 0290 | Time 14.5384(14.1698) | Bit/dim 564.9730(590.5383) | Xent 0.0660(0.0893) | Loss 0.0660(0.0893) | Error 0.0200(0.0267) Steps 452(439.94) | Grad Norm 5.1557(6.7140) | Total Time 10.00(10.00)\n",
      "Iter 0300 | Time 14.1336(14.2936) | Bit/dim 572.8004(588.5736) | Xent 0.0752(0.0852) | Loss 0.0752(0.0852) | Error 0.0233(0.0256) Steps 446(443.51) | Grad Norm 6.9999(6.5994) | Total Time 10.00(10.00)\n",
      "Iter 0310 | Time 14.6966(14.3473) | Bit/dim 596.0179(585.0695) | Xent 0.0463(0.0805) | Loss 0.0463(0.0805) | Error 0.0133(0.0242) Steps 452(444.52) | Grad Norm 5.3363(6.6354) | Total Time 10.00(10.00)\n",
      "Iter 0320 | Time 15.0068(14.4474) | Bit/dim 595.5421(587.0202) | Xent 0.0386(0.0740) | Loss 0.0386(0.0740) | Error 0.0144(0.0229) Steps 464(447.51) | Grad Norm 4.2504(6.1961) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0005 | Time 51.6027, Epoch Time 1023.6236(960.5706), Bit/dim 406.9117, Xent 0.0612, Loss 0.0612, Error 0.0291\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0330 | Time 14.0041(14.5394) | Bit/dim 406.0564(572.0862) | Xent 0.0589(0.0716) | Loss 0.0589(0.0716) | Error 0.0189(0.0223) Steps 440(450.31) | Grad Norm 3.9694(5.8427) | Total Time 10.00(10.00)\n",
      "Iter 0340 | Time 15.0573(14.6029) | Bit/dim 781.6785(586.9518) | Xent 0.0624(0.0669) | Loss 0.0624(0.0669) | Error 0.0156(0.0208) Steps 470(452.86) | Grad Norm 5.3165(5.6668) | Total Time 10.00(10.00)\n",
      "Iter 0350 | Time 14.0903(14.6223) | Bit/dim 368.0558(575.3958) | Xent 0.0488(0.0652) | Loss 0.0488(0.0652) | Error 0.0144(0.0199) Steps 440(454.28) | Grad Norm 4.5180(5.5098) | Total Time 10.00(10.00)\n",
      "Iter 0360 | Time 15.0533(14.6684) | Bit/dim 710.6136(570.9032) | Xent 0.0733(0.0614) | Loss 0.0733(0.0614) | Error 0.0211(0.0187) Steps 470(455.63) | Grad Norm 4.5280(5.1483) | Total Time 10.00(10.00)\n",
      "Iter 0370 | Time 14.7057(14.7309) | Bit/dim 397.3994(577.4210) | Xent 0.0552(0.0596) | Loss 0.0552(0.0596) | Error 0.0178(0.0182) Steps 452(457.23) | Grad Norm 4.8465(5.1027) | Total Time 10.00(10.00)\n",
      "Iter 0380 | Time 14.8130(14.7006) | Bit/dim 649.8511(557.0685) | Xent 0.0675(0.0581) | Loss 0.0675(0.0581) | Error 0.0233(0.0183) Steps 464(456.39) | Grad Norm 6.7667(5.1142) | Total Time 10.00(10.00)\n",
      "Iter 0390 | Time 14.0605(14.6911) | Bit/dim 365.3947(554.6659) | Xent 0.0747(0.0575) | Loss 0.0747(0.0575) | Error 0.0233(0.0180) Steps 446(456.79) | Grad Norm 5.5506(5.2084) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0006 | Time 54.1443, Epoch Time 1038.2652(962.9014), Bit/dim 558.9383, Xent 0.0419, Loss 0.0419, Error 0.0237\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0400 | Time 15.0201(14.6736) | Bit/dim 596.4283(546.0341) | Xent 0.0424(0.0553) | Loss 0.0424(0.0553) | Error 0.0111(0.0171) Steps 464(456.93) | Grad Norm 5.4690(5.1075) | Total Time 10.00(10.00)\n",
      "Iter 0410 | Time 14.4985(14.6085) | Bit/dim 410.9550(522.1411) | Xent 0.0506(0.0527) | Loss 0.0506(0.0527) | Error 0.0156(0.0162) Steps 452(455.58) | Grad Norm 3.9737(4.8647) | Total Time 10.00(10.00)\n",
      "Iter 0420 | Time 14.4919(14.5469) | Bit/dim 528.8400(498.0120) | Xent 0.0243(0.0483) | Loss 0.0243(0.0483) | Error 0.0111(0.0151) Steps 452(453.52) | Grad Norm 4.0659(4.4995) | Total Time 10.00(10.00)\n",
      "Iter 0430 | Time 14.7787(14.5730) | Bit/dim 418.2438(497.6531) | Xent 0.0415(0.0475) | Loss 0.0415(0.0475) | Error 0.0122(0.0148) Steps 452(454.45) | Grad Norm 3.2400(4.3472) | Total Time 10.00(10.00)\n",
      "Iter 0440 | Time 15.1227(14.6479) | Bit/dim 477.6345(480.0424) | Xent 0.0368(0.0463) | Loss 0.0368(0.0463) | Error 0.0144(0.0145) Steps 470(455.52) | Grad Norm 3.6160(4.2266) | Total Time 10.00(10.00)\n",
      "Iter 0450 | Time 14.9795(14.6609) | Bit/dim 453.1049(462.4210) | Xent 0.0561(0.0462) | Loss 0.0561(0.0462) | Error 0.0189(0.0144) Steps 470(456.43) | Grad Norm 5.1716(4.1458) | Total Time 10.00(10.00)\n",
      "Iter 0460 | Time 15.2314(14.7703) | Bit/dim 454.9065(452.0223) | Xent 0.0367(0.0444) | Loss 0.0367(0.0444) | Error 0.0122(0.0140) Steps 464(459.07) | Grad Norm 3.2867(3.9611) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0007 | Time 55.9285, Epoch Time 1040.1621(965.2193), Bit/dim 509.9152, Xent 0.0416, Loss 0.0416, Error 0.0232\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0470 | Time 15.3622(14.9008) | Bit/dim 615.6793(469.6680) | Xent 0.0539(0.0424) | Loss 0.0539(0.0424) | Error 0.0133(0.0133) Steps 476(463.08) | Grad Norm 4.1660(3.8903) | Total Time 10.00(10.00)\n",
      "Iter 0480 | Time 15.0843(14.8969) | Bit/dim 475.3229(468.4731) | Xent 0.0308(0.0401) | Loss 0.0308(0.0401) | Error 0.0122(0.0128) Steps 470(463.47) | Grad Norm 3.4156(3.8197) | Total Time 10.00(10.00)\n",
      "Iter 0490 | Time 14.4419(14.8873) | Bit/dim 317.7379(463.3672) | Xent 0.0241(0.0398) | Loss 0.0241(0.0398) | Error 0.0022(0.0123) Steps 452(463.58) | Grad Norm 2.7724(3.7605) | Total Time 10.00(10.00)\n",
      "Iter 0500 | Time 14.9863(14.8722) | Bit/dim 415.2356(437.3855) | Xent 0.0402(0.0407) | Loss 0.0402(0.0407) | Error 0.0133(0.0123) Steps 452(461.94) | Grad Norm 3.8017(3.6775) | Total Time 10.00(10.00)\n",
      "Iter 0510 | Time 14.1271(14.7938) | Bit/dim 350.6039(431.4079) | Xent 0.0313(0.0387) | Loss 0.0313(0.0387) | Error 0.0111(0.0120) Steps 446(458.95) | Grad Norm 3.2200(3.5974) | Total Time 10.00(10.00)\n",
      "Iter 0520 | Time 14.8721(14.7993) | Bit/dim 387.0707(432.3622) | Xent 0.0307(0.0375) | Loss 0.0307(0.0375) | Error 0.0122(0.0115) Steps 464(459.24) | Grad Norm 2.9523(3.4680) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0008 | Time 54.4634, Epoch Time 1047.8169(967.6972), Bit/dim 330.0158, Xent 0.0350, Loss 0.0350, Error 0.0227\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0530 | Time 14.8841(14.7590) | Bit/dim 370.2459(407.2118) | Xent 0.0301(0.0367) | Loss 0.0301(0.0367) | Error 0.0067(0.0112) Steps 464(458.97) | Grad Norm 2.8119(3.3397) | Total Time 10.00(10.00)\n",
      "Iter 0540 | Time 15.0478(14.8285) | Bit/dim 553.8817(429.5216) | Xent 0.0311(0.0343) | Loss 0.0311(0.0343) | Error 0.0144(0.0107) Steps 476(462.30) | Grad Norm 2.9477(3.2739) | Total Time 10.00(10.00)\n",
      "Iter 0550 | Time 14.4250(14.7720) | Bit/dim 326.7089(415.8955) | Xent 0.0232(0.0346) | Loss 0.0232(0.0346) | Error 0.0067(0.0108) Steps 458(461.54) | Grad Norm 2.2131(3.2623) | Total Time 10.00(10.00)\n",
      "Iter 0560 | Time 14.5889(14.7140) | Bit/dim 477.5065(424.1115) | Xent 0.0207(0.0327) | Loss 0.0207(0.0327) | Error 0.0078(0.0104) Steps 464(460.82) | Grad Norm 2.8028(3.1829) | Total Time 10.00(10.00)\n",
      "Iter 0570 | Time 14.6468(14.7130) | Bit/dim 336.9639(408.3152) | Xent 0.0382(0.0323) | Loss 0.0382(0.0323) | Error 0.0189(0.0105) Steps 458(460.66) | Grad Norm 3.7095(3.1035) | Total Time 10.00(10.00)\n",
      "Iter 0580 | Time 14.5199(14.7727) | Bit/dim 317.1289(400.3347) | Xent 0.0335(0.0321) | Loss 0.0335(0.0321) | Error 0.0089(0.0103) Steps 452(461.95) | Grad Norm 2.6558(3.0554) | Total Time 10.00(10.00)\n",
      "Iter 0590 | Time 14.3313(14.6493) | Bit/dim 267.2271(372.5134) | Xent 0.0198(0.0338) | Loss 0.0198(0.0338) | Error 0.0078(0.0107) Steps 446(458.04) | Grad Norm 1.6548(3.0516) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0009 | Time 53.1800, Epoch Time 1035.9818(969.7457), Bit/dim 332.3416, Xent 0.0383, Loss 0.0383, Error 0.0218\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0600 | Time 15.2956(14.6582) | Bit/dim 426.9763(368.2908) | Xent 0.0261(0.0319) | Loss 0.0261(0.0319) | Error 0.0078(0.0099) Steps 470(457.90) | Grad Norm 2.0414(2.9069) | Total Time 10.00(10.00)\n",
      "Iter 0610 | Time 14.7393(14.6343) | Bit/dim 363.6647(364.5375) | Xent 0.0223(0.0301) | Loss 0.0223(0.0301) | Error 0.0078(0.0094) Steps 458(457.11) | Grad Norm 2.3560(2.8576) | Total Time 10.00(10.00)\n",
      "Iter 0620 | Time 14.5135(14.6166) | Bit/dim 295.7167(360.1502) | Xent 0.0499(0.0319) | Loss 0.0499(0.0319) | Error 0.0167(0.0103) Steps 446(456.49) | Grad Norm 4.1717(3.0390) | Total Time 10.00(10.00)\n",
      "Iter 0630 | Time 14.4023(14.5465) | Bit/dim 279.1720(342.0274) | Xent 0.0244(0.0334) | Loss 0.0244(0.0334) | Error 0.0056(0.0107) Steps 452(454.99) | Grad Norm 1.8527(3.0521) | Total Time 10.00(10.00)\n",
      "Iter 0640 | Time 14.8305(14.5893) | Bit/dim 333.0102(332.2068) | Xent 0.0357(0.0316) | Loss 0.0357(0.0316) | Error 0.0122(0.0103) Steps 458(455.50) | Grad Norm 3.2774(2.9640) | Total Time 10.00(10.00)\n",
      "Iter 0650 | Time 14.5181(14.6007) | Bit/dim 343.4532(326.5869) | Xent 0.0153(0.0308) | Loss 0.0153(0.0308) | Error 0.0078(0.0099) Steps 458(455.57) | Grad Norm 2.1191(2.8332) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0010 | Time 53.4722, Epoch Time 1027.8808(971.4898), Bit/dim 192.4378, Xent 0.0374, Loss 0.0374, Error 0.0221\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0660 | Time 14.3235(14.5215) | Bit/dim 192.7587(309.7544) | Xent 0.0255(0.0324) | Loss 0.0255(0.0324) | Error 0.0056(0.0100) Steps 446(453.91) | Grad Norm 1.9192(2.8065) | Total Time 10.00(10.00)\n",
      "Iter 0670 | Time 14.9955(14.5816) | Bit/dim 422.2046(314.0252) | Xent 0.0377(0.0312) | Loss 0.0377(0.0312) | Error 0.0156(0.0099) Steps 470(455.82) | Grad Norm 3.8772(2.7522) | Total Time 10.00(10.00)\n",
      "Iter 0680 | Time 14.3851(14.5119) | Bit/dim 290.7282(311.2371) | Xent 0.0271(0.0305) | Loss 0.0271(0.0305) | Error 0.0089(0.0098) Steps 446(454.64) | Grad Norm 2.8244(2.7734) | Total Time 10.00(10.00)\n",
      "Iter 0690 | Time 14.1799(14.4633) | Bit/dim 305.5264(318.7071) | Xent 0.0265(0.0292) | Loss 0.0265(0.0292) | Error 0.0122(0.0097) Steps 446(453.47) | Grad Norm 3.6109(2.7572) | Total Time 10.00(10.00)\n",
      "Iter 0700 | Time 14.2917(14.4329) | Bit/dim 262.1118(305.6640) | Xent 0.0319(0.0287) | Loss 0.0319(0.0287) | Error 0.0100(0.0096) Steps 446(452.54) | Grad Norm 2.5808(2.6620) | Total Time 10.00(10.00)\n",
      "Iter 0710 | Time 14.1749(14.3633) | Bit/dim 208.9573(281.1215) | Xent 0.0302(0.0286) | Loss 0.0302(0.0286) | Error 0.0100(0.0092) Steps 446(450.82) | Grad Norm 1.9033(2.5060) | Total Time 10.00(10.00)\n",
      "Iter 0720 | Time 14.2874(14.3317) | Bit/dim 225.1505(268.0847) | Xent 0.0093(0.0297) | Loss 0.0093(0.0297) | Error 0.0011(0.0095) Steps 446(449.56) | Grad Norm 1.1227(2.5308) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0011 | Time 53.9952, Epoch Time 1014.5223(972.7808), Bit/dim 202.2749, Xent 0.0342, Loss 0.0342, Error 0.0210\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0730 | Time 15.0413(14.3678) | Bit/dim 325.0005(259.3424) | Xent 0.0385(0.0289) | Loss 0.0385(0.0289) | Error 0.0111(0.0089) Steps 464(450.34) | Grad Norm 2.8345(2.4102) | Total Time 10.00(10.00)\n",
      "Iter 0740 | Time 14.8595(14.5041) | Bit/dim 325.5208(269.2177) | Xent 0.0266(0.0265) | Loss 0.0266(0.0265) | Error 0.0078(0.0082) Steps 470(453.89) | Grad Norm 2.4701(2.3250) | Total Time 10.00(10.00)\n",
      "Iter 0750 | Time 14.3311(14.5457) | Bit/dim 238.3261(270.9988) | Xent 0.0210(0.0258) | Loss 0.0210(0.0258) | Error 0.0067(0.0080) Steps 452(455.07) | Grad Norm 1.8548(2.2594) | Total Time 10.00(10.00)\n",
      "Iter 0760 | Time 14.2990(14.5047) | Bit/dim 208.5991(258.9412) | Xent 0.0291(0.0252) | Loss 0.0291(0.0252) | Error 0.0078(0.0076) Steps 446(453.30) | Grad Norm 1.9255(2.1727) | Total Time 10.00(10.00)\n",
      "Iter 0770 | Time 14.8304(14.5192) | Bit/dim 278.2323(252.4148) | Xent 0.0317(0.0245) | Loss 0.0317(0.0245) | Error 0.0044(0.0074) Steps 458(452.92) | Grad Norm 1.8669(2.1119) | Total Time 10.00(10.00)\n",
      "Iter 0780 | Time 15.1394(14.6693) | Bit/dim 233.8242(259.1915) | Xent 0.0170(0.0230) | Loss 0.0170(0.0230) | Error 0.0067(0.0071) Steps 470(457.32) | Grad Norm 1.6127(2.0120) | Total Time 10.00(10.00)\n",
      "Iter 0790 | Time 14.3471(14.7213) | Bit/dim 177.4211(246.4026) | Xent 0.0206(0.0230) | Loss 0.0206(0.0230) | Error 0.0067(0.0073) Steps 446(458.74) | Grad Norm 1.9611(2.0024) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0012 | Time 53.2205, Epoch Time 1039.8038(974.7915), Bit/dim 184.5332, Xent 0.0292, Loss 0.0292, Error 0.0201\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0800 | Time 14.4064(14.6741) | Bit/dim 276.9817(244.4312) | Xent 0.0124(0.0222) | Loss 0.0124(0.0222) | Error 0.0044(0.0070) Steps 446(457.03) | Grad Norm 1.6123(1.9309) | Total Time 10.00(10.00)\n",
      "Iter 0810 | Time 14.9272(14.6967) | Bit/dim 303.7150(253.4768) | Xent 0.0254(0.0220) | Loss 0.0254(0.0220) | Error 0.0033(0.0070) Steps 464(458.00) | Grad Norm 1.4130(1.9742) | Total Time 10.00(10.00)\n",
      "Iter 0820 | Time 14.9854(14.8218) | Bit/dim 285.8801(267.5254) | Xent 0.0276(0.0208) | Loss 0.0276(0.0208) | Error 0.0100(0.0066) Steps 464(461.93) | Grad Norm 2.1231(1.9510) | Total Time 10.00(10.00)\n",
      "Iter 0830 | Time 14.4131(14.6856) | Bit/dim 250.2898(253.4967) | Xent 0.0166(0.0206) | Loss 0.0166(0.0206) | Error 0.0067(0.0067) Steps 458(458.58) | Grad Norm 1.7465(1.8604) | Total Time 10.00(10.00)\n",
      "Iter 0840 | Time 14.2708(14.6598) | Bit/dim 200.7215(250.6833) | Xent 0.0232(0.0212) | Loss 0.0232(0.0212) | Error 0.0078(0.0069) Steps 446(457.33) | Grad Norm 2.0629(1.9023) | Total Time 10.00(10.00)\n",
      "Iter 0850 | Time 14.4637(14.6570) | Bit/dim 195.8541(240.3601) | Xent 0.0290(0.0225) | Loss 0.0290(0.0225) | Error 0.0089(0.0074) Steps 458(457.31) | Grad Norm 1.8540(2.0113) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0013 | Time 53.4654, Epoch Time 1033.5708(976.5548), Bit/dim 143.0863, Xent 0.0290, Loss 0.0290, Error 0.0193\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0860 | Time 14.3884(14.5733) | Bit/dim 175.2098(219.8683) | Xent 0.0136(0.0222) | Loss 0.0136(0.0222) | Error 0.0056(0.0072) Steps 446(455.18) | Grad Norm 1.2018(1.9116) | Total Time 10.00(10.00)\n",
      "Iter 0870 | Time 15.3692(14.6942) | Bit/dim 284.3771(225.9423) | Xent 0.0258(0.0211) | Loss 0.0258(0.0211) | Error 0.0089(0.0068) Steps 470(458.27) | Grad Norm 2.6593(1.8636) | Total Time 10.00(10.00)\n",
      "Iter 0880 | Time 14.6674(14.7097) | Bit/dim 178.3290(230.0143) | Xent 0.0105(0.0210) | Loss 0.0105(0.0210) | Error 0.0044(0.0069) Steps 458(458.72) | Grad Norm 1.3193(1.9303) | Total Time 10.00(10.00)\n",
      "Iter 0890 | Time 14.3780(14.6624) | Bit/dim 197.2338(219.0199) | Xent 0.0480(0.0220) | Loss 0.0480(0.0220) | Error 0.0144(0.0072) Steps 452(457.81) | Grad Norm 2.8078(1.9374) | Total Time 10.00(10.00)\n",
      "Iter 0900 | Time 14.2657(14.5780) | Bit/dim 196.5031(209.6960) | Xent 0.0140(0.0209) | Loss 0.0140(0.0209) | Error 0.0056(0.0068) Steps 446(455.46) | Grad Norm 1.2683(1.8135) | Total Time 10.00(10.00)\n",
      "Iter 0910 | Time 14.2196(14.5050) | Bit/dim 225.1055(208.3779) | Xent 0.0285(0.0206) | Loss 0.0285(0.0206) | Error 0.0078(0.0070) Steps 446(453.15) | Grad Norm 2.0486(1.8094) | Total Time 10.00(10.00)\n",
      "Iter 0920 | Time 14.5353(14.5085) | Bit/dim 158.7841(199.4751) | Xent 0.0239(0.0223) | Loss 0.0239(0.0223) | Error 0.0100(0.0076) Steps 452(453.30) | Grad Norm 1.7254(1.8735) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0014 | Time 54.3093, Epoch Time 1029.2359(978.1353), Bit/dim 221.4711, Xent 0.0417, Loss 0.0417, Error 0.0216\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0930 | Time 15.0346(14.5889) | Bit/dim 291.5602(212.9398) | Xent 0.0239(0.0204) | Loss 0.0239(0.0204) | Error 0.0044(0.0070) Steps 464(455.99) | Grad Norm 1.5201(1.7902) | Total Time 10.00(10.00)\n",
      "Iter 0940 | Time 14.1381(14.5868) | Bit/dim 136.2501(207.2243) | Xent 0.0143(0.0203) | Loss 0.0143(0.0203) | Error 0.0044(0.0068) Steps 446(455.25) | Grad Norm 1.6305(1.7582) | Total Time 10.00(10.00)\n",
      "Iter 0950 | Time 15.2766(14.6721) | Bit/dim 228.5337(203.9586) | Xent 0.0400(0.0194) | Loss 0.0400(0.0194) | Error 0.0122(0.0066) Steps 476(458.54) | Grad Norm 2.4388(1.7006) | Total Time 10.00(10.00)\n",
      "Iter 0960 | Time 14.4727(14.7425) | Bit/dim 147.0655(202.8181) | Xent 0.0169(0.0199) | Loss 0.0169(0.0199) | Error 0.0078(0.0066) Steps 458(460.28) | Grad Norm 1.5274(1.7161) | Total Time 10.00(10.00)\n",
      "Iter 0970 | Time 15.1602(14.6929) | Bit/dim 201.3383(188.0046) | Xent 0.0196(0.0190) | Loss 0.0196(0.0190) | Error 0.0067(0.0062) Steps 464(457.59) | Grad Norm 1.7929(1.6295) | Total Time 10.00(10.00)\n",
      "Iter 0980 | Time 14.0707(14.6528) | Bit/dim 125.8924(186.7715) | Xent 0.0110(0.0200) | Loss 0.0110(0.0200) | Error 0.0022(0.0064) Steps 446(456.84) | Grad Norm 0.8622(1.6563) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0015 | Time 53.0723, Epoch Time 1035.5345(979.8572), Bit/dim 139.3084, Xent 0.0384, Loss 0.0384, Error 0.0210\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0990 | Time 14.3520(14.5643) | Bit/dim 141.8242(173.7252) | Xent 0.0090(0.0194) | Loss 0.0090(0.0194) | Error 0.0033(0.0062) Steps 446(454.00) | Grad Norm 1.0350(1.5498) | Total Time 10.00(10.00)\n",
      "Iter 1000 | Time 14.2511(14.5058) | Bit/dim 195.2023(175.3658) | Xent 0.0189(0.0183) | Loss 0.0189(0.0183) | Error 0.0056(0.0059) Steps 446(452.24) | Grad Norm 1.2998(1.4868) | Total Time 10.00(10.00)\n",
      "Iter 1010 | Time 14.5340(14.4692) | Bit/dim 150.6943(175.0426) | Xent 0.0407(0.0184) | Loss 0.0407(0.0184) | Error 0.0089(0.0058) Steps 458(451.82) | Grad Norm 2.6235(1.5176) | Total Time 10.00(10.00)\n",
      "Iter 1020 | Time 14.8889(14.4927) | Bit/dim 164.0375(169.4984) | Xent 0.0093(0.0177) | Loss 0.0093(0.0177) | Error 0.0033(0.0056) Steps 464(453.35) | Grad Norm 1.1448(1.4493) | Total Time 10.00(10.00)\n",
      "Iter 1030 | Time 15.0102(14.5750) | Bit/dim 222.8726(175.1849) | Xent 0.0108(0.0160) | Loss 0.0108(0.0160) | Error 0.0044(0.0052) Steps 470(456.06) | Grad Norm 1.3384(1.3753) | Total Time 10.00(10.00)\n",
      "Iter 1040 | Time 14.7696(14.6527) | Bit/dim 154.7068(181.0831) | Xent 0.0042(0.0147) | Loss 0.0042(0.0147) | Error 0.0022(0.0051) Steps 458(458.19) | Grad Norm 0.6420(1.3117) | Total Time 10.00(10.00)\n",
      "Iter 1050 | Time 14.5998(14.6425) | Bit/dim 146.5768(174.2453) | Xent 0.0104(0.0146) | Loss 0.0104(0.0146) | Error 0.0056(0.0050) Steps 458(458.14) | Grad Norm 0.9137(1.2537) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0016 | Time 53.7073, Epoch Time 1028.7604(981.3243), Bit/dim 160.6821, Xent 0.0276, Loss 0.0276, Error 0.0180\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1060 | Time 14.7545(14.5741) | Bit/dim 189.1047(171.5368) | Xent 0.0099(0.0142) | Loss 0.0099(0.0142) | Error 0.0022(0.0045) Steps 458(456.22) | Grad Norm 1.1231(1.2144) | Total Time 10.00(10.00)\n",
      "Iter 1070 | Time 15.2865(14.6838) | Bit/dim 261.6039(188.3345) | Xent 0.0176(0.0123) | Loss 0.0176(0.0123) | Error 0.0067(0.0040) Steps 476(459.67) | Grad Norm 1.7153(1.1239) | Total Time 10.00(10.00)\n",
      "Iter 1080 | Time 15.1523(14.8159) | Bit/dim 265.4254(210.2380) | Xent 0.0141(0.0115) | Loss 0.0141(0.0115) | Error 0.0033(0.0038) Steps 476(463.80) | Grad Norm 2.1400(1.2038) | Total Time 10.00(10.00)\n",
      "Iter 1090 | Time 14.5530(14.8842) | Bit/dim 223.7328(215.8682) | Xent 0.0221(0.0125) | Loss 0.0221(0.0125) | Error 0.0078(0.0041) Steps 452(466.10) | Grad Norm 1.9207(1.2662) | Total Time 10.00(10.00)\n",
      "Iter 1100 | Time 14.3953(14.7290) | Bit/dim 137.0575(200.6384) | Xent 0.0234(0.0136) | Loss 0.0234(0.0136) | Error 0.0067(0.0046) Steps 458(461.32) | Grad Norm 1.5517(1.3180) | Total Time 10.00(10.00)\n",
      "Iter 1110 | Time 14.9377(14.7145) | Bit/dim 194.0158(193.0965) | Xent 0.0043(0.0132) | Loss 0.0043(0.0132) | Error 0.0022(0.0045) Steps 464(460.63) | Grad Norm 0.9922(1.2770) | Total Time 10.00(10.00)\n",
      "Iter 1120 | Time 14.4995(14.6344) | Bit/dim 147.6799(187.4822) | Xent 0.0143(0.0136) | Loss 0.0143(0.0136) | Error 0.0056(0.0045) Steps 458(458.62) | Grad Norm 1.3248(1.3085) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0017 | Time 54.5404, Epoch Time 1041.3847(983.1261), Bit/dim 146.6620, Xent 0.0334, Loss 0.0334, Error 0.0189\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1130 | Time 14.8877(14.6481) | Bit/dim 226.6194(184.1391) | Xent 0.0139(0.0130) | Loss 0.0139(0.0130) | Error 0.0033(0.0042) Steps 464(458.64) | Grad Norm 1.3973(1.2474) | Total Time 10.00(10.00)\n",
      "Iter 1140 | Time 15.2268(14.8036) | Bit/dim 208.1084(198.0943) | Xent 0.0069(0.0124) | Loss 0.0069(0.0124) | Error 0.0033(0.0042) Steps 470(462.75) | Grad Norm 1.2111(1.2634) | Total Time 10.00(10.00)\n",
      "Iter 1150 | Time 14.6130(14.7827) | Bit/dim 132.7064(186.0679) | Xent 0.0179(0.0129) | Loss 0.0179(0.0129) | Error 0.0089(0.0046) Steps 446(461.38) | Grad Norm 1.4562(1.2705) | Total Time 10.00(10.00)\n",
      "Iter 1160 | Time 14.8522(14.7452) | Bit/dim 218.8384(180.8077) | Xent 0.0140(0.0131) | Loss 0.0140(0.0131) | Error 0.0056(0.0046) Steps 464(459.46) | Grad Norm 1.5824(1.2713) | Total Time 10.00(10.00)\n",
      "Iter 1170 | Time 14.6786(14.7997) | Bit/dim 143.6072(181.4413) | Xent 0.0166(0.0131) | Loss 0.0166(0.0131) | Error 0.0033(0.0046) Steps 464(461.54) | Grad Norm 1.2373(1.3080) | Total Time 10.00(10.00)\n",
      "Iter 1180 | Time 14.1510(14.7900) | Bit/dim 138.5077(171.0574) | Xent 0.0081(0.0131) | Loss 0.0081(0.0131) | Error 0.0022(0.0045) Steps 446(461.47) | Grad Norm 0.7887(1.2520) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0018 | Time 55.6139, Epoch Time 1047.8864(985.0690), Bit/dim 203.1584, Xent 0.0340, Loss 0.0340, Error 0.0187\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1190 | Time 15.3459(14.8228) | Bit/dim 201.7706(174.1563) | Xent 0.0078(0.0122) | Loss 0.0078(0.0122) | Error 0.0022(0.0042) Steps 476(462.08) | Grad Norm 1.0876(1.2049) | Total Time 10.00(10.00)\n",
      "Iter 1200 | Time 15.1942(14.9578) | Bit/dim 213.0391(184.2469) | Xent 0.0125(0.0116) | Loss 0.0125(0.0116) | Error 0.0033(0.0040) Steps 470(465.38) | Grad Norm 1.3647(1.1784) | Total Time 10.00(10.00)\n",
      "Iter 1210 | Time 14.8184(14.8750) | Bit/dim 161.6371(181.4590) | Xent 0.0070(0.0110) | Loss 0.0070(0.0110) | Error 0.0033(0.0038) Steps 464(463.08) | Grad Norm 0.7836(1.1502) | Total Time 10.00(10.00)\n",
      "Iter 1220 | Time 14.4925(14.7970) | Bit/dim 142.9185(171.0983) | Xent 0.0141(0.0110) | Loss 0.0141(0.0110) | Error 0.0033(0.0038) Steps 452(460.61) | Grad Norm 0.9779(1.1149) | Total Time 10.00(10.00)\n",
      "Iter 1230 | Time 14.3841(14.6717) | Bit/dim 134.9982(166.6298) | Xent 0.0062(0.0111) | Loss 0.0062(0.0111) | Error 0.0033(0.0040) Steps 452(457.12) | Grad Norm 0.9196(1.1311) | Total Time 10.00(10.00)\n",
      "Iter 1240 | Time 14.6134(14.6258) | Bit/dim 142.2359(156.5852) | Xent 0.0046(0.0105) | Loss 0.0046(0.0105) | Error 0.0011(0.0037) Steps 458(457.09) | Grad Norm 0.5532(1.0409) | Total Time 10.00(10.00)\n",
      "Iter 1250 | Time 14.8748(14.6899) | Bit/dim 165.6842(161.1543) | Xent 0.0020(0.0103) | Loss 0.0020(0.0103) | Error 0.0000(0.0035) Steps 464(459.57) | Grad Norm 0.3541(1.0144) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0019 | Time 54.6325, Epoch Time 1041.3316(986.7568), Bit/dim 147.1787, Xent 0.0370, Loss 0.0370, Error 0.0199\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1260 | Time 15.0487(14.7616) | Bit/dim 162.2543(160.1653) | Xent 0.0214(0.0104) | Loss 0.0214(0.0104) | Error 0.0056(0.0037) Steps 470(461.42) | Grad Norm 1.7118(1.0454) | Total Time 10.00(10.00)\n",
      "Iter 1270 | Time 14.9696(14.8158) | Bit/dim 189.2099(161.1355) | Xent 0.0028(0.0098) | Loss 0.0028(0.0098) | Error 0.0011(0.0035) Steps 464(462.24) | Grad Norm 0.5602(1.0059) | Total Time 10.00(10.00)\n",
      "Iter 1280 | Time 15.2635(14.9591) | Bit/dim 227.1431(180.9841) | Xent 0.0121(0.0094) | Loss 0.0121(0.0094) | Error 0.0022(0.0034) Steps 476(466.44) | Grad Norm 1.3621(1.0353) | Total Time 10.00(10.00)\n",
      "Iter 1290 | Time 14.2256(14.9052) | Bit/dim 183.6110(187.1779) | Xent 0.0107(0.0095) | Loss 0.0107(0.0095) | Error 0.0033(0.0035) Steps 446(464.94) | Grad Norm 1.1085(1.0663) | Total Time 10.00(10.00)\n",
      "Iter 1300 | Time 14.4997(14.8066) | Bit/dim 169.5361(186.8696) | Xent 0.0158(0.0099) | Loss 0.0158(0.0099) | Error 0.0056(0.0037) Steps 458(462.56) | Grad Norm 1.6506(1.1088) | Total Time 10.00(10.00)\n",
      "Iter 1310 | Time 15.5886(14.8580) | Bit/dim 180.3337(180.5509) | Xent 0.0116(0.0095) | Loss 0.0116(0.0095) | Error 0.0033(0.0035) Steps 476(464.36) | Grad Norm 1.0492(1.0563) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0020 | Time 56.0506, Epoch Time 1059.0712(988.9263), Bit/dim 162.0046, Xent 0.0358, Loss 0.0358, Error 0.0187\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1320 | Time 15.2603(14.9934) | Bit/dim 164.4425(181.9814) | Xent 0.0103(0.0094) | Loss 0.0103(0.0094) | Error 0.0067(0.0034) Steps 476(467.71) | Grad Norm 1.1291(1.0355) | Total Time 10.00(10.00)\n",
      "Iter 1330 | Time 15.3683(15.0810) | Bit/dim 169.1635(178.4372) | Xent 0.0133(0.0095) | Loss 0.0133(0.0095) | Error 0.0033(0.0033) Steps 476(469.89) | Grad Norm 1.1864(1.0268) | Total Time 10.00(10.00)\n",
      "Iter 1340 | Time 14.9691(15.0442) | Bit/dim 166.8881(172.2755) | Xent 0.0078(0.0088) | Loss 0.0078(0.0088) | Error 0.0033(0.0030) Steps 470(468.60) | Grad Norm 1.0515(0.9722) | Total Time 10.00(10.00)\n",
      "Iter 1350 | Time 15.2310(15.1019) | Bit/dim 169.2315(172.1401) | Xent 0.0027(0.0090) | Loss 0.0027(0.0090) | Error 0.0011(0.0029) Steps 476(470.54) | Grad Norm 0.5266(0.9603) | Total Time 10.00(10.00)\n",
      "Iter 1360 | Time 14.6266(15.0509) | Bit/dim 144.7140(169.7058) | Xent 0.0081(0.0090) | Loss 0.0081(0.0090) | Error 0.0044(0.0030) Steps 464(470.14) | Grad Norm 0.8962(0.9672) | Total Time 10.00(10.00)\n",
      "Iter 1370 | Time 14.7873(15.0170) | Bit/dim 168.2027(169.0878) | Xent 0.0136(0.0086) | Loss 0.0136(0.0086) | Error 0.0056(0.0030) Steps 464(469.12) | Grad Norm 1.4689(0.9840) | Total Time 10.00(10.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p train_cnf_conditional_sup.py --data mnist --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 900 --save experiments/cnf_cond_bs900_wy_0_5_sup --conditional True --log_freq 10 --weight_y 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
