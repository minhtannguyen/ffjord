{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_conditional_tol.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.odenvp_conditional_tol as odenvp\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=True, choices=[True, False])\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"./data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"./data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"./data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"./data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"./data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"./data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            './data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            './data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz = modules.GaussianDiag.logp(mean, logs, z).view(-1,1)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(z)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol},)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    # fixed_y = torch.randint(high=10, size=(100,)).type(torch.long).to(device, non_blocking=True)\n",
      "    fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "    fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "    with torch.no_grad():\n",
      "        mean, logs = model.module._prior(fixed_y_onehot)\n",
      "        fixed_z = modules.GaussianDiag.sample(mean, logs)\n",
      "    \n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    if args.conditional: best_error_score = float(\"inf\")\n",
      "    \n",
      "    itr = 0\n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                loss =  loss_nll + args.weight_y * loss_xent\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            loss.backward()\n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            xent_meter.update(loss_xent.item())\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits/dim', {'train': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses_xent.append(loss_xent.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'validation': time.time() - start}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits/dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}, Xent {:.4f}, Loss {:.4f}, Error {:.4f}\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, loss_xent, loss, error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=0.0001, autoencode=False, batch_norm=False, batch_size=8000, batch_size_schedule='', begin_epoch=1, conditional=True, conv=True, data='mnist', dims='64,64,64', divergence_fn='approximate', dl2int=None, imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=1, lr=0.01, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume=None, rtol=0.0001, save='experiments/cnf_cond_bs8K_ilr_0_01_tol_1e_m4_wy_0_5', solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=8000, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, val_freq=1, warmup_iters=113.0, weight_decay=0.0, weight_y=0.5)\n",
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=1568, bias=True)\n",
      "  (project_class): LinearZeros(in_features=784, out_features=10, bias=True)\n",
      ")\n",
      "Number of trainable parameters: 828890\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc.py:286: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "Iter 0000 | Time 60.3306(60.3306) | Bit/dim 21.1924(21.1924) | Xent 2.3026(2.3026) | Loss 22.3437(22.3437) | Error 0.8985(0.8985) Steps 296(296.00) | Grad Norm 160.6639(160.6639) | Total Time 10.00(10.00)\n",
      "Iter 0001 | Time 20.6319(59.1396) | Bit/dim 18.3213(21.1063) | Xent 2.2576(2.3012) | Loss 19.4502(22.2569) | Error 0.6758(0.8918) Steps 302(296.18) | Grad Norm 131.9300(159.8019) | Total Time 10.00(10.00)\n",
      "Iter 0002 | Time 21.8473(58.0208) | Bit/dim 14.4560(20.9068) | Xent 2.1579(2.2969) | Loss 15.5350(22.0552) | Error 0.6937(0.8859) Steps 326(297.07) | Grad Norm 80.4661(157.4218) | Total Time 10.00(10.00)\n",
      "Iter 0003 | Time 27.1730(57.0954) | Bit/dim 12.5046(20.6547) | Xent 2.0312(2.2890) | Loss 13.5203(21.7992) | Error 0.3739(0.8705) Steps 386(299.74) | Grad Norm 33.4261(153.7019) | Total Time 10.00(10.00)\n",
      "Iter 0004 | Time 26.5440(56.1789) | Bit/dim 13.3381(20.4352) | Xent 1.9105(2.2776) | Loss 14.2933(21.5740) | Error 0.4144(0.8568) Steps 404(302.87) | Grad Norm 73.8974(151.3078) | Total Time 10.00(10.00)\n",
      "Iter 0005 | Time 27.1622(55.3084) | Bit/dim 13.5489(20.2286) | Xent 1.8085(2.2635) | Loss 14.4532(21.3604) | Error 0.4610(0.8450) Steps 404(305.90) | Grad Norm 95.5800(149.6360) | Total Time 10.00(10.00)\n",
      "Iter 0006 | Time 27.5256(54.4749) | Bit/dim 11.8478(19.9772) | Xent 1.6529(2.2452) | Loss 12.6743(21.0998) | Error 0.3280(0.8294) Steps 398(308.67) | Grad Norm 79.6100(147.5352) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0001 | Time 8.6784, Epoch Time 232.6204(232.6204), Bit/dim 9.4606, Xent 1.5077, Loss 10.2144, Error 0.3806\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0007 | Time 29.3942(53.7225) | Bit/dim 9.4838(19.6624) | Xent 1.5100(2.2232) | Loss 10.2388(20.7740) | Error 0.2325(0.8115) Steps 404(311.53) | Grad Norm 48.1539(144.5537) | Total Time 10.00(10.00)\n",
      "Iter 0008 | Time 26.1759(52.8961) | Bit/dim 8.1638(19.3174) | Xent 1.3896(2.1982) | Loss 8.8586(20.4165) | Error 0.2212(0.7938) Steps 386(313.76) | Grad Norm 25.3740(140.9784) | Total Time 10.00(10.00)\n",
      "Iter 0009 | Time 24.7942(52.0530) | Bit/dim 7.8896(18.9746) | Xent 1.2905(2.1709) | Loss 8.5349(20.0601) | Error 0.2379(0.7772) Steps 362(315.21) | Grad Norm 30.3907(137.6607) | Total Time 10.00(10.00)\n",
      "Iter 0010 | Time 23.8991(51.2084) | Bit/dim 7.8014(18.6394) | Xent 1.2057(2.1420) | Loss 8.4043(19.7104) | Error 0.2478(0.7613) Steps 350(316.25) | Grad Norm 38.9609(134.6997) | Total Time 10.00(10.00)\n",
      "Iter 0011 | Time 23.5650(50.3791) | Bit/dim 7.0052(18.2904) | Xent 1.1557(2.1124) | Loss 7.5830(19.3466) | Error 0.2401(0.7456) Steps 350(317.26) | Grad Norm 36.6004(131.7568) | Total Time 10.00(10.00)\n",
      "Iter 0012 | Time 23.9767(49.5870) | Bit/dim 5.6679(17.9117) | Xent 1.1388(2.0832) | Loss 6.2373(18.9533) | Error 0.2314(0.7302) Steps 356(318.43) | Grad Norm 26.4387(128.5972) | Total Time 10.00(10.00)\n",
      "Iter 0013 | Time 27.1171(48.9129) | Bit/dim 4.4374(17.5075) | Xent 1.1561(2.0554) | Loss 5.0154(18.5351) | Error 0.2256(0.7151) Steps 392(320.63) | Grad Norm 14.5156(125.1748) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0002 | Time 8.0004, Epoch Time 199.0738(231.6140), Bit/dim 3.6648, Xent 1.2251, Loss 4.2773, Error 0.4077\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0014 | Time 26.8878(48.2522) | Bit/dim 3.6883(17.0929) | Xent 1.2200(2.0303) | Loss 4.2983(18.1080) | Error 0.2570(0.7013) Steps 374(322.23) | Grad Norm 6.1575(121.6042) | Total Time 10.00(10.00)\n",
      "Iter 0015 | Time 25.5345(47.5706) | Bit/dim 3.3848(16.6816) | Xent 1.3674(2.0104) | Loss 4.0685(17.6869) | Error 0.3485(0.6907) Steps 368(323.61) | Grad Norm 7.6724(118.1863) | Total Time 10.00(10.00)\n",
      "Iter 0016 | Time 24.9084(46.8908) | Bit/dim 3.3203(16.2808) | Xent 1.5758(1.9974) | Loss 4.1082(17.2795) | Error 0.5169(0.6855) Steps 368(324.94) | Grad Norm 11.4431(114.9840) | Total Time 10.00(10.00)\n",
      "Iter 0017 | Time 24.4336(46.2170) | Bit/dim 3.2192(15.8890) | Xent 1.6943(1.9883) | Loss 4.0663(16.8831) | Error 0.5874(0.6826) Steps 368(326.23) | Grad Norm 11.9934(111.8943) | Total Time 10.00(10.00)\n",
      "Iter 0018 | Time 25.0132(45.5809) | Bit/dim 3.0326(15.5033) | Xent 1.7641(1.9816) | Loss 3.9146(16.4940) | Error 0.5504(0.6786) Steps 368(327.48) | Grad Norm 10.1177(108.8410) | Total Time 10.00(10.00)\n",
      "Iter 0019 | Time 25.6580(44.9832) | Bit/dim 2.8244(15.1229) | Xent 1.8140(1.9765) | Loss 3.7314(16.1112) | Error 0.4501(0.6718) Steps 380(329.06) | Grad Norm 7.5140(105.8012) | Total Time 10.00(10.00)\n",
      "Iter 0020 | Time 28.4303(44.4867) | Bit/dim 2.6784(14.7496) | Xent 1.8748(1.9735) | Loss 3.6158(15.7363) | Error 0.3979(0.6635) Steps 410(331.49) | Grad Norm 5.5174(102.7927) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0003 | Time 8.7909, Epoch Time 201.8693(230.7217), Bit/dim 2.5956, Xent 1.9445, Loss 3.5679, Error 0.5508\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0021 | Time 29.0255(44.0228) | Bit/dim 2.6073(14.3853) | Xent 1.9444(1.9726) | Loss 3.5795(15.3716) | Error 0.4290(0.6565) Steps 410(333.84) | Grad Norm 4.4056(99.8410) | Total Time 10.00(10.00)\n",
      "Iter 0022 | Time 30.1442(43.6065) | Bit/dim 2.6117(14.0321) | Xent 1.9886(1.9731) | Loss 3.6060(15.0186) | Error 0.5511(0.6533) Steps 416(336.31) | Grad Norm 4.1342(96.9698) | Total Time 10.00(10.00)\n",
      "Iter 0023 | Time 30.0077(43.1985) | Bit/dim 2.6422(13.6904) | Xent 2.0210(1.9745) | Loss 3.6527(14.6777) | Error 0.6214(0.6524) Steps 416(338.70) | Grad Norm 4.2405(94.1880) | Total Time 10.00(10.00)\n",
      "Iter 0024 | Time 28.9543(42.7712) | Bit/dim 2.6731(13.3599) | Xent 2.0338(1.9763) | Loss 3.6901(14.3480) | Error 0.6155(0.6513) Steps 416(341.02) | Grad Norm 4.4230(91.4950) | Total Time 10.00(10.00)\n",
      "Iter 0025 | Time 28.9524(42.3566) | Bit/dim 2.6741(13.0393) | Xent 2.0135(1.9774) | Loss 3.6809(14.0280) | Error 0.5769(0.6490) Steps 416(343.27) | Grad Norm 4.5115(88.8855) | Total Time 10.00(10.00)\n",
      "Iter 0026 | Time 28.9195(41.9535) | Bit/dim 2.6428(12.7274) | Xent 1.9453(1.9765) | Loss 3.6155(13.7156) | Error 0.4868(0.6442) Steps 404(345.09) | Grad Norm 4.4890(86.3536) | Total Time 10.00(10.00)\n",
      "Iter 0027 | Time 28.0275(41.5357) | Bit/dim 2.6006(12.4236) | Xent 1.8632(1.9731) | Loss 3.5322(13.4101) | Error 0.4300(0.6378) Steps 386(346.32) | Grad Norm 4.4851(83.8976) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0004 | Time 8.3533, Epoch Time 224.7256(230.5418), Bit/dim 2.5265, Xent 1.7608, Loss 3.4069, Error 0.5298\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0028 | Time 27.8927(41.1264) | Bit/dim 2.5498(12.1274) | Xent 1.7537(1.9665) | Loss 3.4267(13.1106) | Error 0.4096(0.6309) Steps 386(347.51) | Grad Norm 4.5494(81.5171) | Total Time 10.00(10.00)\n",
      "Iter 0029 | Time 29.0680(40.7647) | Bit/dim 2.4865(11.8382) | Xent 1.6001(1.9555) | Loss 3.2866(12.8159) | Error 0.3814(0.6234) Steps 386(348.66) | Grad Norm 4.6517(79.2111) | Total Time 10.00(10.00)\n",
      "Iter 0030 | Time 29.6704(40.4318) | Bit/dim 2.4560(11.5567) | Xent 1.4145(1.9393) | Loss 3.1632(12.5263) | Error 0.3445(0.6151) Steps 398(350.14) | Grad Norm 4.5361(76.9709) | Total Time 10.00(10.00)\n",
      "Iter 0031 | Time 31.3414(40.1591) | Bit/dim 2.4434(11.2833) | Xent 1.2160(1.9176) | Loss 3.0513(12.2421) | Error 0.3039(0.6057) Steps 404(351.76) | Grad Norm 4.1466(74.7862) | Total Time 10.00(10.00)\n",
      "Iter 0032 | Time 29.1573(39.8291) | Bit/dim 2.4565(11.0185) | Xent 1.0430(1.8913) | Loss 2.9780(11.9642) | Error 0.2709(0.5957) Steps 392(352.97) | Grad Norm 3.0184(72.6331) | Total Time 10.00(10.00)\n",
      "Iter 0033 | Time 29.2707(39.5123) | Bit/dim 2.4956(10.7628) | Xent 0.9347(1.8626) | Loss 2.9630(11.6941) | Error 0.2576(0.5855) Steps 392(354.14) | Grad Norm 2.1458(70.5185) | Total Time 10.00(10.00)\n",
      "Iter 0034 | Time 29.0394(39.1981) | Bit/dim 2.5411(10.5162) | Xent 0.8427(1.8320) | Loss 2.9625(11.4322) | Error 0.2359(0.5750) Steps 386(355.09) | Grad Norm 3.5153(68.5084) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0005 | Time 8.4542, Epoch Time 226.2013(230.4116), Bit/dim 2.5509, Xent 0.7881, Loss 2.9450, Error 0.3799\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0035 | Time 26.6652(38.8222) | Bit/dim 2.5623(10.2775) | Xent 0.7799(1.8005) | Loss 2.9522(11.1778) | Error 0.2191(0.5644) Steps 386(356.02) | Grad Norm 5.0327(66.6041) | Total Time 10.00(10.00)\n",
      "Iter 0036 | Time 25.7076(38.4287) | Bit/dim 2.5339(10.0452) | Xent 0.7292(1.7683) | Loss 2.8985(10.9294) | Error 0.2027(0.5535) Steps 380(356.74) | Grad Norm 4.8301(64.7509) | Total Time 10.00(10.00)\n",
      "Iter 0037 | Time 26.3977(38.0678) | Bit/dim 2.4614(9.8177) | Xent 0.7178(1.7368) | Loss 2.8203(10.6861) | Error 0.1939(0.5427) Steps 386(357.62) | Grad Norm 3.1051(62.9016) | Total Time 10.00(10.00)\n",
      "Iter 0038 | Time 27.1550(37.7404) | Bit/dim 2.4000(9.5952) | Xent 0.7175(1.7062) | Loss 2.7587(10.4483) | Error 0.1897(0.5321) Steps 398(358.83) | Grad Norm 3.9617(61.1334) | Total Time 10.00(10.00)\n",
      "Iter 0039 | Time 26.7677(37.4112) | Bit/dim 2.3536(9.3779) | Xent 0.7547(1.6777) | Loss 2.7310(10.2168) | Error 0.1927(0.5220) Steps 386(359.64) | Grad Norm 3.7615(59.4122) | Total Time 10.00(10.00)\n",
      "Iter 0040 | Time 26.8256(37.0937) | Bit/dim 2.3062(9.1658) | Xent 0.8206(1.6520) | Loss 2.7165(9.9918) | Error 0.2093(0.5126) Steps 380(360.25) | Grad Norm 2.7070(57.7110) | Total Time 10.00(10.00)\n",
      "Iter 0041 | Time 28.7433(36.8431) | Bit/dim 2.2706(8.9589) | Xent 0.8635(1.6283) | Loss 2.7023(9.7731) | Error 0.2131(0.5036) Steps 380(360.85) | Grad Norm 4.1120(56.1031) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0006 | Time 8.3533, Epoch Time 208.9901(229.7689), Bit/dim 2.2378, Xent 0.8474, Loss 2.6614, Error 0.3566\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0042 | Time 26.2725(36.5260) | Bit/dim 2.2489(8.7576) | Xent 0.8228(1.6042) | Loss 2.6603(9.5597) | Error 0.1851(0.4940) Steps 380(361.42) | Grad Norm 3.3654(54.5209) | Total Time 10.00(10.00)\n",
      "Iter 0043 | Time 26.7276(36.2321) | Bit/dim 2.2395(8.5621) | Xent 0.7921(1.5798) | Loss 2.6356(9.3520) | Error 0.1905(0.4849) Steps 380(361.98) | Grad Norm 2.8647(52.9713) | Total Time 10.00(10.00)\n",
      "Iter 0044 | Time 26.2872(35.9337) | Bit/dim 2.2308(8.3721) | Xent 0.7318(1.5544) | Loss 2.5967(9.1493) | Error 0.1900(0.4761) Steps 374(362.34) | Grad Norm 2.5528(51.4587) | Total Time 10.00(10.00)\n",
      "Iter 0045 | Time 26.2757(35.6440) | Bit/dim 2.2252(8.1877) | Xent 0.6458(1.5271) | Loss 2.5481(8.9513) | Error 0.1735(0.4670) Steps 374(362.69) | Grad Norm 1.3676(49.9560) | Total Time 10.00(10.00)\n",
      "Iter 0046 | Time 25.9750(35.3539) | Bit/dim 2.2398(8.0093) | Xent 0.5997(1.4993) | Loss 2.5396(8.7589) | Error 0.1690(0.4581) Steps 374(363.03) | Grad Norm 2.5025(48.5324) | Total Time 10.00(10.00)\n",
      "Iter 0047 | Time 26.1175(35.0768) | Bit/dim 2.2423(7.8363) | Xent 0.5748(1.4715) | Loss 2.5297(8.5721) | Error 0.1689(0.4494) Steps 374(363.36) | Grad Norm 2.6341(47.1554) | Total Time 10.00(10.00)\n",
      "Iter 0048 | Time 25.9636(34.8034) | Bit/dim 2.2188(7.6678) | Xent 0.5760(1.4447) | Loss 2.5068(8.3901) | Error 0.1666(0.4409) Steps 380(363.86) | Grad Norm 2.2453(45.8081) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0007 | Time 8.6162, Epoch Time 204.7610(229.0187), Bit/dim 2.1811, Xent 0.5741, Loss 2.4682, Error 0.3322\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0049 | Time 26.1482(34.5438) | Bit/dim 2.1978(7.5037) | Xent 0.5697(1.4184) | Loss 2.4827(8.2129) | Error 0.1627(0.4326) Steps 386(364.52) | Grad Norm 2.3846(44.5054) | Total Time 10.00(10.00)\n",
      "Iter 0050 | Time 26.1000(34.2905) | Bit/dim 2.1553(7.3432) | Xent 0.5730(1.3931) | Loss 2.4418(8.0397) | Error 0.1545(0.4242) Steps 386(365.17) | Grad Norm 1.6427(43.2195) | Total Time 10.00(10.00)\n",
      "Iter 0051 | Time 26.6733(34.0619) | Bit/dim 2.1281(7.1868) | Xent 0.6101(1.3696) | Loss 2.4331(7.8715) | Error 0.1650(0.4164) Steps 368(365.25) | Grad Norm 2.3406(41.9932) | Total Time 10.00(10.00)\n",
      "Iter 0052 | Time 24.8073(33.7843) | Bit/dim 2.1083(7.0344) | Xent 0.6046(1.3466) | Loss 2.4106(7.7077) | Error 0.1538(0.4086) Steps 362(365.15) | Grad Norm 2.4002(40.8054) | Total Time 10.00(10.00)\n",
      "Iter 0053 | Time 24.3093(33.5001) | Bit/dim 2.1052(6.8865) | Xent 0.5965(1.3241) | Loss 2.4035(7.5486) | Error 0.1574(0.4010) Steps 350(364.70) | Grad Norm 2.7456(39.6636) | Total Time 10.00(10.00)\n",
      "Iter 0054 | Time 23.9435(33.2134) | Bit/dim 2.1000(6.7429) | Xent 0.5444(1.3007) | Loss 2.3721(7.3933) | Error 0.1522(0.3936) Steps 350(364.26) | Grad Norm 1.0487(38.5051) | Total Time 10.00(10.00)\n",
      "Iter 0055 | Time 24.1115(32.9403) | Bit/dim 2.0968(6.6035) | Xent 0.5060(1.2769) | Loss 2.3498(7.2420) | Error 0.1489(0.3862) Steps 356(364.01) | Grad Norm 2.3947(37.4218) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0008 | Time 8.1076, Epoch Time 196.6077(228.0463), Bit/dim 2.0781, Xent 0.5058, Loss 2.3310, Error 0.3181\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0056 | Time 24.0153(32.6726) | Bit/dim 2.0943(6.4683) | Xent 0.4948(1.2534) | Loss 2.3417(7.0950) | Error 0.1501(0.3791) Steps 356(363.77) | Grad Norm 2.5947(36.3770) | Total Time 10.00(10.00)\n",
      "Iter 0057 | Time 23.0029(32.3825) | Bit/dim 2.0667(6.3362) | Xent 0.5079(1.2311) | Loss 2.3207(6.9518) | Error 0.1460(0.3721) Steps 350(363.36) | Grad Norm 1.3416(35.3259) | Total Time 10.00(10.00)\n",
      "Iter 0058 | Time 24.0597(32.1328) | Bit/dim 2.0411(6.2074) | Xent 0.5147(1.2096) | Loss 2.2985(6.8122) | Error 0.1398(0.3652) Steps 350(362.96) | Grad Norm 2.3749(34.3374) | Total Time 10.00(10.00)\n",
      "Iter 0059 | Time 25.6265(31.9376) | Bit/dim 2.0326(6.0821) | Xent 0.5099(1.1886) | Loss 2.2875(6.6764) | Error 0.1454(0.3586) Steps 362(362.93) | Grad Norm 1.6112(33.3556) | Total Time 10.00(10.00)\n",
      "Iter 0060 | Time 24.2234(31.7062) | Bit/dim 2.0178(5.9602) | Xent 0.5180(1.1685) | Loss 2.2768(6.5444) | Error 0.1452(0.3522) Steps 350(362.54) | Grad Norm 2.5836(32.4325) | Total Time 10.00(10.00)\n",
      "Iter 0061 | Time 23.8791(31.4714) | Bit/dim 2.0155(5.8419) | Xent 0.4896(1.1481) | Loss 2.2603(6.4159) | Error 0.1375(0.3457) Steps 350(362.16) | Grad Norm 0.8592(31.4853) | Total Time 10.00(10.00)\n",
      "Iter 0062 | Time 24.5398(31.2634) | Bit/dim 2.0111(5.7269) | Xent 0.4665(1.1276) | Loss 2.2444(6.2908) | Error 0.1369(0.3395) Steps 356(361.98) | Grad Norm 1.8376(30.5958) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0009 | Time 7.6240, Epoch Time 189.7322(226.8969), Bit/dim 1.9955, Xent 0.4567, Loss 2.2238, Error 0.3080\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0063 | Time 25.2946(31.0843) | Bit/dim 2.0055(5.6153) | Xent 0.4438(1.1071) | Loss 2.2274(6.1689) | Error 0.1328(0.3333) Steps 350(361.62) | Grad Norm 2.3479(29.7484) | Total Time 10.00(10.00)\n",
      "Iter 0064 | Time 23.6256(30.8606) | Bit/dim 1.9893(5.5065) | Xent 0.4490(1.0874) | Loss 2.2138(6.0502) | Error 0.1319(0.3272) Steps 350(361.27) | Grad Norm 1.9101(28.9132) | Total Time 10.00(10.00)\n",
      "Iter 0065 | Time 23.9468(30.6532) | Bit/dim 1.9676(5.4003) | Xent 0.4547(1.0684) | Loss 2.1950(5.9345) | Error 0.1265(0.3212) Steps 350(360.93) | Grad Norm 2.2297(28.1127) | Total Time 10.00(10.00)\n",
      "Iter 0066 | Time 23.9090(30.4508) | Bit/dim 1.9633(5.2972) | Xent 0.4732(1.0506) | Loss 2.1999(5.8225) | Error 0.1322(0.3155) Steps 350(360.60) | Grad Norm 1.5570(27.3161) | Total Time 10.00(10.00)\n",
      "Iter 0067 | Time 23.8371(30.2524) | Bit/dim 1.9519(5.1969) | Xent 0.4529(1.0326) | Loss 2.1784(5.7132) | Error 0.1358(0.3101) Steps 344(360.11) | Grad Norm 1.8972(26.5535) | Total Time 10.00(10.00)\n",
      "Iter 0068 | Time 25.6309(30.1138) | Bit/dim 1.9523(5.0995) | Xent 0.4376(1.0148) | Loss 2.1710(5.6069) | Error 0.1286(0.3047) Steps 356(359.98) | Grad Norm 1.2231(25.7936) | Total Time 10.00(10.00)\n",
      "Iter 0069 | Time 25.4918(29.9751) | Bit/dim 1.9373(5.0047) | Xent 0.4348(0.9974) | Loss 2.1547(5.5034) | Error 0.1311(0.2995) Steps 356(359.86) | Grad Norm 1.5798(25.0672) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0010 | Time 8.1240, Epoch Time 192.5678(225.8670), Bit/dim 1.9141, Xent 0.4412, Loss 2.1347, Error 0.3087\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0070 | Time 25.9054(29.8530) | Bit/dim 1.9271(4.9123) | Xent 0.4212(0.9801) | Loss 2.1377(5.4024) | Error 0.1219(0.2942) Steps 362(359.93) | Grad Norm 1.1228(24.3488) | Total Time 10.00(10.00)\n",
      "Iter 0071 | Time 25.7592(29.7302) | Bit/dim 1.9139(4.8224) | Xent 0.4356(0.9637) | Loss 2.1317(5.3043) | Error 0.1229(0.2890) Steps 362(359.99) | Grad Norm 1.2254(23.6551) | Total Time 10.00(10.00)\n",
      "Iter 0072 | Time 25.9534(29.6169) | Bit/dim 1.9106(4.7350) | Xent 0.4224(0.9475) | Loss 2.1218(5.2088) | Error 0.1220(0.2840) Steps 362(360.05) | Grad Norm 2.0220(23.0061) | Total Time 10.00(10.00)\n",
      "Iter 0073 | Time 26.2418(29.5157) | Bit/dim 1.9217(4.6506) | Xent 0.4116(0.9314) | Loss 2.1275(5.1163) | Error 0.1221(0.2792) Steps 368(360.29) | Grad Norm 4.1914(22.4417) | Total Time 10.00(10.00)\n",
      "Iter 0074 | Time 25.1507(29.3847) | Bit/dim 1.9063(4.5683) | Xent 0.4703(0.9176) | Loss 2.1415(5.0271) | Error 0.1408(0.2750) Steps 356(360.16) | Grad Norm 7.9144(22.0059) | Total Time 10.00(10.00)\n",
      "Iter 0075 | Time 27.8074(29.3374) | Bit/dim 1.9728(4.4904) | Xent 0.5571(0.9068) | Loss 2.2514(4.9438) | Error 0.1641(0.2717) Steps 368(360.39) | Grad Norm 16.5260(21.8415) | Total Time 10.00(10.00)\n",
      "Iter 0076 | Time 25.8901(29.2340) | Bit/dim 2.0032(4.4158) | Xent 0.8063(0.9038) | Loss 2.4063(4.8677) | Error 0.2660(0.2715) Steps 374(360.80) | Grad Norm 22.7732(21.8694) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0011 | Time 7.8301, Epoch Time 202.7950(225.1749), Bit/dim 2.0317, Xent 0.5121, Loss 2.2878, Error 0.3186\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0077 | Time 27.1303(29.1709) | Bit/dim 2.0424(4.3446) | Xent 0.5171(0.8922) | Loss 2.3010(4.7907) | Error 0.1490(0.2678) Steps 380(361.38) | Grad Norm 14.4982(21.6483) | Total Time 10.00(10.00)\n",
      "Iter 0078 | Time 25.6773(29.0661) | Bit/dim 2.0372(4.2754) | Xent 0.4947(0.8802) | Loss 2.2846(4.7155) | Error 0.1440(0.2641) Steps 374(361.76) | Grad Norm 9.8041(21.2930) | Total Time 10.00(10.00)\n",
      "Iter 0079 | Time 27.1567(29.0088) | Bit/dim 1.9130(4.2045) | Xent 0.5891(0.8715) | Loss 2.2075(4.6403) | Error 0.1843(0.2617) Steps 374(362.12) | Grad Norm 9.4171(20.9367) | Total Time 10.00(10.00)\n",
      "Iter 0080 | Time 26.3499(28.9290) | Bit/dim 1.9159(4.1359) | Xent 0.5279(0.8612) | Loss 2.1798(4.5665) | Error 0.1511(0.2584) Steps 374(362.48) | Grad Norm 4.5646(20.4455) | Total Time 10.00(10.00)\n",
      "Iter 0081 | Time 24.6743(28.8014) | Bit/dim 1.9349(4.0698) | Xent 0.5693(0.8524) | Loss 2.2196(4.4961) | Error 0.1780(0.2560) Steps 356(362.29) | Grad Norm 10.4617(20.1460) | Total Time 10.00(10.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p train_cnf_conditional_tol.py --data mnist --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 8000 --test_batch_size 8000 --save experiments/cnf_cond_bs8K_ilr_0_01_tol_1e_m4_wy_0_5 --conditional True --lr 0.01 --warmup_iters 113 --atol 1e-4  --rtol 1e-4 --weight_y 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
