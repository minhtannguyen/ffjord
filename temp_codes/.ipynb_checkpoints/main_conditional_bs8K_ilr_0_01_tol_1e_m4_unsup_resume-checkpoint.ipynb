{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_conditional_tol_unsup.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.odenvp_conditional_tol as odenvp\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=True, choices=[True, False])\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"./data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"./data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"./data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"./data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"./data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"./data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            './data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            './data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz = modules.GaussianDiag.logp(mean, logs, z).view(-1,1)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(z)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol},)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    # fixed_y = torch.randint(high=10, size=(100,)).type(torch.long).to(device, non_blocking=True)\n",
      "    fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "    fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "    with torch.no_grad():\n",
      "        mean, logs = model.module._prior(fixed_y_onehot)\n",
      "        fixed_z = modules.GaussianDiag.sample(mean, logs)\n",
      "    \n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    if args.conditional: best_error_score = float(\"inf\")\n",
      "    \n",
      "    itr = 0\n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                loss = loss_nll\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            loss.backward()\n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            xent_meter.update(loss_xent.item())\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits/dim', {'train': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        loss = loss_nll\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses_xent.append(loss_xent.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'validation': time.time() - start}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits/dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}, Xent {:.4f}, Loss {:.4f}, Error {:.4f}\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, loss_xent, loss, error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=0.0001, autoencode=False, batch_norm=False, batch_size=8000, batch_size_schedule='', begin_epoch=1, conditional=True, conv=True, data='mnist', dims='64,64,64', divergence_fn='approximate', dl2int=None, imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=1, lr=0.01, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume=None, rtol=0.0001, save='experiments/cnf_cond_bs8K_ilr_0_01_tol_1e_m4_unsup', solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=8000, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, val_freq=1, warmup_iters=113.0, weight_decay=0.0, weight_y=0.5)\n",
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=1568, bias=True)\n",
      "  (project_class): LinearZeros(in_features=784, out_features=10, bias=True)\n",
      ")\n",
      "Number of trainable parameters: 828890\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc.py:286: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "Iter 0000 | Time 58.5542(58.5542) | Bit/dim 32.7422(32.7422) | Xent 2.3026(2.3026) | Loss 32.7422(32.7422) | Error 0.8975(0.8975) Steps 290(290.00) | Grad Norm 265.8103(265.8103) | Total Time 10.00(10.00)\n",
      "Iter 0001 | Time 20.4161(57.4101) | Bit/dim 27.9422(32.5982) | Xent 2.3026(2.3026) | Loss 27.9422(32.5982) | Error 0.9055(0.8977) Steps 290(290.00) | Grad Norm 227.5963(264.6639) | Total Time 10.00(10.00)\n",
      "Iter 0002 | Time 19.3633(56.2687) | Bit/dim 20.6568(32.2400) | Xent 2.3026(2.3026) | Loss 20.6568(32.2400) | Error 0.9045(0.8979) Steps 290(290.00) | Grad Norm 162.3289(261.5938) | Total Time 10.00(10.00)\n",
      "Iter 0003 | Time 21.6786(55.2310) | Bit/dim 14.4202(31.7054) | Xent 2.3026(2.3026) | Loss 14.4202(31.7054) | Error 0.9005(0.8980) Steps 320(290.90) | Grad Norm 84.8754(256.2923) | Total Time 10.00(10.00)\n",
      "Iter 0004 | Time 27.6354(54.4031) | Bit/dim 12.8282(31.1390) | Xent 2.3026(2.3026) | Loss 12.8282(31.1390) | Error 0.9047(0.8982) Steps 410(294.47) | Grad Norm 42.5664(249.8805) | Total Time 10.00(10.00)\n",
      "Iter 0005 | Time 26.7831(53.5745) | Bit/dim 15.5782(30.6722) | Xent 2.3026(2.3026) | Loss 15.5782(30.6722) | Error 0.8994(0.8983) Steps 410(297.94) | Grad Norm 110.0870(245.6867) | Total Time 10.00(10.00)\n",
      "Iter 0006 | Time 26.8982(52.7742) | Bit/dim 16.9987(30.2620) | Xent 2.3026(2.3026) | Loss 16.9987(30.2620) | Error 0.8986(0.8983) Steps 410(301.30) | Grad Norm 134.0506(242.3376) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0001 | Time 8.8367, Epoch Time 222.8336(222.8336), Bit/dim 14.9491, Xent 2.3026, Loss 14.9491, Error 0.9227\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0007 | Time 28.8681(52.0570) | Bit/dim 14.9701(29.8033) | Xent 2.3026(2.3026) | Loss 14.9701(29.8033) | Error 0.9051(0.8985) Steps 410(304.56) | Grad Norm 112.5406(238.4437) | Total Time 10.00(10.00)\n",
      "Iter 0008 | Time 26.8804(51.3017) | Bit/dim 11.7427(29.2614) | Xent 2.3026(2.3026) | Loss 11.7427(29.2614) | Error 0.9012(0.8986) Steps 410(307.72) | Grad Norm 77.3584(233.6111) | Total Time 10.00(10.00)\n",
      "Iter 0009 | Time 27.6812(50.5931) | Bit/dim 9.1473(28.6580) | Xent 2.3026(2.3026) | Loss 9.1473(28.6580) | Error 0.9034(0.8987) Steps 398(310.43) | Grad Norm 44.8620(227.9487) | Total Time 10.00(10.00)\n",
      "Iter 0010 | Time 26.0998(49.8583) | Bit/dim 7.7508(28.0308) | Xent 2.3026(2.3026) | Loss 7.7508(28.0308) | Error 0.9027(0.8988) Steps 362(311.98) | Grad Norm 22.7394(221.7924) | Total Time 10.00(10.00)\n",
      "Iter 0011 | Time 23.5547(49.0692) | Bit/dim 7.3638(27.4108) | Xent 2.3026(2.3026) | Loss 7.3638(27.4108) | Error 0.9029(0.8989) Steps 338(312.76) | Grad Norm 20.9524(215.7672) | Total Time 10.00(10.00)\n",
      "Iter 0012 | Time 23.1342(48.2912) | Bit/dim 7.4927(26.8133) | Xent 2.3026(2.3026) | Loss 7.4927(26.8133) | Error 0.8995(0.8990) Steps 338(313.52) | Grad Norm 29.2745(210.1724) | Total Time 10.00(10.00)\n",
      "Iter 0013 | Time 23.7607(47.5552) | Bit/dim 7.4846(26.2334) | Xent 2.3026(2.3026) | Loss 7.4846(26.2334) | Error 0.8960(0.8989) Steps 338(314.25) | Grad Norm 34.1919(204.8930) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0002 | Time 7.7034, Epoch Time 200.0514(222.1501), Bit/dim 7.0317, Xent 2.3026, Loss 7.0317, Error 0.9227\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0014 | Time 23.2093(46.8249) | Bit/dim 7.0549(25.6580) | Xent 2.3026(2.3026) | Loss 7.0549(25.6580) | Error 0.9040(0.8990) Steps 338(314.96) | Grad Norm 33.7349(199.7582) | Total Time 10.00(10.00)\n",
      "Iter 0015 | Time 24.8636(46.1660) | Bit/dim 6.2114(25.0746) | Xent 2.3026(2.3026) | Loss 6.2114(25.0746) | Error 0.9006(0.8991) Steps 338(315.66) | Grad Norm 29.2083(194.6417) | Total Time 10.00(10.00)\n",
      "Iter 0016 | Time 23.8399(45.4962) | Bit/dim 5.2117(24.4787) | Xent 2.3026(2.3026) | Loss 5.2117(24.4787) | Error 0.9012(0.8991) Steps 338(316.33) | Grad Norm 22.4660(189.4765) | Total Time 10.00(10.00)\n",
      "Iter 0017 | Time 24.9325(44.8793) | Bit/dim 4.3003(23.8734) | Xent 2.3026(2.3026) | Loss 4.3003(23.8734) | Error 0.9010(0.8992) Steps 362(317.70) | Grad Norm 15.4239(184.2549) | Total Time 10.00(10.00)\n",
      "Iter 0018 | Time 25.8559(44.3086) | Bit/dim 3.6569(23.2669) | Xent 2.3026(2.3026) | Loss 3.6569(23.2669) | Error 0.8984(0.8992) Steps 380(319.57) | Grad Norm 9.7308(179.0192) | Total Time 10.00(10.00)\n",
      "Iter 0019 | Time 25.7060(43.7506) | Bit/dim 3.2391(22.6661) | Xent 2.3026(2.3026) | Loss 3.2391(22.6661) | Error 0.9009(0.8992) Steps 380(321.38) | Grad Norm 6.4817(173.8430) | Total Time 10.00(10.00)\n",
      "Iter 0020 | Time 24.8728(43.1842) | Bit/dim 3.0354(22.0771) | Xent 2.3026(2.3026) | Loss 3.0354(22.0771) | Error 0.9016(0.8993) Steps 374(322.96) | Grad Norm 5.6076(168.7960) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0003 | Time 8.3888, Epoch Time 194.0545(221.3073), Bit/dim 2.9107, Xent 2.3026, Loss 2.9107, Error 0.9227\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0021 | Time 26.2738(42.6769) | Bit/dim 2.9334(21.5028) | Xent 2.3026(2.3026) | Loss 2.9334(21.5028) | Error 0.9015(0.8994) Steps 374(324.49) | Grad Norm 5.5512(163.8986) | Total Time 10.00(10.00)\n",
      "Iter 0022 | Time 25.9080(42.1738) | Bit/dim 2.8812(20.9442) | Xent 2.3026(2.3026) | Loss 2.8812(20.9442) | Error 0.8981(0.8993) Steps 380(326.15) | Grad Norm 5.5281(159.1475) | Total Time 10.00(10.00)\n",
      "Iter 0023 | Time 25.1147(41.6621) | Bit/dim 2.8295(20.4007) | Xent 2.3026(2.3026) | Loss 2.8295(20.4007) | Error 0.9004(0.8994) Steps 374(327.59) | Grad Norm 5.4206(154.5357) | Total Time 10.00(10.00)\n",
      "Iter 0024 | Time 28.1820(41.2577) | Bit/dim 2.7886(19.8724) | Xent 2.3026(2.3026) | Loss 2.7886(19.8724) | Error 0.9084(0.8996) Steps 392(329.52) | Grad Norm 5.1846(150.0552) | Total Time 10.00(10.00)\n",
      "Iter 0025 | Time 29.1569(40.8946) | Bit/dim 2.7657(19.3592) | Xent 2.3026(2.3026) | Loss 2.7657(19.3592) | Error 0.8991(0.8996) Steps 404(331.76) | Grad Norm 4.8473(145.6989) | Total Time 10.00(10.00)\n",
      "Iter 0026 | Time 30.0889(40.5705) | Bit/dim 2.7542(18.8610) | Xent 2.3026(2.3026) | Loss 2.7542(18.8610) | Error 0.9036(0.8997) Steps 404(333.92) | Grad Norm 4.5604(141.4648) | Total Time 10.00(10.00)\n",
      "Iter 0027 | Time 30.1662(40.2583) | Bit/dim 2.7782(18.3785) | Xent 2.3026(2.3026) | Loss 2.7782(18.3785) | Error 0.9009(0.8998) Steps 404(336.03) | Grad Norm 4.4421(137.3541) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0004 | Time 9.4633, Epoch Time 216.7836(221.1715), Bit/dim 2.7909, Xent 2.3026, Loss 2.7909, Error 0.9227\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0028 | Time 30.4003(39.9626) | Bit/dim 2.8050(17.9113) | Xent 2.3026(2.3026) | Loss 2.8050(17.9113) | Error 0.9012(0.8998) Steps 422(338.60) | Grad Norm 4.4398(133.3667) | Total Time 10.00(10.00)\n",
      "Iter 0029 | Time 30.1547(39.6684) | Bit/dim 2.8361(17.4591) | Xent 2.3026(2.3026) | Loss 2.8361(17.4591) | Error 0.9042(0.8999) Steps 422(341.11) | Grad Norm 4.4937(129.5005) | Total Time 10.00(10.00)\n",
      "Iter 0030 | Time 32.6848(39.4589) | Bit/dim 2.8572(17.0210) | Xent 2.3026(2.3026) | Loss 2.8572(17.0210) | Error 0.9034(0.9000) Steps 428(343.71) | Grad Norm 4.5380(125.7516) | Total Time 10.00(10.00)\n",
      "Iter 0031 | Time 31.9203(39.2327) | Bit/dim 2.8636(16.5963) | Xent 2.3026(2.3026) | Loss 2.8636(16.5963) | Error 0.8972(0.9000) Steps 440(346.60) | Grad Norm 4.5404(122.1153) | Total Time 10.00(10.00)\n",
      "Iter 0032 | Time 31.3921(38.9975) | Bit/dim 2.8309(16.1833) | Xent 2.3026(2.3026) | Loss 2.8309(16.1833) | Error 0.9036(0.9001) Steps 428(349.04) | Grad Norm 4.4736(118.5860) | Total Time 10.00(10.00)\n",
      "Iter 0033 | Time 31.9574(38.7863) | Bit/dim 2.7856(15.7814) | Xent 2.3026(2.3026) | Loss 2.7856(15.7814) | Error 0.8966(0.9000) Steps 440(351.77) | Grad Norm 4.3706(115.1596) | Total Time 10.00(10.00)\n",
      "Iter 0034 | Time 29.9045(38.5198) | Bit/dim 2.7039(15.3891) | Xent 2.3026(2.3026) | Loss 2.7039(15.3891) | Error 0.9020(0.9000) Steps 434(354.24) | Grad Norm 4.1606(111.8296) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0005 | Time 9.8522, Epoch Time 240.6960(221.7573), Bit/dim 2.6135, Xent 2.3026, Loss 2.6135, Error 0.9227\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0035 | Time 29.0801(38.2366) | Bit/dim 2.6263(15.0062) | Xent 2.3026(2.3026) | Loss 2.6263(15.0062) | Error 0.8996(0.9000) Steps 428(356.45) | Grad Norm 3.8615(108.5906) | Total Time 10.00(10.00)\n",
      "Iter 0036 | Time 29.4329(37.9725) | Bit/dim 2.5462(14.6324) | Xent 2.3026(2.3026) | Loss 2.5462(14.6324) | Error 0.8945(0.8999) Steps 428(358.60) | Grad Norm 3.4803(105.4372) | Total Time 10.00(10.00)\n",
      "Iter 0037 | Time 30.5737(37.7506) | Bit/dim 2.4629(14.2673) | Xent 2.3026(2.3026) | Loss 2.4629(14.2673) | Error 0.9034(0.9000) Steps 428(360.68) | Grad Norm 2.9940(102.3639) | Total Time 10.00(10.00)\n",
      "Iter 0038 | Time 30.0092(37.5183) | Bit/dim 2.3913(13.9110) | Xent 2.3026(2.3026) | Loss 2.3913(13.9110) | Error 0.9041(0.9001) Steps 410(362.16) | Grad Norm 2.5079(99.3683) | Total Time 10.00(10.00)\n",
      "Iter 0039 | Time 30.0477(37.2942) | Bit/dim 2.3287(13.5636) | Xent 2.3026(2.3026) | Loss 2.3287(13.5636) | Error 0.8998(0.9001) Steps 434(364.32) | Grad Norm 2.1048(96.4504) | Total Time 10.00(10.00)\n",
      "Iter 0040 | Time 31.7228(37.1271) | Bit/dim 2.3013(13.2257) | Xent 2.3026(2.3026) | Loss 2.3013(13.2257) | Error 0.9025(0.9001) Steps 416(365.87) | Grad Norm 1.9334(93.6149) | Total Time 10.00(10.00)\n",
      "Iter 0041 | Time 29.3173(36.8928) | Bit/dim 2.2888(12.8976) | Xent 2.3026(2.3026) | Loss 2.2888(12.8976) | Error 0.9019(0.9002) Steps 416(367.37) | Grad Norm 2.3805(90.8778) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0006 | Time 9.1427, Epoch Time 231.4924(222.0493), Bit/dim 2.2751, Xent 2.3026, Loss 2.2751, Error 0.9227\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0042 | Time 29.4029(36.6681) | Bit/dim 2.2809(12.5791) | Xent 2.3026(2.3026) | Loss 2.2809(12.5791) | Error 0.8986(0.9001) Steps 410(368.65) | Grad Norm 3.2628(88.2494) | Total Time 10.00(10.00)\n",
      "Iter 0043 | Time 28.6144(36.4265) | Bit/dim 2.2801(12.2701) | Xent 2.3026(2.3026) | Loss 2.2801(12.2701) | Error 0.9018(0.9002) Steps 410(369.89) | Grad Norm 3.5931(85.7097) | Total Time 10.00(10.00)\n",
      "Iter 0044 | Time 27.7811(36.1671) | Bit/dim 2.2670(11.9700) | Xent 2.3026(2.3026) | Loss 2.2670(11.9700) | Error 0.8995(0.9002) Steps 410(371.09) | Grad Norm 2.8534(83.2240) | Total Time 10.00(10.00)\n",
      "Iter 0045 | Time 27.8339(35.9171) | Bit/dim 2.2419(11.6782) | Xent 2.3026(2.3026) | Loss 2.2419(11.6782) | Error 0.9060(0.9004) Steps 410(372.26) | Grad Norm 2.6249(80.8060) | Total Time 10.00(10.00)\n",
      "Iter 0046 | Time 27.8572(35.6753) | Bit/dim 2.2195(11.3944) | Xent 2.3026(2.3026) | Loss 2.2195(11.3944) | Error 0.9011(0.9004) Steps 410(373.39) | Grad Norm 2.9275(78.4697) | Total Time 10.00(10.00)\n",
      "Iter 0047 | Time 27.9737(35.4443) | Bit/dim 2.1942(11.1184) | Xent 2.3026(2.3026) | Loss 2.1942(11.1184) | Error 0.9024(0.9004) Steps 416(374.67) | Grad Norm 2.8843(76.2021) | Total Time 10.00(10.00)\n",
      "Iter 0048 | Time 27.0037(35.1910) | Bit/dim 2.1644(10.8498) | Xent 2.3026(2.3026) | Loss 2.1644(10.8498) | Error 0.9000(0.9004) Steps 392(375.19) | Grad Norm 2.1940(73.9819) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0007 | Time 8.9012, Epoch Time 217.8561(221.9235), Bit/dim 2.1314, Xent 2.3026, Loss 2.1314, Error 0.9227\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0049 | Time 26.2568(34.9230) | Bit/dim 2.1439(10.5886) | Xent 2.3026(2.3026) | Loss 2.1439(10.5886) | Error 0.9027(0.9005) Steps 392(375.69) | Grad Norm 1.2629(71.8003) | Total Time 10.00(10.00)\n",
      "Iter 0050 | Time 26.6568(34.6750) | Bit/dim 2.1397(10.3352) | Xent 2.3026(2.3026) | Loss 2.1397(10.3352) | Error 0.8987(0.9004) Steps 392(376.18) | Grad Norm 1.3287(69.6861) | Total Time 10.00(10.00)\n",
      "Iter 0051 | Time 26.9759(34.4441) | Bit/dim 2.1314(10.0890) | Xent 2.3026(2.3026) | Loss 2.1314(10.0890) | Error 0.8970(0.9003) Steps 392(376.66) | Grad Norm 2.1454(67.6599) | Total Time 10.00(10.00)\n",
      "Iter 0052 | Time 26.0866(34.1933) | Bit/dim 2.1271(9.8502) | Xent 2.3026(2.3026) | Loss 2.1271(9.8502) | Error 0.9001(0.9003) Steps 386(376.94) | Grad Norm 2.3449(65.7005) | Total Time 10.00(10.00)\n",
      "Iter 0053 | Time 26.2063(33.9537) | Bit/dim 2.1039(9.6178) | Xent 2.3026(2.3026) | Loss 2.1039(9.6178) | Error 0.9046(0.9005) Steps 380(377.03) | Grad Norm 1.8786(63.7858) | Total Time 10.00(10.00)\n",
      "Iter 0054 | Time 26.1773(33.7204) | Bit/dim 2.0826(9.3917) | Xent 2.3026(2.3026) | Loss 2.0826(9.3917) | Error 0.9021(0.9005) Steps 374(376.94) | Grad Norm 1.3809(61.9137) | Total Time 10.00(10.00)\n",
      "Iter 0055 | Time 26.1078(33.4920) | Bit/dim 2.0636(9.1719) | Xent 2.3026(2.3026) | Loss 2.0636(9.1719) | Error 0.9047(0.9006) Steps 374(376.85) | Grad Norm 1.6206(60.1049) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0008 | Time 8.4379, Epoch Time 205.3534(221.4264), Bit/dim 2.0394, Xent 2.3026, Loss 2.0394, Error 0.9227\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0056 | Time 27.3810(33.3087) | Bit/dim 2.0496(8.9582) | Xent 2.3026(2.3026) | Loss 2.0496(8.9582) | Error 0.8968(0.9005) Steps 380(376.95) | Grad Norm 2.1783(58.3671) | Total Time 10.00(10.00)\n",
      "Iter 0057 | Time 26.4330(33.1024) | Bit/dim 2.0410(8.7507) | Xent 2.3026(2.3026) | Loss 2.0410(8.7507) | Error 0.9087(0.9008) Steps 380(377.04) | Grad Norm 2.3510(56.6866) | Total Time 10.00(10.00)\n",
      "Iter 0058 | Time 26.9568(32.9181) | Bit/dim 2.0301(8.5491) | Xent 2.3026(2.3026) | Loss 2.0301(8.5491) | Error 0.8979(0.9007) Steps 386(377.31) | Grad Norm 1.8909(55.0427) | Total Time 10.00(10.00)\n",
      "Iter 0059 | Time 26.8737(32.7367) | Bit/dim 2.0188(8.3532) | Xent 2.3026(2.3026) | Loss 2.0188(8.3532) | Error 0.8986(0.9006) Steps 386(377.57) | Grad Norm 1.2030(53.4275) | Total Time 10.00(10.00)\n",
      "Iter 0060 | Time 26.2228(32.5413) | Bit/dim 2.0060(8.1628) | Xent 2.3026(2.3026) | Loss 2.0060(8.1628) | Error 0.9058(0.9008) Steps 386(377.82) | Grad Norm 1.4007(51.8667) | Total Time 10.00(10.00)\n",
      "Iter 0061 | Time 26.8202(32.3697) | Bit/dim 1.9995(7.9779) | Xent 2.3026(2.3026) | Loss 1.9995(7.9779) | Error 0.8995(0.9007) Steps 392(378.25) | Grad Norm 1.7376(50.3629) | Total Time 10.00(10.00)\n",
      "Iter 0062 | Time 26.9152(32.2061) | Bit/dim 1.9909(7.7983) | Xent 2.3026(2.3026) | Loss 1.9909(7.7983) | Error 0.9039(0.9008) Steps 392(378.66) | Grad Norm 1.5355(48.8980) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0009 | Time 8.7065, Epoch Time 208.8276(221.0485), Bit/dim 1.9658, Xent 2.3026, Loss 1.9658, Error 0.9227\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0063 | Time 27.1328(32.0539) | Bit/dim 1.9824(7.6238) | Xent 2.3026(2.3026) | Loss 1.9824(7.6238) | Error 0.9054(0.9010) Steps 392(379.06) | Grad Norm 0.9897(47.4608) | Total Time 10.00(10.00)\n",
      "Iter 0064 | Time 27.6393(31.9214) | Bit/dim 1.9782(7.4544) | Xent 2.3026(2.3026) | Loss 1.9782(7.4544) | Error 0.9019(0.9010) Steps 398(379.63) | Grad Norm 1.0404(46.0682) | Total Time 10.00(10.00)\n",
      "Iter 0065 | Time 27.0091(31.7741) | Bit/dim 1.9722(7.2900) | Xent 2.3026(2.3026) | Loss 1.9722(7.2900) | Error 0.8984(0.9009) Steps 392(380.00) | Grad Norm 1.2595(44.7239) | Total Time 10.00(10.00)\n",
      "Iter 0066 | Time 29.0072(31.6910) | Bit/dim 1.9629(7.1301) | Xent 2.3026(2.3026) | Loss 1.9629(7.1301) | Error 0.8948(0.9007) Steps 386(380.18) | Grad Norm 0.9624(43.4111) | Total Time 10.00(10.00)\n",
      "Iter 0067 | Time 27.1723(31.5555) | Bit/dim 1.9492(6.9747) | Xent 2.3026(2.3026) | Loss 1.9492(6.9747) | Error 0.9009(0.9007) Steps 392(380.53) | Grad Norm 0.8166(42.1332) | Total Time 10.00(10.00)\n",
      "Iter 0068 | Time 29.2414(31.4861) | Bit/dim 1.9455(6.8238) | Xent 2.3026(2.3026) | Loss 1.9455(6.8238) | Error 0.9078(0.9009) Steps 398(381.06) | Grad Norm 0.7683(40.8923) | Total Time 10.00(10.00)\n",
      "Iter 0069 | Time 27.2127(31.3579) | Bit/dim 1.9411(6.6774) | Xent 2.3026(2.3026) | Loss 1.9411(6.6774) | Error 0.9004(0.9009) Steps 380(381.02) | Grad Norm 1.0720(39.6977) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0010 | Time 8.3045, Epoch Time 215.0904(220.8697), Bit/dim 1.9218, Xent 2.3026, Loss 1.9218, Error 0.9227\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0070 | Time 28.5427(31.2734) | Bit/dim 1.9436(6.5353) | Xent 2.3026(2.3026) | Loss 1.9436(6.5353) | Error 0.9042(0.9010) Steps 386(381.17) | Grad Norm 0.7814(38.5302) | Total Time 10.00(10.00)\n",
      "Iter 0071 | Time 27.1406(31.1494) | Bit/dim 1.9363(6.3974) | Xent 2.3026(2.3026) | Loss 1.9363(6.3974) | Error 0.8980(0.9009) Steps 386(381.32) | Grad Norm 1.2733(37.4125) | Total Time 10.00(10.00)\n",
      "Iter 0072 | Time 26.7913(31.0187) | Bit/dim 1.9223(6.2631) | Xent 2.3026(2.3026) | Loss 1.9223(6.2631) | Error 0.9046(0.9010) Steps 386(381.46) | Grad Norm 1.0855(36.3227) | Total Time 10.00(10.00)\n",
      "Iter 0073 | Time 28.2270(30.9349) | Bit/dim 1.9200(6.1328) | Xent 2.3026(2.3026) | Loss 1.9200(6.1328) | Error 0.9065(0.9012) Steps 374(381.24) | Grad Norm 0.7635(35.2559) | Total Time 10.00(10.00)\n",
      "Iter 0074 | Time 26.4338(30.7999) | Bit/dim 1.9137(6.0063) | Xent 2.3026(2.3026) | Loss 1.9137(6.0063) | Error 0.8944(0.9010) Steps 374(381.02) | Grad Norm 1.3387(34.2384) | Total Time 10.00(10.00)\n",
      "Iter 0075 | Time 26.8816(30.6823) | Bit/dim 1.9122(5.8834) | Xent 2.3026(2.3026) | Loss 1.9122(5.8834) | Error 0.9025(0.9010) Steps 374(380.81) | Grad Norm 0.5391(33.2274) | Total Time 10.00(10.00)\n",
      "Iter 0076 | Time 26.7957(30.5657) | Bit/dim 1.9050(5.7641) | Xent 2.3026(2.3026) | Loss 1.9050(5.7641) | Error 0.8990(0.9010) Steps 374(380.60) | Grad Norm 1.3814(32.2720) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0011 | Time 7.9714, Epoch Time 211.2139(220.5800), Bit/dim 1.8854, Xent 2.3026, Loss 1.8854, Error 0.9227\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0077 | Time 26.6823(30.4492) | Bit/dim 1.8989(5.6481) | Xent 2.3026(2.3026) | Loss 1.8989(5.6481) | Error 0.9011(0.9010) Steps 374(380.41) | Grad Norm 0.9506(31.3324) | Total Time 10.00(10.00)\n",
      "Iter 0078 | Time 27.6563(30.3655) | Bit/dim 1.8908(5.5354) | Xent 2.3026(2.3026) | Loss 1.8908(5.5354) | Error 0.9021(0.9010) Steps 362(379.85) | Grad Norm 1.1692(30.4275) | Total Time 10.00(10.00)\n",
      "Iter 0079 | Time 25.6363(30.2236) | Bit/dim 1.8854(5.4259) | Xent 2.3026(2.3026) | Loss 1.8854(5.4259) | Error 0.9020(0.9011) Steps 362(379.32) | Grad Norm 0.7611(29.5375) | Total Time 10.00(10.00)\n",
      "Iter 0080 | Time 26.9910(30.1266) | Bit/dim 1.8850(5.3197) | Xent 2.3026(2.3026) | Loss 1.8850(5.3197) | Error 0.9062(0.9012) Steps 362(378.80) | Grad Norm 0.9175(28.6789) | Total Time 10.00(10.00)\n",
      "Iter 0081 | Time 27.4321(30.0458) | Bit/dim 1.8776(5.2164) | Xent 2.3026(2.3026) | Loss 1.8776(5.2164) | Error 0.9018(0.9012) Steps 362(378.29) | Grad Norm 0.7727(27.8417) | Total Time 10.00(10.00)\n",
      "Iter 0082 | Time 26.3866(29.9360) | Bit/dim 1.8760(5.1162) | Xent 2.3026(2.3026) | Loss 1.8760(5.1162) | Error 0.8956(0.9011) Steps 368(377.99) | Grad Norm 1.3164(27.0460) | Total Time 10.00(10.00)\n",
      "Iter 0083 | Time 26.1260(29.8217) | Bit/dim 1.8661(5.0187) | Xent 2.3026(2.3026) | Loss 1.8661(5.0187) | Error 0.9006(0.9010) Steps 368(377.69) | Grad Norm 0.9140(26.2620) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0012 | Time 8.2031, Epoch Time 207.3411(220.1829), Bit/dim 1.8449, Xent 2.3026, Loss 1.8449, Error 0.9227\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0084 | Time 26.0232(29.7077) | Bit/dim 1.8612(4.9240) | Xent 2.3026(2.3026) | Loss 1.8612(4.9240) | Error 0.9060(0.9012) Steps 368(377.40) | Grad Norm 0.8217(25.4988) | Total Time 10.00(10.00)\n",
      "Iter 0085 | Time 26.0732(29.5987) | Bit/dim 1.8562(4.8319) | Xent 2.3026(2.3026) | Loss 1.8562(4.8319) | Error 0.9070(0.9014) Steps 368(377.11) | Grad Norm 1.3929(24.7756) | Total Time 10.00(10.00)\n",
      "Iter 0086 | Time 26.3745(29.5020) | Bit/dim 1.8523(4.7426) | Xent 2.3026(2.3026) | Loss 1.8523(4.7426) | Error 0.9050(0.9015) Steps 368(376.84) | Grad Norm 1.6646(24.0823) | Total Time 10.00(10.00)\n",
      "Iter 0087 | Time 26.2116(29.4033) | Bit/dim 1.8438(4.6556) | Xent 2.3026(2.3026) | Loss 1.8438(4.6556) | Error 0.9021(0.9015) Steps 368(376.57) | Grad Norm 1.8409(23.4150) | Total Time 10.00(10.00)\n",
      "Iter 0088 | Time 26.0402(29.3024) | Bit/dim 1.8332(4.5709) | Xent 2.3026(2.3026) | Loss 1.8332(4.5709) | Error 0.8961(0.9013) Steps 368(376.32) | Grad Norm 3.2202(22.8092) | Total Time 10.00(10.00)\n",
      "Iter 0089 | Time 26.4806(29.2177) | Bit/dim 1.8438(4.4891) | Xent 2.3026(2.3026) | Loss 1.8438(4.4891) | Error 0.9001(0.9013) Steps 368(376.07) | Grad Norm 7.8647(22.3609) | Total Time 10.00(10.00)\n",
      "Iter 0090 | Time 26.7507(29.1437) | Bit/dim 1.9329(4.4124) | Xent 2.3026(2.3026) | Loss 1.9329(4.4124) | Error 0.8959(0.9011) Steps 380(376.19) | Grad Norm 27.0354(22.5011) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0013 | Time 8.4920, Epoch Time 204.7596(219.7202), Bit/dim 2.4252, Xent 2.3026, Loss 2.4252, Error 0.9227\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0091 | Time 27.6642(29.0993) | Bit/dim 2.4434(4.3534) | Xent 2.3026(2.3026) | Loss 2.4434(4.3534) | Error 0.8980(0.9010) Steps 380(376.30) | Grad Norm 51.1564(23.3608) | Total Time 10.00(10.00)\n",
      "Iter 0092 | Time 30.8881(29.1530) | Bit/dim 2.1190(4.2863) | Xent 2.3026(2.3026) | Loss 2.1190(4.2863) | Error 0.9047(0.9012) Steps 422(377.67) | Grad Norm 7.6945(22.8908) | Total Time 10.00(10.00)\n",
      "Iter 0093 | Time 33.9321(29.2964) | Bit/dim 2.3286(4.2276) | Xent 2.3026(2.3026) | Loss 2.3286(4.2276) | Error 0.9035(0.9012) Steps 434(379.36) | Grad Norm 18.7118(22.7654) | Total Time 10.00(10.00)\n",
      "Iter 0094 | Time 33.9448(29.4358) | Bit/dim 2.0459(4.1621) | Xent 2.3026(2.3026) | Loss 2.0459(4.1621) | Error 0.9064(0.9014) Steps 440(381.18) | Grad Norm 4.5613(22.2193) | Total Time 10.00(10.00)\n",
      "Iter 0095 | Time 34.4696(29.5868) | Bit/dim 2.0211(4.0979) | Xent 2.3026(2.3026) | Loss 2.0211(4.0979) | Error 0.8946(0.9012) Steps 440(382.95) | Grad Norm 3.3282(21.6525) | Total Time 10.00(10.00)\n",
      "Iter 0096 | Time 31.1343(29.6333) | Bit/dim 2.0682(4.0370) | Xent 2.3026(2.3026) | Loss 2.0682(4.0370) | Error 0.9015(0.9012) Steps 428(384.30) | Grad Norm 3.8142(21.1174) | Total Time 10.00(10.00)\n",
      "Iter 0097 | Time 32.8115(29.7286) | Bit/dim 2.0640(3.9778) | Xent 2.3026(2.3026) | Loss 2.0640(3.9778) | Error 0.9004(0.9012) Steps 470(386.87) | Grad Norm 3.4755(20.5881) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0014 | Time 10.0578, Epoch Time 247.4727(220.5528), Bit/dim 2.0318, Xent 2.3026, Loss 2.0318, Error 0.9227\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0098 | Time 34.4242(29.8695) | Bit/dim 2.0509(3.9200) | Xent 2.3026(2.3026) | Loss 2.0509(3.9200) | Error 0.8995(0.9011) Steps 482(389.72) | Grad Norm 5.7646(20.1434) | Total Time 10.00(10.00)\n",
      "Iter 0099 | Time 32.5746(29.9506) | Bit/dim 2.0210(3.8630) | Xent 2.3026(2.3026) | Loss 2.0210(3.8630) | Error 0.9006(0.9011) Steps 464(391.95) | Grad Norm 5.2771(19.6974) | Total Time 10.00(10.00)\n",
      "Iter 0100 | Time 31.0601(29.9839) | Bit/dim 1.9935(3.8070) | Xent 2.3026(2.3026) | Loss 1.9935(3.8070) | Error 0.8992(0.9010) Steps 458(393.93) | Grad Norm 2.4275(19.1793) | Total Time 10.00(10.00)\n",
      "Iter 0101 | Time 32.7537(30.0670) | Bit/dim 2.0046(3.7529) | Xent 2.3026(2.3026) | Loss 2.0046(3.7529) | Error 0.9026(0.9011) Steps 464(396.03) | Grad Norm 2.6304(18.6829) | Total Time 10.00(10.00)\n",
      "Iter 0102 | Time 31.9135(30.1224) | Bit/dim 1.9844(3.6998) | Xent 2.3026(2.3026) | Loss 1.9844(3.6998) | Error 0.8982(0.9010) Steps 458(397.89) | Grad Norm 2.3268(18.1922) | Total Time 10.00(10.00)\n",
      "Iter 0103 | Time 32.6922(30.1995) | Bit/dim 1.9626(3.6477) | Xent 2.3026(2.3026) | Loss 1.9626(3.6477) | Error 0.9060(0.9012) Steps 458(399.70) | Grad Norm 1.6225(17.6951) | Total Time 10.00(10.00)\n",
      "Iter 0104 | Time 30.8781(30.2198) | Bit/dim 1.9570(3.5970) | Xent 2.3026(2.3026) | Loss 1.9570(3.5970) | Error 0.9038(0.9012) Steps 446(401.08) | Grad Norm 1.4608(17.2081) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0015 | Time 9.3479, Epoch Time 247.9557(221.3748), Bit/dim 1.9431, Xent 2.3026, Loss 1.9431, Error 0.9227\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0105 | Time 31.8611(30.2691) | Bit/dim 1.9609(3.5479) | Xent 2.3026(2.3026) | Loss 1.9609(3.5479) | Error 0.8994(0.9012) Steps 446(402.43) | Grad Norm 1.6858(16.7424) | Total Time 10.00(10.00)\n",
      "Iter 0106 | Time 28.9259(30.2288) | Bit/dim 1.9429(3.4998) | Xent 2.3026(2.3026) | Loss 1.9429(3.4998) | Error 0.9016(0.9012) Steps 416(402.84) | Grad Norm 1.4852(16.2847) | Total Time 10.00(10.00)\n",
      "Iter 0107 | Time 29.9259(30.2197) | Bit/dim 1.9226(3.4524) | Xent 2.3026(2.3026) | Loss 1.9226(3.4524) | Error 0.9004(0.9012) Steps 422(403.41) | Grad Norm 1.7238(15.8479) | Total Time 10.00(10.00)\n",
      "Iter 0108 | Time 30.9354(30.2412) | Bit/dim 1.9079(3.4061) | Xent 2.3026(2.3026) | Loss 1.9079(3.4061) | Error 0.9040(0.9013) Steps 422(403.97) | Grad Norm 2.3571(15.4431) | Total Time 10.00(10.00)\n",
      "Iter 0109 | Time 29.1277(30.2078) | Bit/dim 1.9018(3.3610) | Xent 2.3026(2.3026) | Loss 1.9018(3.3610) | Error 0.9035(0.9013) Steps 416(404.33) | Grad Norm 1.4732(15.0240) | Total Time 10.00(10.00)\n",
      "Iter 0110 | Time 32.8520(30.2871) | Bit/dim 1.8840(3.3167) | Xent 2.3026(2.3026) | Loss 1.8840(3.3167) | Error 0.9050(0.9014) Steps 416(404.68) | Grad Norm 1.4654(14.6173) | Total Time 10.00(10.00)\n",
      "Iter 0111 | Time 29.1889(30.2542) | Bit/dim 1.8849(3.2737) | Xent 2.3026(2.3026) | Loss 1.8849(3.2737) | Error 0.9001(0.9014) Steps 434(405.56) | Grad Norm 1.6941(14.2296) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0016 | Time 9.0814, Epoch Time 234.1564(221.7583), Bit/dim 1.8628, Xent 2.3026, Loss 1.8628, Error 0.9227\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0112 | Time 29.0416(30.2178) | Bit/dim 1.8737(3.2317) | Xent 2.3026(2.3026) | Loss 1.8737(3.2317) | Error 0.9060(0.9015) Steps 434(406.42) | Grad Norm 1.7561(13.8554) | Total Time 10.00(10.00)\n",
      "Iter 0113 | Time 30.3086(30.2205) | Bit/dim 1.8646(3.1907) | Xent 2.3026(2.3026) | Loss 1.8646(3.1907) | Error 0.9025(0.9016) Steps 440(407.42) | Grad Norm 1.0416(13.4710) | Total Time 10.00(10.00)\n",
      "Iter 0114 | Time 28.6749(30.1741) | Bit/dim 1.8586(3.1507) | Xent 2.3026(2.3026) | Loss 1.8586(3.1507) | Error 0.9031(0.9016) Steps 410(407.50) | Grad Norm 1.7388(13.1190) | Total Time 10.00(10.00)\n",
      "Iter 0115 | Time 30.1195(30.1725) | Bit/dim 1.8508(3.1117) | Xent 2.3026(2.3026) | Loss 1.8508(3.1117) | Error 0.8949(0.9014) Steps 410(407.58) | Grad Norm 1.1840(12.7609) | Total Time 10.00(10.00)\n",
      "Iter 0116 | Time 28.7100(30.1286) | Bit/dim 1.8400(3.0736) | Xent 2.3026(2.3026) | Loss 1.8400(3.0736) | Error 0.8995(0.9013) Steps 404(407.47) | Grad Norm 1.3224(12.4178) | Total Time 10.00(10.00)\n",
      "Iter 0117 | Time 31.5698(30.1719) | Bit/dim 1.8354(3.0364) | Xent 2.3026(2.3026) | Loss 1.8354(3.0364) | Error 0.9046(0.9014) Steps 428(408.08) | Grad Norm 1.5872(12.0929) | Total Time 10.00(10.00)\n",
      "Iter 0118 | Time 31.3140(30.2061) | Bit/dim 1.8281(3.0002) | Xent 2.3026(2.3026) | Loss 1.8281(3.0002) | Error 0.9024(0.9015) Steps 428(408.68) | Grad Norm 2.0448(11.7914) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0017 | Time 8.7910, Epoch Time 230.8939(222.0324), Bit/dim 1.8054, Xent 2.3026, Loss 1.8054, Error 0.9227\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0119 | Time 30.8985(30.2269) | Bit/dim 1.8125(2.9646) | Xent 2.3026(2.3026) | Loss 1.8125(2.9646) | Error 0.9045(0.9016) Steps 416(408.90) | Grad Norm 1.6075(11.4859) | Total Time 10.00(10.00)\n",
      "Iter 0120 | Time 28.9787(30.1894) | Bit/dim 1.8218(2.9303) | Xent 2.3026(2.3026) | Loss 1.8218(2.9303) | Error 0.9005(0.9015) Steps 398(408.57) | Grad Norm 1.0496(11.1728) | Total Time 10.00(10.00)\n",
      "Iter 0121 | Time 28.9325(30.1517) | Bit/dim 1.8186(2.8969) | Xent 2.3026(2.3026) | Loss 1.8186(2.8969) | Error 0.8930(0.9013) Steps 398(408.26) | Grad Norm 1.6546(10.8873) | Total Time 10.00(10.00)\n",
      "Iter 0122 | Time 27.4080(30.0694) | Bit/dim 1.8213(2.8647) | Xent 2.3026(2.3026) | Loss 1.8213(2.8647) | Error 0.9038(0.9013) Steps 410(408.31) | Grad Norm 3.0865(10.6533) | Total Time 10.00(10.00)\n",
      "Iter 0123 | Time 29.4079(30.0496) | Bit/dim 1.8021(2.8328) | Xent 2.3026(2.3026) | Loss 1.8021(2.8328) | Error 0.9041(0.9014) Steps 410(408.36) | Grad Norm 0.9642(10.3626) | Total Time 10.00(10.00)\n",
      "Iter 0124 | Time 29.4800(30.0325) | Bit/dim 1.7983(2.8018) | Xent 2.3026(2.3026) | Loss 1.7983(2.8018) | Error 0.9025(0.9015) Steps 410(408.41) | Grad Norm 3.8220(10.1664) | Total Time 10.00(10.00)\n",
      "Iter 0125 | Time 27.4937(29.9563) | Bit/dim 1.8259(2.7725) | Xent 2.3026(2.3026) | Loss 1.8259(2.7725) | Error 0.9022(0.9015) Steps 410(408.46) | Grad Norm 4.7162(10.0029) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0018 | Time 10.2462, Epoch Time 225.2135(222.1278), Bit/dim 1.7794, Xent 2.3026, Loss 1.7794, Error 0.9227\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0126 | Time 28.4744(29.9119) | Bit/dim 1.7898(2.7430) | Xent 2.3026(2.3026) | Loss 1.7898(2.7430) | Error 0.9030(0.9015) Steps 422(408.86) | Grad Norm 2.6166(9.7813) | Total Time 10.00(10.00)\n",
      "Iter 0127 | Time 32.8225(29.9992) | Bit/dim 1.9813(2.7201) | Xent 2.3026(2.3026) | Loss 1.9813(2.7201) | Error 0.9019(0.9015) Steps 452(410.16) | Grad Norm 32.3951(10.4597) | Total Time 10.00(10.00)\n",
      "Iter 0128 | Time 35.1274(30.1530) | Bit/dim 1.9937(2.6984) | Xent 2.3026(2.3026) | Loss 1.9937(2.6984) | Error 0.8984(0.9014) Steps 458(411.59) | Grad Norm 5.4962(10.3108) | Total Time 10.00(10.00)\n",
      "Iter 0129 | Time 39.1930(30.4242) | Bit/dim 2.0386(2.6786) | Xent 2.3026(2.3026) | Loss 2.0386(2.6786) | Error 0.9016(0.9015) Steps 482(413.70) | Grad Norm 2.5486(10.0779) | Total Time 10.00(10.00)\n",
      "Iter 0130 | Time 32.9815(30.5009) | Bit/dim 2.0706(2.6603) | Xent 2.3026(2.3026) | Loss 2.0706(2.6603) | Error 0.9022(0.9015) Steps 440(414.49) | Grad Norm 3.5111(9.8809) | Total Time 10.00(10.00)\n",
      "Iter 0131 | Time 36.8600(30.6917) | Bit/dim 2.0805(2.6429) | Xent 2.3026(2.3026) | Loss 2.0805(2.6429) | Error 0.9026(0.9015) Steps 470(416.16) | Grad Norm 5.1038(9.7376) | Total Time 10.00(10.00)\n",
      "Iter 0132 | Time 37.3812(30.8924) | Bit/dim 2.0531(2.6252) | Xent 2.3026(2.3026) | Loss 2.0531(2.6252) | Error 0.9010(0.9015) Steps 476(417.95) | Grad Norm 1.1881(9.4811) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0019 | Time 10.1751, Epoch Time 265.6194(223.4325), Bit/dim 2.0288, Xent 2.3026, Loss 2.0288, Error 0.9227\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0133 | Time 35.4855(31.0302) | Bit/dim 2.0440(2.6078) | Xent 2.3026(2.3026) | Loss 2.0440(2.6078) | Error 0.9031(0.9015) Steps 470(419.52) | Grad Norm 3.3802(9.2981) | Total Time 10.00(10.00)\n",
      "Iter 0134 | Time 37.3548(31.2199) | Bit/dim 2.0544(2.5912) | Xent 2.3026(2.3026) | Loss 2.0544(2.5912) | Error 0.8974(0.9014) Steps 482(421.39) | Grad Norm 6.9527(9.2277) | Total Time 10.00(10.00)\n",
      "Iter 0135 | Time 37.0127(31.3937) | Bit/dim 2.0900(2.5762) | Xent 2.3026(2.3026) | Loss 2.0900(2.5762) | Error 0.8966(0.9013) Steps 476(423.03) | Grad Norm 7.5673(9.1779) | Total Time 10.00(10.00)\n",
      "Iter 0136 | Time 36.3751(31.5432) | Bit/dim 1.9897(2.5586) | Xent 2.3026(2.3026) | Loss 1.9897(2.5586) | Error 0.9000(0.9012) Steps 470(424.44) | Grad Norm 1.2283(8.9394) | Total Time 10.00(10.00)\n",
      "Iter 0137 | Time 38.7274(31.7587) | Bit/dim 2.0765(2.5441) | Xent 2.3026(2.3026) | Loss 2.0765(2.5441) | Error 0.9051(0.9014) Steps 482(426.16) | Grad Norm 8.2947(8.9201) | Total Time 10.00(10.00)\n",
      "Iter 0138 | Time 37.5512(31.9325) | Bit/dim 2.0726(2.5300) | Xent 2.3026(2.3026) | Loss 2.0726(2.5300) | Error 0.9035(0.9014) Steps 464(427.30) | Grad Norm 5.7213(8.8241) | Total Time 10.00(10.00)\n",
      "Iter 0139 | Time 37.3936(32.0963) | Bit/dim 2.0556(2.5157) | Xent 2.3026(2.3026) | Loss 2.0556(2.5157) | Error 0.9000(0.9014) Steps 464(428.40) | Grad Norm 4.8505(8.7049) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0020 | Time 9.9189, Epoch Time 282.3324(225.1995), Bit/dim 1.9754, Xent 2.3026, Loss 1.9754, Error 0.9227\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0140 | Time 38.1938(32.2792) | Bit/dim 1.9867(2.4999) | Xent 2.3026(2.3026) | Loss 1.9867(2.4999) | Error 0.9011(0.9014) Steps 488(430.19) | Grad Norm 2.3349(8.5138) | Total Time 10.00(10.00)\n",
      "Iter 0141 | Time 39.1413(32.4851) | Bit/dim 2.0440(2.4862) | Xent 2.3026(2.3026) | Loss 2.0440(2.4862) | Error 0.9005(0.9013) Steps 500(432.28) | Grad Norm 5.3950(8.4202) | Total Time 10.00(10.00)\n",
      "Iter 0142 | Time 38.3400(32.6607) | Bit/dim 1.9677(2.4706) | Xent 2.3026(2.3026) | Loss 1.9677(2.4706) | Error 0.9014(0.9013) Steps 488(433.95) | Grad Norm 0.7607(8.1905) | Total Time 10.00(10.00)\n",
      "Iter 0143 | Time 36.0179(32.7614) | Bit/dim 1.9903(2.4562) | Xent 2.3026(2.3026) | Loss 1.9903(2.4562) | Error 0.8969(0.9012) Steps 470(435.04) | Grad Norm 2.0283(8.0056) | Total Time 10.00(10.00)\n",
      "Iter 0144 | Time 35.2437(32.8359) | Bit/dim 1.9822(2.4420) | Xent 2.3026(2.3026) | Loss 1.9822(2.4420) | Error 0.8971(0.9011) Steps 476(436.26) | Grad Norm 1.6157(7.8139) | Total Time 10.00(10.00)\n",
      "Iter 0145 | Time 36.2002(32.9368) | Bit/dim 1.9567(2.4274) | Xent 2.3026(2.3026) | Loss 1.9567(2.4274) | Error 0.9020(0.9011) Steps 476(437.46) | Grad Norm 0.7766(7.6028) | Total Time 10.00(10.00)\n",
      "Iter 0146 | Time 37.5240(33.0745) | Bit/dim 1.9607(2.4134) | Xent 2.3026(2.3026) | Loss 1.9607(2.4134) | Error 0.9074(0.9013) Steps 482(438.79) | Grad Norm 1.5402(7.4209) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0021 | Time 9.7215, Epoch Time 282.7671(226.9266), Bit/dim 1.9384, Xent 2.3026, Loss 1.9384, Error 0.9227\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0147 | Time 36.5167(33.1777) | Bit/dim 1.9474(2.3995) | Xent 2.3026(2.3026) | Loss 1.9474(2.3995) | Error 0.9044(0.9014) Steps 464(439.55) | Grad Norm 1.3623(7.2391) | Total Time 10.00(10.00)\n",
      "Iter 0148 | Time 35.6772(33.2527) | Bit/dim 1.9377(2.3856) | Xent 2.3026(2.3026) | Loss 1.9377(2.3856) | Error 0.9019(0.9014) Steps 464(440.28) | Grad Norm 0.8313(7.0469) | Total Time 10.00(10.00)\n",
      "Iter 0149 | Time 37.4979(33.3801) | Bit/dim 1.9320(2.3720) | Xent 2.3026(2.3026) | Loss 1.9320(2.3720) | Error 0.8986(0.9013) Steps 482(441.53) | Grad Norm 1.0529(6.8671) | Total Time 10.00(10.00)\n",
      "Iter 0150 | Time 36.1341(33.4627) | Bit/dim 1.9351(2.3589) | Xent 2.3026(2.3026) | Loss 1.9351(2.3589) | Error 0.8976(0.9012) Steps 482(442.75) | Grad Norm 1.0676(6.6931) | Total Time 10.00(10.00)\n",
      "Iter 0151 | Time 36.8041(33.5629) | Bit/dim 1.9211(2.3458) | Xent 2.3026(2.3026) | Loss 1.9211(2.3458) | Error 0.9075(0.9014) Steps 470(443.57) | Grad Norm 0.9992(6.5223) | Total Time 10.00(10.00)\n",
      "Iter 0152 | Time 35.1056(33.6092) | Bit/dim 1.9142(2.3328) | Xent 2.3026(2.3026) | Loss 1.9142(2.3328) | Error 0.9015(0.9014) Steps 458(444.00) | Grad Norm 0.7575(6.3493) | Total Time 10.00(10.00)\n",
      "Iter 0153 | Time 33.6719(33.6111) | Bit/dim 1.9072(2.3200) | Xent 2.3026(2.3026) | Loss 1.9072(2.3200) | Error 0.8959(0.9012) Steps 446(444.06) | Grad Norm 0.6728(6.1790) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0022 | Time 9.3964, Epoch Time 273.3717(228.3199), Bit/dim 1.8903, Xent 2.3026, Loss 1.8903, Error 0.9227\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0154 | Time 32.9702(33.5919) | Bit/dim 1.9071(2.3077) | Xent 2.3026(2.3026) | Loss 1.9071(2.3077) | Error 0.8990(0.9012) Steps 446(444.12) | Grad Norm 0.9125(6.0210) | Total Time 10.00(10.00)\n",
      "Iter 0155 | Time 33.8335(33.5991) | Bit/dim 1.8923(2.2952) | Xent 2.3026(2.3026) | Loss 1.8923(2.2952) | Error 0.8986(0.9011) Steps 446(444.17) | Grad Norm 0.7098(5.8617) | Total Time 10.00(10.00)\n",
      "Iter 0156 | Time 32.6848(33.5717) | Bit/dim 1.8776(2.2827) | Xent 2.3026(2.3026) | Loss 1.8776(2.2827) | Error 0.9021(0.9011) Steps 434(443.87) | Grad Norm 0.3751(5.6971) | Total Time 10.00(10.00)\n",
      "Iter 0157 | Time 31.9257(33.5223) | Bit/dim 1.8848(2.2707) | Xent 2.3026(2.3026) | Loss 1.8848(2.2707) | Error 0.9040(0.9012) Steps 440(443.75) | Grad Norm 0.6196(5.5448) | Total Time 10.00(10.00)\n",
      "Iter 0158 | Time 34.1006(33.5396) | Bit/dim 1.8771(2.2589) | Xent 2.3026(2.3026) | Loss 1.8771(2.2589) | Error 0.9016(0.9012) Steps 452(444.00) | Grad Norm 0.8406(5.4037) | Total Time 10.00(10.00)\n",
      "Iter 0159 | Time 35.1428(33.5877) | Bit/dim 1.8639(2.2471) | Xent 2.3026(2.3026) | Loss 1.8639(2.2471) | Error 0.9011(0.9012) Steps 452(444.24) | Grad Norm 0.5783(5.2589) | Total Time 10.00(10.00)\n",
      "Iter 0160 | Time 31.8377(33.5352) | Bit/dim 1.8599(2.2355) | Xent 2.3026(2.3026) | Loss 1.8599(2.2355) | Error 0.9016(0.9012) Steps 440(444.11) | Grad Norm 0.4834(5.1156) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0023 | Time 9.3709, Epoch Time 254.2317(229.0973), Bit/dim 1.8414, Xent 2.3026, Loss 1.8414, Error 0.9227\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0161 | Time 34.7511(33.5717) | Bit/dim 1.8534(2.2240) | Xent 2.3026(2.3026) | Loss 1.8534(2.2240) | Error 0.9038(0.9013) Steps 452(444.35) | Grad Norm 0.9315(4.9901) | Total Time 10.00(10.00)\n",
      "Iter 0162 | Time 35.8492(33.6400) | Bit/dim 1.8518(2.2128) | Xent 2.3026(2.3026) | Loss 1.8518(2.2128) | Error 0.9002(0.9013) Steps 458(444.76) | Grad Norm 0.6276(4.8592) | Total Time 10.00(10.00)\n",
      "Iter 0163 | Time 35.2305(33.6878) | Bit/dim 1.8439(2.2018) | Xent 2.3026(2.3026) | Loss 1.8439(2.2018) | Error 0.8994(0.9012) Steps 452(444.98) | Grad Norm 0.5764(4.7307) | Total Time 10.00(10.00)\n",
      "Iter 0164 | Time 33.0138(33.6675) | Bit/dim 1.8319(2.1907) | Xent 2.3026(2.3026) | Loss 1.8319(2.1907) | Error 0.8998(0.9012) Steps 440(444.83) | Grad Norm 0.9350(4.6169) | Total Time 10.00(10.00)\n",
      "Iter 0165 | Time 33.1471(33.6519) | Bit/dim 1.8266(2.1797) | Xent 2.3026(2.3026) | Loss 1.8266(2.1797) | Error 0.9004(0.9012) Steps 446(444.86) | Grad Norm 0.5312(4.4943) | Total Time 10.00(10.00)\n",
      "Iter 0166 | Time 30.4847(33.5569) | Bit/dim 1.8158(2.1688) | Xent 2.3026(2.3026) | Loss 1.8158(2.1688) | Error 0.9004(0.9011) Steps 428(444.36) | Grad Norm 0.5553(4.3761) | Total Time 10.00(10.00)\n",
      "Iter 0167 | Time 30.2624(33.4581) | Bit/dim 1.8113(2.1581) | Xent 2.3026(2.3026) | Loss 1.8113(2.1581) | Error 0.9026(0.9012) Steps 422(443.69) | Grad Norm 0.4637(4.2588) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0024 | Time 9.1438, Epoch Time 254.1687(229.8494), Bit/dim 1.7866, Xent 2.3026, Loss 1.7866, Error 0.9227\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0168 | Time 30.3835(33.3658) | Bit/dim 1.8052(2.1475) | Xent 2.3026(2.3026) | Loss 1.8052(2.1475) | Error 0.9012(0.9012) Steps 422(443.03) | Grad Norm 0.5328(4.1470) | Total Time 10.00(10.00)\n",
      "Iter 0169 | Time 30.0711(33.2670) | Bit/dim 1.7889(2.1368) | Xent 2.3026(2.3026) | Loss 1.7889(2.1368) | Error 0.9074(0.9014) Steps 416(442.22) | Grad Norm 0.6218(4.0412) | Total Time 10.00(10.00)\n",
      "Iter 0170 | Time 30.7811(33.1924) | Bit/dim 1.7853(2.1262) | Xent 2.3026(2.3026) | Loss 1.7853(2.1262) | Error 0.9071(0.9015) Steps 416(441.44) | Grad Norm 0.5153(3.9354) | Total Time 10.00(10.00)\n",
      "Iter 0171 | Time 31.3324(33.1366) | Bit/dim 1.7668(2.1154) | Xent 2.3026(2.3026) | Loss 1.7668(2.1154) | Error 0.9041(0.9016) Steps 416(440.67) | Grad Norm 0.8562(3.8431) | Total Time 10.00(10.00)\n",
      "Iter 0172 | Time 30.6676(33.0625) | Bit/dim 1.7673(2.1050) | Xent 2.3026(2.3026) | Loss 1.7673(2.1050) | Error 0.8979(0.9015) Steps 416(439.93) | Grad Norm 0.7908(3.7515) | Total Time 10.00(10.00)\n",
      "Iter 0173 | Time 28.1695(32.9158) | Bit/dim 1.7554(2.0945) | Xent 2.3026(2.3026) | Loss 1.7554(2.0945) | Error 0.8972(0.9014) Steps 404(438.86) | Grad Norm 0.5214(3.6546) | Total Time 10.00(10.00)\n",
      "Iter 0174 | Time 29.9158(32.8258) | Bit/dim 1.7443(2.0840) | Xent 2.3026(2.3026) | Loss 1.7443(2.0840) | Error 0.8941(0.9012) Steps 416(438.17) | Grad Norm 0.5159(3.5604) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0025 | Time 9.0403, Epoch Time 232.8518(229.9395), Bit/dim 1.7183, Xent 2.3026, Loss 1.7183, Error 0.9227\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0175 | Time 29.9831(32.7405) | Bit/dim 1.7306(2.0734) | Xent 2.3026(2.3026) | Loss 1.7306(2.0734) | Error 0.9036(0.9012) Steps 416(437.50) | Grad Norm 0.8372(3.4787) | Total Time 10.00(10.00)\n",
      "Iter 0176 | Time 29.9841(32.6578) | Bit/dim 1.7267(2.0630) | Xent 2.3026(2.3026) | Loss 1.7267(2.0630) | Error 0.8975(0.9011) Steps 416(436.86) | Grad Norm 1.8770(3.4307) | Total Time 10.00(10.00)\n",
      "Iter 0177 | Time 29.7732(32.5713) | Bit/dim 1.7165(2.0526) | Xent 2.3026(2.3026) | Loss 1.7165(2.0526) | Error 0.9032(0.9012) Steps 404(435.87) | Grad Norm 3.4509(3.4313) | Total Time 10.00(10.00)\n",
      "Iter 0178 | Time 29.4222(32.4768) | Bit/dim 1.7742(2.0442) | Xent 2.3026(2.3026) | Loss 1.7742(2.0442) | Error 0.8984(0.9011) Steps 398(434.74) | Grad Norm 7.0237(3.5391) | Total Time 10.00(10.00)\n",
      "Iter 0179 | Time 26.3212(32.2921) | Bit/dim 1.9360(2.0410) | Xent 2.3026(2.3026) | Loss 1.9360(2.0410) | Error 0.9014(0.9011) Steps 386(433.28) | Grad Norm 18.5661(3.9899) | Total Time 10.00(10.00)\n",
      "Iter 0180 | Time 30.1245(32.2271) | Bit/dim 2.2308(2.0467) | Xent 2.3026(2.3026) | Loss 2.2308(2.0467) | Error 0.9007(0.9011) Steps 440(433.48) | Grad Norm 11.2498(4.2077) | Total Time 10.00(10.00)\n",
      "Iter 0181 | Time 29.0624(32.1321) | Bit/dim 2.1369(2.0494) | Xent 2.3026(2.3026) | Loss 2.1369(2.0494) | Error 0.9026(0.9011) Steps 410(432.77) | Grad Norm 6.4717(4.2756) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0026 | Time 8.8709, Epoch Time 225.9176(229.8188), Bit/dim 1.9401, Xent 2.3026, Loss 1.9401, Error 0.9227\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0182 | Time 27.3271(31.9880) | Bit/dim 1.9639(2.0468) | Xent 2.3026(2.3026) | Loss 1.9639(2.0468) | Error 0.8940(0.9009) Steps 398(431.73) | Grad Norm 2.9981(4.2373) | Total Time 10.00(10.00)\n",
      "Iter 0183 | Time 24.6932(31.7691) | Bit/dim 1.9042(2.0426) | Xent 2.3026(2.3026) | Loss 1.9042(2.0426) | Error 0.9005(0.9009) Steps 374(430.00) | Grad Norm 3.8925(4.2269) | Total Time 10.00(10.00)\n",
      "Iter 0184 | Time 24.6927(31.5569) | Bit/dim 1.9083(2.0385) | Xent 2.3026(2.3026) | Loss 1.9083(2.0385) | Error 0.9035(0.9010) Steps 374(428.32) | Grad Norm 5.6275(4.2689) | Total Time 10.00(10.00)\n",
      "Iter 0185 | Time 26.7126(31.4115) | Bit/dim 1.8540(2.0330) | Xent 2.3026(2.3026) | Loss 1.8540(2.0330) | Error 0.9014(0.9010) Steps 392(427.23) | Grad Norm 2.9397(4.2291) | Total Time 10.00(10.00)\n",
      "Iter 0186 | Time 31.3415(31.4094) | Bit/dim 1.8488(2.0275) | Xent 2.3026(2.3026) | Loss 1.8488(2.0275) | Error 0.8990(0.9009) Steps 434(427.43) | Grad Norm 1.8768(4.1585) | Total Time 10.00(10.00)\n",
      "Iter 0187 | Time 31.7220(31.4188) | Bit/dim 1.8512(2.0222) | Xent 2.3026(2.3026) | Loss 1.8512(2.0222) | Error 0.9052(0.9011) Steps 440(427.81) | Grad Norm 3.0266(4.1245) | Total Time 10.00(10.00)\n",
      "Iter 0188 | Time 30.4168(31.3887) | Bit/dim 1.8459(2.0169) | Xent 2.3026(2.3026) | Loss 1.8459(2.0169) | Error 0.9055(0.9012) Steps 434(427.99) | Grad Norm 2.4979(4.0757) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0027 | Time 8.9127, Epoch Time 218.2278(229.4711), Bit/dim 1.8034, Xent 2.3026, Loss 1.8034, Error 0.9227\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0189 | Time 26.2113(31.2334) | Bit/dim 1.8185(2.0109) | Xent 2.3026(2.3026) | Loss 1.8185(2.0109) | Error 0.9065(0.9014) Steps 392(426.91) | Grad Norm 1.2411(3.9907) | Total Time 10.00(10.00)\n",
      "Iter 0190 | Time 26.8725(31.1026) | Bit/dim 1.7992(2.0046) | Xent 2.3026(2.3026) | Loss 1.7992(2.0046) | Error 0.9005(0.9013) Steps 398(426.05) | Grad Norm 1.1786(3.9063) | Total Time 10.00(10.00)\n",
      "Iter 0191 | Time 27.0252(30.9803) | Bit/dim 1.7969(1.9983) | Xent 2.3026(2.3026) | Loss 1.7969(1.9983) | Error 0.9038(0.9014) Steps 410(425.57) | Grad Norm 1.8241(3.8439) | Total Time 10.00(10.00)\n",
      "Iter 0192 | Time 27.3132(30.8703) | Bit/dim 1.7854(1.9920) | Xent 2.3026(2.3026) | Loss 1.7854(1.9920) | Error 0.8986(0.9013) Steps 410(425.10) | Grad Norm 1.7417(3.7808) | Total Time 10.00(10.00)\n",
      "Iter 0193 | Time 27.1909(30.7599) | Bit/dim 1.7997(1.9862) | Xent 2.3026(2.3026) | Loss 1.7997(1.9862) | Error 0.8980(0.9012) Steps 410(424.65) | Grad Norm 1.3749(3.7086) | Total Time 10.00(10.00)\n",
      "Iter 0194 | Time 27.5120(30.6624) | Bit/dim 1.7543(1.9792) | Xent 2.3026(2.3026) | Loss 1.7543(1.9792) | Error 0.9065(0.9014) Steps 410(424.21) | Grad Norm 1.0401(3.6286) | Total Time 10.00(10.00)\n",
      "Iter 0195 | Time 27.5217(30.5682) | Bit/dim 1.7564(1.9725) | Xent 2.3026(2.3026) | Loss 1.7564(1.9725) | Error 0.8987(0.9013) Steps 410(423.78) | Grad Norm 1.2145(3.5562) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0028 | Time 9.1238, Epoch Time 211.5495(228.9335), Bit/dim 1.7304, Xent 2.3026, Loss 1.7304, Error 0.9227\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0196 | Time 27.1624(30.4660) | Bit/dim 1.7421(1.9656) | Xent 2.3026(2.3026) | Loss 1.7421(1.9656) | Error 0.9016(0.9013) Steps 410(423.37) | Grad Norm 1.2517(3.4870) | Total Time 10.00(10.00)\n",
      "Iter 0197 | Time 27.2844(30.3706) | Bit/dim 1.7206(1.9583) | Xent 2.3026(2.3026) | Loss 1.7206(1.9583) | Error 0.9095(0.9016) Steps 410(422.97) | Grad Norm 0.9873(3.4120) | Total Time 10.00(10.00)\n",
      "Iter 0198 | Time 27.4449(30.2828) | Bit/dim 1.7147(1.9510) | Xent 2.3026(2.3026) | Loss 1.7147(1.9510) | Error 0.8965(0.9014) Steps 416(422.76) | Grad Norm 0.9128(3.3371) | Total Time 10.00(10.00)\n",
      "Iter 0199 | Time 26.3899(30.1660) | Bit/dim 1.7047(1.9436) | Xent 2.3026(2.3026) | Loss 1.7047(1.9436) | Error 0.8970(0.9013) Steps 404(422.19) | Grad Norm 1.1453(3.2713) | Total Time 10.00(10.00)\n",
      "Iter 0200 | Time 26.4565(30.0548) | Bit/dim 1.6901(1.9360) | Xent 2.3026(2.3026) | Loss 1.6901(1.9360) | Error 0.9014(0.9013) Steps 398(421.47) | Grad Norm 1.5876(3.2208) | Total Time 10.00(10.00)\n",
      "Iter 0201 | Time 25.6205(29.9217) | Bit/dim 1.6787(1.9283) | Xent 2.3026(2.3026) | Loss 1.6787(1.9283) | Error 0.9031(0.9013) Steps 392(420.58) | Grad Norm 1.7825(3.1776) | Total Time 10.00(10.00)\n",
      "Iter 0202 | Time 27.4165(29.8466) | Bit/dim 1.6644(1.9203) | Xent 2.3026(2.3026) | Loss 1.6644(1.9203) | Error 0.9021(0.9014) Steps 410(420.27) | Grad Norm 0.8062(3.1065) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0029 | Time 9.2968, Epoch Time 209.4939(228.3503), Bit/dim 1.6506, Xent 2.3026, Loss 1.6506, Error 0.9227\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0203 | Time 27.1264(29.7650) | Bit/dim 1.6649(1.9127) | Xent 2.3026(2.3026) | Loss 1.6649(1.9127) | Error 0.9018(0.9014) Steps 398(419.60) | Grad Norm 1.8411(3.0685) | Total Time 10.00(10.00)\n",
      "Iter 0204 | Time 27.3194(29.6916) | Bit/dim 1.6548(1.9050) | Xent 2.3026(2.3026) | Loss 1.6548(1.9050) | Error 0.9007(0.9014) Steps 416(419.49) | Grad Norm 1.4420(3.0197) | Total Time 10.00(10.00)\n",
      "Iter 0205 | Time 28.0985(29.6438) | Bit/dim 1.6425(1.8971) | Xent 2.3026(2.3026) | Loss 1.6425(1.8971) | Error 0.8970(0.9012) Steps 422(419.57) | Grad Norm 1.4785(2.9735) | Total Time 10.00(10.00)\n",
      "Iter 0206 | Time 27.6096(29.5828) | Bit/dim 1.6454(1.8895) | Xent 2.3026(2.3026) | Loss 1.6454(1.8895) | Error 0.9024(0.9013) Steps 398(418.92) | Grad Norm 2.0230(2.9450) | Total Time 10.00(10.00)\n",
      "Iter 0207 | Time 26.9049(29.5024) | Bit/dim 1.6353(1.8819) | Xent 2.3026(2.3026) | Loss 1.6353(1.8819) | Error 0.9045(0.9014) Steps 404(418.47) | Grad Norm 1.2683(2.8947) | Total Time 10.00(10.00)\n",
      "Iter 0208 | Time 25.9892(29.3970) | Bit/dim 1.6311(1.8744) | Xent 2.3026(2.3026) | Loss 1.6311(1.8744) | Error 0.8985(0.9013) Steps 392(417.68) | Grad Norm 1.7065(2.8590) | Total Time 10.00(10.00)\n",
      "Iter 0209 | Time 28.2738(29.3634) | Bit/dim 1.6154(1.8666) | Xent 2.3026(2.3026) | Loss 1.6154(1.8666) | Error 0.9041(0.9014) Steps 404(417.27) | Grad Norm 1.2722(2.8114) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0030 | Time 8.8058, Epoch Time 212.6430(227.8791), Bit/dim 1.5949, Xent 2.3026, Loss 1.5949, Error 0.9227\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0210 | Time 28.0455(29.3238) | Bit/dim 1.6056(1.8588) | Xent 2.3026(2.3026) | Loss 1.6056(1.8588) | Error 0.8981(0.9013) Steps 410(417.05) | Grad Norm 1.8586(2.7829) | Total Time 10.00(10.00)\n",
      "Iter 0211 | Time 27.0787(29.2565) | Bit/dim 1.5974(1.8509) | Xent 2.3026(2.3026) | Loss 1.5974(1.8509) | Error 0.9030(0.9013) Steps 398(416.48) | Grad Norm 1.4576(2.7431) | Total Time 10.00(10.00)\n",
      "Iter 0212 | Time 26.3880(29.1704) | Bit/dim 1.5958(1.8433) | Xent 2.3026(2.3026) | Loss 1.5958(1.8433) | Error 0.9040(0.9014) Steps 386(415.56) | Grad Norm 1.6365(2.7099) | Total Time 10.00(10.00)\n",
      "Iter 0213 | Time 26.6788(29.0957) | Bit/dim 1.5783(1.8353) | Xent 2.3026(2.3026) | Loss 1.5783(1.8353) | Error 0.9078(0.9016) Steps 392(414.86) | Grad Norm 0.9880(2.6582) | Total Time 10.00(10.00)\n",
      "Iter 0214 | Time 27.6577(29.0525) | Bit/dim 1.5772(1.8276) | Xent 2.3026(2.3026) | Loss 1.5772(1.8276) | Error 0.8961(0.9014) Steps 398(414.35) | Grad Norm 1.4658(2.6225) | Total Time 10.00(10.00)\n",
      "Iter 0215 | Time 27.0421(28.9922) | Bit/dim 1.5759(1.8200) | Xent 2.3026(2.3026) | Loss 1.5759(1.8200) | Error 0.8981(0.9013) Steps 398(413.86) | Grad Norm 0.8772(2.5701) | Total Time 10.00(10.00)\n",
      "Iter 0216 | Time 26.4036(28.9145) | Bit/dim 1.5720(1.8126) | Xent 2.3026(2.3026) | Loss 1.5720(1.8126) | Error 0.8995(0.9013) Steps 386(413.02) | Grad Norm 2.0908(2.5557) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0031 | Time 8.9193, Epoch Time 210.6012(227.3607), Bit/dim 1.5404, Xent 2.3026, Loss 1.5404, Error 0.9227\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0217 | Time 26.8634(28.8530) | Bit/dim 1.5590(1.8050) | Xent 2.3026(2.3026) | Loss 1.5590(1.8050) | Error 0.8978(0.9012) Steps 398(412.57) | Grad Norm 1.0398(2.5103) | Total Time 10.00(10.00)\n",
      "Iter 0218 | Time 26.1307(28.7713) | Bit/dim 1.5481(1.7973) | Xent 2.3026(2.3026) | Loss 1.5481(1.7973) | Error 0.9054(0.9013) Steps 392(411.96) | Grad Norm 1.4151(2.4774) | Total Time 10.00(10.00)\n",
      "Iter 0219 | Time 26.7842(28.7117) | Bit/dim 1.5462(1.7898) | Xent 2.3026(2.3026) | Loss 1.5462(1.7898) | Error 0.9036(0.9014) Steps 392(411.36) | Grad Norm 1.7483(2.4555) | Total Time 10.00(10.00)\n",
      "Iter 0220 | Time 26.6794(28.6508) | Bit/dim 1.5340(1.7821) | Xent 2.3026(2.3026) | Loss 1.5340(1.7821) | Error 0.9007(0.9013) Steps 392(410.78) | Grad Norm 0.8509(2.4074) | Total Time 10.00(10.00)\n",
      "Iter 0221 | Time 26.5027(28.5863) | Bit/dim 1.5345(1.7747) | Xent 2.3026(2.3026) | Loss 1.5345(1.7747) | Error 0.9010(0.9013) Steps 392(410.21) | Grad Norm 1.2172(2.3717) | Total Time 10.00(10.00)\n",
      "Iter 0222 | Time 26.6342(28.5278) | Bit/dim 1.5267(1.7672) | Xent 2.3026(2.3026) | Loss 1.5267(1.7672) | Error 0.9030(0.9014) Steps 392(409.67) | Grad Norm 1.9987(2.3605) | Total Time 10.00(10.00)\n",
      "Iter 0223 | Time 26.4664(28.4659) | Bit/dim 1.5272(1.7600) | Xent 2.3026(2.3026) | Loss 1.5272(1.7600) | Error 0.8950(0.9012) Steps 386(408.96) | Grad Norm 2.7701(2.3728) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0032 | Time 8.8633, Epoch Time 207.7073(226.7711), Bit/dim 1.5067, Xent 2.3026, Loss 1.5067, Error 0.9227\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0224 | Time 27.4777(28.4363) | Bit/dim 1.5253(1.7530) | Xent 2.3026(2.3026) | Loss 1.5253(1.7530) | Error 0.9039(0.9013) Steps 392(408.45) | Grad Norm 2.9484(2.3900) | Total Time 10.00(10.00)\n",
      "Iter 0225 | Time 26.8699(28.3893) | Bit/dim 1.5193(1.7460) | Xent 2.3026(2.3026) | Loss 1.5193(1.7460) | Error 0.9016(0.9013) Steps 386(407.78) | Grad Norm 4.7022(2.4594) | Total Time 10.00(10.00)\n",
      "Iter 0226 | Time 26.6385(28.3368) | Bit/dim 1.5537(1.7402) | Xent 2.3026(2.3026) | Loss 1.5537(1.7402) | Error 0.8964(0.9011) Steps 380(406.94) | Grad Norm 8.1643(2.6306) | Total Time 10.00(10.00)\n",
      "Iter 0227 | Time 28.1636(28.3316) | Bit/dim 1.6582(1.7377) | Xent 2.3026(2.3026) | Loss 1.6582(1.7377) | Error 0.9049(0.9012) Steps 398(406.67) | Grad Norm 15.1081(3.0049) | Total Time 10.00(10.00)\n",
      "Iter 0228 | Time 29.4800(28.3660) | Bit/dim 1.8560(1.7413) | Xent 2.3026(2.3026) | Loss 1.8560(1.7413) | Error 0.9036(0.9013) Steps 428(407.31) | Grad Norm 8.9385(3.1829) | Total Time 10.00(10.00)\n",
      "Iter 0229 | Time 27.4121(28.3374) | Bit/dim 1.8702(1.7452) | Xent 2.3026(2.3026) | Loss 1.8702(1.7452) | Error 0.8985(0.9012) Steps 404(407.21) | Grad Norm 8.6613(3.3472) | Total Time 10.00(10.00)\n",
      "Iter 0230 | Time 27.6100(28.3156) | Bit/dim 1.5650(1.7397) | Xent 2.3026(2.3026) | Loss 1.5650(1.7397) | Error 0.8996(0.9012) Steps 404(407.12) | Grad Norm 3.0364(3.3379) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0033 | Time 9.3262, Epoch Time 215.3403(226.4282), Bit/dim 1.8239, Xent 2.3026, Loss 1.8239, Error 0.9227\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0231 | Time 26.7799(28.2695) | Bit/dim 1.8397(1.7427) | Xent 2.3026(2.3026) | Loss 1.8397(1.7427) | Error 0.9001(0.9011) Steps 392(406.66) | Grad Norm 12.3675(3.6088) | Total Time 10.00(10.00)\n",
      "Iter 0232 | Time 27.9823(28.2609) | Bit/dim 1.6596(1.7403) | Xent 2.3026(2.3026) | Loss 1.6596(1.7403) | Error 0.8989(0.9011) Steps 392(406.22) | Grad Norm 4.9727(3.6497) | Total Time 10.00(10.00)\n",
      "Iter 0233 | Time 27.6163(28.2415) | Bit/dim 1.5945(1.7359) | Xent 2.3026(2.3026) | Loss 1.5945(1.7359) | Error 0.8981(0.9010) Steps 398(405.98) | Grad Norm 2.4039(3.6124) | Total Time 10.00(10.00)\n",
      "Iter 0234 | Time 30.0774(28.2966) | Bit/dim 1.6847(1.7343) | Xent 2.3026(2.3026) | Loss 1.6847(1.7343) | Error 0.8975(0.9009) Steps 422(406.46) | Grad Norm 3.4533(3.6076) | Total Time 10.00(10.00)\n",
      "Iter 0235 | Time 28.1493(28.2922) | Bit/dim 1.6258(1.7311) | Xent 2.3026(2.3026) | Loss 1.6258(1.7311) | Error 0.9024(0.9009) Steps 422(406.92) | Grad Norm 2.8603(3.5852) | Total Time 10.00(10.00)\n",
      "Iter 0236 | Time 28.0236(28.2841) | Bit/dim 1.5825(1.7266) | Xent 2.3026(2.3026) | Loss 1.5825(1.7266) | Error 0.9046(0.9010) Steps 416(407.20) | Grad Norm 1.1917(3.5134) | Total Time 10.00(10.00)\n",
      "Iter 0237 | Time 30.1730(28.3408) | Bit/dim 1.6383(1.7240) | Xent 2.3026(2.3026) | Loss 1.6383(1.7240) | Error 0.9073(0.9012) Steps 416(407.46) | Grad Norm 4.0871(3.5306) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0034 | Time 9.6430, Epoch Time 220.9443(226.2637), Bit/dim 1.6026, Xent 2.3026, Loss 1.6026, Error 0.9227\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0238 | Time 27.8589(28.3264) | Bit/dim 1.6174(1.7208) | Xent 2.3026(2.3026) | Loss 1.6174(1.7208) | Error 0.8995(0.9012) Steps 416(407.72) | Grad Norm 3.2592(3.5224) | Total Time 10.00(10.00)\n",
      "Iter 0239 | Time 28.6916(28.3373) | Bit/dim 1.5869(1.7168) | Xent 2.3026(2.3026) | Loss 1.5869(1.7168) | Error 0.9002(0.9011) Steps 416(407.97) | Grad Norm 2.7925(3.5005) | Total Time 10.00(10.00)\n",
      "Iter 0240 | Time 28.5451(28.3435) | Bit/dim 1.5965(1.7132) | Xent 2.3026(2.3026) | Loss 1.5965(1.7132) | Error 0.9005(0.9011) Steps 410(408.03) | Grad Norm 2.2145(3.4620) | Total Time 10.00(10.00)\n",
      "Iter 0241 | Time 27.7431(28.3255) | Bit/dim 1.6055(1.7099) | Xent 2.3026(2.3026) | Loss 1.6055(1.7099) | Error 0.9055(0.9013) Steps 410(408.09) | Grad Norm 3.4060(3.4603) | Total Time 10.00(10.00)\n",
      "Iter 0242 | Time 27.7057(28.3069) | Bit/dim 1.5781(1.7060) | Xent 2.3026(2.3026) | Loss 1.5781(1.7060) | Error 0.9015(0.9013) Steps 416(408.32) | Grad Norm 2.0478(3.4179) | Total Time 10.00(10.00)\n",
      "Iter 0243 | Time 27.9350(28.2958) | Bit/dim 1.5624(1.7017) | Xent 2.3026(2.3026) | Loss 1.5624(1.7017) | Error 0.9081(0.9015) Steps 416(408.55) | Grad Norm 1.8435(3.3707) | Total Time 10.00(10.00)\n",
      "Iter 0244 | Time 29.6641(28.3368) | Bit/dim 1.5774(1.6979) | Xent 2.3026(2.3026) | Loss 1.5774(1.6979) | Error 0.8966(0.9013) Steps 422(408.96) | Grad Norm 2.9755(3.3588) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0035 | Time 9.3704, Epoch Time 220.0779(226.0781), Bit/dim 1.5440, Xent 2.3026, Loss 1.5440, Error 0.9227\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0245 | Time 29.7719(28.3799) | Bit/dim 1.5599(1.6938) | Xent 2.3026(2.3026) | Loss 1.5599(1.6938) | Error 0.9000(0.9013) Steps 422(409.35) | Grad Norm 2.4546(3.3317) | Total Time 10.00(10.00)\n",
      "Iter 0246 | Time 27.7356(28.3606) | Bit/dim 1.5407(1.6892) | Xent 2.3026(2.3026) | Loss 1.5407(1.6892) | Error 0.9002(0.9013) Steps 410(409.37) | Grad Norm 2.5870(3.3093) | Total Time 10.00(10.00)\n",
      "Iter 0247 | Time 28.5631(28.3666) | Bit/dim 1.5441(1.6848) | Xent 2.3026(2.3026) | Loss 1.5441(1.6848) | Error 0.9006(0.9012) Steps 422(409.75) | Grad Norm 1.7074(3.2613) | Total Time 10.00(10.00)\n",
      "Iter 0248 | Time 28.6897(28.3763) | Bit/dim 1.5513(1.6808) | Xent 2.3026(2.3026) | Loss 1.5513(1.6808) | Error 0.9038(0.9013) Steps 404(409.57) | Grad Norm 2.1136(3.2269) | Total Time 10.00(10.00)\n",
      "Iter 0249 | Time 27.9078(28.3623) | Bit/dim 1.5225(1.6761) | Xent 2.3026(2.3026) | Loss 1.5225(1.6761) | Error 0.9010(0.9013) Steps 404(409.41) | Grad Norm 1.7173(3.1816) | Total Time 10.00(10.00)\n",
      "Iter 0250 | Time 27.3329(28.3314) | Bit/dim 1.5215(1.6715) | Xent 2.3026(2.3026) | Loss 1.5215(1.6715) | Error 0.9032(0.9014) Steps 410(409.42) | Grad Norm 2.1717(3.1513) | Total Time 10.00(10.00)\n",
      "Iter 0251 | Time 27.2642(28.2994) | Bit/dim 1.5199(1.6669) | Xent 2.3026(2.3026) | Loss 1.5199(1.6669) | Error 0.8978(0.9013) Steps 410(409.44) | Grad Norm 2.6071(3.1349) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0036 | Time 9.4046, Epoch Time 219.0794(225.8681), Bit/dim 1.4949, Xent 2.3026, Loss 1.4949, Error 0.9227\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0252 | Time 27.8806(28.2868) | Bit/dim 1.5122(1.6623) | Xent 2.3026(2.3026) | Loss 1.5122(1.6623) | Error 0.9012(0.9013) Steps 410(409.46) | Grad Norm 2.2137(3.1073) | Total Time 10.00(10.00)\n",
      "Iter 0253 | Time 28.8655(28.3042) | Bit/dim 1.5126(1.6578) | Xent 2.3026(2.3026) | Loss 1.5126(1.6578) | Error 0.9019(0.9013) Steps 410(409.47) | Grad Norm 2.0027(3.0742) | Total Time 10.00(10.00)\n",
      "Iter 0254 | Time 27.9425(28.2933) | Bit/dim 1.4982(1.6530) | Xent 2.3026(2.3026) | Loss 1.4982(1.6530) | Error 0.9030(0.9013) Steps 404(409.31) | Grad Norm 2.4096(3.0542) | Total Time 10.00(10.00)\n",
      "Iter 0255 | Time 27.8093(28.2788) | Bit/dim 1.4880(1.6480) | Xent 2.3026(2.3026) | Loss 1.4880(1.6480) | Error 0.9040(0.9014) Steps 398(408.97) | Grad Norm 2.1124(3.0260) | Total Time 10.00(10.00)\n",
      "Iter 0256 | Time 28.0133(28.2708) | Bit/dim 1.5063(1.6438) | Xent 2.3026(2.3026) | Loss 1.5063(1.6438) | Error 0.8985(0.9013) Steps 398(408.64) | Grad Norm 4.1419(3.0595) | Total Time 10.00(10.00)\n",
      "Iter 0257 | Time 27.5081(28.2479) | Bit/dim 1.4784(1.6388) | Xent 2.3026(2.3026) | Loss 1.4784(1.6388) | Error 0.8984(0.9012) Steps 398(408.32) | Grad Norm 1.3640(3.0086) | Total Time 10.00(10.00)\n",
      "Iter 0258 | Time 27.8958(28.2374) | Bit/dim 1.4931(1.6345) | Xent 2.3026(2.3026) | Loss 1.4931(1.6345) | Error 0.8994(0.9012) Steps 392(407.83) | Grad Norm 3.3753(3.0196) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0037 | Time 9.3835, Epoch Time 217.5954(225.6200), Bit/dim 1.4678, Xent 2.3026, Loss 1.4678, Error 0.9227\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0259 | Time 27.6959(28.2211) | Bit/dim 1.4806(1.6298) | Xent 2.3026(2.3026) | Loss 1.4806(1.6298) | Error 0.9060(0.9013) Steps 398(407.54) | Grad Norm 3.6578(3.0387) | Total Time 10.00(10.00)\n",
      "Iter 0260 | Time 28.9484(28.2430) | Bit/dim 1.4590(1.6247) | Xent 2.3026(2.3026) | Loss 1.4590(1.6247) | Error 0.9030(0.9014) Steps 398(407.25) | Grad Norm 1.3984(2.9895) | Total Time 10.00(10.00)\n",
      "Iter 0261 | Time 29.0736(28.2679) | Bit/dim 1.4888(1.6206) | Xent 2.3026(2.3026) | Loss 1.4888(1.6206) | Error 0.8979(0.9013) Steps 404(407.15) | Grad Norm 6.9536(3.1085) | Total Time 10.00(10.00)\n",
      "Iter 0262 | Time 29.1580(28.2946) | Bit/dim 1.5292(1.6179) | Xent 2.3026(2.3026) | Loss 1.5292(1.6179) | Error 0.8987(0.9012) Steps 422(407.60) | Grad Norm 4.0821(3.1377) | Total Time 10.00(10.00)\n",
      "Iter 0263 | Time 30.4823(28.3602) | Bit/dim 1.5481(1.6158) | Xent 2.3026(2.3026) | Loss 1.5481(1.6158) | Error 0.8999(0.9011) Steps 428(408.21) | Grad Norm 3.1640(3.1385) | Total Time 10.00(10.00)\n",
      "Iter 0264 | Time 29.3782(28.3907) | Bit/dim 1.5220(1.6130) | Xent 2.3026(2.3026) | Loss 1.5220(1.6130) | Error 0.8982(0.9011) Steps 422(408.63) | Grad Norm 2.5051(3.1195) | Total Time 10.00(10.00)\n",
      "Iter 0265 | Time 28.0803(28.3814) | Bit/dim 1.4768(1.6089) | Xent 2.3026(2.3026) | Loss 1.4768(1.6089) | Error 0.9040(0.9012) Steps 398(408.31) | Grad Norm 1.9220(3.0835) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0038 | Time 9.1417, Epoch Time 224.4410(225.5846), Bit/dim 1.4914, Xent 2.3026, Loss 1.4914, Error 0.9227\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0266 | Time 27.2945(28.3488) | Bit/dim 1.5066(1.6058) | Xent 2.3026(2.3026) | Loss 1.5066(1.6058) | Error 0.8996(0.9011) Steps 392(407.82) | Grad Norm 7.3112(3.2104) | Total Time 10.00(10.00)\n",
      "Iter 0267 | Time 27.2018(28.3144) | Bit/dim 1.4665(1.6016) | Xent 2.3026(2.3026) | Loss 1.4665(1.6016) | Error 0.8975(0.9010) Steps 398(407.52) | Grad Norm 1.2582(3.1518) | Total Time 10.00(10.00)\n",
      "Iter 0268 | Time 27.2088(28.2812) | Bit/dim 1.4750(1.5978) | Xent 2.3026(2.3026) | Loss 1.4750(1.5978) | Error 0.9070(0.9012) Steps 398(407.24) | Grad Norm 2.9423(3.1455) | Total Time 10.00(10.00)\n",
      "Iter 0269 | Time 28.4048(28.2850) | Bit/dim 1.4682(1.5940) | Xent 2.3026(2.3026) | Loss 1.4682(1.5940) | Error 0.8945(0.9010) Steps 404(407.14) | Grad Norm 2.1634(3.1160) | Total Time 10.00(10.00)\n",
      "Iter 0270 | Time 27.3379(28.2565) | Bit/dim 1.4473(1.5896) | Xent 2.3026(2.3026) | Loss 1.4473(1.5896) | Error 0.9025(0.9010) Steps 392(406.69) | Grad Norm 1.4257(3.0653) | Total Time 10.00(10.00)\n",
      "Iter 0271 | Time 26.6634(28.2088) | Bit/dim 1.4608(1.5857) | Xent 2.3026(2.3026) | Loss 1.4608(1.5857) | Error 0.9036(0.9011) Steps 386(406.07) | Grad Norm 3.7741(3.0866) | Total Time 10.00(10.00)\n",
      "Iter 0272 | Time 27.0034(28.1726) | Bit/dim 1.4475(1.5815) | Xent 2.3026(2.3026) | Loss 1.4475(1.5815) | Error 0.9036(0.9012) Steps 392(405.64) | Grad Norm 2.6353(3.0731) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0039 | Time 9.2022, Epoch Time 212.7803(225.2005), Bit/dim 1.4750, Xent 2.3026, Loss 1.4750, Error 0.9227\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0273 | Time 28.9289(28.1953) | Bit/dim 1.4930(1.5789) | Xent 2.3026(2.3026) | Loss 1.4930(1.5789) | Error 0.9016(0.9012) Steps 398(405.41) | Grad Norm 5.7873(3.1545) | Total Time 10.00(10.00)\n",
      "Iter 0274 | Time 26.7483(28.1519) | Bit/dim 1.6212(1.5802) | Xent 2.3026(2.3026) | Loss 1.6212(1.5802) | Error 0.9030(0.9012) Steps 374(404.47) | Grad Norm 9.2943(3.3387) | Total Time 10.00(10.00)\n",
      "Iter 0275 | Time 27.1005(28.1203) | Bit/dim 1.4983(1.5777) | Xent 2.3026(2.3026) | Loss 1.4983(1.5777) | Error 0.9016(0.9013) Steps 392(404.10) | Grad Norm 6.0666(3.4205) | Total Time 10.00(10.00)\n",
      "Iter 0276 | Time 26.4503(28.0702) | Bit/dim 1.5056(1.5755) | Xent 2.3026(2.3026) | Loss 1.5056(1.5755) | Error 0.9056(0.9014) Steps 380(403.37) | Grad Norm 4.9342(3.4659) | Total Time 10.00(10.00)\n",
      "Iter 0277 | Time 26.7720(28.0313) | Bit/dim 1.7574(1.5810) | Xent 2.3026(2.3026) | Loss 1.7574(1.5810) | Error 0.8961(0.9012) Steps 380(402.67) | Grad Norm 13.5083(3.7672) | Total Time 10.00(10.00)\n",
      "Iter 0278 | Time 28.1592(28.0351) | Bit/dim 1.9584(1.5923) | Xent 2.3026(2.3026) | Loss 1.9584(1.5923) | Error 0.8991(0.9012) Steps 404(402.71) | Grad Norm 7.9166(3.8917) | Total Time 10.00(10.00)\n",
      "Iter 0279 | Time 29.6370(28.0832) | Bit/dim 1.5994(1.5925) | Xent 2.3026(2.3026) | Loss 1.5994(1.5925) | Error 0.9021(0.9012) Steps 416(403.11) | Grad Norm 4.1179(3.8985) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0040 | Time 9.0976, Epoch Time 215.1819(224.8999), Bit/dim 1.6813, Xent 2.3026, Loss 1.6813, Error 0.9227\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0280 | Time 27.5026(28.0658) | Bit/dim 1.7002(1.5958) | Xent 2.3026(2.3026) | Loss 1.7002(1.5958) | Error 0.8991(0.9011) Steps 410(403.32) | Grad Norm 6.0194(3.9621) | Total Time 10.00(10.00)\n",
      "Iter 0281 | Time 28.5653(28.0807) | Bit/dim 1.6888(1.5986) | Xent 2.3026(2.3026) | Loss 1.6888(1.5986) | Error 0.8984(0.9010) Steps 404(403.34) | Grad Norm 4.9383(3.9914) | Total Time 10.00(10.00)\n",
      "Iter 0282 | Time 30.9652(28.1673) | Bit/dim 1.6394(1.5998) | Xent 2.3026(2.3026) | Loss 1.6394(1.5998) | Error 0.8989(0.9010) Steps 428(404.08) | Grad Norm 3.9750(3.9909) | Total Time 10.00(10.00)\n",
      "Iter 0283 | Time 31.9149(28.2797) | Bit/dim 1.6833(1.6023) | Xent 2.3026(2.3026) | Loss 1.6833(1.6023) | Error 0.9014(0.9010) Steps 428(404.80) | Grad Norm 4.2178(3.9977) | Total Time 10.00(10.00)\n",
      "Iter 0284 | Time 31.0118(28.3617) | Bit/dim 1.6619(1.6041) | Xent 2.3026(2.3026) | Loss 1.6619(1.6041) | Error 0.9044(0.9011) Steps 428(405.49) | Grad Norm 2.2749(3.9460) | Total Time 10.00(10.00)\n",
      "Iter 0285 | Time 30.0783(28.4132) | Bit/dim 1.6318(1.6049) | Xent 2.3026(2.3026) | Loss 1.6318(1.6049) | Error 0.9042(0.9012) Steps 398(405.27) | Grad Norm 2.7131(3.9090) | Total Time 10.00(10.00)\n",
      "Iter 0286 | Time 27.8655(28.3967) | Bit/dim 1.6103(1.6051) | Xent 2.3026(2.3026) | Loss 1.6103(1.6051) | Error 0.9027(0.9012) Steps 404(405.23) | Grad Norm 3.3139(3.8912) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0041 | Time 8.6097, Epoch Time 228.9142(225.0203), Bit/dim 1.5699, Xent 2.3026, Loss 1.5699, Error 0.9227\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0287 | Time 26.9997(28.3548) | Bit/dim 1.5866(1.6045) | Xent 2.3026(2.3026) | Loss 1.5866(1.6045) | Error 0.8995(0.9012) Steps 398(405.01) | Grad Norm 1.8235(3.8291) | Total Time 10.00(10.00)\n",
      "Iter 0288 | Time 27.2413(28.3214) | Bit/dim 1.6012(1.6044) | Xent 2.3026(2.3026) | Loss 1.6012(1.6044) | Error 0.9000(0.9012) Steps 404(404.98) | Grad Norm 3.4606(3.8181) | Total Time 10.00(10.00)\n",
      "Iter 0289 | Time 27.9455(28.3101) | Bit/dim 1.5538(1.6029) | Xent 2.3026(2.3026) | Loss 1.5538(1.6029) | Error 0.9061(0.9013) Steps 404(404.95) | Grad Norm 2.8397(3.7887) | Total Time 10.00(10.00)\n",
      "Iter 0290 | Time 27.0361(28.2719) | Bit/dim 1.5285(1.6007) | Xent 2.3026(2.3026) | Loss 1.5285(1.6007) | Error 0.9062(0.9014) Steps 398(404.74) | Grad Norm 1.2580(3.7128) | Total Time 10.00(10.00)\n",
      "Iter 0291 | Time 26.8924(28.2305) | Bit/dim 1.5502(1.5991) | Xent 2.3026(2.3026) | Loss 1.5502(1.5991) | Error 0.8978(0.9013) Steps 398(404.54) | Grad Norm 2.5326(3.6774) | Total Time 10.00(10.00)\n",
      "Iter 0292 | Time 26.6489(28.1831) | Bit/dim 1.5404(1.5974) | Xent 2.3026(2.3026) | Loss 1.5404(1.5974) | Error 0.8965(0.9012) Steps 392(404.17) | Grad Norm 1.9698(3.6262) | Total Time 10.00(10.00)\n",
      "Iter 0293 | Time 25.9973(28.1175) | Bit/dim 1.5261(1.5952) | Xent 2.3026(2.3026) | Loss 1.5261(1.5952) | Error 0.9011(0.9012) Steps 386(403.62) | Grad Norm 1.5386(3.5636) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0042 | Time 8.6695, Epoch Time 209.9144(224.5672), Bit/dim 1.5045, Xent 2.3026, Loss 1.5045, Error 0.9227\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0294 | Time 26.9983(28.0839) | Bit/dim 1.5126(1.5928) | Xent 2.3026(2.3026) | Loss 1.5126(1.5928) | Error 0.9001(0.9012) Steps 392(403.27) | Grad Norm 2.2880(3.5253) | Total Time 10.00(10.00)\n",
      "Iter 0295 | Time 27.3857(28.0630) | Bit/dim 1.4921(1.5897) | Xent 2.3026(2.3026) | Loss 1.4921(1.5897) | Error 0.9089(0.9014) Steps 404(403.29) | Grad Norm 1.4145(3.4620) | Total Time 10.00(10.00)\n",
      "Iter 0296 | Time 27.3049(28.0403) | Bit/dim 1.5079(1.5873) | Xent 2.3026(2.3026) | Loss 1.5079(1.5873) | Error 0.8991(0.9013) Steps 404(403.32) | Grad Norm 2.3988(3.4301) | Total Time 10.00(10.00)\n",
      "Iter 0297 | Time 27.6577(28.0288) | Bit/dim 1.4929(1.5845) | Xent 2.3026(2.3026) | Loss 1.4929(1.5845) | Error 0.9020(0.9013) Steps 398(403.16) | Grad Norm 1.9718(3.3863) | Total Time 10.00(10.00)\n",
      "Iter 0298 | Time 29.8024(28.0820) | Bit/dim 1.4776(1.5813) | Xent 2.3026(2.3026) | Loss 1.4776(1.5813) | Error 0.8946(0.9011) Steps 404(403.18) | Grad Norm 2.4457(3.3581) | Total Time 10.00(10.00)\n",
      "Iter 0299 | Time 27.3441(28.0598) | Bit/dim 1.4617(1.5777) | Xent 2.3026(2.3026) | Loss 1.4617(1.5777) | Error 0.9020(0.9012) Steps 392(402.85) | Grad Norm 1.6352(3.3064) | Total Time 10.00(10.00)\n",
      "Iter 0300 | Time 27.7095(28.0493) | Bit/dim 1.4646(1.5743) | Xent 2.3026(2.3026) | Loss 1.4646(1.5743) | Error 0.9018(0.9012) Steps 398(402.70) | Grad Norm 3.1905(3.3029) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0043 | Time 9.0774, Epoch Time 215.7527(224.3027), Bit/dim 1.4351, Xent 2.3026, Loss 1.4351, Error 0.9227\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0301 | Time 27.8757(28.0441) | Bit/dim 1.4556(1.5707) | Xent 2.3026(2.3026) | Loss 1.4556(1.5707) | Error 0.8999(0.9011) Steps 398(402.56) | Grad Norm 1.1612(3.2387) | Total Time 10.00(10.00)\n",
      "Iter 0302 | Time 29.0314(28.0737) | Bit/dim 1.4436(1.5669) | Xent 2.3026(2.3026) | Loss 1.4436(1.5669) | Error 0.9042(0.9012) Steps 404(402.60) | Grad Norm 2.7395(3.2237) | Total Time 10.00(10.00)\n",
      "Iter 0303 | Time 27.4942(28.0564) | Bit/dim 1.4415(1.5631) | Xent 2.3026(2.3026) | Loss 1.4415(1.5631) | Error 0.9011(0.9012) Steps 398(402.46) | Grad Norm 3.0592(3.2188) | Total Time 10.00(10.00)\n",
      "Iter 0304 | Time 28.4391(28.0678) | Bit/dim 1.4295(1.5591) | Xent 2.3026(2.3026) | Loss 1.4295(1.5591) | Error 0.9030(0.9013) Steps 392(402.15) | Grad Norm 2.6606(3.2020) | Total Time 10.00(10.00)\n",
      "Iter 0305 | Time 27.7808(28.0592) | Bit/dim 1.4167(1.5549) | Xent 2.3026(2.3026) | Loss 1.4167(1.5549) | Error 0.9001(0.9013) Steps 386(401.67) | Grad Norm 2.4884(3.1806) | Total Time 10.00(10.00)\n",
      "Iter 0306 | Time 27.0344(28.0285) | Bit/dim 1.4168(1.5507) | Xent 2.3026(2.3026) | Loss 1.4168(1.5507) | Error 0.8992(0.9012) Steps 386(401.20) | Grad Norm 1.5809(3.1326) | Total Time 10.00(10.00)\n",
      "Iter 0307 | Time 27.1241(28.0014) | Bit/dim 1.4138(1.5466) | Xent 2.3026(2.3026) | Loss 1.4138(1.5466) | Error 0.9018(0.9012) Steps 386(400.74) | Grad Norm 2.5006(3.1137) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0044 | Time 8.9385, Epoch Time 216.1607(224.0585), Bit/dim 1.4178, Xent 2.3026, Loss 1.4178, Error 0.9227\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0308 | Time 27.3668(27.9823) | Bit/dim 1.4324(1.5432) | Xent 2.3026(2.3026) | Loss 1.4324(1.5432) | Error 0.9012(0.9012) Steps 392(400.48) | Grad Norm 5.5290(3.1861) | Total Time 10.00(10.00)\n",
      "Iter 0309 | Time 27.2132(27.9592) | Bit/dim 1.5725(1.5441) | Xent 2.3026(2.3026) | Loss 1.5725(1.5441) | Error 0.9052(0.9013) Steps 386(400.04) | Grad Norm 10.3819(3.4020) | Total Time 10.00(10.00)\n",
      "Iter 0310 | Time 28.4592(27.9742) | Bit/dim 1.5817(1.5452) | Xent 2.3026(2.3026) | Loss 1.5817(1.5452) | Error 0.9039(0.9014) Steps 380(399.44) | Grad Norm 10.7592(3.6227) | Total Time 10.00(10.00)\n",
      "Iter 0311 | Time 27.5389(27.9612) | Bit/dim 1.4448(1.5422) | Xent 2.3026(2.3026) | Loss 1.4448(1.5422) | Error 0.8995(0.9014) Steps 404(399.58) | Grad Norm 3.0782(3.6064) | Total Time 10.00(10.00)\n",
      "Iter 0312 | Time 27.5855(27.9499) | Bit/dim 1.4962(1.5408) | Xent 2.3026(2.3026) | Loss 1.4962(1.5408) | Error 0.8985(0.9013) Steps 398(399.53) | Grad Norm 7.5363(3.7243) | Total Time 10.00(10.00)\n",
      "Iter 0313 | Time 27.2487(27.9289) | Bit/dim 1.5379(1.5407) | Xent 2.3026(2.3026) | Loss 1.5379(1.5407) | Error 0.9038(0.9013) Steps 398(399.49) | Grad Norm 6.2998(3.8015) | Total Time 10.00(10.00)\n",
      "Iter 0314 | Time 28.7859(27.9546) | Bit/dim 1.4435(1.5378) | Xent 2.3026(2.3026) | Loss 1.4435(1.5378) | Error 0.8989(0.9013) Steps 404(399.62) | Grad Norm 5.1167(3.8410) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0045 | Time 8.6842, Epoch Time 215.2665(223.7947), Bit/dim 1.5450, Xent 2.3026, Loss 1.5450, Error 0.9227\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0315 | Time 25.3502(27.8764) | Bit/dim 1.5567(1.5384) | Xent 2.3026(2.3026) | Loss 1.5567(1.5384) | Error 0.9054(0.9014) Steps 374(398.85) | Grad Norm 6.5326(3.9217) | Total Time 10.00(10.00)\n",
      "Iter 0316 | Time 27.5016(27.8652) | Bit/dim 1.4672(1.5362) | Xent 2.3026(2.3026) | Loss 1.4672(1.5362) | Error 0.9062(0.9015) Steps 398(398.83) | Grad Norm 4.8269(3.9489) | Total Time 10.00(10.00)\n",
      "Iter 0317 | Time 27.0072(27.8395) | Bit/dim 1.5431(1.5364) | Xent 2.3026(2.3026) | Loss 1.5431(1.5364) | Error 0.9015(0.9015) Steps 386(398.44) | Grad Norm 6.2656(4.0184) | Total Time 10.00(10.00)\n",
      "Iter 0318 | Time 27.3125(27.8237) | Bit/dim 1.5248(1.5361) | Xent 2.3026(2.3026) | Loss 1.5248(1.5361) | Error 0.8975(0.9014) Steps 392(398.25) | Grad Norm 5.3389(4.0580) | Total Time 10.00(10.00)\n",
      "Iter 0319 | Time 26.5129(27.7843) | Bit/dim 1.4406(1.5332) | Xent 2.3026(2.3026) | Loss 1.4406(1.5332) | Error 0.8981(0.9013) Steps 386(397.88) | Grad Norm 2.3431(4.0066) | Total Time 10.00(10.00)\n",
      "Iter 0320 | Time 26.4064(27.7430) | Bit/dim 1.5126(1.5326) | Xent 2.3026(2.3026) | Loss 1.5126(1.5326) | Error 0.9034(0.9014) Steps 380(397.34) | Grad Norm 5.2432(4.0437) | Total Time 10.00(10.00)\n",
      "Iter 0321 | Time 25.9936(27.6905) | Bit/dim 1.4535(1.5302) | Xent 2.3026(2.3026) | Loss 1.4535(1.5302) | Error 0.8970(0.9012) Steps 380(396.82) | Grad Norm 2.7471(4.0048) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0046 | Time 8.5414, Epoch Time 207.0736(223.2931), Bit/dim 1.4427, Xent 2.3026, Loss 1.4427, Error 0.9227\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0322 | Time 26.2107(27.6461) | Bit/dim 1.4575(1.5280) | Xent 2.3026(2.3026) | Loss 1.4575(1.5280) | Error 0.9016(0.9013) Steps 386(396.50) | Grad Norm 2.9941(3.9745) | Total Time 10.00(10.00)\n",
      "Iter 0323 | Time 27.6688(27.6468) | Bit/dim 1.4872(1.5268) | Xent 2.3026(2.3026) | Loss 1.4872(1.5268) | Error 0.8991(0.9012) Steps 398(396.54) | Grad Norm 3.6152(3.9637) | Total Time 10.00(10.00)\n",
      "Iter 0324 | Time 25.7220(27.5891) | Bit/dim 1.4371(1.5241) | Xent 2.3026(2.3026) | Loss 1.4371(1.5241) | Error 0.9010(0.9012) Steps 374(395.87) | Grad Norm 1.9161(3.9022) | Total Time 10.00(10.00)\n",
      "Iter 0325 | Time 25.2051(27.5175) | Bit/dim 1.4512(1.5219) | Xent 2.3026(2.3026) | Loss 1.4512(1.5219) | Error 0.9024(0.9012) Steps 374(395.21) | Grad Norm 2.9815(3.8746) | Total Time 10.00(10.00)\n",
      "Iter 0326 | Time 26.1554(27.4767) | Bit/dim 1.4584(1.5200) | Xent 2.3026(2.3026) | Loss 1.4584(1.5200) | Error 0.8951(0.9010) Steps 374(394.58) | Grad Norm 3.2109(3.8547) | Total Time 10.00(10.00)\n",
      "Iter 0327 | Time 24.9994(27.4024) | Bit/dim 1.4153(1.5169) | Xent 2.3026(2.3026) | Loss 1.4153(1.5169) | Error 0.9061(0.9012) Steps 374(393.96) | Grad Norm 1.1089(3.7723) | Total Time 10.00(10.00)\n",
      "Iter 0328 | Time 27.8851(27.4168) | Bit/dim 1.4433(1.5147) | Xent 2.3026(2.3026) | Loss 1.4433(1.5147) | Error 0.9016(0.9012) Steps 386(393.72) | Grad Norm 2.4788(3.7335) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0047 | Time 8.8933, Epoch Time 205.2581(222.7520), Bit/dim 1.4127, Xent 2.3026, Loss 1.4127, Error 0.9227\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0329 | Time 26.4198(27.3869) | Bit/dim 1.4268(1.5120) | Xent 2.3026(2.3026) | Loss 1.4268(1.5120) | Error 0.9046(0.9013) Steps 386(393.49) | Grad Norm 1.7749(3.6748) | Total Time 10.00(10.00)\n",
      "Iter 0330 | Time 25.6783(27.3357) | Bit/dim 1.4123(1.5091) | Xent 2.3026(2.3026) | Loss 1.4123(1.5091) | Error 0.9051(0.9014) Steps 374(392.90) | Grad Norm 1.4388(3.6077) | Total Time 10.00(10.00)\n",
      "Iter 0331 | Time 24.9506(27.2641) | Bit/dim 1.4291(1.5067) | Xent 2.3026(2.3026) | Loss 1.4291(1.5067) | Error 0.8928(0.9012) Steps 368(392.16) | Grad Norm 2.8643(3.5854) | Total Time 10.00(10.00)\n",
      "Iter 0332 | Time 26.3653(27.2371) | Bit/dim 1.4031(1.5035) | Xent 2.3026(2.3026) | Loss 1.4031(1.5035) | Error 0.8991(0.9011) Steps 380(391.79) | Grad Norm 1.5571(3.5245) | Total Time 10.00(10.00)\n",
      "Iter 0333 | Time 26.3011(27.2091) | Bit/dim 1.4124(1.5008) | Xent 2.3026(2.3026) | Loss 1.4124(1.5008) | Error 0.9021(0.9011) Steps 380(391.44) | Grad Norm 2.4731(3.4930) | Total Time 10.00(10.00)\n",
      "Iter 0334 | Time 26.5861(27.1904) | Bit/dim 1.3898(1.4975) | Xent 2.3026(2.3026) | Loss 1.3898(1.4975) | Error 0.8984(0.9010) Steps 380(391.10) | Grad Norm 1.2178(3.4247) | Total Time 10.00(10.00)\n",
      "Iter 0335 | Time 26.5981(27.1726) | Bit/dim 1.4049(1.4947) | Xent 2.3026(2.3026) | Loss 1.4049(1.4947) | Error 0.9042(0.9011) Steps 380(390.76) | Grad Norm 2.6742(3.4022) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0048 | Time 8.8924, Epoch Time 204.3487(222.1999), Bit/dim 1.3705, Xent 2.3026, Loss 1.3705, Error 0.9227\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0336 | Time 26.1746(27.1427) | Bit/dim 1.3852(1.4914) | Xent 2.3026(2.3026) | Loss 1.3852(1.4914) | Error 0.8975(0.9010) Steps 380(390.44) | Grad Norm 1.3784(3.3415) | Total Time 10.00(10.00)\n",
      "Iter 0337 | Time 27.1376(27.1425) | Bit/dim 1.3890(1.4883) | Xent 2.3026(2.3026) | Loss 1.3890(1.4883) | Error 0.9061(0.9012) Steps 374(389.95) | Grad Norm 2.6436(3.3206) | Total Time 10.00(10.00)\n",
      "Iter 0338 | Time 25.7083(27.0995) | Bit/dim 1.3749(1.4849) | Xent 2.3026(2.3026) | Loss 1.3749(1.4849) | Error 0.9002(0.9012) Steps 380(389.65) | Grad Norm 1.1584(3.2557) | Total Time 10.00(10.00)\n",
      "Iter 0339 | Time 26.2928(27.0753) | Bit/dim 1.3875(1.4820) | Xent 2.3026(2.3026) | Loss 1.3875(1.4820) | Error 0.9032(0.9012) Steps 380(389.36) | Grad Norm 4.0765(3.2803) | Total Time 10.00(10.00)\n",
      "Iter 0340 | Time 25.6264(27.0318) | Bit/dim 1.3818(1.4790) | Xent 2.3026(2.3026) | Loss 1.3818(1.4790) | Error 0.9009(0.9012) Steps 374(388.90) | Grad Norm 3.9907(3.3016) | Total Time 10.00(10.00)\n",
      "Iter 0341 | Time 25.6812(26.9913) | Bit/dim 1.3710(1.4758) | Xent 2.3026(2.3026) | Loss 1.3710(1.4758) | Error 0.9009(0.9012) Steps 374(388.45) | Grad Norm 1.4673(3.2466) | Total Time 10.00(10.00)\n",
      "Iter 0342 | Time 26.4219(26.9742) | Bit/dim 1.3740(1.4727) | Xent 2.3026(2.3026) | Loss 1.3740(1.4727) | Error 0.9021(0.9012) Steps 380(388.20) | Grad Norm 3.5529(3.2558) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0049 | Time 8.7553, Epoch Time 204.3150(221.6634), Bit/dim 1.3773, Xent 2.3026, Loss 1.3773, Error 0.9227\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0343 | Time 25.8035(26.9391) | Bit/dim 1.3982(1.4705) | Xent 2.3026(2.3026) | Loss 1.3982(1.4705) | Error 0.9000(0.9012) Steps 374(387.77) | Grad Norm 5.6439(3.3274) | Total Time 10.00(10.00)\n",
      "Iter 0344 | Time 26.8231(26.9356) | Bit/dim 1.4196(1.4690) | Xent 2.3026(2.3026) | Loss 1.4196(1.4690) | Error 0.9004(0.9012) Steps 392(387.90) | Grad Norm 8.4224(3.4803) | Total Time 10.00(10.00)\n",
      "Iter 0345 | Time 25.7691(26.9006) | Bit/dim 1.4329(1.4679) | Xent 2.3026(2.3026) | Loss 1.4329(1.4679) | Error 0.9022(0.9012) Steps 374(387.48) | Grad Norm 8.0854(3.6184) | Total Time 10.00(10.00)\n",
      "Iter 0346 | Time 26.1269(26.8774) | Bit/dim 1.3709(1.4650) | Xent 2.3026(2.3026) | Loss 1.3709(1.4650) | Error 0.9011(0.9012) Steps 380(387.26) | Grad Norm 4.3466(3.6403) | Total Time 10.00(10.00)\n",
      "Iter 0347 | Time 26.3193(26.8607) | Bit/dim 1.3494(1.4615) | Xent 2.3026(2.3026) | Loss 1.3494(1.4615) | Error 0.9035(0.9013) Steps 380(387.04) | Grad Norm 0.7901(3.5548) | Total Time 10.00(10.00)\n",
      "Iter 0348 | Time 26.8644(26.8608) | Bit/dim 1.3520(1.4582) | Xent 2.3026(2.3026) | Loss 1.3520(1.4582) | Error 0.8995(0.9012) Steps 380(386.83) | Grad Norm 2.8464(3.5335) | Total Time 10.00(10.00)\n",
      "Iter 0349 | Time 26.1511(26.8395) | Bit/dim 1.3908(1.4562) | Xent 2.3026(2.3026) | Loss 1.3908(1.4562) | Error 0.9014(0.9012) Steps 380(386.62) | Grad Norm 6.9220(3.6352) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0050 | Time 8.6051, Epoch Time 204.7645(221.1564), Bit/dim 1.4474, Xent 2.3026, Loss 1.4474, Error 0.9227\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0350 | Time 26.4730(26.8285) | Bit/dim 1.4629(1.4564) | Xent 2.3026(2.3026) | Loss 1.4629(1.4564) | Error 0.9021(0.9012) Steps 374(386.24) | Grad Norm 9.9377(3.8243) | Total Time 10.00(10.00)\n",
      "Iter 0351 | Time 26.6685(26.8237) | Bit/dim 1.3935(1.4545) | Xent 2.3026(2.3026) | Loss 1.3935(1.4545) | Error 0.9016(0.9013) Steps 380(386.06) | Grad Norm 6.3685(3.9006) | Total Time 10.00(10.00)\n",
      "Iter 0352 | Time 26.9598(26.8278) | Bit/dim 1.3545(1.4515) | Xent 2.3026(2.3026) | Loss 1.3545(1.4515) | Error 0.9026(0.9013) Steps 380(385.88) | Grad Norm 1.3260(3.8234) | Total Time 10.00(10.00)\n",
      "Iter 0353 | Time 27.5262(26.8487) | Bit/dim 1.3944(1.4498) | Xent 2.3026(2.3026) | Loss 1.3944(1.4498) | Error 0.9036(0.9014) Steps 380(385.70) | Grad Norm 5.3032(3.8677) | Total Time 10.00(10.00)\n",
      "Iter 0354 | Time 27.3912(26.8650) | Bit/dim 1.3930(1.4481) | Xent 2.3026(2.3026) | Loss 1.3930(1.4481) | Error 0.9058(0.9015) Steps 380(385.53) | Grad Norm 6.9871(3.9613) | Total Time 10.00(10.00)\n",
      "Iter 0358 | Time 27.2022(26.8166) | Bit/dim 1.3635(1.4384) | Xent 2.3026(2.3026) | Loss 1.3635(1.4384) | Error 0.8982(0.9012) Steps 386(384.90) | Grad Norm 4.1311(3.9892) | Total Time 10.00(10.00)\n",
      "Iter 0359 | Time 25.6138(26.7806) | Bit/dim 1.3472(1.4357) | Xent 2.3026(2.3026) | Loss 1.3472(1.4357) | Error 0.9039(0.9013) Steps 380(384.76) | Grad Norm 1.4314(3.9125) | Total Time 10.00(10.00)\n",
      "Iter 0360 | Time 26.1956(26.7630) | Bit/dim 1.3504(1.4331) | Xent 2.3026(2.3026) | Loss 1.3504(1.4331) | Error 0.9010(0.9013) Steps 380(384.61) | Grad Norm 4.4252(3.9278) | Total Time 10.00(10.00)\n",
      "Iter 0361 | Time 25.6052(26.7283) | Bit/dim 1.3727(1.4313) | Xent 2.3026(2.3026) | Loss 1.3727(1.4313) | Error 0.9000(0.9012) Steps 374(384.30) | Grad Norm 6.2806(3.9984) | Total Time 10.00(10.00)\n",
      "Iter 0362 | Time 26.2904(26.7151) | Bit/dim 1.3521(1.4289) | Xent 2.3026(2.3026) | Loss 1.3521(1.4289) | Error 0.9025(0.9013) Steps 380(384.17) | Grad Norm 4.2798(4.0069) | Total Time 10.00(10.00)\n",
      "Iter 0363 | Time 26.5701(26.7108) | Bit/dim 1.3341(1.4261) | Xent 2.3026(2.3026) | Loss 1.3341(1.4261) | Error 0.9019(0.9013) Steps 386(384.22) | Grad Norm 1.1626(3.9215) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0052 | Time 8.8940, Epoch Time 204.3750(220.3100), Bit/dim 1.3231, Xent 2.3026, Loss 1.3231, Error 0.9227\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0364 | Time 27.1910(26.7252) | Bit/dim 1.3406(1.4235) | Xent 2.3026(2.3026) | Loss 1.3406(1.4235) | Error 0.9032(0.9013) Steps 386(384.28) | Grad Norm 2.6916(3.8846) | Total Time 10.00(10.00)\n",
      "Iter 0365 | Time 26.3124(26.7128) | Bit/dim 1.3349(1.4209) | Xent 2.3026(2.3026) | Loss 1.3349(1.4209) | Error 0.9000(0.9013) Steps 380(384.15) | Grad Norm 3.3501(3.8686) | Total Time 10.00(10.00)\n",
      "Iter 0366 | Time 26.4608(26.7053) | Bit/dim 1.3405(1.4185) | Xent 2.3026(2.3026) | Loss 1.3405(1.4185) | Error 0.9061(0.9015) Steps 374(383.84) | Grad Norm 5.9116(3.9299) | Total Time 10.00(10.00)\n",
      "Iter 0367 | Time 26.9724(26.7133) | Bit/dim 1.4457(1.4193) | Xent 2.3026(2.3026) | Loss 1.4457(1.4193) | Error 0.8984(0.9014) Steps 380(383.73) | Grad Norm 9.4245(4.0947) | Total Time 10.00(10.00)\n",
      "Iter 0368 | Time 26.0293(26.6928) | Bit/dim 1.4114(1.4190) | Xent 2.3026(2.3026) | Loss 1.4114(1.4190) | Error 0.8969(0.9012) Steps 374(383.44) | Grad Norm 9.3052(4.2510) | Total Time 10.00(10.00)\n",
      "Iter 0369 | Time 26.4905(26.6867) | Bit/dim 1.3612(1.4173) | Xent 2.3026(2.3026) | Loss 1.3612(1.4173) | Error 0.9038(0.9013) Steps 386(383.51) | Grad Norm 5.6533(4.2931) | Total Time 10.00(10.00)\n",
      "Iter 0370 | Time 26.8444(26.6914) | Bit/dim 1.3451(1.4151) | Xent 2.3026(2.3026) | Loss 1.3451(1.4151) | Error 0.8998(0.9013) Steps 386(383.59) | Grad Norm 2.6609(4.2442) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0053 | Time 8.8429, Epoch Time 207.7728(219.9339), Bit/dim 1.3363, Xent 2.3026, Loss 1.3363, Error 0.9227\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0371 | Time 26.9698(26.6998) | Bit/dim 1.3518(1.4132) | Xent 2.3026(2.3026) | Loss 1.3518(1.4132) | Error 0.8965(0.9011) Steps 380(383.48) | Grad Norm 3.8693(4.2329) | Total Time 10.00(10.00)\n",
      "Iter 0372 | Time 26.5133(26.6942) | Bit/dim 1.3849(1.4124) | Xent 2.3026(2.3026) | Loss 1.3849(1.4124) | Error 0.9087(0.9013) Steps 386(383.56) | Grad Norm 7.9631(4.3448) | Total Time 10.00(10.00)\n",
      "Iter 0373 | Time 25.6933(26.6641) | Bit/dim 1.3908(1.4117) | Xent 2.3026(2.3026) | Loss 1.3908(1.4117) | Error 0.9019(0.9014) Steps 374(383.27) | Grad Norm 7.5086(4.4397) | Total Time 10.00(10.00)\n",
      "Iter 0374 | Time 25.8766(26.6405) | Bit/dim 1.3324(1.4094) | Xent 2.3026(2.3026) | Loss 1.3324(1.4094) | Error 0.9007(0.9013) Steps 374(382.99) | Grad Norm 1.0979(4.3395) | Total Time 10.00(10.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p train_cnf_conditional_tol_unsup.py --data mnist --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 8000 --test_batch_size 8000 --save experiments/cnf_cond_bs8K_ilr_0_01_tol_1e_m4_unsup --conditional True --lr 0.01 --warmup_iters 113 --atol 1e-4  --rtol 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
