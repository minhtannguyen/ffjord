{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0,1,2,3,4,5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_conditional_sup.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.odenvp_conditional as odenvp\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=True, choices=[True, False])\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"./data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"./data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"./data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"./data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"./data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"./data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            './data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            './data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz = modules.GaussianDiag.logp(mean, logs, z).view(-1,1)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(z)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol},)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    # fixed_y = torch.randint(high=10, size=(100,)).type(torch.long).to(device, non_blocking=True)\n",
      "    fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "    fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "    with torch.no_grad():\n",
      "        mean, logs = model.module._prior(fixed_y_onehot)\n",
      "        fixed_z = modules.GaussianDiag.sample(mean, logs)\n",
      "    \n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    if args.conditional: best_error_score = float(\"inf\")\n",
      "    \n",
      "    itr = 0\n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                loss = loss_xent\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            loss.backward()\n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            xent_meter.update(loss_xent.item())\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits/dim', {'train': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        loss = loss_xent\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses_xent.append(loss_xent.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'validation': time.time() - start}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits/dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}, Xent {:.4f}, Loss {:.4f}, Error {:.4f}\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, loss_xent, loss, error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, conditional=True, conv=True, data='mnist', dims='64,64,64', divergence_fn='approximate', dl2int=None, imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.0001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume='experiments/cnf_cond_bs900_wy_0_5_sup/best_error_checkpt.pth', rtol=1e-05, save='experiments/cnf_cond_bs900_wy_0_5_sup_cont_lr_0_0001', solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=900, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5)\n",
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=1568, bias=True)\n",
      "  (project_class): LinearZeros(in_features=784, out_features=10, bias=True)\n",
      ")\n",
      "Number of trainable parameters: 828890\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc.py:286: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "Iter 0000 | Time 40.8359(40.8359) | Bit/dim 132.6608(132.6608) | Xent 0.0002(0.0002) | Loss 0.0002(0.0002) | Error 0.0000(0.0000) Steps 506(506.00) | Grad Norm 0.0509(0.0509) | Total Time 10.00(10.00)\n",
      "Iter 0010 | Time 15.6629(34.2706) | Bit/dim 132.3462(132.4589) | Xent 0.0017(0.0003) | Loss 0.0017(0.0003) | Error 0.0011(0.0001) Steps 494(503.46) | Grad Norm 0.3270(0.0776) | Total Time 10.00(10.00)\n",
      "Iter 0020 | Time 15.7548(29.4222) | Bit/dim 136.0521(132.4107) | Xent 0.0019(0.0005) | Loss 0.0019(0.0005) | Error 0.0011(0.0001) Steps 500(502.25) | Grad Norm 0.5186(0.1132) | Total Time 10.00(10.00)\n",
      "Iter 0030 | Time 16.0784(25.8545) | Bit/dim 134.5953(132.4923) | Xent 0.0001(0.0005) | Loss 0.0001(0.0005) | Error 0.0000(0.0002) Steps 506(502.58) | Grad Norm 0.0261(0.1195) | Total Time 10.00(10.00)\n",
      "Iter 0040 | Time 15.7191(23.2170) | Bit/dim 134.2578(132.4943) | Xent 0.0004(0.0006) | Loss 0.0004(0.0006) | Error 0.0000(0.0002) Steps 506(501.92) | Grad Norm 0.0749(0.1349) | Total Time 10.00(10.00)\n",
      "Iter 0050 | Time 15.9230(21.2800) | Bit/dim 129.6698(132.3443) | Xent 0.0002(0.0006) | Loss 0.0002(0.0006) | Error 0.0000(0.0002) Steps 500(501.75) | Grad Norm 0.0725(0.1327) | Total Time 10.00(10.00)\n",
      "Iter 0060 | Time 15.8614(19.8376) | Bit/dim 129.5846(132.2675) | Xent 0.0043(0.0006) | Loss 0.0043(0.0006) | Error 0.0011(0.0001) Steps 500(501.45) | Grad Norm 0.4722(0.1290) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0001 | Time 58.7270, Epoch Time 1140.6185(1140.6185), Bit/dim 131.8461, Xent 0.0355, Loss 0.0355, Error 0.0160\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0070 | Time 15.6547(18.7788) | Bit/dim 130.7569(132.2326) | Xent 0.0017(0.0007) | Loss 0.0017(0.0007) | Error 0.0011(0.0002) Steps 500(500.60) | Grad Norm 0.5240(0.1534) | Total Time 10.00(10.00)\n",
      "Iter 0080 | Time 15.9449(18.0287) | Bit/dim 129.3769(132.2658) | Xent 0.0002(0.0006) | Loss 0.0002(0.0006) | Error 0.0000(0.0002) Steps 494(500.27) | Grad Norm 0.0598(0.1357) | Total Time 10.00(10.00)\n",
      "Iter 0090 | Time 15.7537(17.4674) | Bit/dim 131.5637(132.3382) | Xent 0.0006(0.0006) | Loss 0.0006(0.0006) | Error 0.0000(0.0002) Steps 494(499.61) | Grad Norm 0.1391(0.1462) | Total Time 10.00(10.00)\n",
      "Iter 0100 | Time 15.8508(17.0744) | Bit/dim 131.2125(132.5077) | Xent 0.0002(0.0006) | Loss 0.0002(0.0006) | Error 0.0000(0.0002) Steps 506(499.57) | Grad Norm 0.0512(0.1536) | Total Time 10.00(10.00)\n",
      "Iter 0110 | Time 15.9755(16.7562) | Bit/dim 133.6720(132.4690) | Xent 0.0002(0.0007) | Loss 0.0002(0.0007) | Error 0.0000(0.0002) Steps 506(499.56) | Grad Norm 0.0609(0.1481) | Total Time 10.00(10.00)\n",
      "Iter 0120 | Time 15.5712(16.5562) | Bit/dim 136.7118(132.6085) | Xent 0.0002(0.0006) | Loss 0.0002(0.0006) | Error 0.0000(0.0002) Steps 500(499.62) | Grad Norm 0.0470(0.1362) | Total Time 10.00(10.00)\n",
      "Iter 0130 | Time 16.0814(16.3790) | Bit/dim 130.9962(132.6702) | Xent 0.0003(0.0005) | Loss 0.0003(0.0005) | Error 0.0000(0.0002) Steps 500(500.06) | Grad Norm 0.0989(0.1218) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0002 | Time 59.5515, Epoch Time 1123.5785(1140.1073), Bit/dim 132.8342, Xent 0.0352, Loss 0.0352, Error 0.0159\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0140 | Time 15.9342(16.2583) | Bit/dim 131.7009(132.8993) | Xent 0.0000(0.0005) | Loss 0.0000(0.0005) | Error 0.0000(0.0002) Steps 500(499.60) | Grad Norm 0.0157(0.1171) | Total Time 10.00(10.00)\n",
      "Iter 0150 | Time 15.5489(16.1701) | Bit/dim 132.3505(133.2403) | Xent 0.0004(0.0006) | Loss 0.0004(0.0006) | Error 0.0000(0.0002) Steps 494(499.69) | Grad Norm 0.0971(0.1313) | Total Time 10.00(10.00)\n",
      "Iter 0160 | Time 16.0997(16.1268) | Bit/dim 133.1259(133.4781) | Xent 0.0003(0.0005) | Loss 0.0003(0.0005) | Error 0.0000(0.0001) Steps 506(499.96) | Grad Norm 0.1315(0.1236) | Total Time 10.00(10.00)\n",
      "Iter 0170 | Time 15.9621(16.1172) | Bit/dim 133.0547(133.4005) | Xent 0.0009(0.0005) | Loss 0.0009(0.0005) | Error 0.0000(0.0002) Steps 500(500.96) | Grad Norm 0.2346(0.1246) | Total Time 10.00(10.00)\n",
      "Iter 0180 | Time 16.0567(16.0927) | Bit/dim 135.7695(133.6049) | Xent 0.0002(0.0004) | Loss 0.0002(0.0004) | Error 0.0000(0.0001) Steps 506(501.09) | Grad Norm 0.0411(0.1078) | Total Time 10.00(10.00)\n",
      "Iter 0190 | Time 15.9603(16.0841) | Bit/dim 135.9078(134.0669) | Xent 0.0005(0.0004) | Loss 0.0005(0.0004) | Error 0.0000(0.0001) Steps 506(501.49) | Grad Norm 0.0992(0.1010) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0003 | Time 60.0385, Epoch Time 1129.5223(1139.7898), Bit/dim 134.9069, Xent 0.0345, Loss 0.0345, Error 0.0156\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0200 | Time 16.2788(16.0517) | Bit/dim 134.4309(134.5121) | Xent 0.0008(0.0004) | Loss 0.0008(0.0004) | Error 0.0000(0.0001) Steps 506(501.68) | Grad Norm 0.1805(0.0854) | Total Time 10.00(10.00)\n",
      "Iter 0210 | Time 16.1474(16.0440) | Bit/dim 138.0778(134.9419) | Xent 0.0003(0.0003) | Loss 0.0003(0.0003) | Error 0.0000(0.0001) Steps 506(501.11) | Grad Norm 0.0942(0.0881) | Total Time 10.00(10.00)\n",
      "Iter 0220 | Time 16.1629(16.0297) | Bit/dim 138.8016(135.3831) | Xent 0.0001(0.0003) | Loss 0.0001(0.0003) | Error 0.0000(0.0001) Steps 506(500.87) | Grad Norm 0.0385(0.0770) | Total Time 10.00(10.00)\n",
      "Iter 0230 | Time 15.7497(15.9946) | Bit/dim 134.2451(135.4379) | Xent 0.0001(0.0003) | Loss 0.0001(0.0003) | Error 0.0000(0.0001) Steps 506(500.94) | Grad Norm 0.0242(0.0876) | Total Time 10.00(10.00)\n",
      "Iter 0240 | Time 16.1346(15.9817) | Bit/dim 135.1162(135.6573) | Xent 0.0002(0.0003) | Loss 0.0002(0.0003) | Error 0.0000(0.0001) Steps 488(501.13) | Grad Norm 0.0726(0.0800) | Total Time 10.00(10.00)\n",
      "Iter 0250 | Time 16.1755(15.9801) | Bit/dim 135.1000(135.7643) | Xent 0.0004(0.0003) | Loss 0.0004(0.0003) | Error 0.0000(0.0000) Steps 500(501.19) | Grad Norm 0.0863(0.0777) | Total Time 10.00(10.00)\n",
      "Iter 0260 | Time 15.6679(15.9753) | Bit/dim 137.0161(136.2714) | Xent 0.0002(0.0003) | Loss 0.0002(0.0003) | Error 0.0000(0.0000) Steps 500(501.98) | Grad Norm 0.0370(0.0714) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0004 | Time 60.5377, Epoch Time 1127.8323(1139.4310), Bit/dim 137.3428, Xent 0.0341, Loss 0.0341, Error 0.0156\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0270 | Time 16.2263(15.9969) | Bit/dim 137.5477(136.6111) | Xent 0.0001(0.0002) | Loss 0.0001(0.0002) | Error 0.0000(0.0000) Steps 506(502.59) | Grad Norm 0.0394(0.0632) | Total Time 10.00(10.00)\n",
      "Iter 0280 | Time 15.9588(16.0048) | Bit/dim 139.9154(137.1136) | Xent 0.0002(0.0002) | Loss 0.0002(0.0002) | Error 0.0000(0.0000) Steps 506(503.02) | Grad Norm 0.0603(0.0644) | Total Time 10.00(10.00)\n",
      "Iter 0290 | Time 15.9815(15.9847) | Bit/dim 138.5384(137.5465) | Xent 0.0002(0.0002) | Loss 0.0002(0.0002) | Error 0.0000(0.0000) Steps 500(503.18) | Grad Norm 0.0478(0.0659) | Total Time 10.00(10.00)\n",
      "Iter 0300 | Time 16.1455(16.0073) | Bit/dim 139.2598(138.1850) | Xent 0.0002(0.0002) | Loss 0.0002(0.0002) | Error 0.0000(0.0000) Steps 500(502.80) | Grad Norm 0.1075(0.0649) | Total Time 10.00(10.00)\n",
      "Iter 0310 | Time 15.9514(16.0275) | Bit/dim 142.3400(138.6553) | Xent 0.0001(0.0002) | Loss 0.0001(0.0002) | Error 0.0000(0.0000) Steps 500(502.88) | Grad Norm 0.0183(0.0603) | Total Time 10.00(10.00)\n",
      "Iter 0320 | Time 16.0581(16.0385) | Bit/dim 141.8083(139.0105) | Xent 0.0000(0.0002) | Loss 0.0000(0.0002) | Error 0.0000(0.0000) Steps 506(503.05) | Grad Norm 0.0178(0.0511) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0005 | Time 60.7987, Epoch Time 1132.7873(1139.2317), Bit/dim 140.9634, Xent 0.0353, Loss 0.0353, Error 0.0158\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0330 | Time 16.3336(16.0428) | Bit/dim 139.6815(139.5970) | Xent 0.0001(0.0002) | Loss 0.0001(0.0002) | Error 0.0000(0.0000) Steps 506(503.49) | Grad Norm 0.0338(0.0474) | Total Time 10.00(10.00)\n",
      "Iter 0340 | Time 15.9544(16.0320) | Bit/dim 138.8184(139.7384) | Xent 0.0002(0.0002) | Loss 0.0002(0.0002) | Error 0.0000(0.0000) Steps 500(503.01) | Grad Norm 0.0683(0.0463) | Total Time 10.00(10.00)\n",
      "Iter 0350 | Time 15.8399(16.0433) | Bit/dim 141.6689(140.3303) | Xent 0.0004(0.0002) | Loss 0.0004(0.0002) | Error 0.0000(0.0000) Steps 494(503.44) | Grad Norm 0.1272(0.0440) | Total Time 10.00(10.00)\n",
      "Iter 0360 | Time 16.0758(16.0148) | Bit/dim 143.4420(140.9086) | Xent 0.0001(0.0001) | Loss 0.0001(0.0001) | Error 0.0000(0.0000) Steps 506(503.83) | Grad Norm 0.0376(0.0421) | Total Time 10.00(10.00)\n",
      "Iter 0370 | Time 16.0380(16.0022) | Bit/dim 145.6746(141.6350) | Xent 0.0001(0.0001) | Loss 0.0001(0.0001) | Error 0.0000(0.0000) Steps 506(504.23) | Grad Norm 0.0419(0.0428) | Total Time 10.00(10.00)\n",
      "Iter 0380 | Time 15.5141(15.9715) | Bit/dim 142.2815(142.2753) | Xent 0.0001(0.0001) | Loss 0.0001(0.0001) | Error 0.0000(0.0000) Steps 506(504.40) | Grad Norm 0.0244(0.0441) | Total Time 10.00(10.00)\n",
      "Iter 0390 | Time 15.9172(15.9710) | Bit/dim 143.6138(142.7952) | Xent 0.0001(0.0001) | Loss 0.0001(0.0001) | Error 0.0000(0.0000) Steps 494(504.30) | Grad Norm 0.0398(0.0403) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0006 | Time 61.2793, Epoch Time 1129.6335(1138.9438), Bit/dim 144.4813, Xent 0.0339, Loss 0.0339, Error 0.0156\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0400 | Time 16.3804(15.9904) | Bit/dim 142.4295(143.4160) | Xent 0.0001(0.0001) | Loss 0.0001(0.0001) | Error 0.0000(0.0000) Steps 506(503.46) | Grad Norm 0.0231(0.0383) | Total Time 10.00(10.00)\n",
      "Iter 0410 | Time 15.8148(15.9968) | Bit/dim 147.0362(144.2724) | Xent 0.0000(0.0001) | Loss 0.0000(0.0001) | Error 0.0000(0.0000) Steps 506(503.70) | Grad Norm 0.0061(0.0338) | Total Time 10.00(10.00)\n",
      "Iter 0420 | Time 15.8669(16.0187) | Bit/dim 145.3996(144.6191) | Xent 0.0000(0.0001) | Loss 0.0000(0.0001) | Error 0.0000(0.0000) Steps 506(503.88) | Grad Norm 0.0093(0.0318) | Total Time 10.00(10.00)\n",
      "Iter 0430 | Time 16.0914(16.0575) | Bit/dim 148.9675(145.5053) | Xent 0.0001(0.0001) | Loss 0.0001(0.0001) | Error 0.0000(0.0000) Steps 512(504.44) | Grad Norm 0.0162(0.0309) | Total Time 10.00(10.00)\n",
      "Iter 0440 | Time 16.0316(16.0407) | Bit/dim 147.9418(146.1706) | Xent 0.0002(0.0001) | Loss 0.0002(0.0001) | Error 0.0000(0.0000) Steps 506(504.54) | Grad Norm 0.0392(0.0292) | Total Time 10.00(10.00)\n",
      "Iter 0450 | Time 15.9081(16.0320) | Bit/dim 149.0263(146.6169) | Xent 0.0001(0.0001) | Loss 0.0001(0.0001) | Error 0.0000(0.0000) Steps 506(504.29) | Grad Norm 0.0159(0.0274) | Total Time 10.00(10.00)\n",
      "Iter 0460 | Time 16.0498(16.0445) | Bit/dim 146.6170(146.9214) | Xent 0.0001(0.0001) | Loss 0.0001(0.0001) | Error 0.0000(0.0000) Steps 506(504.60) | Grad Norm 0.0225(0.0241) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0007 | Time 60.7139, Epoch Time 1134.5895(1138.8132), Bit/dim 148.8216, Xent 0.0347, Loss 0.0347, Error 0.0157\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0470 | Time 15.8689(16.0108) | Bit/dim 147.7347(147.6126) | Xent 0.0002(0.0001) | Loss 0.0002(0.0001) | Error 0.0000(0.0000) Steps 506(504.82) | Grad Norm 0.0461(0.0235) | Total Time 10.00(10.00)\n",
      "Iter 0480 | Time 16.1411(16.0122) | Bit/dim 151.3230(148.3278) | Xent 0.0000(0.0001) | Loss 0.0000(0.0001) | Error 0.0000(0.0000) Steps 506(504.39) | Grad Norm 0.0067(0.0265) | Total Time 10.00(10.00)\n",
      "Iter 0490 | Time 15.8297(15.9859) | Bit/dim 151.4856(149.2209) | Xent 0.0001(0.0001) | Loss 0.0001(0.0001) | Error 0.0000(0.0000) Steps 506(504.66) | Grad Norm 0.0204(0.0251) | Total Time 10.00(10.00)\n",
      "Iter 0500 | Time 16.2252(15.9978) | Bit/dim 151.6283(149.5692) | Xent 0.0000(0.0001) | Loss 0.0000(0.0001) | Error 0.0000(0.0000) Steps 506(504.85) | Grad Norm 0.0033(0.0258) | Total Time 10.00(10.00)\n",
      "Iter 0510 | Time 15.8926(15.9937) | Bit/dim 152.6768(150.0568) | Xent 0.0000(0.0001) | Loss 0.0000(0.0001) | Error 0.0000(0.0000) Steps 494(504.31) | Grad Norm 0.0118(0.0221) | Total Time 10.00(10.00)\n",
      "Iter 0520 | Time 16.3097(16.0502) | Bit/dim 154.3449(150.8900) | Xent 0.0000(0.0001) | Loss 0.0000(0.0001) | Error 0.0000(0.0000) Steps 500(504.41) | Grad Norm 0.0138(0.0223) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0008 | Time 60.9172, Epoch Time 1131.6493(1138.5982), Bit/dim 153.4893, Xent 0.0348, Loss 0.0348, Error 0.0157\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0530 | Time 15.6956(16.0366) | Bit/dim 156.7674(151.6559) | Xent 0.0000(0.0001) | Loss 0.0000(0.0001) | Error 0.0000(0.0000) Steps 506(504.21) | Grad Norm 0.0063(0.0245) | Total Time 10.00(10.00)\n",
      "Iter 0540 | Time 16.5302(16.0416) | Bit/dim 152.3734(152.2215) | Xent 0.0000(0.0001) | Loss 0.0000(0.0001) | Error 0.0000(0.0000) Steps 506(504.37) | Grad Norm 0.0046(0.0257) | Total Time 10.00(10.00)\n",
      "Iter 0550 | Time 16.0026(16.0595) | Bit/dim 152.9743(152.8189) | Xent 0.0001(0.0001) | Loss 0.0001(0.0001) | Error 0.0000(0.0000) Steps 506(504.79) | Grad Norm 0.0163(0.0241) | Total Time 10.00(10.00)\n",
      "Iter 0560 | Time 15.8378(16.0575) | Bit/dim 153.6298(153.7782) | Xent 0.0001(0.0001) | Loss 0.0001(0.0001) | Error 0.0000(0.0000) Steps 500(504.61) | Grad Norm 0.0244(0.0216) | Total Time 10.00(10.00)\n",
      "Iter 0570 | Time 16.1820(16.0675) | Bit/dim 152.9831(154.4235) | Xent 0.0000(0.0001) | Loss 0.0000(0.0001) | Error 0.0000(0.0000) Steps 506(504.97) | Grad Norm 0.0027(0.0199) | Total Time 10.00(10.00)\n",
      "Iter 0580 | Time 16.1281(16.0805) | Bit/dim 158.8202(155.0907) | Xent 0.0000(0.0001) | Loss 0.0000(0.0001) | Error 0.0000(0.0000) Steps 506(505.24) | Grad Norm 0.0058(0.0174) | Total Time 10.00(10.00)\n",
      "Iter 0590 | Time 16.3123(16.0741) | Bit/dim 160.4250(156.1321) | Xent 0.0000(0.0001) | Loss 0.0000(0.0001) | Error 0.0000(0.0000) Steps 506(505.11) | Grad Norm 0.0013(0.0183) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0009 | Time 61.2678, Epoch Time 1134.8517(1138.4858), Bit/dim 158.5311, Xent 0.0349, Loss 0.0349, Error 0.0159\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0600 | Time 15.9199(16.0525) | Bit/dim 158.6028(156.9573) | Xent 0.0002(0.0001) | Loss 0.0002(0.0001) | Error 0.0000(0.0000) Steps 506(505.04) | Grad Norm 0.0466(0.0211) | Total Time 10.00(10.00)\n",
      "Iter 0610 | Time 15.8723(16.0631) | Bit/dim 159.5183(157.8532) | Xent 0.0001(0.0001) | Loss 0.0001(0.0001) | Error 0.0000(0.0000) Steps 506(505.29) | Grad Norm 0.0348(0.0194) | Total Time 10.00(10.00)\n",
      "Iter 0620 | Time 16.0111(16.0727) | Bit/dim 161.3733(158.5719) | Xent 0.0001(0.0001) | Loss 0.0001(0.0001) | Error 0.0000(0.0000) Steps 506(505.48) | Grad Norm 0.0218(0.0186) | Total Time 10.00(10.00)\n",
      "Iter 0630 | Time 16.3112(16.0677) | Bit/dim 160.1579(159.2395) | Xent 0.0001(0.0001) | Loss 0.0001(0.0001) | Error 0.0000(0.0000) Steps 506(505.47) | Grad Norm 0.0232(0.0168) | Total Time 10.00(10.00)\n",
      "Iter 0640 | Time 16.1519(16.0974) | Bit/dim 161.6281(159.9972) | Xent 0.0000(0.0001) | Loss 0.0000(0.0001) | Error 0.0000(0.0000) Steps 506(505.61) | Grad Norm 0.0204(0.0161) | Total Time 10.00(10.00)\n",
      "Iter 0650 | Time 15.9651(16.0949) | Bit/dim 163.3991(160.6759) | Xent 0.0000(0.0001) | Loss 0.0000(0.0001) | Error 0.0000(0.0000) Steps 506(505.71) | Grad Norm 0.0072(0.0140) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0010 | Time 61.3012, Epoch Time 1137.3705(1138.4524), Bit/dim 163.4182, Xent 0.0356, Loss 0.0356, Error 0.0156\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0660 | Time 16.0578(16.0974) | Bit/dim 161.4683(161.4031) | Xent 0.0001(0.0001) | Loss 0.0001(0.0001) | Error 0.0000(0.0000) Steps 506(505.79) | Grad Norm 0.0177(0.0139) | Total Time 10.00(10.00)\n",
      "Iter 0670 | Time 15.8795(16.0730) | Bit/dim 161.6044(162.3654) | Xent 0.0001(0.0000) | Loss 0.0001(0.0000) | Error 0.0000(0.0000) Steps 506(505.53) | Grad Norm 0.0224(0.0128) | Total Time 10.00(10.00)\n",
      "Iter 0680 | Time 15.8321(16.0731) | Bit/dim 163.5941(162.9067) | Xent 0.0000(0.0000) | Loss 0.0000(0.0000) | Error 0.0000(0.0000) Steps 506(505.52) | Grad Norm 0.0141(0.0128) | Total Time 10.00(10.00)\n",
      "Iter 0690 | Time 16.2019(16.0698) | Bit/dim 166.8689(163.8546) | Xent 0.0001(0.0000) | Loss 0.0001(0.0000) | Error 0.0000(0.0000) Steps 506(505.30) | Grad Norm 0.0261(0.0134) | Total Time 10.00(10.00)\n",
      "Iter 0700 | Time 15.9148(16.0527) | Bit/dim 166.7994(164.3916) | Xent 0.0001(0.0000) | Loss 0.0001(0.0000) | Error 0.0000(0.0000) Steps 506(505.48) | Grad Norm 0.0173(0.0132) | Total Time 10.00(10.00)\n",
      "Iter 0710 | Time 15.8848(16.0570) | Bit/dim 166.1790(165.0386) | Xent 0.0001(0.0000) | Loss 0.0001(0.0000) | Error 0.0000(0.0000) Steps 506(505.79) | Grad Norm 0.0118(0.0121) | Total Time 10.00(10.00)\n",
      "Iter 0720 | Time 16.3676(16.0724) | Bit/dim 167.5176(165.9986) | Xent 0.0000(0.0000) | Loss 0.0000(0.0000) | Error 0.0000(0.0000) Steps 506(505.85) | Grad Norm 0.0085(0.0115) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0011 | Time 61.1366, Epoch Time 1134.7319(1138.3408), Bit/dim 168.8054, Xent 0.0361, Loss 0.0361, Error 0.0153\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0730 | Time 16.1878(16.0904) | Bit/dim 168.5853(166.7066) | Xent 0.0000(0.0000) | Loss 0.0000(0.0000) | Error 0.0000(0.0000) Steps 500(505.71) | Grad Norm 0.0060(0.0117) | Total Time 10.00(10.00)\n",
      "Iter 0740 | Time 15.9949(16.0948) | Bit/dim 170.8718(167.5976) | Xent 0.0000(0.0000) | Loss 0.0000(0.0000) | Error 0.0000(0.0000) Steps 506(505.78) | Grad Norm 0.0036(0.0107) | Total Time 10.00(10.00)\n",
      "Iter 0750 | Time 16.1110(16.1227) | Bit/dim 173.0434(168.3976) | Xent 0.0000(0.0000) | Loss 0.0000(0.0000) | Error 0.0000(0.0000) Steps 506(505.84) | Grad Norm 0.0057(0.0119) | Total Time 10.00(10.00)\n",
      "Iter 0760 | Time 15.9969(16.1162) | Bit/dim 174.0902(169.6327) | Xent 0.0001(0.0000) | Loss 0.0001(0.0000) | Error 0.0000(0.0000) Steps 506(505.88) | Grad Norm 0.0137(0.0114) | Total Time 10.00(10.00)\n",
      "Iter 0770 | Time 16.2723(16.1516) | Bit/dim 171.7656(170.2749) | Xent 0.0001(0.0000) | Loss 0.0001(0.0000) | Error 0.0000(0.0000) Steps 506(505.75) | Grad Norm 0.0227(0.0112) | Total Time 10.00(10.00)\n",
      "Iter 0780 | Time 16.2124(16.1734) | Bit/dim 169.9915(170.9207) | Xent 0.0001(0.0000) | Loss 0.0001(0.0000) | Error 0.0000(0.0000) Steps 506(505.68) | Grad Norm 0.0632(0.0117) | Total Time 10.00(10.00)\n",
      "Iter 0790 | Time 16.0278(16.1584) | Bit/dim 174.5849(171.6000) | Xent 0.0001(0.0000) | Loss 0.0001(0.0000) | Error 0.0000(0.0000) Steps 506(505.76) | Grad Norm 0.0231(0.0122) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0012 | Time 61.4449, Epoch Time 1142.0225(1138.4512), Bit/dim 174.1445, Xent 0.0359, Loss 0.0359, Error 0.0148\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0800 | Time 15.8518(16.1278) | Bit/dim 175.7672(172.4066) | Xent 0.0000(0.0000) | Loss 0.0000(0.0000) | Error 0.0000(0.0000) Steps 506(505.82) | Grad Norm 0.0030(0.0117) | Total Time 10.00(10.00)\n",
      "Iter 0810 | Time 16.3661(16.1271) | Bit/dim 176.9633(173.2389) | Xent 0.0000(0.0000) | Loss 0.0000(0.0000) | Error 0.0000(0.0000) Steps 506(505.87) | Grad Norm 0.0120(0.0123) | Total Time 10.00(10.00)\n",
      "Iter 0820 | Time 16.0128(16.1552) | Bit/dim 176.2048(173.8840) | Xent 0.0000(0.0000) | Loss 0.0000(0.0000) | Error 0.0000(0.0000) Steps 506(505.90) | Grad Norm 0.0027(0.0117) | Total Time 10.00(10.00)\n",
      "Iter 0830 | Time 16.0559(16.1525) | Bit/dim 179.3001(174.5727) | Xent 0.0001(0.0000) | Loss 0.0001(0.0000) | Error 0.0000(0.0000) Steps 506(505.93) | Grad Norm 0.0110(0.0102) | Total Time 10.00(10.00)\n",
      "Iter 0840 | Time 16.2057(16.1372) | Bit/dim 182.9797(176.0754) | Xent 0.0000(0.0000) | Loss 0.0000(0.0000) | Error 0.0000(0.0000) Steps 506(506.09) | Grad Norm 0.0041(0.0106) | Total Time 10.00(10.00)\n",
      "Iter 0850 | Time 15.9409(16.1018) | Bit/dim 178.1087(176.7653) | Xent 0.0000(0.0000) | Loss 0.0000(0.0000) | Error 0.0000(0.0000) Steps 506(506.07) | Grad Norm 0.0019(0.0091) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0013 | Time 61.7988, Epoch Time 1138.2163(1138.4442), Bit/dim 179.7131, Xent 0.0379, Loss 0.0379, Error 0.0156\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0860 | Time 16.1581(16.0925) | Bit/dim 182.3755(177.6645) | Xent 0.0000(0.0000) | Loss 0.0000(0.0000) | Error 0.0000(0.0000) Steps 506(506.05) | Grad Norm 0.0047(0.0090) | Total Time 10.00(10.00)\n",
      "Iter 0870 | Time 16.2186(16.1241) | Bit/dim 182.7119(178.2550) | Xent 0.0000(0.0000) | Loss 0.0000(0.0000) | Error 0.0000(0.0000) Steps 506(506.32) | Grad Norm 0.0037(0.0087) | Total Time 10.00(10.00)\n",
      "Iter 0880 | Time 16.0054(16.1210) | Bit/dim 186.0667(179.4048) | Xent 0.0000(0.0000) | Loss 0.0000(0.0000) | Error 0.0000(0.0000) Steps 506(506.24) | Grad Norm 0.0036(0.0082) | Total Time 10.00(10.00)\n",
      "Iter 0890 | Time 16.1018(16.1196) | Bit/dim 182.7036(180.2114) | Xent 0.0000(0.0000) | Loss 0.0000(0.0000) | Error 0.0000(0.0000) Steps 506(506.18) | Grad Norm 0.0018(0.0079) | Total Time 10.00(10.00)\n",
      "Iter 0900 | Time 16.2534(16.1441) | Bit/dim 180.9999(181.0283) | Xent 0.0000(0.0000) | Loss 0.0000(0.0000) | Error 0.0000(0.0000) Steps 506(506.13) | Grad Norm 0.0164(0.0076) | Total Time 10.00(10.00)\n",
      "Iter 0910 | Time 16.3520(16.1349) | Bit/dim 179.3750(181.7752) | Xent 0.0000(0.0000) | Loss 0.0000(0.0000) | Error 0.0000(0.0000) Steps 506(506.10) | Grad Norm 0.0152(0.0078) | Total Time 10.00(10.00)\n",
      "Iter 0920 | Time 16.0988(16.1288) | Bit/dim 185.5761(182.7677) | Xent 0.0000(0.0000) | Loss 0.0000(0.0000) | Error 0.0000(0.0000) Steps 506(506.07) | Grad Norm 0.0035(0.0080) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0014 | Time 61.6795, Epoch Time 1141.0709(1138.5230), Bit/dim 185.2372, Xent 0.0371, Loss 0.0371, Error 0.0156\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0930 | Time 16.3013(16.1530) | Bit/dim 188.9938(183.6990) | Xent 0.0000(0.0000) | Loss 0.0000(0.0000) | Error 0.0000(0.0000) Steps 506(506.05) | Grad Norm 0.0040(0.0078) | Total Time 10.00(10.00)\n",
      "Iter 0940 | Time 16.2098(16.1747) | Bit/dim 189.4512(184.5481) | Xent 0.0000(0.0000) | Loss 0.0000(0.0000) | Error 0.0000(0.0000) Steps 506(506.04) | Grad Norm 0.0014(0.0076) | Total Time 10.00(10.00)\n",
      "Iter 0950 | Time 15.9105(16.1728) | Bit/dim 185.8208(185.2011) | Xent 0.0000(0.0000) | Loss 0.0000(0.0000) | Error 0.0000(0.0000) Steps 506(506.03) | Grad Norm 0.0031(0.0077) | Total Time 10.00(10.00)\n",
      "Iter 0960 | Time 16.3088(16.1832) | Bit/dim 187.5348(185.7619) | Xent 0.0000(0.0000) | Loss 0.0000(0.0000) | Error 0.0000(0.0000) Steps 506(506.02) | Grad Norm 0.0008(0.0068) | Total Time 10.00(10.00)\n",
      "Iter 0970 | Time 16.1664(16.1910) | Bit/dim 189.4450(186.6277) | Xent 0.0000(0.0000) | Loss 0.0000(0.0000) | Error 0.0000(0.0000) Steps 506(506.02) | Grad Norm 0.0022(0.0065) | Total Time 10.00(10.00)\n",
      "Iter 0980 | Time 15.9140(16.1496) | Bit/dim 191.0062(187.5326) | Xent 0.0000(0.0000) | Loss 0.0000(0.0000) | Error 0.0000(0.0000) Steps 506(506.01) | Grad Norm 0.0030(0.0075) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0015 | Time 61.6387, Epoch Time 1142.4063(1138.6395), Bit/dim 190.7251, Xent 0.0377, Loss 0.0377, Error 0.0156\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0990 | Time 16.1156(16.1498) | Bit/dim 191.2995(188.3855) | Xent 0.0000(0.0000) | Loss 0.0000(0.0000) | Error 0.0000(0.0000) Steps 506(506.01) | Grad Norm 0.0020(0.0069) | Total Time 10.00(10.00)\n",
      "Iter 1000 | Time 16.2500(16.1726) | Bit/dim 190.8881(189.2306) | Xent 0.0000(0.0000) | Loss 0.0000(0.0000) | Error 0.0000(0.0000) Steps 506(506.01) | Grad Norm 0.0039(0.0071) | Total Time 10.00(10.00)\n",
      "Iter 1010 | Time 16.1482(16.1649) | Bit/dim 193.7954(190.2661) | Xent 0.0000(0.0000) | Loss 0.0000(0.0000) | Error 0.0000(0.0000) Steps 506(506.00) | Grad Norm 0.0039(0.0082) | Total Time 10.00(10.00)\n",
      "Iter 1020 | Time 16.5514(16.1719) | Bit/dim 190.6419(190.9843) | Xent 0.0000(0.0000) | Loss 0.0000(0.0000) | Error 0.0000(0.0000) Steps 506(506.00) | Grad Norm 0.0044(0.0074) | Total Time 10.00(10.00)\n",
      "Iter 1030 | Time 16.0437(16.1613) | Bit/dim 194.0709(191.8240) | Xent 0.0000(0.0000) | Loss 0.0000(0.0000) | Error 0.0000(0.0000) Steps 506(506.15) | Grad Norm 0.0060(0.0067) | Total Time 10.00(10.00)\n",
      "Iter 1040 | Time 15.9969(16.1345) | Bit/dim 189.1234(192.7232) | Xent 0.0000(0.0000) | Loss 0.0000(0.0000) | Error 0.0000(0.0000) Steps 506(506.11) | Grad Norm 0.0048(0.0062) | Total Time 10.00(10.00)\n",
      "Iter 1050 | Time 16.1120(16.1383) | Bit/dim 199.4341(193.4950) | Xent 0.0000(0.0000) | Loss 0.0000(0.0000) | Error 0.0000(0.0000) Steps 506(506.08) | Grad Norm 0.0023(0.0054) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0016 | Time 61.7966, Epoch Time 1141.7234(1138.7320), Bit/dim 196.2551, Xent 0.0379, Loss 0.0379, Error 0.0152\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1060 | Time 16.0506(16.1330) | Bit/dim 200.0852(194.5178) | Xent 0.0000(0.0000) | Loss 0.0000(0.0000) | Error 0.0000(0.0000) Steps 506(506.38) | Grad Norm 0.0058(0.0054) | Total Time 10.00(10.00)\n",
      "Iter 1070 | Time 16.0414(16.1270) | Bit/dim 197.8879(195.2972) | Xent 0.0000(0.0000) | Loss 0.0000(0.0000) | Error 0.0000(0.0000) Steps 506(506.28) | Grad Norm 0.0043(0.0052) | Total Time 10.00(10.00)\n",
      "Iter 1080 | Time 16.1392(16.1307) | Bit/dim 198.9301(196.0839) | Xent 0.0000(0.0000) | Loss 0.0000(0.0000) | Error 0.0000(0.0000) Steps 506(506.21) | Grad Norm 0.0010(0.0051) | Total Time 10.00(10.00)\n",
      "Iter 1090 | Time 15.9752(16.1290) | Bit/dim 195.7332(196.6388) | Xent 0.0000(0.0000) | Loss 0.0000(0.0000) | Error 0.0000(0.0000) Steps 506(506.15) | Grad Norm 0.0020(0.0065) | Total Time 10.00(10.00)\n",
      "Iter 1100 | Time 16.3007(16.1302) | Bit/dim 196.4980(197.1784) | Xent 0.0000(0.0000) | Loss 0.0000(0.0000) | Error 0.0000(0.0000) Steps 506(506.11) | Grad Norm 0.0033(0.0065) | Total Time 10.00(10.00)\n",
      "Iter 1110 | Time 16.1772(16.1265) | Bit/dim 202.1177(197.8397) | Xent 0.0000(0.0000) | Loss 0.0000(0.0000) | Error 0.0000(0.0000) Steps 506(506.22) | Grad Norm 0.0010(0.0057) | Total Time 10.00(10.00)\n",
      "Iter 1120 | Time 16.1025(16.0835) | Bit/dim 201.2311(198.4268) | Xent 0.0000(0.0000) | Loss 0.0000(0.0000) | Error 0.0000(0.0000) Steps 506(506.16) | Grad Norm 0.0084(0.0054) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0017 | Time 61.4084, Epoch Time 1137.1329(1138.6840), Bit/dim 200.5197, Xent 0.0381, Loss 0.0381, Error 0.0151\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1130 | Time 16.0841(16.1117) | Bit/dim 203.3301(199.0798) | Xent 0.0001(0.0000) | Loss 0.0001(0.0000) | Error 0.0000(0.0000) Steps 506(506.12) | Grad Norm 0.0278(0.0053) | Total Time 10.00(10.00)\n",
      "Iter 1140 | Time 16.2092(16.1412) | Bit/dim 203.3324(199.7696) | Xent 0.0000(0.0000) | Loss 0.0000(0.0000) | Error 0.0000(0.0000) Steps 506(506.23) | Grad Norm 0.0126(0.0058) | Total Time 10.00(10.00)\n",
      "Iter 1150 | Time 15.9209(16.1100) | Bit/dim 200.7391(200.4170) | Xent 0.0000(0.0000) | Loss 0.0000(0.0000) | Error 0.0000(0.0000) Steps 506(506.17) | Grad Norm 0.0045(0.0055) | Total Time 10.00(10.00)\n",
      "Iter 1160 | Time 16.0831(16.1255) | Bit/dim 204.6872(201.4200) | Xent 0.0000(0.0000) | Loss 0.0000(0.0000) | Error 0.0000(0.0000) Steps 506(506.12) | Grad Norm 0.0029(0.0051) | Total Time 10.00(10.00)\n",
      "Iter 1170 | Time 16.0929(16.1609) | Bit/dim 203.9018(202.1313) | Xent 0.0000(0.0000) | Loss 0.0000(0.0000) | Error 0.0000(0.0000) Steps 506(506.09) | Grad Norm 0.0029(0.0053) | Total Time 10.00(10.00)\n",
      "Iter 1180 | Time 16.0233(16.1729) | Bit/dim 208.4786(203.0987) | Xent 0.0000(0.0000) | Loss 0.0000(0.0000) | Error 0.0000(0.0000) Steps 506(506.07) | Grad Norm 0.0029(0.0053) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0018 | Time 61.3186, Epoch Time 1142.4681(1138.7975), Bit/dim 206.0405, Xent 0.0392, Loss 0.0392, Error 0.0156\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1190 | Time 16.1812(16.1677) | Bit/dim 206.3428(204.0753) | Xent 0.0000(0.0000) | Loss 0.0000(0.0000) | Error 0.0000(0.0000) Steps 506(506.05) | Grad Norm 0.0010(0.0052) | Total Time 10.00(10.00)\n",
      "Iter 1200 | Time 16.3140(16.1452) | Bit/dim 209.6911(204.9640) | Xent 0.0000(0.0000) | Loss 0.0000(0.0000) | Error 0.0000(0.0000) Steps 506(506.21) | Grad Norm 0.0019(0.0045) | Total Time 10.00(10.00)\n",
      "Iter 1210 | Time 16.2043(16.1339) | Bit/dim 204.7641(205.2951) | Xent 0.0000(0.0000) | Loss 0.0000(0.0000) | Error 0.0000(0.0000) Steps 506(506.15) | Grad Norm 0.0052(0.0043) | Total Time 10.00(10.00)\n",
      "Iter 1220 | Time 15.9553(16.1426) | Bit/dim 210.2466(206.2043) | Xent 0.0000(0.0000) | Loss 0.0000(0.0000) | Error 0.0000(0.0000) Steps 506(506.26) | Grad Norm 0.0031(0.0042) | Total Time 10.00(10.00)\n",
      "Iter 1230 | Time 16.3910(16.1496) | Bit/dim 211.5569(206.4889) | Xent 0.0001(0.0000) | Loss 0.0001(0.0000) | Error 0.0000(0.0000) Steps 506(506.19) | Grad Norm 0.0218(0.0047) | Total Time 10.00(10.00)\n",
      "Iter 1240 | Time 16.2766(16.1790) | Bit/dim 211.9987(207.2741) | Xent 0.0000(0.0000) | Loss 0.0000(0.0000) | Error 0.0000(0.0000) Steps 506(506.14) | Grad Norm 0.0011(0.0042) | Total Time 10.00(10.00)\n",
      "Iter 1250 | Time 16.2159(16.1813) | Bit/dim 210.8337(208.2225) | Xent 0.0000(0.0000) | Loss 0.0000(0.0000) | Error 0.0000(0.0000) Steps 506(506.10) | Grad Norm 0.0022(0.0045) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0019 | Time 61.5220, Epoch Time 1141.6292(1138.8825), Bit/dim 210.6025, Xent 0.0388, Loss 0.0388, Error 0.0152\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1260 | Time 16.1778(16.1601) | Bit/dim 210.0840(208.5815) | Xent 0.0000(0.0000) | Loss 0.0000(0.0000) | Error 0.0000(0.0000) Steps 506(506.08) | Grad Norm 0.0026(0.0043) | Total Time 10.00(10.00)\n",
      "Iter 1270 | Time 16.0440(16.1500) | Bit/dim 210.0836(209.3764) | Xent 0.0000(0.0000) | Loss 0.0000(0.0000) | Error 0.0000(0.0000) Steps 506(506.06) | Grad Norm 0.0029(0.0045) | Total Time 10.00(10.00)\n",
      "Iter 1280 | Time 15.9851(16.1253) | Bit/dim 212.1255(210.2916) | Xent 0.0000(0.0000) | Loss 0.0000(0.0000) | Error 0.0000(0.0000) Steps 506(506.20) | Grad Norm 0.0035(0.0044) | Total Time 10.00(10.00)\n",
      "Iter 1290 | Time 16.6716(16.1476) | Bit/dim 215.1134(211.2442) | Xent 0.0000(0.0000) | Loss 0.0000(0.0000) | Error 0.0000(0.0000) Steps 506(506.30) | Grad Norm 0.0023(0.0042) | Total Time 10.00(10.00)\n",
      "Iter 1300 | Time 16.1696(16.1492) | Bit/dim 215.4356(212.1507) | Xent 0.0000(0.0000) | Loss 0.0000(0.0000) | Error 0.0000(0.0000) Steps 506(506.22) | Grad Norm 0.0055(0.0049) | Total Time 10.00(10.00)\n",
      "Iter 1310 | Time 16.2275(16.1698) | Bit/dim 216.1107(213.1005) | Xent 0.0000(0.0000) | Loss 0.0000(0.0000) | Error 0.0000(0.0000) Steps 506(506.31) | Grad Norm 0.0054(0.0046) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0020 | Time 61.9317, Epoch Time 1141.5259(1138.9618), Bit/dim 216.1218, Xent 0.0397, Loss 0.0397, Error 0.0154\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1320 | Time 16.1150(16.1711) | Bit/dim 220.2189(214.1289) | Xent 0.0000(0.0000) | Loss 0.0000(0.0000) | Error 0.0000(0.0000) Steps 506(506.23) | Grad Norm 0.0017(0.0041) | Total Time 10.00(10.00)\n",
      "Iter 1330 | Time 16.0422(16.1431) | Bit/dim 214.6979(214.8082) | Xent 0.0000(0.0000) | Loss 0.0000(0.0000) | Error 0.0000(0.0000) Steps 506(506.49) | Grad Norm 0.0084(0.0041) | Total Time 10.00(10.00)\n",
      "Iter 1340 | Time 15.9004(16.1435) | Bit/dim 217.2056(215.4300) | Xent 0.0000(0.0000) | Loss 0.0000(0.0000) | Error 0.0000(0.0000) Steps 506(506.36) | Grad Norm 0.0027(0.0037) | Total Time 10.00(10.00)\n",
      "Iter 1350 | Time 16.0869(16.1186) | Bit/dim 215.4488(215.9985) | Xent 0.0000(0.0000) | Loss 0.0000(0.0000) | Error 0.0000(0.0000) Steps 506(506.41) | Grad Norm 0.0045(0.0039) | Total Time 10.00(10.00)\n",
      "Iter 1360 | Time 16.3819(16.1406) | Bit/dim 219.0392(217.1192) | Xent 0.0000(0.0000) | Loss 0.0000(0.0000) | Error 0.0000(0.0000) Steps 512(506.48) | Grad Norm 0.0053(0.0035) | Total Time 10.00(10.00)\n",
      "Iter 1370 | Time 16.0980(16.1256) | Bit/dim 220.5399(217.6189) | Xent 0.0000(0.0000) | Loss 0.0000(0.0000) | Error 0.0000(0.0000) Steps 506(506.51) | Grad Norm 0.0015(0.0035) | Total Time 10.00(10.00)\n",
      "Iter 1380 | Time 16.2345(16.1406) | Bit/dim 218.3333(218.0728) | Xent 0.0000(0.0000) | Loss 0.0000(0.0000) | Error 0.0000(0.0000) Steps 506(506.53) | Grad Norm 0.0038(0.0033) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0021 | Time 62.0258, Epoch Time 1139.6159(1138.9814), Bit/dim 220.3874, Xent 0.0398, Loss 0.0398, Error 0.0152\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1390 | Time 15.9965(16.1389) | Bit/dim 225.0512(218.6866) | Xent 0.0000(0.0000) | Loss 0.0000(0.0000) | Error 0.0000(0.0000) Steps 506(506.56) | Grad Norm 0.0065(0.0031) | Total Time 10.00(10.00)\n",
      "Iter 1400 | Time 16.0433(16.1431) | Bit/dim 216.0485(219.0331) | Xent 0.0000(0.0000) | Loss 0.0000(0.0000) | Error 0.0000(0.0000) Steps 506(506.41) | Grad Norm 0.0015(0.0031) | Total Time 10.00(10.00)\n",
      "Iter 1410 | Time 16.4915(16.1888) | Bit/dim 222.0028(219.7588) | Xent 0.0000(0.0000) | Loss 0.0000(0.0000) | Error 0.0000(0.0000) Steps 506(506.30) | Grad Norm 0.0026(0.0035) | Total Time 10.00(10.00)\n",
      "Iter 1420 | Time 16.2047(16.2002) | Bit/dim 221.4203(220.5697) | Xent 0.0000(0.0000) | Loss 0.0000(0.0000) | Error 0.0000(0.0000) Steps 506(506.38) | Grad Norm 0.0008(0.0030) | Total Time 10.00(10.00)\n",
      "Iter 1430 | Time 16.1798(16.1969) | Bit/dim 223.6016(221.6649) | Xent 0.0000(0.0000) | Loss 0.0000(0.0000) | Error 0.0000(0.0000) Steps 506(506.80) | Grad Norm 0.0033(0.0028) | Total Time 10.00(10.00)\n",
      "Iter 1440 | Time 16.4656(16.2011) | Bit/dim 223.6144(221.9747) | Xent 0.0000(0.0000) | Loss 0.0000(0.0000) | Error 0.0000(0.0000) Steps 506(506.75) | Grad Norm 0.0040(0.0035) | Total Time 10.00(10.00)\n",
      "Iter 1450 | Time 16.1776(16.2045) | Bit/dim 229.3030(223.2173) | Xent 0.0000(0.0000) | Loss 0.0000(0.0000) | Error 0.0000(0.0000) Steps 506(506.89) | Grad Norm 0.0010(0.0032) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0022 | Time 61.3442, Epoch Time 1144.8091(1139.1563), Bit/dim 224.9285, Xent 0.0410, Loss 0.0410, Error 0.0155\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1460 | Time 15.9093(16.1689) | Bit/dim 226.9541(223.5389) | Xent 0.0000(0.0000) | Loss 0.0000(0.0000) | Error 0.0000(0.0000) Steps 506(506.82) | Grad Norm 0.0026(0.0038) | Total Time 10.00(10.00)\n",
      "Iter 1470 | Time 16.0724(16.1522) | Bit/dim 223.6487(224.0562) | Xent 0.0000(0.0000) | Loss 0.0000(0.0000) | Error 0.0000(0.0000) Steps 506(506.60) | Grad Norm 0.0006(0.0034) | Total Time 10.00(10.00)\n",
      "Iter 1480 | Time 16.0492(16.1501) | Bit/dim 229.1936(224.7123) | Xent 0.0000(0.0000) | Loss 0.0000(0.0000) | Error 0.0000(0.0000) Steps 506(506.62) | Grad Norm 0.0017(0.0032) | Total Time 10.00(10.00)\n",
      "Iter 1490 | Time 16.2395(16.1420) | Bit/dim 228.1485(225.4241) | Xent 0.0000(0.0000) | Loss 0.0000(0.0000) | Error 0.0000(0.0000) Steps 506(506.62) | Grad Norm 0.0009(0.0029) | Total Time 10.00(10.00)\n",
      "Iter 1500 | Time 16.1581(16.1380) | Bit/dim 225.7615(226.5291) | Xent 0.0000(0.0000) | Loss 0.0000(0.0000) | Error 0.0000(0.0000) Steps 506(506.62) | Grad Norm 0.0025(0.0028) | Total Time 10.00(10.00)\n",
      "Iter 1510 | Time 15.9953(16.1545) | Bit/dim 229.0431(227.5746) | Xent 0.0000(0.0000) | Loss 0.0000(0.0000) | Error 0.0000(0.0000) Steps 512(507.43) | Grad Norm 0.0047(0.0029) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0023 | Time 61.4251, Epoch Time 1139.6509(1139.1711), Bit/dim 229.4016, Xent 0.0403, Loss 0.0403, Error 0.0154\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1520 | Time 15.8642(16.1444) | Bit/dim 233.0241(228.3623) | Xent 0.0000(0.0000) | Loss 0.0000(0.0000) | Error 0.0000(0.0000) Steps 506(507.06) | Grad Norm 0.0015(0.0027) | Total Time 10.00(10.00)\n",
      "Iter 1530 | Time 16.2304(16.1360) | Bit/dim 233.8162(228.8459) | Xent 0.0000(0.0000) | Loss 0.0000(0.0000) | Error 0.0000(0.0000) Steps 506(506.78) | Grad Norm 0.0041(0.0024) | Total Time 10.00(10.00)\n",
      "Iter 1540 | Time 16.1455(16.1581) | Bit/dim 234.8632(229.4777) | Xent 0.0000(0.0000) | Loss 0.0000(0.0000) | Error 0.0000(0.0000) Steps 506(506.73) | Grad Norm 0.0019(0.0025) | Total Time 10.00(10.00)\n",
      "Iter 1550 | Time 16.3774(16.1908) | Bit/dim 233.1919(229.8645) | Xent 0.0000(0.0000) | Loss 0.0000(0.0000) | Error 0.0000(0.0000) Steps 506(506.70) | Grad Norm 0.0012(0.0025) | Total Time 10.00(10.00)\n",
      "Iter 1560 | Time 16.2275(16.2011) | Bit/dim 232.8941(230.2578) | Xent 0.0000(0.0000) | Loss 0.0000(0.0000) | Error 0.0000(0.0000) Steps 512(506.85) | Grad Norm 0.0010(0.0024) | Total Time 10.00(10.00)\n",
      "Iter 1570 | Time 16.1503(16.2102) | Bit/dim 232.9880(230.5145) | Xent 0.0000(0.0000) | Loss 0.0000(0.0000) | Error 0.0000(0.0000) Steps 506(506.63) | Grad Norm 0.0044(0.0028) | Total Time 10.00(10.00)\n",
      "Iter 1580 | Time 16.0444(16.2139) | Bit/dim 233.3399(231.3994) | Xent 0.0000(0.0000) | Loss 0.0000(0.0000) | Error 0.0000(0.0000) Steps 506(506.46) | Grad Norm 0.0023(0.0027) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0024 | Time 61.5364, Epoch Time 1144.9432(1139.3443), Bit/dim 233.4221, Xent 0.0411, Loss 0.0411, Error 0.0155\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1590 | Time 16.4044(16.1964) | Bit/dim 238.6069(232.3728) | Xent 0.0000(0.0000) | Loss 0.0000(0.0000) | Error 0.0000(0.0000) Steps 506(506.66) | Grad Norm 0.0008(0.0026) | Total Time 10.00(10.00)\n",
      "Iter 1600 | Time 16.4196(16.2075) | Bit/dim 236.4750(233.5330) | Xent 0.0000(0.0000) | Loss 0.0000(0.0000) | Error 0.0000(0.0000) Steps 512(506.98) | Grad Norm 0.0013(0.0023) | Total Time 10.00(10.00)\n",
      "Iter 1610 | Time 16.1129(16.2218) | Bit/dim 235.8232(233.8412) | Xent 0.0000(0.0000) | Loss 0.0000(0.0000) | Error 0.0000(0.0000) Steps 506(506.72) | Grad Norm 0.0029(0.0025) | Total Time 10.00(10.00)\n",
      "Iter 1620 | Time 16.3664(16.1947) | Bit/dim 235.0941(234.2535) | Xent 0.0000(0.0000) | Loss 0.0000(0.0000) | Error 0.0000(0.0000) Steps 512(506.86) | Grad Norm 0.0033(0.0025) | Total Time 10.00(10.00)\n",
      "Iter 1630 | Time 16.2075(16.1952) | Bit/dim 236.9296(234.9370) | Xent 0.0000(0.0000) | Loss 0.0000(0.0000) | Error 0.0000(0.0000) Steps 512(507.27) | Grad Norm 0.0010(0.0024) | Total Time 10.00(10.00)\n",
      "Iter 1640 | Time 16.1408(16.1893) | Bit/dim 239.8530(235.3788) | Xent 0.0000(0.0000) | Loss 0.0000(0.0000) | Error 0.0000(0.0000) Steps 506(507.68) | Grad Norm 0.0041(0.0030) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0025 | Time 61.7401, Epoch Time 1143.9568(1139.4826), Bit/dim 237.9792, Xent 0.0421, Loss 0.0421, Error 0.0152\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1650 | Time 16.2782(16.1979) | Bit/dim 240.6892(235.8527) | Xent 0.0000(0.0000) | Loss 0.0000(0.0000) | Error 0.0000(0.0000) Steps 506(507.40) | Grad Norm 0.0068(0.0028) | Total Time 10.00(10.00)\n",
      "Iter 1660 | Time 16.0831(16.1645) | Bit/dim 238.1822(236.3076) | Xent 0.0000(0.0000) | Loss 0.0000(0.0000) | Error 0.0000(0.0000) Steps 506(507.17) | Grad Norm 0.0025(0.0029) | Total Time 10.00(10.00)\n",
      "Iter 1670 | Time 15.9884(16.1596) | Bit/dim 239.2938(237.3902) | Xent 0.0000(0.0000) | Loss 0.0000(0.0000) | Error 0.0000(0.0000) Steps 506(507.48) | Grad Norm 0.0031(0.0026) | Total Time 10.00(10.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p train_cnf_conditional_sup.py --data mnist --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 900 --save experiments/cnf_cond_bs900_wy_0_5_sup_cont_lr_0_0001 --resume experiments/cnf_cond_bs900_wy_0_5_sup/best_error_checkpt.pth --lr 1e-4 --conditional True --log_freq 10 --weight_y 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
