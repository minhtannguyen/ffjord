{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0,1,2,3,4,5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_conditional_gate.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.odenvp_conditional_gate as odenvp\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument('--scale', type=float, default=1.0)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=True, choices=[True, False])\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"./data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"./data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"./data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"./data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"./data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"./data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            './data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            './data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim, atol, rtol\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol = model(x, zero)  # run model forward\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz = modules.GaussianDiag.logp(mean, logs, z).view(-1,1)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(z)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted, atol, rtol\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol, 'scale': args.scale},)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z = modules.GaussianDiag.sample(mean, logs)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    if args.conditional: best_error_score = float(\"inf\")\n",
      "    \n",
      "    itr = 0\n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted, atol, rtol = compute_bits_per_dim_conditional(x, y, model)\n",
      "                loss =  loss_nll + args.weight_y * loss_xent\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss, atol, rtol = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            loss.backward()\n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits/dim', {'train': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'train': atol[tol_indx].mean()}, itr)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'train': rtol[tol_indx].mean()}, itr)\n",
      "                                \n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted, atol, rtol = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss, atol, rtol = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                        \n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'validation': time.time() - start}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits/dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}, Xent {:.4f}, Loss {:.4f}, Error {:.4f}\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, loss_xent, loss, error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples, atol, rtol = model(fixed_z, reverse=True)\n",
      "            generated_samples = generated_samples.view(-1, *data_shape)\n",
      "            for tol_indx in range(len(atol)):\n",
      "                writer.add_scalars('atol_gen_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                writer.add_scalars('rtol_gen_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, conditional=False, conv=True, data='mnist', dims='64,64,64', divergence_fn='approximate', dl2int=None, imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume=None, rtol=1e-05, save='experiments/cnf_bs900_gate_dev_scale_1e_3', scale=0.001, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=900, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.0)\n",
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(4, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(4, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(2, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(2, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(8, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(8, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(4, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(4, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=1568, bias=True)\n",
      "  (project_class): LinearZeros(in_features=784, out_features=10, bias=True)\n",
      ")\n",
      "Number of trainable parameters: 841930\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc.py:286: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Iter 0000 | Time 35.8398(35.8398) | Bit/dim 31.1951(31.1951) | Xent 0.0000(0.0000) | Loss 31.1951(31.1951) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 230.5194(230.5194) | Total Time 0.00(0.00)\n",
      "Iter 0010 | Time 9.1007(28.8550) | Bit/dim 28.4752(30.8873) | Xent 0.0000(0.0000) | Loss 28.4752(30.8873) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 210.6318(228.3888) | Total Time 0.00(0.00)\n",
      "Iter 0020 | Time 9.4687(23.7054) | Bit/dim 22.3543(29.3737) | Xent 0.0000(0.0000) | Loss 22.3543(29.3737) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 163.2354(216.9319) | Total Time 0.00(0.00)\n",
      "Iter 0030 | Time 9.2944(19.9200) | Bit/dim 16.2732(26.5730) | Xent 0.0000(0.0000) | Loss 16.2732(26.5730) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 100.6931(193.5531) | Total Time 0.00(0.00)\n",
      "Iter 0040 | Time 9.4151(17.1274) | Bit/dim 12.4546(23.2247) | Xent 0.0000(0.0000) | Loss 12.4546(23.2247) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 46.6347(160.7516) | Total Time 0.00(0.00)\n",
      "Iter 0050 | Time 9.3450(15.0559) | Bit/dim 10.4208(20.0421) | Xent 0.0000(0.0000) | Loss 10.4208(20.0421) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 22.2848(126.4557) | Total Time 0.00(0.00)\n",
      "Iter 0060 | Time 9.5146(13.5622) | Bit/dim 8.5985(17.2021) | Xent 0.0000(0.0000) | Loss 8.5985(17.2021) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 15.4482(97.8528) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0001 | Time 36.0146, Epoch Time 688.6353(688.6353), Bit/dim 7.7739, Xent 0.0000, Loss 7.7739, Error 1.0000\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0070 | Time 9.3240(12.4601) | Bit/dim 7.1360(14.7177) | Xent 0.0000(0.0000) | Loss 7.1360(14.7177) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 15.9312(76.2287) | Total Time 0.00(0.00)\n",
      "Iter 0080 | Time 9.2510(11.6159) | Bit/dim 5.8704(12.5437) | Xent 0.0000(0.0000) | Loss 5.8704(12.5437) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 14.5475(60.2677) | Total Time 0.00(0.00)\n",
      "Iter 0090 | Time 9.7005(11.0832) | Bit/dim 4.7146(10.6141) | Xent 0.0000(0.0000) | Loss 4.7146(10.6141) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 11.7896(47.8404) | Total Time 0.00(0.00)\n",
      "Iter 0100 | Time 9.5853(10.6997) | Bit/dim 3.8222(8.9217) | Xent 0.0000(0.0000) | Loss 3.8222(8.9217) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 9.8048(38.0722) | Total Time 0.00(0.00)\n",
      "Iter 0110 | Time 9.7078(10.4161) | Bit/dim 3.1494(7.4739) | Xent 0.0000(0.0000) | Loss 3.1494(7.4739) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 7.3123(30.2671) | Total Time 0.00(0.00)\n",
      "Iter 0120 | Time 9.7918(10.2264) | Bit/dim 2.7523(6.2767) | Xent 0.0000(0.0000) | Loss 2.7523(6.2767) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 4.7712(23.8511) | Total Time 0.00(0.00)\n",
      "Iter 0130 | Time 9.9325(10.1087) | Bit/dim 2.5243(5.3155) | Xent 0.0000(0.0000) | Loss 2.5243(5.3155) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 3.0125(18.5648) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0002 | Time 38.7798, Epoch Time 685.0149(688.5267), Bit/dim 2.4883, Xent 0.0000, Loss 2.4883, Error 1.0000\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0140 | Time 9.9550(10.0415) | Bit/dim 2.4144(4.5634) | Xent 0.0000(0.0000) | Loss 2.4144(4.5634) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 1.8560(14.2956) | Total Time 0.00(0.00)\n",
      "Iter 0150 | Time 10.3044(10.0916) | Bit/dim 2.3401(3.9840) | Xent 0.0000(0.0000) | Loss 2.3401(3.9840) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 1.1612(10.9133) | Total Time 0.00(0.00)\n",
      "Iter 0160 | Time 10.4053(10.1804) | Bit/dim 2.2937(3.5445) | Xent 0.0000(0.0000) | Loss 2.2937(3.5445) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 0.8705(8.3022) | Total Time 0.00(0.00)\n",
      "Iter 0170 | Time 10.5887(10.2344) | Bit/dim 2.2668(3.2130) | Xent 0.0000(0.0000) | Loss 2.2668(3.2130) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 0.7713(6.3341) | Total Time 0.00(0.00)\n",
      "Iter 0180 | Time 10.4227(10.2791) | Bit/dim 2.2606(2.9642) | Xent 0.0000(0.0000) | Loss 2.2606(2.9642) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 0.7143(4.8670) | Total Time 0.00(0.00)\n",
      "Iter 0190 | Time 10.1962(10.2911) | Bit/dim 2.2426(2.7754) | Xent 0.0000(0.0000) | Loss 2.2426(2.7754) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 0.6606(3.7700) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0003 | Time 39.0606, Epoch Time 728.7772(689.7342), Bit/dim 2.2272, Xent 0.0000, Loss 2.2272, Error 1.0000\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0200 | Time 9.9490(10.2012) | Bit/dim 2.2395(2.6342) | Xent 0.0000(0.0000) | Loss 2.2395(2.6342) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 0.6167(2.9451) | Total Time 0.00(0.00)\n",
      "Iter 0210 | Time 9.6774(10.1032) | Bit/dim 2.2232(2.5244) | Xent 0.0000(0.0000) | Loss 2.2232(2.5244) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 0.5635(2.3252) | Total Time 0.00(0.00)\n",
      "Iter 0220 | Time 9.8386(10.0230) | Bit/dim 2.2170(2.4442) | Xent 0.0000(0.0000) | Loss 2.2170(2.4442) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 0.5412(1.8615) | Total Time 0.00(0.00)\n",
      "Iter 0230 | Time 9.7707(9.9684) | Bit/dim 2.2109(2.3812) | Xent 0.0000(0.0000) | Loss 2.2109(2.3812) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 0.5678(1.5152) | Total Time 0.00(0.00)\n",
      "Iter 0240 | Time 9.6251(9.9226) | Bit/dim 2.1748(2.3309) | Xent 0.0000(0.0000) | Loss 2.1748(2.3309) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 0.5222(1.2577) | Total Time 0.00(0.00)\n",
      "Iter 0250 | Time 9.7960(9.8938) | Bit/dim 2.1679(2.2912) | Xent 0.0000(0.0000) | Loss 2.1679(2.2912) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 0.5283(1.0691) | Total Time 0.00(0.00)\n",
      "Iter 0260 | Time 9.8223(9.8719) | Bit/dim 2.1564(2.2594) | Xent 0.0000(0.0000) | Loss 2.1564(2.2594) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 0.5282(0.9286) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0004 | Time 39.2634, Epoch Time 701.2272(690.0790), Bit/dim 2.1522, Xent 0.0000, Loss 2.1522, Error 1.0000\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0270 | Time 9.7911(9.8552) | Bit/dim 2.1387(2.2316) | Xent 0.0000(0.0000) | Loss 2.1387(2.2316) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 0.5477(0.8244) | Total Time 0.00(0.00)\n",
      "Iter 0280 | Time 9.8678(9.8555) | Bit/dim 2.1396(2.2078) | Xent 0.0000(0.0000) | Loss 2.1396(2.2078) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 0.5200(0.7491) | Total Time 0.00(0.00)\n",
      "Iter 0290 | Time 9.8437(9.8548) | Bit/dim 2.1090(2.1860) | Xent 0.0000(0.0000) | Loss 2.1090(2.1860) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 0.5314(0.6913) | Total Time 0.00(0.00)\n",
      "Iter 0300 | Time 9.9871(9.8596) | Bit/dim 2.1244(2.1691) | Xent 0.0000(0.0000) | Loss 2.1244(2.1691) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 0.5252(0.6494) | Total Time 0.00(0.00)\n",
      "Iter 0310 | Time 9.9931(9.9154) | Bit/dim 2.0918(2.1512) | Xent 0.0000(0.0000) | Loss 2.0918(2.1512) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 0.5466(0.6199) | Total Time 0.00(0.00)\n",
      "Iter 0320 | Time 10.0715(9.9367) | Bit/dim 2.0721(2.1334) | Xent 0.0000(0.0000) | Loss 2.0721(2.1334) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 0.5284(0.5986) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0005 | Time 39.2413, Epoch Time 709.3799(690.6580), Bit/dim 2.0547, Xent 0.0000, Loss 2.0547, Error 1.0000\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0330 | Time 10.1025(9.9857) | Bit/dim 2.0649(2.1158) | Xent 0.0000(0.0000) | Loss 2.0649(2.1158) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 0.5178(0.5850) | Total Time 0.00(0.00)\n",
      "Iter 0340 | Time 10.0686(9.9910) | Bit/dim 2.0431(2.0990) | Xent 0.0000(0.0000) | Loss 2.0431(2.0990) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 0.5246(0.5715) | Total Time 0.00(0.00)\n",
      "Iter 0350 | Time 10.0644(10.0232) | Bit/dim 1.9990(2.0807) | Xent 0.0000(0.0000) | Loss 1.9990(2.0807) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 0.5015(0.5608) | Total Time 0.00(0.00)\n",
      "Iter 0360 | Time 10.1263(10.0430) | Bit/dim 2.0043(2.0632) | Xent 0.0000(0.0000) | Loss 2.0043(2.0632) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 0.5076(0.5424) | Total Time 0.00(0.00)\n",
      "Iter 0370 | Time 9.9257(10.0650) | Bit/dim 1.9773(2.0438) | Xent 0.0000(0.0000) | Loss 1.9773(2.0438) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 0.4912(0.5289) | Total Time 0.00(0.00)\n",
      "Iter 0380 | Time 10.1253(10.0763) | Bit/dim 1.9655(2.0268) | Xent 0.0000(0.0000) | Loss 1.9655(2.0268) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 0.4502(0.5184) | Total Time 0.00(0.00)\n",
      "Iter 0390 | Time 10.1775(10.1169) | Bit/dim 1.9547(2.0086) | Xent 0.0000(0.0000) | Loss 1.9547(2.0086) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 0.4787(0.5063) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0006 | Time 39.3037, Epoch Time 721.0198(691.5689), Bit/dim 1.9391, Xent 0.0000, Loss 1.9391, Error 1.0000\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0400 | Time 10.0234(10.1247) | Bit/dim 1.9456(1.9920) | Xent 0.0000(0.0000) | Loss 1.9456(1.9920) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 0.4467(0.4832) | Total Time 0.00(0.00)\n",
      "Iter 0410 | Time 10.3063(10.1247) | Bit/dim 1.9241(1.9766) | Xent 0.0000(0.0000) | Loss 1.9241(1.9766) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 0.3848(0.4658) | Total Time 0.00(0.00)\n",
      "Iter 0420 | Time 10.2276(10.1588) | Bit/dim 1.8959(1.9598) | Xent 0.0000(0.0000) | Loss 1.8959(1.9598) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 0.4069(0.4440) | Total Time 0.00(0.00)\n",
      "Iter 0430 | Time 10.2401(10.1674) | Bit/dim 1.8920(1.9475) | Xent 0.0000(0.0000) | Loss 1.8920(1.9475) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 0.4726(0.4254) | Total Time 0.00(0.00)\n",
      "Iter 0440 | Time 10.2338(10.1873) | Bit/dim 1.9075(1.9354) | Xent 0.0000(0.0000) | Loss 1.9075(1.9354) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 0.2877(0.4079) | Total Time 0.00(0.00)\n",
      "Iter 0450 | Time 10.2070(10.2051) | Bit/dim 1.8984(1.9248) | Xent 0.0000(0.0000) | Loss 1.8984(1.9248) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 0.3221(0.3891) | Total Time 0.00(0.00)\n",
      "Iter 0460 | Time 10.4261(10.2169) | Bit/dim 1.8964(1.9131) | Xent 0.0000(0.0000) | Loss 1.8964(1.9131) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 0.3621(0.3841) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0007 | Time 40.4092, Epoch Time 727.8525(692.6574), Bit/dim 1.8738, Xent 0.0000, Loss 1.8738, Error 1.0000\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0470 | Time 10.0625(10.2077) | Bit/dim 1.8676(1.9028) | Xent 0.0000(0.0000) | Loss 1.8676(1.9028) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 0.4848(0.3792) | Total Time 0.00(0.00)\n",
      "Iter 0480 | Time 10.1325(10.2040) | Bit/dim 1.8610(1.8925) | Xent 0.0000(0.0000) | Loss 1.8610(1.8925) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 0.4623(0.3987) | Total Time 0.00(0.00)\n",
      "Iter 0490 | Time 10.3201(10.2421) | Bit/dim 1.8392(1.8841) | Xent 0.0000(0.0000) | Loss 1.8392(1.8841) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 0.5440(0.4127) | Total Time 0.00(0.00)\n",
      "Iter 0500 | Time 10.3091(10.2500) | Bit/dim 1.8403(1.8746) | Xent 0.0000(0.0000) | Loss 1.8403(1.8746) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 0.2534(0.4282) | Total Time 0.00(0.00)\n",
      "Iter 0510 | Time 10.3151(10.2442) | Bit/dim 1.8417(1.8660) | Xent 0.0000(0.0000) | Loss 1.8417(1.8660) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 0.2874(0.4103) | Total Time 0.00(0.00)\n",
      "Iter 0520 | Time 10.1917(10.2459) | Bit/dim 1.8244(1.8552) | Xent 0.0000(0.0000) | Loss 1.8244(1.8552) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 0.5857(0.4166) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0008 | Time 41.3002, Epoch Time 732.4670(693.8517), Bit/dim 1.8059, Xent 0.0000, Loss 1.8059, Error 1.0000\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0530 | Time 10.6357(10.2959) | Bit/dim 1.8236(1.8448) | Xent 0.0000(0.0000) | Loss 1.8236(1.8448) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 0.6871(0.4454) | Total Time 0.00(0.00)\n",
      "Iter 0540 | Time 10.5891(10.3837) | Bit/dim 1.7836(1.8326) | Xent 0.0000(0.0000) | Loss 1.7836(1.8326) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 0.4853(0.4503) | Total Time 0.00(0.00)\n",
      "Iter 0550 | Time 10.8399(10.4520) | Bit/dim 1.7773(1.8203) | Xent 0.0000(0.0000) | Loss 1.7773(1.8203) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 0.5827(0.4740) | Total Time 0.00(0.00)\n",
      "Iter 0560 | Time 10.5861(10.5155) | Bit/dim 1.7417(1.8041) | Xent 0.0000(0.0000) | Loss 1.7417(1.8041) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 0.6382(0.5119) | Total Time 0.00(0.00)\n",
      "Iter 0570 | Time 10.9606(10.5909) | Bit/dim 1.7037(1.7835) | Xent 0.0000(0.0000) | Loss 1.7037(1.7835) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 1.0311(0.6259) | Total Time 0.00(0.00)\n",
      "Iter 0580 | Time 10.9756(10.6768) | Bit/dim 1.6728(1.7602) | Xent 0.0000(0.0000) | Loss 1.6728(1.7602) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 0.5915(0.6444) | Total Time 0.00(0.00)\n",
      "Iter 0590 | Time 11.0274(10.7583) | Bit/dim 1.6438(1.7321) | Xent 0.0000(0.0000) | Loss 1.6438(1.7321) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 1.8861(0.6704) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0009 | Time 42.6950, Epoch Time 767.2557(696.0538), Bit/dim 1.6305, Xent 0.0000, Loss 1.6305, Error 1.0000\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0600 | Time 10.9467(10.7861) | Bit/dim 1.6181(1.7064) | Xent 0.0000(0.0000) | Loss 1.6181(1.7064) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 4.7489(3.1391) | Total Time 0.00(0.00)\n",
      "Iter 0610 | Time 10.6676(10.8205) | Bit/dim 1.5876(1.6789) | Xent 0.0000(0.0000) | Loss 1.5876(1.6789) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 0.8480(4.4147) | Total Time 0.00(0.00)\n",
      "Iter 0620 | Time 10.9427(10.8189) | Bit/dim 1.5670(1.6526) | Xent 0.0000(0.0000) | Loss 1.5670(1.6526) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 1.4678(5.4641) | Total Time 0.00(0.00)\n",
      "Iter 0630 | Time 10.8319(10.8011) | Bit/dim 1.5564(1.6332) | Xent 0.0000(0.0000) | Loss 1.5564(1.6332) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 11.8537(8.0284) | Total Time 0.00(0.00)\n",
      "Iter 0640 | Time 10.6127(10.7867) | Bit/dim 1.5394(1.6135) | Xent 0.0000(0.0000) | Loss 1.5394(1.6135) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 10.9670(9.3931) | Total Time 0.00(0.00)\n",
      "Iter 0650 | Time 10.7389(10.7813) | Bit/dim 1.5256(1.5947) | Xent 0.0000(0.0000) | Loss 1.5256(1.5947) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 9.3557(8.8573) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0010 | Time 42.0510, Epoch Time 769.3539(698.2528), Bit/dim 1.5165, Xent 0.0000, Loss 1.5165, Error 1.0000\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0660 | Time 10.6794(10.7742) | Bit/dim 1.5220(1.5792) | Xent 0.0000(0.0000) | Loss 1.5220(1.5792) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 0.9263(8.6101) | Total Time 0.00(0.00)\n",
      "Iter 0670 | Time 10.7036(10.7317) | Bit/dim 1.5166(1.5673) | Xent 0.0000(0.0000) | Loss 1.5166(1.5673) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 0.5620(10.0654) | Total Time 0.00(0.00)\n",
      "Iter 0680 | Time 10.6300(10.7254) | Bit/dim 1.5161(1.5541) | Xent 0.0000(0.0000) | Loss 1.5161(1.5541) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 16.3506(10.5756) | Total Time 0.00(0.00)\n",
      "Iter 0690 | Time 10.6173(10.7088) | Bit/dim 1.5090(1.5434) | Xent 0.0000(0.0000) | Loss 1.5090(1.5434) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 17.8862(11.5214) | Total Time 0.00(0.00)\n",
      "Iter 0700 | Time 10.6345(10.7068) | Bit/dim 1.4758(1.5315) | Xent 0.0000(0.0000) | Loss 1.4758(1.5315) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 4.1896(11.6638) | Total Time 0.00(0.00)\n",
      "Iter 0710 | Time 10.8530(10.7026) | Bit/dim 1.5118(1.5241) | Xent 0.0000(0.0000) | Loss 1.5118(1.5241) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 14.8172(12.7729) | Total Time 0.00(0.00)\n",
      "Iter 0720 | Time 10.7457(10.7050) | Bit/dim 1.4817(1.5150) | Xent 0.0000(0.0000) | Loss 1.4817(1.5150) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 4.7891(12.9060) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0011 | Time 42.7537, Epoch Time 762.0051(700.1653), Bit/dim 1.4850, Xent 0.0000, Loss 1.4850, Error 1.0000\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0730 | Time 10.6727(10.7019) | Bit/dim 1.4872(1.5060) | Xent 0.0000(0.0000) | Loss 1.4872(1.5060) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 16.0599(13.5853) | Total Time 0.00(0.00)\n",
      "Iter 0740 | Time 10.9571(10.7171) | Bit/dim 1.4819(1.4983) | Xent 0.0000(0.0000) | Loss 1.4819(1.4983) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 26.8096(14.7253) | Total Time 0.00(0.00)\n",
      "Iter 0750 | Time 10.8571(10.7279) | Bit/dim 1.4690(1.4922) | Xent 0.0000(0.0000) | Loss 1.4690(1.4922) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 5.4534(14.5306) | Total Time 0.00(0.00)\n",
      "Iter 0760 | Time 10.9395(10.7590) | Bit/dim 1.4555(1.4864) | Xent 0.0000(0.0000) | Loss 1.4555(1.4864) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 7.1815(14.5015) | Total Time 0.00(0.00)\n",
      "Iter 0770 | Time 11.0881(10.8173) | Bit/dim 1.4567(1.4813) | Xent 0.0000(0.0000) | Loss 1.4567(1.4813) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 19.4824(15.5351) | Total Time 0.00(0.00)\n",
      "Iter 0780 | Time 11.1607(10.8706) | Bit/dim 1.4709(1.4746) | Xent 0.0000(0.0000) | Loss 1.4709(1.4746) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 25.1549(15.2776) | Total Time 0.00(0.00)\n",
      "Iter 0790 | Time 11.0950(10.9220) | Bit/dim 1.4717(1.4711) | Xent 0.0000(0.0000) | Loss 1.4717(1.4711) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 24.5809(16.0618) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0012 | Time 45.9294, Epoch Time 778.5979(702.5183), Bit/dim 1.4308, Xent 0.0000, Loss 1.4308, Error 1.0000\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0800 | Time 11.1132(10.9458) | Bit/dim 1.4468(1.4632) | Xent 0.0000(0.0000) | Loss 1.4468(1.4632) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 19.0567(15.4399) | Total Time 0.00(0.00)\n",
      "Iter 0810 | Time 10.9377(10.9736) | Bit/dim 1.4819(1.4570) | Xent 0.0000(0.0000) | Loss 1.4819(1.4570) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 29.9225(15.6513) | Total Time 0.00(0.00)\n",
      "Iter 0820 | Time 11.0005(10.9875) | Bit/dim 1.4316(1.4515) | Xent 0.0000(0.0000) | Loss 1.4316(1.4515) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 18.4006(15.3020) | Total Time 0.00(0.00)\n",
      "Iter 0830 | Time 11.1134(10.9796) | Bit/dim 1.4437(1.4491) | Xent 0.0000(0.0000) | Loss 1.4437(1.4491) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 25.4127(16.1672) | Total Time 0.00(0.00)\n",
      "Iter 0840 | Time 11.1263(10.9821) | Bit/dim 1.4471(1.4431) | Xent 0.0000(0.0000) | Loss 1.4471(1.4431) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 15.5085(15.5899) | Total Time 0.00(0.00)\n",
      "Iter 0850 | Time 11.0599(10.9915) | Bit/dim 1.4050(1.4391) | Xent 0.0000(0.0000) | Loss 1.4050(1.4391) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 10.1498(15.7782) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0013 | Time 46.1131, Epoch Time 786.5842(705.0403), Bit/dim 1.4096, Xent 0.0000, Loss 1.4096, Error 1.0000\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0860 | Time 10.9287(10.9943) | Bit/dim 1.4024(1.4333) | Xent 0.0000(0.0000) | Loss 1.4024(1.4333) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 5.0161(15.4755) | Total Time 0.00(0.00)\n",
      "Iter 0870 | Time 11.0862(10.9911) | Bit/dim 1.4064(1.4280) | Xent 0.0000(0.0000) | Loss 1.4064(1.4280) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 13.0035(15.0409) | Total Time 0.00(0.00)\n",
      "Iter 0880 | Time 11.1304(11.0253) | Bit/dim 1.4127(1.4248) | Xent 0.0000(0.0000) | Loss 1.4127(1.4248) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 23.7849(15.9517) | Total Time 0.00(0.00)\n",
      "Iter 0890 | Time 11.1755(11.0375) | Bit/dim 1.3924(1.4177) | Xent 0.0000(0.0000) | Loss 1.3924(1.4177) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 17.3195(14.5203) | Total Time 0.00(0.00)\n",
      "Iter 0900 | Time 11.1181(11.0602) | Bit/dim 1.3722(1.4176) | Xent 0.0000(0.0000) | Loss 1.3722(1.4176) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 6.5003(15.6687) | Total Time 0.00(0.00)\n",
      "Iter 0910 | Time 11.0381(11.0552) | Bit/dim 1.4130(1.4153) | Xent 0.0000(0.0000) | Loss 1.4130(1.4153) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 22.7778(15.8225) | Total Time 0.00(0.00)\n",
      "Iter 0920 | Time 11.0847(11.0422) | Bit/dim 1.3653(1.4102) | Xent 0.0000(0.0000) | Loss 1.3653(1.4102) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 2.0632(15.7759) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0014 | Time 46.1059, Epoch Time 789.5453(707.5754), Bit/dim 1.3870, Xent 0.0000, Loss 1.3870, Error 1.0000\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0930 | Time 11.0599(11.0444) | Bit/dim 1.4226(1.4066) | Xent 0.0000(0.0000) | Loss 1.4226(1.4066) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 27.1641(15.6187) | Total Time 0.00(0.00)\n",
      "Iter 0940 | Time 10.9946(11.0355) | Bit/dim 1.3843(1.4036) | Xent 0.0000(0.0000) | Loss 1.3843(1.4036) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 6.4630(15.9057) | Total Time 0.00(0.00)\n",
      "Iter 0950 | Time 11.2367(11.0612) | Bit/dim 1.3709(1.4001) | Xent 0.0000(0.0000) | Loss 1.3709(1.4001) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 10.3486(15.8743) | Total Time 0.00(0.00)\n",
      "Iter 0960 | Time 11.0837(11.0671) | Bit/dim 1.3921(1.4000) | Xent 0.0000(0.0000) | Loss 1.3921(1.4000) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 11.6114(16.4815) | Total Time 0.00(0.00)\n",
      "Iter 0970 | Time 11.0892(11.0512) | Bit/dim 1.4035(1.3963) | Xent 0.0000(0.0000) | Loss 1.4035(1.3963) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 20.6335(15.8161) | Total Time 0.00(0.00)\n",
      "Iter 0980 | Time 10.8464(11.0276) | Bit/dim 1.3524(1.3914) | Xent 0.0000(0.0000) | Loss 1.3524(1.3914) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 10.1278(15.2808) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0015 | Time 45.5984, Epoch Time 787.9491(709.9867), Bit/dim 1.3756, Xent 0.0000, Loss 1.3756, Error 1.0000\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0990 | Time 10.7421(11.0082) | Bit/dim 1.3873(1.3894) | Xent 0.0000(0.0000) | Loss 1.3873(1.3894) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 19.2662(15.8595) | Total Time 0.00(0.00)\n",
      "Iter 1000 | Time 10.8768(10.9867) | Bit/dim 1.4228(1.3919) | Xent 0.0000(0.0000) | Loss 1.4228(1.3919) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 26.6214(17.2826) | Total Time 0.00(0.00)\n",
      "Iter 1010 | Time 10.8636(10.9630) | Bit/dim 1.3749(1.3885) | Xent 0.0000(0.0000) | Loss 1.3749(1.3885) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 9.9240(16.5885) | Total Time 0.00(0.00)\n",
      "Iter 1020 | Time 11.0949(10.9621) | Bit/dim 1.3642(1.3819) | Xent 0.0000(0.0000) | Loss 1.3642(1.3819) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 17.2370(15.0111) | Total Time 0.00(0.00)\n",
      "Iter 1030 | Time 10.8941(10.9251) | Bit/dim 1.4002(1.3775) | Xent 0.0000(0.0000) | Loss 1.4002(1.3775) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 24.1462(14.3762) | Total Time 0.00(0.00)\n",
      "Iter 1040 | Time 11.1080(10.9293) | Bit/dim 1.3684(1.3787) | Xent 0.0000(0.0000) | Loss 1.3684(1.3787) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 24.9330(15.7284) | Total Time 0.00(0.00)\n",
      "Iter 1050 | Time 11.1774(10.9453) | Bit/dim 1.3816(1.3749) | Xent 0.0000(0.0000) | Loss 1.3816(1.3749) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 13.3679(15.3168) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0016 | Time 45.0567, Epoch Time 779.6732(712.0773), Bit/dim 1.3427, Xent 0.0000, Loss 1.3427, Error 1.0000\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1060 | Time 10.7209(10.9300) | Bit/dim 1.3586(1.3692) | Xent 0.0000(0.0000) | Loss 1.3586(1.3692) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 17.4595(14.1903) | Total Time 0.00(0.00)\n",
      "Iter 1070 | Time 10.9645(10.9357) | Bit/dim 1.3685(1.3681) | Xent 0.0000(0.0000) | Loss 1.3685(1.3681) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 22.3873(15.4785) | Total Time 0.00(0.00)\n",
      "Iter 1080 | Time 10.9635(10.9508) | Bit/dim 1.3319(1.3641) | Xent 0.0000(0.0000) | Loss 1.3319(1.3641) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 11.9998(15.7343) | Total Time 0.00(0.00)\n",
      "Iter 1090 | Time 10.9204(10.9340) | Bit/dim 1.3395(1.3594) | Xent 0.0000(0.0000) | Loss 1.3395(1.3594) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 4.8674(14.5559) | Total Time 0.00(0.00)\n",
      "Iter 1100 | Time 11.0847(10.9388) | Bit/dim 1.3542(1.3578) | Xent 0.0000(0.0000) | Loss 1.3542(1.3578) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 27.2257(15.5936) | Total Time 0.00(0.00)\n",
      "Iter 1110 | Time 10.8802(10.9318) | Bit/dim 1.3600(1.3601) | Xent 0.0000(0.0000) | Loss 1.3600(1.3601) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 14.5891(16.5130) | Total Time 0.00(0.00)\n",
      "Iter 1120 | Time 10.9917(10.9309) | Bit/dim 1.3603(1.3576) | Xent 0.0000(0.0000) | Loss 1.3603(1.3576) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 19.7640(15.7867) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0017 | Time 45.1431, Epoch Time 781.3159(714.1544), Bit/dim 1.3446, Xent 0.0000, Loss 1.3446, Error 1.0000\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1130 | Time 11.0842(10.9396) | Bit/dim 1.3383(1.3536) | Xent 0.0000(0.0000) | Loss 1.3383(1.3536) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 15.8607(15.0786) | Total Time 0.00(0.00)\n",
      "Iter 1140 | Time 10.8453(10.9310) | Bit/dim 1.3414(1.3494) | Xent 0.0000(0.0000) | Loss 1.3414(1.3494) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 19.1524(14.4793) | Total Time 0.00(0.00)\n",
      "Iter 1150 | Time 10.9059(10.9392) | Bit/dim 1.3494(1.3468) | Xent 0.0000(0.0000) | Loss 1.3494(1.3468) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 23.5526(14.6139) | Total Time 0.00(0.00)\n",
      "Iter 1160 | Time 11.0163(10.9432) | Bit/dim 1.3587(1.3444) | Xent 0.0000(0.0000) | Loss 1.3587(1.3444) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 25.3001(14.6468) | Total Time 0.00(0.00)\n",
      "Iter 1170 | Time 10.9336(10.9470) | Bit/dim 1.3024(1.3410) | Xent 0.0000(0.0000) | Loss 1.3024(1.3410) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 2.6464(14.7399) | Total Time 0.00(0.00)\n",
      "Iter 1180 | Time 11.0244(10.9381) | Bit/dim 1.3308(1.3400) | Xent 0.0000(0.0000) | Loss 1.3308(1.3400) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 16.6030(14.8447) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0018 | Time 44.9029, Epoch Time 781.2914(716.1685), Bit/dim 1.3140, Xent 0.0000, Loss 1.3140, Error 1.0000\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1190 | Time 10.8837(10.9437) | Bit/dim 1.3120(1.3375) | Xent 0.0000(0.0000) | Loss 1.3120(1.3375) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 3.1154(14.0305) | Total Time 0.00(0.00)\n",
      "Iter 1200 | Time 10.8977(10.9286) | Bit/dim 1.3524(1.3395) | Xent 0.0000(0.0000) | Loss 1.3524(1.3395) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 24.7313(15.4147) | Total Time 0.00(0.00)\n",
      "Iter 1210 | Time 11.0470(10.9110) | Bit/dim 1.3512(1.3378) | Xent 0.0000(0.0000) | Loss 1.3512(1.3378) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 19.9911(15.9356) | Total Time 0.00(0.00)\n",
      "Iter 1220 | Time 11.0912(10.9185) | Bit/dim 1.3110(1.3344) | Xent 0.0000(0.0000) | Loss 1.3110(1.3344) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 3.4074(14.6529) | Total Time 0.00(0.00)\n",
      "Iter 1230 | Time 10.7644(10.9169) | Bit/dim 1.3208(1.3288) | Xent 0.0000(0.0000) | Loss 1.3208(1.3288) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 13.5526(13.2435) | Total Time 0.00(0.00)\n",
      "Iter 1240 | Time 10.9007(10.9213) | Bit/dim 1.3282(1.3334) | Xent 0.0000(0.0000) | Loss 1.3282(1.3334) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 16.6894(15.4754) | Total Time 0.00(0.00)\n",
      "Iter 1250 | Time 10.7661(10.9114) | Bit/dim 1.3138(1.3291) | Xent 0.0000(0.0000) | Loss 1.3138(1.3291) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 5.3577(14.3603) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0019 | Time 44.7510, Epoch Time 777.9708(718.0226), Bit/dim 1.3116, Xent 0.0000, Loss 1.3116, Error 1.0000\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1260 | Time 10.7027(10.9023) | Bit/dim 1.2951(1.3248) | Xent 0.0000(0.0000) | Loss 1.2951(1.3248) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 4.3796(12.6489) | Total Time 0.00(0.00)\n",
      "Iter 1270 | Time 10.9081(10.9011) | Bit/dim 1.3186(1.3253) | Xent 0.0000(0.0000) | Loss 1.3186(1.3253) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 18.8670(13.9367) | Total Time 0.00(0.00)\n",
      "Iter 1280 | Time 10.9409(10.9293) | Bit/dim 1.3316(1.3284) | Xent 0.0000(0.0000) | Loss 1.3316(1.3284) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 22.7858(15.5579) | Total Time 0.00(0.00)\n",
      "Iter 1290 | Time 10.9882(10.9257) | Bit/dim 1.3105(1.3244) | Xent 0.0000(0.0000) | Loss 1.3105(1.3244) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 8.9429(13.8481) | Total Time 0.00(0.00)\n",
      "Iter 1300 | Time 10.8808(10.9359) | Bit/dim 1.3398(1.3218) | Xent 0.0000(0.0000) | Loss 1.3398(1.3218) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 22.7767(13.6327) | Total Time 0.00(0.00)\n",
      "Iter 1310 | Time 10.9418(10.9597) | Bit/dim 1.3038(1.3182) | Xent 0.0000(0.0000) | Loss 1.3038(1.3182) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 13.9864(13.3781) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0020 | Time 45.2823, Epoch Time 782.1695(719.9470), Bit/dim 1.3043, Xent 0.0000, Loss 1.3043, Error 1.0000\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1320 | Time 10.6802(10.9445) | Bit/dim 1.3158(1.3172) | Xent 0.0000(0.0000) | Loss 1.3158(1.3172) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 15.4376(14.4068) | Total Time 0.00(0.00)\n",
      "Iter 1330 | Time 10.8879(10.9295) | Bit/dim 1.2863(1.3139) | Xent 0.0000(0.0000) | Loss 1.2863(1.3139) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 8.4246(14.0480) | Total Time 0.00(0.00)\n",
      "Iter 1340 | Time 10.8546(10.9140) | Bit/dim 1.3358(1.3151) | Xent 0.0000(0.0000) | Loss 1.3358(1.3151) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 21.5191(14.8113) | Total Time 0.00(0.00)\n",
      "Iter 1350 | Time 10.9060(10.9073) | Bit/dim 1.3258(1.3154) | Xent 0.0000(0.0000) | Loss 1.3258(1.3154) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 19.8695(15.3606) | Total Time 0.00(0.00)\n",
      "Iter 1360 | Time 10.8254(10.8998) | Bit/dim 1.2898(1.3130) | Xent 0.0000(0.0000) | Loss 1.2898(1.3130) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 14.1655(14.6801) | Total Time 0.00(0.00)\n",
      "Iter 1370 | Time 10.7543(10.9073) | Bit/dim 1.2873(1.3094) | Xent 0.0000(0.0000) | Loss 1.2873(1.3094) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 3.4967(14.2675) | Total Time 0.00(0.00)\n",
      "Iter 1380 | Time 10.9662(10.9058) | Bit/dim 1.3018(1.3061) | Xent 0.0000(0.0000) | Loss 1.3018(1.3061) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 13.8873(13.8632) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0021 | Time 45.2823, Epoch Time 778.3152(721.6980), Bit/dim 1.2836, Xent 0.0000, Loss 1.2836, Error 1.0000\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1390 | Time 10.8500(10.9110) | Bit/dim 1.2825(1.3063) | Xent 0.0000(0.0000) | Loss 1.2825(1.3063) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 14.3334(15.0394) | Total Time 0.00(0.00)\n",
      "Iter 1400 | Time 10.8614(10.9144) | Bit/dim 1.3137(1.3061) | Xent 0.0000(0.0000) | Loss 1.3137(1.3061) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 13.5878(15.5012) | Total Time 0.00(0.00)\n",
      "Iter 1410 | Time 10.9354(10.9371) | Bit/dim 1.2967(1.3074) | Xent 0.0000(0.0000) | Loss 1.2967(1.3074) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 9.4579(15.5726) | Total Time 0.00(0.00)\n",
      "Iter 1420 | Time 10.9651(10.9169) | Bit/dim 1.2815(1.3023) | Xent 0.0000(0.0000) | Loss 1.2815(1.3023) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 16.7527(14.5760) | Total Time 0.00(0.00)\n",
      "Iter 1430 | Time 10.7292(10.9155) | Bit/dim 1.2964(1.3007) | Xent 0.0000(0.0000) | Loss 1.2964(1.3007) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 17.2440(14.7463) | Total Time 0.00(0.00)\n",
      "Iter 1440 | Time 10.8887(10.9021) | Bit/dim 1.2812(1.2967) | Xent 0.0000(0.0000) | Loss 1.2812(1.2967) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 15.7088(13.8867) | Total Time 0.00(0.00)\n",
      "Iter 1450 | Time 10.6693(10.9046) | Bit/dim 1.2925(1.2960) | Xent 0.0000(0.0000) | Loss 1.2925(1.2960) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 12.8352(13.4262) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0022 | Time 44.9031, Epoch Time 779.0038(723.4172), Bit/dim 1.3194, Xent 0.0000, Loss 1.3194, Error 1.0000\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1460 | Time 10.8851(10.9177) | Bit/dim 1.3072(1.3005) | Xent 0.0000(0.0000) | Loss 1.3072(1.3005) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 13.0695(15.2369) | Total Time 0.00(0.00)\n",
      "Iter 1470 | Time 10.8341(10.9320) | Bit/dim 1.2916(1.2998) | Xent 0.0000(0.0000) | Loss 1.2916(1.2998) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 12.5943(15.3521) | Total Time 0.00(0.00)\n",
      "Iter 1480 | Time 10.9445(10.9172) | Bit/dim 1.2784(1.2934) | Xent 0.0000(0.0000) | Loss 1.2784(1.2934) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 4.0720(13.1695) | Total Time 0.00(0.00)\n",
      "Iter 1490 | Time 11.0678(10.9376) | Bit/dim 1.2832(1.2937) | Xent 0.0000(0.0000) | Loss 1.2832(1.2937) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 15.6963(13.8976) | Total Time 0.00(0.00)\n",
      "Iter 1500 | Time 10.9623(10.9556) | Bit/dim 1.2987(1.2991) | Xent 0.0000(0.0000) | Loss 1.2987(1.2991) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 21.3898(15.8166) | Total Time 0.00(0.00)\n",
      "Epoch 0023 | Time 45.5846, Epoch Time 783.3053(725.2139), Bit/dim 1.2706, Xent 0.0000, Loss 1.2706, Error 1.0000\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1520 | Time 10.8292(10.9472) | Bit/dim 1.2875(1.2918) | Xent 0.0000(0.0000) | Loss 1.2875(1.2918) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 20.5610(14.0352) | Total Time 0.00(0.00)\n",
      "Iter 1530 | Time 10.9679(10.9299) | Bit/dim 1.2676(1.2868) | Xent 0.0000(0.0000) | Loss 1.2676(1.2868) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 20.5047(14.0250) | Total Time 0.00(0.00)\n",
      "Iter 1540 | Time 11.0084(10.9322) | Bit/dim 1.2901(1.2857) | Xent 0.0000(0.0000) | Loss 1.2901(1.2857) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 22.1250(14.3402) | Total Time 0.00(0.00)\n",
      "Iter 1550 | Time 10.8485(10.9441) | Bit/dim 1.2863(1.2849) | Xent 0.0000(0.0000) | Loss 1.2863(1.2849) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 7.6442(14.2974) | Total Time 0.00(0.00)\n",
      "Iter 1560 | Time 10.9839(10.9557) | Bit/dim 1.2657(1.2830) | Xent 0.0000(0.0000) | Loss 1.2657(1.2830) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 12.2271(14.4542) | Total Time 0.00(0.00)\n",
      "Iter 1570 | Time 10.9218(10.9627) | Bit/dim 1.2425(1.2802) | Xent 0.0000(0.0000) | Loss 1.2425(1.2802) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 3.3961(14.1135) | Total Time 0.00(0.00)\n",
      "Iter 1580 | Time 10.8429(10.9714) | Bit/dim 1.2699(1.2812) | Xent 0.0000(0.0000) | Loss 1.2699(1.2812) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 16.1365(14.5685) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0024 | Time 44.6565, Epoch Time 782.0208(726.9181), Bit/dim 1.2568, Xent 0.0000, Loss 1.2568, Error 1.0000\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1590 | Time 11.2645(10.9908) | Bit/dim 1.3010(1.2828) | Xent 0.0000(0.0000) | Loss 1.3010(1.2828) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 22.2578(15.3791) | Total Time 0.00(0.00)\n",
      "Iter 1600 | Time 10.7950(11.0050) | Bit/dim 1.2725(1.2816) | Xent 0.0000(0.0000) | Loss 1.2725(1.2816) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 14.4373(15.2693) | Total Time 0.00(0.00)\n",
      "Iter 1610 | Time 10.9261(11.0002) | Bit/dim 1.2640(1.2781) | Xent 0.0000(0.0000) | Loss 1.2640(1.2781) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 8.5636(13.9744) | Total Time 0.00(0.00)\n",
      "Iter 1620 | Time 10.6638(11.0142) | Bit/dim 1.2667(1.2756) | Xent 0.0000(0.0000) | Loss 1.2667(1.2756) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 3.0893(13.8681) | Total Time 0.00(0.00)\n",
      "Iter 1630 | Time 10.9763(11.0094) | Bit/dim 1.2964(1.2801) | Xent 0.0000(0.0000) | Loss 1.2964(1.2801) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 20.4383(15.3244) | Total Time 0.00(0.00)\n",
      "Iter 1640 | Time 11.0533(10.9967) | Bit/dim 1.2514(1.2787) | Xent 0.0000(0.0000) | Loss 1.2514(1.2787) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 11.4125(14.4964) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0025 | Time 45.3866, Epoch Time 784.3973(728.6424), Bit/dim 1.2701, Xent 0.0000, Loss 1.2701, Error 1.0000\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1650 | Time 10.8734(10.9503) | Bit/dim 1.2764(1.2757) | Xent 0.0000(0.0000) | Loss 1.2764(1.2757) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 19.6875(13.6517) | Total Time 0.00(0.00)\n",
      "Iter 1670 | Time 11.0268(10.9401) | Bit/dim 1.2582(1.2716) | Xent 0.0000(0.0000) | Loss 1.2582(1.2716) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 8.7323(13.5972) | Total Time 0.00(0.00)\n",
      "Iter 1680 | Time 11.0140(10.9346) | Bit/dim 1.3521(1.2709) | Xent 0.0000(0.0000) | Loss 1.3521(1.2709) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 34.8674(13.7620) | Total Time 0.00(0.00)\n",
      "Iter 1690 | Time 11.0141(10.9656) | Bit/dim 1.2524(1.2716) | Xent 0.0000(0.0000) | Loss 1.2524(1.2716) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 10.0794(14.4028) | Total Time 0.00(0.00)\n",
      "Iter 1700 | Time 10.9872(10.9868) | Bit/dim 1.2689(1.2683) | Xent 0.0000(0.0000) | Loss 1.2689(1.2683) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 16.0738(13.1834) | Total Time 0.00(0.00)\n",
      "Iter 1710 | Time 10.8385(10.9945) | Bit/dim 1.2388(1.2665) | Xent 0.0000(0.0000) | Loss 1.2388(1.2665) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 9.9120(13.3700) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0026 | Time 44.9342, Epoch Time 783.5622(730.2900), Bit/dim 1.2629, Xent 0.0000, Loss 1.2629, Error 1.0000\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1720 | Time 10.8362(10.9902) | Bit/dim 1.2497(1.2652) | Xent 0.0000(0.0000) | Loss 1.2497(1.2652) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 10.0870(13.8140) | Total Time 0.00(0.00)\n",
      "Iter 1730 | Time 11.2140(10.9821) | Bit/dim 1.2763(1.2659) | Xent 0.0000(0.0000) | Loss 1.2763(1.2659) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 23.8849(14.5079) | Total Time 0.00(0.00)\n",
      "Iter 1740 | Time 10.8941(10.9928) | Bit/dim 1.2828(1.2710) | Xent 0.0000(0.0000) | Loss 1.2828(1.2710) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 17.2150(16.1346) | Total Time 0.00(0.00)\n",
      "Iter 1750 | Time 10.8136(10.9958) | Bit/dim 1.2853(1.2752) | Xent 0.0000(0.0000) | Loss 1.2853(1.2752) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 22.7370(16.8953) | Total Time 0.00(0.00)\n",
      "Iter 1760 | Time 10.8542(10.9931) | Bit/dim 1.2635(1.2726) | Xent 0.0000(0.0000) | Loss 1.2635(1.2726) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 8.9373(15.9867) | Total Time 0.00(0.00)\n",
      "Iter 1770 | Time 11.0073(10.9820) | Bit/dim 1.2551(1.2674) | Xent 0.0000(0.0000) | Loss 1.2551(1.2674) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 5.8888(14.2635) | Total Time 0.00(0.00)\n",
      "Iter 1780 | Time 11.0807(10.9805) | Bit/dim 1.2527(1.2635) | Xent 0.0000(0.0000) | Loss 1.2527(1.2635) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 14.3433(13.4388) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0027 | Time 45.1132, Epoch Time 783.7664(731.8943), Bit/dim 1.2384, Xent 0.0000, Loss 1.2384, Error 1.0000\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1790 | Time 10.8802(10.9588) | Bit/dim 1.2544(1.2602) | Xent 0.0000(0.0000) | Loss 1.2544(1.2602) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 2.2685(12.4153) | Total Time 0.00(0.00)\n",
      "Iter 1800 | Time 10.9172(10.9331) | Bit/dim 1.2649(1.2587) | Xent 0.0000(0.0000) | Loss 1.2649(1.2587) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 14.9419(12.9011) | Total Time 0.00(0.00)\n",
      "Iter 1810 | Time 11.0359(10.9468) | Bit/dim 1.2475(1.2573) | Xent 0.0000(0.0000) | Loss 1.2475(1.2573) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 17.8541(13.3179) | Total Time 0.00(0.00)\n",
      "Iter 1820 | Time 10.8774(10.9319) | Bit/dim 1.2824(1.2562) | Xent 0.0000(0.0000) | Loss 1.2824(1.2562) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 29.3999(13.8761) | Total Time 0.00(0.00)\n",
      "Iter 1830 | Time 10.8782(10.9405) | Bit/dim 1.2465(1.2576) | Xent 0.0000(0.0000) | Loss 1.2465(1.2576) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 7.7093(14.9194) | Total Time 0.00(0.00)\n",
      "Iter 1840 | Time 10.7941(10.9374) | Bit/dim 1.2516(1.2588) | Xent 0.0000(0.0000) | Loss 1.2516(1.2588) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 6.3821(15.1081) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0028 | Time 45.1799, Epoch Time 780.3487(733.3480), Bit/dim 1.2586, Xent 0.0000, Loss 1.2586, Error 1.0000\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1850 | Time 10.8783(10.9364) | Bit/dim 1.2281(1.2570) | Xent 0.0000(0.0000) | Loss 1.2281(1.2570) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 4.4901(14.8091) | Total Time 0.00(0.00)\n",
      "Iter 1860 | Time 10.9127(10.9380) | Bit/dim 1.2451(1.2556) | Xent 0.0000(0.0000) | Loss 1.2451(1.2556) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 3.2791(14.4309) | Total Time 0.00(0.00)\n",
      "Iter 1870 | Time 11.1573(10.9551) | Bit/dim 1.2297(1.2550) | Xent 0.0000(0.0000) | Loss 1.2297(1.2550) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 9.4746(14.8085) | Total Time 0.00(0.00)\n",
      "Iter 1880 | Time 10.9899(10.9802) | Bit/dim 1.2382(1.2518) | Xent 0.0000(0.0000) | Loss 1.2382(1.2518) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 5.0389(14.3645) | Total Time 0.00(0.00)\n",
      "Iter 1890 | Time 11.0290(10.9728) | Bit/dim 1.2676(1.2500) | Xent 0.0000(0.0000) | Loss 1.2676(1.2500) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 20.3953(13.7426) | Total Time 0.00(0.00)\n",
      "Iter 1900 | Time 11.2062(10.9810) | Bit/dim 1.2422(1.2486) | Xent 0.0000(0.0000) | Loss 1.2422(1.2486) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 20.7455(14.0919) | Total Time 0.00(0.00)\n",
      "Iter 1910 | Time 11.0711(10.9619) | Bit/dim 1.2250(1.2462) | Xent 0.0000(0.0000) | Loss 1.2250(1.2462) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 2.5256(12.8936) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0029 | Time 45.1440, Epoch Time 783.2177(734.8440), Bit/dim 1.2535, Xent 0.0000, Loss 1.2535, Error 1.0000\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1920 | Time 10.7378(10.9786) | Bit/dim 1.2850(1.2494) | Xent 0.0000(0.0000) | Loss 1.2850(1.2494) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 29.0760(14.2264) | Total Time 0.00(0.00)\n",
      "Iter 1930 | Time 11.1299(10.9759) | Bit/dim 1.2818(1.2551) | Xent 0.0000(0.0000) | Loss 1.2818(1.2551) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 24.0018(15.7210) | Total Time 0.00(0.00)\n",
      "Iter 1940 | Time 10.8760(10.9711) | Bit/dim 1.2540(1.2546) | Xent 0.0000(0.0000) | Loss 1.2540(1.2546) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 17.7868(15.8516) | Total Time 0.00(0.00)\n",
      "Iter 1950 | Time 10.9179(10.9610) | Bit/dim 1.2277(1.2499) | Xent 0.0000(0.0000) | Loss 1.2277(1.2499) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 12.1808(14.6287) | Total Time 0.00(0.00)\n",
      "Iter 1960 | Time 10.9453(10.9566) | Bit/dim 1.2264(1.2446) | Xent 0.0000(0.0000) | Loss 1.2264(1.2446) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 10.7227(12.9125) | Total Time 0.00(0.00)\n",
      "Iter 1970 | Time 10.6720(10.9466) | Bit/dim 1.2345(1.2411) | Xent 0.0000(0.0000) | Loss 1.2345(1.2411) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 5.3302(12.5002) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0030 | Time 45.5048, Epoch Time 782.8298(736.2836), Bit/dim 1.2304, Xent 0.0000, Loss 1.2304, Error 1.0000\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1980 | Time 10.8037(10.9437) | Bit/dim 1.2103(1.2432) | Xent 0.0000(0.0000) | Loss 1.2103(1.2432) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 8.3998(14.1940) | Total Time 0.00(0.00)\n",
      "Iter 1990 | Time 10.7962(10.9283) | Bit/dim 1.2435(1.2433) | Xent 0.0000(0.0000) | Loss 1.2435(1.2433) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 3.7871(14.4967) | Total Time 0.00(0.00)\n",
      "Iter 2000 | Time 10.7906(10.9414) | Bit/dim 1.2199(1.2415) | Xent 0.0000(0.0000) | Loss 1.2199(1.2415) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 3.3843(14.2713) | Total Time 0.00(0.00)\n",
      "Iter 2010 | Time 11.0229(10.9502) | Bit/dim 1.2227(1.2410) | Xent 0.0000(0.0000) | Loss 1.2227(1.2410) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 12.3005(14.6601) | Total Time 0.00(0.00)\n",
      "Iter 2020 | Time 10.8701(10.9708) | Bit/dim 1.2371(1.2407) | Xent 0.0000(0.0000) | Loss 1.2371(1.2407) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 19.2673(15.0943) | Total Time 0.00(0.00)\n",
      "Iter 2030 | Time 10.9095(10.9612) | Bit/dim 1.2242(1.2387) | Xent 0.0000(0.0000) | Loss 1.2242(1.2387) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 4.1564(14.0132) | Total Time 0.00(0.00)\n",
      "Iter 2040 | Time 10.9222(10.9560) | Bit/dim 1.2238(1.2356) | Xent 0.0000(0.0000) | Loss 1.2238(1.2356) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 4.3025(12.7750) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0031 | Time 45.1792, Epoch Time 781.8251(737.6499), Bit/dim 1.2519, Xent 0.0000, Loss 1.2519, Error 1.0000\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2050 | Time 11.1321(10.9543) | Bit/dim 1.2418(1.2365) | Xent 0.0000(0.0000) | Loss 1.2418(1.2365) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 20.1372(13.3198) | Total Time 0.00(0.00)\n",
      "Iter 2060 | Time 10.8569(10.9504) | Bit/dim 1.2260(1.2396) | Xent 0.0000(0.0000) | Loss 1.2260(1.2396) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 13.0926(14.5818) | Total Time 0.00(0.00)\n",
      "Iter 2070 | Time 11.0292(10.9595) | Bit/dim 1.2284(1.2371) | Xent 0.0000(0.0000) | Loss 1.2284(1.2371) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 12.7388(13.8928) | Total Time 0.00(0.00)\n",
      "Iter 2080 | Time 11.0497(10.9729) | Bit/dim 1.2299(1.2369) | Xent 0.0000(0.0000) | Loss 1.2299(1.2369) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 15.4209(13.9920) | Total Time 0.00(0.00)\n",
      "Iter 2090 | Time 10.7627(10.9721) | Bit/dim 1.2250(1.2364) | Xent 0.0000(0.0000) | Loss 1.2250(1.2364) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 4.1277(14.4697) | Total Time 0.00(0.00)\n",
      "Iter 2100 | Time 10.7066(10.9763) | Bit/dim 1.2123(1.2352) | Xent 0.0000(0.0000) | Loss 1.2123(1.2352) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 2.3656(14.3002) | Total Time 0.00(0.00)\n",
      "Iter 2110 | Time 10.7704(10.9547) | Bit/dim 1.2151(1.2353) | Xent 0.0000(0.0000) | Loss 1.2151(1.2353) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 6.6672(14.3472) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0032 | Time 44.7213, Epoch Time 782.9168(739.0079), Bit/dim 1.2441, Xent 0.0000, Loss 1.2441, Error 1.0000\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2120 | Time 10.8683(10.9384) | Bit/dim 1.2330(1.2352) | Xent 0.0000(0.0000) | Loss 1.2330(1.2352) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 15.0938(14.6008) | Total Time 0.00(0.00)\n",
      "Iter 2130 | Time 10.8282(10.9299) | Bit/dim 1.2399(1.2351) | Xent 0.0000(0.0000) | Loss 1.2399(1.2351) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 12.1580(14.2749) | Total Time 0.00(0.00)\n",
      "Iter 2140 | Time 10.8687(10.9192) | Bit/dim 1.2262(1.2351) | Xent 0.0000(0.0000) | Loss 1.2262(1.2351) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 18.2062(14.9225) | Total Time 0.00(0.00)\n",
      "Iter 2150 | Time 10.9580(10.9036) | Bit/dim 1.2225(1.2318) | Xent 0.0000(0.0000) | Loss 1.2225(1.2318) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 8.7993(13.5471) | Total Time 0.00(0.00)\n",
      "Iter 2160 | Time 10.8301(10.9056) | Bit/dim 1.2242(1.2296) | Xent 0.0000(0.0000) | Loss 1.2242(1.2296) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 6.5565(13.4696) | Total Time 0.00(0.00)\n",
      "Iter 2170 | Time 10.7946(10.9236) | Bit/dim 1.2879(1.2356) | Xent 0.0000(0.0000) | Loss 1.2879(1.2356) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 31.6701(15.2879) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0033 | Time 44.3671, Epoch Time 778.3743(740.1889), Bit/dim 1.2765, Xent 0.0000, Loss 1.2765, Error 1.0000\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2180 | Time 11.1037(10.9388) | Bit/dim 1.2828(1.2434) | Xent 0.0000(0.0000) | Loss 1.2828(1.2434) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 19.3221(16.3158) | Total Time 0.00(0.00)\n",
      "Iter 2190 | Time 10.9369(10.9501) | Bit/dim 1.2328(1.2423) | Xent 0.0000(0.0000) | Loss 1.2328(1.2423) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 10.4123(15.2596) | Total Time 0.00(0.00)\n",
      "Iter 2200 | Time 10.9257(10.9428) | Bit/dim 1.2164(1.2381) | Xent 0.0000(0.0000) | Loss 1.2164(1.2381) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 1.9899(13.3053) | Total Time 0.00(0.00)\n",
      "Iter 2210 | Time 11.0881(10.9456) | Bit/dim 1.2137(1.2322) | Xent 0.0000(0.0000) | Loss 1.2137(1.2322) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 9.5186(12.3395) | Total Time 0.00(0.00)\n",
      "Iter 2220 | Time 10.9219(10.9473) | Bit/dim 1.2108(1.2296) | Xent 0.0000(0.0000) | Loss 1.2108(1.2296) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 9.5768(12.0759) | Total Time 0.00(0.00)\n",
      "Iter 2230 | Time 11.1328(10.9675) | Bit/dim 1.2117(1.2265) | Xent 0.0000(0.0000) | Loss 1.2117(1.2265) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 9.1192(12.1012) | Total Time 0.00(0.00)\n",
      "Iter 2240 | Time 10.8950(10.9854) | Bit/dim 1.2110(1.2266) | Xent 0.0000(0.0000) | Loss 1.2110(1.2266) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 8.0488(13.1266) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0034 | Time 44.5110, Epoch Time 782.9924(741.4730), Bit/dim 1.2113, Xent 0.0000, Loss 1.2113, Error 1.0000\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2250 | Time 10.8963(10.9679) | Bit/dim 1.2274(1.2268) | Xent 0.0000(0.0000) | Loss 1.2274(1.2268) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 13.9500(13.6307) | Total Time 0.00(0.00)\n",
      "Iter 2260 | Time 10.9119(10.9644) | Bit/dim 1.2086(1.2244) | Xent 0.0000(0.0000) | Loss 1.2086(1.2244) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 5.4799(13.1069) | Total Time 0.00(0.00)\n",
      "Iter 2270 | Time 10.7945(10.9635) | Bit/dim 1.2584(1.2287) | Xent 0.0000(0.0000) | Loss 1.2584(1.2287) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 28.3800(14.9212) | Total Time 0.00(0.00)\n",
      "Iter 2280 | Time 10.9985(10.9893) | Bit/dim 1.2658(1.2350) | Xent 0.0000(0.0000) | Loss 1.2658(1.2350) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 18.6939(15.5339) | Total Time 0.00(0.00)\n",
      "Iter 2290 | Time 10.9631(10.9809) | Bit/dim 1.2166(1.2315) | Xent 0.0000(0.0000) | Loss 1.2166(1.2315) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 10.9269(14.5571) | Total Time 0.00(0.00)\n",
      "Iter 2300 | Time 11.0064(10.9709) | Bit/dim 1.2212(1.2321) | Xent 0.0000(0.0000) | Loss 1.2212(1.2321) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 4.5343(14.7592) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0035 | Time 45.0505, Epoch Time 783.1277(742.7226), Bit/dim 1.2208, Xent 0.0000, Loss 1.2208, Error 1.0000\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2310 | Time 10.8790(10.9696) | Bit/dim 1.2377(1.2349) | Xent 0.0000(0.0000) | Loss 1.2377(1.2349) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 12.0973(15.6556) | Total Time 0.00(0.00)\n",
      "Iter 2320 | Time 10.9429(10.9507) | Bit/dim 1.2279(1.2321) | Xent 0.0000(0.0000) | Loss 1.2279(1.2321) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 14.6963(15.0368) | Total Time 0.00(0.00)\n",
      "Iter 2330 | Time 10.7791(10.9226) | Bit/dim 1.2472(1.2310) | Xent 0.0000(0.0000) | Loss 1.2472(1.2310) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 23.7324(15.2632) | Total Time 0.00(0.00)\n",
      "Iter 2340 | Time 10.9993(10.9406) | Bit/dim 1.2352(1.2315) | Xent 0.0000(0.0000) | Loss 1.2352(1.2315) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 15.3878(15.7522) | Total Time 0.00(0.00)\n",
      "Iter 2350 | Time 10.9286(10.9462) | Bit/dim 1.2232(1.2279) | Xent 0.0000(0.0000) | Loss 1.2232(1.2279) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 12.3522(14.8464) | Total Time 0.00(0.00)\n",
      "Iter 2360 | Time 10.9094(10.9529) | Bit/dim 1.2135(1.2226) | Xent 0.0000(0.0000) | Loss 1.2135(1.2226) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 2.9815(13.3057) | Total Time 0.00(0.00)\n",
      "Iter 2370 | Time 11.0891(10.9715) | Bit/dim 1.2094(1.2192) | Xent 0.0000(0.0000) | Loss 1.2094(1.2192) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 6.3464(12.0258) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0036 | Time 45.2964, Epoch Time 782.2663(743.9089), Bit/dim 1.1931, Xent 0.0000, Loss 1.1931, Error 1.0000\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2380 | Time 10.9764(10.9577) | Bit/dim 1.2103(1.2157) | Xent 0.0000(0.0000) | Loss 1.2103(1.2157) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 13.3850(11.4028) | Total Time 0.00(0.00)\n",
      "Iter 2390 | Time 10.8455(10.9643) | Bit/dim 1.2375(1.2136) | Xent 0.0000(0.0000) | Loss 1.2375(1.2136) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 23.1579(11.2503) | Total Time 0.00(0.00)\n",
      "Iter 2400 | Time 10.8714(10.9628) | Bit/dim 1.2264(1.2127) | Xent 0.0000(0.0000) | Loss 1.2264(1.2127) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 9.4570(11.4818) | Total Time 0.00(0.00)\n",
      "Iter 2410 | Time 10.8306(10.9483) | Bit/dim 1.2427(1.2131) | Xent 0.0000(0.0000) | Loss 1.2427(1.2131) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 21.7978(12.1876) | Total Time 0.00(0.00)\n",
      "Iter 2420 | Time 10.7260(10.9454) | Bit/dim 1.2110(1.2131) | Xent 0.0000(0.0000) | Loss 1.2110(1.2131) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 21.0406(13.0236) | Total Time 0.00(0.00)\n",
      "Iter 2430 | Time 10.8356(10.9360) | Bit/dim 1.2079(1.2128) | Xent 0.0000(0.0000) | Loss 1.2079(1.2128) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 12.7007(13.5414) | Total Time 0.00(0.00)\n",
      "Iter 2440 | Time 11.2937(10.9742) | Bit/dim 1.2890(1.2262) | Xent 0.0000(0.0000) | Loss 1.2890(1.2262) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 30.0766(16.1397) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0037 | Time 45.2640, Epoch Time 782.6735(745.0719), Bit/dim 1.2435, Xent 0.0000, Loss 1.2435, Error 1.0000\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2450 | Time 11.3386(10.9874) | Bit/dim 1.2196(1.2311) | Xent 0.0000(0.0000) | Loss 1.2196(1.2311) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 13.2576(16.1552) | Total Time 0.00(0.00)\n",
      "Iter 2460 | Time 11.1790(11.0254) | Bit/dim 1.2048(1.2287) | Xent 0.0000(0.0000) | Loss 1.2048(1.2287) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 8.8485(14.8115) | Total Time 0.00(0.00)\n",
      "Iter 2470 | Time 11.1196(11.0367) | Bit/dim 1.2110(1.2245) | Xent 0.0000(0.0000) | Loss 1.2110(1.2245) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 7.0497(13.7491) | Total Time 0.00(0.00)\n",
      "Iter 2480 | Time 11.1384(11.0290) | Bit/dim 1.1971(1.2195) | Xent 0.0000(0.0000) | Loss 1.1971(1.2195) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 14.2097(12.8510) | Total Time 0.00(0.00)\n",
      "Iter 2490 | Time 11.1780(11.0628) | Bit/dim 1.1904(1.2143) | Xent 0.0000(0.0000) | Loss 1.1904(1.2143) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 9.0119(11.9924) | Total Time 0.00(0.00)\n",
      "Iter 2500 | Time 11.0163(11.0470) | Bit/dim 1.2026(1.2114) | Xent 0.0000(0.0000) | Loss 1.2026(1.2114) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 10.7305(11.7723) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0038 | Time 46.0086, Epoch Time 790.0908(746.4224), Bit/dim 1.2282, Xent 0.0000, Loss 1.2282, Error 1.0000\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2510 | Time 11.1563(11.0393) | Bit/dim 1.1837(1.2082) | Xent 0.0000(0.0000) | Loss 1.1837(1.2082) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 3.7939(11.4999) | Total Time 0.00(0.00)\n",
      "Iter 2520 | Time 11.2222(11.0732) | Bit/dim 1.2563(1.2160) | Xent 0.0000(0.0000) | Loss 1.2563(1.2160) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 16.9442(14.0248) | Total Time 0.00(0.00)\n",
      "Iter 2530 | Time 11.3630(11.0714) | Bit/dim 1.2484(1.2223) | Xent 0.0000(0.0000) | Loss 1.2484(1.2223) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 22.1544(15.1234) | Total Time 0.00(0.00)\n",
      "Iter 2540 | Time 11.1958(11.0683) | Bit/dim 1.2409(1.2230) | Xent 0.0000(0.0000) | Loss 1.2409(1.2230) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 20.3295(15.7427) | Total Time 0.00(0.00)\n",
      "Iter 2550 | Time 11.3373(11.0781) | Bit/dim 1.2154(1.2211) | Xent 0.0000(0.0000) | Loss 1.2154(1.2211) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 16.9955(15.4793) | Total Time 0.00(0.00)\n",
      "Iter 2560 | Time 11.1388(11.0823) | Bit/dim 1.1860(1.2190) | Xent 0.0000(0.0000) | Loss 1.1860(1.2190) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 10.4785(14.9465) | Total Time 0.00(0.00)\n",
      "Iter 2570 | Time 11.4693(11.0882) | Bit/dim 1.2114(1.2167) | Xent 0.0000(0.0000) | Loss 1.2114(1.2167) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 19.8440(14.4387) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0039 | Time 45.9529, Epoch Time 792.1682(747.7948), Bit/dim 1.2321, Xent 0.0000, Loss 1.2321, Error 1.0000\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2580 | Time 11.1746(11.0776) | Bit/dim 1.1777(1.2170) | Xent 0.0000(0.0000) | Loss 1.1777(1.2170) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 9.0304(15.3465) | Total Time 0.00(0.00)\n",
      "Iter 2590 | Time 10.9217(11.0580) | Bit/dim 1.1965(1.2133) | Xent 0.0000(0.0000) | Loss 1.1965(1.2133) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 14.8489(14.4707) | Total Time 0.00(0.00)\n",
      "Iter 2600 | Time 10.9893(11.0506) | Bit/dim 1.1786(1.2073) | Xent 0.0000(0.0000) | Loss 1.1786(1.2073) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 2.9119(12.2615) | Total Time 0.00(0.00)\n",
      "Iter 2610 | Time 11.0067(11.0278) | Bit/dim 1.1802(1.2022) | Xent 0.0000(0.0000) | Loss 1.1802(1.2022) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 4.6268(9.8640) | Total Time 0.00(0.00)\n",
      "Iter 2620 | Time 11.0022(10.9812) | Bit/dim 1.1704(1.2000) | Xent 0.0000(0.0000) | Loss 1.1704(1.2000) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 6.9487(9.9349) | Total Time 0.00(0.00)\n",
      "Iter 2630 | Time 10.7659(10.9821) | Bit/dim 1.1845(1.1983) | Xent 0.0000(0.0000) | Loss 1.1845(1.1983) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 6.3908(10.2158) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0040 | Time 45.7400, Epoch Time 784.4549(748.8946), Bit/dim 1.1817, Xent 0.0000, Loss 1.1817, Error 1.0000\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2640 | Time 10.8967(10.9833) | Bit/dim 1.1775(1.1993) | Xent 0.0000(0.0000) | Loss 1.1775(1.1993) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 3.0529(11.3615) | Total Time 0.00(0.00)\n",
      "Iter 2650 | Time 10.9667(10.9927) | Bit/dim 1.2395(1.2034) | Xent 0.0000(0.0000) | Loss 1.2395(1.2034) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 27.0693(13.0669) | Total Time 0.00(0.00)\n",
      "Iter 2660 | Time 11.3018(11.0364) | Bit/dim 1.2042(1.2107) | Xent 0.0000(0.0000) | Loss 1.2042(1.2107) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 15.8421(14.5248) | Total Time 0.00(0.00)\n",
      "Iter 2670 | Time 11.2018(11.0618) | Bit/dim 1.2031(1.2110) | Xent 0.0000(0.0000) | Loss 1.2031(1.2110) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 18.5498(14.1173) | Total Time 0.00(0.00)\n",
      "Iter 2680 | Time 11.3142(11.0785) | Bit/dim 1.1792(1.2137) | Xent 0.0000(0.0000) | Loss 1.1792(1.2137) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 5.9676(14.9944) | Total Time 0.00(0.00)\n",
      "Iter 2690 | Time 10.9518(11.0958) | Bit/dim 1.1948(1.2120) | Xent 0.0000(0.0000) | Loss 1.1948(1.2120) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 7.2591(14.9014) | Total Time 0.00(0.00)\n",
      "Iter 2700 | Time 10.9279(11.1051) | Bit/dim 1.2042(1.2128) | Xent 0.0000(0.0000) | Loss 1.2042(1.2128) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 10.8690(15.3440) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0041 | Time 45.6407, Epoch Time 793.1290(750.2216), Bit/dim 1.1976, Xent 0.0000, Loss 1.1976, Error 1.0000\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2710 | Time 11.0706(11.0779) | Bit/dim 1.1900(1.2095) | Xent 0.0000(0.0000) | Loss 1.1900(1.2095) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 6.5264(14.2090) | Total Time 0.00(0.00)\n",
      "Iter 2720 | Time 11.2390(11.0592) | Bit/dim 1.1917(1.2027) | Xent 0.0000(0.0000) | Loss 1.1917(1.2027) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 1.1525(11.8743) | Total Time 0.00(0.00)\n",
      "Iter 2730 | Time 11.1525(11.0480) | Bit/dim 1.1787(1.1983) | Xent 0.0000(0.0000) | Loss 1.1787(1.1983) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 5.9548(10.2488) | Total Time 0.00(0.00)\n",
      "Iter 2740 | Time 10.9234(11.0348) | Bit/dim 1.1752(1.1926) | Xent 0.0000(0.0000) | Loss 1.1752(1.1926) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 11.7410(8.8991) | Total Time 0.00(0.00)\n",
      "Iter 2750 | Time 10.9490(11.0383) | Bit/dim 1.2221(1.1936) | Xent 0.0000(0.0000) | Loss 1.2221(1.1936) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 21.6309(10.5605) | Total Time 0.00(0.00)\n",
      "Iter 2760 | Time 11.1861(11.0844) | Bit/dim 1.2097(1.2065) | Xent 0.0000(0.0000) | Loss 1.2097(1.2065) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 15.1277(13.8462) | Total Time 0.00(0.00)\n",
      "Iter 2770 | Time 11.2588(11.0942) | Bit/dim 1.2281(1.2117) | Xent 0.0000(0.0000) | Loss 1.2281(1.2117) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 10.9885(14.5657) | Total Time 0.00(0.00)\n",
      "validating...\n"
     ]
    }
   ],
   "source": [
    "%run -p train_cnf_conditional_gate.py --data mnist --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 900 --save experiments/cnf_bs900_gate_dev_scale_1e_3 --conditional False --log_freq 10 --weight_y 0. --scale 0.001"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
