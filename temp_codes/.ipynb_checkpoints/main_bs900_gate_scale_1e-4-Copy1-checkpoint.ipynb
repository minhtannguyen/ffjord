{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0,1,2,3,4,5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_conditional_gate.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.odenvp_conditional_gate as odenvp\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument('--scale', type=float, default=1.0)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=True, choices=[True, False])\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"./data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"./data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"./data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"./data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"./data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"./data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            './data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            './data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim, atol, rtol\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol = model(x, zero)  # run model forward\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz = modules.GaussianDiag.logp(mean, logs, z).view(-1,1)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(z)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted, atol, rtol\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol, 'scale': args.scale},)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z = modules.GaussianDiag.sample(mean, logs)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    if args.conditional: best_error_score = float(\"inf\")\n",
      "    \n",
      "    itr = 0\n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted, atol, rtol = compute_bits_per_dim_conditional(x, y, model)\n",
      "                loss =  loss_nll + args.weight_y * loss_xent\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss, atol, rtol = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            loss.backward()\n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits/dim', {'train': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'train': atol[tol_indx].mean()}, itr)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'train': rtol[tol_indx].mean()}, itr)\n",
      "                                \n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted, atol, rtol = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss, atol, rtol = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                        \n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'validation': time.time() - start}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits/dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}, Xent {:.4f}, Loss {:.4f}, Error {:.4f}\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, loss_xent, loss, error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples, atol, rtol = model(fixed_z, reverse=True)\n",
      "            generated_samples = generated_samples.view(-1, *data_shape)\n",
      "            for tol_indx in range(len(atol)):\n",
      "                writer.add_scalars('atol_gen_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                writer.add_scalars('rtol_gen_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, conditional=False, conv=True, data='mnist', dims='64,64,64', divergence_fn='approximate', dl2int=None, imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume=None, rtol=1e-05, save='experiments/cnf_bs900_gate_dev_scale_1e_4', scale=0.0001, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=900, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.0)\n",
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(4, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(4, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(2, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(2, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(8, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(8, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(4, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(4, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=1568, bias=True)\n",
      "  (project_class): LinearZeros(in_features=784, out_features=10, bias=True)\n",
      ")\n",
      "Number of trainable parameters: 841930\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc.py:286: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Iter 0000 | Time 37.8496(37.8496) | Bit/dim 30.5922(30.5922) | Xent 0.0000(0.0000) | Loss 30.5922(30.5922) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 244.3027(244.3027) | Total Time 0.00(0.00)\n",
      "Iter 0010 | Time 11.8694(31.0145) | Bit/dim 27.7994(30.2773) | Xent 0.0000(0.0000) | Loss 27.7994(30.2773) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 223.7017(242.0177) | Total Time 0.00(0.00)\n",
      "Iter 0020 | Time 11.7268(25.9271) | Bit/dim 21.5557(28.7196) | Xent 0.0000(0.0000) | Loss 21.5557(28.7196) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 167.2352(229.0094) | Total Time 0.00(0.00)\n",
      "Iter 0030 | Time 12.1923(22.2377) | Bit/dim 15.5023(25.8838) | Xent 0.0000(0.0000) | Loss 15.5023(25.8838) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 98.1028(202.3615) | Total Time 0.00(0.00)\n",
      "Iter 0040 | Time 13.3544(19.8140) | Bit/dim 12.2702(22.6277) | Xent 0.0000(0.0000) | Loss 12.2702(22.6277) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 40.4461(165.4346) | Total Time 0.00(0.00)\n",
      "Iter 0050 | Time 13.2242(18.0785) | Bit/dim 10.2740(19.6092) | Xent 0.0000(0.0000) | Loss 10.2740(19.6092) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 19.1032(128.8458) | Total Time 0.00(0.00)\n",
      "Iter 0060 | Time 13.2766(16.7858) | Bit/dim 8.8840(16.9413) | Xent 0.0000(0.0000) | Loss 8.8840(16.9413) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 14.9243(99.0939) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0001 | Time 49.1458, Epoch Time 913.2338(913.2338), Bit/dim 8.1523, Xent 0.0000, Loss 8.1523, Error 1.0000\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0070 | Time 13.0306(15.8327) | Bit/dim 7.5533(14.6240) | Xent 0.0000(0.0000) | Loss 7.5533(14.6240) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 16.6995(77.3214) | Total Time 0.00(0.00)\n",
      "Iter 0080 | Time 13.0912(15.1200) | Bit/dim 6.2879(12.5813) | Xent 0.0000(0.0000) | Loss 6.2879(12.5813) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 14.3464(61.1052) | Total Time 0.00(0.00)\n",
      "Iter 0090 | Time 13.2768(14.6126) | Bit/dim 5.1685(10.7500) | Xent 0.0000(0.0000) | Loss 5.1685(10.7500) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 11.9073(48.4266) | Total Time 0.00(0.00)\n",
      "Iter 0100 | Time 13.0783(14.2208) | Bit/dim 4.1485(9.1252) | Xent 0.0000(0.0000) | Loss 4.1485(9.1252) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 10.1999(38.5752) | Total Time 0.00(0.00)\n",
      "Iter 0110 | Time 13.2350(13.9665) | Bit/dim 3.4360(7.7030) | Xent 0.0000(0.0000) | Loss 3.4360(7.7030) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 8.0869(30.8248) | Total Time 0.00(0.00)\n",
      "Iter 0120 | Time 13.7984(13.8345) | Bit/dim 2.9620(6.5005) | Xent 0.0000(0.0000) | Loss 2.9620(6.5005) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 5.5695(24.4533) | Total Time 0.00(0.00)\n",
      "Iter 0130 | Time 13.2413(13.6900) | Bit/dim 2.6733(5.5210) | Xent 0.0000(0.0000) | Loss 2.6733(5.5210) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 3.9302(19.2366) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0002 | Time 50.1176, Epoch Time 937.7068(913.9680), Bit/dim 2.6102, Xent 0.0000, Loss 2.6102, Error 1.0000\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0140 | Time 13.5703(13.6161) | Bit/dim 2.4732(4.7424) | Xent 0.0000(0.0000) | Loss 2.4732(4.7424) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 2.4570(14.9883) | Total Time 0.00(0.00)\n",
      "Iter 0150 | Time 13.5730(13.6114) | Bit/dim 2.3997(4.1364) | Xent 0.0000(0.0000) | Loss 2.3997(4.1364) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 1.7477(11.5811) | Total Time 0.00(0.00)\n",
      "Iter 0160 | Time 13.7113(13.6307) | Bit/dim 2.3164(3.6672) | Xent 0.0000(0.0000) | Loss 2.3164(3.6672) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 1.3194(8.9318) | Total Time 0.00(0.00)\n",
      "Iter 0170 | Time 13.6373(13.6304) | Bit/dim 2.2674(3.3064) | Xent 0.0000(0.0000) | Loss 2.2674(3.3064) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 1.0831(6.8950) | Total Time 0.00(0.00)\n",
      "Iter 0180 | Time 13.4227(13.6017) | Bit/dim 2.2253(3.0308) | Xent 0.0000(0.0000) | Loss 2.2253(3.0308) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 0.8900(5.3336) | Total Time 0.00(0.00)\n",
      "Iter 0190 | Time 13.4841(13.5944) | Bit/dim 2.2103(2.8194) | Xent 0.0000(0.0000) | Loss 2.2103(2.8194) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 0.7766(4.1486) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0003 | Time 52.0336, Epoch Time 962.8703(915.4351), Bit/dim 2.1942, Xent 0.0000, Loss 2.1942, Error 1.0000\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0200 | Time 13.8022(13.6089) | Bit/dim 2.1932(2.6576) | Xent 0.0000(0.0000) | Loss 2.1932(2.6576) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 0.7038(3.2499) | Total Time 0.00(0.00)\n",
      "Iter 0210 | Time 13.8718(13.6553) | Bit/dim 2.1736(2.5338) | Xent 0.0000(0.0000) | Loss 2.1736(2.5338) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 0.6382(2.5721) | Total Time 0.00(0.00)\n",
      "Iter 0220 | Time 13.6757(13.6603) | Bit/dim 2.1544(2.4385) | Xent 0.0000(0.0000) | Loss 2.1544(2.4385) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 0.6181(2.0624) | Total Time 0.00(0.00)\n",
      "Iter 0230 | Time 13.7465(13.6707) | Bit/dim 2.1504(2.3645) | Xent 0.0000(0.0000) | Loss 2.1504(2.3645) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 0.5697(1.6763) | Total Time 0.00(0.00)\n",
      "Iter 0240 | Time 13.6069(13.6861) | Bit/dim 2.1390(2.3066) | Xent 0.0000(0.0000) | Loss 2.1390(2.3066) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 0.5585(1.3823) | Total Time 0.00(0.00)\n",
      "Iter 0250 | Time 13.7121(13.6822) | Bit/dim 2.1266(2.2599) | Xent 0.0000(0.0000) | Loss 2.1266(2.2599) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 0.5076(1.1569) | Total Time 0.00(0.00)\n",
      "Iter 0260 | Time 13.7378(13.6762) | Bit/dim 2.1077(2.2221) | Xent 0.0000(0.0000) | Loss 2.1077(2.2221) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 0.4610(0.9824) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0004 | Time 52.5208, Epoch Time 970.7244(917.0938), Bit/dim 2.1002, Xent 0.0000, Loss 2.1002, Error 1.0000\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0270 | Time 13.6579(13.6659) | Bit/dim 2.1076(2.1919) | Xent 0.0000(0.0000) | Loss 2.1076(2.1919) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 0.4587(0.8497) | Total Time 0.00(0.00)\n",
      "Iter 0280 | Time 13.6811(13.6337) | Bit/dim 2.0890(2.1666) | Xent 0.0000(0.0000) | Loss 2.0890(2.1666) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 0.4090(0.7414) | Total Time 0.00(0.00)\n",
      "Iter 0290 | Time 13.5197(13.6155) | Bit/dim 2.0660(2.1426) | Xent 0.0000(0.0000) | Loss 2.0660(2.1426) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 0.4216(0.6550) | Total Time 0.00(0.00)\n",
      "Iter 0300 | Time 13.7149(13.5985) | Bit/dim 2.0580(2.1227) | Xent 0.0000(0.0000) | Loss 2.0580(2.1227) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 0.3710(0.5884) | Total Time 0.00(0.00)\n",
      "Iter 0310 | Time 13.5088(13.5961) | Bit/dim 2.0510(2.1056) | Xent 0.0000(0.0000) | Loss 2.0510(2.1056) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 0.3906(0.5347) | Total Time 0.00(0.00)\n",
      "Iter 0320 | Time 13.6893(13.5959) | Bit/dim 2.0413(2.0888) | Xent 0.0000(0.0000) | Loss 2.0413(2.0888) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 0.3420(0.4932) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0005 | Time 51.6452, Epoch Time 961.2528(918.4185), Bit/dim 2.0215, Xent 0.0000, Loss 2.0215, Error 1.0000\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0330 | Time 13.5773(13.5750) | Bit/dim 2.0194(2.0724) | Xent 0.0000(0.0000) | Loss 2.0194(2.0724) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 0.3667(0.4626) | Total Time 0.00(0.00)\n",
      "Iter 0340 | Time 13.5259(13.5740) | Bit/dim 1.9826(2.0573) | Xent 0.0000(0.0000) | Loss 1.9826(2.0573) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 0.3580(0.4376) | Total Time 0.00(0.00)\n",
      "Iter 0350 | Time 13.6209(13.5476) | Bit/dim 2.0100(2.0435) | Xent 0.0000(0.0000) | Loss 2.0100(2.0435) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 0.3621(0.4187) | Total Time 0.00(0.00)\n",
      "Iter 0360 | Time 13.6353(13.5502) | Bit/dim 1.9966(2.0304) | Xent 0.0000(0.0000) | Loss 1.9966(2.0304) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 0.3429(0.3964) | Total Time 0.00(0.00)\n",
      "Iter 0370 | Time 13.8620(13.5820) | Bit/dim 1.9833(2.0186) | Xent 0.0000(0.0000) | Loss 1.9833(2.0186) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 0.3038(0.3759) | Total Time 0.00(0.00)\n",
      "Iter 0380 | Time 13.5882(13.6156) | Bit/dim 1.9743(2.0059) | Xent 0.0000(0.0000) | Loss 1.9743(2.0059) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 0.3379(0.3620) | Total Time 0.00(0.00)\n",
      "Iter 0390 | Time 13.5834(13.6347) | Bit/dim 1.9379(1.9931) | Xent 0.0000(0.0000) | Loss 1.9379(1.9931) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 0.3464(0.3589) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0006 | Time 52.3414, Epoch Time 965.6444(919.8353), Bit/dim 1.9426, Xent 0.0000, Loss 1.9426, Error 1.0000\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0400 | Time 13.7732(13.6670) | Bit/dim 1.9558(1.9805) | Xent 0.0000(0.0000) | Loss 1.9558(1.9805) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 0.3334(0.3556) | Total Time 0.00(0.00)\n",
      "Iter 0410 | Time 13.9024(13.6673) | Bit/dim 1.9292(1.9703) | Xent 0.0000(0.0000) | Loss 1.9292(1.9703) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 0.3624(0.3412) | Total Time 0.00(0.00)\n",
      "Iter 0420 | Time 13.4672(13.6548) | Bit/dim 1.9178(1.9591) | Xent 0.0000(0.0000) | Loss 1.9178(1.9591) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 0.3283(0.3293) | Total Time 0.00(0.00)\n",
      "Iter 0430 | Time 13.8772(13.6588) | Bit/dim 1.9190(1.9475) | Xent 0.0000(0.0000) | Loss 1.9190(1.9475) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 0.2721(0.3332) | Total Time 0.00(0.00)\n",
      "Iter 0440 | Time 13.8980(13.7031) | Bit/dim 1.8837(1.9366) | Xent 0.0000(0.0000) | Loss 1.8837(1.9366) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 0.2536(0.3313) | Total Time 0.00(0.00)\n",
      "Iter 0450 | Time 13.6656(13.7042) | Bit/dim 1.8904(1.9279) | Xent 0.0000(0.0000) | Loss 1.8904(1.9279) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 0.2771(0.3256) | Total Time 0.00(0.00)\n",
      "Iter 0460 | Time 13.7881(13.6885) | Bit/dim 1.8961(1.9184) | Xent 0.0000(0.0000) | Loss 1.8961(1.9184) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 0.2639(0.3180) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0007 | Time 52.0903, Epoch Time 970.0037(921.3404), Bit/dim 1.8826, Xent 0.0000, Loss 1.8826, Error 1.0000\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0470 | Time 13.6904(13.6847) | Bit/dim 1.8796(1.9105) | Xent 0.0000(0.0000) | Loss 1.8796(1.9105) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 0.2898(0.3116) | Total Time 0.00(0.00)\n",
      "Iter 0480 | Time 13.5122(13.6739) | Bit/dim 1.8860(1.9013) | Xent 0.0000(0.0000) | Loss 1.8860(1.9013) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 0.3229(0.3174) | Total Time 0.00(0.00)\n",
      "Iter 0490 | Time 13.7943(13.6815) | Bit/dim 1.8490(1.8927) | Xent 0.0000(0.0000) | Loss 1.8490(1.8927) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 0.2820(0.3388) | Total Time 0.00(0.00)\n",
      "Iter 0500 | Time 13.5155(13.6561) | Bit/dim 1.8534(1.8838) | Xent 0.0000(0.0000) | Loss 1.8534(1.8838) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 0.3195(0.3472) | Total Time 0.00(0.00)\n",
      "Iter 0510 | Time 13.8159(13.6541) | Bit/dim 1.8452(1.8751) | Xent 0.0000(0.0000) | Loss 1.8452(1.8751) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 0.3658(0.3463) | Total Time 0.00(0.00)\n",
      "Iter 0520 | Time 13.7954(13.6806) | Bit/dim 1.8499(1.8669) | Xent 0.0000(0.0000) | Loss 1.8499(1.8669) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 0.5376(0.3673) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0008 | Time 52.8232, Epoch Time 969.1178(922.7737), Bit/dim 1.8267, Xent 0.0000, Loss 1.8267, Error 1.0000\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0530 | Time 13.8704(13.6983) | Bit/dim 1.8158(1.8583) | Xent 0.0000(0.0000) | Loss 1.8158(1.8583) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 0.5824(0.3738) | Total Time 0.00(0.00)\n",
      "Iter 0540 | Time 13.7529(13.7235) | Bit/dim 1.8329(1.8511) | Xent 0.0000(0.0000) | Loss 1.8329(1.8511) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 0.3967(0.3931) | Total Time 0.00(0.00)\n",
      "Iter 0550 | Time 13.7524(13.7431) | Bit/dim 1.7947(1.8409) | Xent 0.0000(0.0000) | Loss 1.7947(1.8409) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 0.7577(0.4143) | Total Time 0.00(0.00)\n",
      "Iter 0560 | Time 13.6983(13.7352) | Bit/dim 1.7977(1.8303) | Xent 0.0000(0.0000) | Loss 1.7977(1.8303) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 0.7855(0.4364) | Total Time 0.00(0.00)\n",
      "Iter 0570 | Time 13.8738(13.7221) | Bit/dim 1.7918(1.8211) | Xent 0.0000(0.0000) | Loss 1.7918(1.8211) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 1.1061(0.8284) | Total Time 0.00(0.00)\n",
      "Iter 0580 | Time 13.9035(13.7217) | Bit/dim 1.7655(1.8078) | Xent 0.0000(0.0000) | Loss 1.7655(1.8078) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 3.6367(1.2254) | Total Time 0.00(0.00)\n",
      "Iter 0590 | Time 13.7950(13.7322) | Bit/dim 1.7458(1.7968) | Xent 0.0000(0.0000) | Loss 1.7458(1.7968) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 5.3502(2.4287) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0009 | Time 53.0589, Epoch Time 974.4746(924.3247), Bit/dim 1.7404, Xent 0.0000, Loss 1.7404, Error 1.0000\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0600 | Time 13.6679(13.7472) | Bit/dim 1.7267(1.7808) | Xent 0.0000(0.0000) | Loss 1.7267(1.7808) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 3.7297(2.6695) | Total Time 0.00(0.00)\n",
      "Iter 0610 | Time 13.9834(13.8053) | Bit/dim 1.7087(1.7643) | Xent 0.0000(0.0000) | Loss 1.7087(1.7643) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 8.8174(3.9431) | Total Time 0.00(0.00)\n",
      "Iter 0620 | Time 13.7555(13.8475) | Bit/dim 1.6805(1.7475) | Xent 0.0000(0.0000) | Loss 1.6805(1.7475) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 7.5490(4.8950) | Total Time 0.00(0.00)\n",
      "Iter 0630 | Time 14.5241(13.9448) | Bit/dim 1.6651(1.7274) | Xent 0.0000(0.0000) | Loss 1.6651(1.7274) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 3.4599(5.8465) | Total Time 0.00(0.00)\n",
      "Iter 0640 | Time 14.4504(14.0266) | Bit/dim 1.6295(1.7059) | Xent 0.0000(0.0000) | Loss 1.6295(1.7059) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 15.4219(7.1746) | Total Time 0.00(0.00)\n",
      "Iter 0650 | Time 14.4486(14.1074) | Bit/dim 1.5729(1.6808) | Xent 0.0000(0.0000) | Loss 1.5729(1.6808) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 4.4084(7.4824) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0010 | Time 57.3372, Epoch Time 1002.2785(926.6633), Bit/dim 1.6039, Xent 0.0000, Loss 1.6039, Error 1.0000\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0660 | Time 14.3288(14.1235) | Bit/dim 1.6155(1.6582) | Xent 0.0000(0.0000) | Loss 1.6155(1.6582) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 19.8350(8.9727) | Total Time 0.00(0.00)\n",
      "Iter 0670 | Time 14.2275(14.1519) | Bit/dim 1.5637(1.6356) | Xent 0.0000(0.0000) | Loss 1.5637(1.6356) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 8.3386(9.7743) | Total Time 0.00(0.00)\n",
      "Iter 0680 | Time 14.1063(14.1738) | Bit/dim 1.5496(1.6158) | Xent 0.0000(0.0000) | Loss 1.5496(1.6158) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 7.2682(10.1860) | Total Time 0.00(0.00)\n",
      "Iter 0690 | Time 13.9578(14.1991) | Bit/dim 1.5617(1.5990) | Xent 0.0000(0.0000) | Loss 1.5617(1.5990) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 17.1753(11.4230) | Total Time 0.00(0.00)\n",
      "Iter 0700 | Time 14.2602(14.2267) | Bit/dim 1.5047(1.5804) | Xent 0.0000(0.0000) | Loss 1.5047(1.5804) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 4.8842(11.1011) | Total Time 0.00(0.00)\n",
      "Iter 0710 | Time 14.2893(14.2952) | Bit/dim 1.5446(1.5671) | Xent 0.0000(0.0000) | Loss 1.5446(1.5671) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 21.5596(12.0380) | Total Time 0.00(0.00)\n",
      "Iter 0720 | Time 14.3988(14.3100) | Bit/dim 1.5181(1.5537) | Xent 0.0000(0.0000) | Loss 1.5181(1.5537) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 19.5068(12.5804) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0011 | Time 55.3950, Epoch Time 1013.6899(929.2741), Bit/dim 1.4869, Xent 0.0000, Loss 1.4869, Error 1.0000\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0730 | Time 14.2773(14.2894) | Bit/dim 1.5183(1.5382) | Xent 0.0000(0.0000) | Loss 1.5183(1.5382) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 22.7016(11.7820) | Total Time 0.00(0.00)\n",
      "Iter 0740 | Time 14.9514(14.3318) | Bit/dim 1.5016(1.5310) | Xent 0.0000(0.0000) | Loss 1.5016(1.5310) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 17.3727(13.2929) | Total Time 0.00(0.00)\n",
      "Iter 0750 | Time 14.3473(14.3509) | Bit/dim 1.5034(1.5212) | Xent 0.0000(0.0000) | Loss 1.5034(1.5212) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 23.4104(13.0883) | Total Time 0.00(0.00)\n",
      "Iter 0760 | Time 14.6595(14.3730) | Bit/dim 1.5157(1.5132) | Xent 0.0000(0.0000) | Loss 1.5157(1.5132) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 27.9265(13.5169) | Total Time 0.00(0.00)\n",
      "Iter 0770 | Time 14.2995(14.4083) | Bit/dim 1.4483(1.5052) | Xent 0.0000(0.0000) | Loss 1.4483(1.5052) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 8.6978(13.8339) | Total Time 0.00(0.00)\n",
      "Iter 0780 | Time 14.7426(14.4358) | Bit/dim 1.4718(1.4977) | Xent 0.0000(0.0000) | Loss 1.4718(1.4977) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 11.8917(14.1659) | Total Time 0.00(0.00)\n",
      "Iter 0790 | Time 14.3509(14.4410) | Bit/dim 1.4585(1.4915) | Xent 0.0000(0.0000) | Loss 1.4585(1.4915) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 3.7743(14.1737) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0012 | Time 56.8663, Epoch Time 1023.7127(932.1073), Bit/dim 1.4523, Xent 0.0000, Loss 1.4523, Error 1.0000\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0800 | Time 14.8508(14.4929) | Bit/dim 1.4520(1.4852) | Xent 0.0000(0.0000) | Loss 1.4520(1.4852) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 7.6486(14.3650) | Total Time 0.00(0.00)\n",
      "Iter 0810 | Time 14.4917(14.4978) | Bit/dim 1.5261(1.4776) | Xent 0.0000(0.0000) | Loss 1.5261(1.4776) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 33.6335(13.0375) | Total Time 0.00(0.00)\n",
      "Iter 0820 | Time 14.4707(14.5242) | Bit/dim 1.5436(1.5016) | Xent 0.0000(0.0000) | Loss 1.5436(1.5016) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 24.9142(17.6893) | Total Time 0.00(0.00)\n",
      "Iter 0830 | Time 14.2317(14.5076) | Bit/dim 1.4603(1.5034) | Xent 0.0000(0.0000) | Loss 1.4603(1.5034) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 12.2574(17.9617) | Total Time 0.00(0.00)\n",
      "Iter 0840 | Time 14.2922(14.4634) | Bit/dim 1.4578(1.4928) | Xent 0.0000(0.0000) | Loss 1.4578(1.4928) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 3.8085(16.2864) | Total Time 0.00(0.00)\n",
      "Iter 0850 | Time 14.2758(14.4653) | Bit/dim 1.4315(1.4805) | Xent 0.0000(0.0000) | Loss 1.4315(1.4805) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 8.1349(14.3724) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0013 | Time 56.2074, Epoch Time 1027.5473(934.9705), Bit/dim 1.4206, Xent 0.0000, Loss 1.4206, Error 1.0000\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0860 | Time 14.6215(14.4688) | Bit/dim 1.4379(1.4687) | Xent 0.0000(0.0000) | Loss 1.4379(1.4687) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 9.6495(12.5003) | Total Time 0.00(0.00)\n",
      "Iter 0870 | Time 14.6477(14.4649) | Bit/dim 1.4235(1.4567) | Xent 0.0000(0.0000) | Loss 1.4235(1.4567) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 7.0875(10.9331) | Total Time 0.00(0.00)\n",
      "Iter 0880 | Time 14.1643(14.5009) | Bit/dim 1.4428(1.4469) | Xent 0.0000(0.0000) | Loss 1.4428(1.4469) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 21.3181(10.1595) | Total Time 0.00(0.00)\n",
      "Iter 0890 | Time 14.7699(14.5132) | Bit/dim 1.4535(1.4430) | Xent 0.0000(0.0000) | Loss 1.4535(1.4430) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 26.3702(11.7902) | Total Time 0.00(0.00)\n",
      "Iter 0900 | Time 14.3770(14.5430) | Bit/dim 1.4348(1.4370) | Xent 0.0000(0.0000) | Loss 1.4348(1.4370) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 19.8923(12.7074) | Total Time 0.00(0.00)\n",
      "Iter 0910 | Time 14.3350(14.5762) | Bit/dim 1.4325(1.4313) | Xent 0.0000(0.0000) | Loss 1.4325(1.4313) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 23.7281(13.9407) | Total Time 0.00(0.00)\n",
      "Iter 0920 | Time 14.4933(14.5966) | Bit/dim 1.4080(1.4252) | Xent 0.0000(0.0000) | Loss 1.4080(1.4252) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 21.8259(14.5210) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0014 | Time 56.4360, Epoch Time 1033.2341(937.9184), Bit/dim 1.4020, Xent 0.0000, Loss 1.4020, Error 1.0000\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0930 | Time 14.7544(14.6131) | Bit/dim 1.3898(1.4190) | Xent 0.0000(0.0000) | Loss 1.3898(1.4190) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 6.5525(13.5499) | Total Time 0.00(0.00)\n",
      "Iter 0940 | Time 14.5299(14.6082) | Bit/dim 1.4322(1.4188) | Xent 0.0000(0.0000) | Loss 1.4322(1.4188) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 24.9312(14.8782) | Total Time 0.00(0.00)\n",
      "Iter 0950 | Time 14.5760(14.5960) | Bit/dim 1.4085(1.4173) | Xent 0.0000(0.0000) | Loss 1.4085(1.4173) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 3.4907(15.3938) | Total Time 0.00(0.00)\n",
      "Iter 0960 | Time 14.7675(14.5840) | Bit/dim 1.4092(1.4147) | Xent 0.0000(0.0000) | Loss 1.4092(1.4147) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 25.1283(16.1347) | Total Time 0.00(0.00)\n",
      "Iter 0970 | Time 14.9826(14.5699) | Bit/dim 1.3892(1.4093) | Xent 0.0000(0.0000) | Loss 1.3892(1.4093) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 18.1955(15.9256) | Total Time 0.00(0.00)\n",
      "Iter 0980 | Time 14.5493(14.5868) | Bit/dim 1.3674(1.4041) | Xent 0.0000(0.0000) | Loss 1.3674(1.4041) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 6.2617(15.9567) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0015 | Time 56.2483, Epoch Time 1034.2599(940.8086), Bit/dim 1.4021, Xent 0.0000, Loss 1.4021, Error 1.0000\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0990 | Time 13.9480(14.6056) | Bit/dim 1.3972(1.3998) | Xent 0.0000(0.0000) | Loss 1.3972(1.3998) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 23.9094(15.7307) | Total Time 0.00(0.00)\n",
      "Iter 1000 | Time 14.5623(14.6014) | Bit/dim 1.3939(1.3988) | Xent 0.0000(0.0000) | Loss 1.3939(1.3988) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 19.9164(16.3978) | Total Time 0.00(0.00)\n",
      "Iter 1010 | Time 14.6619(14.6176) | Bit/dim 1.3747(1.3947) | Xent 0.0000(0.0000) | Loss 1.3747(1.3947) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 6.8330(15.7554) | Total Time 0.00(0.00)\n",
      "Iter 1020 | Time 14.2528(14.6158) | Bit/dim 1.3594(1.3896) | Xent 0.0000(0.0000) | Loss 1.3594(1.3896) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 8.5221(15.2147) | Total Time 0.00(0.00)\n",
      "Iter 1030 | Time 14.5749(14.6096) | Bit/dim 1.3574(1.3864) | Xent 0.0000(0.0000) | Loss 1.3574(1.3864) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 2.0548(15.3200) | Total Time 0.00(0.00)\n",
      "Iter 1040 | Time 14.1814(14.5850) | Bit/dim 1.3454(1.3840) | Xent 0.0000(0.0000) | Loss 1.3454(1.3840) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 6.5457(16.2916) | Total Time 0.00(0.00)\n",
      "Iter 1050 | Time 14.4048(14.5510) | Bit/dim 1.3912(1.3800) | Xent 0.0000(0.0000) | Loss 1.3912(1.3800) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 23.7418(16.0518) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0016 | Time 58.0101, Epoch Time 1033.1086(943.5776), Bit/dim 1.3757, Xent 0.0000, Loss 1.3757, Error 1.0000\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1060 | Time 13.9219(14.5205) | Bit/dim 1.3475(1.3749) | Xent 0.0000(0.0000) | Loss 1.3475(1.3749) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 11.7619(15.3351) | Total Time 0.00(0.00)\n",
      "Iter 1070 | Time 14.8304(14.5218) | Bit/dim 1.3492(1.3722) | Xent 0.0000(0.0000) | Loss 1.3492(1.3722) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 3.8537(15.9785) | Total Time 0.00(0.00)\n",
      "Iter 1080 | Time 14.3374(14.5345) | Bit/dim 1.3597(1.3692) | Xent 0.0000(0.0000) | Loss 1.3597(1.3692) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 12.0527(16.1742) | Total Time 0.00(0.00)\n",
      "Iter 1090 | Time 14.7257(14.5344) | Bit/dim 1.3457(1.3638) | Xent 0.0000(0.0000) | Loss 1.3457(1.3638) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 6.3171(14.9453) | Total Time 0.00(0.00)\n",
      "Iter 1100 | Time 14.8723(14.5499) | Bit/dim 1.3394(1.3602) | Xent 0.0000(0.0000) | Loss 1.3394(1.3602) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 13.4232(14.7586) | Total Time 0.00(0.00)\n",
      "Iter 1110 | Time 14.4803(14.5285) | Bit/dim 1.3470(1.3612) | Xent 0.0000(0.0000) | Loss 1.3470(1.3612) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 13.3268(15.6370) | Total Time 0.00(0.00)\n",
      "Iter 1120 | Time 14.7279(14.5056) | Bit/dim 1.3506(1.3566) | Xent 0.0000(0.0000) | Loss 1.3506(1.3566) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 26.3375(15.2708) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0017 | Time 57.4958, Epoch Time 1028.3788(946.1217), Bit/dim 1.3382, Xent 0.0000, Loss 1.3382, Error 1.0000\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1130 | Time 14.7457(14.5185) | Bit/dim 1.3437(1.3552) | Xent 0.0000(0.0000) | Loss 1.3437(1.3552) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 20.4395(16.2891) | Total Time 0.00(0.00)\n",
      "Iter 1140 | Time 14.3731(14.5173) | Bit/dim 1.3427(1.3486) | Xent 0.0000(0.0000) | Loss 1.3427(1.3486) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 13.9986(14.9131) | Total Time 0.00(0.00)\n",
      "Iter 1150 | Time 14.2396(14.5415) | Bit/dim 1.3901(1.3458) | Xent 0.0000(0.0000) | Loss 1.3901(1.3458) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 28.3457(15.0548) | Total Time 0.00(0.00)\n",
      "Iter 1160 | Time 14.9640(14.5643) | Bit/dim 1.3426(1.3467) | Xent 0.0000(0.0000) | Loss 1.3426(1.3467) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 13.9152(15.9668) | Total Time 0.00(0.00)\n",
      "Iter 1170 | Time 14.7179(14.5475) | Bit/dim 1.3429(1.3456) | Xent 0.0000(0.0000) | Loss 1.3429(1.3456) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 21.8669(16.3886) | Total Time 0.00(0.00)\n",
      "Iter 1180 | Time 14.5238(14.5242) | Bit/dim 1.2979(1.3398) | Xent 0.0000(0.0000) | Loss 1.2979(1.3398) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 4.5872(15.0897) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0018 | Time 56.5056, Epoch Time 1029.8908(948.6347), Bit/dim 1.3038, Xent 0.0000, Loss 1.3038, Error 1.0000\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1190 | Time 14.1864(14.4999) | Bit/dim 1.3040(1.3334) | Xent 0.0000(0.0000) | Loss 1.3040(1.3334) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 9.4847(13.8994) | Total Time 0.00(0.00)\n",
      "Iter 1200 | Time 14.6104(14.5094) | Bit/dim 1.3446(1.3326) | Xent 0.0000(0.0000) | Loss 1.3446(1.3326) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 27.4823(14.9670) | Total Time 0.00(0.00)\n",
      "Iter 1210 | Time 14.8320(14.5144) | Bit/dim 1.3156(1.3336) | Xent 0.0000(0.0000) | Loss 1.3156(1.3336) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 17.0496(16.1501) | Total Time 0.00(0.00)\n",
      "Iter 1220 | Time 14.5734(14.5132) | Bit/dim 1.3082(1.3286) | Xent 0.0000(0.0000) | Loss 1.3082(1.3286) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 7.5684(15.8520) | Total Time 0.00(0.00)\n",
      "Iter 1230 | Time 14.5074(14.5182) | Bit/dim 1.3296(1.3257) | Xent 0.0000(0.0000) | Loss 1.3296(1.3257) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 25.4193(15.4253) | Total Time 0.00(0.00)\n",
      "Iter 1240 | Time 14.7391(14.5448) | Bit/dim 1.3043(1.3251) | Xent 0.0000(0.0000) | Loss 1.3043(1.3251) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 17.4918(16.2145) | Total Time 0.00(0.00)\n",
      "Iter 1250 | Time 14.7428(14.5474) | Bit/dim 1.3277(1.3209) | Xent 0.0000(0.0000) | Loss 1.3277(1.3209) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 21.6978(15.4403) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0019 | Time 54.9766, Epoch Time 1028.7991(951.0397), Bit/dim 1.2966, Xent 0.0000, Loss 1.2966, Error 1.0000\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1260 | Time 14.7590(14.5392) | Bit/dim 1.3216(1.3186) | Xent 0.0000(0.0000) | Loss 1.3216(1.3186) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 28.2336(15.6741) | Total Time 0.00(0.00)\n",
      "Iter 1270 | Time 14.7885(14.5432) | Bit/dim 1.2948(1.3177) | Xent 0.0000(0.0000) | Loss 1.2948(1.3177) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 1.7671(15.8799) | Total Time 0.00(0.00)\n",
      "Iter 1280 | Time 14.2336(14.5326) | Bit/dim 1.3264(1.3174) | Xent 0.0000(0.0000) | Loss 1.3264(1.3174) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 22.0748(16.5178) | Total Time 0.00(0.00)\n",
      "Iter 1290 | Time 14.3841(14.5194) | Bit/dim 1.3675(1.3272) | Xent 0.0000(0.0000) | Loss 1.3675(1.3272) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 26.4013(18.2011) | Total Time 0.00(0.00)\n",
      "Iter 1300 | Time 14.3305(14.5321) | Bit/dim 1.3391(1.3308) | Xent 0.0000(0.0000) | Loss 1.3391(1.3308) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 19.0527(18.7256) | Total Time 0.00(0.00)\n",
      "Iter 1310 | Time 14.4669(14.5281) | Bit/dim 1.3077(1.3277) | Xent 0.0000(0.0000) | Loss 1.3077(1.3277) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 15.2249(18.2366) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0020 | Time 56.3098, Epoch Time 1029.7214(953.4001), Bit/dim 1.2813, Xent 0.0000, Loss 1.2813, Error 1.0000\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1320 | Time 14.7676(14.5683) | Bit/dim 1.2737(1.3199) | Xent 0.0000(0.0000) | Loss 1.2737(1.3199) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 1.3074(15.8341) | Total Time 0.00(0.00)\n",
      "Iter 1330 | Time 15.0185(14.5625) | Bit/dim 1.3372(1.3164) | Xent 0.0000(0.0000) | Loss 1.3372(1.3164) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 31.0941(15.9211) | Total Time 0.00(0.00)\n",
      "Iter 1340 | Time 14.6456(14.5170) | Bit/dim 1.2880(1.3145) | Xent 0.0000(0.0000) | Loss 1.2880(1.3145) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 5.7556(16.5178) | Total Time 0.00(0.00)\n",
      "Iter 1350 | Time 14.6405(14.5055) | Bit/dim 1.2692(1.3068) | Xent 0.0000(0.0000) | Loss 1.2692(1.3068) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 0.9181(14.1024) | Total Time 0.00(0.00)\n",
      "Iter 1360 | Time 14.5640(14.5237) | Bit/dim 1.2661(1.2998) | Xent 0.0000(0.0000) | Loss 1.2661(1.2998) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 3.8697(11.6435) | Total Time 0.00(0.00)\n",
      "Iter 1370 | Time 14.4714(14.5313) | Bit/dim 1.2611(1.2931) | Xent 0.0000(0.0000) | Loss 1.2611(1.2931) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 1.4259(9.2405) | Total Time 0.00(0.00)\n",
      "Iter 1380 | Time 14.5537(14.5439) | Bit/dim 1.2718(1.2870) | Xent 0.0000(0.0000) | Loss 1.2718(1.2870) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 1.0170(7.3498) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0021 | Time 56.8270, Epoch Time 1029.9473(955.6965), Bit/dim 1.2701, Xent 0.0000, Loss 1.2701, Error 1.0000\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1390 | Time 15.0390(14.5697) | Bit/dim 1.2872(1.2847) | Xent 0.0000(0.0000) | Loss 1.2872(1.2847) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 18.6634(8.6529) | Total Time 0.00(0.00)\n",
      "Iter 1400 | Time 14.6404(14.5946) | Bit/dim 1.2581(1.2832) | Xent 0.0000(0.0000) | Loss 1.2581(1.2832) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 4.1051(9.7340) | Total Time 0.00(0.00)\n",
      "Iter 1410 | Time 14.0685(14.5863) | Bit/dim 1.3733(1.3021) | Xent 0.0000(0.0000) | Loss 1.3733(1.3021) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 23.3959(14.3206) | Total Time 0.00(0.00)\n",
      "Iter 1420 | Time 14.6510(14.6171) | Bit/dim 1.3091(1.3209) | Xent 0.0000(0.0000) | Loss 1.3091(1.3209) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 7.7040(17.5916) | Total Time 0.00(0.00)\n",
      "Iter 1430 | Time 14.9886(14.6801) | Bit/dim 1.2900(1.3170) | Xent 0.0000(0.0000) | Loss 1.2900(1.3170) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 11.0143(16.9336) | Total Time 0.00(0.00)\n",
      "Iter 1440 | Time 14.6971(14.7042) | Bit/dim 1.2634(1.3088) | Xent 0.0000(0.0000) | Loss 1.2634(1.3088) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 3.1285(15.4495) | Total Time 0.00(0.00)\n",
      "Iter 1450 | Time 14.8823(14.7574) | Bit/dim 1.2709(1.3002) | Xent 0.0000(0.0000) | Loss 1.2709(1.3002) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 4.4696(12.7424) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0022 | Time 57.3017, Epoch Time 1044.2176(958.3522), Bit/dim 1.2603, Xent 0.0000, Loss 1.2603, Error 1.0000\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1460 | Time 14.7315(14.7511) | Bit/dim 1.2734(1.2914) | Xent 0.0000(0.0000) | Loss 1.2734(1.2914) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 0.6381(10.1002) | Total Time 0.00(0.00)\n",
      "Iter 1470 | Time 14.8446(14.7385) | Bit/dim 1.2505(1.2837) | Xent 0.0000(0.0000) | Loss 1.2505(1.2837) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 2.6238(7.8853) | Total Time 0.00(0.00)\n",
      "Iter 1480 | Time 14.4712(14.7427) | Bit/dim 1.2614(1.2767) | Xent 0.0000(0.0000) | Loss 1.2614(1.2767) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 7.3171(6.5870) | Total Time 0.00(0.00)\n",
      "Iter 1490 | Time 14.3974(14.7229) | Bit/dim 1.2663(1.2738) | Xent 0.0000(0.0000) | Loss 1.2663(1.2738) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 13.1396(8.0098) | Total Time 0.00(0.00)\n",
      "Iter 1500 | Time 14.9224(14.7213) | Bit/dim 1.2584(1.2711) | Xent 0.0000(0.0000) | Loss 1.2584(1.2711) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 11.0471(9.5615) | Total Time 0.00(0.00)\n",
      "Iter 1510 | Time 14.7769(14.7169) | Bit/dim 1.2328(1.2679) | Xent 0.0000(0.0000) | Loss 1.2328(1.2679) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 16.5832(10.4042) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0023 | Time 53.5777, Epoch Time 1037.7835(960.7351), Bit/dim 1.3170, Xent 0.0000, Loss 1.3170, Error 1.0000\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1520 | Time 14.9325(14.6949) | Bit/dim 1.2979(1.2695) | Xent 0.0000(0.0000) | Loss 1.2979(1.2695) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 35.1630(12.4213) | Total Time 0.00(0.00)\n",
      "Iter 1530 | Time 14.2465(14.6330) | Bit/dim 1.4806(1.3039) | Xent 0.0000(0.0000) | Loss 1.4806(1.3039) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 16.3898(16.5507) | Total Time 0.00(0.00)\n",
      "Iter 1540 | Time 14.9787(14.6252) | Bit/dim 1.2949(1.3267) | Xent 0.0000(0.0000) | Loss 1.2949(1.3267) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 7.9582(17.1266) | Total Time 0.00(0.00)\n",
      "Iter 1550 | Time 13.9320(14.6050) | Bit/dim 1.3062(1.3274) | Xent 0.0000(0.0000) | Loss 1.3062(1.3274) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 15.7023(17.4868) | Total Time 0.00(0.00)\n",
      "Iter 1560 | Time 14.8681(14.6116) | Bit/dim 1.2843(1.3154) | Xent 0.0000(0.0000) | Loss 1.2843(1.3154) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 14.9484(15.8744) | Total Time 0.00(0.00)\n",
      "Iter 1570 | Time 14.9244(14.6289) | Bit/dim 1.2729(1.3026) | Xent 0.0000(0.0000) | Loss 1.2729(1.3026) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 8.2035(13.7411) | Total Time 0.00(0.00)\n",
      "Iter 1580 | Time 14.7338(14.6483) | Bit/dim 1.2491(1.2900) | Xent 0.0000(0.0000) | Loss 1.2491(1.2900) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 7.1616(11.3849) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0024 | Time 54.2932, Epoch Time 1032.7153(962.8945), Bit/dim 1.2465, Xent 0.0000, Loss 1.2465, Error 1.0000\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1590 | Time 14.4568(14.6570) | Bit/dim 1.2528(1.2802) | Xent 0.0000(0.0000) | Loss 1.2528(1.2802) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 6.4768(9.6142) | Total Time 0.00(0.00)\n",
      "Iter 1600 | Time 14.9520(14.7007) | Bit/dim 1.2411(1.2712) | Xent 0.0000(0.0000) | Loss 1.2411(1.2712) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 4.6910(8.0763) | Total Time 0.00(0.00)\n",
      "Iter 1610 | Time 14.6882(14.6994) | Bit/dim 1.2536(1.2647) | Xent 0.0000(0.0000) | Loss 1.2536(1.2647) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 2.7120(6.7087) | Total Time 0.00(0.00)\n",
      "Iter 1620 | Time 14.5733(14.6937) | Bit/dim 1.2399(1.2593) | Xent 0.0000(0.0000) | Loss 1.2399(1.2593) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 1.4827(5.8080) | Total Time 0.00(0.00)\n",
      "Iter 1630 | Time 14.6784(14.7043) | Bit/dim 1.2305(1.2560) | Xent 0.0000(0.0000) | Loss 1.2305(1.2560) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 7.4131(6.3893) | Total Time 0.00(0.00)\n",
      "Iter 1640 | Time 14.9985(14.6760) | Bit/dim 1.2464(1.2537) | Xent 0.0000(0.0000) | Loss 1.2464(1.2537) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 9.9534(7.8264) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0025 | Time 54.8354, Epoch Time 1038.4400(965.1609), Bit/dim 1.2323, Xent 0.0000, Loss 1.2323, Error 1.0000\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1650 | Time 14.8186(14.6677) | Bit/dim 1.2340(1.2502) | Xent 0.0000(0.0000) | Loss 1.2340(1.2502) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 3.9189(8.4911) | Total Time 0.00(0.00)\n",
      "Iter 1660 | Time 14.3562(14.6485) | Bit/dim 1.4707(1.2638) | Xent 0.0000(0.0000) | Loss 1.4707(1.2638) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 32.8897(12.4193) | Total Time 0.00(0.00)\n",
      "Iter 1670 | Time 14.9367(14.6175) | Bit/dim 1.5077(1.3135) | Xent 0.0000(0.0000) | Loss 1.5077(1.3135) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 40.6372(15.0601) | Total Time 0.00(0.00)\n",
      "Iter 1680 | Time 14.7529(14.5839) | Bit/dim 1.2918(1.3322) | Xent 0.0000(0.0000) | Loss 1.2918(1.3322) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 9.4829(15.6119) | Total Time 0.00(0.00)\n",
      "Iter 1690 | Time 14.5719(14.5589) | Bit/dim 1.2681(1.3252) | Xent 0.0000(0.0000) | Loss 1.2681(1.3252) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 10.4581(15.1521) | Total Time 0.00(0.00)\n",
      "Iter 1700 | Time 14.9340(14.5471) | Bit/dim 1.2544(1.3076) | Xent 0.0000(0.0000) | Loss 1.2544(1.3076) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 6.3569(13.6958) | Total Time 0.00(0.00)\n",
      "Iter 1710 | Time 14.8348(14.5533) | Bit/dim 1.2362(1.2923) | Xent 0.0000(0.0000) | Loss 1.2362(1.2923) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 2.4801(11.9012) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0026 | Time 53.8711, Epoch Time 1027.0922(967.0188), Bit/dim 1.2362, Xent 0.0000, Loss 1.2362, Error 1.0000\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1720 | Time 14.6201(14.5518) | Bit/dim 1.2522(1.2794) | Xent 0.0000(0.0000) | Loss 1.2522(1.2794) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 6.2923(10.2689) | Total Time 0.00(0.00)\n",
      "Iter 1730 | Time 14.7303(14.5694) | Bit/dim 1.2363(1.2693) | Xent 0.0000(0.0000) | Loss 1.2363(1.2693) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 0.8433(8.3276) | Total Time 0.00(0.00)\n",
      "Iter 1740 | Time 14.5679(14.5614) | Bit/dim 1.2372(1.2609) | Xent 0.0000(0.0000) | Loss 1.2372(1.2609) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 0.4884(6.5955) | Total Time 0.00(0.00)\n",
      "Iter 1750 | Time 14.5466(14.5631) | Bit/dim 1.2341(1.2528) | Xent 0.0000(0.0000) | Loss 1.2341(1.2528) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 1.2836(5.1767) | Total Time 0.00(0.00)\n",
      "Iter 1760 | Time 14.7568(14.5979) | Bit/dim 1.2302(1.2474) | Xent 0.0000(0.0000) | Loss 1.2302(1.2474) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 2.1234(4.4775) | Total Time 0.00(0.00)\n",
      "Iter 1770 | Time 14.6998(14.6251) | Bit/dim 1.2435(1.2425) | Xent 0.0000(0.0000) | Loss 1.2435(1.2425) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 8.5637(4.8045) | Total Time 0.00(0.00)\n",
      "Iter 1780 | Time 14.2905(14.6480) | Bit/dim 1.2296(1.2390) | Xent 0.0000(0.0000) | Loss 1.2296(1.2390) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 13.7351(6.2572) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0027 | Time 54.8657, Epoch Time 1034.9280(969.0561), Bit/dim 1.2321, Xent 0.0000, Loss 1.2321, Error 1.0000\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1790 | Time 14.6417(14.6317) | Bit/dim 1.2260(1.2380) | Xent 0.0000(0.0000) | Loss 1.2260(1.2380) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 4.8746(7.5930) | Total Time 0.00(0.00)\n",
      "Iter 1800 | Time 14.3891(14.5877) | Bit/dim 1.4486(1.2600) | Xent 0.0000(0.0000) | Loss 1.4486(1.2600) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 17.0403(12.1230) | Total Time 0.00(0.00)\n",
      "Iter 1810 | Time 14.4109(14.5760) | Bit/dim 1.3577(1.2938) | Xent 0.0000(0.0000) | Loss 1.3577(1.2938) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 13.5312(14.2720) | Total Time 0.00(0.00)\n",
      "Iter 1820 | Time 13.8784(14.5248) | Bit/dim 1.3387(1.3052) | Xent 0.0000(0.0000) | Loss 1.3387(1.3052) | Error 0.0000(0.0000) Steps 0(0.00) | Grad Norm 22.7614(15.4825) | Total Time 0.00(0.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p train_cnf_conditional_gate.py --data mnist --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 900 --save experiments/cnf_bs900_gate_dev_scale_1e_4 --conditional False --log_freq 10 --weight_y 0. --scale 0.0001"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
