{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0,1,2,3,4,5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_generative.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "if args.controlled_tol:\n",
      "    import lib.odenvp_conditional_tol as odenvp\n",
      "else:\n",
      "    import lib.odenvp_conditional as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    logpz_sup_per_dim = torch.sum(logpz_sup) / z[:, 0:dim_sup].nelement()  # averaged over batches\n",
      "    bits_sup_per_dim = -(logpz_sup_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute the predicted labels\n",
      "    all_y = torch.from_numpy(np.arange(model.module.y_class)).type(torch.long).to(x.get_device(), non_blocking=True)\n",
      "    all_y_onehot = thops.onehot(all_y, num_classes=model.module.y_class)\n",
      "    all_mean, all_logs = model.module._prior(all_y_onehot)\n",
      "    \n",
      "    zsup = zsup.view(zsup.shape[0], 1, zsup.shape[1])\n",
      "    zsup = zsup.repeat(1,model.module.y_class,1)\n",
      "    likelihood = modules.GaussianDiag.likelihood(all_mean, all_logs, zsup)\n",
      "    dims = [i+2 for i in range(len(likelihood.size()) - 2)]\n",
      "    all_logpz_sup = thops.sum(likelihood, dim=dims)\n",
      "    y_predicted = np.argmax(all_logpz_sup.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, bits_sup_per_dim, y_predicted\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    if args.conditional: best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            loss.backward()\n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}, Xent {:.4f}, Loss {:.4f}, Error {:.4f}\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, loss_xent, loss, error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, condition_ratio=0.25, conditional=True, controlled_tol=False, conv=True, data='mnist', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume=None, rtol=1e-05, save='../experiments/cnf_conditional_generative_bs900_sratio_0_25_drop_0_5_seed_1', seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5)\n",
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=392, bias=True)\n",
      "  (project_class): LinearZeros(in_features=196, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 807722\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc.py:286: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "Iter 0010 | Time 13.3110(33.1115) | Bit/dim 17.7701(19.1182) | Xent 26.3626(27.6169) | Loss 30.9514(32.9266) | Error 0.5189(0.8127) Steps 410(410.00) | Grad Norm 200.4031(220.4214) | Total Time 10.00(10.00)\n",
      "Iter 0020 | Time 13.2043(27.8449) | Bit/dim 14.7442(18.3155) | Xent 23.2340(26.8302) | Loss 26.3612(31.7306) | Error 0.6222(0.7535) Steps 410(410.00) | Grad Norm 138.7900(206.3084) | Total Time 10.00(10.00)\n",
      "Iter 0030 | Time 13.2630(23.9688) | Bit/dim 12.1435(16.9494) | Xent 20.1802(25.3949) | Loss 22.2336(29.6469) | Error 0.4933(0.7005) Steps 410(410.00) | Grad Norm 70.1873(178.2410) | Total Time 10.00(10.00)\n",
      "Iter 0040 | Time 13.0979(21.0989) | Bit/dim 10.4487(15.4157) | Xent 18.5968(23.7500) | Loss 19.7470(27.2907) | Error 0.4811(0.6401) Steps 410(410.00) | Grad Norm 27.9119(142.5459) | Total Time 10.00(10.00)\n",
      "Iter 0050 | Time 12.9657(18.9911) | Bit/dim 8.7341(13.8468) | Xent 17.3929(22.2206) | Loss 17.4306(24.9571) | Error 0.4544(0.5914) Steps 410(410.00) | Grad Norm 23.0854(111.4333) | Total Time 10.00(10.00)\n",
      "Iter 0060 | Time 12.7606(17.4037) | Bit/dim 7.3752(12.3105) | Xent 15.9894(20.7606) | Loss 15.3699(22.6908) | Error 0.4356(0.5498) Steps 410(410.00) | Grad Norm 22.4021(88.2099) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0001 | Time 93.6332, Epoch Time 994.4096(994.4096), Bit/dim 6.5677, Xent 15.0795, Loss 14.1075, Error 0.2691\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0070 | Time 13.1791(16.2781) | Bit/dim 6.1482(10.8365) | Xent 14.5995(19.3055) | Loss 13.4479(20.4892) | Error 0.4233(0.5132) Steps 410(410.00) | Grad Norm 19.2913(70.4781) | Total Time 10.00(10.00)\n",
      "Iter 0080 | Time 12.9354(15.4180) | Bit/dim 4.8841(9.4114) | Xent 13.2084(17.8598) | Loss 11.4883(18.3413) | Error 0.4267(0.4901) Steps 410(410.00) | Grad Norm 16.9180(56.6765) | Total Time 10.00(10.00)\n",
      "Iter 0090 | Time 13.1132(14.8062) | Bit/dim 3.8268(8.0625) | Xent 12.0304(16.4635) | Loss 9.8420(16.2942) | Error 0.4611(0.4774) Steps 416(410.35) | Grad Norm 13.4671(45.7647) | Total Time 10.00(10.00)\n",
      "Iter 0100 | Time 13.3290(14.4346) | Bit/dim 3.1526(6.8447) | Xent 11.2726(15.1815) | Loss 8.7889(14.4354) | Error 0.4922(0.4758) Steps 422(413.13) | Grad Norm 9.1009(36.6272) | Total Time 10.00(10.00)\n",
      "Iter 0110 | Time 13.6411(14.1804) | Bit/dim 2.7561(5.8094) | Xent 10.7516(14.0716) | Loss 8.1319(12.8452) | Error 0.5589(0.4868) Steps 428(415.99) | Grad Norm 5.8177(28.8625) | Total Time 10.00(10.00)\n",
      "Iter 0120 | Time 13.8624(14.0931) | Bit/dim 2.5347(4.9749) | Xent 10.4556(13.1568) | Loss 7.7625(11.5533) | Error 0.5733(0.5003) Steps 440(421.58) | Grad Norm 3.5589(22.4421) | Total Time 10.00(10.00)\n",
      "Iter 0130 | Time 14.1732(14.0911) | Bit/dim 2.4339(4.3179) | Xent 10.3130(12.4251) | Loss 7.5904(10.5304) | Error 0.5544(0.5159) Steps 440(426.42) | Grad Norm 2.7281(17.3453) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0002 | Time 96.3498, Epoch Time 999.5253(994.5630), Bit/dim 2.3906, Xent 10.2524, Loss 7.5168, Error 0.3464\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0140 | Time 13.8821(14.0884) | Bit/dim 2.3565(3.8095) | Xent 10.1911(11.8485) | Loss 7.4520(9.7338) | Error 0.5922(0.5293) Steps 446(431.42) | Grad Norm 2.1250(13.4196) | Total Time 10.00(10.00)\n",
      "Iter 0150 | Time 14.1397(14.0900) | Bit/dim 2.2703(3.4165) | Xent 10.0873(11.3966) | Loss 7.3140(9.1148) | Error 0.5533(0.5401) Steps 446(435.25) | Grad Norm 1.5284(10.3650) | Total Time 10.00(10.00)\n",
      "Iter 0160 | Time 14.3918(14.1064) | Bit/dim 2.2704(3.1176) | Xent 10.0241(11.0425) | Loss 7.2824(8.6389) | Error 0.5467(0.5500) Steps 446(438.07) | Grad Norm 1.1424(7.9801) | Total Time 10.00(10.00)\n",
      "Iter 0170 | Time 14.4903(14.1648) | Bit/dim 2.2533(2.8922) | Xent 9.9641(10.7639) | Loss 7.2354(8.2742) | Error 0.5633(0.5556) Steps 458(441.53) | Grad Norm 0.8792(6.1371) | Total Time 10.00(10.00)\n",
      "Iter 0180 | Time 14.6059(14.2542) | Bit/dim 2.2377(2.7243) | Xent 9.9144(10.5455) | Loss 7.1949(7.9971) | Error 0.5600(0.5579) Steps 452(444.57) | Grad Norm 0.7672(4.7405) | Total Time 10.00(10.00)\n",
      "Iter 0190 | Time 14.6494(14.3237) | Bit/dim 2.2342(2.5977) | Xent 9.8772(10.3735) | Loss 7.1728(7.7845) | Error 0.5500(0.5595) Steps 452(446.52) | Grad Norm 0.7210(3.6887) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0003 | Time 94.5267, Epoch Time 1052.6064(996.3043), Bit/dim 2.2303, Xent 9.8463, Loss 7.1535, Error 0.4365\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0200 | Time 14.3938(14.3705) | Bit/dim 2.2395(2.5042) | Xent 9.8509(10.2389) | Loss 7.1649(7.6237) | Error 0.5378(0.5568) Steps 452(447.96) | Grad Norm 0.6753(2.9014) | Total Time 10.00(10.00)\n",
      "Iter 0210 | Time 14.4339(14.3994) | Bit/dim 2.2206(2.4313) | Xent 9.8270(10.1330) | Loss 7.1341(7.4978) | Error 0.5400(0.5529) Steps 452(449.02) | Grad Norm 0.6539(2.3087) | Total Time 10.00(10.00)\n",
      "Iter 0220 | Time 13.9822(14.3962) | Bit/dim 2.1921(2.3750) | Xent 9.8085(10.0504) | Loss 7.0963(7.4002) | Error 0.5456(0.5518) Steps 440(448.76) | Grad Norm 0.6107(1.8637) | Total Time 10.00(10.00)\n",
      "Iter 0230 | Time 14.1465(14.3140) | Bit/dim 2.2042(2.3310) | Xent 9.8023(9.9856) | Loss 7.1054(7.3237) | Error 0.5500(0.5495) Steps 440(446.46) | Grad Norm 0.5541(1.5251) | Total Time 10.00(10.00)\n",
      "Iter 0240 | Time 13.7810(14.2374) | Bit/dim 2.1958(2.2967) | Xent 9.7814(9.9344) | Loss 7.0865(7.2639) | Error 0.5144(0.5451) Steps 440(444.76) | Grad Norm 0.5160(1.2663) | Total Time 10.00(10.00)\n",
      "Iter 0250 | Time 14.0011(14.1889) | Bit/dim 2.1949(2.2688) | Xent 9.7739(9.8934) | Loss 7.0819(7.2155) | Error 0.5333(0.5445) Steps 440(443.51) | Grad Norm 0.4935(1.0691) | Total Time 10.00(10.00)\n",
      "Iter 0260 | Time 14.0984(14.1493) | Bit/dim 2.1765(2.2465) | Xent 9.7695(9.8609) | Loss 7.0613(7.1770) | Error 0.5533(0.5422) Steps 446(442.77) | Grad Norm 0.4840(0.9178) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0004 | Time 96.4006, Epoch Time 1046.1841(997.8007), Bit/dim 2.1656, Xent 9.7580, Loss 7.0446, Error 0.4847\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0270 | Time 14.1493(14.1467) | Bit/dim 2.1624(2.2248) | Xent 9.7554(9.8344) | Loss 7.0401(7.1420) | Error 0.5600(0.5428) Steps 446(443.62) | Grad Norm 0.4664(0.8086) | Total Time 10.00(10.00)\n",
      "Iter 0280 | Time 14.4812(14.1872) | Bit/dim 2.1373(2.2068) | Xent 9.7446(9.8119) | Loss 7.0096(7.1127) | Error 0.5122(0.5408) Steps 452(444.26) | Grad Norm 0.4628(0.7174) | Total Time 10.00(10.00)\n",
      "Iter 0290 | Time 13.9691(14.1757) | Bit/dim 2.1434(2.1900) | Xent 9.7382(9.7933) | Loss 7.0125(7.0867) | Error 0.5411(0.5426) Steps 446(445.00) | Grad Norm 0.4987(0.6580) | Total Time 10.00(10.00)\n",
      "Iter 0300 | Time 14.1099(14.1757) | Bit/dim 2.1365(2.1762) | Xent 9.7305(9.7771) | Loss 7.0018(7.0647) | Error 0.5767(0.5456) Steps 446(445.26) | Grad Norm 0.4998(0.6112) | Total Time 10.00(10.00)\n",
      "Iter 0310 | Time 14.0104(14.1375) | Bit/dim 2.1295(2.1607) | Xent 9.7187(9.7627) | Loss 6.9888(7.0420) | Error 0.5633(0.5526) Steps 434(443.51) | Grad Norm 0.4103(0.5686) | Total Time 10.00(10.00)\n",
      "Iter 0320 | Time 13.7866(14.0584) | Bit/dim 2.1089(2.1472) | Xent 9.7058(9.7494) | Loss 6.9618(7.0219) | Error 0.5800(0.5591) Steps 434(441.29) | Grad Norm 0.4994(0.5430) | Total Time 10.00(10.00)\n",
      "Iter 0330 | Time 13.9451(14.0082) | Bit/dim 2.0971(2.1343) | Xent 9.6994(9.7375) | Loss 6.9468(7.0030) | Error 0.6078(0.5682) Steps 434(439.37) | Grad Norm 0.6205(0.5305) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0005 | Time 91.6977, Epoch Time 1033.6678(998.8767), Bit/dim 2.0863, Xent 9.6966, Loss 6.9346, Error 0.6231\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0340 | Time 14.0229(13.9791) | Bit/dim 2.0805(2.1212) | Xent 9.6864(9.7259) | Loss 6.9237(6.9842) | Error 0.5667(0.5754) Steps 434(437.96) | Grad Norm 0.4005(0.5207) | Total Time 10.00(10.00)\n",
      "Iter 0350 | Time 14.1689(13.9723) | Bit/dim 2.0738(2.1087) | Xent 9.6799(9.7149) | Loss 6.9137(6.9661) | Error 0.6233(0.5857) Steps 440(437.10) | Grad Norm 0.5528(0.5051) | Total Time 10.00(10.00)\n",
      "Iter 0360 | Time 14.1907(14.0234) | Bit/dim 2.0387(2.0960) | Xent 9.6738(9.7042) | Loss 6.8756(6.9481) | Error 0.6489(0.5986) Steps 440(437.73) | Grad Norm 0.4175(0.4890) | Total Time 10.00(10.00)\n",
      "Iter 0370 | Time 14.0160(14.0516) | Bit/dim 2.0371(2.0853) | Xent 9.6658(9.6942) | Loss 6.8700(6.9324) | Error 0.6600(0.6124) Steps 440(438.32) | Grad Norm 0.4207(0.4710) | Total Time 10.00(10.00)\n",
      "Iter 0380 | Time 14.2260(14.0935) | Bit/dim 2.0524(2.0747) | Xent 9.6518(9.6844) | Loss 6.8783(6.9169) | Error 0.6667(0.6244) Steps 440(438.76) | Grad Norm 0.4617(0.4834) | Total Time 10.00(10.00)\n",
      "Iter 0390 | Time 14.3079(14.1031) | Bit/dim 2.0212(2.0631) | Xent 9.6474(9.6752) | Loss 6.8449(6.9007) | Error 0.6633(0.6350) Steps 440(439.43) | Grad Norm 0.4054(0.4821) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0006 | Time 93.8281, Epoch Time 1035.9998(999.9904), Bit/dim 2.0194, Xent 9.6391, Loss 6.8389, Error 0.7399\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0400 | Time 14.2714(14.0947) | Bit/dim 2.0362(2.0538) | Xent 9.6348(9.6665) | Loss 6.8536(6.8870) | Error 0.6811(0.6456) Steps 440(440.10) | Grad Norm 0.6046(0.4716) | Total Time 10.00(10.00)\n",
      "Iter 0410 | Time 14.6592(14.2092) | Bit/dim 2.0110(2.0431) | Xent 9.6336(9.6582) | Loss 6.8278(6.8722) | Error 0.7056(0.6573) Steps 458(444.80) | Grad Norm 0.4946(0.4692) | Total Time 10.00(10.00)\n",
      "Iter 0420 | Time 14.6065(14.2833) | Bit/dim 1.9999(2.0334) | Xent 9.6309(9.6506) | Loss 6.8153(6.8587) | Error 0.7044(0.6680) Steps 458(448.27) | Grad Norm 0.3961(0.4846) | Total Time 10.00(10.00)\n",
      "Iter 0430 | Time 14.4242(14.3402) | Bit/dim 1.9674(2.0237) | Xent 9.6207(9.6435) | Loss 6.7778(6.8455) | Error 0.7022(0.6781) Steps 458(450.82) | Grad Norm 0.7970(0.5441) | Total Time 10.00(10.00)\n",
      "Iter 0440 | Time 14.3530(14.3654) | Bit/dim 1.9791(2.0155) | Xent 9.6178(9.6367) | Loss 6.7881(6.8338) | Error 0.7144(0.6860) Steps 458(452.71) | Grad Norm 1.0070(0.6768) | Total Time 10.00(10.00)\n",
      "Iter 0450 | Time 14.8677(14.4379) | Bit/dim 1.9703(2.0060) | Xent 9.6114(9.6308) | Loss 6.7760(6.8214) | Error 0.7067(0.6921) Steps 464(454.94) | Grad Norm 2.1871(0.9155) | Total Time 10.00(10.00)\n",
      "Iter 0460 | Time 14.7440(14.5267) | Bit/dim 1.9966(1.9994) | Xent 9.6094(9.6253) | Loss 6.8013(6.8121) | Error 0.7211(0.6979) Steps 464(457.32) | Grad Norm 1.2175(1.1629) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0007 | Time 99.1492, Epoch Time 1072.6721(1002.1709), Bit/dim 1.9680, Xent 9.6054, Loss 6.7707, Error 0.7774\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0470 | Time 14.7820(14.5976) | Bit/dim 1.9912(1.9919) | Xent 9.6002(9.6201) | Loss 6.7913(6.8019) | Error 0.7422(0.7045) Steps 470(459.43) | Grad Norm 0.4216(1.1588) | Total Time 10.00(10.00)\n",
      "Iter 0480 | Time 14.8196(14.6470) | Bit/dim 1.9696(1.9816) | Xent 9.5984(9.6149) | Loss 6.7687(6.7891) | Error 0.7189(0.7078) Steps 470(462.21) | Grad Norm 0.6464(1.0111) | Total Time 10.00(10.00)\n",
      "Iter 0490 | Time 14.8392(14.6706) | Bit/dim 1.9497(1.9755) | Xent 9.5932(9.6097) | Loss 6.7463(6.7803) | Error 0.7267(0.7125) Steps 470(464.25) | Grad Norm 3.6081(1.1947) | Total Time 10.00(10.00)\n",
      "Iter 0500 | Time 14.6961(14.7086) | Bit/dim 1.9657(1.9719) | Xent 9.5869(9.6047) | Loss 6.7592(6.7743) | Error 0.7489(0.7159) Steps 470(465.76) | Grad Norm 4.1790(2.1353) | Total Time 10.00(10.00)\n",
      "Iter 0510 | Time 14.3651(14.7138) | Bit/dim 1.9507(1.9641) | Xent 9.5845(9.6000) | Loss 6.7429(6.7641) | Error 0.7400(0.7204) Steps 470(466.87) | Grad Norm 1.4200(2.0748) | Total Time 10.00(10.00)\n",
      "Iter 0520 | Time 14.7412(14.7416) | Bit/dim 1.9305(1.9561) | Xent 9.5792(9.5947) | Loss 6.7201(6.7534) | Error 0.7500(0.7227) Steps 470(467.70) | Grad Norm 0.5048(1.8407) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0008 | Time 101.6440, Epoch Time 1091.8209(1004.8604), Bit/dim 1.9255, Xent 9.5682, Loss 6.7096, Error 0.7785\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0530 | Time 15.2878(14.8137) | Bit/dim 1.9304(1.9505) | Xent 9.5705(9.5890) | Loss 6.7156(6.7450) | Error 0.7478(0.7239) Steps 476(469.30) | Grad Norm 0.6709(1.5445) | Total Time 10.00(10.00)\n",
      "Iter 0540 | Time 15.3941(14.9128) | Bit/dim 1.9267(1.9440) | Xent 9.5595(9.5822) | Loss 6.7064(6.7351) | Error 0.7356(0.7257) Steps 482(472.06) | Grad Norm 0.6481(1.2902) | Total Time 10.00(10.00)\n",
      "Iter 0550 | Time 15.1553(15.0203) | Bit/dim 1.9081(1.9359) | Xent 9.5543(9.5753) | Loss 6.6853(6.7235) | Error 0.7122(0.7247) Steps 482(474.67) | Grad Norm 0.4164(1.0988) | Total Time 10.00(10.00)\n",
      "Iter 0560 | Time 15.0837(15.0816) | Bit/dim 1.9042(1.9303) | Xent 9.5415(9.5672) | Loss 6.6749(6.7139) | Error 0.7144(0.7254) Steps 482(476.59) | Grad Norm 0.4289(0.9888) | Total Time 10.00(10.00)\n",
      "Iter 0570 | Time 15.3102(15.1255) | Bit/dim 1.8825(1.9232) | Xent 9.5339(9.5589) | Loss 6.6495(6.7027) | Error 0.7178(0.7273) Steps 482(478.01) | Grad Norm 0.5824(0.8581) | Total Time 10.00(10.00)\n",
      "Iter 0580 | Time 15.3367(15.1502) | Bit/dim 1.9107(1.9161) | Xent 9.5170(9.5498) | Loss 6.6692(6.6910) | Error 0.7389(0.7297) Steps 482(479.06) | Grad Norm 6.4450(1.1781) | Total Time 10.00(10.00)\n",
      "Iter 0590 | Time 15.0180(15.1586) | Bit/dim 1.9026(1.9166) | Xent 9.5100(9.5409) | Loss 6.6576(6.6871) | Error 0.7367(0.7291) Steps 482(479.83) | Grad Norm 8.1760(3.4596) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0009 | Time 104.7523, Epoch Time 1124.0402(1008.4358), Bit/dim 1.8740, Xent 9.5089, Loss 6.6284, Error 0.7868\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0600 | Time 15.1552(15.2005) | Bit/dim 1.8735(1.9062) | Xent 9.5024(9.5325) | Loss 6.6247(6.6724) | Error 0.7411(0.7268) Steps 482(480.40) | Grad Norm 2.2352(3.4331) | Total Time 10.00(10.00)\n",
      "Iter 0610 | Time 15.3151(15.2294) | Bit/dim 1.8550(1.8936) | Xent 9.4899(9.5228) | Loss 6.6000(6.6550) | Error 0.7489(0.7277) Steps 482(480.82) | Grad Norm 1.7688(3.1381) | Total Time 10.00(10.00)\n",
      "Iter 0620 | Time 15.5811(15.2686) | Bit/dim 1.8272(1.8806) | Xent 9.4628(9.5104) | Loss 6.5586(6.6358) | Error 0.7378(0.7302) Steps 482(481.13) | Grad Norm 2.0253(2.8123) | Total Time 10.00(10.00)\n",
      "Iter 0630 | Time 15.0378(15.2749) | Bit/dim 1.7997(1.8644) | Xent 9.4443(9.4953) | Loss 6.5218(6.6120) | Error 0.7378(0.7326) Steps 482(481.36) | Grad Norm 0.5052(2.3138) | Total Time 10.00(10.00)\n",
      "Iter 0640 | Time 15.2627(15.2463) | Bit/dim 1.7620(1.8428) | Xent 9.4191(9.4776) | Loss 6.4715(6.5816) | Error 0.7078(0.7299) Steps 482(481.24) | Grad Norm 1.1007(1.9522) | Total Time 10.00(10.00)\n",
      "Iter 0650 | Time 15.1746(15.2325) | Bit/dim 1.7483(1.8185) | Xent 9.3848(9.4568) | Loss 6.4407(6.5469) | Error 0.7122(0.7298) Steps 482(481.14) | Grad Norm 0.8073(1.9083) | Total Time 10.00(10.00)\n",
      "Iter 0660 | Time 15.2067(15.1949) | Bit/dim 1.7313(1.7972) | Xent 9.3476(9.4343) | Loss 6.4051(6.5144) | Error 0.7533(0.7328) Steps 482(480.88) | Grad Norm 7.7750(3.7709) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0010 | Time 104.5394, Epoch Time 1124.1580(1011.9074), Bit/dim 1.7541, Xent 9.3552, Loss 6.4317, Error 0.7738\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0670 | Time 15.3102(15.1693) | Bit/dim 1.6840(1.7725) | Xent 9.3113(9.4075) | Loss 6.3396(6.4763) | Error 0.7644(0.7380) Steps 482(480.74) | Grad Norm 6.0485(4.8333) | Total Time 10.00(10.00)\n",
      "Iter 0680 | Time 15.0546(15.1583) | Bit/dim 1.6579(1.7467) | Xent 9.2738(9.3776) | Loss 6.2948(6.4355) | Error 0.7300(0.7411) Steps 482(481.07) | Grad Norm 3.5104(4.5620) | Total Time 10.00(10.00)\n",
      "Iter 0690 | Time 14.9773(15.1433) | Bit/dim 1.6348(1.7228) | Xent 9.2265(9.3437) | Loss 6.2480(6.3947) | Error 0.7189(0.7425) Steps 482(481.32) | Grad Norm 1.3798(4.2652) | Total Time 10.00(10.00)\n",
      "Iter 0700 | Time 15.1988(15.1778) | Bit/dim 1.6623(1.7066) | Xent 9.1899(9.3138) | Loss 6.2572(6.3635) | Error 0.8022(0.7508) Steps 488(482.31) | Grad Norm 11.8539(6.9329) | Total Time 10.00(10.00)\n",
      "Iter 0710 | Time 15.2600(15.2459) | Bit/dim 1.6577(1.6927) | Xent 9.1198(9.2727) | Loss 6.2176(6.3290) | Error 0.7433(0.7544) Steps 476(483.80) | Grad Norm 14.2550(8.4087) | Total Time 10.00(10.00)\n",
      "Iter 0720 | Time 15.8660(15.3786) | Bit/dim 1.6250(1.6800) | Xent 9.0653(9.2256) | Loss 6.1577(6.2928) | Error 0.7400(0.7586) Steps 506(487.01) | Grad Norm 3.9538(8.7552) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0011 | Time 110.2485, Epoch Time 1136.3005(1015.6392), Bit/dim 1.6151, Xent 9.0250, Loss 6.1276, Error 0.8286\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0730 | Time 16.2792(15.5216) | Bit/dim 1.6107(1.6634) | Xent 8.9924(9.1729) | Loss 6.1069(6.2499) | Error 0.7567(0.7561) Steps 512(490.79) | Grad Norm 7.7715(8.2122) | Total Time 10.00(10.00)\n",
      "Iter 0740 | Time 15.8409(15.6544) | Bit/dim 1.6650(1.6667) | Xent 8.8889(9.1572) | Loss 6.1094(6.2453) | Error 0.7578(0.7656) Steps 512(495.90) | Grad Norm 9.7381(12.8329) | Total Time 10.00(10.00)\n",
      "Iter 0750 | Time 16.3007(15.7949) | Bit/dim 1.6407(1.6627) | Xent 8.9363(9.1131) | Loss 6.1089(6.2193) | Error 0.7956(0.7787) Steps 512(500.14) | Grad Norm 11.2022(13.4999) | Total Time 10.00(10.00)\n",
      "Iter 0760 | Time 16.7271(15.9820) | Bit/dim 1.6025(1.6490) | Xent 8.8389(9.0498) | Loss 6.0220(6.1739) | Error 0.8022(0.7805) Steps 524(506.27) | Grad Norm 3.1788(11.3968) | Total Time 10.00(10.00)\n",
      "Iter 0770 | Time 16.1730(16.0846) | Bit/dim 1.6080(1.6349) | Xent 8.7220(8.9774) | Loss 5.9690(6.1236) | Error 0.7744(0.7738) Steps 518(510.73) | Grad Norm 2.5975(9.8999) | Total Time 10.00(10.00)\n",
      "Iter 0780 | Time 16.4991(16.1922) | Bit/dim 1.6110(1.6280) | Xent 8.6014(8.8949) | Loss 5.9117(6.0755) | Error 0.7811(0.7711) Steps 524(514.12) | Grad Norm 9.1489(10.6782) | Total Time 10.00(10.00)\n",
      "Iter 0790 | Time 16.4779(16.2676) | Bit/dim 1.6060(1.6247) | Xent 8.5001(8.8001) | Loss 5.8560(6.0248) | Error 0.7922(0.7689) Steps 524(516.58) | Grad Norm 15.2364(10.9969) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0012 | Time 113.0232, Epoch Time 1204.0955(1021.2929), Bit/dim 1.5884, Xent 8.4311, Loss 5.8040, Error 0.8276\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0800 | Time 16.4910(16.3048) | Bit/dim 1.5967(1.6191) | Xent 8.3258(8.6890) | Loss 5.7596(5.9636) | Error 0.7522(0.7635) Steps 524(518.18) | Grad Norm 11.4519(10.3485) | Total Time 10.00(10.00)\n",
      "Iter 0810 | Time 17.0277(16.4262) | Bit/dim 1.6487(1.6386) | Xent 8.2391(8.5972) | Loss 5.7682(5.9372) | Error 0.8444(0.7737) Steps 548(522.18) | Grad Norm 22.3102(15.7720) | Total Time 10.00(10.00)\n",
      "Iter 0820 | Time 17.2553(16.5235) | Bit/dim 1.6344(1.6395) | Xent 8.0638(8.4656) | Loss 5.6663(5.8723) | Error 0.7811(0.7711) Steps 548(525.88) | Grad Norm 13.7009(14.5542) | Total Time 10.00(10.00)\n",
      "Iter 0830 | Time 18.9359(16.7353) | Bit/dim 2.0876(1.7238) | Xent 8.5327(8.4987) | Loss 6.3540(5.9732) | Error 0.8878(0.7833) Steps 608(533.28) | Grad Norm 46.3311(23.0142) | Total Time 10.00(10.00)\n",
      "Iter 0840 | Time 19.8332(17.3953) | Bit/dim 1.8416(1.7753) | Xent 8.2791(8.4837) | Loss 5.9811(6.0171) | Error 0.7944(0.7983) Steps 644(556.63) | Grad Norm 13.5237(23.8288) | Total Time 10.00(10.00)\n",
      "Iter 0850 | Time 18.8161(17.9840) | Bit/dim 1.7319(1.7621) | Xent 8.0591(8.4063) | Loss 5.7615(5.9653) | Error 0.8278(0.8001) Steps 614(578.08) | Grad Norm 7.1031(20.2486) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0013 | Time 127.6179, Epoch Time 1316.0589(1030.1359), Bit/dim 1.6261, Xent 8.0122, Loss 5.6322, Error 0.8613\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0860 | Time 18.1890(18.0812) | Bit/dim 1.6059(1.7299) | Xent 8.0247(8.3168) | Loss 5.6182(5.8883) | Error 0.8278(0.8007) Steps 584(582.60) | Grad Norm 3.5964(16.5253) | Total Time 10.00(10.00)\n",
      "Iter 0870 | Time 17.3354(17.9953) | Bit/dim 1.5703(1.6896) | Xent 7.8286(8.2166) | Loss 5.4846(5.7979) | Error 0.7633(0.7959) Steps 560(580.76) | Grad Norm 4.4545(13.2295) | Total Time 10.00(10.00)\n",
      "Iter 0880 | Time 17.3873(17.8245) | Bit/dim 1.5666(1.6579) | Xent 7.6288(8.0876) | Loss 5.3810(5.7018) | Error 0.7811(0.7892) Steps 560(575.66) | Grad Norm 3.9920(10.5287) | Total Time 10.00(10.00)\n",
      "Iter 0890 | Time 17.0602(17.6987) | Bit/dim 1.5370(1.6292) | Xent 7.4049(7.9358) | Loss 5.2394(5.5971) | Error 0.7522(0.7816) Steps 560(571.55) | Grad Norm 1.9803(8.3468) | Total Time 10.00(10.00)\n",
      "Iter 0900 | Time 17.3735(17.6159) | Bit/dim 1.5651(1.6088) | Xent 7.1057(7.7543) | Loss 5.1179(5.4860) | Error 0.7656(0.7739) Steps 560(569.15) | Grad Norm 7.6954(7.3138) | Total Time 10.00(10.00)\n",
      "Iter 0910 | Time 17.6797(17.5858) | Bit/dim 1.5462(1.6019) | Xent 6.8905(7.5530) | Loss 4.9915(5.3784) | Error 0.8400(0.7795) Steps 566(566.98) | Grad Norm 19.3143(11.0904) | Total Time 10.00(10.00)\n",
      "Iter 0920 | Time 17.6080(17.6724) | Bit/dim 1.9964(1.6944) | Xent 6.6845(7.6269) | Loss 5.3386(5.5079) | Error 0.8100(0.7919) Steps 566(569.91) | Grad Norm 26.6439(25.5964) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0014 | Time 131.3010, Epoch Time 1306.8205(1038.4364), Bit/dim 1.7449, Xent 7.1531, Loss 5.3214, Error 0.7067\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0930 | Time 18.3731(17.8725) | Bit/dim 1.6548(1.7234) | Xent 7.1159(7.5209) | Loss 5.2128(5.4838) | Error 0.8467(0.8083) Steps 596(578.34) | Grad Norm 10.2489(24.5707) | Total Time 10.00(10.00)\n",
      "Iter 0940 | Time 18.1077(18.0068) | Bit/dim 1.5743(1.7032) | Xent 7.0537(7.3872) | Loss 5.1011(5.3968) | Error 0.7589(0.8085) Steps 584(584.15) | Grad Norm 7.9439(20.3219) | Total Time 10.00(10.00)\n",
      "Iter 0950 | Time 18.3143(18.0476) | Bit/dim 1.5496(1.6702) | Xent 6.7426(7.2406) | Loss 4.9209(5.2905) | Error 0.7278(0.8057) Steps 596(585.41) | Grad Norm 4.4552(16.3360) | Total Time 10.00(10.00)\n",
      "Iter 0960 | Time 18.6388(18.2103) | Bit/dim 1.5172(1.6365) | Xent 6.4591(7.0642) | Loss 4.7468(5.1686) | Error 0.7811(0.8001) Steps 602(589.76) | Grad Norm 3.1975(12.9328) | Total Time 10.00(10.00)\n",
      "Iter 0970 | Time 18.3232(18.3393) | Bit/dim 1.5246(1.6087) | Xent 6.1052(6.8514) | Loss 4.5772(5.0343) | Error 0.7344(0.7940) Steps 590(592.91) | Grad Norm 3.0635(10.2119) | Total Time 10.00(10.00)\n",
      "Iter 0980 | Time 18.3041(18.3113) | Bit/dim 1.5274(1.5875) | Xent 5.7261(6.6009) | Loss 4.3905(4.8880) | Error 0.8189(0.7952) Steps 590(592.15) | Grad Norm 5.2177(8.4757) | Total Time 10.00(10.00)\n",
      "Iter 0990 | Time 18.5540(18.3292) | Bit/dim 1.7333(1.5979) | Xent 6.0715(6.3814) | Loss 4.7691(4.7886) | Error 0.9433(0.8081) Steps 602(593.42) | Grad Norm 74.8294(17.0060) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0015 | Time 132.0564, Epoch Time 1361.7814(1048.1368), Bit/dim 1.7124, Xent 5.7237, Loss 4.5742, Error 0.5921\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1000 | Time 18.7162(18.4180) | Bit/dim 1.5824(1.5952) | Xent 5.2491(6.1359) | Loss 4.2069(4.6631) | Error 0.9022(0.8083) Steps 614(596.81) | Grad Norm 25.0895(21.3732) | Total Time 10.00(10.00)\n",
      "Iter 1010 | Time 19.0737(18.5343) | Bit/dim 1.7036(1.5897) | Xent 5.3954(5.8933) | Loss 4.4013(4.5364) | Error 0.6644(0.8042) Steps 614(600.14) | Grad Norm 72.3362(25.3842) | Total Time 10.00(10.00)\n",
      "Iter 1020 | Time 22.8760(19.3254) | Bit/dim 2.7841(2.0260) | Xent 6.8880(6.4542) | Loss 6.2281(5.2531) | Error 0.9256(0.8256) Steps 722(624.28) | Grad Norm 56.3159(51.0164) | Total Time 10.00(10.00)\n",
      "Iter 1030 | Time 23.3013(20.6363) | Bit/dim 1.9834(2.0879) | Xent 6.2164(6.4208) | Loss 5.0916(5.2983) | Error 0.8867(0.8434) Steps 752(666.35) | Grad Norm 10.2450(43.0123) | Total Time 10.00(10.00)\n",
      "Iter 1040 | Time 19.5340(20.6722) | Bit/dim 1.8255(2.0408) | Xent 6.2446(6.3808) | Loss 4.9479(5.2312) | Error 0.8056(0.8442) Steps 626(666.62) | Grad Norm 5.9446(33.9537) | Total Time 10.00(10.00)\n",
      "Iter 1050 | Time 19.6408(20.3672) | Bit/dim 1.7148(1.9635) | Xent 6.1215(6.3334) | Loss 4.7755(5.1302) | Error 0.7167(0.8247) Steps 632(655.41) | Grad Norm 3.8235(26.2590) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0016 | Time 132.8961, Epoch Time 1498.8819(1061.6591), Bit/dim 1.6724, Xent 5.9998, Loss 4.6723, Error 0.5851\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1060 | Time 19.0942(20.1044) | Bit/dim 1.6451(1.8879) | Xent 5.9432(6.2542) | Loss 4.6167(5.0150) | Error 0.6811(0.7935) Steps 614(646.34) | Grad Norm 1.8462(20.0385) | Total Time 10.00(10.00)\n",
      "Iter 1070 | Time 18.9046(19.8181) | Bit/dim 1.6184(1.8233) | Xent 5.7249(6.1405) | Loss 4.4808(4.8935) | Error 0.6656(0.7640) Steps 608(636.46) | Grad Norm 1.4788(15.3755) | Total Time 10.00(10.00)\n",
      "Iter 1080 | Time 18.4935(19.5657) | Bit/dim 1.6077(1.7712) | Xent 5.4975(5.9977) | Loss 4.3564(4.7701) | Error 0.6867(0.7439) Steps 608(628.23) | Grad Norm 1.6581(11.8127) | Total Time 10.00(10.00)\n",
      "Iter 1090 | Time 18.9323(19.4129) | Bit/dim 1.5710(1.7262) | Xent 5.2492(5.8293) | Loss 4.1956(4.6409) | Error 0.7233(0.7320) Steps 608(622.92) | Grad Norm 2.3813(9.1100) | Total Time 10.00(10.00)\n",
      "Iter 1100 | Time 19.2764(19.3091) | Bit/dim 1.5835(1.6903) | Xent 4.9783(5.6372) | Loss 4.0726(4.5089) | Error 0.7322(0.7296) Steps 608(617.94) | Grad Norm 2.1454(7.3019) | Total Time 10.00(10.00)\n",
      "Iter 1110 | Time 18.7010(19.3724) | Bit/dim 1.8739(1.7066) | Xent 5.8658(5.5775) | Loss 4.8068(4.4954) | Error 0.7878(0.7448) Steps 602(621.11) | Grad Norm 90.8828(19.6089) | Total Time 10.00(10.00)\n",
      "Iter 1120 | Time 18.8753(19.3680) | Bit/dim 1.6441(1.7026) | Xent 5.0588(5.4632) | Loss 4.1735(4.4342) | Error 0.8067(0.7522) Steps 602(621.56) | Grad Norm 23.7257(23.6245) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0017 | Time 132.6831, Epoch Time 1409.3934(1072.0912), Bit/dim 1.6450, Xent 4.8010, Loss 4.0455, Error 0.6624\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1130 | Time 19.2917(19.3263) | Bit/dim 1.5802(1.6861) | Xent 4.8528(5.2991) | Loss 4.0066(4.3357) | Error 0.7300(0.7450) Steps 620(621.05) | Grad Norm 13.6661(21.0362) | Total Time 10.00(10.00)\n",
      "Iter 1140 | Time 20.1927(19.3792) | Bit/dim 1.5358(1.6556) | Xent 4.5732(5.1261) | Loss 3.8224(4.2186) | Error 0.7533(0.7366) Steps 638(621.08) | Grad Norm 9.9505(18.4207) | Total Time 10.00(10.00)\n",
      "Iter 1150 | Time 19.6011(19.4236) | Bit/dim 1.5502(1.6297) | Xent 4.2441(4.9212) | Loss 3.6723(4.0903) | Error 0.7822(0.7403) Steps 620(622.16) | Grad Norm 11.1240(15.8839) | Total Time 10.00(10.00)\n",
      "Iter 1160 | Time 20.3886(19.4621) | Bit/dim 3.4550(1.7926) | Xent 11.3127(5.3866) | Loss 9.1114(4.4859) | Error 0.9033(0.7618) Steps 650(622.98) | Grad Norm 309.7737(46.8695) | Total Time 10.00(10.00)\n",
      "Iter 1170 | Time 19.6199(19.5811) | Bit/dim 1.6805(1.8398) | Xent 5.2042(5.4473) | Loss 4.2826(4.5635) | Error 0.7556(0.7875) Steps 626(626.12) | Grad Norm 21.4680(49.9625) | Total Time 10.00(10.00)\n",
      "Iter 1180 | Time 20.2812(19.9904) | Bit/dim 1.6694(1.8045) | Xent 5.2955(5.3833) | Loss 4.3172(4.4962) | Error 0.6944(0.7898) Steps 644(638.85) | Grad Norm 14.2455(40.6025) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0018 | Time 138.3245, Epoch Time 1460.5378(1083.7446), Bit/dim 1.5656, Xent 5.1586, Loss 4.1449, Error 0.5881\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1190 | Time 18.9732(19.9201) | Bit/dim 1.5903(1.7474) | Xent 5.1427(5.3426) | Loss 4.1617(4.4187) | Error 0.5422(0.7568) Steps 620(638.31) | Grad Norm 6.3604(31.8916) | Total Time 10.00(10.00)\n",
      "Iter 1200 | Time 18.8030(19.6659) | Bit/dim 1.5178(1.6969) | Xent 5.0175(5.2715) | Loss 4.0266(4.3327) | Error 0.5967(0.7128) Steps 608(632.60) | Grad Norm 4.4336(24.7803) | Total Time 10.00(10.00)\n",
      "Iter 1210 | Time 19.2581(19.5196) | Bit/dim 1.4954(1.6512) | Xent 4.8093(5.1708) | Loss 3.9000(4.2366) | Error 0.5600(0.6725) Steps 626(629.72) | Grad Norm 4.2680(19.1251) | Total Time 10.00(10.00)\n",
      "Iter 1220 | Time 19.0570(19.4478) | Bit/dim 1.5218(1.6160) | Xent 4.5719(5.0391) | Loss 3.8077(4.1355) | Error 0.5489(0.6380) Steps 620(627.81) | Grad Norm 2.9248(14.7686) | Total Time 10.00(10.00)\n",
      "Iter 1230 | Time 19.1152(19.3826) | Bit/dim 1.5224(1.5892) | Xent 4.3336(4.8810) | Loss 3.6892(4.0297) | Error 0.5700(0.6186) Steps 626(626.77) | Grad Norm 2.2473(11.4573) | Total Time 10.00(10.00)\n",
      "Iter 1240 | Time 19.1982(19.3371) | Bit/dim 1.5309(1.5862) | Xent 4.1235(4.7722) | Loss 3.5926(3.9723) | Error 0.6978(0.6396) Steps 626(626.41) | Grad Norm 8.6375(14.6707) | Total Time 10.00(10.00)\n",
      "Iter 1250 | Time 19.4802(19.3815) | Bit/dim 1.5393(1.5825) | Xent 4.0347(4.6208) | Loss 3.5567(3.8929) | Error 0.7222(0.6671) Steps 632(629.01) | Grad Norm 10.7297(15.9569) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0019 | Time 136.1039, Epoch Time 1419.0804(1093.8046), Bit/dim 1.5611, Xent 3.8750, Loss 3.4987, Error 0.5087\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1260 | Time 19.6447(19.4515) | Bit/dim 1.5054(1.5718) | Xent 3.8499(4.4387) | Loss 3.4304(3.7911) | Error 0.7922(0.6982) Steps 632(630.87) | Grad Norm 6.8724(15.1389) | Total Time 10.00(10.00)\n",
      "Iter 1270 | Time 19.6493(19.4591) | Bit/dim 1.5033(1.5603) | Xent 3.6098(4.2447) | Loss 3.3082(3.6826) | Error 0.8289(0.7289) Steps 638(631.97) | Grad Norm 10.0147(14.0478) | Total Time 10.00(10.00)\n",
      "Iter 1280 | Time 19.6653(19.4990) | Bit/dim 2.0015(1.5683) | Xent 5.1655(4.1085) | Loss 4.5843(3.6225) | Error 0.8822(0.7577) Steps 638(633.70) | Grad Norm 134.8663(19.1924) | Total Time 10.00(10.00)\n",
      "Iter 1290 | Time 20.4184(19.8097) | Bit/dim 1.8824(1.7034) | Xent 4.0038(4.3771) | Loss 3.8844(3.8920) | Error 0.8589(0.7898) Steps 662(642.33) | Grad Norm 36.1326(43.5309) | Total Time 10.00(10.00)\n",
      "Iter 1300 | Time 20.2662(20.0423) | Bit/dim 1.6337(1.6995) | Xent 3.9836(4.2779) | Loss 3.6255(3.8385) | Error 0.8056(0.8066) Steps 656(649.70) | Grad Norm 19.6278(39.4133) | Total Time 10.00(10.00)\n",
      "Iter 1310 | Time 19.6072(20.0806) | Bit/dim 1.5683(1.6671) | Xent 3.8166(4.1780) | Loss 3.4766(3.7560) | Error 0.7844(0.8049) Steps 644(650.54) | Grad Norm 9.7817(32.7448) | Total Time 10.00(10.00)\n",
      "Iter 1320 | Time 19.9631(20.0144) | Bit/dim 1.5046(1.6294) | Xent 3.6705(4.0636) | Loss 3.3398(3.6612) | Error 0.8267(0.8049) Steps 650(650.86) | Grad Norm 7.0594(26.4083) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0020 | Time 140.6326, Epoch Time 1476.9341(1105.2985), Bit/dim 1.5048, Xent 3.6334, Loss 3.3215, Error 0.3941\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1330 | Time 19.9198(19.9701) | Bit/dim 1.5044(1.5971) | Xent 3.4323(3.9218) | Loss 3.2206(3.5580) | Error 0.8522(0.8104) Steps 656(650.39) | Grad Norm 2.5019(21.2559) | Total Time 10.00(10.00)\n",
      "Iter 1340 | Time 19.8675(19.9678) | Bit/dim 1.5142(1.5698) | Xent 3.1691(3.7549) | Loss 3.0987(3.4472) | Error 0.8700(0.8216) Steps 644(649.81) | Grad Norm 7.6076(17.2063) | Total Time 10.00(10.00)\n",
      "Iter 1350 | Time 20.6341(19.9937) | Bit/dim 2.3238(1.5801) | Xent 6.2289(3.6865) | Loss 5.4383(3.4233) | Error 0.8744(0.8321) Steps 662(650.15) | Grad Norm 300.7948(29.0710) | Total Time 10.00(10.00)\n",
      "Iter 1360 | Time 24.2385(20.4957) | Bit/dim 2.4955(2.1509) | Xent 5.4768(5.4208) | Loss 5.2339(4.8613) | Error 0.8889(0.8498) Steps 764(664.53) | Grad Norm 54.8891(81.7866) | Total Time 10.00(10.00)\n",
      "Iter 1370 | Time 22.3739(21.2174) | Bit/dim 2.0215(2.1322) | Xent 4.8256(5.3076) | Loss 4.4343(4.7860) | Error 0.9033(0.8615) Steps 722(684.89) | Grad Norm 18.3459(66.9818) | Total Time 10.00(10.00)\n",
      "Iter 1380 | Time 23.4857(21.8026) | Bit/dim 1.7906(2.0482) | Xent 4.8531(5.1987) | Loss 4.2171(4.6476) | Error 0.8822(0.8595) Steps 734(702.36) | Grad Norm 9.8650(52.5712) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0021 | Time 147.2010, Epoch Time 1579.0558(1119.5112), Bit/dim 1.6662, Xent 4.8787, Loss 4.1055, Error 0.7667\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1390 | Time 21.1736(21.8764) | Bit/dim 1.6744(1.9570) | Xent 4.8235(5.1118) | Loss 4.0862(4.5129) | Error 0.7178(0.8441) Steps 698(705.81) | Grad Norm 6.0622(40.5298) | Total Time 10.00(10.00)\n",
      "Iter 1400 | Time 21.5358(21.7843) | Bit/dim 1.6106(1.8688) | Xent 4.7601(5.0312) | Loss 3.9907(4.3844) | Error 0.7378(0.8131) Steps 698(703.89) | Grad Norm 4.1582(31.1241) | Total Time 10.00(10.00)\n",
      "Iter 1410 | Time 21.3648(21.6533) | Bit/dim 1.5657(1.7951) | Xent 4.6660(4.9445) | Loss 3.8987(4.2674) | Error 0.6933(0.7874) Steps 704(701.49) | Grad Norm 3.4115(23.7840) | Total Time 10.00(10.00)\n",
      "Iter 1420 | Time 21.3385(21.5851) | Bit/dim 1.5365(1.7305) | Xent 4.5456(4.8543) | Loss 3.8093(4.1577) | Error 0.6589(0.7560) Steps 686(698.01) | Grad Norm 1.5032(18.0660) | Total Time 10.00(10.00)\n",
      "Iter 1430 | Time 21.5073(21.5288) | Bit/dim 1.4984(1.6752) | Xent 4.4431(4.7571) | Loss 3.7200(4.0537) | Error 0.6278(0.7247) Steps 686(695.34) | Grad Norm 1.7420(13.7335) | Total Time 10.00(10.00)\n",
      "Iter 1440 | Time 20.9566(21.4039) | Bit/dim 1.5158(1.6318) | Xent 4.3164(4.6549) | Loss 3.6740(3.9592) | Error 0.6111(0.6992) Steps 674(691.55) | Grad Norm 1.2358(10.4762) | Total Time 10.00(10.00)\n",
      "Iter 1450 | Time 20.9160(21.3177) | Bit/dim 1.4804(1.5943) | Xent 4.2009(4.5486) | Loss 3.5808(3.8686) | Error 0.6067(0.6799) Steps 674(688.05) | Grad Norm 1.3745(8.1062) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0022 | Time 146.1447, Epoch Time 1565.3941(1132.8877), Bit/dim 1.4815, Xent 4.1597, Loss 3.5613, Error 0.5539\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1460 | Time 20.7505(21.2270) | Bit/dim 1.4725(1.5654) | Xent 4.0754(4.4379) | Loss 3.5101(3.7843) | Error 0.6589(0.6676) Steps 668(685.41) | Grad Norm 1.0422(6.2982) | Total Time 10.00(10.00)\n",
      "Iter 1470 | Time 20.7777(21.0882) | Bit/dim 1.4761(1.5416) | Xent 3.9528(4.3236) | Loss 3.4525(3.7034) | Error 0.6456(0.6634) Steps 674(682.04) | Grad Norm 1.2988(5.0000) | Total Time 10.00(10.00)\n",
      "Iter 1480 | Time 20.7538(21.0013) | Bit/dim 1.4725(1.5231) | Xent 3.8228(4.2063) | Loss 3.3839(3.6263) | Error 0.7311(0.6694) Steps 668(679.28) | Grad Norm 1.4308(4.0549) | Total Time 10.00(10.00)\n",
      "Iter 1490 | Time 20.6477(20.9025) | Bit/dim 1.4541(1.5072) | Xent 3.6977(4.0859) | Loss 3.3029(3.5502) | Error 0.7456(0.6854) Steps 680(678.38) | Grad Norm 1.4615(3.3445) | Total Time 10.00(10.00)\n",
      "Iter 1500 | Time 20.5987(20.8299) | Bit/dim 1.4523(1.4950) | Xent 3.5567(3.9621) | Loss 3.2306(3.4760) | Error 0.8033(0.7041) Steps 674(677.69) | Grad Norm 3.0974(3.0242) | Total Time 10.00(10.00)\n",
      "Iter 1510 | Time 21.0386(20.8457) | Bit/dim 1.4597(1.4850) | Xent 3.4160(3.8353) | Loss 3.1677(3.4026) | Error 0.7878(0.7282) Steps 674(676.89) | Grad Norm 6.7866(3.2555) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0023 | Time 145.1569, Epoch Time 1530.3089(1144.8104), Bit/dim 1.5422, Xent 3.6366, Loss 3.3605, Error 0.6590\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1520 | Time 20.7305(20.8683) | Bit/dim 1.5198(1.4987) | Xent 3.3549(3.7762) | Loss 3.1972(3.3868) | Error 0.8456(0.7565) Steps 668(677.28) | Grad Norm 25.9997(14.3234) | Total Time 10.00(10.00)\n",
      "Iter 1530 | Time 21.1389(20.8741) | Bit/dim 1.4910(1.5079) | Xent 3.3708(3.6694) | Loss 3.1764(3.3426) | Error 0.8411(0.7799) Steps 674(676.50) | Grad Norm 22.6906(17.1727) | Total Time 10.00(10.00)\n",
      "Iter 1540 | Time 20.6715(20.9136) | Bit/dim 1.4615(1.5000) | Xent 3.1841(3.5555) | Loss 3.0536(3.2778) | Error 0.8656(0.8002) Steps 674(676.47) | Grad Norm 9.6764(16.6892) | Total Time 10.00(10.00)\n",
      "Iter 1550 | Time 20.9964(20.9462) | Bit/dim 1.4453(1.4898) | Xent 3.0473(3.4381) | Loss 2.9689(3.2089) | Error 0.8900(0.8178) Steps 686(678.07) | Grad Norm 8.9802(16.0737) | Total Time 10.00(10.00)\n",
      "Iter 1560 | Time 21.2212(20.9395) | Bit/dim 1.4587(1.4825) | Xent 2.9091(3.3120) | Loss 2.9132(3.1385) | Error 0.8656(0.8313) Steps 686(678.41) | Grad Norm 8.9082(14.9292) | Total Time 10.00(10.00)\n",
      "Iter 1570 | Time 21.0676(20.9068) | Bit/dim 1.5776(1.4899) | Xent 3.2841(3.2306) | Loss 3.2196(3.1052) | Error 0.8800(0.8444) Steps 686(678.88) | Grad Norm 92.0755(23.4841) | Total Time 10.00(10.00)\n",
      "Iter 1580 | Time 21.1396(20.9355) | Bit/dim 1.5087(1.4945) | Xent 2.6629(3.1159) | Loss 2.8401(3.0524) | Error 0.8900(0.8542) Steps 668(679.63) | Grad Norm 20.6372(26.5759) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0024 | Time 144.8934, Epoch Time 1541.5266(1156.7119), Bit/dim 1.4857, Xent 2.5989, Loss 2.7852, Error 0.4898\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1590 | Time 21.4453(21.0174) | Bit/dim 1.4575(1.4925) | Xent 2.5720(2.9839) | Loss 2.7435(2.9844) | Error 0.8833(0.8604) Steps 704(681.74) | Grad Norm 20.7008(25.0180) | Total Time 10.00(10.00)\n",
      "Iter 1600 | Time 20.6478(21.0262) | Bit/dim 1.4785(1.4867) | Xent 2.4536(2.8564) | Loss 2.7053(2.9149) | Error 0.8811(0.8663) Steps 674(682.41) | Grad Norm 35.1548(24.6475) | Total Time 10.00(10.00)\n",
      "Iter 1610 | Time 20.2839(21.0059) | Bit/dim 2.9443(1.5712) | Xent 7.4552(3.0577) | Loss 6.6718(3.1000) | Error 0.9022(0.8736) Steps 656(681.70) | Grad Norm 424.0742(57.4012) | Total Time 10.00(10.00)\n",
      "Iter 1620 | Time 20.7050(21.0502) | Bit/dim 1.5971(1.7492) | Xent 3.1315(3.6699) | Loss 3.1628(3.5842) | Error 0.8667(0.8796) Steps 680(683.06) | Grad Norm 27.4341(91.7573) | Total Time 10.00(10.00)\n",
      "Iter 1630 | Time 20.9319(21.0333) | Bit/dim 1.5801(1.7162) | Xent 3.4268(3.6384) | Loss 3.2935(3.5354) | Error 0.8733(0.8779) Steps 686(682.30) | Grad Norm 24.4208(78.8378) | Total Time 10.00(10.00)\n",
      "Iter 1640 | Time 21.2196(21.0416) | Bit/dim 1.4949(1.6675) | Xent 3.4008(3.5828) | Loss 3.1953(3.4589) | Error 0.7500(0.8583) Steps 680(682.58) | Grad Norm 14.4271(63.4404) | Total Time 10.00(10.00)\n",
      "Iter 1650 | Time 20.4861(21.0116) | Bit/dim 1.4711(1.6209) | Xent 3.2826(3.5178) | Loss 3.1124(3.3798) | Error 0.7544(0.8362) Steps 668(681.62) | Grad Norm 7.6138(49.8327) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0025 | Time 145.9228, Epoch Time 1548.8105(1168.4748), Bit/dim 1.4507, Xent 3.2914, Loss 3.0964, Error 0.5273\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1660 | Time 21.2647(20.9809) | Bit/dim 1.4528(1.5792) | Xent 3.1722(3.4385) | Loss 3.0389(3.2985) | Error 0.7744(0.8225) Steps 680(681.85) | Grad Norm 5.4297(38.4937) | Total Time 10.00(10.00)\n",
      "Iter 1670 | Time 21.3611(21.0676) | Bit/dim 1.4397(1.5416) | Xent 3.0113(3.3434) | Loss 2.9454(3.2133) | Error 0.7733(0.8097) Steps 686(683.36) | Grad Norm 2.5158(29.4814) | Total Time 10.00(10.00)\n",
      "Iter 1680 | Time 21.2166(21.1232) | Bit/dim 1.4273(1.5121) | Xent 2.8582(3.2330) | Loss 2.8564(3.1286) | Error 0.8200(0.8064) Steps 692(685.81) | Grad Norm 2.8393(22.5496) | Total Time 10.00(10.00)\n",
      "Iter 1690 | Time 21.3386(21.1665) | Bit/dim 1.4169(1.4882) | Xent 2.7011(3.1114) | Loss 2.7674(3.0439) | Error 0.8156(0.8059) Steps 680(686.77) | Grad Norm 2.7168(17.3061) | Total Time 10.00(10.00)\n",
      "Iter 1700 | Time 21.2002(21.2468) | Bit/dim 1.4312(1.4698) | Xent 2.5403(2.9797) | Loss 2.7014(2.9597) | Error 0.8422(0.8125) Steps 692(688.01) | Grad Norm 2.5636(13.3839) | Total Time 10.00(10.00)\n",
      "Iter 1710 | Time 21.8038(21.3318) | Bit/dim 1.4160(1.4559) | Xent 2.3770(2.8400) | Loss 2.6045(2.8759) | Error 0.8511(0.8218) Steps 692(689.06) | Grad Norm 2.5889(10.4321) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0026 | Time 147.3916, Epoch Time 1567.2083(1180.4368), Bit/dim 1.4174, Xent 2.2544, Loss 2.5446, Error 0.4739\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1720 | Time 21.1861(21.3093) | Bit/dim 1.4268(1.4464) | Xent 2.2165(2.6933) | Loss 2.5350(2.7930) | Error 0.8689(0.8344) Steps 686(688.36) | Grad Norm 5.6746(8.7268) | Total Time 10.00(10.00)\n",
      "Iter 1730 | Time 21.2197(21.2905) | Bit/dim 1.4583(1.4407) | Xent 2.0372(2.5416) | Loss 2.4769(2.7115) | Error 0.8589(0.8453) Steps 674(686.89) | Grad Norm 22.7262(9.0759) | Total Time 10.00(10.00)\n",
      "Iter 1740 | Time 23.8258(21.5612) | Bit/dim 3.4480(2.0285) | Xent 4.3495(4.2094) | Loss 5.6228(4.1332) | Error 0.8933(0.8577) Steps 752(693.40) | Grad Norm 99.1082(88.9854) | Total Time 10.00(10.00)\n",
      "Iter 1750 | Time 22.5147(21.8394) | Bit/dim 2.1871(2.1666) | Xent 3.6286(4.1396) | Loss 4.0014(4.2364) | Error 0.8611(0.8642) Steps 716(701.27) | Grad Norm 34.3044(81.5138) | Total Time 10.00(10.00)\n",
      "Iter 1760 | Time 20.6177(21.7810) | Bit/dim 1.7838(2.1050) | Xent 3.5339(3.9928) | Loss 3.5507(4.1014) | Error 0.8333(0.8640) Steps 686(699.76) | Grad Norm 13.5002(64.8686) | Total Time 10.00(10.00)\n",
      "Iter 1770 | Time 20.8480(21.6693) | Bit/dim 1.6405(1.9928) | Xent 3.4476(3.8626) | Loss 3.3643(3.9241) | Error 0.8111(0.8367) Steps 680(695.00) | Grad Norm 8.1838(50.2779) | Total Time 10.00(10.00)\n",
      "Iter 1780 | Time 20.1390(21.3050) | Bit/dim 1.5545(1.8822) | Xent 3.3955(3.7513) | Loss 3.2523(3.7578) | Error 0.8011(0.8144) Steps 644(683.77) | Grad Norm 4.7071(38.6275) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0027 | Time 140.1423, Epoch Time 1572.2608(1192.1915), Bit/dim 1.5321, Xent 3.3601, Loss 3.2121, Error 0.3801\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1790 | Time 20.0884(20.9665) | Bit/dim 1.5095(1.7886) | Xent 3.2732(3.6402) | Loss 3.1461(3.6087) | Error 0.7856(0.8030) Steps 650(673.85) | Grad Norm 4.0042(29.5385) | Total Time 10.00(10.00)\n",
      "Iter 1800 | Time 20.4167(20.7366) | Bit/dim 1.4923(1.7095) | Xent 3.1818(3.5324) | Loss 3.0833(3.4757) | Error 0.8178(0.7988) Steps 644(666.94) | Grad Norm 2.9674(22.5520) | Total Time 10.00(10.00)\n",
      "Iter 1810 | Time 20.0135(20.5315) | Bit/dim 1.4530(1.6461) | Xent 3.0684(3.4232) | Loss 2.9872(3.3577) | Error 0.8256(0.8004) Steps 650(661.27) | Grad Norm 2.1581(17.2612) | Total Time 10.00(10.00)\n",
      "Iter 1820 | Time 20.0510(20.4424) | Bit/dim 1.4533(1.5970) | Xent 2.9535(3.3119) | Loss 2.9300(3.2529) | Error 0.8133(0.8019) Steps 656(658.37) | Grad Norm 2.1611(13.1995) | Total Time 10.00(10.00)\n",
      "Iter 1830 | Time 20.7041(20.4477) | Bit/dim 1.4259(1.5549) | Xent 2.8240(3.1978) | Loss 2.8379(3.1538) | Error 0.8344(0.8091) Steps 656(657.75) | Grad Norm 1.2378(10.2445) | Total Time 10.00(10.00)\n",
      "Iter 1840 | Time 20.2585(20.4448) | Bit/dim 1.4390(1.5249) | Xent 2.6962(3.0814) | Loss 2.7871(3.0656) | Error 0.8311(0.8146) Steps 656(657.29) | Grad Norm 3.0919(8.1029) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0028 | Time 144.9512, Epoch Time 1492.3153(1201.1952), Bit/dim 1.4249, Xent 2.5882, Loss 2.7191, Error 0.3807\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1850 | Time 19.9738(20.4139) | Bit/dim 1.4246(1.5014) | Xent 2.5698(2.9627) | Loss 2.7095(2.9827) | Error 0.8444(0.8233) Steps 656(657.55) | Grad Norm 1.3729(6.4023) | Total Time 10.00(10.00)\n",
      "Iter 1860 | Time 20.3633(20.3558) | Bit/dim 1.4348(1.4826) | Xent 2.4404(2.8410) | Loss 2.6550(2.9031) | Error 0.8611(0.8308) Steps 650(656.96) | Grad Norm 3.2731(5.3012) | Total Time 10.00(10.00)\n",
      "Iter 1870 | Time 20.1737(20.3302) | Bit/dim 1.4227(1.4687) | Xent 2.3079(2.7167) | Loss 2.5767(2.8271) | Error 0.8667(0.8377) Steps 650(655.73) | Grad Norm 3.7050(4.6233) | Total Time 10.00(10.00)\n",
      "Iter 1880 | Time 20.9202(20.3029) | Bit/dim 1.4384(1.4600) | Xent 2.2286(2.5921) | Loss 2.5527(2.7560) | Error 0.8733(0.8454) Steps 674(654.42) | Grad Norm 48.2362(7.7585) | Total Time 10.00(10.00)\n",
      "Iter 1890 | Time 22.6112(20.4537) | Bit/dim 1.8222(1.6156) | Xent 2.7069(3.0307) | Loss 3.1756(3.1309) | Error 0.9000(0.8535) Steps 710(658.38) | Grad Norm 57.3823(77.5705) | Total Time 10.00(10.00)\n",
      "Iter 1900 | Time 22.1458(20.7707) | Bit/dim 1.5567(1.6494) | Xent 2.9119(2.9887) | Loss 3.0127(3.1438) | Error 0.8944(0.8658) Steps 698(667.51) | Grad Norm 60.0277(77.2797) | Total Time 10.00(10.00)\n",
      "Iter 1910 | Time 21.4590(20.9268) | Bit/dim 1.4954(1.6184) | Xent 2.6502(2.9164) | Loss 2.8205(3.0766) | Error 0.8911(0.8750) Steps 680(671.78) | Grad Norm 19.0410(65.3951) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0029 | Time 150.2696, Epoch Time 1532.6999(1211.1404), Bit/dim 1.4677, Xent 2.6266, Loss 2.7811, Error 0.5694\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1920 | Time 21.0446(20.9616) | Bit/dim 1.4461(1.5795) | Xent 2.5899(2.8379) | Loss 2.7410(2.9985) | Error 0.9089(0.8828) Steps 680(674.10) | Grad Norm 21.2541(53.0566) | Total Time 10.00(10.00)\n",
      "Iter 1930 | Time 20.6361(20.9252) | Bit/dim 1.4403(1.5414) | Xent 2.4491(2.7503) | Loss 2.6648(2.9166) | Error 0.9056(0.8885) Steps 674(675.05) | Grad Norm 12.0447(42.2149) | Total Time 10.00(10.00)\n",
      "Iter 1940 | Time 20.8424(20.9516) | Bit/dim 1.4193(1.5106) | Xent 2.3232(2.6514) | Loss 2.5809(2.8363) | Error 0.9022(0.8921) Steps 674(676.31) | Grad Norm 10.5174(33.4896) | Total Time 10.00(10.00)\n",
      "Iter 1950 | Time 20.8084(20.9126) | Bit/dim 1.4019(1.4843) | Xent 2.1771(2.5430) | Loss 2.4904(2.7558) | Error 0.8933(0.8933) Steps 668(675.35) | Grad Norm 6.3236(26.2490) | Total Time 10.00(10.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_generative.py --data mnist --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments/cnf_conditional_generative_bs900_sratio_0_25_drop_0_5_seed_1 --seed 1 --conditional True --controlled_tol False --train_mode semisup --log_freq 10 --weight_y 0.5 --condition_ratio 0.25 --dropout_rate 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
